{"meta":{"title":"Blair's Blog","subtitle":"春有百花秋有月，夏有涼風冬有雪 .","description":"Everyone should not forget his dream","author":"Blair Chan","url":"http://www.iequa.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2021-06-17T09:48:28.964Z","updated":"2021-06-17T09:48:28.955Z","comments":true,"path":"404.html","permalink":"http://www.iequa.com/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2019-10-20T04:30:37.076Z","updated":"2019-10-20T04:30:37.076Z","comments":true,"path":"52binge.blog.src.html","permalink":"http://www.iequa.com/52binge.blog.src.html","excerpt":"","text":"…or create a new repository on the command line echo “# 52binge.blog.source” &gt;&gt; README.md git init git add README.md git commit -m “first commit” git remote add origin https://github.com/52binge/ 52binge.blog.source.git git push -u origin master …or push an existing repository from the command line git remote add origin https://github.com/52binge/ 52binge.blog.source.git git push -u origin master …or import code from another repository You can initialize this repository with code from a Subversion, Mercurial, or TFS project."},{"title":"","date":"2021-06-13T16:33:08.861Z","updated":"2021-06-13T16:33:08.861Z","comments":true,"path":"manifest.json","permalink":"http://www.iequa.com/manifest.json","excerpt":"","text":"{\"short_name\":\"Volantis\",\"name\":\"Volantis\",\"icons\":[{\"src\":\"/assets/favicon/favicon_256.png\",\"type\":\"image/png\",\"sizes\":\"256x256\"},{\"src\":\"/assets/favicon/favicon_192.png\",\"type\":\"image/png\",\"sizes\":\"192x192\"},{\"src\":\"/assets/favicon/favicon_180.png\",\"type\":\"image/png\",\"sizes\":\"180x180\"},{\"src\":\"/assets/favicon/favicon_144.png\",\"type\":\"image/png\",\"sizes\":\"144x144\"},{\"src\":\"/assets/favicon/favicon_128.png\",\"type\":\"image/png\",\"sizes\":\"128x128\"}],\"background_color\":\"#ffffff\",\"theme_color\":\"#ffffff\",\"display\":\"standalone\",\"start_url\":\"./index.html\"}"},{"title":"About","date":"2021-06-15T03:30:48.000Z","updated":"2021-06-20T07:02:30.827Z","comments":true,"path":"about/index.html","permalink":"http://www.iequa.com/about/index.html","excerpt":"","text":"👨🏻‍🎓 𝗺𝗲, always 18 years old, a student, ‍シンガポールに住🇸🇬 Kimo Otaku, Lazy🐶, Vegetable, Want to study IELTS, No Offer, Poverty. Welcome to communicate, learn and progress together ! contact: email-to-me. ⭐ Who am I Blair Chen a data developer, GTD practitioner, live in Singapore, Singapore. Blair Chen focus on python/SQL, data engineering, data modeling, computer science. S.E.A. Aquarium Singapore. 🔑 Classic line 友情觀： Whatever you do in this life, it’s not legendary, unless your friends are there to see it. 人生觀： Life is full of changes. One day you have an apartment, the next day it’s a house of dumplings. But the important stuff doesn’t change. To the important stuff. 愛情觀： When you’re single, and your friends start to get married, every wedding invitation presents a strange moment of self-evaluation: “Will you be bringing a guest, or will you be attending alone?” ⛅️ ChengeLog 2021-06-13 : Blair Chen, more info, please click. 2017-10-08 : Blair Chen, blairos theme improve. 2016-03-22 : Blair Chen, build this blog website."},{"title":"About","date":"2021-06-15T03:30:48.000Z","updated":"2021-06-20T07:02:16.324Z","comments":true,"path":"aboutMe/index.html","permalink":"http://www.iequa.com/aboutMe/index.html","excerpt":"","text":"👨🏻‍🎓 𝗺𝗲, always 18 years old, a student, ‍シンガポールに住🇸🇬 Kimo Otaku, Lazy🐶, Vegetable, Want to study IELTS, No Offer, Poverty. Welcome to communicate, learn and progress together ! contact: email-to-me. ⭐ Who am I Blair Chen a data developer, GTD practitioner, live in Singapore, Singapore. Blair Chen focus on python/SQL, data engineering, data modeling, computer science. S.E.A. Aquarium Singapore. 🔑 Classic line 友情觀： Whatever you do in this life, it’s not legendary, unless your friends are there to see it. 人生觀： Life is full of changes. One day you have an apartment, the next day it’s a house of dumplings. But the important stuff doesn’t change. To the important stuff. 愛情觀： When you’re single, and your friends start to get married, every wedding invitation presents a strange moment of self-evaluation: “Will you be bringing a guest, or will you be attending alone?” ⛅️ ChengeLog 2021-06-13 : Blair Chen, more info, please click. 2017-10-08 : Blair Chen, blairos theme improve. 2016-03-22 : Blair Chen, build this blog website."},{"title":"","date":"2020-12-07T00:05:52.079Z","updated":"2020-12-07T00:05:52.078Z","comments":true,"path":"ai/index.html","permalink":"http://www.iequa.com/ai/index.html","excerpt":"","text":"NLP 的发展 NLP 神经网络发展历史中最重要的 8 个里程碑 Language Model (语言模型就是要看到上文预测下文, So NNLM) n-gram model（n元模型）（基于 马尔可夫假设 思想）上下文相关的特性 建立数学模型。 2001 - NNLM , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。 2008 - Multi-task learning 2013 - Word2Vec (Word Embedding的工具word2vec : CBOW 和 Skip-gram) 2014 - sequence-to-sequence 2015 - Attention 2015 - Memory-based networks 2018 - Pretrained language models good 张俊林: 深度学习中的注意力模型（2017版） good 张俊林: 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史 1. Language Model P(wi∣w1,w2,...,wi−1)=P(wi∣wi−N+1,wi−N+2,...,wi−1)P(w_{i}|w_{1}, w_{2}, ..., w_{i-1}) = P(w_i | w_{i-N+1}, w_{i-N+2}, ..., w_{i-1}) P(wi​∣w1​,w2​,...,wi−1​)=P(wi​∣wi−N+1​,wi−N+2​,...,wi−1​) 2. Perplexity 计算perplexity的公式如下： perplexity 刻画的是语言模型预测一个语言样本的能力. 比如已经知道 (w1,w2,w3,…,wm) 这句话会出现在语料库之中，那么通过语言模型计算得到的这句话的概率越高，说明语言模型对这个语料库拟合得越好。 perplexity 实际是计算每一个单词得到的概率倒数的 几何平均(geometric mean) ，因此 perplexity 可以理解为平均分支系数（average branching factor），即模型预测下一个词时的平均可选择数量。 参见： arithmetic average vs geometric mean 在语言模型的训练中，通常采用 perplexity 的 log 表达形式： 相比较乘积求平方根的方式，加法的形式可加速计算，同时避免概率乘积数值过小而导致浮点数向下溢出的问题. 在数学上，log perplexity 可以看作真实分布与预测分布之间的交叉熵 Cross Entropy, 交叉熵描述了两个概率分布之间的一种距离. log perplexity 和 Cross Entropy 是等价的 2. Recurrent Neural Networks 输入和输出的长度不尽相同 无法共享从其他位置学来的特征 很多数据是以序列形式存在的，例如文本、语音、视频、点击流等等。 Typical RNN Structure: 在 hTh_ThT​ 后面直接接一个 Softmax 层，输出文本所属类别的预测概率 yyy，就可以实现文本分类. 可应用于多种具体任务： nett=Uxt+Wht−1net_{t}=U x_{t}+W h_{t-1} nett​=Uxt​+Wht−1​ ht=f(nett)h_{t}=f\\left(\\text {net}_{t}\\right) ht​=f(nett​) y=g(VhT)y=g\\left(V h_{T}\\right) y=g(VhT​) 其中 fff 和 ggg 为激活函数，UUU 为输入层到隐含层的权重矩阵，WWW 为隐含层从上一时刻到下一时刻状态转移的权重矩阵。在文本分类任务中，fff 可以选取 Tanh 函数或者 ReLU 函数，ggg 可以采用 Softmax 函数。 TensorFlow RNN 更多详情参见本博： TensorFlow：第8章 Recurrent Neural Networks 1 RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。 RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。 Recurrent Neural Networks 3. NNLM NNLM,直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程. 使用词嵌三步 获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库 应用词嵌：将获得的词嵌应用在我们的训练任务中 可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了） 4. word2vec word2vec 并不是一个模型， 而是一个 2013年 google 发表的工具. 该工具包含2个模型： Skip-Gram CBOW. 该工具包含2种高效训练方法： negative sampling hierarchicam softmax. 词向量（词的特征向量）既能够降低维度，又能够capture到当前词在本句子中上下文的信息 CBOW (context(W)-&gt;center) 纠错 : 上图”目标函数“的第一个公式，应该是 连乘 公式，不是 连加 运算。 理解 : 背景词向量与 中心词向量 内积 等部分，你可考虑 softmax w \\* x+b 中 xxx 和 www 的关系来理解. 4.1 Negative Sampling 1）如何通过一个正例和neg个负例进行二元逻辑回归呢？ 2） 如何进行负采样呢？ 如何进行 negative sampling？ 每个词𝑤的线段长度由下式决定： len(w)=count(w)∑u∈vocabcount(u)len(w) = \\frac{count(w)}{\\sum\\limits_{u \\in vocab} count(u)} len(w)=u∈vocab∑​count(u)count(w)​ 在word2vec中，分子和分母都取了3/4次幂如下： len(w)=count(w)3/4∑u∈vocabcount(u)3/4len(w) = \\frac{count(w)^{3/4}}{\\sum\\limits_{u \\in vocab} count(u)^{3/4}} len(w)=u∈vocab∑​count(u)3/4count(w)3/4​ 负采样这个点引入 word2vec 非常巧妙，两个作用， 加速了模型计算 保证了模型训练的效果 第一，model 每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢。 第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点聪明. good, word2vec Negative Sampling 刘建平Pinard 4.2 Hierarchicam Softmax 知乎: Huffman Tree 给定n权值作为n个叶子节点，构造一棵二叉树，若这棵二叉树的带权路径长度达到最小，则称这样的二叉树为最优二叉树，也称为Huffman树。 word2vec vs glove 目标函数不同 （crossentrpy vs 平方损失函数） glove 全局统计固定语料信息 word2vec 是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。 总体来看，glove 可以被看作是更换了目标函数和权重函数的全局 word2vec。 5. fastText FastText是一个快速文本分类算法，在使用标准多核CPU的情况下，在10分钟内可以对超过10亿个单词进行训练。 不需要使用预先训练好的词向量，因为FastText会自己训练词向量。 fastText 能够做到效果好，速度快，主要依靠两个秘密武器： 结构与CBOW类似，但学习目标是人工标注的分类结果； 用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick. 引入 N-gram，考虑词序特征； 引入 subword 来处理长词，处理未登陆词问题； 6. Seq2Seq Source 和 Target 分别由各自的单词序列构成： Source=(x1,x2,...,xm)Source = ({x}_1, {x}_2, ..., {x}_m) Source=(x1​,x2​,...,xm​) Target=(y1,y2,...,yn)Target = ({y}_1, {y}_2, ..., {y}_n) Target=(y1​,y2​,...,yn​) Encoder 顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C： C=F(x1,x2,...,xm)C = F({x}_1, {x}_2, ..., {x}_m) C=F(x1​,x2​,...,xm​) 对于 Decoder 来说，其任务是根据句子 Source 的 中间语义表示 C 和 之前已经生成的历史信息 (y1,y2,...,yi−1)({y}_1, {y}_2, ..., {y}_{i-1}) (y1​,y2​,...,yi−1​) 来生成 i时刻 要生成的单词 yi{y}_{i}yi​ yi=g(C,y1,y2,...,yi−1)y_{i} = g(C, {y}_1, {y}_2, ..., {y}_{i-1}) yi​=g(C,y1​,y2​,...,yi−1​) 每个 yiy_iyi​ 都依次这么产生，那么看起来就是整个系统根据输入 句子Source 生成了目标句子Target。 (1). 如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题； (2). 如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要； (3). 如果Source是一句问句，Target是一句回答，那么这是问答系统。 在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。 train seq2seq model 根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率 \\begin{split}\\begin{aligned} {P}(y_1, \\ldots, y_{T&#039;} \\mid x_1, \\ldots, x_T) &amp;= \\prod_{t&#039;=1}^{T&#039;} {P}(y_{t&#039;} \\mid y_1, \\ldots, y_{t&#039;-1}, x_1, \\ldots, x_T)\\\\\\\\ &amp;= \\prod_{t&#039;=1}^{T&#039;} {P}(y_{t&#039;} \\mid y_1, \\ldots, y_{t&#039;-1}, \\boldsymbol{c}), \\end{aligned}\\end{split} 并得到该输出序列的损失 −log⁡P(y1,…,yT′∣x1,…,xT)=−∑t′=1T′log⁡P(yt′∣y1,…,yt′−1,c), - \\log{P}(y_1, \\ldots, y_{T&#x27;} \\mid x_1, \\ldots, x_T) = -\\sum_{t&#x27;=1}^{T&#x27;} \\log {P}(y_{t&#x27;} \\mid y_1, \\ldots, y_{t&#x27;-1}, \\boldsymbol{c}), −logP(y1​,…,yT′​∣x1​,…,xT​)=−t′=1∑T′​logP(yt′​∣y1​,…,yt′−1​,c), 在 train 中，所有输出序列损失的均值通常作为需要最小化的损失函数。 在 predict 中，我们需要将decode在上一个时间步的输出作为当前时间步的输入 Or teacher forcing。 summary Encoder—Decoder（seq2seq）可以输入并输出不定长的序列。Encoder—Decoder 使用了两个 RNN . Encoder—Decoder 的训练中，我们可以采用 teacher forcing。(这也是 Seq2Seq 2 的内容) 将source进行反序输入：输入的时候将“ABC”变成“CBA”，这样做的好处是解决了长序列的long-term依赖，使得模型可以学习到更多的对应关系，从而达到比较好的效果。 Beam Search：这是在test时的技巧，也就是在训练过程中不会使用。 7. Attention 请务必要阅读： 张俊林 深度学习中的注意力模型（2017版） Attention 本质思想 把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易懂: Attention 的三阶段 第一个阶段根据Query和Key计算两者的相似性或者相关性； 第二个阶段对第一阶段的原始分值进行归一化处理； 根据权重系数对Value进行加权求和。 8. GPT &amp; ELMO ELMO: Embedding from Language Models ELMO的论文题目：“Deep contextualized word representation” NAACL 2018 最佳论文 - ELMO： Deep contextualized word representation ELMO 本身是个根据当前上下文对Word Embedding动态调整的思路。 ELMO 有什么缺点？ LSTM 抽取特征能力远弱于 Transformer 拼接方式双向融合特征能力偏弱 **GPT (Generative Pre-Training) ** 第一个阶段是利用 language 进行 Pre-Training. 第二阶段通过 Fine-tuning 的模式解决下游任务。 GPT: 有什么缺点？ 要是把 language model 改造成双向就好了 不太会炒作，GPT 也是非常重要的工作. Bert 亮点 : 效果好 和 普适性强 Transformer 特征抽取器 Language Model 作为训练任务 (双向) Bert 采用和 GPT 完全相同的 两阶段 模型： Pre-Train Language Model； Fine-&gt; Tuning模式解决下游任务。 9. Transformer Transformer 改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。 Transformer 可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。 务必阅读： The Illustrated Transformer 中文版 Q、K、V 它们都是有助于计算和理解注意力机制的抽象概念 所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。 解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。 一个公式来计算自注意力层的输出 解码组件 Transformer作为新模型，并不是完美无缺的。它也有明显的缺点：首先，对于长输入的任务，典型的比如篇章级别的任务（例如文本摘要），因为任务的输入太长，Transformer会有巨大的计算复杂度，导致速度会急剧变慢。 10. Task tricks 在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的tricks？ 数据预处理时vocab的选取（前N个高频词或者过滤掉出现次数小于3的词等等） 词向量的选择，可以使用预训练好的词向量如谷歌、facebook开源出来的，当训练集比较大的时候也可以进行微调或者随机初始化与训练同时进行。训练集较小时就别微调了 结合要使用的模型，这里可以把数据处理成char、word或者都用等 有时将词性标注信息也加入训练数据会收到比较好的效果 至于PAD的话，取均值或者一个稍微比较大的数，但是别取最大值那种应该都还好 神经网络结构的话到没有什么要说的，可以多试几种比如fastText、TextCNN、RCNN、char-CNN/RNN、HAN等等。加上dropout和BN可能会有意外收获。反正模型这块还是要具体问题具体分析吧，（比如之前参加知乎竞赛的时候，最终的分类标签也有文本描述，所以就可以把这部分信息也加到模型之中等等） Overfiting 8 条 1). get more data 2). Data augmentation 3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验) 4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；) 5). Choosing Right Network Structure 6). Early stopping 7). Model Ensumble 8). Batch Normalization 张俊林 - Batch Normalization导读 、 张俊林 - 深度学习中的Normalization模型 Internal Covariate Shift &amp; Independent and identically distributed，缩写为 IID Batch Normalization 可以有效避免复杂参数对网络训练产生的影响，也可提高泛化能力. 神经网路的训练过程的本质是学习数据分布，如果训练数据与测试数据分布不同，将大大降低网络泛化能力， BN 是针对每一批数据，在网络的每一层输入之前增加 BN，(均值0，标准差1)。 Dropout 可以抑制过拟合，作用于每份小批量的训练数据，随机丢弃部分神经元机制. bagging 原理. ML算法： 关于防止过拟合，整理了 8 条迭代方向 Reference Language Model and Perplexity sklearn: TfidfVectorizer 中文处理及一些使用参数 sklearn.feature_extraction.text.TfidfVectorizer函数说明"},{"title":"Categories","date":"2021-06-19T16:13:17.979Z","updated":"2021-06-19T16:13:17.979Z","comments":false,"path":"categories/index.html","permalink":"http://www.iequa.com/categories/index.html","excerpt":"","text":""},{"title":"Categories","date":"2021-06-16T06:39:36.977Z","updated":"2021-06-16T06:39:36.977Z","comments":false,"path":"categoriesc/index.html","permalink":"http://www.iequa.com/categoriesc/index.html","excerpt":"","text":""},{"title":"ChengeLog","date":"2021-06-22T07:01:22.217Z","updated":"2021-06-22T07:01:22.217Z","comments":true,"path":"changelog/index.html","permalink":"http://www.iequa.com/changelog/index.html","excerpt":"","text":"本站更新摘要 Volantis 2020-06-22 2.6.6 -&gt; 4.0 更新 icon, 修改在 blog/_config.yml rel=“icon” and rel=“shortcut icon”转移 trip 类 articles 到该 blog 下.将所有 articles 中的 logo image 大于 500px 的都 改为 500px. 2020-06-21 2.6.6 -&gt; 4.0 注册 leancloud国际版 记录 webinfo .blog theme/_config.yml 改为 blog/_config.my_volantis.yml, 这种方式不会每次修改自动加载, 需要重启server 2020-06-20 2.6.6 -&gt; 4.0 将 articles 中所有的 img 标签 改为 Image 标签 的形式将 articles 中所有的 头部的 toc: true 和 mathjax: true 去掉. 2021-06-18 2.6.2 -&gt; 2.6.3 更新 highlightjs and clipboard and comment typing mode修改 light 模式的 p 文字样式, p: #444 to #000增加 friends 页面, 并调试其样式更新 pay-blog fa-alipay 图标 2021-06-17 2.6.2 -&gt; 2.6.3 update theme/_config.yml webinfoupdate webinfo.ejs 具体修改位置: hexo-theme-volantis/layout/_widget/webinfo.ejs 2021-06-16 2.6.2 -&gt; 2.6.3 创建 home page (为了博客的另一种形式展示) 并修改样式, 具体修改位置: layout/_partial/scripts/coverCtrl.ejs创建 categoryc url add cover (为了博客的另一种形式展示)更新 blogger social youtube and twitter update in theme/_configsidebar 的 blogger title BlairChen 居中样式修改, 具体修改位置: theme css/_layout/sidebar.styl text-alignupdate 底部分享 share 选项, 修改位置： update layout.ejs sharetheme/_config comments, qrcode. 其他修改 copyright. 2021-06-15 2.6.2 -&gt; 2.6.3 调大 整个版面 max_width. 修改位置： theme/_config.yml更新 body 内的 fontfamily. 修改位置: theme/_config.yml custom_css.fontfamily.bodyfont使用 search bar 并 update my avatar logo 2021-06-14 2.6.2 -&gt; 2.6.3 使用 theme hexo-theme-volantis 并更新配置: author, public views, language更新 theme: parallax, aplayer, darkmodejs, comments 2021-06-13 2.6.2 -&gt; 2.6.3 theme hexo-theme-volantis research… 2017-10-08 2.6.2 -&gt; 2.6.3 blairos theme improve. 2016-03-22 2.6.2 -&gt; 2.6.3 build this blog website."},{"title":"鸣谢项目和社区贡献者","date":"2021-06-16T06:31:21.527Z","updated":"2021-06-16T06:31:21.527Z","comments":true,"path":"contributors/index.html","permalink":"http://www.iequa.com/contributors/index.html","excerpt":"Volantis 社区的发展离不开团队大佬们的无私奉献和社区小伙伴们的热情互助。每一个心怀梦想、有着独特见解的朋友都可以成为团队的一员。目前 Volantis 社区正处于建设初期，我们缺少各方面的人才，如果您使用主题至少一个月且长期活跃于社区，例如： 解答 issues / discussions 提交有效的建议 官网文档补全/纠错 发现并收录有意思的文章（搭建博客方面）到官网 我们非常欢迎您的加入，请在论坛发帖告诉我们。","text":"Volantis 社区的发展离不开团队大佬们的无私奉献和社区小伙伴们的热情互助。每一个心怀梦想、有着独特见解的朋友都可以成为团队的一员。目前 Volantis 社区正处于建设初期，我们缺少各方面的人才，如果您使用主题至少一个月且长期活跃于社区，例如： 解答 issues / discussions 提交有效的建议 官网文档补全/纠错 发现并收录有意思的文章（搭建博客方面）到官网 我们非常欢迎您的加入，请在论坛发帖告诉我们。 感谢开发者 感谢社区建设者 如何参与社区建设 社区建设主要包括 Issues Discussions(论坛) 官网博客收录 官网文档维护 几个方面。 如何维护文档 目前 Volantis 4.x 已是已知的 Hexo 主题中文档最全面的，但仍有部分功能缺少明确的文档、部分文档已经过时，如果您发现了遗漏或者错误之处，我们非常希望您能够帮忙完善一下。 Volantis 官网支持 CI ，您可以直接在线编辑源码： 在线编辑文档https://github.com/volantis-x/community/tree/master/source 如何收录博客 每位用户在使用或更新主题的时候都需要阅读官网的文档，收录的相关内容能帮助用户更高效地上手，同时也能够提高被收录的文章的曝光率。如果在官网能看到更多的有价值的文章，就会有更多 Volantis 用户或者非 Volantis 用户来访问。 如果您有或者发现了与 Hexo 博客搭建相关的文章，可以转载外链到 Volantis 官网，示例如下： _posts/blogs/2020-05-17-pjax.md1234567891011---title: Volantis 主题部署 Pjaxdate: 2020-05-17updated: 2020-08-07categories: [开发心得]author: inksslink: https://inkss.cn/blog/76993423/description: 本篇文章记录了我对 Volantis 主题做 Pjax 兼容的种种，大抵算是种记录吧~headimg: # 可以设置文章头图backup: https://archive.vn/U36NG # 将页面存档到 archive.tody 网页快照档案馆的存档链接 https://archive.tody--- 如果这篇文章的作者是第一次出现在官网，还需要在 _data/author.yml 文件中添加作者信息，例如： _data/author.yml12345...inkss: name: 枋柚梓 avatar: https://cdn.jsdelivr.net/gh/inkss/common@1.4.2/hexo/img/static/avatar.jpg url: https://inkss.cn 注意事项文章存放在 _posts/blogs/ 目录中，且文件名格式为「年-月-日-文章话题」。鼓励原创文章，摘要部分300字符以内。如果想不出摘要可以不写，不要在摘要里重复一遍文章标题。最好选择一个文章分类，如果现有的分类中没有合适的，可以自己新增。如果有文章头图，请确保图片内最多只有一个单词或短语，图片不清晰或者与文章无关的话不建议使用。不仅限于自己的文章，可以在征得文章作者同意的情况下将其链接收录到官网。 在线编辑文档https://github.com/volantis-x/community/tree/master/source/_posts/blogs"},{"title":"Cheer UP！","date":"2018-01-29T08:20:48.000Z","updated":"2020-10-23T02:09:56.687Z","comments":true,"path":"english/index.html","permalink":"http://www.iequa.com/english/index.html","excerpt":"","text":"Open Language learn what you love. love what you learn. 你不得不学的减肥英语！ Go on a diet 自驾游可不是 self-driving！ 巴黎圣母院叫 Notre-Dame How 用看電影、聽音樂、聊天、等生活化的方式邊玩邊學. How I Met Your Mother S1 ep1 How I Met Your Mother S1 ep2 How I Met Your Mother S1 ep3 How I Met Your Mother S1 ep4 How I Met Your Mother S1 ep5 Reference How scripts Engoo 线上英文家教 Engvid: Taking care of your pet DOG!"},{"title":"","date":"2021-06-13T16:29:17.973Z","updated":"2021-06-13T16:29:17.973Z","comments":true,"path":"examples/index.html","permalink":"http://www.iequa.com/examples/index.html","excerpt":"示 例 博 客 社区维护团队的博客 使用 Volantis 的博客示例","text":"示 例 博 客 社区维护团队的博客 使用 Volantis 的博客示例 如何添加自己的博客链接 第一步：新建 Issue 按照格式填写并提交 12345678&#123; &quot;title&quot;: &quot;&quot;, &quot;description&quot;: &quot;&quot;, &quot;screenshot&quot;: &quot;&quot;, &quot;url&quot;: &quot;&quot;, &quot;avatar&quot;: &quot;&quot;, &quot;version&quot;: &quot;版本：^4.0&quot;&#125;为了提高图片加载速度，建议优化图片尺寸：打开 压缩图 上传自己的截图，将图片的高度调整到 360px 后下载。将压缩后的图片上传到 去不图床 并使用此图片链接作为截图链接。 第二步：刷新 回来刷新即可生效。 如何更新自己的博客链接 如果是自己创建的 issue ，可以自己修改。 如果是管理员创建的，请自己重新创建一份，然后让管理员删掉旧的。"},{"title":"常见问题与反馈渠道","date":"2021-06-13T16:29:24.950Z","updated":"2021-06-13T16:29:24.949Z","comments":true,"path":"faqs/index.html","permalink":"http://www.iequa.com/faqs/index.html","excerpt":"通常来说，一个全新的工程全部使用默认配置是正常没有故障的。如果无法使用或者效果与示例有较大区别，可以使用 Hexo 官方提供的用于单元测试的博客应用本主题查看样式是否正常，对比 _config.yml 文件排查问题。 Hexo 官方的单元测试项目： https://github.com/hexojs/hexo-theme-unit-test 向开发者反馈问题","text":"通常来说，一个全新的工程全部使用默认配置是正常没有故障的。如果无法使用或者效果与示例有较大区别，可以使用 Hexo 官方提供的用于单元测试的博客应用本主题查看样式是否正常，对比 _config.yml 文件排查问题。 Hexo 官方的单元测试项目： https://github.com/hexojs/hexo-theme-unit-test 向开发者反馈问题 如何更新主题 使用主题的时候，尽量 fork 主题到自己的 GitHub，然后进行修改并使用。这样做的好处是：当主题进行重要更新的时候可以根据需要拉取合并代码，使自己 DIY 的主题能够通过更新获取 BUG 修复或者新特性。 如果不懂请自行搜索关键词：fork 更新 实用小技巧所有需要写在主题配置文件中的配置都可以写在站点配置文件的 theme_config: 中，在 Hexo 5.0 后，还可以写在 _config.volantis.yml 文件中，详见 Hexo 官方文档：覆盖主题配置https://hexo.io/zh-cn/docs/configuration#%E8%A6%86%E7%9B%96%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE也可以直接查看本站源码中站点配置文件的写法：_config.volantis.yml 无法成功运行本地预览 可能是没有安装依赖，请按照「开始」页面中的步骤进行安装，并安装所需依赖。 如果开启了某些第三方服务，请查看文档中是否要求安装插件。 如果报错信息有 lastIndex，你可以尝试把博客根目录配置文件中找到 highlight，并将 auto_detect 设置为 false。 主题配置修改没有生效 请确认文档中要求修改的是博客主目录的配置文件 blog/_config.yml 还是主题的配置文件 blog/_config.volantis.yml。 主题样式修改没有生效 如果主题配置文件中开启了 cdn 服务，那么修改本地的样式是不会生效的，需要关闭 cdn 服务。 修改什么都没有生效 需要 hexo clean 然后重新 hexo s 如果安装了「相关文章推荐」插件后，每次修改 md 文件后都需要重新 hexo s 关掉 CDN 之后样式错乱 请前往文档「开始」页面，检查是否安装了必要的依赖包。 搜索无法使用 请前往文档「开始」页面，检查是否安装了必要的依赖包。 检查根目录配置文件是否有 search 字段冲突。 如果以上两步都无法找到问题，请下载示例源码进行对比。 搜索结果链接不正确 请检查根目录配置文件中的链接是否正确，如： blog/_config.yml12url: https://xaoxuu.comroot: / 教程与指南 Hexo 官方文档 | Valine 官方文档请一定要阅读官方文档！ 可供交流的渠道 解决问题 渠道 方式 用途 Issues @volantis-x/hexo-theme-volantis 和开发者沟通的唯一渠道，用于跟进和解决问题 请不要发送邮件开源项目的开发者很反感别人不通过正确的途径如 Issues 而是通过私人邮件询问开源项目问题，所以一般不会回复此类邮件。 交流 渠道 方式 用途 论坛 @volantis-x/discussions 慢，相对正式，方便检索，可以给其他用户参考 QQ群 1146399464 (验证码: vlts-2021) 非正式，即时通讯，易于斗图；不利于给其他用户参考 佛系互动 渠道 方式 用途 评论区 留言 可以测试、灌水、推广自己的博客。"},{"title":"Friends","date":"2021-06-20T07:21:24.556Z","updated":"2021-06-20T07:21:24.556Z","comments":false,"path":"friends/index.html","permalink":"http://www.iequa.com/friends/index.html","excerpt":"","text":"使用hexo过程中认识的大佬们，排名分组不分先后 Volantis Volantis主题 W4J1e`s Blog xaoxuu 枋柚梓 Font-Awesome Exploring 开拓职场 Offer帮 小Lin@知乎 Data WareHouse &amp; Analyst 小萝卜算子 花木兰 Friends 博客爱好者 PPJ后端 Blair`s Blog 七海の参考書 qinxs E.w IndustryVeteran 业界大佬 廖雪峰 阮一峰 陈皓 王垠 Runoob Codeblock hello.py12345678n=eval(input())if n==0: print(&quot;Hello World&quot;)elif n&gt;0: print(&quot;He\\nll\\no \\nWo\\nrl\\nd&quot;)else: for c in &quot;Hello World&quot;: print(c) code snippet 11234var allp=$(&quot;div p&quot;);allp.attr(&quot;class&quot;,function(i,n)&#123; return Number(n)+1; &#125;);"},{"title":"Home","date":"2021-06-16T15:50:50.059Z","updated":"2021-06-16T15:50:50.059Z","comments":true,"path":"home/index.html","permalink":"http://www.iequa.com/home/index.html","excerpt":"","text":""},{"title":"","date":"2021-06-13T15:28:54.374Z","updated":"2021-06-13T15:28:54.374Z","comments":true,"path":"hzbank/index.html","permalink":"http://www.iequa.com/hzbank/index.html","excerpt":"","text":"1. Data Warehouse OLTP (on-line transaction processing) OLAP（On-Line Analytical Processing） 数据在系统中产生 本身不产生数据，基础数据来源于产生系统 基于交易的处理系统 基于查询的分析系统 牵扯的数据量很小 牵扯的数据量庞大 (复杂查询经常使用全表扫描等) 对响应时间要求非常高 响应时间与具体查询有很大关系 用户数量大，为操作用户 用户数量少，主要有技术人员与业务人员 各种操作主要基于索引进行 业务问题不固定，数据库的各种操作不能完全基于索引进行 Data Warehouse 面向主题 数据仓库的由来 数仓特点: 主题性, 集成性, 时变性, 历史性 心理姿势: 放空, 对自己负责 2. DW 技术选型 No. Title Tech 1. 数据采集 flume, kafka, sqoop, logstach, datax 2. 数据存储 mysql, hdfs, hbase, redis, elastic, kudu, mongodb 3. 数据计算 hive, tez, spark, flink, storm 4. 数据查询 presto, kylin, impala, druid, clickhouse 5. 数据可视化 echarts, superset, quickbl, dataV 6. 任务调度 azkaban, airflow, Oozie 7. 集群监控 Zabbix 8. 元数据管理 Apache Atlas 9. 权限管理 Aapche Ranger 3. 项目背景 互联网金融行业, 信贷, 理财 关键性数据需求： 采集客户系统, 风控系统, 核心放款系统, 产品, 组织管理等系统数据, 进行整合 客户主题进行分析: 客户结构, 客户质量, 客户转化率 经营效率的分析 风险主题分析: 风险, 逾期率, 产品的违约率 财务主题分析: 贡献率 数据可视化: 通过报表或大屏的形式展示给管理者 4. 数据调研 No. Table Name Desc 1. channel_info 2. com_manager_info 3. dict_citys 4. dict_product 5. dict_provinces 6. drawal_address 7. drawal_apply 借款申请ID, 信用审核ID, 金额, 期限, 待还金额, 放款时间, 协议ID, 下一个还款时间, 放款资金源ID, 协议核对标识, 信用审核类型, 用户类型, 放款类型 8. drawal_companys 9. loan_apply 10. loan_apply_credit_report 11. loan_apply_salary 12. loan_credit 审核状态, 时间, 结论, 产品, 批准金额, 期限, 分数 13. repay_plan user_id, apply_id, contract_amount, loan_term期限, paid_amount 已还金额, 预存金额, 尚欠金额, 减免金额, 提前结清违约金, 与核心同步时间 14. repay_plan_item drawal_apply_id提款申请ID, repay_plan_id还款计划ID, repay_item还款期数编号, due_data逾期时间, dest_principal, dest_interest, dest_service, dest_pty_fee 本息滞纳金, … 15. repay_plan_item_his 16. user_det 17. user_ocr_log 18. user_md5 19. user_quota 信用额度, 已使用额, 未使用额, 失效日期, 额度失效日期… 20. users 123456SELECT user_id, count(id) as xFROM loan_applygroup by user_idhaving count(id) &gt;= 2 5. DW分层 清晰的数据结构: 每一层的作用不一样, 目的是我更好的定位和理解数据 方便数据血缘追踪: 减少重复性的开发: 三个不同的需求, 都需要从5张表获取数据, 都需求进行清洗和转换. 把复杂问题简单化: 客户价值： 购买额度， 次数 (产品) 产品1: 次数, 额度 --table1 产品2: 次数, 额度 --table1 DW 4 大特征: Subject Oriented、Integrate、Non-Volatil、Time Variant . 数仓分层 STG Stage （不做任何加工, 禁止重复进入） ODS（Operational Data Store）不做处理，存放原始数据 (该层在stage上仅数据格式到标准格式转换) DWD（Data Warehouse Summary 明细数据层）进行简单数据清洗，降维 DWS（Data Warehouse Summary 服务数据层）进行轻度汇总（做宽表） ADS（Application Data Summary 数据应用层）为报表提供数据 5.1 DWH basic data warehouse 逻辑分层架构： ODS层作用： 为DW做数据准备, 减少DW减少对业务库的影响 ODS层数据来源： 业务库, 埋点数据, 消息队列 ODS层建设原则： 数据保留时间根据业务具体确定 DW层: Data Warehouse 数据来源: 只能是 ODS 层 建设方式: 根据ODS层数据按照主题性归类建模 4个基本概念： 维度, 事实, 指标(度量), 粒度 逾期一期的客户表现: 逾期金额 (客户, 产品, 组织, 时间) 客户注册事实表: 一条客户注册记录就是一个事实. 指标: 客户注册量 DW层内部是一个细分子层: DWD, DWB DWD: 明细粒度的数据, 清洗, 解决数据的质量问题. NULL, 编码转换问题. 监管报送或合规核查提供最基本的数据 留快照: DWB层: 基础数据层, 客户统计数据, 中间层使用 (粒度还是比较细) DWS层: 在DWB层的基础上进行汇总 DM层： 对明细数据进行预加工, 汇总 与关联, 建立多维立方体的数据量, 提高查询效率 APP层: 服务于终端用户, 高度汇总 DW: 一张事实表有20个维度, 5个指标, DM层就可能有：10个维度，5个指标; APP层3个,5个指标 3.0数据仓库建表规范 DW层： dw_fact_主题_table_name (dw_fact_cus_regeste_detail) Dim层：dim_table_name, 例如： dim_product Dm层： dm_fact_集市名_table_name, 例如： dm_fact_risk_first_overdue 客户首期逾期 图 数据仓库建模 user_md5 你持有的信息和公安部的信息是否一致 客户经理统计表 DM 层的 在数据仓库中，有些表既可以作为 fact 也可以作为 dim MD5认证 多数人 1.1 注册 立马 1.2 OCR认证 1.3 MD5认证 99%同一天完成 power-design 协议 - 支用申请，签署了支用协议，才产生借据， 借据号 的信息在这里. 事件 - 还款，借款 产品 - 业务流程 ， 只有维度表 客户 - 用户信息, 非常多, 人行征信信息， 个人资产信息 机构 - 线下有哪些团队, 浙江区，团队长，客户经理， 有 600 个. 只有维度表 (1). 同业借款, 100多亿 (2). 放贷款 - 京东，百度 (3). 签协议 产生 产品 营销之后的，商务经理和渠道，谈下来之后， 后端 渠道， 资产， 账务 产品维度表： 产品编号(分好几级), 产品名称, dim_code, dim_name， 上架， 下架 京东金条， code， 展示给财务 事件主题 - 还款流水, 授信流水, 支用流水, 放款流水, 还款计划. 在后端资产的模块. DWD - 这些流水表都在这层. (1). 授信支用后，就会产生借据号 还款流水 + 支用流水 -&gt; 还款事实表 ？ 个贷业务数据，包括申请ID，机构代号，贷款合同编号，担保类型，贷款期限，贷款起期，贷款止期，诉讼标志，逾期利息利率，还款帐号，终审金额，贷款金额，贷款余额，经办人编号，结清日期，还款方式，客户姓名，客户种类，客户性质，客户分类，证件类型，证件号码，流水号，借据号，借据金额，借据余额，借据利息余额，借据利率，借据起期，借据止期，借款状态，逾期天数，结息方式，放款账号，放款账户户名用途详情，还款账户户名，还款账户账号等。 还款事实表 - 借据号(可以关联到用户和产品), 还款流水号, 还款金额(本金，利息，手续费), user_id, product_id, custom_id 是在 DWD， 是根据流水表关联出来的 下游可能看，借据粒度，还了多少钱， 聚合 每天拉昨天新增的流水 很多问题是上游数据的问题，还有需求的问题 Reference 基于笔记, 刻意练习"},{"title":"","date":"2021-03-07T11:02:34.512Z","updated":"2021-01-17T11:49:00.681Z","comments":true,"path":"hzbank/pre_index.html","permalink":"http://www.iequa.com/hzbank/pre_index.html","excerpt":"","text":"No. desc Flag 0. 客户信息表、合同信息表和还款计划表分别是什么？玩不透老板会怀疑我的能力？ 0. 字节跳动-数据仓库高级工程师面试 detail 1. 星型模型和雪花型模型比较 星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花型模型要高 星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化 No. desc Flag 2. 事实表，维度，度量，指标之间的关系 3. 面试必知的Spark SQL几种Join实现 1. 客户信息表 客户信息表中，客户编号是表的主键（唯一值，用于关联其他表），其他数据大部分是标准化的连续变量和分类变量，比如“婚姻状态”、“户籍省”、“家庭子女数”。当然，也会存在类似“id_no&quot;、”mobile“这种非标准化变量，这些变量主要用于欺诈关联分析，贷后催收结果分析。 2. 合同信息表 当借款客户完成申请全流程，确认借款且审批通过后，金融机构会针对每个借款人生成与之对应的贷款合同。合同信息表里详细记载了客户的贷款信息、贷款用途、放款信息、逾期状态、合同状态、利率。 3. 还款计划表 还款计划表严格意义上应该分为 约定还款 和 实际还款 两部分. 一部分是合同生成时就产生了，而另一部分是记录合同结束前客户的实际还款情况。 还款计划表主要包括三大信息： 约定还款、实际还款及其他信息 其中约定还款核心字段包括“当前期数”、“计划还款日”、“应还本金”、”应还利息“； 实际还款核心字段包括”实际还款日期“、”已还本/利/罚“、“逾期本/利/罚”、“减免本/利/罚”； 其他信息则包括“交易状态”、“逾期天数“、”剩余期数“等。 还款计划表记录了客户的还款轨迹，能够直接定性一个客户的好坏，计算逾期率、账龄以及定义建模时的目标变量，这些都需要通过这个表的数据完成。因此，还款计划表是策略迭代、建模、数据分析中最为重要的表。 power-design 协议 - 支用申请，签署了支用协议，才产生借据， 借据号 的信息在这里. 事件 - 还款，借款 产品 - 业务流程 ， 只有维度表 客户 - 用户信息, 非常多, 人行征信信息， 个人资产信息 机构 - 线下有哪些团队, 浙江区，团队长，客户经理， 有 600 个. 只有维度表 (1). 同业借款, 100多亿 (2). 放贷款 - 京东，百度 (3). 签协议 产生 产品 营销之后的，商务经理和渠道，谈下来之后， 后端 渠道， 资产， 账务 产品维度表： 产品编号(分好几级), 产品名称, dim_code, dim_name， 上架， 下架 京东金条， code， 展示给财务 事件主题 - 还款流水, 授信流水, 支用流水, 放款流水, 还款计划. 在后端资产的模块. DWD - 这些流水表都在这层. (1). 授信支用后，就会产生借据号 还款流水 + 支用流水 -&gt; 还款事实表 ？ 个贷业务数据，包括申请ID，机构代号，贷款合同编号，担保类型，贷款期限，贷款起期，贷款止期，诉讼标志，逾期利息利率，还款帐号，终审金额，贷款金额，贷款余额，经办人编号，结清日期，还款方式，客户姓名，客户种类，客户性质，客户分类，证件类型，证件号码，流水号，借据号，借据金额，借据余额，借据利息余额，借据利率，借据起期，借据止期，借款状态，逾期天数，结息方式，放款账号，放款账户户名用途详情，还款账户户名，还款账户账号等。 还款事实表 - 借据号(可以关联到用户和产品), 还款流水号, 还款金额(本金，利息，手续费), user_id, product_id, custom_id 是在 DWD， 是根据流水表关联出来的 下游可能看，借据粒度，还了多少钱， 聚合 每天拉昨天新增的流水 很多问题是上游数据的问题，还有需求的问题"},{"title":"","date":"2021-06-17T09:57:34.804Z","updated":"2021-06-17T09:57:34.804Z","comments":true,"path":"mylist/index.html","permalink":"http://www.iequa.com/mylist/index.html","excerpt":"","text":""},{"title":"Matplotlib","date":"2018-01-06T08:46:48.000Z","updated":"2019-10-20T04:30:35.085Z","comments":true,"path":"python_matplotlib/index.html","permalink":"http://www.iequa.com/python_matplotlib/index.html","excerpt":"","text":"Matplotlib 是一个非常强大的 Python 画图工具 基本使用 4.1 : Matplotlib Why ? 4.2 : Matplotlib Basic Use 4.3 : Matplotlib Figure 4.4 : Matplotlib Coordinate axis 4.5 : Matplotlib Legend 4.6 : Matplotlib Annotation 4.7 : Matplotlib Tick bbox 画图种类 4.08 : Matplotlib Scatter 4.09 : Matplotlib Bar 4.10 : Contours * 4.11 : Image 图片 * 4.12 : 3D 数据 * 多图合并显示 4.13 : Subplot 多合一显示 * 4.14 : Subplot 分格显示 * 4.15 : 图中图 * 4.16 : 次坐标轴 * next ⋯⋯ notes：next …"},{"title":"Numpy & Pandas","date":"2018-01-06T08:46:48.000Z","updated":"2019-10-20T04:30:37.078Z","comments":true,"path":"python_numpy_pandas/index.html","permalink":"http://www.iequa.com/python_numpy_pandas/index.html","excerpt":"","text":"任何关于数据分析的模块都少不了它们两个 Numpy 2.1 : Numpy Attribute 2.2 : Numpy Array 2.3 : Numpy Basic Operation 1 2.4 : Numpy Basic Operation 2 2.5 : Numpy Index 2.6 : Numpy Array Merge 2.7 : Numpy Array Split 2.8 : Numpy Copy &amp; Deep Copy Pandas 3.1 : Pandas Basic Intro 3.2 : Pandas Select Data 3.3 : Pandas Set Value – loc/iloc 3.4 : Pandas Deal NaN Data 3.5 : Pandas Import &amp; Output 3.6 : Pandas Concat、Join 3.7 : Pandas Merge 3.8 : Pandas Matplotlib Intro notes：next …"},{"title":"","date":"2021-06-13T15:25:37.159Z","updated":"2021-06-13T15:25:37.158Z","comments":true,"path":"sina_project/index.html","permalink":"http://www.iequa.com/sina_project/index.html","excerpt":"","text":"1. user profile 内容兴趣挖掘 新浪用户兴趣挖掘系统，据用户在互联网上的访问行为，利用 LR、Decision tree、Random Forest 等模型预测用户的兴趣。 挖掘用户标签信息有助于广告主更加定向，准确，个性化的投放广告，使广告被转化的价值尽可能最大化。 主要流程分为4大模块：数据源获取、网页规范化、特征计算，兴趣策略融合 . 2. dmp 项目 新浪dmp基于广告程序化购买场景，将广告主数据整合接入，并结合新浪自有数据，标准化统一管理数据。同时，通过对数据进行细化数据标签，完善分类体系，向用户提供多样化的数据应用服务。 12345678本人职责 ：(1). 用户标签规范化(2). 提供 redis 存储服务, 相关接口的封装 1) 支持接收不同来源的数据。 2) 支持访问 redis 集群策略可配，读数据负载均衡 3) 某机器或实例出故障，易于维护(3). 提供第三方数据用户推荐商品, 10种用户行为数据 的接入接口 12345678910111213141516171819202122host.txt 集群节点实例配置## REDIS1 DMP_INFO ## (128个节点，每个节点配置 60% 机器内存给 redis 实例)REDIS1#pool1:10.**.*.**:6382$10.**.*.**:6382|pool2:10.**.***.**:6382$10.**.*.**:6382|pool3:10.**.**.**:6382$10.**.*.**:6382......## REDIS3 ##REDIS3#pool1:172.**.***.**:6571$1|pool2:172.**.***.**:6571$1REDIS3#pool1:172.**.***.**:6572$2|pool2:172.**.***.**:6572$2...regionToRedis region选择集群DMP_INFO=REDIS1CROSS_INFO=REDIS33RD_INFO=REDIS3strategy.properties 策略配置REDIS1=read:random|pool2,pool3#write:pool1REDIS3=read:order|pool2,pool1#write:pool1 Coder 更多项目详情请点击… (为了更好的互相了解，脱敏后暂时放上git,后会迅速移除) 2.1 用户行为数据接收与挖掘 本模块是扶翼效果平台动态创意项目的一部分，主要完成用户在客户网站上的行为数据接收与分发。本期动态创意专指个性化重定向，即主要针对电商客户利用其站内用户行为数据和商品数据为广告用户展示最合适的一组商品信息（图片、价钱、折扣等）组成的创意 用户访问广告客户网站时，触发部署的监测代码，向新浪发送各种用户行为数据。 数据接收服务收到请求，进行数据解析与验证 根据接收的数据类型，更新数据对接状态 将接收数据处理为下游需要的格式，抓发到消息队列 为支持离线的数据挖掘，将数据写入日志，并实时发送至HDFS. 2.2 相关配置文件 config a) 修改配置bin/catalina.sh，添加java配置 12345678i. JAVA_OPTS=&#x27;-Xms40000m -Xmx40000m -Xmn10000m -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+DisableExplicitGC -XX:+CMSParallelRemarkEnabled -Dsun.rmi.dgc.server.gcInterval=86400000 -Dsun.rmi.dgc.client.gcInterval=86400000 -XX:+ExplicitGCInvokesConcurrent -XX:+CMSScavengeBeforeRemark -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=60 -XX:+UseCMSInitiatingOccupancyOnly&#x27; b) 修改配置conf/server.xml，connectors配置，添加 1234i. &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;ii. connectionTimeout=&quot;20000&quot;iii. redirectPort=&quot;8443&quot; iv. acceptCount=&quot;5000&quot; maxThreads=&quot;4000&quot;/&gt; 2.3 推荐商品接收接口 pc uv 3500W+ , wap uv 4500W+, weibo uv 1.6~1.8亿 200个并发做接口压测，qps是3000+，均延时65ms。 延时主要耗在域名解析上，内网压测qps是3万+，均延时10ms 3. 离线分析调度框架 这是一个 shell 等其他语言配合写成的灵活调度框架. 该示例模块架 适用于离线分析调度，特别是每天跑的crontab任务，或者是每周、每月跑的任务. 多用于 hive 语句 或 其他脚本离线运行. 提供一些 best practice 提高各模块结构及代码的一致性 降低开发新模块的成本 便于离线大数据分析流程控制 具备报警，日志定位，常用检测依赖库函数 等功能 当然它也适用于对任何离线Job进行调度 不同人负责的模块不同，模块目录结构是一样的，如下所示 开发模块的时候，每个人只需要编写自己模块 script 下脚本 和 少量 crontab_job 脚本的改动即可. 目录结构 | 功能 | 详细说明 :— | :-- | -:- ─ crontab_job | crontab 脚本 | 由 linux crontab 触发, 检测整条流水线任务依赖关系, 调用 script 目录脚本等 ─ script | 主脚本 | shell、python脚本. shell 里主为 hql 或 streaming MR、spark-submit 任务等 ─ conf | 配置文件 | default.conf、vars.conf、alert.conf ─ log | 日志文件 | 主脚本log (多次运行多个log file)、crontab 脚本log (一个log file) ─ flag | 标记文件 | (标志该模块已运行，或运行完毕， crontab 会轮询检测) crontab_label.2017-12-13、ods_e_coupon/2017-12-13.all.done ─ util | 工具脚本 | logging、static_functions、static_hive_lib、crontab_job_env、env ─ alert | 报警封装 | (发送报警邮件的封装 alert、send_mail.py、constant_mail.py) | | ─ create_table | 建表脚本 | 建里 hive table 的语句 ─ jar | jar包 | 如 无 则不需要建立 ─ java | udf、udaf | 如 无 则不需要建立 conf/default.conf 123456789101112131415161718192021222324#系统环境变量export HADOOP=&quot;$&#123;HADOOP_HOME&#125;/bin/hadoop&quot;export HIVE=&quot;$&#123;HIVE_HOME&#125;/bin/hive&quot;export JAVA=&quot;$&#123;JAVA_HOME&#125;/bin/java&quot;#hadoop jarhadoop_jar=&quot;$&#123;HADOOP_HOME&#125;/share/packages/hadoop2/hadoop-streaming-2.7.2.jar&quot;#log_utilscheduler_log_script=&quot;$&#123;util_dir&#125;/logging&quot;#控制日志运行方式open_log=true#运行hadoop任务的用户名user=&quot;data_mining&quot;#hive表路径ods_hive_dir=&quot;/data_mining/dm/ods/&quot;mds_hive_dir=&quot;/data_mining/dm/mds/&quot;tmp_hive_dir=&quot;/data_mining/dm/tmp/&quot;## OSSOSS_URL=&quot;oss://your-key:your-value@x-bigdata.oss-cn-hangzhou-internal.aliyuncs.com&quot; crontab_job 123456789101112131415161718192021222324#check crontab_label whether existif check_local_crontab_label $&#123;flag_dir&#125; $&#123;d1&#125; then echo &quot;[INFO] script already run!&quot;else echo &quot;[INFO] check dependention&quot; check_hive_partitions tablename 2018-04-19# check_local_file()# check_hdfs_file()# ... echo &quot;[INFO] script run!&quot;# generate crontab_label touch_local_crontab_label $&#123;flag_dir&#125; $&#123;d1&#125;# run main script echo &quot;[INFO] start run...&quot; sh ods_dm_e_coupon.sh $d1 check_crontab_success $&#123;flag_dir&#125; $&#123;d1&#125; fiecho_ex &quot;run $0 end!&quot; bigdata-offline-demo 更多项目详情请点击… 4. 爱客仕领券项目 这是一款基于地理位置，为城市生活人群提供优惠卡券的聚合平台APP. 实时位置基于6个距离段的券店实时距离计算展示. (离线地标计算) 一张券适用于多家店带来的数据膨胀. 地标关联店券等带来数据膨胀 (嵌套对象结构) 一张券适用于多家店，不同店分类不同，一张券多分类解决 (嵌套对象结构) 多店合一、同店比价.（两条线走…标记 &amp; 策略） … lingquan-offline-part 更多项目详情请点击… (为了更好的互相了解，脱敏后暂时放上git,会迅速移除) 多店合一、同店比价 (标记连锁品牌、策略非连锁品牌) shop 123456789101112131415161718192021222324252627282930313233343536373839404142 &quot;shop_id&quot;,... &quot;shop_name_show&quot;,...(用于展示) &quot;shop_name&quot;: &#123; (用于搜索) &quot;type&quot;: &quot;string&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125; &#125;, &quot;shop_position&quot;: &#123; &quot;geohash&quot;: True, &quot;geohash_precision&quot;: 7, &quot;type&quot;: &quot;geo_point&quot;, &quot;geohash_prefix&quot;: True &#125;, &quot;coupon_list&quot;: &#123; &quot;type&quot;: &quot;nested&quot;, &quot;properties&quot;: &#123; &quot;unique_coupon_id&quot;: &#123; &quot;index&quot;: &quot;not_analyzed&quot;, &quot;type&quot;: &quot;string&quot; &#125;, &quot;coupon_name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125; &#125;, &quot;coupon_source&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;status&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;,... shop_business_center , shop_address, shop_open_hours, shop_power_count, level1_code , level2_code , shop_review_count, shop_avg_price, coupon_count, shop_source, status coupon 1234567891011121314151617181920mapping = &#123; index_type_name: &#123; &quot;properties&quot;: &#123; ... &quot;shop_list&quot;: &#123; &quot;type&quot;: &quot;nested&quot;, &quot;properties&quot;: &#123; &quot;unique_shop_id&quot;: &#123;&#x27;index&#x27;: &#x27;not_analyzed&#x27;, &quot;type&quot;: &quot;string&quot;&#125;, &quot;shop_address&quot;: &#123;&quot;type&quot;: &quot;string&quot;&#125;, &quot;shop_position&quot;: &#123; &quot;geohash&quot;: True, &quot;geohash_precision&quot;: 7, &quot;type&quot;: &quot;geo_point&quot;, &quot;geohash_prefix&quot;: True &#125; &#125; &#125;...... coupon_id , coupon_name_show, coupon_name, coupon_store, coupon_condition , coupon_source, coupon_desc, coupon_type, coupon_sold, shop_count, level1_code_list, … business center business center amap name address geo 湖滨银泰 杭州湖滨银泰in77A区 杭州市上城区东坡路7号 杭州来福士 杭州来福士 杭州市江干区钱江新城富春路与新业路交汇处往北200米 嘉里中心 杭州嘉里中心 杭州市下城区延安路353号 … … … landmark、landmark_shop_coupon、shopping、… 5. 领券/推荐系统 基于用户在APP上产生的行为信息，基于过去6个月的行为记录(点击、下单、支付、分享)，并结合时间衰减构造用户的行为评分矩阵， 进而挖掘出用户的偏好，经过一些策略融合与候选集重排序，为用户提供其可能 感兴趣的商户卡券推荐列表。 采用基于隐因子模型 FunkSVD 的CF，中间权重值采用频次 限制处理后，非用户兴趣内商户权重调整，融合最近商圈或者明显地标的 热销券. 对于新用户采用商圈内热销券和城市内热销券进行补充. 后可再融合负反馈的数据与用户区域行为等追踪，搜索数据等,采用其他模型方法等尝试做更丰富的个性化推荐 … next… Machine Learning Notes Python &amp; Hive &amp; Spark… SpringMVC demo &amp; Csdn Java 分类，旧版学笔记"},{"title":"","date":"2021-06-13T15:38:14.046Z","updated":"2021-06-13T15:38:14.038Z","comments":true,"path":"sina_project/pro4_index.html","permalink":"http://www.iequa.com/sina_project/pro4_index.html","excerpt":"","text":"4. 领券项目 这是一款基于地理位置，为城市生活人群提供优惠卡券的聚合平台APP. 实时位置基于6个距离段的券店实时距离计算展示. (离线地标计算) 一张券适用于多家店带来的数据膨胀. 地标关联店券等带来数据膨胀 (嵌套对象结构) 一张券适用于多家店，不同店分类不同，一张券多分类解决 (嵌套对象结构) 多店合一、同店比价.（两条线走…标记 &amp; 策略） … lingquan-offline-part 更多项目详情请点击… (为了更好的互相了解，脱敏后暂时放上git,会迅速移除) 多店合一、同店比价 (标记连锁品牌、策略非连锁品牌) shop 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;shop_id&quot;, &quot;shop_name_show&quot;, &quot;shop_name&quot; &#123; &quot;type&quot; &quot;string&quot;, &quot;fields&quot; &#123; &quot;raw&quot; &#123; &quot;type&quot; &quot;string&quot;, &quot;index&quot; &quot;not_analyzed&quot; &#125; &#125; &#125;, &quot;shop_position&quot; &#123; &quot;geohash&quot; True, &quot;geohash_precision&quot; 7, &quot;type&quot; &quot;geo_point&quot;, &quot;geohash_prefix&quot; True &#125;, &quot;coupon_list&quot; &#123; &quot;type&quot; &quot;nested&quot;, &quot;properties&quot; &#123; &quot;unique_coupon_id&quot; &#123; &quot;index&quot; &quot;not_analyzed&quot;, &quot;type&quot; &quot;string&quot; &#125;, &quot;coupon_name&quot; &#123; &quot;type&quot; &quot;string&quot;, &quot;fields&quot; &#123; &quot;raw&quot; &#123; &quot;type&quot; &quot;string&quot;, &quot;index&quot; &quot;not_analyzed&quot; &#125; &#125; &#125;, &quot;coupon_source&quot; &#123; &quot;type&quot; &quot;integer&quot; &#125;, &quot;status&quot; &#123; &quot;type&quot; &quot;integer&quot; &#125; &#125; &#125;&#125; shop_business_center , shop_address, shop_open_hours, shop_power_count, level1_code , level2_code , shop_review_count, shop_avg_price, coupon_count, shop_source, status coupon 1234567891011121314151617181920mapping = &#123; index_type_name &#123; &quot;properties&quot; &#123; ... &quot;shop_list&quot; &#123; &quot;type&quot; &quot;nested&quot;, &quot;properties&quot; &#123; &quot;unique_shop_id&quot; &#123;&#x27;index&#x27; &#x27;not_analyzed&#x27;, &quot;type&quot; &quot;string&quot;&#125;, &quot;shop_address&quot; &#123;&quot;type&quot; &quot;string&quot;&#125;, &quot;shop_position&quot; &#123; &quot;geohash&quot; True, &quot;geohash_precision&quot; 7, &quot;type&quot; &quot;geo_point&quot;, &quot;geohash_prefix&quot; True &#125; &#125; &#125;...... coupon_id , coupon_name_show, coupon_name, coupon_store, coupon_condition , coupon_source, coupon_desc, coupon_type, coupon_sold, shop_count, level1_code_list, … business center business center amap name address geo 湖滨银泰 杭州湖滨银泰in77A区 杭州市上城区东坡路7号 杭州来福士 杭州来福士 杭州市江干区钱江新城富春路与新业路交汇处往北200米 嘉里中心 杭州嘉里中心 杭州市下城区延安路353号 … … … landmark、landmark_shop_coupon、shopping、… next… Machine Learning Notes Python &amp; Hive &amp; Spark… SpringMVC demo &amp; Csdn Java 分类，旧版学笔记"},{"title":"博文的赞助方式","date":"2017-11-05T12:05:21.000Z","updated":"2020-02-15T03:18:48.013Z","comments":true,"path":"support/index.html","permalink":"http://www.iequa.com/support/index.html","excerpt":"你的一点激励，可以鼓励作者写出更多的好文章，让更多的人喜欢，从中受益，构建一个良性的循环.","text":"你的一点激励，可以鼓励作者写出更多的好文章，让更多的人喜欢，从中受益，构建一个良性的循环. 你可以使用以下方式付款： 支付宝： 微信：由于微信付款经常出问题，暂时停止使用。 PayPal: 请点击付款链接"},{"title":"Tags☁️","date":"2021-06-15T23:31:43.554Z","updated":"2021-06-15T23:31:43.554Z","comments":false,"path":"tags/index.html","permalink":"http://www.iequa.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2018-07-16T08:59:48.000Z","updated":"2019-10-20T04:30:35.176Z","comments":true,"path":"tensorflow/index.html","permalink":"http://www.iequa.com/tensorflow/index.html","excerpt":"","text":"TensorFlow 用于机器学习和神经网络方面的研究，采用数据流图来进行数值计算的开源软件库. Keras 开发重点是支持快速的实验。能够以最小的时延把你的想法转换为实验结果，是做好研究的关键。 1. TensorFlow 简介 1.1 TensorFlow Why ? 1.2 TensorFlow 快速学习 &amp; 文档 2. Tensorflow 基础构架 2.1 处理结构: 计算图 2.2 完整步骤 例子2 🌰（创建数据、搭建模型、计算误差、传播误差、训练） 2.3 Session 会话控制 2.4 Variable 变量 2.5 Placeholder 传入值 2.6 什么是激励函数 (Activation Function) 2.7 激励函数 Activation Function 2.8 TensorFlow 基本用法总结 🌰🌰🌰 3. 建造我们第一个神经网络 3.1 添加层 def add_layer() 3.2 建造神经网络 🌰🌰🌰 3.3 Speed Up Training &amp; Optimizer (转载自莫烦) 4. Tensorboard 4.1 Tensorboard 可视化好帮手 1 5. Estimator 5.1 tf.contrib.learn 快速入门 5.2 tf.contrib.learn 构建输入函数 5.3 tf.contrib.learn 基础的记录和监控教程 5.4 tf.contrib.learn 创建 Estimator 5.5 TF 保存和加载模型 - 简书 1. Language model 介绍 语言模型是自然语言处理问题中一类最基本的问题，它有着非常广泛的应用。 1.1 RNN 循环神经网络 简介 1.2 LSTM &amp; Bi-RNN &amp; Deep RNN 1.3 Language model 介绍 / 评价方法 perplexity (not finish) 2. NNLM (神经语言模型) 2.2 PTB 数据的 batching 方法 2.3 RNN 的语言模型 TensorFlow 实现 3. MNIST 数字识别问题 3.1 简单前馈网络实现 mnist 分类 3.2 多层 CNNs 实现 mnist 分类, not finish 3.3 name / variable_scope 3.4 多层 LSTM 通俗易懂版 Python Python 哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码. Python 数据分析模块: Numpy &amp; Pandas, 及强大的画图工具 Matplotlib Python Numpy &amp; Pandas Matplotlib Scikit-Learn Sklearn 机器学习领域当中最知名的 Python 模块之一 why 1.1 : Sklearn Choosing The Right Estimator 1.2 : Sklearn General Learning Model 1.3 : Sklearn DataSets 1.4 : Sklearn Common Attributes and Functions 1.5 : Normalization 1.6 : Cross-validation 1 1.7 : Cross-validation 2 1.8 : Cross-validation 3 1.9 : Sklearn Save Model"},{"title":"","date":"2016-07-16T08:59:48.000Z","updated":"2020-12-22T10:31:13.324Z","comments":true,"path":"tweet/index.html","permalink":"http://www.iequa.com/tweet/index.html","excerpt":"","text":"Tweet Blair Chan Is Inputting 2017.05.28 Shuping Yang’s University of Maryland speech 2017.05.20 I and my sister at Qianjiang New Town’s Light Show. 2016.09.04 The people live and work in peace and contentment, treat each other with sincerity. 2016.07.08 从什么时候开始，蓝天☁白云已经是一种非常奢侈的享受 ？"},{"title":"用户内容兴趣挖掘文档","date":"2018-03-05T08:46:48.000Z","updated":"2019-10-20T04:33:53.409Z","comments":true,"path":"user_profile_content_interest/index.html","permalink":"http://www.iequa.com/user_profile_content_interest/index.html","excerpt":"","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\"], jax: [\"input/TeX\"], tex2jax: { inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ], displayMath: [ ['$$','$$']], processEscapes: true } }); 1. 需求 用户兴趣挖掘系统，基于用户在新浪网及博客上的访问历史，通过对日志及网页的分析，推测并记录用户的兴趣。 1.1 系统流程图 2. 数据源 &amp; 规范化 2.1 数据源 数据源 来自 上游 信息系统部提供的 suda log hive表，分别记录了新浪网的网页信息，blog 等博客信息. 2.2 规范化 规范化一些垃圾网页，曝光度低于量 和 去掉一些正文长度小于 200 字的网页… 3. 特征计算 One : 对于每个要推荐的内容,我们需要建立一份资料 : 比如词 k_ik\\_ik_i 在文档 d_jd\\_jd_j 中的权重 w_ijw\\_{ij}w_ij (常用的方法比如 TF-IDF) 有一个词表 Item [w_1w\\_1w_1, w_2w\\_2w_2, … ,w_4000w\\_{4000}w_4000], 对每个 document 建立一个词表 vector。 Two : 需要对用户也建立一份资料 : 比如说定义一个权重向量 (w_c1,...,w_ckw\\_{c1},...,w\\_{ck}w_c1,...,w_ck) , 其中 w_ciw\\_{ci}w_ci 表示第 k_ik\\_ik_i 个词对用户 ccc 的重要度 user 之前有看过的 小说 或 文档。(看过的文档放在一起搞一个doc_vector，或者 将 doc_vector 加权平均) Three : 计算匹配度 余弦距离公式 u(c,s)=cos⁡(w_c⃗,w_s⃗)=wc⃗⋅w_s⃗∣∣w_c∣∣⃗×∣∣w_s∣∣⃗u(c, s) = \\cos(\\vec{w\\_c}, \\vec{w\\_s}) = \\frac{\\vec{w_c} \\cdot \\vec{w\\_s}}{\\vec{||w\\_c||} \\times \\vec{||w\\_s||}} u(c,s)=cos(w_c​,w_s​)=∣∣w_c∣∣​×∣∣w_s∣∣​wc​​⋅w_s​​ 总结三步 : 对 每个 document 建立 vector 对 每个 user 建立 vector 比对 user 向量，与 该user 没有看过的 document 向量 之间的相似度 TF-IDF : 评估一个 word 对当前 document 的重要性。 当前升高而升高，所有doc中，升高而下降。 4. 策略层 4.1 page 分类 内容兴趣二级分类，大部分类可以支持到二级分类，返回二级类目的id；部分类：读书、游戏等，只支持到一级分类，返回一级类目的id。 类别权重用模型预测时返回的分数表示，不做加和为1的归一化处理。 4.2 用户兴趣计算 假设用户 uuu 某一天访问的页面集合 D={d_i},i=1..mD = \\lbrace d\\_i \\rbrace, i = 1..mD={d_i},i=1..m , 页面 d_id\\_id_i 的类别概率分布是 P_i={p_ij},j=1..kP\\_i = \\lbrace p\\_{ij} \\rbrace, j = 1..kP_i={p_ij},j=1..k 4.3 兴趣合并 短期兴趣 假设用户的历史兴趣集合为 {(c_i,t_i)},i=1 to m\\lbrace (c\\_i, t\\_i) \\rbrace, i = 1\\ to\\ m{(c_i,t_i)},i=1 to m mmm 是兴趣总数，c_ic\\_ic_i 是第iii个兴趣，t_it\\_it_i 是第iii个兴趣的生成时间，用距离今天的天数表示。 对 c_ic\\_ic_i 按照时间衰减：c_i′=c_i∗at_i{c\\_i}^{&#x27;} = c\\_i * a^{t\\_{i}}c_i′=c_i∗at_i，aaa 是衰减因子，取值 0.8。 最后，对兴趣数目进行控制，最多只保留 10 种兴趣。 长期兴趣 长期兴趣指用户在较长一段时间(至少3个月)内表现出的兴趣倾向。 策略如下： 每周进行一次用户兴趣的合并。 对于最近3个月的用户兴趣记录，统计如下两个指标: 兴趣的周出现比例 计算公式：occur_ratio = 出现的周数 / 总周数 如果 occur_ratio 小于阈值，则认为该兴趣非长期兴趣，过滤掉。阈值根据经验设定，默认设置为0.5。 兴趣的出现间隔方差 我们认为长期兴趣出现是呈现周期性的，用间隔方差表示。 计算方法：根据兴趣的周出现序列，得到相邻两次出现的间隔序列，然后计算该序列的方差。举例：假设兴趣在第1、3、5、8、9、12周出现，计算出的间隔序列是：[2, 2, 3, 1, 3]，方差是0.56。 如果间隔方差大于阈值，认为兴趣属于临时兴趣，非长期兴趣。 由于cookie容易被用户清理掉，生命周期较短，我们采用跨屏打通提供的新浪统一id对用户的长期行为进行跟踪。 兴趣归一化 对分数进行加和 和 归一化， 得到用户的兴趣类别概率: p(u,c)=score(u,c)∑i=1kscore(u,i)p(u, c) = \\frac {score(u, c)} {\\sum_{i=1}^k score(u, i)} p(u,c)=∑i=1k​score(u,i)score(u,c)​ 4. 技术点 page 词向量 3 种特征处理方式 log 访问次数 + 1 的平滑处理 LR (样本不平) &amp; Random Forest (样本数选择、决策树个数、特征选择、参数训练) 时间衰减 &amp; 间隔方差判断长期兴趣 next ⋯⋯ notes：next …"},{"title":"Python","date":"2018-01-06T08:46:48.000Z","updated":"2019-10-20T04:30:33.520Z","comments":true,"path":"python_language/index.html","permalink":"http://www.iequa.com/python_language/index.html","excerpt":"","text":"Python 是 Guido van Rossum 1989 年圣诞节期间，为了打发无聊的圣诞节而编写的一个编程语言. Python 哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码. Python 开发了很多明星网站，例如 YouTube、Instagram、Douban. Python 1.0 : Python Pyenv &amp; Anaconda3 1.1 : Python Data Analysis Lib Introduce 1.2 : Python Output、Variable、dataType、If、While/For、Py Head 1.3 : Python String-Encoding and Str Python 2 2.1 : Python List、Tuple、Dict、Set 2.2 : Python 定义函数、默认参数 2.3 : Python IO : Read File (open、append、readline、readlines、close、with open …) Class 3.1 : Class Advanced 4.1 : Slice、Iteration、List generation、Generator 4.2 : zip lambda map 4.3 : try … except … as … 4.4 : copy &amp; deepcopy 4.5 : pickle Project 结构化你的工程 next ⋯⋯ notes：next …"},{"title":"用户画像大标签体系结构","date":"2018-03-05T06:01:48.000Z","updated":"2019-10-20T04:33:53.410Z","comments":true,"path":"user_profile_label_system/index.html","permalink":"http://www.iequa.com/user_profile_label_system/index.html","excerpt":"","text":"我们定义的大标签体系结构,首先是一棵大的有一个虚拟根节点的树结构,能够无限扩展 新的分类体系。不同的分类体系之间,也可能存在一定的关系,可以通过关系自动挖掘算法, 建立各个标签节点之间的关联关系,并赋予一定的权重表示关系强弱程度。这种关联关系可以帮助我们实现不同标签体系的自动映射,在实际应用中会非常有用。 大标签体系构建一个数据标签体系,设计目标如下: 支持多维度标签体系。 可扩展。已有的分类体系可以添加、删除、修改,也可以加入新的分类体系。 标签语义及关系支持。为标签赋予语义信息,建立标签的关系图。 标签结构设计 如下 : 标签属性 标签属性 含义 ID 每个标签有全局唯一的 ID。 name 名称,原则上不要求全局唯一,但要能比较准确的描述该标签所指内容。 level 层级,标签所属层级。标签体系可以看做一个树结构,有个虚拟的根节点,层级是从虚根节点到标签节点的距离。所有一级分类的 level 是 1。 parent ID:父亲节点 ID,每个节点有一个唯一的父节点,以此可以还原出整棵树结构。 type:标签类型,对标签维度的描述,每一类标签有一个全局唯一的 type 标识。例如, type age/gender/interest/crowd/area/education/product 等。 concept 标签描述的概念类型,预留。 description 对标签的自然语言描述信息。 version 版本,用于跟踪标签的进化状态。初始是 1.0，持续递增。 creator 标签创建者,默认是所属公司名 (如 : x)。如果标签来自第三方公司,可以用该公司公司名称代替。 create_ts 创建时间,linux 时间戳。(待定预留) 标签关系 说明 标签关系 example 两个标签之间可能存在一定的语义关系。如“爱好奢侈品”跟“消费能力”可能存在某种强相关,而这两个标签属于两个不同的类型。通过关系,可以实现跨标签类型的扩展和挖掘。 关系定义是一个四元组&lt;ID1, ID2, weight, type&gt;。 ID1, ID2 : 发生关系的两个标签节点的 ID 。 weight:权重, 权重越大,表示关系越强。 type:关系类型,预留。 标签详细说明 age 标签 含义 600 - 601 0-20 岁 602 21-25 岁 603 26-30 岁 604 31-35 岁 605 36-40 岁 606 40-99 岁 gender 标签 含义 500 - 501 男 502 女 crowd 标签 含义 901 都市白领 902 运动健身 903 时尚达人 904 购车一族 905 商旅人士 906 美食爱好者 907 投资理财 908 家居家装 909 游戏部落 910 高端人士 911 亲子家庭 912 旅游出行 913 青春校园 914 极客人群 ￼ content interest 标签 含义 801 新闻 80101 新闻-国内 80102 新闻-国际 80103 新闻-社会 80104 新闻-航空 80105 新闻-天气 80106 新闻-灾害 802 军事 803 体育赛事 804 娱乐 805 财经 806 科技 807 时尚丽人 808 健康 809 房产 810 汽车 811 读书 812 历史 813 教育 814 育儿 ￼815 星座 816 旅游 ￼817 游戏 818 美酒美食 ￼819 文化艺术 820 宠物 ￼821 职场 822 科学探索 823 摄影 824 商业产品 825 ￼生活服务 business interest … 某用户画像最终形式示例如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&#123; &quot;com_its&quot;: [ &#123; &quot;id&quot;: &quot;20737&quot;, &quot;weight&quot;: 0.4976 &#125;, &#123; &quot;id&quot;: &quot;20847&quot;, &quot;weight&quot;: 0.25691 &#125; ], &quot;crowds&quot;: [ &#123; &quot;id&quot;: &quot;902&quot;, &quot;weight&quot;: 1.89692 &#125;, &#123; &quot;id&quot;: &quot;912&quot;, &quot;weight&quot;: 0.28242 &#125;, &#123; &quot;id&quot;: &quot;907&quot;, &quot;weight&quot;: 0.27972 &#125; ], &quot;gender&quot;: &quot;501&quot;, &quot;ad_clicks&quot;: [ &#123; &quot;type&quot;: &quot;PDPS000000028570&quot;, &quot;id&quot;: &quot;82901&quot;, &quot;weight&quot;: 8 &#125;, &#123; &quot;type&quot;: &quot;PDPS000000047262&quot;, &quot;id&quot;: &quot;138104&quot;, &quot;weight&quot;: 4 &#125; ], &quot;st_its&quot;: [ &#123; &quot;id&quot;: &quot;80302&quot;, &quot;weight&quot;: 0.49131 &#125;, &#123; &quot;id&quot;: &quot;80501&quot;, &quot;weight&quot;: 0.18479 &#125;, &#123; &quot;id&quot;: &quot;81205&quot;, &quot;weight&quot;: 0.16429 &#125; ], &quot;zones&quot;: [ &#123; &quot;id&quot;: &quot;316009&quot;, &quot;weight&quot;: 1 &#125; ], &quot;ages&quot;: [ &#123; &quot;id&quot;: &quot;603&quot;, &quot;weight&quot;: 0.45696 &#125;, &#123; &quot;id&quot;: &quot;602&quot;, &quot;weight&quot;: 0.40343 &#125; ], &quot;version&quot;: &quot;pc:3.0&quot;&#125; next ⋯⋯ notes：next …"},{"title":"","date":"2019-10-20T04:30:33.518Z","updated":"2019-10-20T04:30:33.517Z","comments":true,"path":"sina_project/pro2/index.html","permalink":"http://www.iequa.com/sina_project/pro2/index.html","excerpt":"","text":"2. dmp 项目 新浪dmp基于广告程序化购买场景，将广告主数据整合接入，并结合新浪自有数据，标准化统一管理数据。同时，通过对数据进行细化数据标签，完善分类体系，向用户提供多样化的数据应用服务。 12345678本人职责 ：(1). 用户标签规范化(2). 提供 redis 存储服务, 相关接口的封装 1) 支持接收不同来源的数据。 2) 支持访问 redis 集群策略可配，读数据负载均衡 3) 某机器或实例出故障，易于维护(3). 提供第三方数据用户推荐商品, 10种用户行为数据 的接入接口 12345678910111213141516171819202122host.txt 集群节点实例配置## REDIS1 DMP_INFO ## (128个节点，每个节点配置 60% 机器内存给 redis 实例)REDIS1#pool1:10.**.*.**:6382$10.**.*.**:6382|pool2:10.**.***.**:6382$10.**.*.**:6382|pool3:10.**.**.**:6382$10.**.*.**:6382......## REDIS3 ##REDIS3#pool1:172.**.***.**:6571$1|pool2:172.**.***.**:6571$1REDIS3#pool1:172.**.***.**:6572$2|pool2:172.**.***.**:6572$2...regionToRedis region选择集群DMP_INFO=REDIS1CROSS_INFO=REDIS33RD_INFO=REDIS3strategy.properties 策略配置REDIS1=read:random|pool2,pool3#write:pool1REDIS3=read:order|pool2,pool1#write:pool1 Coder 更多项目详情请点击… (为了更好的互相了解，脱敏后暂时放上git,后会迅速移除) 2.1 用户行为数据接收与挖掘 本模块是扶翼效果平台动态创意项目的一部分，主要完成用户在客户网站上的行为数据接收与分发。本期动态创意专指个性化重定向，即主要针对电商客户利用其站内用户行为数据和商品数据为广告用户展示最合适的一组商品信息（图片、价钱、折扣等）组成的创意 用户访问广告客户网站时，触发部署的监测代码，向新浪发送各种用户行为数据。 数据接收服务收到请求，进行数据解析与验证 根据接收的数据类型，更新数据对接状态 将接收数据处理为下游需要的格式，抓发到消息队列 为支持离线的数据挖掘，将数据写入日志，并实时发送至HDFS. config a) 修改配置bin/catalina.sh，添加java配置 12345678i. JAVA_OPTS=&#x27;-Xms40000m -Xmx40000m -Xmn10000m -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+DisableExplicitGC -XX:+CMSParallelRemarkEnabled -Dsun.rmi.dgc.server.gcInterval=86400000 -Dsun.rmi.dgc.client.gcInterval=86400000 -XX:+ExplicitGCInvokesConcurrent -XX:+CMSScavengeBeforeRemark -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=60 -XX:+UseCMSInitiatingOccupancyOnly&#x27; b) 修改配置conf/server.xml，connectors配置，添加 1234i. &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;ii. connectionTimeout=&quot;20000&quot;iii. redirectPort=&quot;8443&quot; iv. acceptCount=&quot;5000&quot; maxThreads=&quot;4000&quot;/&gt; 推荐商品接收接口 pc uv 3500W+ , wap uv 4500W+, weibo uv 1.6~1.8亿 200个并发做接口压测，qps是3000+，均延时65ms。延时主要耗在域名解析上，内网压测qps是3万+，均延时10ms next…"},{"title":"","date":"2019-10-20T04:30:33.517Z","updated":"2019-10-20T04:30:33.516Z","comments":true,"path":"sina_project/pro3/index.html","permalink":"http://www.iequa.com/sina_project/pro3/index.html","excerpt":"","text":"3. 离线分析调度框架 这是一个 shell 等其他语言配合写成的灵活调度框架. 该示例模块架 适用于离线分析调度，特别是每天跑的crontab任务，或者是每周、每月跑的任务. 多用于 hive 语句 或 其他脚本离线运行. 提供一些 best practice 提高各模块结构及代码的一致性 降低开发新模块的成本 便于离线大数据分析流程控制 具备报警，日志定位，常用检测依赖库函数 等功能 当然它也适用于对任何离线Job进行调度 不同人负责的模块不同，模块目录结构是一样的，如下所示 开发模块的时候，每个人只需要编写自己模块 script 下脚本 和 少量 crontab_job 脚本的改动即可. 目录结构 | 功能 | 详细说明 :— | :-- | -:- ─ crontab_job | crontab 脚本 | 由 linux crontab 触发, 检测整条流水线任务依赖关系, 调用 script 目录脚本等 ─ script | 主脚本 | shell、python脚本. shell 里主为 hql 或 streaming MR、spark-submit 任务等 ─ conf | 配置文件 | default.conf、vars.conf、alert.conf ─ log | 日志文件 | 主脚本log (多次运行多个log file)、crontab 脚本log (一个log file) ─ flag | 标记文件 | (标志该模块已运行，或运行完毕， crontab 会轮询检测) crontab_label.2017-12-13、ods_e_coupon/2017-12-13.all.done ─ util | 工具脚本 | logging、static_functions、static_hive_lib、crontab_job_env、env ─ alert | 报警封装 | (发送报警邮件的封装 alert、send_mail.py、constant_mail.py) | | ─ create_table | 建表脚本 | 建里 hive table 的语句 ─ jar | jar包 | 如 无 则不需要建立 ─ java | udf、udaf | 如 无 则不需要建立 conf/default.conf 123456789101112131415161718192021222324#系统环境变量export HADOOP=&quot;$&#123;HADOOP_HOME&#125;/bin/hadoop&quot;export HIVE=&quot;$&#123;HIVE_HOME&#125;/bin/hive&quot;export JAVA=&quot;$&#123;JAVA_HOME&#125;/bin/java&quot;#hadoop jarhadoop_jar=&quot;$&#123;HADOOP_HOME&#125;/share/packages/hadoop2/hadoop-streaming-2.7.2.jar&quot;#log_utilscheduler_log_script=&quot;$&#123;util_dir&#125;/logging&quot;#控制日志运行方式open_log=true#运行hadoop任务的用户名user=&quot;data_mining&quot;#hive表路径ods_hive_dir=&quot;/data_mining/dm/ods/&quot;mds_hive_dir=&quot;/data_mining/dm/mds/&quot;tmp_hive_dir=&quot;/data_mining/dm/tmp/&quot;## OSSOSS_URL=&quot;oss://your-key:your-value@x-bigdata.oss-cn-hangzhou-internal.aliyuncs.com&quot; crontab_job 123456789101112131415161718192021222324#check crontab_label whether existif check_local_crontab_label $&#123;flag_dir&#125; $&#123;d1&#125; then echo &quot;[INFO] script already run!&quot;else echo &quot;[INFO] check dependention&quot; check_hive_partitions tablename 2018-04-19# check_local_file()# check_hdfs_file()# ... echo &quot;[INFO] script run!&quot;# generate crontab_label touch_local_crontab_label $&#123;flag_dir&#125; $&#123;d1&#125;# run main script echo &quot;[INFO] start run...&quot; sh ods_dm_e_coupon.sh $d1 check_crontab_success $&#123;flag_dir&#125; $&#123;d1&#125; fiecho_ex &quot;run $0 end!&quot; bigdata-offline-demo 更多项目详情请点击… next…"},{"title":"","date":"2018-03-05T08:46:48.000Z","updated":"2019-10-20T04:30:33.515Z","comments":true,"path":"sina_project/proj/index.html","permalink":"http://www.iequa.com/sina_project/proj/index.html","excerpt":"","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\"], jax: [\"input/TeX\"], tex2jax: { inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ], displayMath: [ ['$$','$$']], processEscapes: true } }); 在所做的几个重点项目如下: ### [用户内容兴趣挖掘][2] 人群划分 dmp 系统 领券 推荐系统 next ⋯⋯ 大标签体系 notes：next …"},{"title":"新浪DMP存储系统","date":"2018-03-05T08:46:48.000Z","updated":"2019-10-20T04:30:33.519Z","comments":true,"path":"sina_project/proj_dmp/index.html","permalink":"http://www.iequa.com/sina_project/proj_dmp/index.html","excerpt":"","text":"新浪dmp基于广告程序化购买场景，将广告主数据整合接入，并结合新浪自有数据，标准化统一管理数据。同时，通过对数据进行细化数据标签，完善分类体系，向用户提供多样化的数据应用服务。 本人职责 ： (1). 用户标签规范化 (2). 提供 redis 存储服务, 相关接口的封装 1) 支持接收不同来源的数据。 2) 支持访问 redis 集群策略可配，读数据负载均衡 3) 某机器或实例出故障，易于维护 (3). 提供第三方数据cookie-mapping, 用户推荐商品, 10种用户行为数据 的接入 (4). 离线人群包挖掘 next ⋯⋯ notes：next …"},{"title":"","date":"2021-04-07T08:59:09.087Z","updated":"2021-04-07T08:59:09.087Z","comments":true,"path":"prepare/index.html","permalink":"http://www.iequa.com/prepare/index.html","excerpt":"","text":"No. desc Flag dict，list，set和tuple的区别？底层实现是hash/list/数组？ 1. list 被实现为长度可变的数组，每次都会分配略大的内存防止频繁的申请分配内存，连续的一块的内存 2. tuple 本身为一个结构体，结构体里面有一个二级指针，这是常量二级指针，可以形成一个指针数组 3. set : 允许空值的dict, 对dict有进行优化，在插入和删除元素的复杂度为常数级别，最坏也是O(n) 4. dict 底层使用的哈希表, 哈希表平均查找时间复杂度O(1) dict的key是不可变对象，因为要确保经过hash算法之后得到的地址唯一 py3.6+ dict是insert ordered，原来是根据hash值,乱序的，pop不一定是最后一个插入的键值对 ❎ 函数定义的时候参数前的*和**分别是什么意思，有什么区别？ fun(1,2,3,4), tuple 1, (2,3,4) / fun(1,a=2,b=3) dict 1, &#123;a:2, b:3&#125;1. ❎ 给变量a赋值int(1)，内存占4字节，后来又给a赋值str(1)，内存占1字节。请问两次赋值之间发生了什么？ python按引用赋值和深、浅拷贝 - [-5,256] 小整数优化， python int 占用 24~28 字节, 动态 ❎ 编码类型UTF-8，unicode，gbk任选两种说一下区别？ Unicode不是一个新的编码规则，而是一套字符集, Unicode 只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储. UTF-8编码: 编码规则就是UTF-8。UTF-8采用1-4个字符进行传输和存储数据，是一种针对Unicode的可变长度字符编码，又称万国码. Unicode符号范围（十六进制）, UTF-8编码方式(二进制) ❎ is和==的区别 ? Answ: is 用于判断两个变量是否引用,会对比其中两个变量的地址 ❎ 选一个module（比如numpy，pandas……）它的整体框架，主要应用场景，底层架构，（优缺点） pd.DataFrame(&#123;'A':[434,54],'B':[4,56]&#125;,index = [1,2]) Pandas 主要数据结构是一维数据(Series)、二维数据（DataFrame），这两种数据结构能满足金融、统计、社会等领域中大多典型用例。Pandas 是基于 NumPy 开发，可以与其它第三方计算支持库完美集成. Pandas缺点：处理大数据集的速度非常慢。 在默认设置下，Pandas只使用单个CPU内核，在单进程模式下运行函数。 3. Leetcode — 输入一个数据流（可以先对数据做预处理，任何预处理都可以，只要得到的数据和原数据是一一映射即可，考官举了一个例子是可以用时间戳），对每一个元素判断是否之前出现过。在尽量减小内存和时间的情况下，如果要求完全精确，如何做？如果允许出现误差，如何做，误差可以控制在多少范围内？ — 两个有序数组，大小分别是m和n，求整体的中位数，要求时间复杂度O(log(m+n)) 编程题：大数求和 ❎ 12345678910111213141516171819202122def big_data_add(a, b): # 1.先获取两个中最大的长度，然后将短进行补充，使长度一致 max_len = len(a) if len(a) &gt; len(b) else len(b) a = a.zfill(max_len) # &quot;abc&quot;.zfill(5) 00abc b = b.zfill(max_len) a = list(a) b = list(b) result = [0 for i in range(max_len+1)] # 这里加1主要是考虑到两数加起来可能比之前的数还多一位 for i in range(max_len-1, -1, -1): temp = int(a[i]) + int(b[i]) if temp &gt;= 10: # 这里result是i+1 是因为result的长度比max_len长度长 result[i+1] += temp % 10 result[i] += temp // 10 else: result[i+1] += temp return result 2个有序数据的中位数： 2分查找 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: def getKthElement(k): &quot;&quot;&quot; - 主要思路：要找到第 k (k&gt;1) 小的元素，那么就取 pivot1 = nums1[k/2-1] 和 pivot2 = nums2[k/2-1] 进行比较 - 这里的 &quot;/&quot; 表示整除 - nums1 中小于等于 pivot1 的元素有 nums1[0 .. k/2-2] 共计 k/2-1 个 - nums2 中小于等于 pivot2 的元素有 nums2[0 .. k/2-2] 共计 k/2-1 个 - 取 pivot = min(pivot1, pivot2)，两个数组中小于等于 pivot 的元素共计不会超过 (k/2-1) + (k/2-1) &lt;= k-2 个 - 这样 pivot 本身最大也只能是第 k-1 小的元素 - 如果 pivot = pivot1，那么 nums1[0 .. k/2-1] 都不可能是第 k 小的元素。把这些元素全部 &quot;删除&quot;，剩下的作为新的 nums1 数组 - 如果 pivot = pivot2，那么 nums2[0 .. k/2-1] 都不可能是第 k 小的元素。把这些元素全部 &quot;删除&quot;，剩下的作为新的 nums2 数组 - 由于我们 &quot;删除&quot; 了一些元素（这些元素都比第 k 小的元素要小），因此需要修改 k 的值，减去删除的数的个数 &quot;&quot;&quot; ix1, ix2 = 0, 0 while True: # 特殊情况 if ix1 == m: return nums2[ix2 + k - 1] if ix2 == n: return nums1[ix1 + k - 1] if k == 1: return min(nums1[ix1], nums2[ix2]) # 正常情况 newIndex1 = min(ix1 + k // 2 - 1, m - 1) newIndex2 = min(ix2 + k // 2 - 1, n - 1) pivot1, pivot2 = nums1[newIndex1], nums2[newIndex2] if pivot1 &lt;= pivot2: k = k - (newIndex1 - ix1 + 1) ix1 = newIndex1 + 1 else: k = k - (newIndex2 - ix2 + 1) ix2 = newIndex2 + 1 m, n = len(nums1), len(nums2) totalLength = m + n if totalLength % 2 == 1: return getKthElement((totalLength + 1) // 2) else: return (getKthElement(totalLength // 2) + getKthElement(totalLength // 2 + 1)) / 2 No. desc Flag 0. 客户信息表、合同信息表和还款计划表分别是什么？玩不透老板会怀疑我的能力？ 0. 字节跳动-数据仓库高级工程师面试 0. 大数据常见面试题之spark sql 1. 2020 BAT大厂数据分析面试经验：“高频面经”之数据分析篇 2. 2020年大厂面试题-数据仓库篇 1.手写&quot;连续活跃登陆&quot;等类似场景的sql - 好题目 ✔️ 3. 数仓大法好！跨境电商 Shopee 的实时数仓之路 ❎ 4. 【数仓面试题】使用Hive窗口函数替换union all处理分组汇总（小计，总计） 5. 字节跳动数仓面试 三道题-JAVA编程+hive窗口 6. 经典sql题目（使用窗口函数解决） Hive 分析函数lead、lag实例应用 手写&quot;连续活跃登陆&quot;等类似场景的sql 1234567891011121314151617181920select * from ( select user_id, date_id, lead(date_id, 1) over(partition by user_id order by date_id) as last_date_id # lead 参数1为列名，参数2为往下第n行（可选，默认为1) from ( select user_id, date_id from wedw_dw.tmp_log where date_id &gt;= &#x27;2020-08-10&#x27; and user_id is not null and length(user_id)&gt; 0 group by user_id, date_id order by user_id, date_id ) t ) t1where datediff(last_date_id,date_id)=1 大数据研发工程师 No. desc Flag 大数据研发工程师（两年）字节跳动面经 1. 数据不一致有没有遇到过，怎么解决的? 回答： 1. 指标体系,数仓 2. 规则引擎，复用逻辑 2. 一道sql的题，一张表，用户id和登录日期，查找连续两天登陆的用户 and (a.pdate = date_sub(b.pdate,1) or a.pdate = date_add(b.pdate,1)) 3. 怎么定位性能问题对应的是哪段sql? 1. spark driver log 看 执行慢的stage（99%） 2. spark ui 上看 该stage 的task 执行完成比率 3. spark ui 上看 该stage 对应的 continer id 和 所属job 4. spark ui 上看 sql 的执行计划 和 执行计划图，最终定位到是哪段sql ❎ 4. 遇到spark性能问题怎么解决的？ 1. 提交参数 2. 开发调优 3. Shuffle调优 4. 小文件 5. 121. 买卖股票的最佳时机 I maxprofit = max(price - minprice, maxprofit), minprice = min(price, minprice) 122. 买卖股票的最佳时机 II, 贪心: tmp = prices[i] - prices[i - 1], if tmp &gt; 0: profit += tmp DP： dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i]) dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i]) 6. linux 求一个文件出现某个单词的行数 linux做完用spark写 spark: long numAs = logData.filter(s -&gt; s.contains(“a”)).count(); 7. cache和persisit 的区别? cache只有一个默认的缓存级别MEMORY_ONLY，即将数据持久化到内存中. persist可以通过传递一个 StorageLevel 对象来设置缓存的存储级别. ❎ 8. 有优化过Spark执行性能吗，怎么优化的, 超全spark性能优化总结 9. spark on service 用过吗， spark context有退出的问题遇到过吗？ 这个知道没用过，所以没答出来，不过通过这个问题能看出来字节大佬还真挺厉害的，三面面试官对技术都这么了解 10. spark dataframe比rdd性能好，为啥? DataFrame运行效率优于RDD，因为它规定了具体的结构对数据加以约束. 由于DataFrame具有定义好的结构, Spark可以在作业运行时应用许多性能增强的方法. 如果你能够使用RDD完美地编写程序，也可以通过RDD实现相同的性能. Spark SQL的核心是Catalyst优化器，它以一种新颖的方式利用高级编程语言功能（例如Scala的模式匹配和quasiquotes）来构建可扩展的查询优化器, 它很容易添加优化规则 11. 堆外内存是干什么用的 netty。结点直接交互数据，spark 最新feature 弃用jvm，直接c++调用内存，都是堆外, Spark 2.x 执行内存和存储内存 相互之间 能 占用 1. (executor内存) JVM 内部 的 On-heap Memory （对于JVM来说叫做 堆内存） 2. (executor外部) JVM 外部/操作系统 的 Off-heap Memory 12. 知道什么是 whole stage codengen吗 面向接口编程太耗时间，主要是方法递归调用，虚函数调用 可以将一个stage的所有task整理成一个方法，并且生成动态字节码 并结合 13. 加强数仓和业务的学习 加强底层原理的学习 14. 我机智的回答：想深入业务 和 技术原理. 想优先考虑： data warehouse (高并发和实时流经验欠缺) 字节跳动大数据研发实习超详细面经（已拿offer） 1. leetcode: 二叉树层序遍历，按层换行输出 ❎ 2. 线程的状态及状态之间的装换 3. B+树的特点? B+树是一种树数据结构，通常用于数据库和操作系统的文件系统中。B+树的特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。B+树元素自底向上插入，这与二叉树恰好相反。 B树是为磁盘或其他直接存取的辅助存储设备而设计的一种平衡搜索树。B树类似于红黑树，但它们在降低磁盘I/O操作数方面要更好一些。 4. Redis支持的数据结构? 为什么性能高？ 为什么是单线程? 答： 将数据存储在内存，读取时候不需要进行磁盘的 IO，单线程也保证了系统没有线程的上下文切换。 String：缓存、计数器、分布式锁等。List：链表、队列、微博关注人时间轴列表等。Hash：用户信息、Hash 表等。Set：去重、赞、踩、共同好友等。Zset：访问量排行榜、点击量排行榜等。Zset 是有序的链表结构，其底层数据结构是跳跃表 skiplist 5. 场景题：如何从百亿条IP信息中得出访问量前10的IP地址 哈希分治法 1. ipv4 地址是一个 32 位的整数，可以用 uint 保存。 2. 我先设计一个哈希函数，把100个G的文件分成10000份，每份大约是 10MB，可以加载进内存了 6. 场景设计题：你自己如何设计一个分布式系统，实现对百亿条数据进行分组并求和 7. Spark shuffle机制? 8. 编程题：一个数组有正数有负数，调整数组中的数使得正负交替 1. 空间 O(1) 2. 保持原来的顺序 - 时间复杂度O(n^2) ， if (i % 2 == 0 &amp;&amp; arr[i] &lt; 0) continue; 3. 不用保持原来的顺序, O(n) 9. 医院排队候诊模型 假设一个医院，M个医生，N个病人，每个病人看病时长已知。写一个函数，做医生和病人的分配，要求医生负载尽量均衡。 10. 5 分钟理解 https 工作流程 11. Kafka如何保证生产者不丢失数据，消费端不丢失数据 字节跳动大数据岗 , 2019.07 1. 除了使用hive、spark。基本统计框架，自己实现一个word统计算法？ 我说了类似与mapreducer算法 2. 问了MapReduce执行流程以及问了RDD属性和问了一些transformation和action算子 3. hive能读取txt文件吗？以及读取哪些类型文件，若不能该怎么让其能读？ load data local inpath ‘/usr/testFile/result.csv’ overwrite into table biao; ❎ 4. 各个文件分布在不同的分布式系统中，如何快速的实现某个字段前三？ 5. 124. 二叉树最大路径和, self.maxSum = float(&quot;-inf&quot;) leftGain = max(maxGain(node.left), 0) 51. N 皇后， def backtrack(row: int) if: else: for 回溯 Hard 6. 225. 用队列实现栈 , self.queue = collections.deque() , push 后，在 reverse 过来 self.queue.append(x) for _ in range(n): self.queue.append(self.queue.popleft()) ❎ 7. 小和问题和逆序对问题 main: smallSum(arr,start,mid)+smallSum(arr,mid+1,end)+merge(arr,start,mid,end) core : Sum=Sum+arr[l]*(end-r+1) ❎ 字节跳动大数据开发工程师技术中台一二三面+hr面 12345select distinct a.uid from tb_log a left join tb_log b on a.uid = b.uid and (a.pdate = date_sub(b.pdate,1) or a.pdate = date_add(b.pdate,1)) 122. 买卖股票的最佳时机 II 1234567891011121314class Solution: def maxProfit(self, prices: List[int]) -&gt; int: size = len(prices) # dp 数组 dp = [[0, 0] for _ in range(size)] # 初始化 dp[0][0] = 0 dp[0][1] = -prices[0] for i in range(1, size): # 状态转移 dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i]) dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i]) return dp[size-1][0] 写sql, 求一个省份下的uv最高的城市 主要考察窗口函数 123456789101112131415161718192021222324252627282930select province, city from ( select province, city, row_number() over( partition by province order by uv desc ) rank from ( select province, city, count(distinct uid) uv from tb_log where pdate = &#123;date&#125; group by province, city ) a ) a1 where a1.rank = 1 二叉树中的最大路径和 12345678910111213141516171819202122232425class Solution: def __init__(self): self.maxSum = float(&quot;-inf&quot;) def maxPathSum(self, root: TreeNode) -&gt; int: def maxGain(node): if not node: return 0 # 递归计算左右子节点的最大贡献值 # 只有在最大贡献值大于 0 时，才会选取对应子节点 leftGain = max(maxGain(node.left), 0) rightGain = max(maxGain(node.right), 0) # 节点的最大路径和取决于该节点的值与该节点的左右子节点的最大贡献值 priceNewpath = node.val + leftGain + rightGain # 更新答案 self.maxSum = max(self.maxSum, priceNewpath) # 返回节点的最大贡献值 return node.val + max(leftGain, rightGain) maxGain(root) return self.maxSum N 皇后 1234567891011121314151617181920212223242526272829303132333435class Solution: def solveNQueens(self, n: int) -&gt; List[List[str]]: def generateBoard(): board = list() for i in range(n): row[queens[i]] = &quot;Q&quot; board.append(&quot;&quot;.join(row)) row[queens[i]] = &quot;.&quot; return board def backtrack(row: int): if row == n: board = generateBoard() solutions.append(board) else: for i in range(n): if i in columns or row - i in diagonal1 or row + i in diagonal2: continue queens[row] = i columns.add(i) diagonal1.add(row - i) diagonal2.add(row + i) backtrack(row + 1) columns.remove(i) diagonal1.remove(row - i) diagonal2.remove(row + i) solutions = list() queens = [-1] * n columns = set() diagonal1 = set() diagonal2 = set() row = [&quot;.&quot;] * n backtrack(0) return solutions Java线程的5种状态及状态之间转换 Hadoop, HDFS, MR, Yarn Job 七、介绍一下HDFS读写流程 HDFS block数据块大小为128MB, 默认情况下每个block有三个副本, NameNode主节点， DataNode 从节点. HDFS client上传数据到HDFS时，首先，在本地缓存数据，当数据达到一个block大小时。请求NameNode分配一个block。 NameNode会把block所在的DataNode的地址告诉HDFS client。 HDFS client会直接和DataNode通信，把数据写到DataNode节点一个block文件里 No. Read HDFS (download) - FSDataInputStream() 4步 Flag 1. FileSystem对象的open == DistributedFileSystem() 2. get block locations from NameNode （rpc） 3. Client 与 DataNode 通信, FSDataInputStream对象，该对象会被封装DFSInputStream对象 4. 假设第一块的数据读完了，就会关闭指向第一块的datanode连接。接着读取下一块. No. Writing HDFS (upload) - FSDataOutputStream() 6不步 ACK queue Flag 1. client通过调用DistributedFileSystem的create方法创建新文件 2. DFileSystem通过RPC调用namenode去创建一个没有blocks关联的新文件, 创建前， namenode做校验. 3. 前两步结束后。会返回FSDataOutputStream的对象，与读文件的时候类似， FSDataOutputStream被封装成DFSOutputStream。DFSOutputStream能够协调namenode和datanode。client開始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小的packet。然后排成队列data quene 4. DataStreamer会去处理接受data quene，它先询问namenode这个新的block最适合存储的在哪 5. DFSOutputStream另一个对列叫ack quene。也是由packet组成，等待datanode的收到响应，当pipeline中的全部datanode都表示已经收到的时候，这时akc quene才会把相应的packet包移除掉。 6. client完毕写数据后调用close方法关闭写入流 MapReduce过程详解 1. Map端整个流程分为4步 来源于HDFS的Block 在经过Mapper运行后，输出是Key/Value - 默认对Key进行哈希运算后，再以ReduceTask数量取模 内存缓冲区的大小是有限的，默认是100MB, Spill，中文译为溢写 每次溢写都在磁盘上生成一溢写文件，如Map结果很大，就有多次溢写发生，磁盘上就会有多个溢写文件, 后merge 2. Reduce端整个流程分为3步 Copy过程. 即简单地拉取数据 Merge阶段: 复制过来 数据会先放到内存缓冲区中，当达到一定阈值时，就会启动内存到磁盘的Merge. Reducer输出文件。不断Merge后，最后生成一个“最终文件”. 当Reducer输入文件已定，整个Shuffle过程才结束. No. MapReduce的Shuffle过程 Flag 1. Map方法之后Reduce方法之前这段处理过程叫「Shuffle」 2. Map方法之后，数据首先进入到分区方法，把数据标记好分区，然后把数据发送到环形缓冲区； 环形缓冲区达到80%时，进行溢写； 排序的手段「快排」；溢写产生大量溢写文件，需要对溢写文件进行「归并排序」；对溢写的文件也可以进行Combiner操作，前提是汇总操作，求平均值不行。最后将文件按照分区存储到磁盘，等待Reduce端拉取。 3. 每个Reduce拉取Map端对应分区的数据。拉取数据后先存储到内存中，内存不够了，再存储到磁盘。拉取完所有数据后，采用归并排序将内存和磁盘中的数据都进行排序。在进入Reduce方法前，可以对数据进行分组操作。 No. Yarn 的 Job 提交流程 8 步， Client-&gt;RM-&gt;C-&gt;AM–rm–&gt;C nums-&gt;NM-&gt;C-&gt;注销AM+C Flag 1. client向RM提交应用程序 2. ResourceManager启动一个Container用于运行ApplicationMaster 3. 启动中的ApplicationMaster向ResourceManager注册自己，启动成功后与RM保持心跳 4. AM 向 RM 发送请求,申请相应数目的 Container 5. 申请成功的Container，由AM进行初始化。Container的启动信息初始化后，AM与对应的NodeManager通信，要求NM启动Container 6. NM启动container 7. container运行期间，AM 对 Container进行监控。Container通过RPC协议向对应的AM汇报自己的进度和状态等信息 8. 应用运行结束后，AM 向 RM 注销自己，并允许属于它的 Container被收回 0. Glassdoor No. Question Flag 1. hashmap questions 哈希冲突解决方法 : 关键字值不同的元素可能会映象到哈希表的同一地址上就会发生哈希冲突1. 开放定址法 2. 再哈希法 3. 链地址法 4. 建立公共溢出区 大厂面试必问！HashMap 怎样解决hash冲突？ 为什么 Map 桶中超过 8 个才转为红黑树？ HashMap指南 HashMap面试 shop大数据面试 No. Question Flag 2. What is the difference between optimistic and pessimistic locks? 数据库锁机制（乐观锁和悲观锁、表锁和行锁） 你了解乐观锁和悲观锁吗？ 1、CAS（Compare And Swap） - CAS只能保证单个变量操作的原子性 2、版本号机制 3、乐观锁加锁吗？ 4、CAS有哪些缺点？ The coding test had 2 questions, were about heaps and double-ended queues. 1234567891011121314# from heapq import heappush, nsmallest, nlargest, ...heap = []for i in range(3): heappush(heap, i)# heappop(heap)：弹出堆中最小的元素# heapify(heap)：将列表转换为堆# heapreplace(heap, x)：弹出堆中最小的元素，然后将新元素插入# nlargest(n, iter)、nsmallest(n, iter)：用来寻找任何可迭代对象iter中的前n个最大的或前n个最小的元素queue = collections.deque()queue.append(5)queue.appendleft(10)cur = queue.popleft()cur = queue.pop() there was a question on writing an SQL query and command line applications. General DWH concepts, Spark internals, mapreduce. They also had few questions on coding which were focused on data structures &amp; algorithms. The interviewers look at how you’re thought process. Explain the map reduce paradigm. Hadoop 是能对大量数据进行分布式处理的软件框架 包括 Hdfs，MapReduce，Yarn Spark: RDD计算时是把数据全部加载至内存么? good - 博客园 Spark Shuffle Shuffle的本质: Stage是以shuffle作为分界的! Shuffle不过是偷偷的帮你加上了个类似saveAsLocalDiskFile的动作。 如果是M/R的话: 每个Stage其实就是上面说的那样，一套数据被N个嵌套的函数处理(也就是你的transform动作)。遇到了Shuffle,就被切开来。Shuffle本质上是把数据按规则临时都落到磁盘上，相当于完成了一个saveAsTextFile的动作，不过是存本地磁盘。然后被切开的下一个Stage则以本地磁盘的这些数据作为数据源，重走上面的流程。 several questions about database, sharding, RMDB vs NoSQL DB, Why distributed NoSQL DB cannot always support transaction? TiDB, 也是可以支持事务的，只是开销非常大 leetcode: solve a problem of top k problem in an online white board 123456789101112131415161718# return heapq.nsmallest(k, arr)import heapqclass Solution: def smallestK(self, arr: List[int], k: int) -&gt; List[int]: if k&gt;len(arr) or k==0: return [] heap = [] for i in arr[:k]: heapq.heappush(heap, -i) for i in arr[k:]: if i &lt; -heap[0]: heapq.heappop(heap) heapq.heappush(heap, -i) result = [] for i in range(k): result.append(-heapq.heappop(heap)) return result[::-1] 扩展: 1). 692. 前K个高频单词 2). 347. 前 K 个高频元素 3). 215. Kth Largest Element in an Array 4). 面试题 17.14. 最小K个数 (排序) Level traverse a binary tree in an online white board. 12345q = deque()q.append(), q.popleft()while q: passlist(res.values()) 1. Operating System No. Question Flag 1. 进程切换说一下 进程切换具体哪些资源？ 2. Linux的Kill命令（-9信号的作用） 3. 进程切换和线程切换： 进程切换：① 切换页目录以使用新地址空间；② 切换内核栈和硬件上下文； 线程切换不用切地址空间，也就是不用做① 上下文切换通过OS内核完成，性能损耗主要来源于① 寄存器内容切出切入；② 切换后CPU原本的缓存作废，TLB（页表缓冲）等都被刷新，导致一段时间的内存访问十分低效（线程切换没有这个问题） 进程性能 与 系统性能 cpu：top, 内存：free, 带宽：netstat, strace 内核时间 vs 用户时间， 库时间 vs 应用程序时间， 细分应用程序时间 gprof vs oprofile gprof用于分析函数调用耗时，可用之抓出最耗时的函数，以便优化程序 gprof是GNU profile工具，可以运行于linux、AIX、Sun等操作系统进行C、C++、Pascal、Fortran程序的性能分析 2. Database No. Question Flag 0. 幻读： InnoDB MVCC 的实现，通过保存数据在某个时间点的快照来实现的 ❎ 1. 事务4个特性ACID 有哪些 并分别解释? 事务是指是程序中一系列严密的逻辑操作，而且所有操作必须全部成功完成. A原子性： 事务是数据库的逻辑工作单位，不可分割C一致性： 数据库从一个一致性状态变到另一个一致性状态 I 隔离性：一个事务的执行不能其它事务干扰 D持久性： 一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的，不能回滚 ❎ 2. MySQL隔离级别有哪些? SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。 1. Read Uncommitted（读取未提交内容）- 也称为 脏读(Dirty Read) - RollBack 2. Read Committed（读取提交内容）- 一个事务只能看见已经提交事务所做的改变 3. Repeatable Read（可重读） - 同一事务并发读同样结果. InnoDB MVCC 解决幻读 4. Serializable（可串行化）- 事务排序解决 幻读问题 1. 脏读(Drity Read): 某事务已更新了数据，RollBack了操作，则后一个事务所读取的数据就会是不正确.2. 不可重复读: 在一事务的两次查询数据不一致，可能中间插入了一个事务更新原有的数据.3. 幻读(Phantom Read): 在一事务的两次查询中数据笔数不一致. 另一事务却在此插入了新的几列数据. ❎ 3. SQL的索引采用什么数据结构？（B+树） ❎ 4. 聚簇索引InnoDB / 非聚簇索引Myisam ❎ 5. 主键和索引的区别？ 1. 主键是为了标识数据库记录唯一性，不允许记录重复，且键值不能为空，主键也是一个特殊索引. 2. 索引可提高查询速度，它相当于字典的目录，可通过它很快查询到想要的结果，而不需要进行全表扫描. 3. 主键也可以由多个字段组成，组成复合主键，同时主键肯定也是唯一索引. ❎ 6. Redis的过期策略和内存淘汰策略不要搞混淆 1. Redis的过期策略 - 在程序中可以设置Redis中缓存的key的过期时间. 1.1 定时过期 - 会占用大量的CPU 1.2 惰性过期 - 占用内存多 1.3 定期过期：每隔一定的时间，会扫描一定数量expires key 2. Redis的内存淘汰策略是指内存不足时，怎么处理需要新写入且需要申请额外空间的数据. ❎ 7. 如何解决哈希冲突 （拉链法，线性探测法…拓展巴拉巴拉） MySQL的存储引擎： 因为面试前看了一篇关于B+数结构的文章，满脑子都是B+树，没答好，续多Innodb的特性都没答到 InnoDB是MySQL目前默认的存储引擎，底层使用了B+树作为数据结构，与MyiSAM不同的时，InnoDB属于聚集索引，主键和数据一起存储在B+树的叶子节点中，而MyiSAM的主键和数据是分开存储的，叶子节点中存储的是数据所在的地址。InnoDB和MyiSAM的区别： 存储方式：前者索引和数据共存于一个文件中；后者索引和数据分开存储 锁粒度：前者支持行锁（MVCC特性)；而后者仅支持到表锁 事务支持：前者支持事务；后者不支持事务 对于写多的场景，由于MyiSAM需要频繁的锁表，性能开销比InnoDB大得多 对于读多写少的场景，由于InnoDB每次操作都需要在事务中，MyiSAM的性能可能会比前者好 3.0 LRU 12345678910111213141516class DLinkedNode: def __init__(self, key=0, value=0): self.key = key self.value = value self.prev = None self.next = None class LRUCache: def __init__(self, capacity: int): self.head = DLinkedNode() self.tail = DLinkedNode() self.head.next = self.tail self.tail.prev = self.head self.capacity = capacity self.size = 0 self.cache = &#123;&#125; 3.1 quickSort 12345678910111213141516171819def quickSort(nums, left, right): if left &lt; right: l, r = left, right x = nums[l] while True: while l &lt; r and nums[r] &gt;= x: r -= 1 while l &lt; r and nums[l] &lt;= x: l += 1 if l &gt;= r: break nums[l], nums[r] = nums[r], nums[l] nums[left], nums[l] = nums[l], nums[left] quickSort(nums, left, l - 1) quickSort(nums, l + 1, right) return nums 3.2 mergeSort 1234567891011121314151617181920212223242526def mergeSort(nums, l, r): if l &gt;= r: return mid = (l + r) // 2 mergeSort(nums, l, mid) mergeSort(nums, mid + 1, r) arr = [0] * (r - l + 1) k, i, j = 0, l, mid + 1 while i &lt;= mid and j &lt;= r: if nums[i] &lt;= nums[j]: arr[k] = nums[i] k, i = k + 1, i + 1 else: arr[k] = nums[j] # ans += (mid+1-i); k, j = k + 1, j + 1 while i &lt;= mid: arr[k] = nums[i] k, i = k + 1, i + 1 while j &lt;= r: arr[k] = nums[j] k, j = k + 1, j + 1 for i in range(l, r+1): nums[i] = arr[i - l] 3.3 isValidBST 123456789101112131415class Solution: def isValidBST(self, root): stack, pre = [], float(&#x27;-inf&#x27;) while stack or root: while root: stack.append(root) root = root.left root = stack.pop() # 如果中序遍历得到的节点的值小于等于前一个 inorder，说明不是二叉搜索树 if root.val &lt;= pre: return False pre = root.val root = root.right return True 123456789101112131415class Solution: def isValidBST(self, root): def helper(node, lower = float(&#x27;-inf&#x27;), upper = float(&#x27;inf&#x27;)): if not node: return True val = node.val if val &lt;= lower or val &gt;= upper: return False if not helper(node.right, val, upper): return False if not helper(node.left, lower, val): return False return True return helper(root) 3.4 isHappy 1234567891011121314def isHappy(self, n: int) -&gt; bool: def get_next(number): total_sum = 0 while number &gt; 0: number, digit = divmod(number, 10) total_sum += digit ** 2 return total_sum slow_runner = n fast_runner = get_next(n) while fast_runner != 1 and slow_runner != fast_runner: slow_runner = get_next(slow_runner) fast_runner = get_next(get_next(fast_runner)) return fast_runner == 1 93. 复原IP地址 123456789101112131415161718192021222324252627282930313233343536class Solution: def restoreIpAddresses(self, s: str) -&gt; List[str]: SEG_COUNT = 4 ans = list() segments = [0] * SEG_COUNT def dfs(segId: int, segStart: int): # 如果找到了 4 段 IP 地址并且遍历完了字符串，那么就是一种答案 if segId == SEG_COUNT: if segStart == len(s): ipAddr = &quot;.&quot;.join(str(seg) for seg in segments) ans.append(ipAddr) return # 如果还没有找到 4 段 IP 地址就已经遍历完了字符串，那么提前回溯 if segStart == len(s): return # 由于不能有前导零，如果当前数字为 0，那么这一段 IP 地址只能为 0 if s[segStart] == &quot;0&quot;: segments[segId] = 0 dfs(segId + 1, segStart + 1) # 一般情况，枚举每一种可能性并递归 addr = 0 for segEnd in range(segStart, len(s)): addr = addr * 10 + (ord(s[segEnd]) - ord(&quot;0&quot;)) if 0 &lt; addr &lt;= 0xFF: segments[segId] = addr dfs(segId + 1, segEnd + 1) else: break dfs(0, 0) return ans 46. 全排列 12345678910111213141516171819class Solution: def permute(self, nums): def backtrack(first=0): # 所有数都填完了 if first == n: res.append(nums[:]) for i in range(first, n): # 动态维护数组 nums[first], nums[i] = nums[i], nums[first] # 继续递归填下一个数 backtrack(first + 1) # 撤销操作 nums[first], nums[i] = nums[i], nums[first] n = len(nums) res = [] backtrack(first=0) return res 1262. 可被三整除的最大和 123输入：nums = [3,6,5,1,8]输出：18解释：选出数字 3, 6, 1 和 8，它们的和是 18（可被 3 整除的最大和）。 方法二：贪心 + 逆向思维 我们把数组中的数分成三部分 a，b 和 c，它们分别包含所有被 3 除余 0，1，2 的数。显然，我们可以选取 a 中所有的数，而对于 b 和 c 中的数，我们需要根据不同的情况选取不同数量的数。 我们设 tot 为数组 nums 中所有元素的和，此时 tot 会有三种情况： tot 是 3 的倍数，那么我们不需要丢弃任何数； tot 模 3 余 1，此时我们有两种选择：要么丢弃 b 中最小的 1 个数，要么丢弃 c 中最小的 2 个&gt; tot 模 3 余 2，此时我们有两种选择：要么丢弃 b 中最小的 2 个数，要么丢弃 c 中最小的 1 个数。 12345678910111213141516171819202122class Solution: def maxSumDivThree(self, nums: List[int]) -&gt; int: a = [x for x in nums if x % 3 == 0] b = sorted([x for x in nums if x % 3 == 1], reverse=True) c = sorted([x for x in nums if x % 3 == 2], reverse=True) tot = sum(nums) ans = 0 if tot % 3 == 0: ans = tot if tot % 3 == 1: if len(b) &gt;= 1: ans = max(ans, tot - b[-1]) if len(c) &gt;= 2: ans = max(ans, tot - sum(c[-2:])) elif tot % 3 == 2: if len(b) &gt;= 2: ans = max(ans, tot - sum(b[-2:])) if len(c) &gt;= 1: ans = max(ans, tot - c[-1]) return ans 4. 寻找两个正序数组的中位数 题解：二分查找 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: def getKthElement(k): &quot;&quot;&quot; - 主要思路：要找到第 k (k&gt;1) 小的元素，那么就取 pivot1 = nums1[k/2-1] 和 pivot2 = nums2[k/2-1] 进行比较 - 这里的 &quot;/&quot; 表示整除 - nums1 中小于等于 pivot1 的元素有 nums1[0 .. k/2-2] 共计 k/2-1 个 - nums2 中小于等于 pivot2 的元素有 nums2[0 .. k/2-2] 共计 k/2-1 个 - 取 pivot = min(pivot1, pivot2)，两个数组中小于等于 pivot 的元素共计不会超过 (k/2-1) + (k/2-1) &lt;= k-2 个 - 这样 pivot 本身最大也只能是第 k-1 小的元素 - 如果 pivot = pivot1，那么 nums1[0 .. k/2-1] 都不可能是第 k 小的元素。把这些元素全部 &quot;删除&quot;，剩下的作为新的 nums1 数组 - 如果 pivot = pivot2，那么 nums2[0 .. k/2-1] 都不可能是第 k 小的元素。把这些元素全部 &quot;删除&quot;，剩下的作为新的 nums2 数组 - 由于我们 &quot;删除&quot; 了一些元素（这些元素都比第 k 小的元素要小），因此需要修改 k 的值，减去删除的数的个数 &quot;&quot;&quot; index1, index2 = 0, 0 while True: # 特殊情况 if index1 == m: return nums2[index2 + k - 1] if index2 == n: return nums1[index1 + k - 1] if k == 1: return min(nums1[index1], nums2[index2]) # 正常情况 newIndex1 = min(index1 + k // 2 - 1, m - 1) # k=6, 2 newIndex2 = min(index2 + k // 2 - 1, n - 1) # k=6, 2 pivot1, pivot2 = nums1[newIndex1], nums2[newIndex2] if pivot1 &lt;= pivot2: k -= newIndex1 - index1 + 1 index1 = newIndex1 + 1 else: k -= newIndex2 - index2 + 1 index2 = newIndex2 + 1 m, n = len(nums1), len(nums2) totalLength = m + n if totalLength % 2 == 1: return getKthElement((totalLength + 1) // 2) else: return (getKthElement(totalLength // 2) + getKthElement(totalLength // 2 + 1)) / 2 经验分享 Operating System 进程与线程的区别 进程是操作系统分配资源的单位 线程(Thread)是进程的一个实体，是CPU调度和分派的基本单位 线程和进程的关系是：线程是属于进程的，线程运行在进程空间内，同一进程所产生的线程共享同一内存空间，当进程退出时该进程所产生的线程都会被强制退出并清除。线程可与属于同一进程的其它线程共享进程所拥有的全部资源，但是其本身基本上不拥有系统资源，只拥有一点在运行中必不可少的信息(如程序计数器、一组寄存器和栈)。 虚拟内存是怎么调度的? github OS笔记 虚拟内存调度方式（页式、段式、段页式） 怎样通俗的理解操作系统中内存管理分页和分段？ 分页方式的优点是页长固定，因而便于构造页表、易于管理，且不存在外碎片。但分页方式的缺点是页长与程序的逻辑大小不相关。 LRU 是什么? 复杂度? Cache &amp; 页面置换 HTTP与HTTPS的区别 ? 为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL协议，SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。 HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。 5. 网络 TCP 5.1 三次握手 TCP 的三次握手, 四次挥手: TCP 协议是如何建立和释放连接的？ 三次握手建立连接: 第一次握手：A给B打电话说，你可以听到我说话吗？（seq=x） 第二次握手：B收到了A的信息，然后对A说：我可以听得到你说话啊，你能听得到我说话吗？（ACK=x+1，seq=y） 第三次握手：A收到了B的信息，然后说可以的，我要给你发信息啦！（ack=y+1） 5.2 四次挥手 四次挥手释放连接: A:喂，我不说了。(FIN) B:我知道了。等下，上一句还没说完。Balabala……（ACK） B:好了，说完了，我也不说了。（FIN） A:我知道了。(ACK) A等待 2MSL,保证B收到了消息,否则重说一次我知道了。 TCP四次挥手中的TIME_WAIT状态 Reference other: Shopee大数据 0086 shopee题汇总 good - 新加坡Singapore Data infra 经验分享 一亩三分地 - Shopee新加坡面经 2020 年 Shopee 秋招面经 Shopee虾皮技术面 操作系统虚拟内存调度方式（页式、段式、段页式） 数仓大法好！跨境电商 Shopee 的实时数仓之路 other: 各大公司近期 data engineer 面经大全 求职面试分享 [2019.07.28] 面圈网 shop大数据"},{"title":"Coding","date":"2021-02-28T07:55:08.295Z","updated":"2021-02-28T07:55:08.295Z","comments":true,"path":"lc/index.html","permalink":"http://www.iequa.com/lc/index.html","excerpt":"","text":"Leetcode 分类总结 LCP 18. 早餐组合, LCP 19. 秋叶收藏集, sort与sorted的区别 1234567# &quot;abc&quot;.zfill(5) 00abc# result.append(-heapq.heappop(heap)) return result[::-1]class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None Data Engineer No. Question Flag 840. Magic Squares In Grid for i in range(len(grid)-2): for j in range(len(grid[i])-2): subset.append(grid[i][j:j+3]) subset.append(grid[i+1][j:j+3]) subset.append(grid[i+2][j:j+3]) ❎ 25. K 个一组翻转链表 1-&gt;2-&gt;3-&gt;4-&gt;5 当 k = 3 时，应当返回: 3-&gt;2-&gt;1-&gt;4-&gt;5 def reverse(self, head: ListNode, tail: ListNode): prev=tail.next p=head def reverseKGroup(head, k): hair = ListNode(0) while head: (1) 查看剩余部分长度是否大于等于 k (2). 把子链表重新接回原链表 hard 1. 股票最大利润(买卖一次) cost, profit = float(&quot;+inf&quot;), 0 cost = min(cost, price); profit = max(profit, price - cost) ❎ 2. Move Zeroes for i in range(len(nums)): if nums[i]: swap(nums[i], nums[j]) ❎ 3. 102. 二叉树的层序遍历from collections import dequequeue = collections.deque(); queue.append(root); while queue: size = len(queue)cur = queue.popleft()queue.append(cur.left)queue.append(cur.right) ❎ 4. 83. 删除排序链表中的重复元素 while cur and cur.next: if … : cur.next = cur.next.next82. 删除排序链表中的重复元素 II - 删除所有含有重复数字的节点 dHead = ListNode(0), dHead.next = head pre,cur = dHead,head; while cur: pre.next = cur.next 跳过重复部分 ❎ 5. 如何实现LRU, 双向链表+Dict+Size+Cap class DLinkedNode(4), removeTail, moveToHead, addToHead, removeNode ✔️❎ 6. 125. 验证回文串, while left &lt; right: while left &lt; right and not s[left].isalnum(): 扩展: 5. 最长回文子串 dp, 枚举长度 for l in range(n): for i in n: dp[i][j] = (dp[i + 1][j - 1] and s[i] == s[j]) ❎ 7. 101. 对称二叉树 class TreeNode: def __init__(self, x): isSymmetricHelper(left.left, right.right) and isSymmetricHelper(left.right, right.left) ❎ 8. 98. 验证二叉搜索树, stack, inorder = [], float(’-inf’)while stack or root: while root ❎ 9. 找出数组里三个数相乘最大的那个（有正有负）, nums.sort() a = nums[-1] * nums[-2] * nums[-3]b = nums[0] * nums[1] * nums[-1] ❎ 10. 做题：两个十六进制数的加法 ❎ 11. 93. 复原IP地址, &quot;.&quot;.join(['1','2','3','4']) == '1.2.3.4', ord(&quot;a&quot;) = 97 dfs(seg_id, seg_start) for seg_end in range(seg_start, len(s)): if 0 &lt; addr &lt;= 0xFF（11111111==255): ✔️❎ 12. 202. 快乐数, divmod(79, 10) = (7,9); while n &gt; 0: n, digit = divmod(n, 10) total_sum += digit ** 2 ❎ 13. 快排归并手撕 for i in range(l, r+1): nums[i] = arr[i - l] ❎ 14. 1143. 最长公共子序列 dp = [[0] * (n + 1) for _ in range(m + 1)] if text1[i - 1] == text2[j - 1]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) ❎ 15. 3. 无重复字符的最长子串, occ=set(); for l in range(n): remove(i-1), while r+1 &lt; n and s[r+1] not in occ: add(r+1) ❎ 16. 405-数字转换为十六进制数, bin(dec), oct(dec), hex(dec), int(‘0b10000’, 2) ❎ 17. 67. 二进制求和， for i, j in zip(a[::-1], b[::-1]): s = int(i) + int(j) + carry, r = str(s % 2) + r, carry = s // 2 list(zip([1,2,3], [4,5,6])) == [(1, 4), (2, 5), (3, 6)] ❎ 18. 4. 寻找两个正序数组的中位数 - hard , 二分查找 O(log (m+n)) , k/2-1=7/2−1=2 def getKthElement(k): A: 1 3 4 9 ↑B: 1 2 3 4 5 6 7 8 9 ↑k=k-k/2=4, 下一个位置是 k/2-1 = 4/2-1 = 1 ✔️❎ 19. 剑指 Offer 55 - II. 平衡二叉树 (1). abs(maxHigh(root.left) - maxHigh(root.right)) &lt;= 1 (2). self.isBalanced(root.left) and self.isBalanced(root.right) ❎ 20. 155. 最小栈, self.stack = [], self.min_stack = [float(‘inf’)] ❎ 21. 非递归单链表反转 现场手写 ❎ 22. 105. 从前序与中序遍历序列构造二叉树 root = TreeNode(preorder[0]) i = inorder.index(preorder[0]) ❎ 23. 全排列, def dfs(x): if x == len© - 1: res.append(’’.join©) for i in range(first, n): ❎ 24. 1262. 可被三整除的最大和, 题解 贪心+逆向思维： a = [x for x in nums if x % 3 == 0] b = sorted([x for x in nums if x % 3 == 1], reverse=True) c = sorted([x for x in nums if x % 3 == 2], reverse=True) ❎ 27. 两千万个文件找最小的一千个（答错了，应该用大顶堆，答成了小顶堆） ❎ 28. 10亿个数中找出最大的10000个数? 将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的10000个，最后在剩下的100*10000个数据里面找出最大的10000个 分治法 29. 1000个数据，查找出现次数最多的k个数字 我们首先一样是要把这十亿个数分成很多份。例如 1000份，每份 10万。然后使用 HashMap&lt;int,int&gt; 来统计。在每一次的统计中，我们可以找出最大的100个数？ 这样100*10000 可以 快排序 解决 1. 分治法HashMap 2. 位图法Bitmap 30. 239. 滑动窗口最大值, 题解 双端队列 (1). # init deque and output: while deq and nums[i] &gt; nums[deq[-1]]: deq.pop() (2). # build output: for i in range(k, n): ✔️❎ HOT No. Question Flag Meeting Meeting Rooms 系列 easy 252 Meeting Rooms I, Sort, right = intervals[0][-1] for : if x&lt;right: return False ❎ heapq 253 Meeting Rooms II , heapq 中的 free_rooms 代表房间个数 intervals.sort(key= lambda x: x[0]) heapq.heappush(free_rooms, intervals[0][1]), for i in intervals[1:] re len(free_rms) ❎ Array 指针, 冒泡 75 Sort Colors, 2遍 单指针固定增加, if nums[i] == 0: nums[i], nums[p] = nums[p], nums[i] ❎ Array Sort idea, 模拟 621 任务调度器， 桶思想 + 模拟计算 两个相同种类的任务间必须有长度为整数 n 的冷却时间for i in set(tasks):tnum.append(tasks.count(i)) maxt=max(tnum) ❎ Array 搜索旋转排序数组 nums = [4,5,6,7,0,1,2], target = 0 , 双if 判断位置 Array 剑指 Offer 11. 旋转数组的最小数字 33. 搜索旋转排序数组, nums[0] &lt;= nums[mid] if nums[0] &lt;= target &lt; nums[mid]: while l &lt;= r: mid = (l + r) // 2 if nums[mid] == target: return mid ❎ Array 单调性有关 ✔️ 插空法 406. 根据身高重建队列 Queue Reconstruction by Height people.sort(key=lambda x:(-x[0], x[1])), 插空法, ans[p[1],p[1]]=[p] , tmp[:] ❎ Stock 股票买卖系列 121. 买卖股票的最佳时机 , inf = int(1e9), int max = sys.maxsize maxprofit = max(price - minprice, maxprofit) ❎ 122. 买卖股票的最佳时机 II, 贪心简单 &amp; DP 分状态讨论 dp[i][0] = max(dp[i - 1][0], dp[i - 1][1] + prices[i]); dp[i][1] = max(dp[i - 1][1], dp[i - 1][0] - prices[i]); 309. Best Time to Buy and Sell Stock with Coo, 题解：最佳买卖股票时机含冷冻期 1. f[i][0]: 手上持有股票的最大收益 2. f[i][1]: 手上不持有股票, 并且处于冷冻期的最大收益 3. f[i][2]: 手上不持有股票, 并且不处于冷冻期的最大收益 ❎ 0-1 背包 416 分割等和子集, 0-1背包 变体 ✔️ DP Tree DP, 偷不偷 337. House Robber III， 偷不偷 ✔️ 二维DP 二维格子 DP 221. Maximal Square 最大的正方形 if i == 0 or j == 0: dp[i][j] = 1 边界 dp = [[0] * columns for _ in range(rows)] dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1 一维DP 子序列 300. 最长上升子序列, dp[0]=1; j in range(i): dp[i] = max(dp[i], dp[j] + 1) ❎ 152. Maximum Product Subarray - 乘积最大的连续子数组, pre_max ,pre_min,num dp[i] = max(nums[i] * pre_max, nums[i] * pre_min, nums[i]) ❎ DFS 二维格子 79. Word Search, 单词搜索 for for if DFS 回溯, visited = set() visited.add((i,j)), visited.remove((i,j)) ❎ 200. Number of Islands, if grid[r][c] == &quot;1&quot;: num_islands += 1 ❎ DFS 全排列 39. 组合总和, [2,3,6,7] = [7], [2,2,3]， DFS 树状图 dfs(candidates, index, path + [candidates[index]], target - candidates[index]) ❎ 78. Subsets 子集, 放不放 dfs(ix=0)， 搜索+回溯 ❎ Bool DP 139. Word Break ， 好题目，3种解法, for i in n: for j in (i+1,n+1) if(dp[i] and (s[i:j] in wordDict)): dp[j]=True ❎ Tree 543. Diameter of Binary Tree, height = return max(L, R) + 1 ❎ stack 单调stack - 后进先出 739. Daily Temperatures 每日温度， while stack and temperature &gt; T[stack[-1]]: ✔️❎ heapq 堆 215. Kth Largest Element in an Array ❎ 贪心 维护最大距离 55 跳跃游戏， 贪心 索引&amp;位置 rightmost = max(rightmost, i + nums[i]) ❎ Linked LinkedList 234. Palindrome Linked List, 巧妙递归+front_point or vals == vals[::-1] ✔️❎ 114. Flatten Binary Tree to Linked List pre_list = list(), for i in pre_list: prev.left=None, prev.right=curr ❎ Tree 538 Convert BST to Greater Tree, 反中序遍, def dfs(root: TreeNode): nonlocal total ❎ Sliding Window 前缀和哈希优化 560. 和为K的子数组 num_times = collections.defaultdict(int), num_times[0]=1 cur_sum - k in nums_times, res += num_times[cur_sum - k] ❎ HOT100 No. Question Flag (1). binary-search good 15. 3Sum == TwoSum， nums.sort(), for for while second &lt; third, sec_ix &amp; thd_ix ❎ Array 283. Move Zeroes， 冒泡思想 ❎ 48. Rotate Image, n*n matrix, 上三角【转置+reverse()】, matrix[i].reverse() ✔️❎ (2). Dynamic programming, DP 53. Maximum Subarray， 连续的最大子序和 ❎ 64. Minimum Path Sum 二维格子的最小路径和 ， 格子 DP（向左和向下走） ❎ 198 打家劫舍 , dp = [0] * size, dp[0], dp[1], dp[i]=max(dp[i - 2] + nums[i], dp[i - 1]) ❎ (3). 模拟 31. Next Permutation == 8.5 下一个更大元素 III ❎ (4). DFS / BFS / Tree / Stack 56. Merge Intervals ， Sort+遍历, 替换结果 intervals.sort(key=lambda x: x[0]) merged[-1][1] = max(merged[-1][1], interval[1]) ❎ 21. Merge Two Sorted Lists ❎ 卡特兰 96. Unique Binary Search Trees , 2(2n+1)/n+1 ❎ (6). LinkedList 142. Linked List Cycle II， 转为环形链表II-龟兔判圈 ❎ 287. Find the Duplicate Number， 转为环形链表II-龟兔判圈 ❎ 匪夷 148. Sort List ✔️❎ 240 搜索二维矩阵 II , while col &lt; width and row &gt;= 0: ❎ 226 翻转二叉树 root.left, root.right = root.right, root.left ❎ 617 合并二叉树, new_root = TreeNode(t1.val + t2.val) ❎ 337. House Robber III 偷,不偷 题解 我们使用一个大小为 2 的数组来表示 int[] res = new int[2] 0 代表不偷，1 代表偷 任何一个节点能偷到的最大钱的状态可以定义为 当前节点选择不偷：当前节点能偷到的最大钱数 = 左孩子能偷到的钱 + 右孩子能偷到的钱 当前节点选择偷：当前节点能偷到的最大钱数 = 左孩子选择自己不偷时能得到的钱 + 右孩子选择不偷时能得到的钱 + 当前节点的钱数 1234567891011121314class Solution: def rob(self, root: TreeNode) -&gt; int: def _rob(root): if not root: return 0, 0 # 偷，不偷 left = _rob(root.left) right = _rob(root.right) # 偷当前节点, 则左右子树都不能偷 v1 = root.val + left[1] + right[1] # 不偷当前节点, 则取左右子树中最大的值 v2 = max(left) + max(right) return v1, v2 return max(_rob(root)) 621 任务调度器， leastInterval 123456class Solution: def leastInterval(self, tasks: List[str], n: int) -&gt; int: tnum=[] for i in set(tasks):tnum.append(tasks.count(i)) maxt=max(tnum) return max((n+1)*(maxt-1)+tnum.count(maxt),len(tasks)) 最大数 12345678class LargerNumKey(str): def __lt__(x, y): return x+y &gt; y+x class Solution: def largestNumber(self, nums): largest_num = &#x27;&#x27;.join(sorted(map(str, nums), key=LargerNumKey)) return &#x27;0&#x27; if largest_num[0] == &#x27;0&#x27; else largest_num Review shop No. Question Flag (1). binary-search 179. 最大数, sorted(iter, key=your_sort_class, __lt__) ❎ 1.1 二分查找, while l &lt;= r ❎ ✔️ 1.2 在排序数组中查找元素的第一个和最后一个位置, def binSearch(nums, t, flag), mid=r-1 or l+1, return r+1 or l-1 ❎ addition 162. 寻找峰值 nums[-1] = nums[n] = -∞ , l=mid+1, r=mid ❎ 278. First Bad Version , if isBadVersion(mid): right = mid - 1 ❎ hard 410. Split Array Largest Sum Input: nums = [7,2,5,10,8], m = 2. Output: 18 「使……最大值尽可能小」是二分搜索题目常见的问法 ❎ 逆向双指针 88. Merge Sorted Array nums1 = [1,2,3,0,0,0], nums2 = [2,5,6] ❎ 双指针 15. 3Sum， for for while , second_ix &amp; third_ix 两边夹 双指针 11. 盛最多水的容器 , 移动 l 和 r 较小的一方才可能增加 area ❎ hard, merge+index 315. Count of Smaller Numbers After Self hard (2). DFS / Stack 2.1 字符串解码 “3[a2[c]]” == “accacc”, stack == [(3, &quot;&quot;), (2,&quot;a&quot;)] ✔️❎ 215. 数组中的第K个最大元素 from heapq import heapify, heappush, heappop python中的heap是小根堆: heapify(hp) , heappop(hp), heappush(hp, v) (3). Digit, 模拟 3.1 回文数 [禁止整数转字符串]， 模拟 123321 -&gt; 2332 -&gt; 33 ❎ 470. 用 Rand7() 实现 Rand10() , 题解: 等概率多次调用 205. 同构字符串, all(s.index(s[i]) == t.index(t[i]) for i in range(len(s))) ❎ (4). DP good 4.1 栅栏涂色 dp[i] = dp[i-2]*(k-1) + dp[i-1]*(k-1) ✔️❎ 4.2 区域和检索 == 连续子数组最大和 ❎ goodfloat('inf')good 4.3 Coin Change [零钱兑换] dp[0] = 0, dp[x] = min(dp[x], dp[x - coin] + 1) F(i)=minj=0…n−1F(i−cj)+1F(i)= min_{j=0…n−1} F(i−c_j)+1F(i)=minj=0…n−1​F(i−cj​)+1 dp = [float('inf')] * (amount + 1) 输入：coins = [1, 2, 5], amount = 11输出：3 解释：11 = 5 + 5 + 1 Tips: float(‘inf’) + 1 = inf ✔️❎ 279. 完全平方数, numSquares(n)=min(numSquares(n-k) + 1)∀k∈square 与 Coin Change 非常类似，但不完全 ✔️❎ 4.4 除自身以外数组的乘积 , [0]*len, range(len(nums)-2, -1, -1) ❎ hard 44. Wildcard Matching Input: s = “aa”, p = “*” Output: true , Input: s = “cb”, p = “?a” Output: false (5). hash 5.1 两数之和, enumerate hash[num] = i ❎ (6). linkedList - 6.1 相交链表 romantic ❎ - 6.2 环形链表 hash ❎ - 6.3 两数相加 I LinkNode 模拟 head = ListNode(0), cur = head, carry = 0 ❎ - 445. Add Two Numbers II, 链表求和, stack+post_p while s1 or s2 or carry != 0: ❎ - 6.4 复制带随机指针的链表 1. while, 2. while (random pointers) 3. while (ptr_old_list） ❎ ✔️✔️✔️✔️ 6.5 LRUCache class DLinkedNode(4), removeTail, moveToHead, addToHead, removeNode ✔️❎ - 6.6 删除链表的倒数第N个节点 ❎ 匪夷所思 6.7 排序链表, slow, fast = head, head.next, mid, slow.next=slow.next, None ✔️❎ - 面试题 02.05. 链表求和 I ❎ hard 25. Reverse Nodes in k-Group hard 23. Merge k Sorted Lists (7). stack - 7.1 有效的括号 if i == ')' and len(stack)&gt; 0 and stack[-1] == '(': stack.pop() ❎ - 402. 移掉K位数字 ❎ (8). string reversed 8.1 字符串相加 给定两个字符串形式的非负整数 num1 和num2 ，计算它们的和 num1 = &quot;&quot;.join(list(reversed(num1))), num1 = num1 + (&quot;0&quot; * diff1) num2 = num2 + (&quot;0&quot; * diff2) ✔️️❎ - 8.2 比较版本号 , split then compare … ❎ - 8.3 字符串解码 ❎ - 8.4 无重复字符的最长子串 sliding window, [l, r], occ=set(), occ.remove(elem) ✔️❎️ - 8.5 下一个更大元素 III ， 模拟复杂 见题解 1,5,8,4,7,6,5,3,1 =&gt; decreasing elem found 1,5,8,4(i-1),7,6,5(j),3,1 (found j, just larger a[i-1]) =&gt; 1,5,8,5,7,6,4,3,1 =&gt; 1,5,8,5,1,3,4,6,7 (reverse these elements) ✔️️❎ - 8.6 全排列 def backtrack(first=0): for i in range(first, n): swap(nums[first], nums[i]) ❎ (9). tree - 9.1 从前序与中序遍历序列构造二叉树 i = inorder.index(preorder[0]) ❎ - 9.2 二叉树的中序遍历 (非递归) while while ❎ - 9.3 二叉树的右视图 ❎ hard 124. Binary Tree Maximum Path Sum sales-person 销售员 12345678910111213SELECT s.nameFROM salesperson sWHERE s.sales_id NOT IN (SELECT o.sales_id FROM orders o LEFT JOIN company c ON o.com_id = c.com_id WHERE c.name = &#x27;RED&#x27;) 3sum 123456789101112131415161718192021222324252627282930class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: n = len(nums) nums.sort() ans = list() # 枚举 a for first in range(n): # 需要和上一次枚举的数不相同 if first &gt; 0 and nums[first] == nums[first - 1]: continue # c 对应的指针初始指向数组的最右端 third = n - 1 target = -nums[first] # 枚举 b for second in range(first + 1, n): # 需要和上一次枚举的数不相同 if second &gt; first + 1 and nums[second] == nums[second - 1]: continue # 需要保证 b 的指针在 c 的指针的左侧 while second &lt; third and nums[second] + nums[third] &gt; target: third -= 1 # 如果指针重合，随着 b 后续的增加 # 就不会有满足 a+b+c=0 并且 b&lt;c 的 c 了，可以退出循环 if second == third: break if nums[second] + nums[third] == target: ans.append([nums[first], nums[second], nums[third]]) return ans 402. 移掉K位数字 1234567891011121314151617class Solution: def removeKdigits(self, num: str, k: int) -&gt; str: numStack = [] # 构建单调递增的数字串 for digit in num: while k and numStack and numStack[-1] &gt; digit: numStack.pop() k -= 1 numStack.append(digit) # 如果 K &gt; 0，删除末尾的 K 个字符 finalStack = numStack[:-k] if k else numStack # 抹去前导零 return &quot;&quot;.join(finalStack).lstrip(&#x27;0&#x27;) or &quot;0&quot; 241. 为运算表达式设计优先级 解题思路 对于一个形如 x op y（op 为运算符，x 和 y 为数） 的算式而言，它的结果组合取决于 x 和 y 的结果组合数，而 x 和 y 又可以写成形如 x op y 的算式。 因此，该问题的子问题就是 x op y 中的 x 和 y：以运算符分隔的左右两侧算式解。 然后我们来进行 分治算法三步走： 分解：按运算符分成左右两部分，分别求解 解决：实现一个递归函数，输入算式，返回算式解 合并：根据运算符合并左右两部分的解，得出最终解 123456789101112131415161718192021222324class Solution: def diffWaysToCompute(self, input: str) -&gt; List[int]: # 如果只有数字，直接返回 if input.isdigit(): return [int(input)] res = [] for i, char in enumerate(input): if char in [&#x27;+&#x27;, &#x27;-&#x27;, &#x27;*&#x27;]: # 1.分解：遇到运算符，计算左右两侧的结果集 # 2.解决：diffWaysToCompute 递归函数求出子问题的解 left = self.diffWaysToCompute(input[:i]) right = self.diffWaysToCompute(input[i+1:]) # 3.合并：根据运算符合并子问题的解 for l in left: for r in right: if char == &#x27;+&#x27;: res.append(l + r) elif char == &#x27;-&#x27;: res.append(l - r) else: res.append(l * r) return res 279. 完全平方数 Recursion 1234567891011121314151617181920class Solution(object): def numSquares(self, n): square_nums = [i**2 for i in range(1, int(math.sqrt(n))+1)] def minNumSquares(k): &quot;&quot;&quot; recursive solution &quot;&quot;&quot; # bottom cases: find a square number if k in square_nums: return 1 min_num = float(&#x27;inf&#x27;) # Find the minimal value among all possible solutions for square in square_nums: if k &lt; square: break new_num = minNumSquares(k-square) + 1 min_num = min(min_num, new_num) return min_num return minNumSquares(n) DP 1234567891011121314151617181920class Solution(object): def numSquares(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; square_nums = [i**2 for i in range(0, int(math.sqrt(n))+1)] dp = [float(&#x27;inf&#x27;)] * (n+1) # bottom case dp[0] = 0 for i in range(1, n+1): for square in square_nums: if i &lt; square: break dp[i] = min(dp[i], dp[i-square] + 1) return dp[-1] 96. 不同的二叉搜索树 12345678910class Solution: def numTrees(self, n): G = [0]*(n+1) G[0], G[1] = 1, 1 for i in range(2, n+1): for j in range(1, i+1): G[i] += G[j-1] * G[i-j] return G[n] 123456789101112131415class Solution: def decodeString(self, s: str) -&gt; str: stack, res, multi = [], &quot;&quot;, 0 for c in s: if c == &#x27;[&#x27;: stack.append([multi, res]) res, multi = &quot;&quot;, 0 elif c == &#x27;]&#x27;: cur_multi, last_res = stack.pop() res = last_res + cur_multi * res elif &#x27;0&#x27; &lt;= c &lt;= &#x27;9&#x27;: multi = multi * 10 + int(c) else: res += c return res LRUCache 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class DLinkedNode: def __init__(self, key=0, value=0): self.key = key self.value = value self.prev = None self.next = None class LRUCache: def __init__(self, capacity: int): self.head = DLinkedNode() self.tail = DLinkedNode() self.head.next = self.tail self.tail.prev = self.head self.capacity = capacity self.size = 0 self.cache = &#123;&#125; def get(self, key: int) -&gt; int: if key not in self.cache: return -1 # 如果 key 存在，先通过哈希表定位，再移到头部 node = self.cache[key] self.moveToHead(node) return node.value def put(self, key: int, value: int) -&gt; None: if key not in self.cache: # 如果 key 不存在，创建一个新的节点 node = DLinkedNode(key, value) # 添加进哈希表 self.cache[key] = node # 添加至双向链表的头部 self.addToHead(node) self.size += 1 if self.size &gt; self.capacity: # 如果超出容量，删除双向链表的尾部节点 removed = self.removeTail() # 删除哈希表中对应的项 self.cache.pop(removed.key) self.size -= 1 else: # 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部 node = self.cache[key] node.value = value self.moveToHead(node) def addToHead(self, node): node.prev = self.head node.next = self.head.next self.head.next.prev = node self.head.next = node def removeNode(self, node): node.prev.next = node.next node.next.prev = node.prev def moveToHead(self, node): self.removeNode(node) self.addToHead(node) def removeTail(self): node = self.tail.prev self.removeNode(node) return node# Your LRUCache object will be instantiated and called as such:# obj = LRUCache(capacity)# param_1 = obj.get(key)# obj.put(key,value) very good sortList: 1234567891011121314151617181920212223242526272829303132# -*- coding: utf-8 -*-# Definition for singly-linked list.class ListNode: def __init__(self, val=0, next=None): self.val = val self.next = next# 在 O(n log n) 时间复杂度和常数级空间复杂度下，对链表进行排序。## 示例 1:## 输入: 4-&gt;2-&gt;1-&gt;3# 输出: 1-&gt;2-&gt;3-&gt;4class Solution: def sortList(self, head: ListNode) -&gt; ListNode: if not head or not head.next: return head # termination. # cut the LinkedList at the mid index. slow, fast = head, head.next while fast and fast.next: fast, slow = fast.next.next, slow.next mid, slow.next = slow.next, None # save and cut. # recursive for cutting. left, right = self.sortList(head), self.sortList(mid) # merge `left` and `right` linked list and return it. h = res = ListNode(0) while left and right: if left.val &lt; right.val: h.next, left = left, left.next else: h.next, right = right, right.next h = h.next h.next = left if left else right return res.next 二叉树的中序遍历 1234567891011121314151617181920212223class Solution: def __init__(self): self.res = [] def inorderTraversal(self, root: TreeNode) -&gt; List[int]: if not root: return [] stack = list() while stack or root: while root: stack.append(root) root = root.left root = stack.pop() self.res.append(root.val) root = root.right return self.res 剑指,Table No. Question Flag easy (1). Tree 1.1 平衡二叉树 abs(maxHigh(root.left) - maxHigh(root.right)) &lt;= 1 and self.isBalanced(root.left) and self.isBalanced(root.right) ❎ 1.2 对称的二叉树 ❎ 1.3 二叉树的镜像： root.left = self.mirrorTree(root.right) swap后+递归 ❎ 1.4 二叉搜索树的第k大节点 [中序遍历 倒序, 右-中-左] ✔️❎ good 1.5 (两个节点)二叉树的最近公共祖先 [Recursion] 后序遍历+路径回溯 ✔️❎ good 1.6 (两个节点)二叉搜索树的最近公共祖先 Recursion + 剪枝 ✔️❎ good 1.7 二叉树中和为某一值的路径 递归回溯 ✔❎️ 1.8 二叉搜索树的后序遍历序列 ❎ 1.9 二叉搜索树与双向链表 additional 求二叉树第K层的节点个数 [Recursion] ，root != None and k==1，返回1 f(root.left, k-1) + f(root.right, k-1) ❎ additional 求二叉树第K层的叶子节点个数 [Recursion] if(k==1 and root.left and root.right is null) return 1; ✔️❎ (2). Stack 394. 字符串解码 [a]2[bc] ❎ 28. 包含min函数的栈 ❎ 29. 最小的k个数【堆排的逆向】 heapq.heappop(hp),heapq.heappush(hp, -arr[i]) ✔️❎ 36. 滑动窗口的最大值 (同理于包含 min 函数的栈) deque.popleft(),双端队列+单调 ✔️❎ 59 II. 队列的最大值 , 维护个单调的deque import queue, queue.deque(), queue.Queue(), deq[0], deq[-1] ✔️❎ (3). linkedList 7. 从尾到头打印链表： reversePrint(head.next) + [head.val] ❎ 8. 反转链表 (循环版 双指针) ❎ 10. 合并两个排序的链表 [Recursion] p.next = self.mergeTwoLists(l1.next, l2) ❎ addition 旋转单链表 (F1. 环 F2. 走n-k%n 断开) 举例： 给定 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;NULL, K=3 则4-&gt;5-&gt;6-&gt;1-&gt;2-&gt;3-&gt;NULL ❎ addition 92. 翻转部分单链表 reverse(head: ListNode, tail: ListNode) 举例：1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;null, from = 2, to = 4 结果：1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;null ❎ addition 链表划分, 描述： 给定一个单链表和数值x，划分链表使得小于x的节点排在大于等于x的节点之前 ❎ addition 82. 删除排序链表中的重复元素 II 链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5. ❎ addition 输入：(7 -&gt; 1 -&gt; 6) + (5 -&gt; 9 -&gt; 2)，即617 + 295 输出：2 -&gt; 1 -&gt; 9，即912 (4). DP 31. n个骰子的点数 dp[i][j] ，表示投掷完 i 枚骰子后，点数 j 的出现次数 ✔️ Summary 20 dynamic programming (4.1) DP表示状态 easy 1. climbing-stairs ， 新建{}or[] ,滚动数组 2. 连续子数组的最大和 ❎ addition 63. 不同路径 II, store = [[0]*n for i in range(m)] 二维初始化 ❎ addition Edit Distance/编辑距离【word1 转换成 word2】 1. dp = [ [0] * (m + 1) for _ in range(n + 1)] 2. dp[i][j] = min(A,B,C) ✔️❎ addition 5. Longest Palindromic Substring/最长回文子串 1. 枚举子串的长度 l+1,从小问题到大问题 2. 枚举子串的起始位置 i, j=i+l 子串结束位置, dp[i][j] = (dp[i+1][j-1] and s[i]==s[j]) ✔️❎ good 把数字翻译成字符串 Fib ✔️❎ addition Leetcode 64. Minimum Path Sum, 最小路径和 grid[i][j] = min(grid[i - 1][j], grid[i][j - 1]) + grid[i][j] ❎ addition 115. Distinct Subsequences I Hard addition 940. 不同的子序列 II Hard addition Interleaving String/交错字符串 Hard (5). DFS / BFS 66. 矩阵中的路径 , 经典好题: 深搜+回溯 def dfs(i, j, k): ✔️❎ 61. 机器人的运动范围 bfs good from queue import Queue, q.get() q.pup() ✔️❎ (6). sliding Window 65. 最长不含重复字符的子字符串 滑动窗口 ✔️❎ 14. 和为s的连续正数序列 [sliding window] input：target = 9 output：[[2,3,4],[4,5]] ✔️❎ (7). 模拟 21. 圆圈中最后剩下的数字 1. 当数到最后一个结点不足m个时，需要跳到第一个结点继续数 2. 每轮都是上一轮被删结点的下一个结点开始数 m 个 3. 寻找 f(n,m) 与 f(n-1,m) 关系 4. A： f(n,m)=(m+x)%n 5. Python 深度不够手动设置 sys.setrecursionlimit(100000) 东大 Lucien 题解,讲得最清楚的那个。官方讲解有误 ✔️❎ 35. 顺时针打印矩阵 left, right, top, bottom = 0, columns - 1, 0, rows - 1 ✔️❎ 56. 把数组排成最小的数, sorted vs sort, strs.sort(key=cmp_to_key(sort_rule)) ✔️❎ 70. 把字符串转换成整数 int_max, int_min, bndry = 231-1, -231, 2**31//10 bndry=2147483647//10=214748364 ，则以下两种情况越界 res &gt; bndry or res == bndry and c &gt;‘7’ ✔️❎ medium 37 0～n-1中缺失的数字 ❎ 42 求1+2+…+n ❎ 43 数组中数字出现的次数 so hard 44 复杂链表的复制 ❎ 45 数组中数字出现的次数 ❎ 46 重建二叉树 ❎ 47 礼物的最大价值 f = [len(grid[0]) * [0]] * len(grid) ❎ 48 从上到下打印二叉树 III queue.append([root, 0]) ❎ 49 丑数 n2, n3, n5 = dp[a] * 2, dp[b] * 3, dp[c] * 5 ❎ 50 二叉搜索树与双向链表 ✔️❎ 51 股票的最大利润 （买卖一次） cost, profit = float(&quot;+inf&quot;), 0 for price in prices: cost, profit = min(cost, price), max(profit, price - cost) 54 构建乘积数组 ❎ 55 二叉树中和为某一值的路径 ✔️❎ 57 剪绳子 (1) n &lt; 4 (2) n == 4 (3) n &gt; 4, 多个 == 3 段 ❎ 58 字符串的排列 c = list(s) res = [] def dfs(x): ❎ 59 把数字翻译成字符串 f[i] = f[i-1] + f[i-2] 同 打家劫舍 ❎ 60 二叉搜索树的后序遍历序列 def recur(i, j): ❎ 68 数值的整数次方 （1）当 n 为偶数 （2）当 n 为奇数 ❎ 71 表示数值的字符串： 确定有限状态自动机 面试题20. 表示数值的字符串（有限状态自动机，清晰图解） hard 72 数据流中的中位数 73 序列化二叉树 64 1～n整数中1出现的次数 74 数组中的逆序对 75 正则表达式匹配 No. Pass Question Flag pass_easy 1 左旋转字符串 ❎ 2 链表中倒数第k个节点 ❎ 3 二叉树的深度 ❎ 5 打印从1到最大的n位数： sum = 10 ** n ❎ 6 替换空格 ❎ 11 二进制中1的个数 [n = n &amp; (n-1)] ❎ 12 用两个栈实现队列 ❎ 16 从上到下打印二叉树II queue.append([root, 0]) 或 for _ in range(queue_size) ❎ 17 数组中出现次数超过一半的数字 ❎ 18 数组中重复的数字 set() ❎ 19 和为s的两个数字 [sliding window] ❎ 20 调整数组顺序使奇数位于偶数前面 ❎ 22 两个链表的第一个公共节点 ❎ 23 第一个只出现一次的字符: Python 3.6 后，默认字典就是有序的，无需用 OrderedDict() ❎ 24 连续子数组的最大和 dp[i] = dp[i-1] + nums[i] ❎ 25 删除链表的节点 pre, p ❎ 30 不用加减乘除做加法 add(a ^ b, (a &amp; b) &lt;&lt; 1) ❎ 32 在排序数组中查找数字I ❎ 33 旋转数组的最小数字 numbers[high] ❎ 34 扑克牌中的顺子 ma - mi &lt; 5 ❎ 38 翻转单词顺序 ❎ 39 青蛙跳台阶问题 ❎ 40 二维数组中的查找 ❎ 41 斐波那契数列 ❎ pass_medium 52 栈的压入、弹出序列 (+stack 辅助) ❎ 53 剑指 Offer 32 - III. 从上到下打印二叉树 III ❎ 63 树的子结构 ❎ 67 数字序列中某一位的数字 找规律, pass NG 69 剪绳子II Not Good, so pass. 1. Tree No. Question Flag 1.1 平衡二叉树 1.2 对称的二叉树 1.3 二叉树的镜像 1.4 二叉树的最近公共祖先 1.6 从上到下打印二叉树 II / III 1.7 二叉树中和为某一值的路径 1.8 二叉搜索树的后序遍历序列 1.9 二叉搜索树与双向链表 1234567891011121314151617class Solution: def treeToDoublyList(self, root: &#x27;Node&#x27;) -&gt; &#x27;Node&#x27;: def dfs(cur): if not cur: return dfs(cur.left) # 递归左子树 if self.pre: # 修改节点引用 self.pre.right, cur.left = cur, self.pre else: # 记录头节点 self.head = cur self.pre = cur # 保存 cur dfs(cur.right) # 递归右子树 if not root: return self.pre = None dfs(root) self.head.left, self.pre.right = self.pre, self.head return self.head 题解链接 1.0 构造二叉树 12345678910111213141516171819202122232425262728293031323334353637class Node: def __init__(self, x): self.val = x self.left = None self.right = Nonedef creatTree(vals): nodes = [] for i in range(len(vals)): cur_val = vals[i] if cur_val is not None: cur_node = Node(cur_val) else: cur_node = None nodes.append(cur_node) if i &gt; 0: # 0, 1-1/2, 2-1/2 par_id = (i - 1) // 2 if (i - 1) % 2 == 0: nodes[par_id].left = cur_node else: nodes[par_id].right = cur_node return nodes[0]def pre_out(root): if not root: return None print(root.val) pre_out(root.left) pre_out(root.right)if __name__ == &#x27;__main__&#x27;: vals = [3,5,1,6,2,0,8,None,None,7,4] root = creatTree(vals) pre_out(root) 1.1 平衡二叉树 123456789101112class Solution: def isBalanced(self, root: TreeNode) -&gt; bool: def maxHigh(root): if root == None: return 0 return max(maxHigh(root.left), maxHigh(root.right)) + 1 if root == None: return True return abs(maxHigh(root.left) - maxHigh(root.right)) &lt;= 1 and self.isBalanced(root.left) and self.isBalanced(root.right) 1.2 对称的二叉树 123456789101112def isSymmetricHelper(left: TreeNode, right: TreeNode): if left == None and right == None: return True if left == None or right == None: return False if left.val != right.val: return False return isSymmetricHelper(left.left, right.right) and isSymmetricHelper(left.right, right.left)class Solution: def isSymmetric(self, root: TreeNode) -&gt; bool: return root == None or isSymmetricHelper(root.left, root.right) 1.3 二叉树的镜像 12345678910class Solution: def mirrorTree(self, root: TreeNode) -&gt; TreeNode: if root == None: return root node = root.left root.left = self.mirrorTree(root.right) root.right = self.mirrorTree(node) return root 1.4 二叉树的最近公共祖先 1234567891011121314151617181920# 1. 从根节点开始遍历树# 2. 如果节点 p 和节点 q 都在右子树上，那么以右孩子为根节点继续 1 的操作# 3. 如果节点 p 和节点 q 都在左子树上，那么以左孩子为根节点继续 1 的操作# 4. 如果条件 2 和条件 3 都不成立，这就意味着我们已经找到节 p 和节点 q 的 LCA 了class Solution: def lowestCommonAncestor(self, root: TreeNode, p: TreeNode, q: TreeNode) -&gt; TreeNode: # 当越过叶节点，则直接返回 null # 当 rootroot 等于 p, q， 则直接返回 root if root == None or root == p or root == q: return root left = self.lowestCommonAncestor(root.left, p, q) right = self.lowestCommonAncestor(root.right, p, q) if not left and not right: return None if not left: return right if not right: return left return root 二叉搜索树的最近公共祖先 123456789101112131415161718192021222324252627282930class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None# 算法:# 1. 从根节点开始遍历树# 2. 如果节点 p 和节点 q 都在右子树上，那么以右孩子为根节点继续 1 的操作# 3. 如果节点 p 和节点 q 都在左子树上，那么以左孩子为根节点继续 1 的操作# 4. 如果条件 2 和条件 3 都不成立，这就意味着我们已经找到节 p 和节点 q 的 LCA 了class Solution: def lowestCommonAncestor(self, root: TreeNode, p: TreeNode, q: TreeNode) -&gt; TreeNode: # Value of current node or parent node. parent_val = root.val # Value of p p_val = p.val # Value of q q_val = q.val # If both p and q are greater than parent if p_val &gt; parent_val and q_val &gt; parent_val: return self.lowestCommonAncestor(root.right, p, q) # If both p and q are lesser than parent elif p_val &lt; parent_val and q_val &lt; parent_val: return self.lowestCommonAncestor(root.left, p, q) # We have found the split point, i.e. the LCA node. else: return root 1.6 从上到下打印二叉树 II / III 从上到下打印二叉树 II 12345678910111213141516171819from collections import dequeclass Solution: def levelOrder(self, root: TreeNode) -&gt; List[int]: if not root: return [] queue = collections.deque() queue.append(root) res = [] while queue: size = len(queue) for _ in range(size): cur = queue.popleft() if not cur: continue res.append(cur.val) queue.append(cur.left) queue.append(cur.right) return res 从上到下打印二叉树 III 12345678910111213141516171819202122232425262728class Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if not root: return [] queue = deque() queue.append([root, 0]) res = [] tmp_dict = dict() while queue: cur, level = queue.popleft() if tmp_dict.get(level) is not None: tmp_dict[level].append(cur.val) else: tmp_dict[level] = [cur.val] if cur.left: queue.append([cur.left, level + 1]) if cur.right: queue.append([cur.right, level + 1]) for ix in range(len(tmp_dict)): if ix % 2 == 1: res.append(tmp_dict[ix][::-1]) else: res.append(tmp_dict[ix]) return res 1.7 二叉树中和为某一值的路径 123456789101112131415161718def pathSum(self, root: TreeNode, sum: int) -&gt; List[List[int]]: if not root: return [] self.path.append(root.val) sum = sum - root.val if sum == 0 and root.left is None and root.right is None: self.res.append(list(self.path)) if root.left: self.pathSum(root.left, sum) if root.right: self.pathSum(root.right, sum) self.path.pop() return self.res 1.8 二叉搜索树的后序遍历序列 12345678910111213141516171819class Solution: def verifyPostorder(self, postorder: [int]) -&gt; bool: def recur(i, j): if i &gt;= j: return True p = i while postorder[p] &lt; postorder[j]: p += 1 m = p while postorder[p] &gt; postorder[j]: p += 1 return p == j and recur(i, m - 1) and recur(m, j - 1) return recur(0, len(postorder) - 1) 1.9 二叉搜索树与双向链表 123456789101112131415161718192021class Solution: def treeToDoublyList(self, root: &#x27;Node&#x27;) -&gt; &#x27;Node&#x27;: def dfs(cur): if not cur: return dfs(cur.left) # 递归左子树 if self.pre: # 修改节点引用 self.pre.right = cur cur.left = self.pre else: # 记录头节点 self.head = cur self.pre = cur # 保存 cur dfs(cur.right) # 递归右子树 if not root: return self.pre = None dfs(root) self.head.left = self.pre self.pre.right = self.head return self.head 2. LinkedList 2.1 复杂链表的复制 12345678910&quot;&quot;&quot;# Definition for a Node.class Node: def __init__(self, x: int, next: &#x27;Node&#x27; = None, random: &#x27;Node&#x27; = None): self.val = int(x) self.next = next self.random = random&quot;&quot;&quot;class Solution: def copyRandomList(self, head: &#x27;Node&#x27;) -&gt; &#x27;Node&#x27;: 3. String 字符串全排列 permutation 1234567891011121314151617181920212223242526# 输入：s = &quot;abc&quot;# 输出：[&quot;abc&quot;,&quot;acb&quot;,&quot;bac&quot;,&quot;bca&quot;,&quot;cab&quot;,&quot;cba&quot;]from typing import Listclass Solution: def permutation(self, s: str) -&gt; List[str]: c = list(s) res = [] def dfs(x): if x == len(c) - 1: res.append(&#x27;&#x27;.join(c)) # 添加排列方案 return dic = set() for i in range(x, len(c)): # if c[i] in dic: continue # 重复，因此剪枝 # dic.add(c[i]) c[i], c[x] = c[x], c[i] # 交换，将 c[i] 固定在第 x 位 dfs(x + 1) # 开启固定第 x + 1 位字符 c[i], c[x] = c[x], c[i] # 恢复交换 dfs(0) return res 4. Array &amp; Sort 4.1 最小的k个数 123456789101112131415161718192021import heapqfrom typing import Listclass Solution: def getLeastNumbers(self, arr: List[int], k: int) -&gt; List[int]: if k == 0: return list() hp = [-x for x in arr[:k]] heapq.heapify(hp) for i in range(k, len(arr)): if -hp[0] &gt; arr[i]: heapq.heappop(hp) heapq.heappush(hp, -arr[i]) ans = [-x for x in hp] return ans 4.2 n个骰子的点数 1234567891011121314151617181920212223# 把n个骰子扔在地上，所有骰子朝上一面的点数之和为s。输入n，打印出s的所有可能的值出现的概率。## 你需要用一个浮点数数组返回答案，其中第 i 个元素代表这 n 个骰子所能掷出的点数集合中第 i 小的那个的概率。## 输入: 1# 输出: [0.16667,0.16667,0.16667,0.16667,0.16667,0.16667]## 输入: 2# 输出: [0.02778,0.05556,0.08333,0.11111,0.13889,0.16667,0.13889,0.11111,0.08333,0.05556,0.02778]# 通过题目我们知道一共投掷 n 枚骰子，那最后一个阶段很显然就是：当投掷完 n 枚骰子后，各个点数出现的次数。## 注意，这里的点数指的是前 n 枚骰子的点数和，而不是第 n 枚骰子的点数，下文同理。## 找出了最后一个阶段，那状态表示就简单了。## 首先用数组的第一维来表示阶段，也就是投掷完了几枚骰子。# 然后用第二维来表示投掷完这些骰子后，可能出现的点数。# 数组的值就表示，该阶段各个点数出现的次数。# 所以状态表示就是这样的：dp[i][j] ，表示投掷完 i 枚骰子后，点数 j 的出现次数。# dp[i][j] += dp[i-1][j-cur]; 1234567891011121314151617181920212223242526272829303132from typing import Listclass Solution: def twoSum(self, n: int) -&gt; List[float]: dp = [[0] * (6 * n + 1) for _ in range(11 + 1)] # 索引0不取，后面取到最大索引6*n for i in range(1, 7): # init dp[1][i] = 1 for i in range(2, n + 1): # 从第二轮抛掷开始算 for j in range(2, 6 * i + 1): # 第二轮抛掷最小和为2，从大到小更新对应的抛掷次数 # dp[j] = 0 # 每次投掷要从0更新dp[j]大小，点数和出现的次数要重新计算 for cur in range(1, 7): # 每次抛掷的点数 if j - cur &lt;= 0: break if j - cur &gt; (i - 1) * 6: continue dp[i][j] += dp[i - 1][j - cur] # 根据上一轮来更新当前轮数据 print(f&#x27;&#123;i&#125;, &#123;j&#125;, ==== &#123;i-1&#125; &#123;j-cur&#125;&#x27;) sum_ = 6 ** n res = [] for i in range(n, 6 * n + 1): res.append(dp[n][i] * 1.0 / sum_) return res 4.3 顺时针打印矩阵 1234567891011121314151617181920212223242526272829303132333435363738# 输入：matrix = [# [1,2,3,4],# [5,6,7,8],# [9,10,11,12]# ]# 输出：[# 1,2,3,4,# 8,12,11,10,# 9,5,6,7# ]from typing import Listclass Solution: def spiralOrder(self, matrix: List[List[int]]) -&gt; List[int]: if not matrix or not matrix[0]: return list() rows, columns = len(matrix), len(matrix[0]) order = list() left, right, top, bottom = 0, columns - 1, 0, rows - 1 while left &lt;= right and top &lt;= bottom: for column in range(left, right + 1): order.append(matrix[top][column]) for row in range(top + 1, bottom + 1): order.append(matrix[row][right]) if left &lt; right and top &lt; bottom: for column in range(right - 1, left, -1): order.append(matrix[bottom][column]) for row in range(bottom, top, -1): order.append(matrix[row][left]) left, right, top, bottom = left + 1, right - 1, top + 1, bottom - 1 return order 4.4 把数组排成最小的数 123456789101112131415161718from functools import cmp_to_keyfrom typing import Listclass Solution: def minNumber(self, nums: List[int]) -&gt; str: def sort_rule(x, y): a, b = x + y, y + x if a &gt; b: return 1 elif a &lt; b: return -1 else: return 0 strs = [str(num) for num in nums] strs.sort(key=cmp_to_key(sort_rule)) return &#x27;&#x27;.join(strs) 4.5 把字符串转换成整数 1234567891011121314class Solution: def strToInt(self, str: str) -&gt; int: str = str.strip() # 删除首尾空格 if not str: return 0 # 字符串为空则直接返回 res, i, sign = 0, 1, 1 int_max, int_min, bndry = 2 ** 31 - 1, -2 ** 31, 2 ** 31 // 10 if str[0] == &#x27;-&#x27;: sign = -1 # 保存负号 elif str[0] != &#x27;+&#x27;: i = 0 # 若无符号位，则需从 i = 0 开始数字拼接 for c in str[i:]: if not &#x27;0&#x27; &lt;= c &lt;= &#x27;9&#x27; : break # 遇到非数字的字符则跳出 if res &gt; bndry or res == bndry and c &gt; &#x27;7&#x27;: return int_max if sign == 1 else int_min # 数字越界处理 res = 10 * res + int(c) # 数字拼接 return sign * res 4.7 数值的整数次方 (递归+2分) 12345678910class Solution: def myPow(self, x: float, n: int) -&gt; float: if x == 0: return 0 res = 1 if n &lt; 0: x, n = 1 / x, -n while n: if n &amp; 1: res *= x x *= x n &gt;&gt;= 1 return res 5. sliding window 剑指 Offer 59 - I. 滑动窗口的最大值 - (同理于包含 min 函数的栈) answ 12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-import collectionsfrom typing import List# 输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3# 输出: [3,3,5,5,6,7]# 解释:## 滑动窗口的位置 最大值# --------------- -----# [1 3 -1] -3 5 3 6 7 3# 1 [3 -1 -3] 5 3 6 7 3# 1 3 [-1 -3 5] 3 6 7 5# 1 3 -1 [-3 5 3] 6 7 5# 1 3 -1 -3 [5 3 6] 7 6# 1 3 -1 -3 5 [3 6 7] 7class Solution: def maxSlidingWindow(self, nums: List[int], k: int) -&gt; List[int]: if not nums or k == 0: return [] deque = collections.deque() for i in range(k): # 未形成窗口 while deque and deque[-1] &lt; nums[i]: deque.pop() deque.append(nums[i]) res = [deque[0]] for i in range(k, len(nums)): # 形成窗口后 #[0~k-1], [1~k], [2~k+1] if deque[0] == nums[i - k]: deque.popleft() while deque and deque[-1] &lt; nums[i]: deque.pop() deque.append(nums[i]) res.append(deque[0]) return res quickSort 1234567891011121314151617181920212223242526272829303132333435363738394041def quickSort(a, left, right): if left &lt; right: l, r, x = left, right, a[l] while True while l &lt; r and a[r] &gt;= x: r-- while l &lt; r and a[l] &lt;= x: l++ if l &gt;= r: break a[r], a[l] = a[l], a[r] a[left], a[l] = a[l], a[left] quickSort(a, left, l-1) quickSort(a, l+1, right)void mergeSort(int a[], int l, int r) &#123; // 8, 5, 4, 9, 2, 3, 6 if(l &gt;= r) return; // exit. int mid = (l+r) / 2; // overflow &lt;-&gt; l + (r-l)/2 mergeSort(a, l, mid); mergeSort(a, mid+1, r); int *arr = new int[r-l+1]; int k = 0; int i = l, j = mid + 1; while(i &lt;= mid &amp;&amp; j &lt;= r) &#123; if(a[i] &lt;= a[j]) &#123; arr[k++] = a[i++]; &#125; else &#123; arr[k++] = a[j++]; // ans += (mid-i+1); &#125; &#125; while(i &lt;= mid) arr[k++] = a[i++]; while(j &lt;= r) arr[k++] = a[j++]; for(int i = l; i &lt;= r; i++) &#123; a[i] = arr[i-l]; &#125; delete []arr;&#125; Reference 成长之路 0607offer 知乎： [Leetcode][动态规划]相关题目汇总/分析/总结 简书： 2019 算法面试相关(leetcode)–动态规划(Dynamic Programming) CSDN leetcode DP 刷完700多题后的首次总结：LeetCode应该怎么刷？ 小白一路走来，连续刷题三年，谈谈我的算法学习经验 codebunk.com 《程序员的算法趣题》-开坑记录"}],"posts":[{"title":"2021 Leetcode","slug":"leetcode/2021-leetcode","date":"2021-03-19T02:54:16.000Z","updated":"2021-06-20T07:23:53.773Z","comments":true,"path":"2021/03/19/leetcode/2021-leetcode/","link":"","permalink":"http://www.iequa.com/2021/03/19/leetcode/2021-leetcode/","excerpt":"","text":"1. binary-search 1.1 二分查找, while l &lt;= r 123456789101112131415161718class Solution: def search(self, nums: List[int], target: int) -&gt; int: if not nums: return -1 l, r = 0, len(nums) - 1 while l &lt;= r: mid = (r - l)//2 + l if nums[mid] &lt; target: l = mid + 1 elif nums[mid] &gt; target: r = mid - 1 else: return mid return -1 34. 在排序数组中查找元素的第一个和最后一个位置 123456789101112131415161718192021222324252627class Solution: def searchRange(self, nums: List[int], target: int) -&gt; List[int]: if not nums: return [-1, -1] def binSearch(nums, t, flag): l, r = 0, len(nums) - 1 while l &lt;= r: mid = (l + r) // 2 if nums[mid] &gt; t: r = mid - 1 elif nums[mid] &lt; t: l = mid + 1 else: if flag == &quot;L&quot;: r = mid - 1 else: l = mid + 1 if flag == &#x27;L&#x27; and r + 1 &lt; len(nums) and nums[r + 1] == t: return r + 1 if flag == &#x27;R&#x27; and l - 1 &gt;= 0 and nums[l - 1] == t: return l - 1 return -1 return [binSearch(nums=nums, t=target, flag=&#x27;L&#x27;), binSearch(nums=nums, t=target, flag=&#x27;R&#x27;)] 88. 合并两个有序数组 - 逆向双指针 123456789101112131415161718192021222324class Solution: def merge(self, A: List[int], m: int, B: List[int], n: int) -&gt; None: &quot;&quot;&quot; Do not return anything, modify A in-place instead. &quot;&quot;&quot; pa, pb = m-1, n-1 tail = m+n-1 while not (pa == -1 and pb == -1): if pa == -1: A[tail] = B[pb] pb -= 1 elif pb == -1: A[tail] = A[pa] pa -= 1 elif A[pa] &gt; B[pb]: A[tail] = A[pa] pa -= 1 else: A[tail] = B[pb] pb -= 1 tail -= 1 return A[:] 15. 3Sum - for for while , second_ix &amp; third_ix 两边夹 123456789101112131415161718192021222324252627282930class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: n = len(nums) nums.sort() ans = list() # 枚举 a for first in range(n): # 需要和上一次枚举的数不相同 if first &gt; 0 and nums[first] == nums[first - 1]: continue # c 对应的指针初始指向数组的最右端 third = n - 1 target = -nums[first] # 枚举 b for second in range(first + 1, n): # 需要和上一次枚举的数不相同 if second &gt; first + 1 and nums[second] == nums[second - 1]: continue # 需要保证 b 的指针在 c 的指针的左侧 while second &lt; third and nums[second] + nums[third] &gt; target: third -= 1 # 如果指针重合，随着 b 后续的增加 # 就不会有满足 a+b+c=0 并且 b&lt;c 的 c 了，可以退出循环 if second == third: break if nums[second] + nums[third] == target: ans.append([nums[first], nums[second], nums[third]]) return ans 11. Container With Most Water 双指针 - 移动 l 和 r 较小的一方才可能增加 area 1234567891011class Solution: def maxArea(self, height: List[int]) -&gt; int: l, r = 0, len(height) - 1 area = 0 while l &lt; r: area = max(area, min(height[l], height[r]) * (r-l)) if height[l] &lt; height[r]: l += 1 else: r -= 1 return area 2. DFS / Stack 2.1 字符串解码 “3[a2[c]]” == “accacc”, stack == [(3, &quot;&quot;), (2,&quot;a&quot;)] 123456789101112131415class Solution: def decodeString(self, s: str) -&gt; str: stack, res, multi = [], &quot;&quot;, 0 for c in s: if c == &#x27;[&#x27;: stack.append([multi, res]) res, multi = &quot;&quot;, 0 elif c == &#x27;]&#x27;: cur_multi, last_res = stack.pop() res = last_res + cur_multi * res elif &#x27;0&#x27; &lt;= c &lt;= &#x27;9&#x27;: multi = multi * 10 + int(c) else: res += c return res 215. 数组中的第K个最大元素 12345678910111213141516171819from heapq import heapify, heappush, heappop # python中的heap是小根堆: heapify(hp) , heappop(hp), heappush(hp, v) class Solution: def findKthLargest(self, nums: List[int], k: int) -&gt; int: n = len(nums) if k == 0 or k &gt; n: return [] hp = nums[:k] heapify(hp) for i in range(k, n): v = nums[i] if v &gt; hp[0]: heappop(hp) heappush(hp, v) return hp[0] 3. DP No. dynamic programming Flag no-gd 31. n个骰子的点数 dp[i][j] ，表示投掷完 i 枚骰子后，点数 j 的出现次数 ✔️ Summary 20 dynamic programming (4.1) DP表示状态 easy 1. climbing-stairs ， 新建{}or[] ,滚动数组 2. 连续子数组的最大和 ❎ addition 63. 多少种 不同路径 II, store = [[0]*n for i in range(m)] 二维初始化 ❎ addition Edit Distance/编辑距离【word1 转换成 word2】 1. dp = [ [0] * (m + 1) for _ in range(n + 1)] 2. dp[i][j] = min(A,B,C) ✔️❎ addition 5. Longest Palindromic Substring/最长回文子串 1. 枚举子串的长度 l+1,从小问题到大问题 2. 枚举子串的起始位置 i, j=i+l 子串结束位置, dp[i][j] = (dp[i+1][j-1] and s[i]==s[j]) ✔️❎ good 把数字翻译成字符串 Fib ✔️❎ addition Leetcode 64. Minimum Path Sum, 最小路径和 grid[i][j] = min(grid[i - 1][j], grid[i][j - 1]) + grid[i][j] ❎ addition 115. Distinct Subsequences I Hard addition 940. 不同的子序列 II Hard addition Interleaving String/交错字符串 Hard 4. sliding window &amp; hash No. Question Flag (6). sliding Window 65. 最长不含重复字符的子字符串 滑动窗口 ✔️❎ 14. 和为s的连续正数序列 [sliding window] input：target = 9 output：[[2,3,4],[4,5]] ✔️❎ (7). 模拟 21. 圆圈中最后剩下的数字 1. 当数到最后一个结点不足m个时，需要跳到第一个结点继续数 2. 每轮都是上一轮被删结点的下一个结点开始数 m 个 3. 寻找 f(n,m) 与 f(n-1,m) 关系 4. A： f(n,m)=(m+x)%n 5. Python 深度不够手动设置 sys.setrecursionlimit(100000) 东大 Lucien 题解,讲得最清楚的那个。官方讲解有误 ✔️❎ 35. 顺时针打印矩阵 left, right, top, bottom = 0, columns - 1, 0, rows - 1 ✔️❎ 56. 把数组排成最小的数, sorted vs sort, strs.sort(key=cmp_to_key(sort_rule)) ✔️❎ 70. 把字符串转换成整数 int_max, int_min, bndry = 231-1, -231, 2**31//10 bndry=2147483647//10=214748364 ，则以下两种情况越界 res &gt; bndry or res == bndry and c &gt;‘7’ ✔️❎ 5. linkedList No. Question Flag (3). linkedList 7. 从尾到头打印链表： reversePrint(head.next) + [head.val] ❎ 8. 反转链表 pre, cur = head, head.next pre.next = None (循环版 双指针) ❎ 10. 合并两个排序的链表 [Recursion] p.next = self.mergeTwoLists(l1.next, l2) ❎ addition 旋转单链表 (F1. 环 F2. 走n-k%n 断开) 举例： 给定 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;NULL, K=3 则4-&gt;5-&gt;6-&gt;1-&gt;2-&gt;3-&gt;NULL ❎ addition 92. 翻转部分单链表 reverse(head: ListNode, tail: ListNode) 举例：1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;null, from = 2, to = 4 结果：1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;null ❎ addition 链表划分, 描述： 给定一个单链表和数值x，划分链表使得小于x的节点排在大于等于x的节点之前 ❎ addition 82. 删除排序链表中的重复元素 II 链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5. ❎ addition 输入：(7 -&gt; 1 -&gt; 6) + (5 -&gt; 9 -&gt; 2)，即617 + 295 输出：2 -&gt; 1 -&gt; 9，即912 6. stack No. Question Flag (2). Stack 394. 字符串解码 [a]2[bc] ❎ 28. 包含min函数的栈 ❎ 29. 最小的k个数【堆排的逆向】 heapq.heappop(hp),heapq.heappush(hp, -arr[i]) ✔️❎ 36. 滑动窗口的最大值 (同理于包含 min 函数的栈) deque.popleft(),双端队列+单调 ✔️❎ 59 II. 队列的最大值 , 维护个单调的deque import queue, queue.deque(), queue.Queue(), deq[0], deq[-1] ✔️❎ (5). DFS / BFS 66. 矩阵中的路径 , 经典好题: 深搜+回溯 def dfs(i, j, k): ✔️❎ 61. 机器人的运动范围 bfs good from queue import Queue, q.get() q.pup() ✔️❎ 7. string 8. Tree 剑指 No. Question Flag easy (1). Tree 1.1 平衡二叉树 abs(maxHigh(root.left) - maxHigh(root.right)) &lt;= 1 and self.isBalanced(root.left) and self.isBalanced(root.right) ❎ 1.2 对称的二叉树 ❎ 1.3 二叉树的镜像： root.left = self.mirrorTree(root.right) swap后+递归 ❎ 1.4 二叉搜索树的第k大节点 [中序遍历 倒序, 右-中-左] ✔️❎ good 1.5 (两个节点)二叉树的最近公共祖先 [Recursion] 后序遍历+路径回溯 ✔️❎ good 1.6 (两个节点)二叉搜索树的最近公共祖先 Recursion + 剪枝 ✔️❎ good 1.7 二叉树中和为某一值的路径 递归回溯 ✔❎️ 1.8 二叉搜索树的后序遍历序列 ❎ 1.9 二叉搜索树与双向链表 additional 求二叉树第K层的节点个数 [Recursion] ，root != None and k==1，返回1 f(root.left, k-1) + f(root.right, k-1) ❎ additional 求二叉树第K层的叶子节点个数 [Recursion] if(k==1 and root.left and root.right is null) return 1; ✔️❎ 1234567891011121314151617class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def isBalanced(self, root: TreeNode) -&gt; bool: def maxHigh(root): if root == None: return 0 return max(maxHigh(root.left), maxHigh(root.right)) + 1 if root == None: return True return abs(maxHigh(root.left) - maxHigh(root.right)) &lt;= 1 and self.isBalanced(root.left) and self.isBalanced(root.right)","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/tags/leetcode/"}]},{"title":"Hive Optimization Solution Notes","slug":"dataware/dwh-summary-5-knowleage","date":"2021-03-12T01:07:21.000Z","updated":"2021-06-22T06:27:34.125Z","comments":true,"path":"2021/03/12/dataware/dwh-summary-5-knowleage/","link":"","permalink":"http://www.iequa.com/2021/03/12/dataware/dwh-summary-5-knowleage/","excerpt":"","text":"1. Hive 优化 再次分享！Hive调优，数据工程师成神之路 2020 大数据/数仓/数开 Questions Hive内部表外部表区别及各自使用场景 12345678910111213141516171819202122232425// 让可以不走mapreduce任务的，就不走mapreduce任务hive&gt; set hive.fetch.task.conversion=more; // 开启任务并行执行 set hive.exec.parallel=true;// 解释：当一个sql中有多个job时候，且这多个job之间没有依赖，则可以让顺序执行变为并行执行（一般为用到union all的时候） // 同一个sql允许并行任务的最大线程数 set hive.exec.parallel.thread.number=8; // 设置jvm重用// JVM重用对hive的性能具有非常大的 影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。set mapred.job.reuse.jvm.num.tasks=10; // 合理设置reduce的数目// 方法1：调整每个reduce所接受的数据量大小set hive.exec.reducers.bytes.per.reducer=500000000; （500M）// 方法2：直接设置reduce数量set mapred.reduce.tasks = 20// map端聚合，降低传给reduce的数据量set hive.map.aggr=true // 开启hive内置的数倾优化机制set hive.groupby.skewindata=true Hadoop高频考点！ No. Hive 优化 Flag 1. explain explain [extended] query 2. Column cropping列裁剪 set hive.optimize.cp = true; # 列裁剪，取数只取查询中需要用的列，默认true 3. 谓词下推Predicate pushdown set hive.optimize.ppd=true; # 默认是trueselect a.*, b.* from a join b on a.id = b.id where b.age &gt; 20; 4. Partition crop 分区裁剪 set hive.optimize.pruner=true; # 默认是true 5. Merge small files合并小文件 Map input merge 如果一个mapreduce job碰到一对小文件作为输入，一个小文件启动一个TaskMap 输入合并:# Map端输入、合并文件之后按照block的大小分割（默认）set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;# Map端输入，不合并set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; 5. 合并小文件 Map/Reduce输出合并 大量的小文件会给 HDFS 带来压力，影响处理效率.可以通过合并 Map 和 Reduce 的结果文件来消除影响 是否合并Map输出文件, 默认值为trueset hive.merge.mapfiles=true; 是否合并Reduce端输出文件,默认值为falseset hive.merge.mapredfiles=true; 6. MapTask并行度 1、减少 MapTask 数是通过合并小文件来实现，这一点主要是针对数据源2、增加 MapTask 数可以通过控制上一个 job 的 reduceTask 个数重点注意：不推荐把这个值进行随意设置！推荐的方式：使用默认的切块大小即可。如果非要调整，最好是切块的N倍数 default_mapper_num = total_size / dfs_block_size set mapred.map.tasks=10; # 默认值是2, 大于 default_mapper_num 才生效 总结一下控制 mapper 个数的方法：1. 如果想增加 MapTask 个数，可以设置 mapred.map.tasks 为一个较大的值2. 如果想减少 MapTask 个数，可以设置 maperd.min.split.size 为一个较大的值3. 如input是大量小文件，想减少 mapper 数，可设置 hive.input.format 合并小文件hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 7. ReduceTask并行度 可以通过改变上述两个参数的值来控制 ReduceTask 的数量. 也可以通过: set mapred.map.tasks=10; set mapreduce.job.reduces=10; 8. Join优化 1. 优先过滤后再进行Join操作，最大限度减少参与join的数据量2. 小表join大表，最好启动mapjoin，hive自启用mapjoin, 小表不能超过25M，可改3. Join on的条件相同的话，最好放入同一个job，并且join表的排列顺序从小到大4. 如果多张表做join, 如果多个链接条件都相同，会转换成一个Job大表Join大表 1. filter null key 2. change null key to rand() case when a.user_id is null then concat('hive',rand()) else a.user_id end = b.user_id; 9. 启用 MapJoin 是否根据输入小表的大小，自动将reduce端的common join 转化为map join，将小表刷入内存中 对应逻辑优化器是MapJoinProcessor set hive.auto.convert.join = true;# 刷入内存表的大小(字节)set hive.mapjoin.smalltable.filesize = 25000000; # hive会基于表的size自动的将普通join转换成mapjoin set hive.auto.convert.join.noconditionaltask=true; # 多大的表可以自动触发放到内层LocalTask中，默认大小10Mset hive.auto.convert.join.noconditionaltask.size=10000000; 也可以手动开启mapjoin： SELECT /*+ MAPJOIN(smallTable) */ smallTable.key, bigTable.value 10. Join_data_skew skew /skjuː/ # join的key对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置set hive.skewjoin.key=100000;# 如果是join过程出现倾斜应该设置为trueset hive.optimize.skewjoin=false;通过 hive.skewjoin.mapjoin.map.tasks 参数还可以控制第二个 job 的 mapper 数量，默认10000set hive.skewjoin.mapjoin.map.tasks=10000; 13. Group By优化 1. Map端部分聚合 # 开启Map端聚合参数设置 set hive.map.aggr=true;# 设置map端预聚合的行数阈值，超过该值就会分拆job，默认值100000set hive.groupby.mapaggr.checkinterval=100000 2. 有数据倾斜时进行负载均衡当 HQL 语句使用 group by 时数据出现倾斜时，如果该变量设置为 true，那么 Hive 会自动进行负载均衡。策略就是把 MapReduce 任务拆分成两个： 第1个先做预汇总，第2个再做最终汇总. # 自动优化，有数据倾斜的时候进行负载均衡（默认是false） set hive.groupby.skewindata=false; 15. Count Distinct优化 优化后（启动2个job，1个job负责子查询(可有多个reduce)，另1个job负责count(1)): select count(1) from (select id from tablename group by id) tmp; 16. 怎样写in/existsleft semi join – in / exists 实现 select a.id, a.name from a where a.id in (select b.id from b);是推荐使用 Hive 的一个高效替代方案：left semi joinselect a.id, a.name from a left semi join b on a.id = b.id; Hive0.11版本之后： 1234hive.auto.convert.join=Truehive.mapjoin.smalltable.filesize# 默认值为2500000(25M),通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中 Notes：使用默认启动该优化的方式如果出现默名奇妙的BUG(比如MAPJOIN并不起作用),就将以下两个属性置为fase手动使用MAPJOIN标记来启动该优化 12hive.auto.convert.join=false(关闭自动MAPJOIN转换操作)hive.ignore.mapjoin.hint=false(不忽略MAPJOIN标记) 手动开启mapjoin： 123456--SQL方式，在SQL语句中添加MapJoin标记（mapjoin hint）--将小表放到内存中，省去shffle操作// 在没有开启mapjoin的情况下，执行的是reduceJoinSELECT /*+ MAPJOIN(smallTable) */ smallTable.key, bigTable.value FROMsmallTable JOIN bigTable ON smallTable.key = bigTable.key;/*+mapjoin(smalltable)*/ 2.大表Join大表 把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。如下： 123456select * from log a left outer join users b on case when a.user_id is null then concat(&#x27;hive&#x27;,rand()) else a.user_id end = b.user_id; 小表不小不大，怎么用 map join 解决倾斜问题 在小表和大表进行join时，将小表放在前边，效率会高。hive会将小表进行缓存。 Reduce 长尾 Count Distinct 的执行原理是将需要去重的字段 以及 Group By 字段 联合作为 key将数据分发到 Reduce端。 因为 Distinct操作，数据无法在 Map 端的 Shuffle 阶段根据 Group By 先做一次聚合操作，以减少传输的数据量，而是将所有的数据都传输到 Reduce 端，当 key 的数据分发不均匀时，就会导致 Reduce 端长尾。 对同一个表按照维度对不同的列进行 Count Distinct操作，造成 Map 端数据膨胀，从而使得下游的 Join 和 Reduce 出现链路上的 长尾。 Map 端直接做聚合时出现 key 值分布不均匀，造成 Reduce 端长尾。 . 动态分区数过多时可能造成小文件过多，从而引起 Reduce 端长尾。 . 多个 Distinct 同时出现在一段 SQL 代码时，数据会被分发多次， 会造成数据膨胀 N 倍，长尾现象放大 N 倍. 2. MapReduce (1) Map方法之后Reduce方法之前这段处理过程叫「Shuffle」 (2) Map方法之后，数据首先进入到分区方法，把数据标记好分区，然后把数据发送到环形缓冲区；环形缓冲区默认大小100m，环形缓冲区达到80%时，进行溢写；溢写前对数据进行排序，排序按照对key的索引进行字典顺序排序，排序的手段「快排」；溢写产生大量溢写文件，需要对溢写文件进行「归并排序」；对溢写的文件也可以进行Combiner操作，前提是汇总操作，求平均值不行。最后将文件按照分区存储到磁盘，等待Reduce端拉取。 3）每个Reduce拉取Map端对应分区的数据。拉取数据后先存储到内存中，内存不够了，再存储到磁盘。拉取完所有数据后，采用归并排序将内存和磁盘中的数据都进行排序。在进入Reduce方法前，可以对数据进行分组操作。 No. Hive 优化 Flag 1. join 优化, order &amp; customer - 先过滤在Join 2. union优化： （union 去掉重复的记录）而是使用 union all 然后在用group by 去重 5. 消灭子查询内的 group by 、 COUNT(DISTINCT)，MAX，MIN。可以减少job的数量 6. spark join 优化 - set hive.auto.convert.join=true; 小表自动判断，在内存 Sort -Merge -Bucket Join 对大表连接大表的优化 7. 数据倾斜 - SQL 导致 1. group by维度变得更细 2. 值为空的情况单独处理 3. 不同数据类型关联产生数据倾斜（int,string） group by维度不能变得更细的时候,就可以在原分组key上添加随机数后分组聚合一次, 然后对结果去掉随机数后再分组聚合,在join时，有大量为null的join key，则可以将null转成随机值，避免聚集 8. 数据倾斜 - 业务数据本身导致 - 热点值和非热点值分别进行处理 9. 数据倾斜 - key本身不均 - 可以在key上加随机数，或者增加reduceTask数量 10. 合并小文件 - 小文件的产生有三个地方，map输入，map输出，reduce输出 1234567891011121314151617181920212223242526WITH x AS ( SELECT app, user_id, count( 1 ) AS rn FROM table1 GROUP BY app, user_id ) SELECT t.app, t.user_id FROM ( SELECT app, user_id, row_number ( ) over ( PARTITION BY app ORDER BY rn DESC ) AS max_user FROM x ) AS t WHERE t.max_user &lt;= 5 3. Spark (1). Data Skew 的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生 Data Skew。 (2). Task 有2个非常慢 不指定语言，写一个WordCount的MapReduce lines = sc.textFile(…) lines.flatMap(lambda x: x.split(’ ')) wco = words.map(lambda x: (x, 1)) word_count = wco.reduceByKey(add) word_count.collect() 1234567891011121314lines = sc.textFile(&quot;/Users/blair/ghome/github/spark3.0/pyspark/spark-src/word_count.text&quot;, 2)lines = lines.filter(lambda x: &#x27;New York&#x27; in x)#lines.take(3)words = lines.flatMap(lambda x: x.split(&#x27; &#x27;))wco = words.map(lambda x: (x, 1))#print(wco.take(5))word_count = wco.reduceByKey(add)word_count.collect() 你能用SQL语句实现上述的MapReduce吗？ 1select id, count(*) from D group by id order by count(*) desc; Spark提交你的jar包时所用的命令是什么？ 1spark-submit 你所理解的Spark的shuffle过程？ Shuffle是MapReduce框架中的一个特定的phase，介于Map phase和Reduce phase之间，当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。 如果我有两个list，如何用Python语言取出这两个list中相同的元素？ 1list(set(list1).intersection(set(list2))) Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？ 在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 1234567891011121314151617./bin/spark-submit \\ --master yarn --deploy-mode cluster --num-executors 100 \\ # 总共申请的executor数目，普通任务十几个或者几十个足够了 --executor-memory 6G \\ --executor-cores 4 \\ # 每个executor内的核数，即每个executor中的任务task数目，此处设置为2 --driver-memory 1G \\ # driver内存大小，一般没有广播变量(broadcast)时，设置1~4g足够 --conf spark.default.parallelism=1000 \\ # 默认每个 satge 的 Task总数 # Spark作业的默认为500~1000个比较合适,如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样会导致并行度偏少，资源利用不充分。该参数设为num-executors * executor-cores的2~3倍比较合适 --conf spark.storage.memoryFraction=0.5 \\ 存储内存 --conf spark.shuffle.memoryFraction=0.3 \\ 执行内存 # shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2，如果shuffle聚合时使用的内存超出了这个20%的限制，多余数据会被溢写到磁盘文件中去，降低shuffle性能 # 该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 # # —-spark.yarn.executor.memoryOverhead 1G ： executor执行的时候，用的内存可能会超过executor-memory， # 所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存 # 默认的 spark.executor.memoryOverhead=6144（6G） 有点浪费 spark-summary-3-trouble-shooting 12spark.sql.shuffle.partitions - 配置join或者聚合操作shuffle数据时分区的数量spark.default.parallelism. - 该参数用于设置每个stage的默认task数量 , 设置该参数为num-executors * executor-cores的2~3倍较为合适 spark sql多维分析优化——提高读取文件的并行度, File (ROW Group - column chunk) 1234spark.sql.files.maxPartitionBytes 默认128MB，单个分区读取的最大文件大小spark.sql.files.maxPartitionBytes 的值来降低 maxSplitBytes 的值parquet.block.size parquet 文件的数据是以row group 存储，一个parquet 文件可能只含有一个row group，也有可能含有多个row group ，row group 的大小 主要由parquet.block.size 决定。 「Parquet是为了使Hadoop生态系统中的任何项目都可以使用压缩的，高效的列式数据表示形式」 parquet.block size:默认值为134217728byte,即128MB,表示 Row Group在内存中的块大小。该值设置得大,可以提升 Parquet文件的读取效率,但是相应在写的时候需要耗费更多的内存 「所以在实际生产中，使用Parquet存储，snappy/lzo压缩的方式更为常见，这种情况下可以避免由于读取不可分割大文件引发的数据倾斜。 读取hdfs文件并行了 22个 tasks 1234567891011121314151617181920211. Cache 缓存 1.1 spark.catalog.cacheTable(“t”) 或 df.cache() Spark SQL会把需要的列压缩后缓存，避免使用和GC的压力 1.2 spark.sql.inMemoryColumnarStorage.compressed 默认true 1.3 spark.sql.inMemoryColumnarStorage.batchSize 默认10000 控制列缓存时的数量，避免OOM风险。引申要点： 行式存储 &amp; 列式存储 优缺点2. 其他配置 2.1 spark.sql.autoBroadcastJoinThreshold 2.2 spark.sql.shuffle.partitions 默认200，配置join和agg的时候的分区数 2.3 spark.sql.broadcastTimeout 默认300秒，广播join时广播等待的时间 2.4 spark.sql.files.maxPartitionBytes 默认128MB，单个分区读取的最大文件大小 2.5 spark.sql.files.openCostInBytesparquet.block.size3. 广播 hash join - BHJ 3.1 当系统 spark.sql.autoBroadcastJoinThreshold 判断满足条件，会自动使用BHJ Spark SQL 在 Spark Core 的基础上针对结构化数据处理进行很多优化和改进. Spark 只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多。 如果对RDD进行cache操作后，数据在哪里？ 执行cache算子时数据会被加载到各个Executor进程的内存. 第二次使用 会直接从内存中读取而不会区磁盘. 华为云Stack全景图 &gt; 开发者指南 &gt; SQL和DataFrame调优 &gt; Spark SQL join优化 逻辑执行计划只是对SQL语句中以什么样的执行顺序做一个整体描述. 物理执行计划包含具体什么操作. 例如：是BroadcastJoin、还是SortMergeJoin… 聚合后cache 默认情况下coalesce不会产生shuffle coalesce, repartition (1) 谓词下推 Predicate PushDown - SQL中的谓词主要有 like、between、is null、in、=、!=等 (2) 列裁剪 Column Pruning 和 映射下推 Project PushDown - 列裁剪和映射下推的目的：过滤掉查询不需要使用到的列 4. hadoop, hive, spark Hive中order by，sort by，distribute by，cluster by的区别 order by会对输入做全局排序，因此只有一个Reducer sort by不是全局排序，其在数据进入reducer前完成排序 distribute by是控制在map端如何拆分数据给reduce端的, sort by为每个reduce产生一个排序文件 1 Hadoop和spark的主要区别 2 Hadoop中一个大文件进行排序，如何保证整体有序？sort只会保证单个节点的数据有序 3 Hive中有哪些udf 4 Hadoop中文件put get的过程详细描述 5 Java中有哪些GC算法? [1. 标记-清除算法 2. 复制算法 3. 标记-整理算法 4. 分代收集算法] 6 Java中的弱引用 强引用和软引用分别在哪些场景中使用 7 Hadoop和spark的主要区别-这个问题基本都会问到 (1). Hadoop和spark的主要区别 spark消除了冗余的 HDFS 读写: Hadoop 每次 shuffle 操作后，必须写到磁盘，而 Spark 在 shuffle 后不一定落盘，可以 cache 到内存中，以便迭代时使用。如果操作复杂，很多的 shufle 操作，那么 Hadoop 的读写 IO 时间会大大增加，也是 Hive 更慢的主要原因了。 spark消除了冗余的 MapReduce 阶段: Hadoop 的 shuffle 操作一定连着完整的 MapReduce 操作，冗余繁琐。而 Spark 基于 RDD 提供了丰富的算子操作，且 reduce 操作产生 shuffle 数据，可以缓存在内存中。 JVM 的优化: Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，基于进程的操作。而 Spark 每次 MapReduce 操作是基于线程的，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多。 (2). Hadoop中一个大文件进行排序，如何保证整体有序？ 一个文件有key值从1到10000的数据，我们用两个分区，将1到5000的key发送到partition1，然后由reduce1处理，5001到10000的key发动到partition2然后由reduce2处理，reduce1的key是按照1到5000的升序排序，reduce2的key是按照5001到10000的升序排序，这就保证了整个MapReduce程序的全局排序。 Hadoop 中的类 TotalOrderPartitioner 5. data warehouse 知乎：如何建设数据仓库？ 华为：数据仓库、主题域、主题概念与定义 other: 美团配送数据治理实践 数仓大山哥 码龄10年 good 数仓大山哥 - Hive数据倾斜的原因及主要解决方法 数仓大山哥 - Hive优化-大表join大表优化 缓慢变化维 (Slowly Changing Dimension) 常见的三种类型及原型设计（转） DWH 建模方法: 范式建模法/维度建模法/实体建模法 这里面就涉及到了数据仓库的架构，简单来说数据仓库分为四个层次： Layering Desc ODS 存放原始数据，直接加载原始日志、数据，数据保存原貌不做处理。 DWD 结构与粒度原始表保持一致，对ODS层数据进行清洗 DWS 以DWD为基础，进行轻度汇总 （少量以ODS为基础） ADS 为各种统计报表提供数据 注意: 数据仓库的架构当中，各个系统的元数据通过ETL同步到操作性数据仓库ODS中，对ODS数据进行面向主题域建模形成DW（数据仓库），DM是针对某一个业务领域建立模型，具体用户（决策层）查看DM生成的报表 最重要的是，要和业务以及产品负责人耐心沟通，认真敲定口径，比如观看人数的统计，就是要确定好哪些观众不算有效观众，观众和主播是同一人的等等细节，耐心是很重要的，需要格外注意的是，开发要学会要抛弃自己的专业知识，用最通俗的方式去解释，并且学会留下记录。 说了这么多，最最重要的，一定要做好规范维护，无论是用前端还是excel，及时更新是必须的。表的作用，设计理念，表字段的取数逻辑，口径的提供人，表结构都要记录在案，时常维护 数据质量 - 数据本身质量：数据开发对数据质量负责，保持对数据的敬畏心 数据建设质量：可以从两方面来考量：易用性和丰富性； Title Desc 指标体系 指标定义规范，目的是统一开发&amp;产品对指标的定义。通过对原子指标的命名规则、派生指标的命名规则和派生词的定义来完成。 粒度 维度 数据主题划分 数据分层 表 命名规范 - dwd_数据域_业务过程_(p全量/i增量) / dws_数据域_维度_统计周期 ods dwd dws dim ads 评价体系： 数据安全 数据质量 开发效率 数据稳定 数据规范 数据建设 元数据管理: Excel 2. DDL 3. Airflow Risk Dept 建立工单 Reference 大数据：元数据（Metadata） 数据治理之元数据管理 数据仓库–数据分层（ETL、ODS、DW、APP、DIM） 数仓–Theory–数仓命名规范 有赞数据仓库元数据系统实践 【数据仓库】——数据仓库概念 Hive数据倾斜优化总结 数据仓库–数据分层（ETL、ODS、DW、APP、DIM） 网易严选数仓规范与评价体系 DE（开发）题(附答案) Spark系列–SparkSQL(三)执行SparkSQL查询","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"},{"name":"hive","slug":"data-warehouse/hive","permalink":"http://www.iequa.com/categories/data-warehouse/hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://www.iequa.com/tags/Hive/"}]},{"title":"DataWare Review Summary 4","slug":"dataware/dwh-summary-4-project","date":"2021-03-09T01:07:21.000Z","updated":"2021-06-22T06:04:44.238Z","comments":true,"path":"2021/03/09/dataware/dwh-summary-4-project/","link":"","permalink":"http://www.iequa.com/2021/03/09/dataware/dwh-summary-4-project/","excerpt":"","text":"1. DW 技术选型 No. Title Tech 1. 数据采集 flume, kafka, sqoop, logstach, datax 2. 数据存储 mysql, hdfs, hbase, redis, elastic, kudu, mongodb 3. 数据计算 hive, tez, spark, flink, storm 4. 数据查询 presto, kylin, impala, druid, clickhouse 5. 数据可视化 echarts, superset, quickbl, dataV 6. 任务调度 azkaban, airflow, Oozie 7. 集群监控 Zabbix 8. 元数据管理 Apache Atlas 9. 权限管理 Aapche Ranger 2. 项目背景 金融行业, 信贷 业务过程的简易图 3. 数据调研 Table List No. Table Name Desc 1. channel_info 2. com_manager_info 3. dict_citys 4. dict_product 5. dict_provinces 6. drawal_address 7. drawal_apply 借款申请ID, 信用审核ID, 金额, 期限, 待还金额, 放款时间, 协议ID, 下一个还款时间, 放款资金源ID, 协议核对标识, 信用审核类型, 用户类型, 放款类型 8. drawal_companys 9. loan_apply 10. loan_apply_credit_report 11. loan_apply_salary 12. loan_credit 审核状态, 时间, 结论, 产品, 批准金额, 期限, 分数 13. repay_plan user_id, apply_id, contract_amount, loan_term期限, paid_amount 已还金额, 预存金额, 尚欠金额, 减免金额, 提前结清违约金, 与核心同步时间 14. repay_plan_item drawal_apply_id提款申请ID, repay_plan_id还款计划ID, repay_item还款期数编号, due_data逾期时间, dest_principal, dest_interest, dest_service, dest_pty_fee 本息滞纳金, … 15. repay_plan_item_his 16. user_det 17. user_ocr_log 18. user_md5 19. user_quota 信用额度, 已使用额, 未使用额, 失效日期, 额度失效日期… 20. users 提款&amp;还款 34 Table Details 3.5 loan_apply 表名 字段 类型 描述 loan_apply id, user_id bigint(20) ID / 用户ID loan_apply apply_time datetime 申请时间 loan_apply is_current tinyint(4) 是否最新申请 loan_apply short_loan_term int(4) 最短借款期限 loan_apply long_loan_term int(4) 最长借款期限 loan_apply short_loan_amount double 最低借款金额 loan_apply long_loan_amount double 最高借款金额 loan_apply credit_id bigint(20) 信用审核ID loan_apply status varchar(20) 状态 loan_apply created_time, updated_time datetime 创建, 更新时间 3.6 loan_apply_salary 表名 字段 类型 描述 loan_apply_salary id, user_id bigint(20) ID / 用户ID loan_apply_salary loan_apply_id bigint(20) 申请ID loan_apply_salary salary_report_url varchar(50) 薪资报告URL loan_apply_salary is_review varchar(10) 是否完成审查 loan_apply_salary created_time, updated_time datetime 创建, 更新时间 3.7 loan_apply_credit_report 表名 字段 类型 描述 loan_apply_credit_report id, user_id bigint(20) ID / 用户ID loan_apply_credit_report loan_apply_id bigint(20) 申请ID loan_apply_credit_report salary_report_url varchar(50) 薪资报告URL loan_apply_credit_report is_review varchar(10) 是否完成审查 loan_apply_credit_report created_time, updated_time datetime 创建, 更新时间 3.8 drawal_apply 表名 字段 类型 描述 drawal_apply id bigint(20) ID drawal_apply user_id bigint(20) ID drawal_apply loan_apply_id bigint(20) ID drawal_apply product_id bigint(20) ID drawal_apply audit_id bigint(20) ID drawal_apply credit_type bigint(20) ID drawal_apply amount bigint(20) ID drawal_apply loan_term bigint(20) ID drawal_apply repay_amount bigint(20) 待还款金额 drawal_apply status bigint(20) 状态 drawal_apply lend_time datetime 放款时间 drawal_apply due_date datetime 逾期时间 drawal_apply id bigint(20) ID 3.9 drawal_companys 表名 字段 类型 描述 drawal_companys id, user_id, drawal_apply_id 支用申请ID drawal_companys occupation_type 行业类型 drawal_companys company_type drawal_companys working_age drawal_companys post drawal_companys title drawal_companys comp_name/comp_address/comp_tel/comp_email/salary drawal_companys social_security drawal_companys loan_usage 4. 主题模型 主题（Subject）是在较高层次上将企业信息系统中的数据进行综合、归类和分析利用的一个抽象概念，每一个主题基本对应一个宏观的分析领域。 No. 主题名称 主题描述 1. 客户 (USER) 当事人, 用户信息, 非常多, 人行征信信息， 个人资产信息 2. 机构 (ORG) 线下有哪些团队, 浙江区/团队长，客户经理， 有 several hundred+ 个. 只有维度表 3. 产品 (PRD) 签协议 产生 产品, 业务流程, 只有维度表 产品维度表： 产品编号, 产品名称， 上架， 下架京金， code， 展示给财务 4. 渠道 (CHL) 5. 事件 (EVT) 1. 业借 / 注册&amp;认证 2. 授信 3. 支用 4. 放款 5. 支付 6. 还款 6. 协议 (AGR) 合约 7. 营销 (CAMP) 营销之后的，商务经理和渠道，谈下来之后， 后端 渠道， 资产， 账务 8. 财务 (RISK) 9. 风险 (FINANCE) 风险部 在逻辑意义上，它是对应企业中某一宏观分析领域所涉及的分析对象。例如“销售分析”就是一个分析领域，因此这个数据仓库应用的主题就是“销售分析”。 数据分层 数据来源: ODS, 多数全量 5. 建模流程 No. data warehosue 建模体系 description 1. 规范化数据仓库 2. dimensional modeling 1. 维度表 dimension ： 表示对分析主题所属类型的描述 2. 事实表 fact table : 对分析主题的度量 3. 独立数据集市 ods_table_name / dw_fact_topic_table_name / dm_fact_mart_name_table_name 粒度定义意味着对 事实表行 Fact Row 实际代表的内容给出明确的说明， 优先考虑最有原子性的信息而开发的维度模型 Business Pipeline 5.1 OCR 认证 No. 指标, 粒度, 维度 描述 统计指标： . 1. OCR 认证量, OCR通过量 统计粒度： 每个用户OCR认证申请, 一条数据 分析维度： 注册日期, 渠道, 用户类型, 性别, 客户经理 未来的可能需求: 原子性, 明细层面 考虑. 短信验证, 2元退化 5.2 MD5 认证 No. 指标, 粒度, 维度 描述 统计指标： . 1. 申请次数, 通过次数, 申请人数, 通过人数 统计粒度： 用户md5请求为一条明细记录 分析维度： 认证日期, 证件类型, 性别, 渠道, 客户经理, 用户类型 摘要: 申请人数, 通过人数 不在DW明细层出现, 而应该放在DM层 5.3 信贷申请 No. 指标, 粒度, 维度 描述 统计指标： . 1. 借款金额, 申请次数 . 2. 提供薪报次数, 提供薪报最早时间 . 3. 提供信报次数, 提供信报最早时间 统计粒度： 用户的一次申请一条记录 分析维度： 申请日期, 证件类型, 性别, 渠道, 客户经理, 用户类型, 最短期数, 最长期数 low and high 借款金额 1234567891011121314151617181920212223---dwd 明细层fact_loan_apply( data_date string, idty_type string, -- 证件类型 channel_id bigint, user_type string, manager_id bigint, sex string, user_id bigint, apply_id bigint, apply_time string, short_loan_term bigint, long_loan_term bigint, id_current string, -- 是否是当前申请，不是一个维度, 是一个标识, 2元退化 salary_cnt int, first_salary_time, last_salary_time, cereport_cnt int, last_cereport_time, loan_app_cnt int, etl_time string) dw/fact_loan_apply 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152INSERT OVERWRITE Table dw/fact_loan_apply partition(partition_date)SELECT ... u.idty_type, u.channel_id, u.user_type, u.manager_id, u.sex, a.user_id, a.id AS apply_id, a.apply_time, a.is_current, b.salary_cnt, b.first_salary_time, b.last_salary_time, c.cereport_cnt, c.first_cereport_time, c.last_cereport_time, ... etl_time, partition_dateFROM ods.loan_apply a LEFT JOIN ods.users u ON a.user_id = u.id LEFT JOIN ( SELECT user_id, loan_apply_id, count( id ) AS salary_cnt, min( created_at ) AS first_salary_time, max( created_at ) AS last_salary_time FROM ods.loan_apply_salary GROUP BY user_id, loan_apply_id ) b ON a.user_id = b.user_id AND a.id = b.loan_apply_id LEFT JOIN ( SELECT user_id, loan_apply_id, count( id ) AS cereport_cnt, min( created_at ) AS first_cereport_time, max( created_at ) AS last_cereport_time FROM ods.loan_apply_credit_report GROUP BY user_id, loan_apply_id ) c ON a.user_id = c.user_id AND a.id = c.loan_apply_id dm/fact_loan_apply_sum 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748--- 借款申请量, 申请人数CREATE TABLE dm/fact_loan_apply_sum ( data_date string, idty_type string, channel_id BIGINT, user_type string, manager_id BIGINT, sex string, short_loan_term INT, long_loan_term INT, apply_num INT, apply_user_num INT, salary_cnt INT, cereport_cnt INT, short_loan_amount deciman ( 11, 2 ), long_loan_amount deciman ( 11, 2 ), etl_time string ) COMMENT &#x27;&#x27; partitioned BY ( partition_date string COMMENT &#x27;分区日期&#x27; ) ROW format delimited FIELDS TERMINATED BY &#x27;\\t&#x27; LINES TERMINATED BY &#x27;\\n&#x27;;INSERT overwrite TABLE dm: fact_loan_apply_sum PARTITION ( partition_date ) SELECT data_date, idty_type, channel_id, user_type, manager_id, sex, short_loan_term, long_loan_term, sum( loan_app_cnt ) AS apply_sum, count( DISTINCT user_id ) AS apply_user_sum, sum( salary_cnt ) AS salary_cnt, sum( cereport_cnt ) AS cereport_cnt, sum( short_loan_amount ) AS short_loan_amount, sum( long_loan_amount ) AS long_loan_amount, max( from_unixtime(...) ) AS etl_time FROM dw: fact_user_regiter_dtlWHERE partition_data = form_unixtime (...) GROUP BY data_date, idty_date, channel_id, user_type, manager_id, sex未完待续 5.4 信贷审核 main table: loan_credit, user_quota loan_credit 表名 字段 类型 描述 loan_credit id, loan_credit apply_id loan_credit user_id loan_credit audit_status 审核状态 loan_credit audit_date datetime 审核时间 loan_credit audit_result 审核结论 loan_credit passed_products varchar(6000) 通过产品集 loan_credit amount 批准金额 loan_credit product_terms 批贷产品期限 loan_credit score varchar(20) 信用分数 loan_credit credit_type 信用审核类型 loan_credit credit_user_id 信用审核用户ID loan_credit created_time / updated_time 表名 字段 类型 描述 user_quota id, 1. dwd/fact_credit_dtl No. 指标 Index, 粒度 Granularity, 维度 dimension 描述 统计指标： . 1. 审核通过金额 2. 信用审核分数 3. 通过申请量 4. 拒绝申请量 5. 通过人数 6. 拒绝人数 统计粒度： 用户的一次审核记录为一条记录 分析维度： 审核日期, 证件类型, 渠道, 用户类型, 客户经理, 性别, 审核人 dwd/fact_credit_dtl 123456789101112131415161718192021222324insert overwrite table dwd/fact_credit_dtl partition(partition_date)select from_unixtime(a.audit_date, &#x27;yyyy-MM-dd&#x27;) as data_date, u.idty_type, u.channel_id, u.user_type, u.manager_id, u.sex, a.id as credit_id, a.apply_id, a.user_id, a.audit_status, a.audit_result, a.passwd_products, a.product_terms, a.credit_type, a.credit_user_id, a.score, a.amount, (case when upper(audit_status) = &#x27;PASS&#x27; then 1 else 0 end) as pass_cnt, (case when upper(audit_status) = &#x27;DENY&#x27; then 1 else 0 end) as deny_cnt, from_unixtime(unix_timestamp(), &#x27;yyyy-MM-dd JH:mm:ss&#x27;) as etl_time, from_unixtime(a.created_time, &#x27;yyyy-MM-dd&#x27;) as partition_date,from ods.loan_credit a left join (dim).users u on a.user_id=u.id 2. dm/fact_loan_credit_sum No. 指标, 粒度, 维度 描述 统计指标： . 初审量 1. 初审通过人数 2. 初审拒绝人数 3. 初审通过金额 终审量 1. 终审通过人数 2. 终审拒绝人数 3. 终审通过金额 分析维度： 审核日期, 证件类型, 性别, 渠道, 客户经理, 用户类型, 审核人 dm/fact_loan_credit_sum 1234567891011121314151617181920212223insert overwrite table dm/fact_loan_credit_sum partition(partition_date)select data_date, idty_type, channel_id, user_type, manager_id, sex, credit_user_id, count(case when credit_type=&#x27;cs&#x27; then credit_id else null end) as cs_num, count(distinct (case when credit_type=&#x27;cs&#x27; then user_id else null end) as cs_user_num, count(distinct (case when credit_type=&#x27;cs&#x27; then apply_id else null end) as cs_app_num, count(distinct (case when credit_type=&#x27;cs&#x27; and upper(audit_status) = &#x27;PASS&#x27; then 1 else 0 end) as cs_pass_num, count(distinct ((case when credit_type=&#x27;cs&#x27; and upper(audit_status) = &#x27;DENY&#x27; then 1 else 0 end) as cs_deny_num, sum(distinct (case when credit_type=&#x27;cs&#x27; and upper(audit_status) = &#x27;PASS&#x27; then amount else null end) as cs_pass_amt, max(from_unixtime(unix_timestamp(), &#x27;yyyy-MM-dd HH:mm:ss&#x27;)) as etl_time, max(from_unixtime(partition_date, &#x27;yyyy-MM-dd HH:mm:ss&#x27;)) as partition_datefrom dw: fact_credit_dtlwhere data_date=from_unixtime(unix_timestamp(), &#x27;yyyy-MM-dd&#x27;)group by data_date, idty_type, channel_id, user_type, manager_id, sex, credit_user_id; 5.5 支用/还款 No. 指标, 粒度, 维度 描述 统计指标： . 1. 支用申请量, 支用申请人数 . 2. 支用通过量, 支用通过人数 . 3. 协议签订量, 协议签订人数 . 4. 申请提款金额, 协议签订金额, 实际提款金额 用户的一次申请， 可能有多条审核记录 12345ods.drawal_apply a left join ods.users u on a.user_id=u.idleft join ( select com.id dm层： 提款统计指标: 12345678910111213---提款统计指标create table dm/fact_drawal_sum ( data_date string, idty_type string, channel_id bigint, user_type string, manager_id bigint, sex string, product_id bigint, loan_term bigint, drawal_app_num int, drawal_appuse_num int, drawal_app_amt decimal(11,2), Reference 数据仓库–数据分层（ETL、ODS、DW、APP、DIM） 数仓–Theory–数仓命名规范 有赞数据仓库元数据系统实践 知乎：大数据环境下该如何优雅地设计数据分层 【数据仓库】——数据仓库概念 Hive数据倾斜优化总结 数据仓库–数据分层（ETL、ODS、DW、APP、DIM） 网易严选数仓规范与评价体系","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"DWH","slug":"DWH","permalink":"http://www.iequa.com/tags/DWH/"}]},{"title":"DataWare Review Summary 3 - Window Function","slug":"dataware/dwh-summary-3-window-function","date":"2021-02-20T01:07:21.000Z","updated":"2021-06-22T06:04:44.249Z","comments":true,"path":"2021/02/20/dataware/dwh-summary-3-window-function/","link":"","permalink":"http://www.iequa.com/2021/02/20/dataware/dwh-summary-3-window-function/","excerpt":"（Window Function）是 SQL2003 标准中定义的一项新特性，并在 SQL2011、SQL2016 中完善","text":"（Window Function）是 SQL2003 标准中定义的一项新特性，并在 SQL2011、SQL2016 中完善 Window Function 不同于 普通函数和聚合函数，它为每行数据进行一次计算：输入多行(一个窗口), 返回一个值. 1. Window Function, SQL 特点就是 OVER 关键字: 1234567window_function (expression) OVER ( [ PARTITION BY part_list ] [ ORDER BY order_list ] [ &#123; ROWS | RANGE &#125; BETWEEN frame_start AND frame_end ] )# 1. PARTITION BY 表示将数据先按 part_list 进行分区# 2. ORDER BY 表示将各个分区内的数据按 order_list 进行排序 最后一项表示 Frame 的定义，即：当前窗口包含哪些数据？ type frame sql example ROWS ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING 表示往前 3 行到往后 3 行，一共 7 行数据（或小于 7 行，如果碰到了边界） RANGE RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING 表示所有值在 [c−3,c+3][c-3,c+3][c−3,c+3] 这个范围内的行，ccc 为当前行的值 逻辑语义上说，一个窗口函数的计算“过程”如下： 按窗口定义，将所有输入数据分区、再排序（如果需要的话） 对每一行数据，计算它的 Frame 范围 将 Frame 内的行集合输入窗口函数，计算结果填入当前行 举个例子： 1234567891011121314151617181920212223SELECT dealer_id, emp_name, sales, ROW_NUMBER() OVER (PARTITION BY dealer_id ORDER BY sales) AS rank, AVG(sales) OVER (PARTITION BY dealer_id) AS avgsales FROM sales# 上述查询中，rank 列表示在当前经销商下，该雇员的销售排名；# avgsales 表示当前经销商下所有雇员的平均销售额。查询结果如下+------------+-----------------+--------+------+---------------+| dealer_id | emp_name | sales | rank | avgsales |+------------+-----------------+--------+------+---------------+| 1 | Raphael Hull | 8227 | 1 | 14356 || 1 | Jack Salazar | 9710 | 2 | 14356 || 1 | Ferris Brown | 19745 | 3 | 14356 || 1 | Noel Meyer | 19745 | 4 | 14356 || 2 | Haviva Montoya | 9308 | 1 | 13924 || 2 | Beverly Lang | 16233 | 2 | 13924 || 2 | Kameko French | 16233 | 3 | 13924 || 3 | May Stout | 9308 | 1 | 12368 || 3 | Abel Kim | 12369 | 2 | 12368 || 3 | Ursa George | 15427 | 3 | 12368 |+------------+-----------------+--------+------+---------------+# 注：Frame 定义并非所有窗口函数都适用，比如 ROW_NUMBER()、RANK()、LEAD() 等。这些函数总是应用于整个分区，而非当前 Frame SQL 各部分的逻辑执行顺序 2. Window Function, Hive/Spark 123456create table window_test_table ( id int, --用户id sq string, --可以标识每个商品 cell_type int, --标识每个商品的类型，比如广告，非广告 rank int --这次搜索下商品的位置，比如第一个广告商品就是1，后面的依次2，3，4...)ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;; 业务方的实现方法： 12345678--业务方的写法select id, sq, cell_type, rank, if(cell_type!=26,row_number() over(partition by id order by rank),null) naturl_rank from window_test_table order by rank; 2. Window Function Principle 123select row_number() over( partition by col1 order by col2 ) from table 2.1 window function part windows函数部分就是所要在窗口上执行的函数，spark支持三中类型的窗口函数： No. title example 1. Aggregate functions (聚合函数) AVG(), COUNT(), MIN(), MAX(), SUM() 2. Ranking functions (排序函数) RANK(), DENSE_RANK(), ROW_NUMBER(), NTILE() 3. Analytic functions (分析窗口函数) FIRST_VALUE(), LAST_VALUE(), LEAD(), LAG() cume_dist函数计算当前值在窗口中的百分位数 2.2 window Function 实现原理 窗口函数的实现，主要借助 Partitioned Table Function （即PTF）； 12345678910select id, sq, cell_type, rank, row_number() over(partition by id order by rank ) naturl_rank, rank() over(partition by id order by rank) as r, dense_rank() over(partition by cell_type order by id) as dr from window_test_table group by id,sq,cell_type,rank; 数据流转如下图： Reference SQL 窗口函数的优化和执行 【社招】快手_数据仓库_面试题整理 2020年大厂面试题-数据仓库篇 『 Spark 』14. 一次 Spark SQL 性能提升10倍的经历 Spark教程-Spark 性能优化——和 shuffle 搏斗 spark中的spark Shuffle详解1 Spark-SQL性能优化","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://www.iequa.com/tags/SQL/"},{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Survey of Data Analyst","slug":"data-analysis/ana","date":"2021-02-17T10:28:21.000Z","updated":"2021-06-20T04:12:28.331Z","comments":true,"path":"2021/02/17/data-analysis/ana/","link":"","permalink":"http://www.iequa.com/2021/02/17/data-analysis/ana/","excerpt":"Job survey of data analysts in Singapore","text":"Job survey of data analysts in Singapore date function btw我忘说了oa复习pandas的时候多看看datefunction是考查重点～ 1, HR call，简单了解工作经验 2, Online test， Python+SQL 3, team manager：自我介绍+case study 4, team lead: 自我介绍+case study 5, department lead: 自我介绍+case study 笔试两套题目，一套SQL，一套Python SQL 6个题 Python主要考pandas，13个题 各30分钟，可以上网搜 渣渣的我Python根本没做完，果然上班不用的是一定会手生的… 没卒成然后跟两个team leader远程一面了 全英文 现在日常工作，处理的metrics和整体数据量，讲一个做过的项目 Python，我说我笔试没做完现在工作也不常用很渣然后就没怎么被问，她们问那咋证明我后面能跟得上工作…我说我也不知道… SQL,没让写代码，感觉就是偏概念的软技术 UNION,UNION ALL, MINUS, INTERSECT分别解释 有一个表在数据库里已存在，如何复制 HAVING和WHERE的区别 还有个啥我忘记了… case study，但是不需要具体数据，算是behavior question A/B testing基本步骤和概念 base Singapore，在官网申请的，大约一周后收到一面通知，全英文面试 刚收到邮件，1个多小时的Python + SQL 测试，要求下载pycharm，用pandas处理他们的excel，，， 用pandas处理他们的excel，sql写六个语句 1st round phone interview with HR, ask general background, your project, and experience 2nd round testing Basic SQL, python questions for two hours, send the answer back after you finished. Ask you to write python code the organize their data Interview Questions analye some data use SQL and python 4 rounds in total. (1) HR for brief chat - simple (2) Technical test using SQL and Python (3) Interview with one of the team lead - this round they will give you a excel and some time to do basic analysis (4) Interview with team manager / lead - more of behavioral and fitting Should you not pass in any of the rounds for the role you applied, HR who see you as potential candidate may direct you to other HR managers for other roles consideration and you will need to go through the whole interview process again. I went through the second round of another role and eventually decided not to take up the role. Something to note is that you are expected to have mandarin competency (i.e. speak business in Chinese) and you may be expected to liaise in mandarin with Chinese counterparts (offshore developers) Interview Questions SQL questions and tell me what you analyze phone interview from HR, and it took about 30 mins SQL assessment from direct boss, and gave you two days behavioral from direct manager, took about 20 mins case interview from direct manager, took about 1 hour HR interview -&gt;code test-&gt;result, code test includes python and sql. python is not that hard, but sql is hard. HR is very nice that she would introduce their department and products, which let you know more about their service. Coding round tested on dataframes: filtering, multi-conditional filtering, and also general theory on programming and Python HR interview asked the standard questions: why Shopee, why data analytics, where do you see yourself in 5 years Data Analyst in Singapore Application I applied through a recruiter. The process took 2 weeks. I interviewed at Shopee Singapore in May 2020. Interview The 1st round is a HR interview (less than 30 minutes). HR will ask you general question such as why do you want to apply to Shopee, and do you know what shopee is. The 2nd round interview is SQL and python test. It is a 1.5 hours online test in which contains 17 questions. There are probably 10 multiple choice questions, 3 SQL coding questions, 4 basic python questions. The SQL coding questions test you on basic SQL coding concept, such as SELECT, JOINS while the python questions test you on understanding about the python pandas package (understanding how datatime and df.groupby is used). If you are an amateur coders who refers to python manual often, you will find yourself not being able to complete those questions. I believe, in order to pass this test, you need to score close to full marks. I wasn’t able to complete the test as i encounter some technical issue on the syntax part of the code in which i wasn’t able to ask for help since it’s an online test (due to COVID-19 situation). I believed i scored around 10-11/17 for my test, which is not enough to move on in the next part of the interview. So in conclusion, Shopee expects you to score close to full marks on your SQL, python test. If you fail to do so, you can forget about moving further in the interview. machine learning Called by HR &amp; explained the process of selection. The coding test too, data cleaning, data extraction &amp; use of machine learning. Given task had a huge data set 150,000 rows and 10 columns with 3 labels Reference Shopee Risk Data Analyst 求职面试分享 [2019.07.28] 数据分析岗面试笔试题汇总 glassdoor da新鲜虾皮挂经 首页 &gt; 笔经面经 &gt; shopee凉经 数据分析岗Python笔试题 数据分析面试题之Pandas中的groupby","categories":[{"name":"data-analysis","slug":"data-analysis","permalink":"http://www.iequa.com/categories/data-analysis/"}],"tags":[{"name":"data-analysis","slug":"data-analysis","permalink":"http://www.iequa.com/tags/data-analysis/"}]},{"title":"SparkSQL Multidimensional Analysis - Group by","slug":"spark/spark-summary-6-sql-optimization-cube-group-by","date":"2021-02-10T07:28:21.000Z","updated":"2021-06-20T04:12:28.346Z","comments":true,"path":"2021/02/10/spark/spark-summary-6-sql-optimization-cube-group-by/","link":"","permalink":"http://www.iequa.com/2021/02/10/spark/spark-summary-6-sql-optimization-cube-group-by/","excerpt":"","text":"1. DataFrame From Row SparkSQL - 创建 DataFrame (Row) 123456789101112131415161718192021222324252627from pyspark.sql import Rowsc = spark.sparkContextrdd1 = sc.parallelize([(1, &#x27;dog&#x27;), (2, &#x27;cat&#x27;), (3, &#x27;dog&#x27;), (4, &#x27;pig&#x27;)], 2) pet = rdd1.map(lambda p: Row(id=p[0], name=p[1]))df1 = spark.createDataFrame(pet)df1.show()# +---+----+# | id|name|# +---+----+# | 1| dog|# | 2| cat|# | 3| dog|# | 4| pig|# +---+----+df1.createOrReplaceTempView(&quot;pet&quot;)# SQL can be run over DataFrames that have been registered as a table.pets = spark.sql(&quot;SELECT id, name FROM pet&quot;)# The results of SQL queries are Dataframe objects.# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.petNames = pets.rdd.map(lambda p: &quot;Name: &quot; + p.name).collect()for name in petNames: print(name) datasets-and-dataframes 2. count distinct 2.1 Hive 12--优化前select count(distinct id) from table_a Hive SQL count(distinct) - optimization 123456789--优化后select count(id)from( select id from table_a group by id) tmp principle of hiveql-select-group-by hiveql-select-group-by multi-reduce 2.2 SparkSql SparkSql: sc.parallelize([]), StructType([]), LongType, createDataFrame, createOrReplaceTempView 123456789101112131415161718192021222324# Import typesfrom pyspark.sql.types import *# Generate comma delimited datastringCSVRDD = sc.parallelize( [ (123, &#x27;Katie&#x27;, 19, &#x27;brown&#x27;), (234, &#x27;Michael&#x27;, 22, &#x27;green&#x27;), (345, &#x27;Simone&#x27;, 23, &#x27;blue&#x27;) ])# Specify schemaschema = StructType( [ StructField(&quot;id&quot;, LongType(), True), StructField(&quot;name&quot;, StringType(), True), StructField(&quot;age&quot;, LongType(), True), StructField(&quot;eyeColor&quot;, StringType(), True) ])# Apply the schema to the RDD and Create DataFrameswimmers = spark.createDataFrame(stringCSVRDD, schema)# Creates a temporary view using the DataFrameswimmers.createOrReplaceTempView(&quot;swimmers&quot;) count(distinct id) - expand 1234567891011spark.sql(&quot;SELECT count(distinct id), count(distinct name) FROM pet&quot;).explain()== Physical Plan ==*(3) HashAggregate(keys=[], functions=[count(if ((gid#245 = 1)) pet.`id`#246L else null), count(if ((gid#245 = 2)) pet.`name`#247 else null)])+- Exchange SinglePartition, true, [id=#510] +- *(2) HashAggregate(keys=[], functions=[partial_count(if ((gid#245 = 1)) pet.`id`#246L else null), partial_count(if ((gid#245 = 2)) pet.`name`#247 else null)]) +- *(2) HashAggregate(keys=[pet.`id`#246L, pet.`name`#247, gid#245], functions=[]) +- Exchange hashpartitioning(pet.`id`#246L, pet.`name`#247, gid#245, 200), true, [id=#505] +- *(1) HashAggregate(keys=[pet.`id`#246L, pet.`name`#247, gid#245], functions=[]) +- *(1) Expand [List(id#222L, null, 1), List(null, name#223, 2)], [pet.`id`#246L, pet.`name`#247, gid#245] +- *(1) Scan ExistingRDD[id#222L,name#223] As see from the execution plan, when dealing with count distinct, the Expand method is used. After expand, use id and name (not id,name) as keys to perform HashAggregate, which is group by, so that it is equivalent to de-duplication. then Calculate count (id) and count (name) directly later, divide and conquer the data. the data skew is alleviated. expand 方式适合维度小的多维分析，这是因为 expand 方式读取数据的次数只有一次，但数据会膨胀n倍 3. GROUP BY 3.1 ROLLUP 1234567SELECT factory, SUM( quantity ) FROM production GROUP BY ROLLUP ( factory ) ORDER BY factory ROLL UP 搭配 GROUP BY 使用，可以为每一个分组返回一个小计行，为所有分组返回一个总计行 1234SELECT factory, department, SUM(quantity)FROM productionGROUP BY ROLLUP(factory, department)ORDER BY factory 如果 ROLLUP(A,B)则先对 A,B进行 GROUP BY，之后对 A 进行 GROUP BY,最后对全表 GROUP BY 如果 ROLLUP(A,B,C)则先对 A,B,C进行 GROUP BY ，然后对 A,B进行GROUP BY,再对 A 进行GROUP BY,最后对全表进行 GROUP BY. 3.2 CUBE 123456SELECT factory, department, SUM(quantity)FROM productionGROUP BY CUBE(factory, department)ORDER BY factory,department; CUBE(A,B)则先对 A,B 进行 GROUP BY，之后对 A 进行 GROUP BY,然后对 B 进行 GROUP BY，最后对全表进行 GROUP BY 如果 CUBE(A,B,C)则先对 A,B,C 进行 GROUP BY,之后对 A,B ，之后对A,C ，之后对 B,C 之后对 A,之后对 B，之后对 C，最后对全表GROUP BY 3.3 GROUPPING The GROUPING() function can only be used with ROLLUP and CUBE. GROUPING() receives a column, and returns 0 if this column is not empty, and returns 1 if it is empty. 12345678SELECT GROUPING(factory), factory, department, SUM(quantity)FROM productionGROUP BY ROLLUP(factory, department)ORDER BY factory, department; 最后一行的 FACTORY 为空，所以 GROUPING()返回 1.也可以与CUBE结合使用 3.4 GROUPING SETS 1234567SELECT factory, department, SUM(quantity)FROM productionGROUP BY GROUPING SETS(factory, department)ORDER BY factory, department GROUPING SETS则对每个参数分别进行分组，GROUPING SETS(A,B)就代表先按照 A 分组，再按照 B分组 3.5 GROUPING_ID() 12345678910SELECT factory, department, GROUPING(factory), GROUPING(department), GROUPING_ID(factory,department), SUM(quantity)FROM production GROUP BY CUBE(factory, department) ORDER BY factory, department; If you select GROUPING_ID=0 With the GROUPING_ID column, we can use the HAVING clause to filter the query results. If you select GROUPING_ID=0, it means that the FACTORY and DEPARTMENT columns are not empty. Reference Hive SQL grouping sets 用法 Spark的分区》、《通过spark.default.parallelism谈Spark并行度》、重要 | Spark分区并行度决定机制 spark的groupping set的优化 Spark SQL / Catalyst 内部原理 与 RBO Spark权威指南—— DataFrame API笔记 GROUP BY都不会！ROLLUP，CUBE，GROUPPING详解","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"sparkSQL","slug":"sparkSQL","permalink":"http://www.iequa.com/tags/sparkSQL/"}]},{"title":"SparkSQL 从入门到调优","slug":"spark/spark-summary-5-sql-optimization-indoor","date":"2021-02-03T07:28:21.000Z","updated":"2021-06-22T05:56:41.964Z","comments":true,"path":"2021/02/03/spark/spark-summary-5-sql-optimization-indoor/","link":"","permalink":"http://www.iequa.com/2021/02/03/spark/spark-summary-5-sql-optimization-indoor/","excerpt":"","text":"1. SparkSQL 发展 SparkSQL 发展史 Version Title Description 1.0以前 Shark Spark-1.1 SparkSQL(只是测试性的) SQL Spark&lt;1.3 DataFrame 称为 SchemaRDD Spark-1.3 SparkSQL(正式版本)+Dataframe API Spark-1.4 增加窗口分析函数 Spark-1.5 SparkSQL 钨丝计划， UDF/UDAF Spark-1.6 SparkSQL 执行的 sql 可以增加注释 Spark-2.x SparkSQL+DataFrame+DataSet(正式版本), 引入 SparkSession 统一编程入口 No. Title SparkSQL 主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame. 1. 原理 将 SparkSQL 转化为 RDD ，然后提交到集群执行 2. 作用 提供一个编程抽象（DataFrame）并且作为分布式SQL查询引擎。 DataFrame 可据很多源进行构建，包括：结构化的数据文件，Hive中的表，MYSQL，以及RDD 3. 特点 1. 容易整合 2. 统一的数据访问方式 3. 兼容Hive 4. 标准的数据连接 … spark 1.x SparkContext sc / SqlContext sqlContextS … spark 2.x SparkContext sc / SparkSession spark 1.1 SparkSession 创建 12345678910from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(&quot;Python Spark SQL basic example&quot;) \\ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \\ .enableHiveSupport() // 在classpath中,必须加入一个配置文件 hive-site.xml .getOrCreate() # hive url : 源数据库在哪里 # hive warehouse : 真是数据在哪里 SparkSQL 数据抽象 1.2 SparkSQL 数据抽象 在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。 RDD[Record] == DataFrame == Table 谈谈RDD、DataFrame、Dataset的区别和各自的优势 2. SparkSQL 使用 SparkSQL, DataFrame, StructType, LongType, createOrReplaceTempView 123456789101112131415161718192021222324# Import typesfrom pyspark.sql.types import *# Generate comma delimited datastringCSVRDD = sc.parallelize( [ (123, &#x27;Katie&#x27;, 19, &#x27;brown&#x27;), (234, &#x27;Michael&#x27;, 22, &#x27;green&#x27;), (345, &#x27;Simone&#x27;, 23, &#x27;blue&#x27;) ])# Specify schemaschema = StructType( [ StructField(&quot;id&quot;, LongType(), True), StructField(&quot;name&quot;, StringType(), True), StructField(&quot;age&quot;, LongType(), True), StructField(&quot;eyeColor&quot;, StringType(), True) ])# Apply the schema to the RDD and Create DataFrameswimmers = spark.createDataFrame(stringCSVRDD, schema)# Creates a temporary view using the DataFrameswimmers.createOrReplaceTempView(&quot;swimmers&quot;) SparkSQL 一些网络链接 No. Title Author Desc 2. Apache SparkSQL Spark SQL Guide 3. SparkSQL Spark2.x学习笔记：14、Spark SQL程序设计 4. DataFrame good 实际例子演示： Spark2.x学习笔记：14、Spark SQL程序设计 5. DataFrame DataFrame常用操作（DSL风格语法），sql风格语法 6. SparkSQL Spark SQL重点知识总结 7. Create DataFrame spark1.x：studentRDD.toDF / sqlContext.createDataFrame(studentRDD) / sqlContext.createDataFrame(rowRDD, schema) spark2.x：spark.read.format().load() SparkSQL 编写代码多种方式 123456789101112131415161718// This code works perfectly from Spark 2.x with Scala 2.11// Import necessary classesimport org.apache.spark.sql.&#123;Row, SparkSession&#125;import org.apache.spark.sql.types.&#123;DoubleType, StringType, StructField, StructType&#125;// Create SparkSession Object, and Here it&#x27;s sparkval spark: SparkSession = SparkSession.builder.master(&quot;local&quot;).getOrCreateval sc = spark.sparkContext // Just used to create test RDDs// Just used to create test RDDs, Let&#x27;s an RDD to make it DataFrameval rdd = sc.parallelize( Seq( (&quot;first&quot;, Array(2.0, 1.0, 2.1, 5.4)), (&quot;test&quot;, Array(1.5, 0.5, 0.9, 3.7)), (&quot;choose&quot;, Array(8.0, 2.9, 9.1, 2.5)) )) Method 1: Using SparkSession.createDataFrame(RDD obj). 12345678910val dfWithoutSchema = spark.createDataFrame(rdd)dfWithoutSchema.show()+------+--------------------+| _1| _2|+------+--------------------+| first|[2.0, 1.0, 2.1, 5.4]|| test|[1.5, 0.5, 0.9, 3.7]||choose|[8.0, 2.9, 9.1, 2.5]|+------+--------------------+ Method 2: Using SparkSession.createDataFrame(RDD obj) and specifying column names. 12345678910val dfWithSchema = spark.createDataFrame(rdd).toDF(&quot;id&quot;, &quot;vals&quot;)dfWithSchema.show()+------+--------------------+| id| vals|+------+--------------------+| first|[2.0, 1.0, 2.1, 5.4]|| test|[1.5, 0.5, 0.9, 3.7]||choose|[8.0, 2.9, 9.1, 2.5]|+------+--------------------+ Method 3 (Actual answer to the question) This way requires the input rdd should be of type RDD[Row]. 1234567val rowsRdd: RDD[Row] = sc.parallelize( Seq( Row(&quot;first&quot;, 2.0, 7.0), Row(&quot;second&quot;, 3.5, 2.5), Row(&quot;third&quot;, 7.0, 5.9) )) create the schema 1234val schema = new StructType() .add(StructField(&quot;id&quot;, StringType, true)) .add(StructField(&quot;val1&quot;, DoubleType, true)) .add(StructField(&quot;val2&quot;, DoubleType, true)) Now apply both rowsRdd and schema to createDataFrame() 12345678910val df = spark.createDataFrame(rowsRdd, schema)df.show()+------+----+----+| id|val1|val2|+------+----+----+| first| 2.0| 7.0||second| 3.5| 2.5|| third| 7.0| 5.9|+------+----+----+ Submitting Applications 12345678spark-submit \\--class com.aura.sparksql.StructTypeDFTest \\--master spark://hadoop02:7077 \\--driver-memory 512M \\--executor-memory 512M \\--total-executor-cores 2 \\/home/hadoop/original-spark232_1805-1.0-SNAPSHOT.jar \\/student_sql_out/ SparkSQL 的数据源操作: to load a json file 12345678910111213df = spark.read.load(&quot;examples/src/main/resources/people.json&quot;, format=&quot;json&quot;)# Displays the content of the DataFrame to stdoutdf.show()# +----+-------+# | age| name|# +----+-------+# |null|Michael|# | 30| Andy|# | 19| Justin|# +----+-------+df.select(&quot;name&quot;, &quot;age&quot;).write.save(&quot;namesAndAges.parquet&quot;, format=&quot;parquet&quot;)# Run SQL on files directlydf = spark.sql(&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;) 默认情况下保存数据到 HDFS 的数据格式： .snappy.parquet .snappy： 结果保存到 HDFS 上的时候自动压缩： 压缩算法： snappy .parquet： 结果使用一种列式文件存储格式保存 parquet / rc / orc / row column 1234df = spark.read.load(&quot;examples/src/main/resources/users.parquet&quot;)df.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;namesAndFavColors.parquet&quot;)# parquet file 默认是这种文件方式 load 举个🌰: spark 安装路径 examples/src/main/resources : 1234567891011121314# /usr/local/xsoft/spark/examples/src/main/resources [23:06:36]➜ lltotal 88drwxr-xr-x@ 5 blair staff 160B Jun 6 21:34 dir1-rw-r--r--@ 1 blair staff 130B Jun 6 21:34 employees.json-rw-r--r--@ 1 blair staff 240B Jun 6 21:34 full_user.avsc-rw-r--r--@ 1 blair staff 5.7K Jun 6 21:34 kv1.txt-rw-r--r--@ 1 blair staff 49B Jun 6 21:34 people.csv-rw-r--r--@ 1 blair staff 73B Jun 6 21:34 people.json-rw-r--r--@ 1 blair staff 32B Jun 6 21:34 people.txt-rw-r--r--@ 1 blair staff 185B Jun 6 21:34 user.avsc-rw-r--r--@ 1 blair staff 334B Jun 6 21:34 users.avro-rw-r--r--@ 1 blair staff 547B Jun 6 21:34 users.orc-rw-r--r--@ 1 blair staff 615B Jun 6 21:34 users.parquet 3. SparkSQL 调优 No. Title Author Link 0. 华为开发者SparkCore知乎大数据SparkSQL 开发者指南 &gt; 组件成功案例 &gt; Spark &gt; 案例10：Spark Core调优 &gt; 经验总结 Spark基础：Spark SQL调优 1. Cache 缓存 1.1 spark.catalog.cacheTable(“t”) 或 df.cache() Spark SQL会把需要的列压缩后缓存，避免使用和GC的压力 1.2 spark.sql.inMemoryColumnarStorage.compressed 默认true 1.3 spark.sql.inMemoryColumnarStorage.batchSize 默认10000 控制列缓存时的数量，避免OOM风险。 引申要点： 行式存储 &amp; 列式存储 优缺点 2. 其他配置 2.1 spark.sql.autoBroadcastJoinThreshold 2.2 spark.sql.shuffle.partitions 默认200，配置join和agg的时候的分区数 2.3 spark.sql.broadcastTimeout 默认300秒，广播join时广播等待的时间 2.4 spark.sql.files.maxPartitionBytes 默认128MB，单个分区读取的最大文件大小 2.5 spark.sql.files.openCostInBytes parquet.block.size3. 广播 hash join - BHJ 3.1 当系统 spark.sql.autoBroadcastJoinThreshold 判断满足条件，会自动使用BHJ 华为云Stack全景图 &gt; 开发者指南 &gt; SQL和DataFrame调优 &gt; Spark SQL join优化 spark不会 注意spark不会确保每次选择广播表都是正确的，因为有的场景比如 full outer join 是不支持BHJ的。手动指定广播: broadcast(spark.table(“src”)).join(spark.table(“records”), “key”).show() 1. Spark学习技巧 Spark SQL从入门到精通 Data Source： json, parquet, jdbc, orc, libsvm, csv, text, Hive 表 3. SparkSQL要懂的 Spark SQL在Spark集群中是如何执行的？ 1. Spark SQL 转换为逻辑执行计划 2. Catalyst Optimizer组件，将逻辑执行计划转换为Optimized逻辑执行计划 3. 将Optimized逻辑执行计划转换为物理执行计划 4. Code Generation对物理执行计划进一步优化，将一些(非shuffle)操作串联在一起 5. 生成Job（DAG）由scheduler调度到spark executors中执行 逻辑执行计划和物理执行计划的区别？ 1. 逻辑执行计划只是对SQL语句中以什么样的执行顺序做一个整体描述.2. 物理执行计划包含具体什么操作. 例如：是BroadcastJoin、还是SortMergeJoin… 4. SparkSQL优化 1. spark.sql.codegen=false/true 每条查询的语句在运行时编译为java的二进制代码 2. spark.sql.inMemoryColumnStorage.compressed 默认false 内存列式存储压缩3. spark.sql.inMemoryColumnStorage.batchSize = 1000 4. spark.sql.parquet.compressed.codec 默认snappy, (uncompressed/gzip/lzo) 5. spark.catalog.cacheTable(“tableName”) 或 dataFrame.cache() 6. --conf “spark.sql.autoBroadcastJoinThreshold = 50485760” 10 MB -&gt; 50M 7. --conf “spark.sql.shuffle.partitions=200” -&gt; 2000 5. SparkSQL调优 http://marsishandsome.github.io/SparkSQL-Internal/03-performance-turning/ 1. 对于数据倾斜，采用加入部分中间步骤，如聚合后cache，具体情况具体分析；2. 适当的使用序化方案以及压缩方案； 3. 善于解决重点矛盾，多观察Stage中的Task，查看最耗时的Task，查找原因并改善； 4. 对于join操作，优先缓存较小的表；5. 要多注意Stage的监控，多思考如何才能更多的Task使用PROCESS_LOCAL； 6. 要多注意Storage的监控，多思考如何才能Fraction cached的比例更多 1.1 基本操作 123456789val df = spark.read.json(“file:///opt/meitu/bigdata/src/main/data/people.json”)df.show()import spark.implicits._df.printSchema()df.select(&quot;name&quot;).show()df.select($&quot;name&quot;, $&quot;age&quot; + 1).show()df.filter($&quot;age&quot; &gt; 21).show()df.groupBy(&quot;age&quot;).count().show()spark.stop() 编码 1234567// spark2+, SparkSessionval spark = SparkSession.builder() .config(sparkConf).getOrCreate()//使用hive元数据val spark = SparkSession.builder() .config(sparkConf).enableHiveSupport().getOrCreate()spark.sql(&quot;select count(*) from student&quot;).show() 使用 createOrReplaceTempView 123val df =spark.read.json(&quot;examples/src/main/resources/people.json&quot;) df.createOrReplaceTempView(&quot;people&quot;) spark.sql(&quot;SELECT * FROM people&quot;).show() 4. SparkSQL 运行过程 5. Catalyst Apache Spark RDD vs DataFrame vs DataSet No. Link 0. 【大数据】SparkSql连接查询中的谓词下推处理(一) 0. 使用explain分析Spark SQL中的谓词下推，列裁剪，映射下推 1. SparkSQL – 从0到1认识Catalyst（转载） 深入研究Spark SQL的Catalyst优化器（原创翻译） 2. Spark SQL Catalyst优化器 3. 【数据库内核】基于规则优化之谓词下推 Reference No. Link 1. W3C School - Spark RDD 操作 2. W3C School - Spark RDD持久化 3. W3C School - Spark SQL性能调优 记录一次spark sql的优化过程 good - spark 内存溢出处理 good - spark内存溢出及其解决方案 SparkSQL解决数据倾斜实战介绍(适用于HiveSQL) 优化Spark中的数据倾斜 spark十亿数据join优化 Apache Spark 在eBay 的优化 字节跳动在Spark SQL上的核心优化实践 | 字节跳动技术沙龙 Apache Spark源码走读之11 – sql的解析与执行 背各种SparkSQL调优参数？这个东西才是SparkSQL必须要懂的 SparkSQL性能调优 一些常用的Spark SQL调优技巧 SparkSQL调优 Spark基础：Spark SQL调优 spark-sql调优技巧 - sparkSQL的前世今生 聊聊spark-submit的几个有用选项","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"sparkSQL","slug":"sparkSQL","permalink":"http://www.iequa.com/tags/sparkSQL/"}]},{"title":"猴子的图解SQL 学习笔记","slug":"dataware/SQL-Monkey","date":"2021-02-01T01:07:21.000Z","updated":"2021-06-22T06:00:50.947Z","comments":true,"path":"2021/02/01/dataware/SQL-Monkey/","link":"","permalink":"http://www.iequa.com/2021/02/01/dataware/SQL-Monkey/","excerpt":"","text":"SQL 的书写规则是什么？ 如何指定查询条件？ SQL 是如何运行的？ 学生表：student(学号,学生姓名,出生年月,性别) 成绩表：score(学号,课程号,成绩) 课程表：course(课程号,课程名称,教师号) 教师表：teacher(教师号,教师姓名) RAND() Function 0 &lt;= ret &lt; 1 1~100 -&gt; SELECT FLOOR(1 + (RAND() * 100)) LIMIT 10; IF(expr1,expr2,expr3) SELECT CustomerName, CONCAT(“H1”, &quot; H2 &quot;, RAND()), CONCAT(Address, &quot; &quot;, PostalCode, &quot; &quot;, City) AS Address FROM Customers; #SQL 如何查询关于【连续几天】的问题 1234567891011121314151617181920212223242526272829303132333435363738SELECT id, created_at, CURDATE( ), -- 2021-03-18 DATE(created_at), -- 2019-11-21 DATE(created_at) - 1 -- 20191120FROM users LIMIT 5;SELECT user_id, MAX( count_date_on ) FROM ( ( SELECT user_id, count( date_on ) count_date_on FROM ( SELECT user_id, date, row_number ( ) over ( PARTITION BY USER ORDER BY date DESC ) rnk, date - ( MAX( date ) - rnk ) date_on FROM TB GROUP BY user_id ) A GROUP BY user_id, date_on ) ) B GROUP BY user_id 1. SQL：查找重复数据？ 知识: group by 列名 having count(列名) &gt; n select 列名 from table group by 列名 having count(列名) > n; 举一反三: 查询平均成绩大于60分的学生的学号和平均成绩 123select 学号 ,avg(成绩) from score group by 学号 having avg(成绩 ) &gt;60 查询各学生的年龄（精确到月份） 12345/*【知识点】时间格式转化 timestampdiff*/select 学号 ,timestampdiff(month ,出生日期 ,now())/12 from student ; 2. SQL：如何查找第N高的数据？ 12345select ifNull( (select distinct salary from Employee order by Salary Desc limit 1,1), null ) as SecondHighestSalary; 知识: limit 1,n 3. SQL：查找不在表里的数据 left join 123456select a.Name as Customersfrom Customers as a left join Orders as bon a.Id=b.CustomerIdwhere b.CustomerId is null; 4. SQL：你有多久没涨过工资了？ left join 5. SQL：如何比较日期数据？ 举一反三： Weather 1234select a.ID, a.datefrom weather as a cross join weather as b on timestampdiff(day, a.date, b.date) = -1where a.temp &gt; b.temp; 6. DiDi: 如何找出最小的N个数？ 1.筛选出2017年入学的“计算机”专业年龄最小的3位同学名单（姓名、年龄） 2.统计每个班同学各科成绩平均分大于80分的人数和人数占比 考点: 1.使用逻辑树分析方法将复杂问题变成简单问题的能力 2.当遇到“每个”问题的时候，要想到用分组汇总 3.查询最小n个数据的问题：先排序（order by），然后使用limit取出前n行数据 4.遇到有筛选条件的统计数量问题时，使用case表达式筛选出符合条件的行为1，否则为0。然后用汇总函数（sum）对case表达式输出列求和。 7. 拼夕夕：连续出现N次的内容？ 方法1： 自连接 123456789101112131415161718192021222324SELECT *FROM Score AS a, Score AS b, Score AS cWHERE a.s_id = b.s_id - 1 AND b.s_id = c.s_id - 1 AND a.s_score = b.s_score AND b.s_score = c.s_score; SELECT DISTINCT a.s_score as 最终答案FROM Score AS a, Score AS b, Score AS cWHERE a.s_id = b.s_id - 1 AND b.s_id = c.s_id - 1 AND a.s_score = b.s_score AND b.s_score = c.s_score; 方法2： window function 12345678910111213141516SELECT DISTINCT 球员姓名 FROM ( SELECT 球员姓名, lead ( 球员姓名, 1 ) over ( PARTITION BY 球队 ORDER BY 得分时间 ) AS 姓名1, lead ( 球员姓名, 2 ) over ( PARTITION BY 球队 ORDER BY 得分时间 ) AS 姓名2 FROM 分数表 ) AS a WHERE ( a.球员姓名 = a.姓名1 AND a.球员姓名 = a.姓名2 ); 10. SQL：经典topN问题 1234567select *from ( select *, row_number() over (partition by 姓名 order by 成绩 desc) as ranking from 成绩表) as awhere ranking &lt;=2 11. 链家：如何分析留存率？ input: 如何分析留存率 指标定义： 某日活跃用户数，某日活跃的去重用户数。 N日活跃用户数，某日活跃的用户数在之后的第N日活跃用户数。 N日活跃留存率，N日留存用户数/某日活跃用户数 例：登陆时间（20180501日）去重用户数10000，这批用户在20180503日仍有7000人活跃，则3日活跃留存率为7000/10000=70% output 11.1 活跃用户数对应的日期 123456789SELECT 登陆时间, count( DISTINCT 用户id ) AS 活跃用户数 FROM 用户行为信息表 WHERE 应用名称 = &#x27;相机&#x27; GROUP BY 登陆时间; 11.2 次日留存用户数 次日留存用户数：在今日登录，明天也有登录的用户数。也就是时间间隔=1 一个表如果涉及到时间间隔，就需要用到自联结，也就是将两个相同的表进行联结 1234567SELECT *, count(DISTINCT CASE WHEN 时间间隔 = 1 THEN 用户id ELSE NULL END) AS 次日留存数 FROM (SELECT *, timestampdiff(DAY, a_t, b_t ) AS 时间间隔 FROM c) GROUP BY a_t; 次日留存用户数","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Spark - Data Skew Advanced","slug":"spark/spark-summary-4-data-skew","date":"2021-01-26T23:07:21.000Z","updated":"2021-06-20T04:12:28.347Z","comments":true,"path":"2021/01/27/spark/spark-summary-4-data-skew/","link":"","permalink":"http://www.iequa.com/2021/01/27/spark/spark-summary-4-data-skew/","excerpt":"","text":"1. Spark Data Skew For example, there are a total of 1,000 tasks, 997 tasks are executed within 1 minute, but the remaining two or three tasks take one or two hours. This situation is very common. Most tasks are executed very fast, but some tasks are extremely slow. the progress of the entire Spark job is determined by the task with the longest running time. 2. The principle of Data Skew when performing shuffle, the same key on each node must be pulled to a task on a node for processing, such as aggregation or join operations according to the key. For example, most keys correspond to 10 pieces of data, but individual keys correspond to 1 million pieces of data, so most tasks may only be assigned to 10 pieces of data, and then run out in 1 second; but individual tasks may be assigned 1 million pieces The data will run for one or two hours. Data skew only occurs during the shuffle process. No. trigger shuffle operations when data skew, it may be caused by using one of these operators. 1. distinct 2. groupByKey 3. reduceByKey 4. aggregateByKey 5. join, cogroup, repartition, etc. 3. The execution of a task slow The first thing to look at is which stage of data skew occurs in. yarn-client submit, you can see the log locally, find which stage is currently running in the log; yarn-cluster submit, Spark Web UI Run to the first few stages. Whether using the yarn-client mode or the yarn-cluster mode, we can take a deep look at the amount of data allocated by each task of this stage on the Spark Web UI, so as to further determine whether the uneven data allocated by the task causes data skew. Data skew only occurs during the shuffle process. After knowing which stage the data skew occurs, then we need to calculate which part of the code corresponds to the stage where the skew occurs based on the principle of stage division. solution: as long as you see a shuffle operator or Spark SQL SQL in the Spark code If there is a statement that will cause shuffle in the statement (such as a group by statement), then it can be determined that the front and the back stage are divided by that place. Word Count 123456789val conf = new SparkConf()val sc = new SparkContext(conf) val lines = sc.textFile(&quot;hdfs://...&quot;)val words = lines.flatMap(_.split(&quot; &quot;))val pairs = words.map((_, 1))val wordCounts = pairs.reduceByKey(_ + _) wordCounts.collect().foreach(println(_)) 3.1 stages divided the entire code, only one reduceByKey operator will shuffle, the front and back stages will be divided. * stage0, mainly to perform operations from textFile to map, and perform shuffle write operations. 3.2 shuffle write The shuffle write operation can be simply understood as partitioning the data in the pairs RDD. In the data processed by each task, the same key will be written to the same disk file. * Stage1 is mainly to perform operations from reduceByKey to collect. 3.3 shuffle read When each task of stage1 starts to run, it will first perform shuffle read operation. The task that performs the shuffle read operation will pull those keys that belong to the node where each task of stage 0 is located, and then perform operations such as global aggregation or join on the same key. Here, the value of the key is accumulated. After stage1 executes the reduceByKey operator, it calculates the final wordCounts RDD, and then executes the collect operator to pull all the data to the Driver for us to traverse and print out. 4. data skew - distribution of keys No. View the data distribution of keys that cause data skew 1. If the data skew caused by the group by and join statements in Spark SQL, then query the key distribution of the table used in SQL Happening. 2. If the data skew is caused by the shuffle operator on Spark RDD, you can view the key distribution in the Spark job, such as RDD.countByKey(). Then, collect/take, see the distribution of the keys. For example, Word Count we can first sample 10% of the sample data for pairs, then use the countByKey operator to count the number of occurrences of each key, and finally traverse and print the number of occurrences of each key in the sample data on the client. 123val sampledPairs = pairs.sample(false, 0.1)val sampledWordCounts = sampledPairs.countByKey()sampledWordCounts.foreach(println(_)) No. solutions of the data skew 1. Improve the parallelism of shuffle operations 在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 Experience： cannot completely solve the data skew，such as the amount of data a key is 1 million. 2. Two-stage aggregation (local aggregation + global aggregation) disadvantages: only solve aggregate shuffle operations. If it is a shuffle operation of the join class, other solutions have to be used. 3. Convert reduce join to map join advantages: The effect is very good for data skew caused by the join operation, because shuffle and data skew will not happen at all. disadvantages: only suitable for a large table and a small table. After all, we need to broadcast the small table, which consumes more memory resources. The driver and each Executor will have a full amount of data of a small RDD in the memory. If the RDD data we broadcast is relatively large, such as 10G or more, then memory overflow may occur. Therefore, it is not suitable for the situation where both are large tables. Data skew only occurs during the shuffle process. 12345import sysimport random from pyspark import SparkContext, SparkConffrom operator import add# sc, random.randint(0,2) # 0 or 1 0r 2 WordCounts reduceByKey More Info... 12345# read data from text file and split each line into wordsinput_file = sc.textFile(&quot;/Users/blair/Desktop/input.txt&quot;, 2)words=input_file.flatMap(lambda line: line.split(&quot; &quot;))# words.collect() # [&#x27;China&#x27;, &#x27;Singapore&#x27;, &#x27;bbb&#x27;, &#x27;Singapore&#x27;, &#x27;hello&#x27;, &#x27;haha&#x27;, &#x27;hello&#x27;, &#x27;world&#x27;] # words.first() 123# count the occurrence of each wordwordCounts = words.map(lambda word: (f&#x27;&#123;random.randint(0,2)&#125;_&#123;word&#125;&#x27;, 1)).reduceByKey(add)wordCounts.take(10) [('0_Singapore', 3), ('2_bbb', 1), ('0_hello', 1), ('2_haha', 1), ('0_world', 1), ('1_ShangHai', 1), ('0_China', 1), ('2_Singapore', 4), ('1_Singapore', 1), ('2_hello', 1)] 1234words_recover = wordCounts.map(lambda x: (x[0][x[0].find(&#x27;_&#x27;)+1:], x[1]))word_count_ret = words_recover.reduceByKey(add)word_count_ret.take(5) [('world', 1), ('ShangHai', 1), ('China', 1), ('Singapore', 8), ('bbb', 1)] 5. Convert reduce join to map join the smaller RDD directly into the Driver memory through the collect operator code: 123456789101112131415161718192021222324252627282930313233// 首先将数据量比较小的RDD的数据，collect到Driver中来。rdd1Data = rdd1.collect()// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。// 可以尽可能节省内存空间，并且减少网络传输性能开销。rdd1DataBroadcast = sc.broadcast(rdd1Data) // 对另外一个RDD执行map类操作，而不再是join类操作。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。 List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。 Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;(); for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123; rdd1DataMap.put(data._1, data._2); &#125; // 获取当前RDD数据的key以及value。 String key = tuple._1; String value = tuple._2; // 从rdd1数据Map中，根据key获取到可以join到的数据。 Row rdd1Value = rdd1DataMap.get(key); return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value)); &#125; &#125;); // 这里得提示一下。// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。// rdd2中每条数据都可能会返回多条join后的数据。 方案实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 Other Solution 1: Use Hive ETL to preprocess data Other Solution 2: Filter a few keys that cause skew Reference https://github.com/wwcom614/Spark/blob/master/src/main/scala/com/ww/rdd/performance_optimize/BroadcastMapJoins.scala 每个Spark工程师都应该知道的 Spark的五种JOIN策略解析 SparkSQL中的三种Join及其具体实现（broadcast join、shuffle hash join和sort merge join Spark Join——Broadcast Join、Shuffle Hash Join、Sort Merge Join spark sql优化：小表大表关联优化 &amp; union替换or &amp; broadcast join Hive数仓建表该选用ORC还是Parquet，压缩选LZO还是Snappy？ Spark实践 – 性能优化基础 尚硅谷2021迎新版大数据Spark从入门到精通 尚硅谷大数据电商数仓V3.0版本教程（数据仓库项目开发实战） Spark Sql 与 MySql 使用 group by 的差别","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"SQL@Leetcode","slug":"dataware/SQL-LC","date":"2021-01-26T01:07:21.000Z","updated":"2021-06-22T06:01:26.581Z","comments":true,"path":"2021/01/26/dataware/SQL-LC/","link":"","permalink":"http://www.iequa.com/2021/01/26/dataware/SQL-LC/","excerpt":"","text":"SQL Practice 图解SQL面试题：经典50题 No. Question Flag 1. 175. Combine Two Tables ❎ 2. 176. Second Highest Salary, ORDER BY, OFFSET, IFNULL(xxx, NULL) SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT 1 OFFSET 1 ✔️ 3. 177. Nth Highest Salary, CREATE FUNCTION func_name RETURNS INT 6种方案诠释MySQL通用查询策略, BEGIN RETURN xxx/select END, SET ✔️ 4. 178. Rank Scores, 摘要: 专用窗口函数rank, dense_rank, row_number有什么区别呢？ 《通俗易懂的学会：SQL窗口函数》 select score, dense_rank() over(order by Score desc) as Ranking from Scores; 5. 180. Consecutive Numbers, l1.Id=l2.Id-1 AND l2.Id=l3.Id-1 AND l1.Num=l2.Num AND l2.Num=l3.Num ❎ 6. 185 Department Top Three Salaries, dense_rank() over (partition by x order by y desc) ❎ 7. 184. Department Highest Salary, (Employee.DepartmentId , Salary) IN ❎ 8. … … 9. 196. Delete Duplicate Emails, “delete” 和 “&gt;” 的解释，推荐！ ❎ 10. 181. Employees Earning More Than Their Managers, 1. 笛卡尔积+WHERE 2. 自连接+ON ❎ 11. 1179. Reformat Department Table, CASE WHEN condition1 THEN result1 END 12. 182. Duplicate Emails, 使用 GROUP BY 和 HAVING 条件 ❎ 13. 197. Rising Temperature, JOIN … ON DATEDIFF(w1.recordDate, w2.recordDate) = 1 14. 601. Human Traffic of Stadium 未 15. 183. 从不订购的客户 NOT IN ❎ 16. 627. Swap Salary 变更性别， sex=CASE WHEN sex='m' THEN 'f' ELSE 'm' END; ❎ 17. 626. Exchange Seats 换座位, (CASE WHEN MOD(id,2) END) AS id FROM table1, 5 ❎ 627. Swap Salary 变更性别 12345678910111213141516UPDATE salarySET sex = CASE WHEN sex=&#x27;m&#x27; THEN &#x27;f&#x27; ELSE &#x27;m&#x27; END;学习要点： SELECT OrderID, Quantity,CASE WHEN Quantity &gt; 30 THEN &#x27;The quantity is greater than 30&#x27; WHEN Quantity = 30 THEN &#x27;The quantity is 30&#x27; ELSE &#x27;The quantity is under 30&#x27;END AS QuantityTextFROM OrderDetails; 626. Exchange Seats 换座位 1234567891011121314SELECT (CASE WHEN MOD(id, 2) != 0 AND counts != id THEN id + 1 WHEN MOD(id, 2) != 0 AND counts = id THEN id ELSE id - 1 END) AS id, studentFROM seat, (SELECT COUNT(*) AS counts FROM seat) AS seat_countsORDER BY id ASC; 1234DELETE p1 FROM Person p1, Person p2WHERE p1.Email = p2.Email AND p1.Id &gt; p2.Id 176. Second Highest Salary 1234567+----+--------+| Id | Salary |+----+--------+| 1 | 100 || 2 | 200 || 3 | 300 |+----+--------+ If there is no second highest salary, then the query should return null. 1234567SELECT IFNULL( (SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT 1 OFFSET 1), NULL) AS SecondHighestSalary学习要点： OFFSET 177. Nth Highest Salary MYSQL: CREATE FUNCTION 123CREATE FUNCTION func_name ( [func_parameter] ) //括号是必须的，参数是可选的RETURNS type[ characteristic ...] routine_body LIMIT 用法： 12345678mysql&gt; SELECT * FROM orange LIMIT 5; mysql&gt; SELECT * FROM orange LIMIT 0,5; mysql&gt; SELECT * FROM orange LIMIT 10,15; // 检索记录11-25mysql&gt; SELECT * FROM orange LIMIT 2 OFFSET 3;//查询4-5两条记录mysql&gt; SELECT * FROM orange LIMIT 3,2;mysql&gt; SELECT * FROM orange LIMIT 95,-1; // 检索记录96-last getNthHighestSalary(N INT) 12345678910111213141516CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INTBEGIN SET N := N-1; // 赋值语句 N = N-1 RETURN ( # Write your MySQL query statement below. SELECT salary FROM employee GROUP BY salary ORDER BY salary DESC LIMIT N, 1 );END 180. Consecutive Numbers 12345678910111213SELECT DISTINCT #l2.*, l1.Num AS ConsecutiveNumsFROM Logs l1, Logs l2, Logs l3WHERE l1.Id = l2.Id - 1 AND l2.Id = l3.Id - 1 AND l1.Num = l2.Num AND l2.Num = l3.Num; 185. Department Top Three Salaries 1234567891011121314151617# Write your MySQL query statement belowSELECT t2.Name as Department, t1.Name as Employee, t1.SalaryFROM(SELECT DepartmentId,Name,SalaryFROM ( SELECT *, dense_rank() over (partition by DepartmentId order by Salary desc) as ranking FROM Employee) as aWHERE ranking &lt;= 3) t1JOIN Department t2ON t1.DepartmentId = t2.Id 184. Department Highest Salary 1234567891011121314151617SELECT Department.name AS &#x27;Department&#x27;, Employee.name AS &#x27;Employee&#x27;, SalaryFROM Employee JOIN Department ON Employee.DepartmentId = Department.IdWHERE (Employee.DepartmentId , Salary) IN ( SELECT DepartmentId, MAX(Salary) FROM Employee GROUP BY DepartmentId ); 181. Employees Earning More Than Their Managers 123456SELECT a.NAME AS EmployeeFROM Employee AS a JOIN Employee AS b ON a.ManagerId = b.Id AND a.Salary &gt; b.Salary; 1179. Reformat Department Table 12345678910111213141516# Write your MySQL query statement belowselect id, sum(case month when &#x27;Jan&#x27; then revenue end) as Jan_Revenue, sum(case month when &#x27;Feb&#x27; then revenue end) as Feb_Revenue, sum(case month when &#x27;Mar&#x27; then revenue end) as Mar_Revenue, sum(case month when &#x27;Apr&#x27; then revenue end) as Apr_Revenue, sum(case month when &#x27;May&#x27; then revenue end) as May_Revenue, sum(case month when &#x27;Jun&#x27; then revenue end) as Jun_Revenue, sum(case month when &#x27;Jul&#x27; then revenue end) as Jul_Revenue, sum(case month when &#x27;Aug&#x27; then revenue end) as Aug_Revenue, sum(case month when &#x27;Sep&#x27; then revenue end) as Sep_Revenue, sum(case month when &#x27;Oct&#x27; then revenue end) as Oct_Revenue, sum(case month when &#x27;Nov&#x27; then revenue end) as Nov_Revenue, sum(case month when &#x27;Dec&#x27; then revenue end) as Dec_Revenuefrom Departmentgroup by id CASE WHEN condition1 THEN result1 END 1234567891011121314151617181920212223242526CASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 WHEN conditionN THEN resultN ELSE resultEND;SELECT CustomerName, City, CountryFROM CustomersORDER BY(CASE WHEN City IS NULL THEN Country ELSE CityEND);CASE case_value WHEN when_value THEN statement_list [WHEN when_value THEN statement_list] ... [ELSE statement_list]END CASECASE WHEN search_condition THEN statement_list [WHEN search_condition THEN statement_list] ... [ELSE statement_list]END CASE 197. Rising Temperature 12345678SELECT w1.id AS &#x27;Id&#x27;FROM weather as w1 JOIN weather as w2 ON DATEDIFF(w1.recordDate, w2.recordDate) = 1AND w1.Temperature &gt; w2.Temperature; concat_ws 123456SELECT CONCAT_WS(&quot;-&quot;, &quot;SQL&quot;, &quot;Tutorial&quot;, &quot;is&quot;, &quot;fun!&quot;) AS ConcatenatedString;# output:## ConcatenatedString# SQL-Tutorial-is-fun! EVT Topic ## Data Warehouse 建模： 说白了就是表字段设计 了解业务场景 平台维度 平台的收入，平台订单完成率 平台订单数量变化 最近7日，15日，30日，60日，90日 建模 主题确定，维度退化，轻度聚合 如何把建模说的高大上一些： atlas jdbc会把tinyint 认为是java.sql.Types.BIT，然后sqoop就会转为Boolean了。 在连接上加上一句话tinyInt1isBit=false Mysql中存在tinyint(1)时，在数据导入到HDFS时，该字段默认会被转化为boolean数据类型。导致数据内容丢失。 tinyInt1isBit=false sqoop从mysql导入hive中tinyint类型变成了boolean类型的问题 1mysql.server restart --init-file=/Users/blair/init.sql Reference hive lateral view 与 explode详解 w3schools sql Mysql自定义变量的使用 常见的SQL面试题：经典50题 - 知乎 Spark SQL 和 传统 SQL 的区别 MySQL root密码忘记，原来还有更优雅的解法！ hive 行转列 lateral view explode Hive Lateral view介绍 缓慢变化维的10种方式","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://www.iequa.com/tags/SQL/"}]},{"title":"Airflow Review 1","slug":"dataware/airflow-1","date":"2021-01-24T01:07:21.000Z","updated":"2021-06-20T04:12:28.287Z","comments":true,"path":"2021/01/24/dataware/airflow-1/","link":"","permalink":"http://www.iequa.com/2021/01/24/dataware/airflow-1/","excerpt":"","text":"hello world 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# -*- coding: utf-8 -*-import airflowfrom airflow import DAGfrom airflow.operators.bash import BashOperatorfrom airflow.operators.python import PythonOperatorfrom datetime import timedelta#-------------------------------------------------------------------------------# these args will get passed on to each operator# you can override them on a per-task basis during operator initializationdefault_args = &#123; &#x27;owner&#x27;: &#x27;jifeng.si&#x27;, &#x27;depends_on_past&#x27;: False, &#x27;start_date&#x27;: airflow.utils.dates.days_ago(2), &#x27;email&#x27;: [&#x27;779844881@qq.com&#x27;], &#x27;email_on_failure&#x27;: False, &#x27;email_on_retry&#x27;: False, &#x27;retries&#x27;: 1, &#x27;retry_delay&#x27;: timedelta(minutes=5), # &#x27;queue&#x27;: &#x27;bash_queue&#x27;, # &#x27;pool&#x27;: &#x27;backfill&#x27;, # &#x27;priority_weight&#x27;: 10, # &#x27;end_date&#x27;: datetime(2016, 1, 1), # &#x27;wait_for_downstream&#x27;: False, # &#x27;dag&#x27;: dag, # &#x27;adhoc&#x27;:False, # &#x27;sla&#x27;: timedelta(hours=2), # &#x27;execution_timeout&#x27;: timedelta(seconds=300), # &#x27;on_failure_callback&#x27;: some_function, # &#x27;on_success_callback&#x27;: some_other_function, # &#x27;on_retry_callback&#x27;: another_function, # &#x27;trigger_rule&#x27;: u&#x27;all_success&#x27;&#125;#-------------------------------------------------------------------------------# dagdag = DAG( &#x27;example_hello_world_dag&#x27;, default_args=default_args, description=&#x27;my first DAG&#x27;, schedule_interval=timedelta(days=1))#-------------------------------------------------------------------------------# first operatordate_operator = BashOperator( task_id=&#x27;date_task&#x27;, bash_command=&#x27;date&#x27;, dag=dag)#-------------------------------------------------------------------------------# second operatorsleep_operator = BashOperator( task_id=&#x27;sleep_task&#x27;, depends_on_past=False, bash_command=&#x27;sleep 5&#x27;, dag=dag)#-------------------------------------------------------------------------------# third operatordef print_hello(): return &#x27;Hello world!&#x27;hello_operator = PythonOperator( task_id=&#x27;hello_task&#x27;, python_callable=print_hello, dag=dag)#-------------------------------------------------------------------------------# dependenciessleep_operator.set_upstream(date_operator)hello_operator.set_upstream(date_operator) 12# ~/airflow/dags [19:59:51]➜ python hello_world.py airflow tasks list example_hello_world_dag 1airflow tasks list example_hello_world_dag 12345678➜ airflow tasks list example_hello_world_dag[2021-01-24 20:00:02,162] &#123;dagbag.py:440&#125; INFO - Filling up the DagBag from /Users/blair/airflow/dags[2021-01-24 20:00:02,200] &#123;example_kubernetes_executor_config.py:174&#125; WARNING - Could not import DAGs in example_kubernetes_executor_config.py: No module named &#x27;kubernetes&#x27;[2021-01-24 20:00:02,200] &#123;example_kubernetes_executor_config.py:175&#125; WARNING - Install kubernetes dependencies with: pip install apache-airflow[&#x27;cncf.kubernetes&#x27;]date_taskhello_tasksleep_task 2.1 测试 date_task 1➜ airflow tasks test example_hello_world_dag date_task 20210124 2.2 测试 hello_task 1234567AIRFLOW_CTX_DAG_EMAIL=779844881@qq.comAIRFLOW_CTX_DAG_OWNER=blair.chanAIRFLOW_CTX_DAG_ID=example_hello_world_dagAIRFLOW_CTX_TASK_ID=hello_taskAIRFLOW_CTX_EXECUTION_DATE=2021-01-24T00:00:00+00:00[2021-01-24 20:05:03,473] &#123;python.py:118&#125; INFO - Done. Returned value was: Hello world![2021-01-24 20:05:03,480] &#123;taskinstance.py:1142&#125; INFO - Marking task as SUCCESS. dag_id=example_hello_world_dag, task_id=hello_task, execution_date=20210124T000000, start_date=20210124T120503, end_date=20210124T120503 2.3 测试 sleep_task 12345 /var/folders/58/kj1q13hs3j52hwvj3t2g2plh0000gp/T[2021-01-24 20:04:30,923] &#123;bash.py:158&#125; INFO - Running command: sleep 5[2021-01-24 20:04:30,929] &#123;bash.py:169&#125; INFO - Output:[2021-01-24 20:04:35,942] &#123;bash.py:177&#125; INFO - Command exited with return code 0[2021-01-24 20:04:35,972] &#123;taskinstance.py:1142&#125; INFO - Marking task as SUCCESS. dag_id=example_hello_world_dag, task_id=sleep_task, execution_date=20210124T000000, start_date=20210124T120430, end_date=20210124T120435 airflow如何设置Dag和Dag之间的依赖啊？ 可以自己写一个脚本来检测父dag的状态，来达到dag之间的依赖 ExternalTaskSensor 高级特性，监工 Reference AirFlow高阶，两个启动时间不同DAG中的任务依赖关联demo 【 airflow 实战系列】 基于 python 的调度和监控工作流的平台 用户画像—Airflow作业调度(ETL) 闲聊调度系统 Apache Airflow 十三、回填任务–BackfillJob ETL principles Airflow 使用及原理分析 data engineer 使用luigi 还是 airflow比较好？ airflow官方教程——一个简单案例 Airflow 中文文档 AirFlow常用命令 [AirFlow]AirFlow使用指南三 第一个DAG示例","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"Airflow","slug":"Airflow","permalink":"http://www.iequa.com/tags/Airflow/"}]},{"title":"DataWare Business Review 3","slug":"dataware/dataware-business-3","date":"2021-01-22T01:07:21.000Z","updated":"2021-06-22T06:02:29.734Z","comments":true,"path":"2021/01/22/dataware/dataware-business-3/","link":"","permalink":"http://www.iequa.com/2021/01/22/dataware/dataware-business-3/","excerpt":"","text":"事件主题 - 还款流水, 授信流水, 支用流水, 放款流水, 还款计划. 在后端资产的模块. DWD - 这些流水表都在这层. (1). 授信支用后，就会产生借据号 个贷业务数据，包括申请ID，机构代号，贷款合同编号，担保类型，贷款期限，贷款起期，贷款止期，诉讼标志，逾期利息利率，还款帐号，终审金额，贷款金额，贷款余额，经办人编号，结清日期，还款方式，客户姓名，客户种类，客户性质，客户分类，证件类型，证件号码，流水号，借据号，借据金额，借据余额，借据利息余额，借据利率，借据起期，借据止期，借款状态，逾期天数，结息方式，放款账号，放款账户户名用途详情，还款账户户名，还款账户账号等。 8大主题模型 No. 主题名称 主题描述 1. 客户 (USER) 当事人, 用户信息, 非常多, 人行征信信息， 个人资产信息 2. 机构 (ORG) 线下有哪些团队, 浙江区，团队长，客户经理， 有好几百. 只有维度表 3. 产品 (PRD) 签协议 产生 产品, 业务流程, 只有维度表 产品维度表： 产品编号(分好几级), 产品名称, dim_code, dim_name， 上架， 下架京东金条， code， 展示给财务 4. 渠道 (CHL) 5. 事件 (EVT) 1. 同业借款(50~200亿) 2. 客户授信 3. 支用 4. 放款 5. 支付 6. 还款 (支付流水总量有1.5亿) , 所以基本每天全量全量关联 6. 协议 (AGR) 合约 7. 营销 (CAMP) 8. 财务 (RISK) 9. 风险 (FINANCE) 风险部 营销之后的，商务经理和渠道，谈下来之后， 后端 渠道， 资产， 账务 ODS 还款流水表： 还款流水号 借据号 还款日 还款金额 还款金额(本金，利息，手续费), user_id product_id … 还款流水 （平均日增40~50W， 偶尔100~200W也是有的）, 年上亿 ODS 支付流水： 支付流水有客户的银行卡，身份证，走的什么支付通道等信息， 支付通道是哪家银行， 客户通过不同银行支付到公司账户 微信，支付宝 也有 ODS 冲正还款流水表（1W内）： 还款流水号 借据号 还款日 还款金额 还款金额(本金，利息，手续费), user_id product_id … DWD： 还款事实表 (Join from 还款流水表 &amp; 冲正还款流水表) 还款事实表 - 借据号(可以关联到用户和产品), 还款流水号, 还款金额(本金，利息，手续费), user_id, product_id, custom_id 下游可能看，借据粒度，还了多少钱， 聚合 每天拉昨天新增的流水 DWS：按照借据号 Group, 借据号是最小的粒度了. 数据量很大，但是这里有一个小技巧: 金额加上去. SQL Data Skew DW Model table name: 主题域_dwd_table EVT Topic 12345678910111213141516171819202122232425262728293031323334授信流水, 几十万，上百万支用流水, 放款流水, 还款流水, 日增 几十万 ~ 百万， 利用sqoop抽取新增一天放款上亿，回收也是上亿客户 4000W / 拮据 800W&gt; 360: 300W+&gt; 分期乐： 200W+&gt; 借呗： 100W+&gt; 尊享贷： 10W+&gt; JD： 50W+&gt; 百度，翼支付，小米，滴滴不同渠道，产品，利率段放款金额，不同区间的，用户数支付流水 1.5亿+ &amp; 还款流水 根据流水号 Join 存快照数据不一致有没有遇到过，怎么解决的。回答：遇到过，最常见，同一个指标，多个人多个团队出，口径不一致；或者相同逻辑多个地方维护，复制粘贴，改一个地方另一个地方忘记改追问解决办法：指标体系，复用数据：按照业务线将一个业务线设计到的所有维度和指标统一建模到一张hive表，上层所有应用或者对商分暴露的表都是同源的，且直接取不用再计算规则引擎，复用逻辑：相同的字段加工逻辑抽离到规则引擎中进行配置，保证一处修改，处处运行分隔符 \\n -&gt; 001通过mysql自动识别Hive表结构hadoop - Sqoop导入将TINYINT转换为BOOLEANjdbc:mysql://127.0.0.1:3306/nfl?tinyInt1isBit=falsecomment &#x27;任务日志-临时表，用于将数据通过动态分区载入ods_task_log中&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\t&#x27; LINES TERMINATED BY &#x27;\\n&#x27; STORED AS TEXTFILE; load data local inpath &#x27;/kkb/datas/gamecenter/ods_task_log.txt&#x27; overwrite into table tmp_ods_task_log; set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nostrict; set hive.exec.max.dynamic.partitions.pernode=1000; insert overwrite table ods_task_log partition(part_date) select plat_id,server_id,channel_id,user_id,role_id,role_name,event_time,task_type,task_id,cost_time,op_type,level_limit,award_exp,award_monetary,award_item,death_count,award_attribute, from_unixtime(event_time,&#x27;yyyy-MM-dd&#x27;) as part_date from tmp_ods_task_log; &quot; 1234567891011a = [ &#123;&quot;row_id&quot;: 2, &quot;text&quot;: &quot;t1&quot;&#125;, &#123;&quot;row_id&quot;: 1, &quot;text&quot;: &quot;t2&quot;&#125;, &#123;&quot;row_id&quot;: 2, &quot;text&quot;: &quot;t3&quot;&#125;, &#123;&quot;row_id&quot;: 2, &quot;text&quot;: &quot;t1&quot;&#125;]ret = pydash.group_by(a, [&quot;row_id&quot;])ret# &#123;2: [&#123;&#x27;row_id&#x27;: 2, &#x27;text&#x27;: &#x27;t1&#x27;&#125;, &#123;&#x27;row_id&#x27;: 2, &#x27;text&#x27;: &#x27;t3&#x27;&#125;], 1: [&#123;&#x27;row_id&#x27;: 1, &#x27;text&#x27;: &#x27;t2&#x27;&#125;]&#125; pydash_groyp_by Reference 浅谈银行数据仓库：金融主题层建设篇 数据仓库-建模实践 独一无二的数据仓库建模指南（升级版） 大数据项目之离线数仓2.0项目实战教程 胡明昊 - 围绕数据建模，谈金融数仓建设的核心 TeraData金融数据模型（银行十大主题划分） 知乎：数据仓库架构及数据模型介绍 Wechat: 数据分析师成长体系漫谈 - 数仓模型设计 大白话系列：HIVE中数据倾斜原理及优化方案","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Spark - troubleshooting","slug":"spark/spark-summary-3-trouble-shooting","date":"2021-01-20T23:07:21.000Z","updated":"2021-06-20T04:12:28.357Z","comments":true,"path":"2021/01/21/spark/spark-summary-3-trouble-shooting/","link":"","permalink":"http://www.iequa.com/2021/01/21/spark/spark-summary-3-trouble-shooting/","excerpt":"","text":"Spark No. Title Desc 1. coalesce 无论是在RDD中还是DataSet，默认情况下coalesce不会产生shuffle，此时通过coalesce创建的RDD分区数小于等于父RDD的分区数。 2. repartition 1）增加分区数 - repartition触发shuffle，shuffle的情况下可以增加分区数.- coalesce默认不触发shuffle，即调用该算子增加分区数，实际情况是分区数仍是当前的分区数. 3. union val rdd4 = rdd1.union(rdd3) - res: Array[Int] = Array(1,2,3,4,5,6,7,8,9,12,14,16,18) 多数情况: 通过union生成的RDD的分区数为父RDD的分区数之和 4. Join join(otherDataset, [numTasks])是连接操作，将输入数据集(K,V)和另外一个数据集(K,W)进行Join， 得到(K, (V,W))；该操作是对于相同K的V和W集合进行笛卡尔积 操作，也即V和W的所有组合 val rdd5 = rdd0.join(rdd0) res3: Array[(Char,(Int, Int))] = Array((d,(9,8)), (c,(6,6)), (c,(6,7))) rdd 算子： leftOuterJoin, fullOuterJoin, … spark sql 之join等函数用法 5. cogroup cogroup(otherDataset, [numTasks])是将输入数据集(K, V)和另外一个数据集(K, W)进行cogroup，得到一个格式为(K, Seq[V], Seq[W])的数据集 val rdd6 = rdd0.cogroup(rdd0)res: Array[(Int, (Iterable[Int], Iterable[Int]))] = Array((1,(ArrayBuffer(1, 2, 3),ArrayBuffer(1, 2, 3))), (2,(ArrayBuffer(1, 2, 3),ArrayBuffer(1, 2, 3)))) spark的union和join操作演示 No. Title Author Link &amp; Solutions 0. GROUPING SETS spark.sql.files.maxPartitionBytes 默认128M, 单个分区读取的最大文件大小 （对于大部分的Parquet压缩表来说,注意压缩要可分割lzo，这个默认设置其实会导致性能问题）可以通过设置spark.sql.files.maxPartitionBytes 来分割每个task 的输入在Hadoop里，任务的并发默认是以hdfs block为单位的，而Spark里多了一种选择，即以RowGroup为基本单位: spark 处理parquet 文件时，一个row group 只能由一个task来处理row group是需要调优的spark参数,重要一点,就是控制任务的并发度:set parquet.block.size=16Mset spark.sql.files.maxPartitionBytes=16M 1. 较多的 DataFrame join 操作时 调大此参数：spark.sql.autoBroadcastJoinThreshold，默认10M，可设置为 100M 2. 华为开发者SparkCore知乎大数据SparkSQL 开发者指南 &gt; 组件成功案例 &gt; Spark &gt; 案例10：Spark Core调优 &gt; 经验总结 Spark基础：Spark SQL调优 1. Cache 缓存 1.1 spark.catalog.cacheTable(“t”) 或 df.cache() Spark SQL会把需要的列压缩后缓存，避免使用和GC的压力 1.2 spark.sql.inMemoryColumnarStorage.compressed 默认true 1.3 spark.sql.inMemoryColumnarStorage.batchSize 默认10000 控制列缓存时的数量，避免OOM风险。 引申要点： 行式存储 &amp; 列式存储 优缺点 2. 其他配置 2.1 spark.sql.autoBroadcastJoinThreshold 2.2 spark.sql.shuffle.partitions 默认200，配置join和agg的时候的分区数 2.3 spark.sql.broadcastTimeout 默认300秒，广播join时广播等待的时间 2.4 spark.sql.files.maxPartitionBytes 默认128MB，单个分区读取的最大文件大小 2.5 spark.sql.files.openCostInBytes parquet.block.size3. 广播 hash join - BHJ 3.1 当系统 spark.sql.autoBroadcastJoinThreshold 判断满足条件，会自动使用BHJ 华为云Stack全景图 &gt; 开发者指南 &gt; SQL和DataFrame调优 &gt; Spark SQL join优化 spark不会 注意spark不会确保每次选择广播表都是正确的，因为有的场景比如 full outer join 是不支持BHJ的。手动指定广播: broadcast(spark.table(“src”)).join(spark.table(“records”), “key”).show() 开发小知识 0. NULL, AVG/NOT IN select avg(amount) as a_mount from orders amount (150, 150, null) avg = 150 不是 100 select * from stores where tag not in (&quot;&quot;) 1. NVL(expr1,expr2) NVL(expr1,expr2) NVL(UnitsOnOrder,0) other simlar: IFNULL(UnitsOnOrder, 0) / coalesce(null, “”) NULLIF(exp1,expr2)函数的作用是如果exp1和exp2相等则返回空(NULL) 2. IF( expr1 , expr2 , expr3 ) expr1 的值为 TRUE，则返回值为 expr2 expr1 的值为FALSE，则返回值为 expr3 3. IFNULL( expr1 , expr2 ) if expr1 not null, return expr1 常见问题 3. 定位性能问题对应的sql 1. spark driver log 看 执行慢的stage（99%） 2. spark ui 上看 该stage 的task 执行完成比率3. spark ui 上看 该stage 对应的 continer id 和 所属job4. spark ui 上看 sql 的执行计划 和 执行计划图，最终定位到是哪段sql 4. 一道sql的题，一张表，用户id和登录日期，查找连续两天登陆的用户 left join tb_log b on a.uid = b.uid on a.uid = b.uid 5. 写sql。求一个省份下的uv最高的城市 主要考察窗口函数 select province,city,row_nnumber()over(partition by province order by uv desc ) rank 6. 数据不一致遇到过吗，是什么原因? 7. 知道什么是 whole stage codengen吗 面向接口编程太耗时间，主要是方法递归调用，虚函数调用 可以将一个stage的所有task整理成一个方法，并且生成动态字节码 并结合 8. spark 3.0 特性 待学 9. wordCount lines=sc.textFile(path) words = lines.flatMap(lambda x: x.split(’ ')) wco = words.map(lambda x: (x, 1))word_count = wco.reduceByKey(add) 123456789101112131415161718192021222324252627282930313233343536373839404142434445SELECT factory, department, SUM(quantity)FROM productionGROUP BY GROUPING SETS(factory, department)ORDER BY factory, departmentHDFS:205.2 M part-00000-30ceee1e-2ed6-4239-8a6b-45fc6cbf1ef6.c000205.2 M part-00001-30ceee1e-2ed6-4239-8a6b-45fc6cbf1ef6.c0003.8 M part-00002-30ceee1e-2ed6-4239-8a6b-45fc6cbf1ef6.c000共三个数据文件，如果设置参数 spark.sql.files.maxPartitionBytes为64M，会把数据分8个块:##part-00000 四块range: 0-67108864 ; range: 67108864-134217728; range: 134217728-201326592range: 201326592-215189723##part-00001 四块range: 0-67108864 ; range: 67108864-134217728; range: 134217728-201326592range: 201326592-215167669##part-00002 一块range: 0-4002630启动7个task：理论上有6个task分别负责每个64M的块数据，然后最后一个task负责part-00000，part-00001剩余的不足64M的两个块以及part-00002分区数确实增加了，由四个增加到了7个，但是新增的3个却没处理什么数据，大部分的数据还是4个partition在处理，所以还是很慢~~~~task数增加了，但是数据并没有均分到每个task，为什么呢?spark 在处理parquet 文件时，一个row group 只能由一个task 来处理，在hdfs 中一个row group 可能横跨hdfs block ，那么spark是怎么保证一个task只处理一个 row group 的呢？检查table_a发现，生成table_a时，parquet.block.size 用的默认值128M ，这样就导致一个row group 有128M 的大小。parquet.block.size 是可以依据实际使用情况来调优的，对于做多维分析表，可以设置稍小一点。最终 经过调试设置parquet.block.size 为16M ；设置spark.sql.files.maxPartitionBytes为16M读取hdfs文件时，并行了22个task，并且每个task处理数据均匀parquet.block.size所控制的parquet row group大小是一个需要调优的spark参数。其中重要一点，就是控制任务的并发度。在Hadoop里，任务的并发默认是以hdfs block为单位的，而Spark里多了一种选择，即以RowGroup为基本单位。在调用HiveContext.read.parquet(path)时，会触发ParquetRelation2对象生成SqlNewHadoopRDD对象，并覆盖其中getPartitions()方法60min -&gt; 3mins No. Title Flag 0. kaike - sparkSQL底层实现原理spark.sql.shuffle.partitions和 spark.default.parallelism 的区别SparkSQL并行度参数设置方法 1. B站 我爱喝假酒 - 性能调优 2. Spark性能调优之合理设置并行度 (稍有误)， Spark实践 – 性能优化基础 3. spark.defalut.parallelism 默认是没有值的，如设置值为10，是在shuffle/窄依赖 的过程才会起作用（val rdd2 = rdd1.reduceByKey(_+_) //rdd2的分区数就是10，rdd1的分区数不受这个参数的影响） 4. 如果读取的数据在HDFS上，增加block数，默认情况下split与block是一对一的，而split又与RDD中的partition对应，所以增加了block数，也就提高了并行度 5. reduceByKey的算子指定partition的数量 val rdd2 = rdd1.reduceByKey(_+_,10) val rdd3 = rdd2.map.filter.reduceByKey(_+_) 6. val rdd3 = rdd1.join（rdd2） rdd3里面partiiton的数量是由父RDD中最多的partition数量来决定，因此使用join算子的时候，增加父RDD中partition的数量 7. 由于Spark SQL所在stage的 并行度无法手动设置如果数据量较大，并且此stage中后续的transformation操作有着复杂的业务逻辑，而Spark SQL自动设置的task数量很少，这就意味着每个task要处理为数不少的数据量，然后还要执行非常复杂的处理逻辑，这就可能表现为第一个有Spark SQL的stage速度很慢，而后续的没有Spark SQL的stage运行速度非常快。 RDD 属性 A list of partitions A function for computing each split A list of dependencies on other RDDs Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) Optionally, a list of preferred locations to compute each split on (block locations for an HDFS file) very good Spark分区 partition 详解 申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task No. Title Flag 1. 一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 ❎ 2. 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 ❎ 3. RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 ❎ 4. 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 ❎ 5. 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置 ❎ 尽量保证每轮Stage里每个task处理的数据量&gt;128M Reference 数仓开发需要了解的5大SQL分析函数 干货：一文读懂数据仓库设计方案 | 使用Hive窗口函数替换union all处理分组汇总（小计，总计） Spark实践 – 性能优化基础 Spark项目实战-troubleshooting之控制shuffle reduce端缓冲大小以避免OOM 结合源码谈谈 - 通过spark.default.parallelism谈Spark并行度 谈谈spark.sql.shuffle.partitions和 spark.default.parallelism 的区别及spark并行度的理解","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"python 的 decorator & getattr() 函数","slug":"python/language/py_getattr","date":"2021-01-20T02:07:21.000Z","updated":"2021-06-22T06:52:37.046Z","comments":true,"path":"2021/01/20/python/language/py_getattr/","link":"","permalink":"http://www.iequa.com/2021/01/20/python/language/py_getattr/","excerpt":"decorator 由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。 getattr(object, name[,default]) 获取对象object的属性或者方法，如果存在打印出来，如果不存在，打印出默认值，默认值可选.","text":"decorator 由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。 getattr(object, name[,default]) 获取对象object的属性或者方法，如果存在打印出来，如果不存在，打印出默认值，默认值可选. 需要注意的是，如果返回的对象的方法，返回的是方法的内存地址，如果需要运行这个方法，可以在后面添加一对括号. 1. decorator 由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。 123456789101112&gt;&gt;&gt; def now():... print(&#x27;2015-3-25&#x27;)...&gt;&gt;&gt; f = now&gt;&gt;&gt; f()2015-3-25def log(func): def wrapper(*args, **kw): print(&#x27;call %s():&#x27; % func.__name__) return func(*args, **kw) return wrapper 如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本： 1234567def log(text): def decorator(func): def wrapper(*args, **kw): print(&#x27;%s %s():&#x27; % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 这个3层嵌套的decorator用法如下： 1234@log(&#x27;execute&#x27;)def now(): print(&#x27;2015-3-25&#x27;) 执行结果如下： 123&gt;&gt;&gt; now()execute now():2015-3-25 2. Callable 3. getattr Reference 使用Future Callable 函数式编程-装饰器 python的getattr（）函数","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"getattr","slug":"getattr","permalink":"http://www.iequa.com/tags/getattr/"}]},{"title":"SparkSQL - Parquet","slug":"spark/spark-summary-7-sql-optimization-parquet","date":"2021-01-12T07:28:21.000Z","updated":"2021-06-20T04:12:28.347Z","comments":true,"path":"2021/01/12/spark/spark-summary-7-sql-optimization-parquet/","link":"","permalink":"http://www.iequa.com/2021/01/12/spark/spark-summary-7-sql-optimization-parquet/","excerpt":"","text":"二、Parquet的精要介绍 Parquet是列式存储格式的一种文件类型，列式存储有以下的核心优势： a）可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。 b）压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如RunLength Encoding和Delta Encoding）进一步节约存储空间。 c）只读取需要的列，支持向量运算，能够获取更好的扫描性能。 相对于其它的列式存储格式，例如ORC,Parquet主要优势在于支持更为广泛，且对嵌套数据的支持更好。详细的比较可以参考 http://dongxicheng.org/mapreduce-nextgen/columnar-storage-parquet-and-orc/ Reference Parquet文件格式解析 spark读取parquet文件，分配的任务个数 Spark+Parquet分片规则 [看图说话] 基于Spark UI性能优化与调试——初级篇 重要 | Spark分区并行度决定机制","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"sparkSQL","slug":"sparkSQL","permalink":"http://www.iequa.com/tags/sparkSQL/"}]},{"title":"DataWare Review Summary 2","slug":"dataware/dwh-summary-2-interview","date":"2021-01-09T01:07:21.000Z","updated":"2021-06-22T06:04:44.247Z","comments":true,"path":"2021/01/09/dataware/dwh-summary-2-interview/","link":"","permalink":"http://www.iequa.com/2021/01/09/dataware/dwh-summary-2-interview/","excerpt":"","text":"No. 2020年大厂-数据仓库篇 Flag 0. Hive SQL count（distinct）效率问题及优化 set mapreduce.map.memory.mb=48192;set mapreduce.reduce.memory.mb=48192;set mapred.reduce.tasks=1000；select count（distinct account） from…where… 加入distinct，map阶段不能用combine消重，数据输出为（k，v）形式然后在reduce阶段进行消重Hive在处理COUNT这种“全聚合(full aggregates)”计算时，忽略指定的Reduce Task数，而强制使用1 insert overwrite table temp select id，account，count(1) as num from tablename group by id，account； MapReduce流程简单解析 ❎ 1. 手写&quot;连续活跃登陆&quot;等类似场景的sql ❎ 2. left semi join和left join区别? left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过；当右表不存在的时候，左表数据不会显示; 相当于SQL的in语句. left join: 当右表不存在的时候，则会显示NULL ❎ 3. 维度建模 和 范式建模(3NF模型) 的区别? 维度建模是面向分析场景的，主要关注点在于快速、灵活: 星型模型 &amp; 雪花模型 &amp; 星系模型 3NF的最终目的就是为了降低数据冗余，保障数据一致性: (2.1) 原子性 - 数据不可分割 (2.2) 基于第一个条件，实体属性完全依赖于主键 (2.3) 消除传递依赖 - 任何非主属性不依赖于其他非主属性 ❎ 4. 数据漂移如何解决 ? 通常是指ods表的同一个业务日期数据中包含了前一天或后一天凌晨附近的数据或者丢失当天变更的数据，这种现象就叫做漂移，且在大部分公司中都会遇到的场景 1. 多获取后一天的数据，保障数据只多不少 2. 通过多个时间戳字段来限制时间获取相对准确的数据 log_time, modified_time, proc_time modified_time 过滤非当天的数据，这样确保数据不会因为系统问题被遗漏 ❎ 5. 拉链表如何设计，拉链表出现数据回滚的需求怎么解决 ? 拉链表使用的场景：1. 数据量大，且表中部分字段会更新，比如用户地址、产品描述信息、订单状态等等2. 需要查看某一个时间段的历史快照信息3. 变化比例和频率不是很大 6. 以 LEFT JOIN 为例： 谈谈 在使用 LEFT JOIN 时，ON 和 WHERE 过滤条件的区别如下： 1. on 条件是在生成临时表时使用的条件，它不管 on 中的条件是否为真，都会返回左边表中的记录 2. where 条件是在临时表生成好后，再对临时表进行过滤的条件。 ❎ 7. 公共层(CDM:dwd和dws) 和 数据集市层的区别和特点？ 分为dwd层和dws层，主要存放明细事实数据、维表数据 及 公共指标汇总数据，其中明细事实数据、维表数据一般是根据ods层数据加工生成的，公共指标汇总数据一般是基于维表和明细事实数据加工生成的. 采用维度模型方法作为理论基础，更多采用一些维度退化的手段，将维度退化到事实表中，减少事实表和维度表之间的关联。同时在汇总层，加强指标的维度退化，采用更多的宽表化手段构建公共指标数据层. Data Mart: 就是满足特定部门或者用户的需求，按照多维方式存储。面向决策分析的数据立方体 8. 从原理上说一下mpp和mr的区别 ? 1. MPP跑的是SQL,而Hadoop底层处理是MapReduce程序 2. 扩展程度：MPP扩展一般是扩展到100左右,因为MPP始终还是DB,一定要考虑到C(Consistency) ❎ 9. Kimball和Inmon的相同和不同？ Inmon： 不强调事实表和维度表的概念， 类似 3NF ❎ 10. 缓慢变化维（Slowly Changing Dimension）处理方式 ? 1. 重写覆盖 2. 增加新行(注意事实表关联更新) 3. 快照 (每天保留全量的快照数据，通过空间换时间) 4. 历史拉链 (拉链表的处理方式，即通过时间标示当前有效记录) ❎ 11. 数据质量/元数据管理/指标体系建设/数据驱动 略 12. hive的row_number()、rank()和dense_rank()的区别以及具体使用 ❎ 13. Hive窗口函数怎么设置窗口大小？, between 1 preceding and 1 following ✔️ 14. Hive 四个by的区别 15. 怎么验证Hive SQL的正确性 ？ 1. 如果只是校验sql的语法正确性，可以通过explain或者执行一下就可以 16. Hive数据选择的什么压缩格式 ? 17. Hive SQL如何转化MR任务 ? HiveSQL -&gt;AST(抽象语法树) -&gt; QB(查询块) -&gt;OperatorTree（操作树）-&gt;优化后的操作树-&gt;mapreduce任务树-&gt;优化后的mapreduce任务树 18. join操作底层 MR 是怎么执行的？ 根据join对应的key进行分区shuffle，然后执行mapreduce那套流程. 19. Parquet数据格式内部结构? 2020 BAT大厂数据分析面试经验：“高频面经”之数据分析篇 1. Mysql中索引是什么？建立索引的目的？ 2. sql语句执行顺序？ from-on-join-where-group by-avg,sum-having 3. 数据库与数据仓库的区别? 4. OLTP和OLAP的区别？ 5. 行存储和列存储的区别? 行存储：传统数据库的存储方式，同一张表内的数据放在一起，插入更新很快。缺点是每次查询即使只涉及几列，也要把所有数据读取列存储：OLAP等情况下，将数据按照列存储会更高效，每一列都可以成为索引，投影很高效。缺点是查询是选择完成时，需要对选择的列进行重新组装。当你的核心业务是 OLTP 时，一个行式数据库，再加上优化操作，可能是个最好的选择。当你的核心业务是 OLAP 时，一个列式数据库，绝对是更好的选择 6. Hive执行流程？ 7. Hive HDFS HBase区别？ Hbase是Hadoop database，即Hadoop数据库. 它是一个适合于非结构化数据存储的数据库，HBase基于列的而不是基于行的模式. 8. 数仓中ODS、DW、DM(Data Mart) 概念及区别？ 9. 窗口函数是什么？实现原理？ 窗口函数又名开窗函数，属于分析函数的一种。用于解决复杂报表统计需求的功能强大的函数。窗口函数用于计算基于组的某种聚合值，它和聚合函数的不同之处是：对于每个组返回多行，而聚合函数对于每个组只返回一行. 下面列举一些常用窗口函数：1. 获取数据排名的：ROW_NUMBER() RAND() DENSE_RANK() PERCENT_RANK()2. 获取分组内的第一名或者最后一名等：FIRST_VALUE() LAST_VALUE() LEAD() LAG()3. 累计分布：vCUME_DIST() NTH_VALUE() NTILE() 漫谈系列 1. 漫谈系列 - 数仓第一篇NO.1 『基础架构』 2. 漫谈系列 - 数仓第二篇NO.2 『数据模型』 3. 漫谈系列 - 数仓第三篇NO.3 『数据ETL』 4. 漫谈系列 - 数仓第四篇NO.4 『数据应用』 5. 漫谈系列 - 数仓第五篇NO.5 『调度系统』 6. 漫谈系列 - 数仓第六篇NO.6 『数据治理』 7. 漫谈系列 - 漫谈数仓第一篇NO.7 『面试真经』 元数据管理解析以及数据仓库和主数据介绍 No. table_type details 1. 事实表 （1）事务事实表 （2）周期快照事实表 （3）累积快照事实表 2. 维度表 （1）退化维度（DegenerateDimension） （2）缓慢变化维（Slowly Changing Dimensions） Reference SQL 窗口函数的优化和执行 【社招】快手_数据仓库_面试题整理 2020年大厂面试题-数据仓库篇","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Spark Summary 2 - Q&A","slug":"spark/spark-summary-2-common-questions","date":"2021-01-07T23:07:21.000Z","updated":"2021-06-20T04:12:28.350Z","comments":true,"path":"2021/01/08/spark/spark-summary-2-common-questions/","link":"","permalink":"http://www.iequa.com/2021/01/08/spark/spark-summary-2-common-questions/","excerpt":"","text":"good - Spark会把数据都载入到内存么？ Spark No. Title Flag 0. kaike - sparkSQL底层实现原理spark.sql.shuffle.partitions和 spark.default.parallelism 的区别SparkSQL并行度参数设置方法 0. SparkSql - 结构化数据处理 (上) 0. Spark Container Executor task之间的关系 0. Spark 漫画 全面解释Spark企业调优点 0 Spark内核及调优 1. RDD 属性？ 5大属性 ❎ 2. 算子分为哪几类(RDD支持哪几种类型的操作) 1. Transformation （lazy模式）2. Action ❎ 3. 创建rdd的几种方式 ❎ 4. spark运行流程 ❎ 5. Spark中coalesce与repartition的区别 ❎ 6. sortBy 和 sortByKey的区别 ❎ 7. map和mapPartitions的区别 sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect() ❎ 8. 数据存入Redis 优先使用map mapPartitions foreach foreachPartions ? def f(x): print(x) sc.parallelize([1, 2, 3, 4, 5]).foreach(f) ❎ 9. reduceByKey和groupBykey的区别 ❎ 10. cache和checkPoint的比较 : 都是做 RDD 持久化的 ❎ 11. 简述map和flatMap的区别和应用场景 map是对每个元素进行操做，flatmap是对每个元素操做后并压平. ❎ 12. 计算曝光数和点击数 13. 分别列出几个常用的transformation和action算子 ❎ 17. Spark应用执行有哪些模式，其中哪几种是集群模式 ❎ 18. 请说明spark中广播变量的用途 ?（1）broadcast (不能改)（2）accumulator, sc.accumulator(0) 使用广播变量，每个 Executor 的内存中，只驻留一份变量副本，而不是对 每个 task 都传输一次大变量，省了很多的网络传输， 对性能提升具有很大帮助， 而且会通过高效的广播算法来减少传输代价.mapper = &#123;'dog':1&#125;, bc=sc.broadcast(mapper), bc.value: &#123;'dog':1&#125; mapper = &#123;'pig': 3&#125; , bc.unpersist(), sc.broadcast(mapper).value 2.2 rdd-programming-guide.html#broadcast-variables ❎ 20. Spark高频考点: 写出你用过的spark中的算子，其中哪些会产生shuffle过程 1. reduceBykey 2. groupByKey 3. …ByKey ❎ 21. good - Spark学习之路 （三）Spark之RDD 扎心了老铁 ❎ 22. 请写出创建Dateset的几种方式 1. 常用的方式通过sparksession读取外部文件或者数据生成dataset 2. 通过调用createDataFrame生成Dataset df.select(&quot;name&quot;).show()df.select(df['name'], df['age'] + 1).show() df.filter(df['age'] &gt; 21).show() df.groupBy(&quot;age&quot;).count().show() df.createOrReplaceTempView(&quot;people&quot;) sqlDF = spark.sql(&quot;SELECT * FROM people&quot;) sqlDF.show() teenNames = teenagers.rdd.map(lambda p: &quot;Name: &quot; + p.name).collect() ❎ 23. 描述一下 RDD，DataFrame，DataSet 的区别？ DataSet 结合了 RDD 和 DataFrame 的优势，并带来的一个新的概念 Encoder。 当序列化数据时，Encoder 产生字节码与 off-heap 进行交互，可以达到按需访问数据的效果，而不用反序列化整个对象。Spark 尚未提供自定义 Encoder 的 API，可是将来会加入 Apache spark DataFrame &amp; Dataset ❎ 24. 描述一下Spark中stage是如何划分的？描述一下shuffle的概念 ✔️❎ 25. Spark 在yarn上运行需要做哪些关键的配置工作？ 如何kill -个Spark在yarn运行中Application: yarn application -kill &lt;appId&gt; 但是这样会导致端口在一段时间（24小时）内被占用 ❎ 26. 通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？ ❎ 27. RDD中的数据在哪？ 不可变的意思是RDD中的每个分区数据是 only-read RDD要做逻辑分区（这里的分区类似hadoop中的逻辑切片split），每个分区可单独在集群节点计算 ❎ 28. 如果对RDD进行cache操作后，数据在哪里？ 1. 执行cache算子时数据会被加载到各个Executor进程的内存. 2. 第二次使用 会直接从内存中读取而不会区磁盘. ❎ 29. Spark中Partition的数量由什么决定? 答： 和MR一样，但是Spark默认最少有两个分区. ❎ 30. Spark判断Shuffle的依据? ❎ 35. Sparkcontext的作用? ❎ 36. Spark SQL 在 Spark Core 的基础上针对结构化数据处理进行很多优化和改进. ❎ 37. 简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作 ❎ 38. 数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免 ❎ 39. 简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作. ❎ 41. 有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条 42. 现有一文件，格式如下，请用spark统计每个单词出现的次数 ❎ 45. 特别大的数据，怎么发送到excutor中？ Answ： broadcast ❎ 46. spark调优都做过哪些方面？ 要非常具体的场景 ❎ 47. spark任务为什么会被yarn kill掉？ ❎ 48. Spark on Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？ ❎ 49. spark中的cache() persist() checkpoint()之间的区别 1. checkpoint 的 RDD 会被计算两次 2. rdd.persist(StorageLevel.DISK_ONLY), partition 由 blockManager 管理, blockManager stop, cache 到磁盘上 RDD 也会被清空 3. checkpoint 将 RDD 持久化到 HDFS 或本地文件夹, 可以被下一个 driver program 使用. ❎ 50. spark算子调优四：repartition解决SparkSQL低并行度问题 你自己通过spark.default.parallelism参数指定的并行度，只会在没有spark sql的stage中生效 hive表，对应了一个hdfs文件，有20个block;你自己设置了spark.default.parallelish参数为100；你的第一个stage的并行度，是不受你设置的参数控制的，就只有20task 51. very good 多弗朗明哥 -【大数据】Spark性能优化和故障处理 算子调优三：filter与coalesce的配合使用算子调优四：repartition解决SparkSQL低并行度问题算子调优五：reduceByKey本地聚合 123456789101112# Register the DataFrame as a SQL temporary viewdf.createOrReplaceTempView(&quot;people&quot;)sqlDF = spark.sql(&quot;SELECT * FROM people&quot;)sqlDF.show()# +----+-------+# | age| name|# +----+-------+# |null|Michael|# | 30| Andy|# | 19| Justin|# +----+-------+ 1. RDD 属性 A list of partitions A function for computing each split A list of dependencies on other RDDs Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) Optionally, a list of preferred locations to compute each split on (block locations for an HDFS file) very good Spark分区 partition 详解 申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task No. Title Flag 1. 一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 ❎ 2. 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 ❎ 3. RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 ❎ 4. 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 ❎ 5. 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置 ❎ 尽量保证每轮Stage里每个task处理的数据量&gt;128M 2. RDD支持的操作 No. Title Flag 1. Transformation： 现有的RDD通过转换生成一个新的RDD。lazy模式，延迟执行。 map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，union,join, coalesce. ❎ 2. Action： 在RDD上运行计算，并返回结果给驱动程序(Driver)或写入文件系统. reduce，collect，count，first，take，countByKey 及 foreach 等等. ❎ 说明 collect 该方法把数据收集到driver端 Array数组类型, transformation只有遇到action才能被执行. ❎ 当执行action之后，数据类型不再是rdd了，数据就会存储到指定文件系统中，或者直接打印结 果或者收集起来. 3. 创建rdd的几种方式 1.集合并行化创建(有数据) 123val arr = Array(1,2,3,4,5)val rdd = sc.parallelize(arr)val rdd =sc.makeRDD(arr) 2.读取外部文件系统，如hdfs，或者读取本地文件(最常用的方式)(没数据) 1234val rdd2 = sc.textFile(&quot;hdfs://hdp-01:9000/words.txt&quot;)// 读取本地文件val rdd2 = sc.textFile(“file:///root/words.txt”) 3.从父RDD转换成新的子RDD 调用Transformation类的方法，生成新的RDD 5. coalesce, repartition区别 repartition 底层调用的就是 coalesce 方法：coalesce(numPartitions, shuffle = true) repartition 一定会发生 shuffle，coalesce 根据传入的参数来判断是否发生 shuffle 一般情况下增大 rdd 的 partition 数量使用 repartition，减少 partition 数量时使用coalesce 6. sortBy / sortByKey区别 sortBy既可以作用于RDD[K] ，还可以作用于RDD[(k,v)] sortByKey 只能作用于 RDD[K,V] 类型上 sortBy : sortBy(lambda x:x[2],ascending = False) 12345#任务：有一批学生信息表格，包括name,age,score, 找出score排名前3的学生, score相同可以任取students = [(&quot;LiLei&quot;,18,87),(&quot;HanMeiMei&quot;,16,77),(&quot;DaChui&quot;,16,66),(&quot;Jim&quot;,18,77),(&quot;RuHua&quot;,18,50)]rdd_students = sc.parallelize(students)rdd_sorted = rdd_students.sortBy(lambda x:x[2],ascending = False)# [(‘LiLei’, 18, 87), (‘HanMeiMei’, 16, 77), (‘Jim’, 18, 77)] sortByKey : sortByKey().map(lambda x:x[0]) 1234567#任务：按从小到大排序并返回序号, 大小相同的序号可以不同data = [1,7,8,5,3,18,34,9,0,12,8]rdd_data = sc.parallelize(data)rdd_sorted = rdd_data.map(lambda x:(x,1)).sortByKey().map(lambda x:x[0]) # [0, 1, 3, 5, 7, 8, 8, 9, 12, 18, 34] 7. map和mapPartitions的区别 8. 数据存入Redis 优先使用什么算子? foreachPartions map mapPartitions foreach foreachPartions 1234567891011from pyspark import SparkFilespath = os.path.join(tempdir, &quot;test.txt&quot;)with open(path, &quot;w&quot;) as testFile: _ = testFile.write(&quot;100&quot;)sc.addFile(path)def func(iterator): with open(SparkFiles.get(&quot;test.txt&quot;)) as testFile: fileVal = int(testFile.readline()) return [x * fileVal for x in iterator]sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()[100, 200, 300, 400] 使用 foreachPartions map mapPartitions 是转换类的算子， 有返回值 写mysql,redis 的链接 pyspark.html 12345678910111213# foreach(f)[source]# Applies a function to all elements of this RDD.def f(x): print(x)sc.parallelize([1, 2, 3, 4, 5]).foreach(f)foreachPartition(f)[source]#Applies a function to each partition of this RDD.def f(iterator): for x in iterator: print(x)sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f) 9. reduceByKey和groupBykey区别 reduceByKey会传一个聚合函数， 至关于 groupByKey + mapValues reduceByKey 会有一个分区内聚合，而groupByKey没有 最核心的区别 结论：reduceByKey有分区内聚合，更高效，优先选择使用reduceByKey 10. cache和checkPoint的比较 都是作 RDD 持久化的 1.缓存，是在触发action以后，把数据写入到内存或者磁盘中。不会截断血缘关系 （设置缓存级别为memory_only：内存不足，只会部分缓存或者没有缓存，缓存会丢失,memory_and_disk :内存不足，会使用磁盘） 2.checkpoint 也是在触发action以后，执行任务。单独再启动一个job，负责写入数据到hdfs中。（把rdd中的数据，以二进制文本的方式写入到hdfs中，有几个分区，就有几个二进制文件） 3.某一个RDD被checkpoint以后，他的父依赖关系会被删除，血缘关系被截断，该RDD转换成了CheckPointRDD，之后再对该rdd的全部操做，都是从hdfs中的checkpoint的具体目录来读取数据。缓存以后，rdd的依赖关系仍是存在的。 checkpoint sc.setCheckpointDir(&quot;/Users/xulijie/Documents/data/checkpoint&quot;) val pairs = sc.parallelize(data, 3) pairs.checkpoint Cache Cache(): 运算时间很长或运算量太大才能得到的 RDD，computing chain 过长或依赖其他 RDD 很多的 RDD. df2.cache() rdd2.cache() cache 机制是每计算出一个要 cache 的 partition 就直接将其 cache 到内存了。但 checkpoint 没有使用这种第一次计算得到就存储的方法，而是等到 job 结束后另外启动专门的 job 去完成 checkpoint 。 也就是说需要 checkpoint 的 RDD 会被计算两次。因此，在使用 rdd.checkpoint() 的时候，建议加上 rdd.cache()， 这样第二次运行的 job 就不用再去计算该 rdd 了，直接读取 cache 写磁盘。 Hadoop vs Spark 区别 Spark比MapReduce运行速度快的原因主要有以下几点： task启动时间比较快，Spark是fork出线程；而MR是启动一个新的进程； 更快的shuffles，Spark只有在shuffle的时候才会将数据放在磁盘，而MR却不是。 更快的工作流：典型的MR工作流是由很多MR作业组成的，他们之间的数据交互需要把数据持久化到磁盘才可以；而Spark支持DAG以及pipelining，在没有遇到shuffle完全可以不把数据缓存到磁盘。 缓存：虽然目前HDFS也支持缓存，但是一般来说，Spark的缓存功能更加高效，特别是在SparkSQL中，我们可以将数据以列式的形式储存在内存中。 所有的这些原因才使得Spark相比Hadoop拥有更好的性能表现；在比较短的作业确实能快上100倍，但是在真实的生产环境下，一般只会快 2.5x ~ 3x！ JVM 的优化: Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，基于进程的操作。而 Spark 每次 MapReduce 操作是基于线程的，Spark 只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多。 Reference very good 多弗朗明哥 -【大数据】Spark性能优化和故障处理 good - Spark分区 partition 详解 good - 2020大数据/数仓/数开面试题真题总结(附答案) 2020大数据/数仓/数开面试题真题总结(附答案) spark中的cache() persist() checkpoint()之间的区别 airflow的使用方法 other: 剖析Spark数据分区之Hadoop分片","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Functions","slug":"spark/spark-review-functions","date":"2021-01-07T12:07:21.000Z","updated":"2021-06-22T05:41:29.731Z","comments":true,"path":"2021/01/07/spark/spark-review-functions/","link":"","permalink":"http://www.iequa.com/2021/01/07/spark/spark-review-functions/","excerpt":"Spark RDD Feature","text":"Spark RDD Feature spark 学习笔记 sample 算子 12有放回取样0.001%data.sample(true,0.00001).collect().foreach(println) Reference","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Practice","slug":"spark/python-spark-practice","date":"2021-01-05T23:07:21.000Z","updated":"2021-06-22T05:30:26.255Z","comments":true,"path":"2021/01/06/spark/python-spark-practice/","link":"","permalink":"http://www.iequa.com/2021/01/06/spark/python-spark-practice/","excerpt":"good - Spark会把数据都载入到内存么？","text":"good - Spark会把数据都载入到内存么？ 1. Spark Functions 1.1 count, first 123456lines.= sc.textFile(&quot;file:///home/blair/../input.txt&quot;)lines.count()lines.first()lines.take(3) 1.2 filter 1234567pythonLines = lines.filter(lambda line: &quot;Python&quot; in line)# 另一种写法def hasPython(line): return &quot;Python&quot; in linepythonLines = lines.filter(hasPython) 1.3 sc init 1234567from pyspark import SparkConf, SparkContext#conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;My App&quot;)#sc = SparkContext(conf = conf)# sc 默认就有了sc.stop() 1.4 avg, 12345678910data = [1,5,7,10,23,20,6,5,10,7,10]rdd_data = sc.parallelize(data)s = rdd_data.reduce(lambda x,y: x+y+0.0)n = rdd_data.count()avg = s/nprint(&quot;average:&quot;,avg) 1.5 求众数 1234567891011121314data = [1,5,7,10,23,20,7,5,10,7,10]rdd_data = sc.parallelize(data)rdd_count = rdd_data.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)max_count = rdd_count.map(lambda x:x[1]).reduce(lambda x,y: x if x&gt;=y else y)print(max_count)rdd_mode = rdd_count.filter(lambda x:x[1]==max_count).map(lambda x:x[0])mode = rdd_mode.reduce(lambda x,y:x+y+0.0)/rdd_mode.count()print(&quot;mode:&quot;,mode) 1.6 TopN 123456789#任务：有一批学生信息表格，包括name,age,score, 找出score排名前3的学生, score相同可以任取students = [(&quot;LiLei&quot;,18,87),(&quot;HanMeiMei&quot;,16,77),(&quot;DaChui&quot;,16,66),(&quot;Jim&quot;,18,77),(&quot;RuHua&quot;,18,50)]n = 3rdd_students = sc.parallelize(students)rdd_sorted = rdd_students.sortBy(lambda x:x[2],ascending = False)students_topn = rdd_sorted.take(n)print(students_topn) [(‘LiLei’, 18, 87), (‘HanMeiMei’, 16, 77), (‘Jim’, 18, 77)] 1.7 排序并返回序号 1234567891011#任务：按从小到大排序并返回序号, 大小相同的序号可以不同data = [1,7,8,5,3,18,34,9,0,12,8]rdd_data = sc.parallelize(data)rdd_sorted = rdd_data.map(lambda x:(x,1)).sortByKey().map(lambda x:x[0]) # [0, 1, 3, 5, 7, 8, 8, 9, 12, 18, 34]rdd_sorted_index = rdd_sorted.zipWithIndex()print(rdd_sorted_index.collect()) output: 1234567891011[(0, 0), (1, 1), (3, 2), (5, 3), (7, 4), (8, 5), (8, 6), (9, 7), (12, 8), (18, 9), (34, 10)] 1.8 二次排序 12345#任务：有一批学生信息表格，包括name,age,score#首先根据学生的score从大到小排序，如果score相同，根据age从大到小students = [(&quot;LiLei&quot;,18,87),(&quot;HanMeiMei&quot;,16,77),(&quot;DaChui&quot;,16,66),(&quot;Jim&quot;,18,77),(&quot;RuHua&quot;,18,50)]rdd_students = sc.parallelize(students) 1234567891011121314%%writefile student.py#为了在RDD中使用自定义类，需要将类的创建代码其写入到一个文件中，否则会有序列化错误class Student: def __init__(self,name,age,score): self.name = name self.age = age self.score = score def __gt__(self,other): if self.score &gt; other.score: return True elif self.score==other.score and self.age&gt;other.age: return True else: return False 1234567891011from student import Studentrdd_sorted = rdd_students \\ .map(lambda t:Student(t[0],t[1],t[2]))\\ .sortBy(lambda x:x,ascending = False)\\ .map(lambda student:(student.name,student.age,student.score))#参考方案：此处巧妙地对score和age进行编码来表达其排序优先级关系，除非age超过100000，以下逻辑无错误。#rdd_sorted = rdd_students.sortBy(lambda x:100000*x[2]+x[1],ascending=False)rdd_sorted.collect() 12345[(&#x27;LiLei&#x27;, 18, 87),(&#x27;Jim&#x27;, 18, 77),(&#x27;HanMeiMei&#x27;, 16, 77),(&#x27;DaChui&#x27;, 16, 66),(&#x27;RuHua&#x27;, 18, 50)] 1.9 连接操作 12345678910111213141516171819#任务：已知班级信息表和成绩表，找出班级平均分在75分以上的班级#班级信息表包括class,name,成绩表包括name,scoreclasses = [(&quot;class1&quot;,&quot;LiLei&quot;), (&quot;class1&quot;,&quot;HanMeiMei&quot;),(&quot;class2&quot;,&quot;DaChui&quot;),(&quot;class2&quot;,&quot;RuHua&quot;)]scores = [(&quot;LiLei&quot;,76),(&quot;HanMeiMei&quot;,80),(&quot;DaChui&quot;,70),(&quot;RuHua&quot;,60)]rdd_classes = sc.parallelize(classes).map(lambda x:(x[1],x[0]))rdd_scores = sc.parallelize(scores)rdd_join = rdd_scores.join(rdd_classes).map(lambda t:(t[1][1],t[1][0]))def average(iterator): data = list(iterator) s = 0.0 for x in data: s = s + x return s/len(data)rdd_result = rdd_join.groupByKey().map(lambda t:(t[0],average(t[1]))).filter(lambda t:t[1]&gt;75)print(rdd_result.collect()) [(‘class1’, 78.0)] 1.10 分组求众数 123456#任务：有一批学生信息表格，包括class和age。求每个班级学生年龄的众数。students = [ (&quot;class1&quot;,15),(&quot;class1&quot;,15),(&quot;class2&quot;,16), (&quot;class2&quot;,16),(&quot;class1&quot;,17),(&quot;class2&quot;,19)] run 12345678910111213141516def mode(arr): dict_cnt = &#123;&#125; for x in arr: dict_cnt[x] = dict_cnt.get(x,0)+1 max_cnt = max(dict_cnt.values()) most_values = [k for k,v in dict_cnt.items() if v==max_cnt] s = 0.0 for x in most_values: s = s + x return s/len(most_values)rdd_students = sc.parallelize(students)rdd_classes = rdd_students.aggregateByKey([],lambda arr,x:arr+[x],lambda arr1,arr2:arr1+arr2)rdd_mode = rdd_classes.map(lambda t:(t[0],mode(t[1])))print(rdd_mode.collect()) [(‘class1’, 15.0), (‘class2’, 16.0)] Reference spark python 练习（一） 7道RDD编程练习题","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Data Analysis - SQL 50","slug":"dataware/dw-sql-50-review","date":"2021-01-05T01:07:21.000Z","updated":"2021-06-22T06:03:38.743Z","comments":true,"path":"2021/01/05/dataware/dw-sql-50-review/","link":"","permalink":"http://www.iequa.com/2021/01/05/dataware/dw-sql-50-review/","excerpt":"","text":"其中重点为：1/2/5/6/7/10/11/12/13/15/17/18/19/22/23/25/31/35/36/40/41/42/45/46 共16题 超级重点 18和23、 22和25 、 41、46 (1). 查询课程编号为“01”的课程比“02”的课程成绩高的所有学生的学号（重点） 123456789101112131415SELECT t1.s_id as t1_s_id, t2.s_id as t2_s_id, t3.s_name, t1.s_score as s_score_01, t2.s_score as s_score_02FROM (select s_id, c_id, s_score from Score WHERE c_id = &#x27;01&#x27;) as t1INNER JOIN (select s_id, c_id, s_score from Score WHERE c_id = &#x27;02&#x27;) as t2ON t1.s_id = t2.s_idINNER JOIN Student as t3 ON t1.s_id = t3.s_idwhere t1.s_score &gt; t2.s_score DISTINCT 123456SELECT DISTINCT column1, column2, ...FROM table_name;SELECT column1, column2, ...FROM table_nameWHERE condition1 AND condition2 AND condition3 ...; 123456INSERT INTO table_name (column1, column2, column3, ...)VALUES (value1, value2, value3, ...);SELECT MIN(column_name), MAX(column_name), COUNT, AVG, SUMFROM table_nameWHERE condition; BETWEEN … AND 123456789101112SELECT column_name(s)FROM table_nameWHERE column_name BETWEEN value1 AND value2;SELECT *FROM OrdersLEFT JOIN CustomersSELECT column_name(s)FROM table1INNER JOIN table2ON table1.column_name = table2.column_name; HAVING 1234SELECT COUNT(CustomerID), CountryFROM CustomersGROUP BY CountryHAVING COUNT(CustomerID) &gt; 5; UNION 1234SELECT City FROM CustomersUNIONSELECT City FROM SuppliersORDER BY City;","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"RDD、DataFrame和DataSet的区别","slug":"spark/spark-rdd-ds-df","date":"2021-01-03T07:28:21.000Z","updated":"2021-06-20T04:12:28.345Z","comments":true,"path":"2021/01/03/spark/spark-rdd-ds-df/","link":"","permalink":"http://www.iequa.com/2021/01/03/spark/spark-rdd-ds-df/","excerpt":"Spark SQL，可对不同格式的数据执行ETL操作（如JSON，Parquet，数据库）然后完成特定的查询操作.","text":"Spark SQL，可对不同格式的数据执行ETL操作（如JSON，Parquet，数据库）然后完成特定的查询操作. 1. RDD和DataFrame 左侧的RDD[Person]虽然以Person为类型参数，Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息. 使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。 RDD是分布式的Java对象的集合。 DataFrame是分布式的Row对象的集合。 DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。 2. DF 提升执行效率 RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。 Spark SQL在框架内部已经在各种可能的情况下 尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。 利用 DataFrame API进行开发，可以免费地享受到这些优化效果。 3. DF 减少数据读取 分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝 上文讨论分区表时提到的 分区剪枝 便是其中一种 -&gt; 当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。 4. RDD和DataSet DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。 DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为Spark SQl类型，然而RDD依赖于运行时反射机制。 5. DataFrame和DataSet Dataset可以认为是DataFrame的特例，区别是Dataset每一个record存储的是一个强类型值而不是一个Row DataSet可以在编译时检查类型 是面向对象的编程接口 DataFrame会继承DataSet，DataFrame是面向Spark SQL的接口 1234567//DataSet,完全使用scala编程，不要切换到DataFrameval wordCount = ds.flatMap(_.split(&quot; &quot;)) .filter(_ != &quot;&quot;) .groupBy(_.toLowerCase()) // Instead of grouping on a column expression (i.e. $&quot;value&quot;) we pass a lambda function .count() DataFrame 123456789// Load a text file and interpret each line as a java.lang.Stringval ds = sqlContext.read.text(&quot;/home/spark/1.6/lines&quot;).as[String]val result = ds .flatMap(_.split(&quot; &quot;)) // Split on whitespace .filter(_ != &quot;&quot;) // Filter empty words .toDF() // Convert to DataFrame to perform aggregation / sorting .groupBy($&quot;value&quot;) // Count number of occurences of each word .agg(count(&quot;*&quot;) as &quot;numOccurances&quot;) .orderBy($&quot;numOccurances&quot; desc) // Show most common words first Reference 用Apache Spark进行大数据处理——第二部分：Spark SQL RDD、DataFrame和DataSet的区别","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Python collections and heapq","slug":"python/language/py_collections_heapq_queue","date":"2020-12-16T02:07:21.000Z","updated":"2021-06-20T04:12:28.222Z","comments":true,"path":"2020/12/16/python/language/py_collections_heapq_queue/","link":"","permalink":"http://www.iequa.com/2020/12/16/python/language/py_collections_heapq_queue/","excerpt":"collections","text":"collections 1. queue 1.1 collections.deque 12345678910111213141516import collectionsqueue = collections.deque()queue.append(5)queue.appendleft(10)queue.append(6)queue.append(7)print(queue) # deque([10, 5, 6, 7]) cur = queue.popleft()cur = queue.pop()print(queue) # deque([5, 6]) 1.2 queue.Queue() 123456789from queue import Queueq = Queue()q.put(5)q.put(3)q.put(9)print(q.qsize()) # 3print(q.get()) # 5, 等于是 pop(), Queue 没有 pop() 只有 get(), get==popprint(q.qsize()) # 2 1.3 queue.PriorityQueue() 1234567891011121314151617from queue import PriorityQueuepq = PriorityQueue()pq.put((1, 2))pq.put((1, 0))pq.put((5, 0))pq.put((2, 3))while not pq.empty(): print (pq.get()) # output# (1, 0) # (1, 2) # (2, 3) # (5, 0) 存放自定义类型： 自定义数据类型，需要自定义 __cmp__ 或者 __lt__ 比价函数 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-from queue import PriorityQueueclass Job(object): def __init__(self, priority, description): self.priority = priority self.description = description def __lt__(self, other): return self.priority &lt; other.priorityq2 = PriorityQueue()q2.put(Job(5, &#x27;Mid-level job&#x27;))q2.put(Job(10, &#x27;Low-level job&#x27;))q2.put(Job(1, &#x27;Important job&#x27;)) # 数字越小，优先级越高print(q2.qsize())while not q2.empty(): next_job = q2.get() # 可根据优先级取序列 print(next_job.description)# 3# Important job# Mid-level job# Low-level job 2. heapq 12345678910from heapq import nsmallest, nlargestlst = [5, 8, 0, 4, 6, 7]print(nsmallest(3, lst))print(nlargest(3, lst))# python优先队列和堆的使用# https://blog.csdn.net/liuweiyuxiang/article/details/97249128 3. stack is list 1234567a = [1, 5, 22, 2, 7]a.pop(2)a.insert(2, 220)print(a) 4. 进制转换 bin, oct, hex, int(‘0b10000’, 2) 1234567891011121314# -*- coding: UTF-8 -*- # Filename : test.py# author by : www.runoob.com # 获取用户输入十进制数dec = int(input(&quot;输入数字：&quot;)) print(&quot;十进制数为：&quot;, dec)print(&quot;转换为二进制为：&quot;, bin(dec))print(&quot;转换为八进制为：&quot;, oct(dec))print(&quot;转换为十六进制为：&quot;, hex(dec))int(hex(2*a),16) 十进制 与 二进制 的互相转换 1234567In [1]: a = 16In [9]: bin(a)Out[9]: &#x27;0b10000&#x27;In [10]: int(&#x27;0b10000&#x27;, 2)Out[10]: 16 Reference","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"interview","slug":"interview","permalink":"http://www.iequa.com/tags/interview/"}]},{"title":"Trip in New Zealand","slug":"world/NewZealand_trip_map","date":"2020-10-31T01:31:48.000Z","updated":"2021-06-22T05:59:40.137Z","comments":true,"path":"2020/10/31/world/NewZealand_trip_map/","link":"","permalink":"http://www.iequa.com/2020/10/31/world/NewZealand_trip_map/","excerpt":"Auckland New Zealand","text":"Auckland New Zealand Auckland New Zealand Auckland New Zealand Reference 奥克兰@马蜂窝 新西兰潮流速报网 Amanda新西兰生活留学笔记 新西兰世界地图上的位置，新西兰十大城市位置地图，新西兰在哪 新西兰葡萄酒产区地图,纽西兰葡萄酒产地地图，新西兰红酒产区地 新西兰奥克兰地图，奥克兰行政地图7个区域 新西兰北岛高速公路地图（新西兰北岛地图，纽西兰北岛自驾游地 紐西蘭22天分享","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"NewZealand","slug":"NewZealand","permalink":"http://www.iequa.com/tags/NewZealand/"}]},{"title":"Spark Summary 1 - Basic knowledge","slug":"spark/spark-summary-1-basic-questions","date":"2020-10-30T23:07:21.000Z","updated":"2021-06-20T04:12:28.359Z","comments":true,"path":"2020/10/31/spark/spark-summary-1-basic-questions/","link":"","permalink":"http://www.iequa.com/2020/10/31/spark/spark-summary-1-basic-questions/","excerpt":"","text":"Spark会把数据都载入到内存么？ 123456789101112131415./bin/spark-submit \\ --master yarn --deploy-mode cluster --num-executors 100 \\ # 总共申请的executor数目，普通任务十几个或者几十个足够了 --executor-memory 6G \\ --executor-cores 4 \\ # 每个executor内的核数，即每个executor中的任务task数目，此处设置为2 --driver-memory 1G \\ # driver内存大小，一般没有广播变量(broadcast)时，设置1~4g足够 --conf spark.default.parallelism=1000 \\ # 默认每个 satge 的 Task总数 # Spark作业的默认为500~1000个比较合适,如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样会导致并行度偏少，资源利用不充分。该参数设为num-executors * executor-cores的2~3倍比较合适 --conf spark.storage.memoryFraction=0.5 \\ 存储内存 --conf spark.shuffle.memoryFraction=0.3 \\ 执行内存 # shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2，如果shuffle聚合时使用的内存超出了这个20%的限制，多余数据会被溢写到磁盘文件中去，降低shuffle性能 # # —-spark.yarn.executor.memoryOverhead 1G ： executor执行的时候，用的内存可能会超过executor-memory， # 所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存 # 默认的 spark.executor.memoryOverhead=6144（6G） 有点浪费 尽量保证每轮Stage里每个task处理的数据量&gt;128M A list of partitions A function for computing each split A list of dependencies on other RDDs Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) Optionally, a list of preferred locations to compute each split on (block locations for an HDFS file) Stage是一个TaskSet，将Stage根据分区数划分成一个个的Task DataFrame和DataSet可以相互转化，df.as[ElementType]这样可以把DataFrame转化为DataSet，ds.toDF()这样可以把DataSet转化为DataFrame DataSet可以在编译时检查类型, DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。 repartition vs coalesce vs coalesce in sql Spark算子：RDD基本转换操作(2)–coalesce、repartition 一般情况下增大rdd的partition数量使用repartition，减少partition数量时使用coalesce No. Title Article 0. Hive杀招 再次分享！Hive调优，数据工程师成神之路 0 HadoopReview杀招 Hadoop高频考点，正在刷新你的认知！ 0. Apache Parquet和Apache Avro 0. RDD是分布式的Java对象的集合 DataFrame是分布式的Row对象的集合 RDD、DataFrame和DataSet的区别 persist、cache和checkpoint的区别与联系 3. Data Warehouse 2020 大数据/数仓/数开 Interview Questions 4. Spark RDD very good Spark原理篇之RDD特征分析讲解 5. Spark Task Spark中Task数量的分析 6. Spark 腾讯总结 Spark 腾讯计算机组总结(一) 7. 大数据学习指南 Github 大数据学习指南 Spark性能优化指南 1. dev Spark性能优化指南——基础篇 2. shuffle Spark性能优化指南——高级篇（8） 3. Spark 1.6 后引入统一内存管理 Apache Spark 内存管理详解 蓦然大数据开发 More Info 请点击 No. Title Article 蓦然大数据开发 1. 蓦然大数据开发 知乎， 公众号：旧时光大数据 2. 蓦然大数据开发 大数据Hadoop（三）——MapReduce 1. Spark 大数据Spark题（一） 2. Spark 大数据Spark题（二） 3. Spark 大数据Spark题（三） 4. Spark 大数据Spark题（四） 5. Spark 大数据Spark题（五）— 几种常见的数据倾斜情况及调优方式 6. Spark 大数据Spark题（六）— Shuffle配置调优 7. Spark 大数据Spark题（七）— 程序开发调优 8. Spark 大数据Spark题（八）— 运行资源调优 Spark 创建RDD、DataFrame各种情况的默认分区数 1). reduceByKey(func, numPartitions=None), Spark可以在每个分区移动数据之前将待输出数据与一个共用的key结合 2). groupByKey(numPartitions=None), 不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作 spark的执行模型的方式，它的特点无非就是多个任务之间数据通信不需要借助硬盘而是通过内存，大大提高了程序的执行效率。而hadoop由于本身的模型特点，多个任务之间数据通信是必须借助硬盘落地的。那么spark的特点就是数据交互不会走硬盘。只能说多个任务的数据交互不走硬盘，但是spark的shuffle过程和hadoop一样仍然必须走硬盘的。 所谓Shuffle不过是把处理流程切分，给切分的上一段(我们称为Stage M)加个存储到磁盘的Action动作，把切分的下一段(Stage M+1)数据源变成Stage M存储的磁盘文件。每个Stage都可以走我上面的描述，让每条数据都可以被N个嵌套的函数处理，最后通过用户指定的动作进行存储。 我们做Cache/Persist意味着什么？ 其实就是给某个Stage加上了一个saveAsMemoryBlockFile的动作，然后下次再要数据的时候，就不用算了。这些存在内存的数据就表示了某个RDD处理后的结果。这个才是说为啥Spark是内存计算引擎的地方。在MR里，你是要放到HDFS里的，但Spark允许你把中间结果放内存里。 所以结论是：Spark并不是基于内存的技术！它其实是一种可以有效地使用内存LRU策略的技术 Spark只有在shuffle的时候才会将数据放在磁盘，而MR却不是 0. Top Questions 1. spark的优化怎么做？ （☆☆☆☆☆） spark调优比较复杂，但是大体可以分为三个方面来进行 No. Title Answer 1 Platform 提高数据的本地性，选择高效的存储格式如parquet 2 Application 处理 Data Skew，复用RDD进行缓存，作业并行化执行等等 3 JVM 层面 启用高效的序列化方法如kyro，增大off head内存等等 Lineage, cognitive ability 2. Spark性能优化指南—高级篇(8) （☆☆☆☆☆） Data Skew 发生的原理 Data Skew 的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生 Data Skew。 No. Spark性能优化指南—高级篇(8) 优缺点 Flag 3. 提高shuffle操作的并行度 reduceByKey(1000) rdd: spark.default.parallelismSQL: spark.sql.shuffle.partitions,shuffle task 默200 总结：实现起来简单，可以缓解和减轻 Data Skew 的影响 ❎ 4. 两阶段聚合（局部聚合+全局聚合） 随机前缀=&gt;原1个Task的数据，现分多Task, 后去掉前缀, 在全局聚合 仅仅适用于聚合类的shuffle操作，适用范围相对较窄 ❎ 5. 将reduce join转为map join 这个方案只适用1个大表和1个小表情况。需将小表进行广播 good 6. 采样倾斜key并分拆join操作 如果导致倾斜的key特别多的话，，那么这种方式也不适合 ✔️ 7. 使用随机前缀和扩容RDD进行join 该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。 而且需要对整个RDD进行扩容，对内存资源要求很高 ✔️ No. 调优概述 1. 大多数Spark作业的性能主要就是消耗在shuffle环节，因为包含了大量的磁盘IO、序列化、网络数据传输等操作。 影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。 2. shuffle相关参数调优 将reduce join转为map join: 12345// 首先将数据量比较小的RDD的数据，collect到Driver中来。List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。// 可以尽可能节省内存空间，并且减少网络传输性能开销。final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data); 3. RDD的弹性表现在哪几点？（☆☆☆☆☆） 1）自动的进行内存和磁盘的存储切换； 2）基于Lineage的高效容错； 3）task如果失败会自动进行特定次数的重试； 4）stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片； 5）checkpoint和persist，数据计算之后持久化缓存； 6）数据调度弹性，DAG TASK调度和资源无关； 7）数据分片的高度弹性。 4. Spark 开发调优 （☆☆☆☆☆） No. Spark性能优化指南基础篇(8) 优缺点 1 ~ 3: 重复利用一个RDD 重复利用一个RDD (1). 避免创建重复 RDD (2). 尽可能复用同一个 RDD (3). 对多次使用的 RDD 进行持久化 4 ~ 6: 提高任务处理的性能 提高任务处理的性能 (4). 尽量避免使用 shuffle 类算子 Spark作业运行过程中，最消耗性能的地方就是shuffle过程 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key (5). 使用 map-side 预聚合的 shuffle 操作 是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner; map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了 减少了磁盘IO以及网络传输开销 (6). 使用高性能算子 reduceByKey/aggregateByKey替代groupByKey 使用mapPartitions替代普通map (7). 广播大变量 (减轻网络负担) (8). 使用 Kryo 优化序列化性能 1. 在算子函数中用到外部变量，该变量会序列化后网络传输 2. RDD泛型类型时，所有自定义类型对象，都会进行序列化 3. 使用可序列化的持久化策略时 Kryo要求最好要注册所有需要进行序列化的自定义类型 (9). 优化数据结构 总结： 如果说有某一个 RDD 会在一个程序中被多次使用，那么就应该不要重复创建，要多次使用这一个RDD (不可变的)，既然要重复利用一个RDD，就应该把这个 RDD 进行持久化. （最好在内存中） cache persist 持久化数据到磁盘或内存 unpersist 如何把持久化到磁盘或内存中的数据给删除掉呢？ 1. Spark 基础 (2) 1）. spark的有几种部署模式，每种模式特点？ 1). local 【启动1~k个executor]】 2). standalone 【分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控】 3). Spark on yarn (yarn-cluster和yarn-client) Spark on yarn模式 分布式部署集群，资源和任务监控交给yarn管理 粗粒度资源分配方式，包含cluster和client运行模式 cluster 适合生产，driver运行在集群子节点，具有容错功能 client 适合调试，dirver运行在客户端 2）. spark有5个组件 (5) master：管理集群和节点，不参与计算。 worker：计算节点，进程本身不参与计算，和master汇报。 Driver：运行程序的main方法，创建spark context对象。 sparkContext：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。 client：用户提交程序的入口。 2. Spark运行细节 (13) No. Topic Flag 1. spark工作机制 ? 【client端提交作业后-&gt;Drive,main,SparkContext-&gt;DAG...】 2. Spark应用程序的执行过程 3. driver的功能是什么？ 【作业主进程，有main函数，且有SparkContext的实例】 4. Spark中worker的主要工作是什么？ [管理当前节点内存，CPU使用状况, worker就类似于包工头，管理分配新进程] 5. task有几种类型？2种 【resultTask 和 shuffleMapTask类型，除了最后一个task都是】 6. 什么是shuffle，以及为什么需要shuffle？ 【某种具有共同特征的数据汇聚到一个计算节点上进行计算】 7. Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么？ 【因为程序在运行之前，已经申请过资源了，driver和Executors通讯，不需要和master进行通讯的】 8. Spark并行度怎么设置比较合适 【spark并行度，每个core承载2~4个partition（并行度）】 9. Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？ 10. Spark中数据的位置是被谁管理的？ 【每个数据分片都对应具体物理位置，数据的位置是被blockManager管理】 11. 为什么要进行序列化? 【减少存储空间，高效存储和传输数据，缺点：使用时需要反序列化，非常消耗CPU】 12. Spark如何处理不能被序列化的对象？ 【封装成object】 2.1 spark工作机制 用户在client端提交作业后，会由Driver运行main方法并创建 sparkContext 执行add算子，形成dag图输入dagscheduler ， (创建job,划分Stage,提交Stage) 按照add之间的依赖关系划分stage输入task scheduler task scheduler会将stage划分为taskset分发到各个节点的executor中执行 2.2 Spark应用程序的执行过程 1 . 构建Spark Application的运行环境（启动SparkContext） 2 . SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源； 3 . 资源管理器分配Executor资源，Executor运行情况将随着心跳发送到资源管理器上； YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 4 . SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler 5 . Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，SparkContext将应用程序代码发放给Executor。 6 . Task在Executor上运行，运行完毕释放所有资源 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。 Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。 task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。 一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。 下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 stage的划分 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。 因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。 No. Executor的内存主要分为三块 1. 第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%； 2. 第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%； 3. 第三块是让RDD持久化时使用，默认占Executor总内存的60%。 Spark性能优化指南——基础篇 123456789./bin/spark-submit \\ --master yarn-cluster \\ --num-executors 100 \\ --executor-memory 6G \\ --executor-cores 4 \\ --driver-memory 1G \\ --conf spark.default.parallelism=1000 \\ --conf spark.storage.memoryFraction=0.5 \\ --conf spark.shuffle.memoryFraction=0.3 \\ 2.3 driver的功能是什么？ Spark作业运行时包括一个Driver进程，也是作业主进程，有main函数，且有SparkContext的实例，是程序入口点； 功能： 向集群申请资源 负责了作业的调度和解析 生成Stage并调度Task到Executor上（包括DAGScheduler，TaskScheduler） 2.4 Spark中Worker工作是什么？ 管理当前节点内存，CPU使用状况,接收master分配过来的资源指令,通过ExecutorRunner启动程序分配任务 worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务 worker不会运行代码，具体运行的是Executor是可以运行具体appliaction写的业务逻辑代码 Process 进程 2.4 Spark 程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？ 有很多小文件的时候，有多少个输入block就会有多少个task启动 spark中有partition，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率 3. Spark 与 Hadoop 比较(7) Mapreduce和Spark的相同和区别 简答说一下hadoop的mapreduce编程模型 简单说一下hadoop和spark的shuffle相同和差异？ 简单说一下hadoop和spark的shuffle过程 partition和block的关联 Spark为什么比mapreduce快？ Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？ 相当于spark中的map算子和reduceByKey算子，区别：MR会自动进行排序的，spark要看具体partitioner 3.1 MR和Spark相同和区别 spark用户提交的任务：application 一个application对应一个SparkContext，app中存在多个job 1). 每触发一次action会产生一个 job -&gt; 这些job可以并行或串行执行 2). 每个job有多个 stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的 3). 每个stage里面有多个 task，组成 taskset 有 TaskSchaduler 分发到各个executor执行 4). executor 生命周期和app一样的，即使没有job运行也存在，所以task可以快速启动读取内存进行计算. 3.2 mapreduce 编程模型 map task会从本地文件系统读取数据，转换成key-value形式的键值对集合 key-value,集合,input to mapper进行业务处理过程，将其转换成需要的key-&gt; value在输出 之后会进行一个partition分区操作，默认使用的是hashpartitioner 之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出 之后进行一个combiner归约操作，其实就是一个本地的reduce预处理，以减小后面shufle和reducer的工作量 reduce task会通过网络将各个数据收集进行reduce处理 最后将数据保存或者显示，结束整个job 3.3 mr/spark 的 shuffle 差异? high-level 角度： 两者并没有大的差别 都是将 mapper（Spark: ShuffleMapTask）的输出进行 partition， 不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask） Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce(). low-level 角度： Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort. 好处：combine/reduce() 可以处理大规模的数据, 因为其输入数据可以通过外排得到 (1) mapper 对每段数据先做排序 (2) reducer 的 shuffle 对排好序的每段数据做 归并 merge Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不提前排序 如果用户需要经过排序的数据：sortByKey() 实现角度： Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spilt, merge, shuffle, sort, reduce() Spark 没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，spill, merge, aggregate 等操作需要蕴含在 transformation() 中 3.4 MR/Spark 的 shuffle 过程 Tech description hadoop map端保存分片数据，通过网络收集到reduce端 spark spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor，减少shuffle可以提高性能 3.5 partition 和 block 的关联 hdfs 中的 block 是分布式存储的最小单元，等分，可设置冗余，这样设计有一部分磁盘空间的浪费，但是整齐的 block大小，便于快速找到、读取对应的内容 Spark中的partition是RDD的最小单元，RDD是由分布在各个节点上的partition组成的. partition是指的spark在计算过程中，生成的数据在计算空间内最小单元. 同一份数据（RDD）的partion大小不一，数量不定，是根据application里的算子和最初读入的数据分块数量决定 block/partition description block 位于存储空间, block的大小是固定的 partition 位于计算空间, partion大小是不固定的 4. Spark RDD(4) 4.1 RDD机制 分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币 所有算子都是基于rdd来执行的 rdd执行过程中会形成dag图，然后形成lineage保证容错性等 从物理的角度来看rdd存储的是block和node之间的映射 4.2 RDD的弹性表现在哪几点？ 自动的进行内存和磁盘的存储切换； 基于Lingage的高效容错； task如果失败会自动进行特定次数的重试； stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片； checkpoint和persist，数据计算之后持久化缓存 数据调度弹性，DAG TASK调度和资源无关 数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par 4.3 RDD有哪些缺陷？ 不支持细粒度的写和更新操作（如网络爬虫） spark写数据是粗粒度的，所谓粗粒度，就是批量写入数据 （批量写） 但是读数据是细粒度的也就是说可以一条条的读 （一条条读） 不支持增量迭代计算，Flink支持 4.4 什么是RDD宽依赖和窄依赖？ RDD和它依赖的parent RDD(s)的关系有两种不同的类型 窄依赖：每一个parent RDD的Partition最多被子RDD的一个Partition使用 （一父一子） 宽依赖：多个子RDD的Partition会依赖同一个parent RDD的Partition （一父多子） 5. RDD操作(13) 5.1 cache和pesist的区别 .cache() == .persist(MEMORY_ONLY) 5.2 cache后面能不能接其他算子,它是不是action操作？ 可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache cache不是action操作 5.3 什么场景下要进行persist操作？ spark所有复杂一点的算法都会有persist身影,spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤 只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或cache的化，就要重头做。 No. 以下场景会使用persist Article 1） 某个步骤计算非常耗时，需要进行persist持久化 ； 2） 计算链条非常长，重新恢复要算很多步骤，很好使，persist ； 3） checkpoint所在的rdd要持久化persist， lazy级别，框架发现有checnkpoint，checkpoint时单独触发一个job，需要重算一遍，checkpoint前 要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist；sc.setCheckpointDir(“hdfs://lijie:9000/checkpoint0727”) rdd.cache() rdd.checkpoint() 4） shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大 ； 5） shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。 5.4 rdd有几种操作类型？三种！ transformation，rdd由一种转为另一种rdd action cronroller，控制算子(cache/persist) 对性能和效率的有很好的支持 5.5 collect功能是什么，其底层是怎么实现的？ driver通过collect把集群中各个节点的内容收集过来汇总成结果 collect返回结果是Array类型的，合并后Array中只有一个元素，是tuple类型（KV类型的）的。 5.6 map与flatMap的区别 map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象 flatMap：对RDD每个元素转换，然后再扁平化，将所有的对象合并为一个对象，会抛弃值为null的值 5.8 列举你常用的action？ collect，reduce,take,count,saveAsTextFile等 5.10 Spark累加器有哪些特点？ 全局的，只增不减，记录全局集群的唯一状态 在exe中修改它，在driver读取 executor级别共享的，广播变量是task级别的共享 两个application不可以共享累加器，但是同一个app不同的job可以共享 5.11 spark hashParitioner的弊端 分区原理：对于给定的key，计算其hashCode 弊端是数据不均匀，容易导致数据倾斜 5.12 RangePartitioner分区的原理 尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，也就是说一个分区中的元素肯定都是比另一个分区内的元素小或者大 分区内的元素是不能保证顺序的 简单的说就是将一定范围内的数映射到某一个分区内 5.13 Spark中的HashShufle的有哪些不足？ shuffle产生海量的小文件在磁盘上，此时会产生大量耗时的、低效的IO操作； 容易导致内存不够用，由于内存需要保存海量的文件操作句柄和临时缓存信息 容易出现数据倾斜，导致OOM 5. Spark 大数据问题(7) No. Title 1. 如何使用Spark解决TopN问题？ 2. 如何使用Spark解决分组排序问题？ lines.groupByKey(), values.toList.sortWith 3. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url? Solution： 划分小文件 Hash &amp; Bloomfilter 4. 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，要求返回频数最高的100个词 (1). hash(x)%5000 5000个小文件 (2). 统计每个小文件词频 5. 现有海量日志数据保存在一个超级大的文件中，该文件无法直接读入内存，要求从中提取某天出访问百度次数最多的那个IP 分而治之+Hash, Hash(IP)%1024 6. 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数 方案1：采用2-Bitmap 方案2：划分小文件,在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素 7. 腾讯面试题：给40亿个不重复的unsignedint的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中? 申请512M的内存，一个bit位代表一个unsignedint值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在 Hive数仓建表该选用ORC还是Parquet，压缩选LZO还是Snappy？ 「所以在实际生产中，使用Parquet存储，lzo压缩的方式更为常见，这种情况下可以避免由于读取不可分割大文件引发的数据倾斜。 但是，如果数据量并不大（预测不会有超大文件，若干G以上）的情况下，使用ORC存储，snappy压缩的效率还是非常高的。」 Reference Spark算子：RDD基本转换操作(2)–coalesce、repartition Spark知识点汇总 Spark总结(一) 知乎 Spark:Yarn-cluster和Yarn-client区别与联系 Spark常用算子 《Spark快速大数据分析》RDD操作总结 Spark常见面试问题有哪些？ Spark学习痛点和路线图 Spark面试题(一) 每个 Spark 工程师都应该知道的五种 Join 策略","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Chap 7 内存模型和资源调优","slug":"spark/spark-aura-7.1-memory","date":"2020-10-19T15:07:21.000Z","updated":"2021-06-20T04:12:28.354Z","comments":true,"path":"2020/10/19/spark/spark-aura-7.1-memory/","link":"","permalink":"http://www.iequa.com/2020/10/19/spark/spark-aura-7.1-memory/","excerpt":"1. Spark性能优化指南——基础篇","text":"1. Spark性能优化指南——基础篇 最重要的5张图 - MapReduce编程案例 by 马中华 mapreduce 的 shuffle 是一通用的 shuffle (既每一个task最终都只会形成一个磁盘文件+一个索引) spark 的4种 shuffle 就是在 mapreduce 的 shuffle 基础之上, 进行了某些动作的删减之后形成的 多种 shuffle 方案选择: mapreduce: partitioner, combiner, sort spark: 可以在 4 种方案中选择使用其中的哪一种。 (就是对上面的 mapreduce shuffle 过程的) 各种全理论： 1) spark 的内存模型 堆内内存 + 堆外内存 执行内存 + 存储内存 静态内存模型 + 统一内存模型 动态占用机制 2) 资源调优 num-executors executor-memory total-executor-cores spark.shuffle.memoryFraction spark.storage.memoryFraction … spark-submit … 3) spark 的 shuffle HashShuffleManager 未优化版本 已优化版本 SortShuffleManager 普通的机制 bypass机制 1. Spark 的 shuffle 调优 spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。 spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 2. spark开发调优和DatSkew复习 2.1 开发调优 开发调优 1 ~ 3: 重复利用一个RDD 重复利用一个RDD (1). 避免创建重复 RDD (2). 尽可能复用同一个 RDD (3). 对多次使用的 RDD 进行持久化 开发调优 4 ~ 6: 提高任务处理的性能 提高任务处理的性能 (4). 尽量避免使用 shuffle 类算子 (5). 使用 map-side 预聚合的 shuffle 操作 (6). 使用高性能算子 (7). 广播大变量 (减轻网络负担) (8). 使用 Kryo 优化序列化性能 (9). 优化数据结构 总结： 如果说有某一个 RDD 会在一个程序中被多次使用，那么就应该不要重复创建，要多次使用这一个RDD (不可变的)，既然要重复利用一个RDD，就应该把这个 RDD 进行持久化. （最好在内存中） cache persist 持久化数据到磁盘或内存 unpersist 如何把持久化到磁盘或内存中的数据给删除掉呢？ rdd. 2.2 Data Skew 数据如何分区？ (随机，hash，范围) Data Skew description (1). DataSkew 发生的现象 (2). DataSkew 发生的原理 &gt; 数据分布不均匀 &gt; 追求的目标： 数据分布要均匀 (3). 如何定位导致 DataSkew 的代码 &gt; 某个 task 执行特别慢的情况 &gt; 某个 task 莫名其妙内存溢出的情况 (4). 查看导致 DataSkew 的 key 的数据分布情况 &gt; 测试 &gt; 采样 3. spark 的内存管理宏观概述 spark 作为基于内存的分布式计算引擎, 其内存管理模块在整个系统中非常重要. 理解 spark 内存管理的基本原理，有助于更好的开发 spark 应用程序 和 进行性能调优. spark的内存模型 spark的shuffle spark的资源调优 3.1 spark的内存模型 No. spark 的产生背景， spark 优于 mapreduce 的五大原因： (1). 减少了磁盘 IO (2). 提高并行度 (3). 避免重复计算 (4). 可选的 shuffle 和排序 (5). 提供了一个灵活的 内存管理策略 good Spark学习之路 （十一）SparkCore的调优之Spark内存模型 3.2 application 内存 No. 划分 application 在运行的时候，会在哪些地方产生数据，需要存储在内存中呢？ 1. 应用程序 2. 执行内存 全局变量，静态变量 3. 执行内存 task 在计算的时候, 数据在内存中(128M) (有的 ptn_data &gt; 128 有的 &lt; 128) 4. 执行内存 mapPartitions (ptn_data =&gt; {}) for (element &lt;- partition) code 执行过程中，使用的临时容器，临时变量 5. 执行内存 stage0 和 stage1 之间有 shuffle，这个将要进行 shuffle 的数据存储在何地？ … 数据的分区数 6. 存储内存 内存占用的大户： rdd.cache() 占用时间 长 + 多 7. 存储内存 广播出来的大变量 sc,broadcast(list) list 会存储在所有 executor 内存中 … 一种合适的内存管理策略，可以提升内存利用率，提高Task执行的成功率 3.3 Spark 内存模型概述 在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程 Driver 主控进程，负责创建 SparkContext，提交 Spark Job，并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度 Executor 负责在工作节点上执行具体的 计算 Task，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。 由于 Driver-memory 1G 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。 Spark 的应用程序2种进程 driver： 主控进程，必须保证不能出错, 而且也只有一个 executor： task 的执行载体，数量也很多 侧重点： executor 上的内存管理 执行 task 过程中会产生哪些数据呢？ 2 3 4 5 spark 帮助我们把应用程序执行构成当中所占用的内存分成 2 个方面： 执行内存 2 3 4 5 必须的 存储内存 6 7 可有可无 3.4 Spark 把内存做分类的目的 假如给每一个 executor 分配的内存是 8G: 执行内存： 1G， 存储内存 7G 2G: 执行内存： 1G， 存储内存 1G 假如给每一个 executor 分配的内存是 8G (1). 当这个 executor 启动一个 task 执行计算的时候，处理的数据量是 2G (2). 当这个 executor 启动一个 task 执行计算的时候，处理的数据量是 6G 结论： 同一个程序在执行不同量级的数据的计算的时候，每个 task 执行的内存所占用的资源其实差不多一致 数据分区的存储 3.5 Spark内存的整体划分 又分为2种不同类型的内存划分： No. spark 能利用的内存有2个区域 1. (executor内存) JVM 内部 的 On-heap Memory （对于JVM来说叫做 堆内存） 2. (executor外部) JVM 外部/操作系统 的 Off-heap Memory 这2个区域，又都分为2个区域： 12345678public class Test1 &#123; private List list; public synchronized void test1() &#123; // visit list &#125;&#125;Test t1 = new Test1();Test t2 = new Test1(); 正常的情况下，一个 JVM 进程中的线程是没法从操作系统中申请内存的 只能从 JVM 中申请内存 但是现在spark的task(一个线程)就可以从操作系统，也就是说JVM之外，申请内存使用，而且还是所有的task公用的. 有什么好处？ spark 的程序中，上面缓存的RDD，在这个应用程序中的任何地方都可以访问 4. spark 的静态内存模型和统一内存模型详解 + 资源调优 Off-Heap 内存 spark 的2种内存管理方式： 1). spark 1.x 静态内存模型 执行内存和存储内存 相互之间 不能 占用 2). spark 2.x 统一内存模型 执行内存和存储内存 相互之间 能 占用 内存管理接口: MemoryManager StaticMemoryManager UnifiedMemoryManager 方法： 重要的方法有 6 个： 3个是申请内存的, 3个是释放内存的 以上这3个申请和3个释放内存的方法，其实就是对申请到的总内存进行一种逻辑上的管理规划 堆内内存 和 堆外内存 是真是存在的一个内存区域 执行内存和存储内存，都是堆内和堆外内存的一个逻辑区划的概念. good Apache Spark 内存管理详解 Memory Management 4.1 spark 的静态内存模型 静态内存管理图示——堆内 静态内存管理图示——堆外 4.2 统一内存模型 统一内存管理图示——堆内 Execution 占用 Storage 是不会归还的, 反之 要归还 统一内存管理图示——堆外 application 中的 job 的执行： FIFO 把 YARN 的资源分拆成多个不同的队列 每个队列中的任务的执行是顺序的 FIFO 执行 整个程序到底有多少个 task？ num-executors 如果现在一个 executor 的 task 数量 一个 executor 分配 3~5个 cpu cores. 5. 资源调优 2 资源调优 params description 参数调优建议 (1) num-executors 一个 executor 就是一个进程, 50~100个左右的Executor进程比较合适 (2) executor-memory 每个Executor进程的内存设置4G~8G较为合适 (3) total-executor-cores Executor的CPU core数量设置为2~4个较为合适每个进程可以使用多少个 cpu core，一个executor 启动 10 个task driver-memory Driver的内存通常来说不设置，或者设置1G左右应该就够了 spark.default.parallelism 该参数用于设置每个stage的默认task数量。这个参数极为重要 Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。 设置该参数为num-executors * executor-cores的2~3倍较为合适 (4) spark.shuffle.memoryFraction (5) spark.storage.memoryFraction … … spark-submit … 如果 master 是 local: 默认的分区数是 1个，而且没有 executore 如果 master 是 spark: 默认的分区数是 2个 min(defaultMinPartitions,2)，有 executore 一个 spark 应用程序： 500个task 50个executor 每一个 executor 执行 10 个 task 左右 每一个 executor 分配 3~5个 总 cpu core 数： 150 ~ 250 每一个 executor 分配的内存是： 2G driver程序分配的内存： 2G 整个应用总消耗：50*2 + 2 = 102 内存 每 1 个 stage 中的到底有多少个 task 由谁决定？ 划分 stage 的标准： shuffle 算子，宽依赖 在一个stage中是有可能有 多个RDD 的 每一个 stage 中的 task 总数, 是由这个 stage 中的最后一个 RDD 的分区数来决定的. spark 从 hdfs 中读取数据，使用的方式，默认情况下依然是 TextInputFormat 默认情况下 mapreduce 中数据读取规则是由 TextInputFormat 和 LineRecordReader 决定 默认情况下，其实就是 1个block 1个task 每个元素的读取方式依然是逐行读取形成为一个元素 mapper: key, value rdd: value RDD[String] spark.storage.memoryFraction spark.shuffle.memoryFraction 资源参数参考示例 12345678910./bin/spark-submit \\ --master yarn --deploy-mode cluster --num-executors 100 \\ --executor-memory 6G \\ --executor-cores 4 \\ --driver-memory 1G \\ --conf spark.default.parallelism=1000 \\ Task总数 --conf spark.storage.memoryFraction=0.5 \\ 存储内存 --conf spark.shuffle.memoryFraction=0.3 \\ 执行内存 一般机器： 32/64cpu, 64G/128G/256G 60cpu core, 240G 内存。 另外16G内存给系统运行用. 每一个 cpu core 4G 内存 每一个 executor 要分配2~3个cpu core， 分配内存： 8~12G 每一个application大概是： 100个executor 6. mapreduce的shuffle复习 key, value kvbuffer: ptn, key, value ptn+key Reference 从一个sql任务理解spark内存模型 Apache Spark 内存管理详解 Spark学习之路 （十一）SparkCore的调优之Spark内存模型 云课堂 SparkSQL 的数据源操作 大数据资料笔记整理 Spark性能优化指南——高级篇 Spark性能优化指南——基础篇","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"SparkSQL 底层实现原理","slug":"spark/sparkSQL-all-knowleage","date":"2020-10-17T07:28:21.000Z","updated":"2021-06-20T04:12:28.353Z","comments":true,"path":"2020/10/17/spark/sparkSQL-all-knowleage/","link":"","permalink":"http://www.iequa.com/2020/10/17/spark/sparkSQL-all-knowleage/","excerpt":"","text":"1.sparksql概述 1.1 sparksql的前世今生 Shark是专门针对于spark的构建大规模数据仓库系统的一个框架 Shark与Hive兼容、同时也依赖于Spark版本 Hivesql底层把sql解析成了mapreduce程序，Shark是把sql语句解析成了Spark任务 随着性能优化的上限，以及集成SQL的一些复杂的分析功能，发现Hive的MapReduce思想限制了Shark的发展。 最后Databricks公司终止对Shark的开发 决定单独开发一个框架，不在依赖hive，把重点转移到了sparksql这个框架上。 1.2 什么是sparksql Spark SQL is Apache Spark’s module for working with structured data. SparkSQL是apache Spark用来处理结构化数据的一个模块 2. sparksql的四大特性 1、易整合 将SQL查询与Spark程序无缝混合 可以使用不同的语言进行代码开发 java scala python R 2、统一的数据源访问 以相同的方式连接到任何数据源 sparksql后期可以采用一种统一的方式去对接任意的外部数据源 1val dataFrame = sparkSession.read.文件格式的方法名(&quot;该文件格式的路径&quot;) 3、兼容hive sparksql可以支持hivesql这种语法 sparksql兼容hivesql 4、支持标准的数据库连接 sparksql支持标准的数据库连接JDBC或者ODBC spark-core-----&gt;去操作RDD----&gt;封装了数据 spark-sql------&gt;编程抽象DataFrame 3. DataFrame概述 3.1 DataFrame发展 DataFrame前身是schemaRDD,这个schemaRDD是直接继承自RDD，它是RDD的一个实现类 在spark1.3.0之后把schemaRDD改名为DataFrame,它不在继承自RDD，而是自己实现RDD上的一些功能 也可以把dataFrame转换成一个rdd，调用rdd这个方法 例如 val rdd1=dataFrame.rdd 3.2 DataFrame是什么 在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库的二维表格 DataFrame带有Schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型，但底层做了更多的优化 DataFrame可以从很多数据源构建 比如：已经存在的RDD、结构化文件、外部数据库、Hive表。 RDD可以把它内部元素看成是一个java对象 DataFrame可以把内部是一个Row对象，它表示一行一行的数据 可以把DataFrame这样去理解 RDD+schema元信息 dataFrame相比于rdd来说，多了对数据的描述信息（schema元信息） 3.3 DataFrame和RDD的优缺点 1、RDD 优点 1、编译时类型安全 开发会进行类型检查，在编译的时候及时发现错误 2、具有面向对象编程的风格 缺点 1、构建大量的java对象占用了大量heap堆空间，导致频繁的GC 1由于数据集RDD它的数据量比较大，后期都需要存储在heap堆中，这里有heap堆中的内存空间有限，出现频繁的垃圾回收（GC），程序在进行垃圾回收的过程中，所有的任务都是暂停。影响程序执行的效率 2、数据的序列化和反序列性能开销很大 1在分布式程序中，对象(对象的内容和结构)是先进行序列化，发送到其他服务器，进行大量的网络传输，然后接受到这些序列化的数据之后，再进行反序列化来恢复该对象 2、DataFrame DataFrame引入了schema元信息和off-heap(堆外) 优点 1、DataFrame引入off-heap，大量的对象构建直接使用操作系统层面上的内存，不在使用heap堆中的内存，这样一来heap堆中的内存空间就比较充足，不会导致频繁GC，程序的运行效率比较高，它是解决了RDD构建大量的java对象占用了大量heap堆空间，导致频繁的GC这个缺点。 2、DataFrame引入了schema元信息—就是数据结构的描述信息，后期spark程序中的大量对象在进行网络传输的时候，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。这样一来数据网络传输的数据量是有所减少，数据的序列化和反序列性能开销就不是很大了。它是解决了RDD数据的序列化和反序列性能开销很大这个缺点 缺点 DataFrame引入了schema元信息和off-heap(堆外)它是分别解决了RDD的缺点，同时它也丢失了RDD的优点 1、编译时类型不安全 编译时不会进行类型的检查，这里也就意味着前期是无法在编译的时候发现错误，只有在运行的时候才会发现 2、不在具有面向对象编程的风格 4. 读取文件构建DataFrame 4.1 读取文本文件创建DataFrame 第一种方式 将数据person.txt上传到node01的/kkb/install/sparkdatas本地路径下 1234567891011121314node01执行以下命令进入spark-shellcd /kkb/install/spark-2.3.3-bin-hadoop2.7/bin/spark-shell --master local[2] --jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jarval personDF=spark.read.text(&quot;file:///kkb/install/sparkdatas/person.txt&quot;)//org.apache.spark.sql.DataFrame = [value: string]//打印schema信息personDF.printSchema//展示数据personDF.show 第二种方式 12345678910111213//加载数据val rdd1=sc.textFile(&quot;file:///kkb/install/sparkdatas/person.txt&quot;).map(x=&gt;x.split(&quot; &quot;))//定义一个样例类case class Person(id:String,name:String,age:Int)//把rdd与样例类进行关联val personRDD=rdd1.map(x=&gt;Person(x(0),x(1),x(2).toInt))//把rdd转换成DataFrameval personDF=personRDD.toDF//打印schema信息personDF.printSchema//展示数据personDF.show 4.2 读取json文件创建DataFrame 123456val peopleDF=spark.read.json(&quot;file:///kkb/install/spark-2.3.3-bin-hadoop2.7/examples/src/main/resources/people.json&quot;)//打印schema信息peopleDF.printSchema//展示数据peopleDF.show 4.3 读取parquet文件创建DataFrame 123456val usersDF=spark.read.parquet(&quot;file:////kkb/install/spark-2.3.3-bin-hadoop2.7/examples/src/main/resources/users.parquet&quot;)//打印schema信息usersDF.printSchema//展示数据usersDF.show 5. DataFrame常用操作 5.1 DSL风格语法 创建maven工程，导入jar包 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.3&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-mapred&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;2.6.0-mr1-cdh5.14.2&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.2.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-common&lt;/artifactId&gt; &lt;version&gt;1.2.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-spark&lt;/artifactId&gt; &lt;version&gt;1.2.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.3&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 就是sparksql中的DataFrame自身提供了一套自己的Api，可以去使用这套api来做相应的处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.SparkSession//定义一个样例类case class Person(id:String,name:String,age:Int)object SparkDSL &#123; def main(args: Array[String]): Unit = &#123; val sparkConf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;sparkDSL&quot;) val sparkSession: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate() val sc: SparkContext = sparkSession.sparkContext sc.setLogLevel(&quot;WARN&quot;) //加载数据 val rdd1=sc.textFile(&quot;file:///D:\\\\开课吧课程资料\\\\15、scala与spark课程资料\\\\2、spark课程\\\\spark_day05\\\\数据/person.txt&quot;).map(x=&gt;x.split(&quot; &quot;)) //把rdd与样例类进行关联 val personRDD=rdd1.map(x=&gt;Person(x(0),x(1),x(2).toInt)) //把rdd转换成DataFrame import sparkSession.implicits._ // 隐式转换 val personDF=personRDD.toDF //打印schema信息 personDF.printSchema //展示数据 personDF.show //查询指定的字段 personDF.select(&quot;name&quot;).show personDF.select($&quot;name&quot;).show //实现age+1 personDF.select($&quot;name&quot;,$&quot;age&quot;,$&quot;age&quot;+1).show() //实现age大于30过滤 personDF.filter($&quot;age&quot; &gt; 30).show //按照age分组统计次数 personDF.groupBy(&quot;age&quot;).count.show //按照age分组统计次数降序 personDF.groupBy(&quot;age&quot;).count().sort($&quot;count&quot;.desc).show sparkSession.stop() sc.stop() &#125;&#125; 5.2 SQL风格语法 可以把DataFrame注册成一张表，然后通过==sparkSession.sql(sql语句)==操作 123456789101112//DataFrame注册成表personDF.createTempView(&quot;person&quot;)//使用SparkSession调用sql方法统计查询spark.sql(&quot;select * from person&quot;).showspark.sql(&quot;select name from person&quot;).showspark.sql(&quot;select name,age from person&quot;).showspark.sql(&quot;select * from person where age &gt;30&quot;).showspark.sql(&quot;select count(*) from person where age &gt;30&quot;).showspark.sql(&quot;select age,count(*) from person group by age&quot;).showspark.sql(&quot;select age,count(*) as count from person group by age&quot;).showspark.sql(&quot;select * from person order by age desc&quot;).show 6. DataSet概述 6.1 DataSet是什么 DataSet是分布式的数据集合，Dataset提供了强类型支持，也是在RDD的每行数据加了类型约束。 DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型和可以用强大lambda函数）以及使用了Spark SQL优化的执行引擎。 6.2 DataFrame、DataSet区别 假设RDD中的两行数据长这样 那么DataFrame中的数据长这样 Dataset中的数据长这样 或者长这样（每行数据是个Object） ​ 123DataSet包含了DataFrame的功能，Spark2.0中两者统一，DataFrame表示为DataSet[Row]，即DataSet的子集。（1）DataSet可以在编译时检查类型（2）并且是面向对象的编程接口 6.3 DataFrame与DataSet互转 1、把一个DataFrame转换成DataSet val dataSet=dataFrame.as[强类型] 2、把一个DataSet转换成DataFrame val dataFrame=dataSet.toDF 补充说明 可以从dataFrame和dataSet获取得到rdd val rdd1=dataFrame.rdd val rdd2=dataSet.rdd 6.4 构建DataSet 1、 通过sparkSession调用createDataset方法 12val ds=spark.createDataset(1 to 10) //scala集合val ds=spark.createDataset(sc.textFile(&quot;/person.txt&quot;)) //rdd 2、使用scala集合和rdd调用toDS方法 12sc.textFile(&quot;/person.txt&quot;).toDSList(1,2,3,4,5).toDS 3、把一个DataFrame转换成DataSet 1val dataSet=dataFrame.as[强类型] 4、通过一个DataSet转换生成一个新的DataSet 1List(1,2,3,4,5).toDS.map(x=&gt;x*10) 6.5 RDD以及DataFrame以及DataSet的关系 首先，Spark RDD、DataFrame和DataSet是Spark的三类API，下图是他们的发展过程： DataFrame是spark1.3.0版本提出来的，spark1.6.0版本又引入了DateSet的，但是在spark2.0版本中，DataFrame和DataSet合并为DataSet。 那么你可能会问了：那么，在2.0以后的版本里，RDD是不是不需要了呢？ 答案是：NO！首先，DataFrame和DataSet是基于RDD的，而且这三者之间可以通过简单的API调用进行无缝切换。 下面，依次介绍这三类API的特点 一、RDD RDD的优点： 1.相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。 2.面向对象编程，直接存储的java对象，类型转化也安全 RDD的缺点： 1.由于它基本和hadoop一样万能的，因此没有针对特殊场景的优化，比如对于结构化数据处理相对于sql来比非常麻烦 2.默认采用的是java序列号方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁 二、DataFrame DataFrame的优点： 1.结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表 2.有针对性的优化，如采用Kryo序列化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数,所以运行更快。 3.hive兼容，支持hql、udf等 DataFrame的缺点： 1.编译时不能类型转化安全检查，运行时才能确定是否有问题 2.对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象 三、DateSet DateSet的优点： 1.DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据 2.和RDD一样，支持自定义对象存储 3.和DataFrame一样，支持结构化数据的sql查询 4.采用堆外内存存储，gc友好 5.类型转化安全，代码友好 7. 通过IDEA开发程序实现把RDD转换DataFrame 7.1 利用反射机制 定义一个样例类，后期直接映射成DataFrame的schema信息 应用场景 123在开发代码之前，是可以先确定好DataFrame的schema元信息case class Person(id:String,name:String,age:Int) 代码开发 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import org.apache.spark.SparkContextimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.&#123;Column, DataFrame, Row, SparkSession&#125;//todo:利用反射机制实现把rdd转成dataFramecase class Person(id:String,name:String,age:Int)object CaseClassSchema &#123; def main(args: Array[String]): Unit = &#123; //1、构建SparkSession对象 val spark: SparkSession = SparkSession.builder().appName(&quot;CaseClassSchema&quot;).master(&quot;local[2]&quot;).getOrCreate() //2、获取sparkContext对象 val sc: SparkContext = spark.sparkContext sc.setLogLevel(&quot;warn&quot;) //3、读取文件数据 val data: RDD[Array[String]] = sc.textFile(&quot;file:///D:\\\\开课吧课程资料\\\\15、scala与spark课程资料\\\\2、spark课程\\\\spark_day05\\\\数据&quot;).map(x=&gt;x.split(&quot; &quot;)) //4、定义一个样例类 //5、将rdd与样例类进行关联 val personRDD: RDD[Person] = data.map(x=&gt;Person(x(0),x(1),x(2).toInt)) //6、将rdd转换成dataFrame //需要手动导入隐式转换 import spark.implicits._ val personDF: DataFrame = personRDD.toDF //7、对dataFrame进行相应的语法操作 //todo：----------------- DSL风格语法-----------------start //打印schema personDF.printSchema() //展示数据 personDF.show() //获取第一行数据 val first: Row = personDF.first() println(&quot;first:&quot;+first) //取出前3位数据 val top3: Array[Row] = personDF.head(3) top3.foreach(println) //获取name字段 personDF.select(&quot;name&quot;).show() personDF.select($&quot;name&quot;).show() personDF.select(new Column(&quot;name&quot;)).show() personDF.select(&quot;name&quot;,&quot;age&quot;).show() //实现age +1 personDF.select($&quot;name&quot;,$&quot;age&quot;,$&quot;age&quot;+1).show() //按照age过滤 personDF.filter($&quot;age&quot; &gt;30).show() val count: Long = personDF.filter($&quot;age&quot; &gt;30).count() println(&quot;count:&quot;+count) //分组 personDF.groupBy(&quot;age&quot;).count().show() personDF.show() personDF.foreach(row =&gt; println(row)) //使用foreach获取每一个row对象中的name字段 personDF.foreach(row =&gt;println(row.getAs[String](&quot;name&quot;))) personDF.foreach(row =&gt;println(row.get(1))) personDF.foreach(row =&gt;println(row.getString(1))) personDF.foreach(row =&gt;println(row.getAs[String](1))) //todo：----------------- DSL风格语法--------------------end //todo：----------------- SQL风格语法-----------------start personDF.createTempView(&quot;person&quot;) //使用SparkSession调用sql方法统计查询 spark.sql(&quot;select * from person&quot;).show spark.sql(&quot;select name from person&quot;).show spark.sql(&quot;select name,age from person&quot;).show spark.sql(&quot;select * from person where age &gt;30&quot;).show spark.sql(&quot;select count(*) from person where age &gt;30&quot;).show spark.sql(&quot;select age,count(*) from person group by age&quot;).show spark.sql(&quot;select age,count(*) as count from person group by age&quot;).show spark.sql(&quot;select * from person order by age desc&quot;).show //todo：----------------- SQL风格语法----------------------end //关闭sparkSession对象 spark.stop() &#125;&#125; 7.2 通过StructType动态指定Schema 应用场景 1在开发代码之前，是无法确定需要的DataFrame对应的schema元信息。需要在开发代码的过程中动态指定。 代码开发 123456789101112131415161718192021222324252627282930313233343536373839import org.apache.spark.SparkContextimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;import org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;//todo；通过动态指定dataFrame对应的schema信息将rdd转换成dataFrameobject StructTypeSchema &#123; def main(args: Array[String]): Unit = &#123; //1、构建SparkSession对象 val spark: SparkSession = SparkSession.builder().appName(&quot;StructTypeSchema&quot;).master(&quot;local[2]&quot;).getOrCreate() //2、获取sparkContext对象 val sc: SparkContext = spark.sparkContext sc.setLogLevel(&quot;warn&quot;) //3、读取文件数据 val data: RDD[Array[String]] = sc.textFile(&quot;file:///D:\\\\开课吧课程资料\\\\15、scala与spark课程资料\\\\2、spark课程\\\\spark_day05\\\\数据&quot;).map(x=&gt;x.split(&quot; &quot;)) //4、将rdd与Row对象进行关联 val rowRDD: RDD[Row] = data.map(x=&gt;Row(x(0),x(1),x(2).toInt)) //5、指定dataFrame的schema信息 //这里指定的字段个数和类型必须要跟Row对象保持一致 val schema=StructType( StructField(&quot;id&quot;,StringType):: StructField(&quot;name&quot;,StringType):: StructField(&quot;age&quot;,IntegerType)::Nil ) val dataFrame: DataFrame = spark.createDataFrame(rowRDD,schema) dataFrame.printSchema() dataFrame.show() dataFrame.createTempView(&quot;user&quot;) spark.sql(&quot;select * from user&quot;).show() spark.stop() &#125;&#125; 8、sparkSQL读取sql数据 spark sql可以通过 JDBC 从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中 添加mysql连接驱动jar包 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt;&lt;/dependency&gt; 代码开发 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.Propertiesimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;//todo:利用sparksql加载mysql表中的数据object DataFromMysql &#123; def main(args: Array[String]): Unit = &#123; //1、创建SparkConf对象 val sparkConf: SparkConf = new SparkConf().setAppName(&quot;DataFromMysql&quot;).setMaster(&quot;local[2]&quot;) //2、创建SparkSession对象 val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate() //3、读取mysql表的数据 //3.1 指定mysql连接地址 val url=&quot;jdbc:mysql://localhost:3306/mydb?characterEncoding=UTF-8&quot; //3.2 指定要加载的表名 val tableName=&quot;jobdetail&quot; // 3.3 配置连接数据库的相关属性 val properties = new Properties() //用户名 properties.setProperty(&quot;user&quot;,&quot;root&quot;) //密码 properties.setProperty(&quot;password&quot;,&quot;123456&quot;) val mysqlDF: DataFrame = spark.read.jdbc(url,tableName,properties) //打印schema信息 mysqlDF.printSchema() //展示数据 mysqlDF.show() //把dataFrame注册成表 mysqlDF.createTempView(&quot;job_detail&quot;) spark.sql(&quot;select * from job_detail where city = &#x27;广东&#x27; &quot;).show() spark.stop() &#125;&#125; 9、sparkSQL操作CSV文件并将结果写入mysql 使用spark程序读取CSV文件，然后将读取到的数据内容，保存到mysql里面去，注意csv文件的换行问题。 1234567891011121314151617181920212223242526272829import java.util.Propertiesimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SaveMode, SparkSession&#125;object CSVOperate &#123; def main(args: Array[String]): Unit = &#123; val sparkConf: SparkConf = new SparkConf().setMaster(&quot;local[8]&quot;).setAppName(&quot;sparkCSV&quot;) val session: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate() session.sparkContext.setLogLevel(&quot;WARN&quot;) val frame: DataFrame = session .read .format(&quot;csv&quot;) .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;) .option(&quot;header&quot;, &quot;true&quot;) .option(&quot;multiLine&quot;, true) .load(&quot;file:///D:\\\\开课吧课程资料\\\\15、scala与spark课程资料\\\\2、spark课程\\\\spark_day05\\\\数据\\\\招聘数据&quot;) frame.createOrReplaceTempView(&quot;job_detail&quot;) //session.sql(&quot;select job_name,job_url,job_location,job_salary,job_company,job_experience,job_class,job_given,job_detail,company_type,company_person,search_key,city from job_detail where job_company = &#x27;北京无极慧通科技有限公司&#x27; &quot;).show(80) val prop = new Properties() prop.put(&quot;user&quot;, &quot;root&quot;) prop.put(&quot;password&quot;, &quot;123456&quot;) frame.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql://localhost:3306/mydb?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8&quot;, &quot;mydb.jobdetail_copy&quot;, prop) &#125;&#125; 10、spark on hive 与hive on spark Spark on hive 与 Hive on Spark 的区别 Spark on hive Spark通过Spark-SQL使用hive 语句,操作hive,底层运行的还是 spark rdd。 （1）就是通过sparksql，加载hive的配置文件，获取到hive的元数据信息 （2）spark sql获取到hive的元数据信息之后就可以拿到hive的所有表的数据 （3）接下来就可以通过spark sql来操作hive表中的数据 Hive on Spark 是把hive查询从mapreduce 的mr (Hadoop计算引擎)操作替换为spark rdd（spark 执行引擎） 操作. 相对于spark on hive,这个要实现起来则麻烦很多, 必须重新编译你的spark和导入jar包，不过目前大部分使用的是spark on hive。 spark与hive.pptx 1、spark_sql与hive进行整合 第一步：拷贝hive-site.xml配置文件 将node03服务器安装的hive家目录下的conf文件夹下面的hive-site.xml拷贝到spark安装的各个机器节点，node03执行以下命令进行拷贝 1234cd /kkb/install/hive-1.1.0-cdh5.14.2/confscp hive-site.xml node01:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/scp hive-site.xml node02:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/scp hive-site.xml node03:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/ 第二步：拷贝mysql连接驱动包 将hive当中mysql的连接驱动包拷贝到spark安装家目录下的lib目录下，node03执行下命令拷贝mysql的lib驱动包 1234cd /kkb/install/hive-1.1.0-cdh5.14.2/lib/scp mysql-connector-java-5.1.38.jar node01:/kkb/install/spark-2.3.3-bin-hadoop2.7/jars/scp mysql-connector-java-5.1.38.jar node02:/kkb/install/spark-2.3.3-bin-hadoop2.7/jars/scp mysql-connector-java-5.1.38.jar node03:/kkb/install/spark-2.3.3-bin-hadoop2.7/jars/ 第三步：进入spark-sql直接操作hive数据库当中的数据 在spark2.0版本后由于出现了sparkSession，在初始化sqlContext的时候，会设置默认的spark.sql.warehouse.dir=spark-warehouse, 此时将hive与sparksql整合完成之后，在通过spark-sql脚本启动的时候，还是会在哪里启动spark-sql脚本，就会在当前目录下创建一个spark.sql.warehouse.dir为spark-warehouse的目录，存放由spark-sql创建数据库和创建表的数据信息，与之前hive的数据息不是放在同一个路径下（可以互相访问）。但是此时spark-sql中表的数据在本地，不利于操作，也不安全。 所有在启动的时候需要加上这样一个参数： –conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse 保证spark-sql启动时不在产生新的存放数据的目录，sparksql与hive最终使用的是hive同一存放数据的目录。 node01直接执行以下命令，进入spark-sql交互界面，然后操作hive当中的数据， 123456cd /kkb/install/spark-2.3.3-bin-hadoop2.7/bin/spark-sql --master local[2] \\--executor-memory 512m --total-executor-cores 3 \\--conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \\--jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar 使用sparkSQL有hive进行整合之后，就可以通过sparkSQL语句来操作hive表数据了 应用场景 123456789#!/bin/sh#定义sparksql提交脚本的头信息SUBMITINFO=&quot;spark-sql --master spark://node01:7077 --executor-memory 1g --total-executor-cores 4 --conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse&quot; #定义一个sql语句SQL=&quot;select * from default.hive_source;&quot; #执行sql语句 类似于 hive -e sql语句echo &quot;$SUBMITINFO&quot; echo &quot;$SQL&quot;$SUBMITINFO -e &quot;$SQL&quot; 2、启用spark的thrift server与hive进行远程交互 除了可以通过spark-shell来与hive进行整合之外，我们也可以通过spark的thrift服务来远程与hive进行交互 第一步：修改hive-site.xml的配置 node03执行以下命令修改hive-site.xml的配置属性，添加以下几个配置 12345678910111213141516171819202122232425cd /kkb/install/hive-1.1.0-cdh5.14.2/confvim hive-site.xml&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node03:9083&lt;/value&gt; &lt;description&gt;Thrift URI for the remote metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.min.worker.threads&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.max.worker.threads&lt;/name&gt; &lt;value&gt;500&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;node03&lt;/value&gt; &lt;/property&gt; 第二步：修改完的配置文件分发到其他机器 node03执行以下命令分发hive配置文件 1234cd /kkb/install/hive-1.1.0-cdh5.14.2/confscp hive-site.xml node01:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/scp hive-site.xml node02:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/scp hive-site.xml node03:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/ 第三步：node03启动metastore服务 node03执行以下命令启动metastore服务 123cd /kkb/install/hive-1.1.0-cdh5.14.2/bin/hive --service metastore 第四步：node03执行以下命令启动spark的thrift server 注意：hive安装在哪一台，就在哪一台服务器启动spark的thrift server 我的hive安装在node03服务器，所以我在node03服务器上面启动spark的thrift server服务 node03执行以下命令启动thrift server服务 1234cd /kkb/install/spark-2.3.3-bin-hadoop2.7sbin/start-thriftserver.sh --master local[2] --executor-memory 5g --total-executor-cores 5 --jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar 第五步：直接使用beeline来连接 直接在node03服务器上面使用beeline来进行连接spark-sql 12345678cd /kkb/install/spark-2.3.3-bin-hadoop2.7bin/beeline beeline&gt; !connect jdbc:hive2://node03:10000Connecting to jdbc:hive2://node03:10000Enter username for jdbc:hive2://node03:10000: hadoopEnter password for jdbc:hive2://node03:10000: ****** 11、sparkSQL自定义函数 用户自定义函数类别分为以下三种： 1).UDF：输入一行，返回一个结果(一对一)，在上篇案例 使用SparkSQL实现根据ip地址计算归属地二 中实现的自定义函数就是UDF，输入一个十进制的ip地址，返回一个省份 2).UDTF：输入一行，返回多行(一对多)，在SparkSQL中没有，因为Spark中使用flatMap即可实现这个功能 3).UDAF：输入多行，返回一行，这里的A是aggregate，聚合的意思，如果业务复杂，需要自己实现聚合函数 1、自定义UDF函数 读取深圳二手房成交数据，对房子的年份进行自定义函数处理，如果没有年份，那么就给默认值1990 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.regex.&#123;Matcher, Pattern&#125;import org.apache.spark.SparkConfimport org.apache.spark.sql.api.java.UDF1import org.apache.spark.sql.types.DataTypesimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;object SparkUDF &#123; def main(args: Array[String]): Unit = &#123; val sparkConf: SparkConf = new SparkConf().setMaster(&quot;local[8]&quot;).setAppName(&quot;sparkCSV&quot;) val session: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate() session.sparkContext.setLogLevel(&quot;WARN&quot;) val frame: DataFrame = session .read .format(&quot;csv&quot;) .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;) .option(&quot;header&quot;, &quot;true&quot;) .option(&quot;multiLine&quot;, true) .load(&quot;file:///D:\\\\开课吧课程资料\\\\15、scala与spark课程资料\\\\2、spark课程\\\\spark_day05\\\\数据\\\\深圳链家二手房成交明细&quot;) frame.createOrReplaceTempView(&quot;house_sale&quot;) session.udf.register(&quot;house_udf&quot;,new UDF1[String,String] &#123; val pattern: Pattern = Pattern.compile(&quot;^[0-9]*$&quot;) override def call(input: String): String = &#123; val matcher: Matcher = pattern.matcher(input) if(matcher.matches())&#123; input &#125;else&#123; &quot;1990&quot; &#125; &#125; &#125;,DataTypes.StringType) session.sql(&quot;select house_udf(house_age) from house_sale limit 200&quot;).show() session.stop() &#125;&#125; 2、自定义UDAF函数 需求：自定义UDAF函数，读取深圳二手房数据，然后按照楼层进行分组，求取每个楼层的平均成交金额 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;import org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types._class MyAverage extends UserDefinedAggregateFunction &#123; // 聚合函数输入参数的数据类型 def inputSchema: StructType = StructType(StructField(&quot;floor&quot;, DoubleType) :: Nil) // 聚合缓冲区中值得数据类型 def bufferSchema: StructType = &#123; StructType(StructField(&quot;sum&quot;, DoubleType) :: StructField(&quot;count&quot;, LongType) :: Nil) &#125; // 返回值的数据类型 def dataType: DataType = DoubleType // 对于相同的输入是否一直返回相同的输出。 def deterministic: Boolean = true // 初始化 def initialize(buffer: MutableAggregationBuffer): Unit = &#123; // 用于存储不同类型的楼房的总成交额 buffer(0) = 0D // 用于存储不同类型的楼房的总个数 buffer(1) = 0L &#125; // 相同Execute间的数据合并。 def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; if (!input.isNullAt(0)) &#123; buffer(0) = buffer.getDouble(0) + input.getDouble(0) buffer(1) = buffer.getLong(1) + 1 &#125; &#125; // 不同Execute间的数据合并 def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getDouble(0) + buffer2.getDouble(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) &#125; // 计算最终结果 def evaluate(buffer: Row): Double = buffer.getDouble(0) / buffer.getLong(1)&#125;object MyAverage&#123; def main(args: Array[String]): Unit = &#123; val sparkConf: SparkConf = new SparkConf().setMaster(&quot;local[8]&quot;).setAppName(&quot;sparkCSV&quot;) val session: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate() session.sparkContext.setLogLevel(&quot;WARN&quot;) val frame: DataFrame = session .read .format(&quot;csv&quot;) .option(&quot;timestampFormat&quot;, &quot;yyyy/MM/dd HH:mm:ss ZZ&quot;) .option(&quot;header&quot;, &quot;true&quot;) .option(&quot;multiLine&quot;, true) .load(&quot;file:///D:\\\\开课吧课程资料\\\\15、scala与spark课程资料\\\\2、spark课程\\\\spark_day05\\\\数据\\\\深圳链家二手房成交明细&quot;) frame.createOrReplaceTempView(&quot;house_sale&quot;) session.sql(&quot;select floor from house_sale limit 30&quot;).show() session.udf.register(&quot;udaf&quot;,new MyAverage) session.sql(&quot;select floor, udaf(house_sale_money) from house_sale group by floor&quot;).show() frame.printSchema() session.stop() &#125;&#125; 12、sparkSQL架构设计 sparkSQL是spark技术栈当中又一非常出彩的模块，通过引入SQL的支持，大大降低了开发人员和学习人员的使用成本，让我们开发人员直接使用SQL的方式就能够实现大数据的开发，它同时支持DSL以及SQL的语法风格，目前在spark的整个架构设计当中，所有的spark模块，例如SQL，SparkML，sparkGrahpx以及Structed Streaming等都是基于 Catalyst Optimization &amp; Tungsten Execution模块之上运行，如下图所示就显示了spark的整体架构模块设计 1、sparkSQL的架构设计实现 sparkSQL 执行先会经过 SQL Parser 解析 SQL，然后经过 Catalyst 优化器处理，最后到 Spark 执行。而 Catalyst 的过程又分为很多个过程，其中包括： Analysis：主要利用 Catalog 信息将 Unresolved Logical Plan 解析成 Analyzed logical plan； Logical Optimizations：利用一些 Rule （规则）将 Analyzed logical plan 解析成 Optimized Logical Plan； Physical Planning：前面的 logical plan 不能被 Spark 执行，而这个过程是把 logical plan 转换成多个 physical plans，然后利用代价模型（cost model）选择最佳的 physical plan； Code Generation：这个过程会把 SQL 查询生成 Java 字 节码。 sparkSQL架构设计.pptx 例如执行以下SQL语句： 1select temp1.class,sum(temp1.degree),avg(temp1.degree) from (SELECT students.sno AS ssno,students.sname,students.ssex,students.sbirthday,students.class, scores.sno,scores.degree,scores.cno FROM students LEFT JOIN scores ON students.sno = scores.sno ) temp1 group by temp1.class 代码实现过程如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.kkb.sparksqlimport java.util.Propertiesimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;//todo:利用sparksql加载mysql表中的数据object DataFromMysqlPlan &#123; def main(args: Array[String]): Unit = &#123; //1、创建SparkConf对象 val sparkConf: SparkConf = new SparkConf().setAppName(&quot;DataFromMysql&quot;).setMaster(&quot;local[2]&quot;) //sparkConf.set(&quot;spark.sql.codegen.wholeStage&quot;,&quot;true&quot;) //2、创建SparkSession对象 val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate() spark.sparkContext.setLogLevel(&quot;WARN&quot;) //3、读取mysql表的数据 //3.1 指定mysql连接地址 val url=&quot;jdbc:mysql://localhost:3306/mydb?characterEncoding=UTF-8&quot; //3.2 指定要加载的表名 val student=&quot;students&quot; val score=&quot;scores&quot; // 3.3 配置连接数据库的相关属性 val properties = new Properties() //用户名 properties.setProperty(&quot;user&quot;,&quot;root&quot;) //密码 properties.setProperty(&quot;password&quot;,&quot;123456&quot;) val studentFrame: DataFrame = spark.read.jdbc(url,student,properties) val scoreFrame: DataFrame = spark.read.jdbc(url,score,properties) //把dataFrame注册成表 studentFrame.createTempView(&quot;students&quot;) scoreFrame.createOrReplaceTempView(&quot;scores&quot;) //spark.sql(&quot;SELECT temp1.class,SUM(temp1.degree),AVG(temp1.degree) FROM (SELECT students.sno AS ssno,students.sname,students.ssex,students.sbirthday,students.class, scores.sno,scores.degree,scores.cno FROM students LEFT JOIN scores ON students.sno = scores.sno ) temp1 GROUP BY temp1.class; &quot;).show() val resultFrame: DataFrame = spark.sql(&quot;SELECT temp1.class,SUM(temp1.degree),AVG(temp1.degree) FROM (SELECT students.sno AS ssno,students.sname,students.ssex,students.sbirthday,students.class, scores.sno,scores.degree,scores.cno FROM students LEFT JOIN scores ON students.sno = scores.sno WHERE degree &gt; 60 AND sbirthday &gt; &#x27;1973-01-01 00:00:00&#x27; ) temp1 GROUP BY temp1.class&quot;) resultFrame.explain(true) resultFrame.show() spark.stop() &#125;&#125; 通过explain方法来查看sql的执行计划，得到以下信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546== Parsed Logical Plan ==&#x27;Aggregate [&#x27;temp1.class], [&#x27;temp1.class, unresolvedalias(&#x27;SUM(&#x27;temp1.degree), None), unresolvedalias(&#x27;AVG(&#x27;temp1.degree), None)]+- &#x27;SubqueryAlias temp1 +- &#x27;Project [&#x27;students.sno AS ssno#16, &#x27;students.sname, &#x27;students.ssex, &#x27;students.sbirthday, &#x27;students.class, &#x27;scores.sno, &#x27;scores.degree, &#x27;scores.cno] +- &#x27;Filter ((&#x27;degree &gt; 60) &amp;&amp; (&#x27;sbirthday &gt; 1973-01-01 00:00:00)) +- &#x27;Join LeftOuter, (&#x27;students.sno = &#x27;scores.sno) :- &#x27;UnresolvedRelation `students` +- &#x27;UnresolvedRelation `scores`== Analyzed Logical Plan ==class: string, sum(degree): decimal(20,1), avg(degree): decimal(14,5)Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, avg(degree#12) AS avg(degree)#28]+- SubqueryAlias temp1 +- Project [sno#0 AS ssno#16, sname#1, ssex#2, sbirthday#3, class#4, sno#10, degree#12, cno#11] +- Filter ((cast(degree#12 as decimal(10,1)) &gt; cast(cast(60 as decimal(2,0)) as decimal(10,1))) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)) +- Join LeftOuter, (sno#0 = sno#10) :- SubqueryAlias students : +- Relation[sno#0,sname#1,ssex#2,sbirthday#3,class#4] JDBCRelation(students) [numPartitions=1] +- SubqueryAlias scores +- Relation[sno#10,cno#11,degree#12] JDBCRelation(scores) [numPartitions=1]== Optimized Logical Plan ==Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, cast((avg(UnscaledValue(degree#12)) / 10.0) as decimal(14,5)) AS avg(degree)#28]+- Project [class#4, degree#12] +- Join Inner, (sno#0 = sno#10) :- Project [sno#0, class#4] : +- Filter ((isnotnull(sbirthday#3) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)) &amp;&amp; isnotnull(sno#0)) : +- Relation[sno#0,sname#1,ssex#2,sbirthday#3,class#4] JDBCRelation(students) [numPartitions=1] +- Project [sno#10, degree#12] +- Filter ((isnotnull(degree#12) &amp;&amp; (degree#12 &gt; 60.0)) &amp;&amp; isnotnull(sno#10)) +- Relation[sno#10,cno#11,degree#12] JDBCRelation(scores) [numPartitions=1]== Physical Plan ==*(6) HashAggregate(keys=[class#4], functions=[sum(degree#12), avg(UnscaledValue(degree#12))], output=[class#4, sum(degree)#27, avg(degree)#28])+- Exchange hashpartitioning(class#4, 200) +- *(5) HashAggregate(keys=[class#4], functions=[partial_sum(degree#12), partial_avg(UnscaledValue(degree#12))], output=[class#4, sum#32, sum#33, count#34L]) +- *(5) Project [class#4, degree#12] +- *(5) SortMergeJoin [sno#0], [sno#10], Inner :- *(2) Sort [sno#0 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(sno#0, 200) : +- *(1) Project [sno#0, class#4] : +- *(1) Filter (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00) : +- *(1) Scan JDBCRelation(students) [numPartitions=1] [sno#0,class#4,sbirthday#3] PushedFilters: [*IsNotNull(sbirthday), *IsNotNull(sno)], ReadSchema: struct&lt;sno:string,class:string,sbirthday:timestamp&gt; +- *(4) Sort [sno#10 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(sno#10, 200) +- *(3) Scan JDBCRelation(scores) [numPartitions=1] [sno#10,degree#12] PushedFilters: [*IsNotNull(degree), *GreaterThan(degree,60.0), *IsNotNull(sno)], ReadSchema: struct&lt;sno:string,degree:decimal(10,1)&gt; 2、 Catalyst执行过程 从上面的查询计划我们可以看得出来，我们编写的sql语句，经过多次转换，最终进行编译成为字节码文件进行执行，这一整个过程经过了好多个步骤，其中包括以下几个重要步骤 sql解析阶段 parse 生成逻辑计划 Analyzer sql语句调优阶段 Optimizer 生成物理查询计划 planner 1、sql解析阶段 Parser 在spark2.x的版本当中，为了解析sparkSQL的sql语句，引入了Antlr。Antlr 是一款强大的语法生成器工具，可用于读取、处理、执行和翻译结构化的文本或二进制文件，是当前 Java 语言中使用最为广泛的语法生成器工具，我们常见的大数据 SQL 解析都用到了这个工具，包括 Hive、Cassandra、Phoenix、Pig 以及 presto 等。目前最新版本的 Spark 使用的是ANTLR4，通过这个对 SQL 进行词法分析并构建语法树。 我们可以通过github去查看spark的源码，具体路径如下： https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4 查看得到sparkSQL支持的SQL语法，所有sparkSQL支持的语法都定义在了这个文件当中。如果我们需要重构sparkSQL的语法，那么我们只需要重新定义好相关语法，然后使用Antlr4对SqlBase.g4进行语法解析，生成相关的java类，其中就包含重要的词法解析器SqlBaseLexer.java和语法解析器SqlBaseParser.java。在我们运行上面的java的时候，第一步就是通过SqlBaseLexer来解析关键词以及各种标识符，然后使用SqlBaseParser来构建语法树。 最终通过Lexer以及parse解析之后，生成语法树，生成语法树之后，使用AstBuilder将语法树转换成为LogicalPlan，这个LogicalPlan也被称为Unresolved LogicalPlan。解析之后的逻辑计划如下， 12345678== Parsed Logical Plan ==&#x27;Aggregate [&#x27;temp1.class], [&#x27;temp1.class, unresolvedalias(&#x27;SUM(&#x27;temp1.degree), None), unresolvedalias(&#x27;AVG(&#x27;temp1.degree), None)]+- &#x27;SubqueryAlias temp1 +- &#x27;Project [&#x27;students.sno AS ssno#16, &#x27;students.sname, &#x27;students.ssex, &#x27;students.sbirthday, &#x27;students.class, &#x27;scores.sno, &#x27;scores.degree, &#x27;scores.cno] +- &#x27;Filter ((&#x27;degree &gt; 60) &amp;&amp; (&#x27;sbirthday &gt; 1973-01-01 00:00:00)) +- &#x27;Join LeftOuter, (&#x27;students.sno = &#x27;scores.sno) :- &#x27;UnresolvedRelation `students` +- &#x27;UnresolvedRelation `scores` 从上图可以看得到，两个表被join之后生成了UnresolvedRelation，选择的列以及聚合的字段都有了，sql解析的第一个阶段就已经完成，接着准备进入到第二个阶段 2、绑定逻辑计划Analyzer 在sql解析parse阶段，生成了很多的unresolvedalias ， UnresolvedRelation等很多未解析出来的有些关键字，这些都是属于 Unresolved LogicalPlan解析的部分。 Unresolved LogicalPlan仅仅是一种数据结构，不包含任何数据信息，例如不知道数据源，数据类型，不同的列来自哪张表等等。。Analyzer 阶段会使用事先定义好的 Rule 以及 SessionCatalog 等信息对 Unresolved LogicalPlan 进行 transform。SessionCatalog 主要用于各种函数资源信息和元数据信息（数据库、数据表、数据视图、数据分区与函数等）的统一管理。而Rule 是定义在 Analyzer 里面的，具体的类的路径如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859org.apache.spark.sql.catalyst.analysis.Analyzer具体的rule规则定义如下： lazy val batches: Seq[Batch] = Seq( Batch(&quot;Hints&quot;, fixedPoint, new ResolveHints.ResolveBroadcastHints(conf), ResolveHints.RemoveAllHints), Batch(&quot;Simple Sanity Check&quot;, Once, LookupFunctions), Batch(&quot;Substitution&quot;, fixedPoint, CTESubstitution, WindowsSubstitution, EliminateUnions, new SubstituteUnresolvedOrdinals(conf)), Batch(&quot;Resolution&quot;, fixedPoint, ResolveTableValuedFunctions :: ResolveRelations :: ResolveReferences :: ResolveCreateNamedStruct :: ResolveDeserializer :: ResolveNewInstance :: ResolveUpCast :: ResolveGroupingAnalytics :: ResolvePivot :: ResolveOrdinalInOrderByAndGroupBy :: ResolveAggAliasInGroupBy :: ResolveMissingReferences :: ExtractGenerator :: ResolveGenerate :: ResolveFunctions :: ResolveAliases :: ResolveSubquery :: ResolveSubqueryColumnAliases :: ResolveWindowOrder :: ResolveWindowFrame :: ResolveNaturalAndUsingJoin :: ExtractWindowExpressions :: GlobalAggregates :: ResolveAggregateFunctions :: TimeWindowing :: ResolveInlineTables(conf) :: ResolveTimeZone(conf) :: ResolvedUuidExpressions :: TypeCoercion.typeCoercionRules(conf) ++ extendedResolutionRules : _*), Batch(&quot;Post-Hoc Resolution&quot;, Once, postHocResolutionRules: _*), Batch(&quot;View&quot;, Once, AliasViewChild(conf)), Batch(&quot;Nondeterministic&quot;, Once, PullOutNondeterministic), Batch(&quot;UDF&quot;, Once, HandleNullInputsForUDF), Batch(&quot;FixNullability&quot;, Once, FixNullability), Batch(&quot;Subquery&quot;, Once, UpdateOuterReferences), Batch(&quot;Cleanup&quot;, fixedPoint, CleanupAliases) ) 从上面代码可以看出，多个性质类似的 Rule 组成一个 Batch，比如上面名为 Hints 的 Batch就是由很多个 Hints Rule 组成；而多个 Batch 构成一个 batches。这些 batches 会由 RuleExecutor 执行，先按一个一个 Batch 顺序执行，然后对 Batch 里面的每个 Rule 顺序执行。每个 Batch 会执行一次（Once）或多次（FixedPoint，由 spark.sql.optimizer.maxIterations 参数决定），执行过程如下： 所以上面的 SQL 经过这个阶段生成的 Analyzed Logical Plan 如下： 1234567891011== Analyzed Logical Plan ==class: string, sum(degree): decimal(20,1), avg(degree): decimal(14,5)Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, avg(degree#12) AS avg(degree)#28]+- SubqueryAlias temp1 +- Project [sno#0 AS ssno#16, sname#1, ssex#2, sbirthday#3, class#4, sno#10, degree#12, cno#11] +- Filter ((cast(degree#12 as decimal(10,1)) &gt; cast(cast(60 as decimal(2,0)) as decimal(10,1))) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)) +- Join LeftOuter, (sno#0 = sno#10) :- SubqueryAlias students : +- Relation[sno#0,sname#1,ssex#2,sbirthday#3,class#4] JDBCRelation(students) [numPartitions=1] +- SubqueryAlias scores +- Relation[sno#10,cno#11,degree#12] JDBCRelation(scores) [numPartitions=1] 从上面的解析过程来看，students和scores表已经被解析成为了带有sno#0 AS ssno#16, sname#1, ssex#2, sbirthday#3, class#4, sno#10, degree#12, cno#11这么具体的字段，其中还有聚合函数 Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, avg(degree#12) AS avg(degree)#28]，并且最终返回的四个字段的类型也已经确定了class: string, sum(degree): decimal(20,1), avg(degree): decimal(14,5)，而且也已经知道了数据来源是JDBCRelation(students)表和 JDBCRelation(scores)表。总结来看Analyzed Logical Plan主要就是干了一些这些事情 1、确定最终返回字段名称以及返回类型： ​ class: string, sum(degree): decimal(20,1), avg(degree): decimal(14,5) 2、确定聚合函数 Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, avg(degree#12) AS avg(degree)#28] 3、确定表当中获取的查询字段 ​ Project [sno#0 AS ssno#16, sname#1, ssex#2, sbirthday#3, class#4, sno#10, degree#12, cno#11] 4、确定过滤条件 Filter ((cast(degree#12 as decimal(10,1)) &gt; cast(cast(60 as decimal(2,0)) as decimal(10,1))) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)) 5、确定join方式 Join LeftOuter, (sno#0 = sno#10) 6、确定表当中的数据来源以及分区个数 JDBCRelation(students) [numPartitions=1] JDBCRelation(scores) [numPartitions=1] 至此Analyzed Logical Plan已经完成。对比Unresolved Logical Plan到Analyzed Logical Plan 过程如下图 到这里， Analyzed LogicalPlan 就完全生成了 3、逻辑优化阶段Optimizer 在前文的绑定逻辑计划阶段对 Unresolved LogicalPlan 进行相关 transform 操作得到了 Analyzed Logical Plan，这个 Analyzed Logical Plan 是可以直接转换成 Physical Plan 然后在 [Spark] 中执行。但是如果直接这么弄的话，得到的 Physical Plan 很可能不是最优的，因为在实际应用中，很多低效的写法会带来执行效率的问题，需要进一步对Analyzed Logical Plan 进行处理，得到更优的逻辑算子树。于是， 针对 SQL 逻辑算子树的优化器 Optimizer 应运而生。 这个阶段的优化器主要是基于规则的（Rule-based Optimizer，简称 RBO），而绝大部分的规则都是启发式规则，也就是基于直观或经验而得出的规则，比如列裁剪（过滤掉查询不需要使用到的列）、谓词下推（将过滤尽可能地下沉到数据源端）、常量累加（比如 1 + 2 这种事先计算好） 以及常量替换（比如 SELECT * FROM table WHERE i = 5 AND j = i + 3 可以转换成 SELECT * FROM table WHERE i = 5 AND j = 8）等等。 与前文介绍绑定逻辑计划阶段类似，这个阶段所有的规则也是实现 Rule 抽象类，多个规则组成一个 Batch，多个 Batch 组成一个 batches，同样也是在 RuleExecutor 中进行执行 这里按照 Rule 执行顺序一一进行说明。 谓词下推 谓词下推在 SparkQL 是由 PushDownPredicate 实现的，这个过程主要将过滤条件尽可能地下推到底层，最好是数据源。所以针对我们上面介绍的 SQL，使用谓词下推优化得到的逻辑计划如下： 从上图可以看出，谓词下推将 Filter 算子直接下推到 Join 之前了（注意，上图是从下往上看的） 。也就是在扫描 student表的时候使用条件过滤条件过滤出满足条件的数据；同时在扫描 t2 表的时候会先使用 isnotnull(id#8) &amp;&amp; (id#8 &gt; 50000) 过滤条件过滤出满足条件的数据。经过这样的操作，可以大大减少 Join 算子处理的数据量，从而加快计算速度。 列裁剪 列裁剪在 Spark SQL 是由 ColumnPruning 实现的。因为我们查询的表可能有很多个字段，但是每次查询我们很大可能不需要扫描出所有的字段，这个时候利用列裁剪可以把那些查询不需要的字段过滤掉，使得扫描的数据量减少。所以针对我们上面介绍的 SQL，使用列裁剪优化得到的逻辑计划如下： 从上图可以看出，经过列裁剪后，students 表只需要查询 sno和 class 两个字段；scores 表只需要查询 sno,degree 字段。这样减少了数据的传输，而且如果底层的文件格式为列存（比如 Parquet），可以大大提高数据的扫描速度的。 常量替换 常量替换在 Spark SQL 是由 ConstantPropagation 实现的。也就是将变量替换成常量，比如 SELECT * FROM table WHERE i = 5 AND j = i + 3 可以转换成 SELECT * FROM table WHERE i = 5 AND j = 8。这个看起来好像没什么的，但是如果扫描的行数非常多可以减少很多的计算时间的开销的。经过这个优化，得到的逻辑计划如下 我们的查询中有 t1.cid = 1 AND t1.did = t1.cid + 1 查询语句，从里面可以看出 t1.cid 其实已经是确定的值了，所以我们完全可以使用它计算出 t1.did。 常量累加 常量累加在 Spark SQL 是由 ConstantFolding 实现的。这个和常量替换类似，也是在这个阶段把一些常量表达式事先计算好。这个看起来改动的不大，但是在数据量非常大的时候可以减少大量的计算，减少 CPU 等资源的使用。经过这个优化，得到的逻辑计划如下： 所以经过上面四个步骤的优化之后，得到的优化之后的逻辑计划为： 12345678910== Optimized Logical Plan ==Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, cast((avg(UnscaledValue(degree#12)) / 10.0) as decimal(14,5)) AS avg(degree)#28]+- Project [class#4, degree#12] +- Join Inner, (sno#0 = sno#10) :- Project [sno#0, class#4] : +- Filter ((isnotnull(sbirthday#3) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)) &amp;&amp; isnotnull(sno#0)) : +- Relation[sno#0,sname#1,ssex#2,sbirthday#3,class#4] JDBCRelation(students) [numPartitions=1] +- Project [sno#10, degree#12] +- Filter ((isnotnull(degree#12) &amp;&amp; (degree#12 &gt; 60.0)) &amp;&amp; isnotnull(sno#10)) +- Relation[sno#10,cno#11,degree#12] JDBCRelation(scores) [numPartitions=1] 到此为止，优化逻辑阶段基本完成，另外更多的其他优化，参见spark源码： https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L59 4、生成可执行的物理计划阶段Physical Plan 经过前面多个步骤，包括parse，analyzer以及Optimizer等多个阶段，得到经过优化之后的sql语句，但是这个sql语句仍然不能执行，为了能够执行这个sql，最终必须得要翻译成为可以被执行的物理计划，到这个阶段spark就知道该如何执行这个sql了，和前面逻辑计划绑定和优化不一样，这个阶段使用的是策略strategy，而且经过前面介绍的逻辑计划绑定和 Transformations 动作之后，树的类型并没有改变，也就是说：Expression 经过 Transformations 之后得到的还是 Transformations ；Logical Plan 经过 Transformations 之后得到的还是 Logical Plan。而到了这个阶段，经过 Transformations 动作之后，树的类型改变了，由 Logical Plan 转换成 Physical Plan 了。 一个逻辑计划（Logical Plan）经过一系列的策略处理之后，得到多个物理计划（Physical Plans），物理计划在 Spark 是由 SparkPlan 实现的。多个物理计划再经过代价模型（Cost Model）得到选择后的物理计划（Selected Physical Plan），整个过程如下所示： Cost Model 对应的就是基于代价的优化（Cost-based Optimizations，CBO，主要由华为的大佬们实现的，详见 SPARK-16026 ），核心思想是计算每个物理计划的代价，然后得到最优的物理计划。但是在目前最新版的 Spark 2.4.3，这一部分并没有实现，直接返回多个物理计划列表的第一个作为最优的物理计划，如下 123456lazy val sparkPlan: SparkPlan = &#123; SparkSession.setActiveSession(sparkSession) // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(optimizedPlan)).next()&#125; 而 SPARK-16026 引入的 CBO 优化主要是在前面介绍的优化逻辑计划阶段 - Optimizer 阶段进行的，对应的 Rule 为 CostBasedJoinReorder，并且默认是关闭的，需要通过 spark.sql.cbo.enabled 或 spark.sql.cbo.joinReorder.enabled 参数开启。 所以到了这个节点，最后得到的物理计划如下： 123456789101112131415== Physical Plan ==*(6) HashAggregate(keys=[class#4], functions=[sum(degree#12), avg(UnscaledValue(degree#12))], output=[class#4, sum(degree)#27, avg(degree)#28])+- Exchange hashpartitioning(class#4, 200) +- *(5) HashAggregate(keys=[class#4], functions=[partial_sum(degree#12), partial_avg(UnscaledValue(degree#12))], output=[class#4, sum#32, sum#33, count#34L]) +- *(5) Project [class#4, degree#12] +- *(5) SortMergeJoin [sno#0], [sno#10], Inner :- *(2) Sort [sno#0 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(sno#0, 200) : +- *(1) Project [sno#0, class#4] : +- *(1) Filter (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00) : +- *(1) Scan JDBCRelation(students) [numPartitions=1] [sno#0,class#4,sbirthday#3] PushedFilters: [*IsNotNull(sbirthday), *IsNotNull(sno)], ReadSchema: struct&lt;sno:string,class:string,sbirthday:timestamp&gt; +- *(4) Sort [sno#10 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(sno#10, 200) +- *(3) Scan JDBCRelation(scores) [numPartitions=1] [sno#10,degree#12] PushedFilters: [*IsNotNull(degree), *GreaterThan(degree,60.0), *IsNotNull(sno)], ReadSchema: struct&lt;sno:string,degree:decimal(10,1)&gt; 从上面的结果可以看出，物理计划阶段已经知道数据源是从 JDBC里面读取了，也知道文件的路径，数据类型等。而且在读取文件的时候，直接将过滤条件（PushedFilters）加进去了 同时，这个 Join 变成了 SortMergeJoin， 到这里， Physical Plan 就完全生成了 5、代码生成阶段 从以上多个过程执行完成之后，例如parser，analyzer，Optimizer，physicalPlan等，最终我们得到的物理执行计划，这个物理执行计划标明了整个的代码执行过程当中我们代码层面的执行过程，以及最终要得到的数据字段以及字段类型，也包含了我们对应的数据源的位置，虽然得到了物理执行计划，但是这个物理执行计划想要被执行，最终还是得要生成完整的代码，底层还是基于sparkRDD去进行处理的，spark最后也还会有一些Rule对生成的物理执行计划进行处理，这个处理过程就是prepareForExecution，这些rule规则定义在 org.apache.spark.sql.execution.QueryExecution 这个类当中的这个方法里面 123456789101112protected def prepareForExecution(plan: SparkPlan): SparkPlan = &#123; preparations.foldLeft(plan) &#123; case (sp, rule) =&gt; rule.apply(sp) &#125; &#125; /** A sequence of rules that will be applied in order to the physical plan before execution. */ protected def preparations: Seq[Rule[SparkPlan]] = Seq( python.ExtractPythonUDFs, //抽取python的自定义函数 PlanSubqueries(sparkSession), //子查询物理计划处理 EnsureRequirements(sparkSession.sessionState.conf), //确保执行计划分区排序正确 CollapseCodegenStages(sparkSession.sessionState.conf), //收集生成代码 ReuseExchange(sparkSession.sessionState.conf), //节点重用 ReuseSubquery(sparkSession.sessionState.conf)) //子查询重用 上面的 Rule 中 CollapseCodegenStages 是重头戏，这就是大家熟知的全代码阶段生成，Catalyst 全阶段代码生成的入口就是这个规则。当然，如果需要 Spark 进行全阶段代码生成，需要将 spark.sql.codegen.wholeStage 设置为 true（默认）。 生成代码与sql解析引擎的区别 在sparkSQL当中，通过生成代码，来实现sql语句的最终生成，说白了最后底层执行的还是代码，那么为什么要这么麻烦，使用代码的方式来执行我们的sql语句，难道没有sql的解析引擎直接执行sql语句嘛？当然是有的，在spark2.0版本之前使用的都是基于Volcano Iterator Model（参见 《Volcano-An Extensible and Parallel Query Evaluation System》） 来实现sql的解析的，这个是由 Goetz Graefe 在 1993 年提出的，当今绝大多数数据库系统处理 SQL 在底层都是基于这个模型的。这个模型的执行可以概括为：首先数据库引擎会将 SQL 翻译成一系列的关系代数算子或表达式，然后依赖这些关系代数算子逐条处理输入数据并产生结果。每个算子在底层都实现同样的接口，比如都实现了 next() 方法，然后最顶层的算子 next() 调用子算子的 next()，子算子的 next() 在调用孙算子的 next()，直到最底层的 next()，具体过程如下图表示： Volcano Iterator Model 的优点是抽象起来很简单，很容易实现，而且可以通过任意组合算子来表达复杂的查询。但是缺点也很明显，存在大量的虚函数调用，会引起 CPU 的中断，最终影响了执行效率。databricks的官方博客对比过使用 Volcano Iterator Model 和手写代码的执行效率，结果发现手写的代码执行效率要高出十倍！ 所以总结起来就是将sql解析成为代码，比sql引擎直接解析sql语句效率要快，所以spark2.0最终选择使用代码生成的方式来执行sql语句 基于上面的发现，从 Apache Spark 2.0 开始，社区开始引入了 Whole-stage Code Generation，参见 SPARK-12795，主要就是想通过这个来模拟手写代码，从而提升 Spark SQL 的执行效率。Whole-stage Code Generation 来自于2011年 Thomas Neumann 发表的 Efficiently Compiling Efficient Query Plans for Modern Hardware论文，这个也是 Tungsten 计划的一部分。 Tungsten 代码生成分为三部分： 表达式代码生成（expression codegen） 全阶段代码生成（Whole-stage Code Generation） 加速序列化和反序列化（speed up serialization/deserialization） 表达式代码生成（expression codegen） 这个其实在 Spark 1.x 就有了。表达式代码生成的基类是 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator，其下有七个子类： 我们前文的 SQL 生成的逻辑计划中的 (isnotnull(sbirthday#3) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00) 就是最基本的表达式。它也是一种 Predicate，所以会调用 org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate 来生成表达式的代码。 全阶段代码生成（Whole-stage Code Generation） 全阶段代码生成（Whole-stage Code Generation），用来将多个处理逻辑整合到单个代码模块中，其中也会用到上面的表达式代码生成。和前面介绍的表达式代码生成不一样，这个是对整个 SQL 过程进行代码生成，前面的表达式代码生成仅对于表达式的。全阶段代码生成都是继承自 org.apache.spark.sql.execution.BufferedRowIterator 的，生成的代码需要实现 processNext() 方法，这个方法会在 org.apache.spark.sql.execution.WholeStageCodegenExec 里面的 doExecute 方法里面被调用。而这个方法里面的 rdd 会将数据传进生成的代码里面 ，比如我们上文 SQL 这个例子的数据源是 JDBC文件，底层使用 org.apache.spark.sql.execution.RowDataSourceScanExec这个类读取文件，然后生成 inputRDD，这个 rdd 在 WholeStageCodegenExec 类中的 doExecute 方法里面调用生成的代码，然后执行我们各种判断得到最后的结果。WholeStageCodegenExec 类中的 doExecute 方法部分代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/** * WholeStageCodegen compiles a subtree of plans that support codegen together into single Java * function. * * Here is the call graph of to generate Java source (plan A supports codegen, but plan B does not): * * WholeStageCodegen Plan A FakeInput Plan B * ========================================================================= * * -&gt; execute() * | * doExecute() ---------&gt; inputRDDs() -------&gt; inputRDDs() ------&gt; execute() * | * +-----------------&gt; produce() * | * doProduce() -------&gt; produce() * | * doProduce() * | * doConsume() &lt;--------- consume() * | * doConsume() &lt;-------- consume() * * SparkPlan A should override `doProduce()` and `doConsume()`. * * `doCodeGen()` will create a `CodeGenContext`, which will hold a list of variables for input, * used to generated code for [[BoundReference]]. */override def doExecute(): RDD[InternalRow] = &#123; val (ctx, cleanedSource) = doCodeGen() // try to compile and fallback if it failed val (_, maxCodeSize) = try &#123; CodeGenerator.compile(cleanedSource) &#125; catch &#123; case _: Exception if !Utils.isTesting &amp;&amp; sqlContext.conf.codegenFallback =&gt; // We should already saw the error message logWarning(s&quot;Whole-stage codegen disabled for plan (id=$codegenStageId):\\n $treeString&quot;) return child.execute() &#125; // Check if compiled code has a too large function if (maxCodeSize &gt; sqlContext.conf.hugeMethodLimit) &#123; logInfo(s&quot;Found too long generated codes and JIT optimization might not work: &quot; + s&quot;the bytecode size ($maxCodeSize) is above the limit &quot; + s&quot;$&#123;sqlContext.conf.hugeMethodLimit&#125;, and the whole-stage codegen was disabled &quot; + s&quot;for this plan (id=$codegenStageId). To avoid this, you can raise the limit &quot; + s&quot;`$&#123;SQLConf.WHOLESTAGE_HUGE_METHOD_LIMIT.key&#125;`:\\n$treeString&quot;) child match &#123; // The fallback solution of batch file source scan still uses WholeStageCodegenExec case f: FileSourceScanExec if f.supportsBatch =&gt; // do nothing case _ =&gt; return child.execute() &#125; &#125; val references = ctx.references.toArray val durationMs = longMetric(&quot;pipelineTime&quot;) val rdds = child.asInstanceOf[CodegenSupport].inputRDDs() assert(rdds.size &lt;= 2, &quot;Up to two input RDDs can be supported&quot;) if (rdds.length == 1) &#123; rdds.head.mapPartitionsWithIndex &#123; (index, iter) =&gt; val (clazz, _) = CodeGenerator.compile(cleanedSource) val buffer = clazz.generate(references).asInstanceOf[BufferedRowIterator] buffer.init(index, Array(iter)) new Iterator[InternalRow] &#123; override def hasNext: Boolean = &#123; val v = buffer.hasNext if (!v) durationMs += buffer.durationMs() v &#125; override def next: InternalRow = buffer.next() &#125; &#125; &#125; else &#123; // Right now, we support up to two input RDDs. rdds.head.zipPartitions(rdds(1)) &#123; (leftIter, rightIter) =&gt; Iterator((leftIter, rightIter)) // a small hack to obtain the correct partition index &#125;.mapPartitionsWithIndex &#123; (index, zippedIter) =&gt; val (leftIter, rightIter) = zippedIter.next() val (clazz, _) = CodeGenerator.compile(cleanedSource) val buffer = clazz.generate(references).asInstanceOf[BufferedRowIterator] buffer.init(index, Array(leftIter, rightIter)) new Iterator[InternalRow] &#123; override def hasNext: Boolean = &#123; val v = buffer.hasNext if (!v) durationMs += buffer.durationMs() v &#125; override def next: InternalRow = buffer.next() &#125; &#125; &#125; &#125; 在WholeStageCodegenExec 这个类的注释当中也说明了，最终生成的代码过程如下 12345678910111213141516171819202122232425262728/** * WholeStageCodegen compiles a subtree of plans that support codegen together into single Java * function. * * Here is the call graph of to generate Java source (plan A supports codegen, but plan B does not): * * WholeStageCodegen Plan A FakeInput Plan B * ========================================================================= * * -&gt; execute() * | * doExecute() ---------&gt; inputRDDs() -------&gt; inputRDDs() ------&gt; execute() * | * +-----------------&gt; produce() * | * doProduce() -------&gt; produce() * | * doProduce() * | * doConsume() &lt;--------- consume() * | * doConsume() &lt;-------- consume() * * SparkPlan A should override `doProduce()` and `doConsume()`. * * `doCodeGen()` will create a `CodeGenContext`, which will hold a list of variables for input, * used to generated code for [[BoundReference]]. */ 相比 Volcano Iterator Model，全阶段代码生成的执行过程如下： 通过引入全阶段代码生成，大大减少了虚函数的调用，减少了 CPU 的调用，使得 SQL 的执行速度有很大提升。 代码编译 生成代码之后需要解决的另一个问题是如何将生成的代码进行编译然后加载到同一个 JVM 中去。在早期 Spark 版本是使用 Scala 的 Reflection 和 Quasiquotes 机制来实现代码生成的。Quasiquotes 是一个简洁的符号，可以让我们轻松操作 Scala 语法树，具体参见 这里。虽然 Quasiquotes 可以很好的为我们解决代码生成等相关的问题，但是带来的新问题是编译代码时间比较长（大约 50ms - 500ms）！所以社区不得不默认关闭表达式代码生成。 为了解决这个问题，Spark 引入了 Janino 项目，参见 SPARK-7956。Janino 是一个超级小但又超级快的 Java™ 编译器. 它不仅能像 javac 工具那样将一组源文件编译成字节码文件，还可以对一些 Java 表达式，代码块，类中的文本(class body)或者内存中源文件进行编译，并把编译后的字节码直接加载到同一个 JVM 中运行。Janino 不是一个开发工具, 而是作为运行时的嵌入式编译器，比如作为表达式求值的翻译器或类似于 JSP 的服务端页面引擎，关于 Janino 的更多知识请参见这里。通过引入了 Janino 来编译生成的代码，结果显示 SQL 表达式的编译时间减少到 5ms。在 Spark 中使用了 ClassBodyEvaluator 来编译生成之后的代码，参见 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator。 需要主要的是，代码生成是在 Driver 端进行的，而代码编译是在 Executor 端进行的。 SQL执行 终于到了 SQL 真正执行的地方了。这个时候 Spark 会执行上阶段生成的代码，然后得到最终的结果，DAG 执行图如下： 3、sparkSQL执行过程深度总结 sparkSQL执行过程总结.pptx 从上面可以看得出来，sparkSQL的执行主要经过了这么几个大的步骤 1、输入sql，dataFrame或者dataSet 2、经过Catalyst过程，生成最终我们得到的最优的物理执行计划 ​ 1、parser阶段 ​ 主要是通过Antlr4解析SqlBase.g4 ，所有spark’支持的语法方式都是定义在sqlBase.g4里面了，如果需要扩展sparkSQL的语法，我们只需要扩展sqlBase.g4即可，通过antlr4解析sqlBase.g4文件，生成了我们的语法解析器SqlBaseLexer.java和词法解析器SqlBaseParser.java ​ parse阶段 ==》 antlr4 ==》解析 ==》 SqlBase.g4 ==》得到 ==》 语法解析器SqlBaseLexer.java + 词法解析器SqlBaseParser.java ​ 2、analyzer阶段 ​ 使用基于Rule的规则解析以及Session Catalog来实现函数资源信息和元数据管理信息 ​ Analyzer 阶段 ==》 使用 ==》 Rule + Session Catalog ==》多个rule ==》 组成一个batch ​ session CataLog ==》 保存函数资源信息以及元数据信息等 ​ 3、optimizer阶段 ​ optimizer调优阶段 ==》 基于规则的RBO优化rule-based optimizer ==&gt; 谓词下推 + 列剪枝 + 常量替换 + 常量累加 ​ 4、planner阶段 ​ 通过analyzer生成多个物理计划 ==》 经过Cost Model进行最优选择 ==》基于代价的CBO优化 ==》 最终选定得到的最优物理执行计划 ​ 5、选定最终的物理计划，准备执行 ​ 最终选定的最优物理执行计划 ==》 准备生成代码去开始执行 3、将最终得到的物理执行计划进行代码生成，提交代码去执行我们的最终任务 13、sparkSQL调优 1、数据缓存 性能调优主要是将数据放入内存中操作，spark缓存注册表的方法 缓存spark表： spark.catalog.cacheTable(“tableName”)缓存表 释放缓存表： spark.catalog.uncacheTable(“tableName”)解除缓存 2、性能优化相关参数 Sparksql仅仅会缓存必要的列，并且自动调整压缩算法来减少内存和GC压力。 属性 默认值 描述 spark.sql.inMemoryColumnarStorage.compressed true Spark SQL 将会基于统计信息自动地为每一列选择一种压缩编码方式。 spark.sql.inMemoryColumnarStorage.batchSize 10000 缓存批处理大小。缓存数据时, 较大的批处理大小可以提高内存利用率和压缩率，但同时也会带来 OOM（Out Of Memory）的风险。 spark.sql.files.maxPartitionBytes 128 MB 读取文件时单个分区可容纳的最大字节数（不过不推荐手动修改，可能在后续版本自动的自适应修改） spark.sql.files.openCostInBytes 4M 打开文件的估算成本, 按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度)。 3、表数据广播 在进行表join的时候，将小表广播可以提高性能，spark2.+中可以调整以下参数、 属性 默认值 描述 spark.sql.broadcastTimeout 300 广播等待超时时间，单位秒 spark.sql.autoBroadcastJoinThreshold 10M 用于配置一个表在执行 join 操作时能够广播给所有 worker 节点的最大字节大小。通过将这个值设置为 -1 可以禁用广播。注意，当前数据统计仅支持已经运行了 ANALYZE TABLE COMPUTE STATISTICS noscan 命令的 Hive Metastore 表。 4、分区数的控制 spark任务并行度的设置中，spark有两个参数可以设置 属性 默认值 描述 spark.sql.shuffle.partitions 200 用于配置 join 或aggregate shuffle数据时使用的分区数。 spark.default.parallelism 对于分布式shuffle操作像reduceByKey和join，父RDD中分区的最大数目。对于无父RDD的并行化等操作，它取决于群集管理器：-本地模式：本地计算机上的核心数-Mesos fine grained mode：8-其他：所有执行节点上的核心总数或2，以较大者为准 分布式shuffle操作的分区数 看起来它们的定义似乎也很相似，但在实际测试中， spark.default.parallelism只有在处理RDD时才会起作用，对Spark SQL的无效。 spark.sql.shuffle.partitions则是对sparks SQL专用的设置 5. 文件与分区 这个总共有两个参数可以调整： 读取文件的时候一个分区接受多少数据； 文件打开的开销，通俗理解就是小文件合并的阈值。 文件打开是有开销的，开销的衡量，Spark 采用了一个比较好的方式就是打开文件的开销用，相同时间能扫描的数据的字节数来衡量。 参数介绍如下： 属性 默认值 描述 spark.sql.files.maxPartitionBytes 134217728 (128 MB) 打包传入一个分区的最大字节，在读取文件的时候 spark.sql.files.openCostInBytes 4194304 (4 MB) 用相同时间内可以扫描的数据的大小来衡量打开一个文件的开销。当将多个文件写入同一个分区的时候该参数有用。该值设置大一点有好处，有小文件的分区会比大文件分区处理速度更快（优先调度） spark.sql.files.maxPartitionBytes该值的调整要结合你想要的并发度及内存的大小来进行。 spark.sql.files.openCostInBytes说直白一些这个参数就是合并小文件的阈值，小于这个阈值的文件将会合并 6、数据的本地性 分布式计算系统的精粹在于移动计算而非移动数据，但是在实际的计算过程中，总存在着移动数据的情况，除非是在集群的所有节点上都保存数据的副本。移动数据，将数据从一个节点移动到另一个节点进行计算，不但消耗了网络IO，也消耗了磁盘IO，降低了整个计算的效率。为了提高数据的本地性，除了优化算法（也就是修改spark内存，难度有点高），就是合理设置数据的副本。设置数据的副本，这需要通过配置参数并长期观察运行状态才能获取的一个经验值。 先来看看一个 stage 里所有 task 运行的一些性能指标，其中的一些说明： Scheduler Delay : spark 分配 task 所花费的时间 Executor Computing Time : executor 执行 task 所花费的时间 Getting Result Time : 获取 task 执行结果所花费的时间 Result Serialization Time : task 执行结果序列化时间 Task Deserialization Time : task 反序列化时间 Shuffle Write Time : shuffle 写数据时间 Shuffle Read Time : shuffle 读数据所花费时间 ​ 下面是spark webUI监控Stage的一个图： PROCESS_LOCAL是指读取缓存在本地节点的数据 NODE_LOCAL是指读取本地节点硬盘数据 ANY是指读取非本地节点数据 通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关。 7、sparkSQL参数调优总结 12345678910111213141516171819202122//1.下列Hive参数对Spark同样起作用。set hive.exec.dynamic.partition=true; // 是否允许动态生成分区set hive.exec.dynamic.partition.mode=nonstrict; // 是否容忍指定分区全部动态生成set hive.exec.max.dynamic.partitions = 100; // 动态生成的最多分区数//2.运行行为set spark.sql.autoBroadcastJoinThreshold; // 大表 JOIN 小表，小表做广播的阈值set spark.dynamicAllocation.enabled; // 开启动态资源分配set spark.dynamicAllocation.maxExecutors; //开启动态资源分配后，最多可分配的Executor数set spark.dynamicAllocation.minExecutors; //开启动态资源分配后，最少可分配的Executor数set spark.sql.shuffle.partitions; // 需要shuffle是mapper端写出的partition个数set spark.sql.adaptive.enabled; // 是否开启调整partition功能，如果开启，spark.sql.shuffle.partitions设置的partition可能会被合并到一个reducer里运行set spark.sql.adaptive.shuffle.targetPostShuffleInputSize; //开启spark.sql.adaptive.enabled后，两个partition的和低于该阈值会合并到一个reducerset spark.sql.adaptive.minNumPostShufflePartitions; // 开启spark.sql.adaptive.enabled后，最小的分区数set spark.hadoop.mapreduce.input.fileinputformat.split.maxsize; //当几个stripe的大小大于该值时，会合并到一个task中处理//3.executor能力set spark.executor.memory; // executor用于缓存数据、代码执行的堆内存以及JVM运行时需要的内存set spark.yarn.executor.memoryOverhead; //Spark运行还需要一些堆外内存，直接向系统申请，如数据传输时的netty等。set spark.sql.windowExec.buffer.spill.threshold; //当用户的SQL中包含窗口函数时，并不会把一个窗口中的所有数据全部读进内存，而是维护一个缓存池，当池中的数据条数大于该参数表示的阈值时，spark将数据写到磁盘set spark.executor.cores; //单个executor上可以同时运行的task数 14、spark的动态资源划分 动态资源划分，主要是spark当中用于对计算的时候资源如果不够或者资源剩余的情况下进行动态的资源划分，以求资源的利用率达到最大 http://spark.apache.org/docs/2.3.3/configuration.html#dynamic-allocation Spark中，所谓资源单位一般指的是executors，和Yarn中的Containers一样，在Spark On Yarn模式下，通常使用–num-executors来指定Application使用的executors数量，而–executor-memory和–executor-cores分别用来指定每个executor所使用的内存和虚拟CPU核数 假设有这样的场景，如果使用Hive，多个用户同时使用hive-cli做数据开发和分析，只有当用户提交执行了Hive SQL时候，才会向YARN申请资源，执行任务，如果不提交执行，无非就是停留在Hive-cli命令行，也就是个JVM而已，并不会浪费YARN的资源。现在想用Spark-SQL代替Hive来做数据开发和分析，也是多用户同时使用，如果按照之前的方式，以yarn-client模式运行spark-sql命令行，在启动时候指定–num-executors 10，那么每个用户启动时候都使用了10个YARN的资源（Container），这10个资源就会一直被占用着，只有当用户退出spark-sql命令行时才会释放。例如通过以下这种方式使用spark-sql 1234567891011121314151617直接通过-e来执行任务，执行完成任务之后，回收资源cd /kkb/install/spark-2.3.3-bin-hadoop2.7bin/spark-sql --master yarn-client \\--executor-memory 512m –num-executors 10 \\--conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \\--jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar \\-e &quot;select count(*) from game_center.ods_task_log;&quot;进入spark-sql客户端，但是不执行任务，一直持续占有资源cd /kkb/install/spark-2.3.3-bin-hadoop2.7bin/spark-sql --master yarn-client \\--executor-memory 512m –num-executors 10 \\--conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \\--jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar 在这种模式下，就算你不提交资源，申请的资源也会一直常驻，这样就明显不合理了 spark-sql On Yarn，能不能像Hive一样，执行SQL的时候才去申请资源，不执行的时候就释放掉资源呢，其实从Spark1.2之后，对于On Yarn模式，已经支持动态资源分配（Dynamic Resource Allocation），这样，就可以根据Application的负载（Task情况），动态的增加和减少executors，这种策略非常适合在YARN上使用spark-sql做数据开发和分析，以及将spark-sql作为长服务来使用的场景。 spark当中支持通过动态资源划分的方式来实现动态资源的配置，尽量减少内存的持久占用，但是动态资源划分又会产生进一步的问题例如： 12345executor动态调整的范围？无限减少？无限制增加？executor动态调整速率？线性增减？指数增减？何时移除Executor？何时新增Executor了？只要由新提交的Task就新增Executor吗？Spark中的executor不仅仅提供计算能力，还可能存储持久化数据，这些数据在宿主executor被kill后，该如何访问？ 通过spark-shell当中最简单的wordcount为例来查看spark当中的资源划分 12345678910# 以yarn模式执行，并指定executor个数为1$ spark-shell --master=yarn --num-executors=1# 提交Job1 wordcountscala&gt; sc.textFile(&quot;file:///etc/hosts&quot;).flatMap(line =&gt; line.split(&quot; &quot;)).map(word =&gt; (word,1)).reduceByKey(_ + _).count();# 提交Job2 wordcountscala&gt; sc.textFile(&quot;file:///etc/profile&quot;).flatMap(line =&gt; line.split(&quot; &quot;)).map(word =&gt; (word,1)).reduceByKey(_ + _).count();# Ctrl+C Kill JVM 上述的Spark应用中，以yarn模式启动spark-shell，并顺序执行两次wordcount，最后Ctrl+C退出spark-shell。此例中Executor的生命周期如下图： 从上图可以看出，Executor在整个应用执行过程中，其状态一直处于Busy（执行Task）或Idle（空等）。处于Idle状态的Executor造成资源浪费这个问题已经在上面提到。下面重点看下开启Spark动态资源分配功能后，Executor如何运作。 下面分析下上图中各个步骤： spark-shell Start：启动spark-shell应用，并通过–num-executor指定了1个执行器。 Executor1 Start：启动执行器Executor1。注意：Executor启动前存在一个AM向ResourceManager申请资源的过程，所以启动时机略微滞后与Driver。 Job1 Start：提交第一个wordcount作业，此时，Executor1处于Busy状态。 Job1 End：作业1结束，Executor1又处于Idle状态。 Executor1 timeout：Executor1空闲一段时间后，超时被Kill。 Job2 Submit：提交第二个wordcount，此时，没有Active的Executor可用。Job2处于Pending状态。 Executor2 Start：检测到有Pending的任务，此时Spark会启动Executor2。 Job2 Start：此时，已经有Active的执行器，Job2会被分配到Executor2上执行。 Job2 End：Job2结束。 Executor2 End：Ctrl+C 杀死Driver，Executor2也会被RM杀死。 上述流程中需要重点关注的几个问题： Executor超时：当Executor不执行任何任务时，会被标记为Idle状态。空闲一段时间后即被认为超时，会被kill。该空闲时间由spark.dynamicAllocation.executorIdleTimeout决定，默认值60s。对应上图中：Job1 End到Executor1 timeout之间的时间。 资源不足时，何时新增Executor：当有Task处于pending状态，意味着资源不足，此时需要增加Executor。这段时间由spark.dynamicAllocation.schedulerBacklogTimeout控制，默认1s。对应上述step6和step7之间的时间。 该新增多少Executor：新增Executor的个数主要依据是当前负载情况，即running和pending任务数以及当前Executor个数决定。用maxNumExecutorsNeeded代表当前实际需要的最大Executor个数，maxNumExecutorsNeeded和当前Executor个数的差值即是潜在的新增Executor的个数。注意：之所以说潜在的个数，是因为最终新增的Executor个数还有别的因素需要考虑，后面会有分析。下面是maxNumExecutorsNeeded计算方法： 123456private def maxNumExecutorsNeeded(): Int = &#123; val numRunningOrPendingTasks = listener.totalPendingTasks + listener.totalRunningTasks math.ceil(numRunningOrPendingTasks * executorAllocationRatio / tasksPerExecutorForFullParallelism) .toInt&#125; 其中numRunningOrPendingTasks为当前running和pending任务数之和。 executorAllocationRatio：最理想的情况下，有多少待执行的任务，那么我们就新增多少个Executor，从而达到最大的任务并发度。但是这也有副作用，如果当前任务都是小任务，那么这一策略就会造成资源浪费。可能最后申请的Executor还没启动，这些小任务已经被执行完了。该值是一个系数值，范围[0~1]。默认1. tasksPerExecutorForFullParallelism：每个Executor的最大并发数，简单理解为：cpu核心数（spark.executor.cores）/ 每个任务占用的核心数（spark.task.cpus）。 问题1：executor动态调整的范围？无限减少？无限制增加？调整速率？ 要实现资源的动态调整，那么限定调整范围是最先考虑的事情，Spark通过下面几个参数实现： spark.dynamicAllocation.minExecutors：Executor调整下限。（默认值：0） spark.dynamicAllocation.maxExecutors：Executor调整上限。（默认值：Integer.MAX_VALUE） spark.dynamicAllocation.initialExecutors：Executor初始数量（默认值：minExecutors）。 三者的关系必须满足：minExecutors &lt;= initialExecutors &lt;= maxExecutors 注意：如果显示指定了num-executors参数，那么initialExecutors就是num-executor指定的值。 问题2：Spark中的Executor既提供计算能力，也提供存储能力。这些因超时被杀死的Executor中持久化的数据如何处理？ 如果Executor中缓存了数据，那么该Executor的Idle-timeout时间就不是由executorIdleTimeout决定，而是用spark.dynamicAllocation.cachedExecutorIdleTimeout控制，默认值：Integer.MAX_VALUE。如果手动设置了该值，当这些缓存数据的Executor被kill后，我们可以通过NodeManannger的External Shuffle Server来访问这些数据。这就要求NodeManager中spark.shuffle.service.enabled必须开启。 如何配置spark的动态资源划分 第一步：修改yarn-site.xml配置文件 12345678910111213&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt;&lt;/property&gt; 第二步：配置spark的配置文件 修改spark-conf的配置选项，开启动态资源划分，或者直接修改spark-defaults.conf，增加以下参数： 1234567spark.shuffle.service.enabled true //启用External shuffle Service服务spark.shuffle.service.port 7337 //Shuffle Service服务端口，必须和yarn-site中的一致spark.dynamicAllocation.enabled true //开启动态资源分配spark.dynamicAllocation.minExecutors 1 //每个Application最小分配的executor数spark.dynamicAllocation.maxExecutors 30 //每个Application最大并发分配的executor数spark.dynamicAllocation.schedulerBacklogTimeout 1s spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s 动态资源分配策略： 开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout(默认1s)时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout(默认1s)时间申请一次，直到申请到足够的资源。每次申请的资源量是指数增长的，即1,2,4,8等。 之所以采用指数增长，出于两方面考虑：其一，开始申请的少是考虑到可能application会马上得到满足；其次要成倍增加，是为了防止application需要很多资源，而该方式可以在很少次数的申请之后得到满足。 动态资源回收策略： 当application的executor空闲时间超过spark.dynamicAllocation.executorIdleTimeout（默认60s）后，就会被回收。","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"sparkSQL","slug":"sparkSQL","permalink":"http://www.iequa.com/tags/sparkSQL/"}]},{"title":"DataWare Review Summary 1","slug":"dataware/dwh-summary-1-warehouse-theory","date":"2020-10-01T01:07:21.000Z","updated":"2021-06-22T06:04:44.233Z","comments":true,"path":"2020/10/01/dataware/dwh-summary-1-warehouse-theory/","link":"","permalink":"http://www.iequa.com/2020/10/01/dataware/dwh-summary-1-warehouse-theory/","excerpt":"","text":"漫谈系列： No. Question Flag 1. good - 漫谈大牛带你从0到1构建数据仓库实战 2. good - 数据模型设计（推荐收藏） 3. 数据仓库（二）, 数据仓库（一） 4. Hadoop的元数据治理–Apache Atlas 5. 1. 漫画：什么是数据仓库？ 6. 2. 传统数仓和大数据数仓的区别是什么？ ✔️ 7. 4. 数仓那点事：从入门到佛系 8. 5. 从8个角度5分钟搞定数据仓库 9. 6. 滴滴数据仓库指标体系建设实践 11. 9. 手把手教你如何搭建一个数据仓库 12. 10. 基于spark快速构建数仓项目（文末Q&amp;A） 13. 11. 数据湖VS数据仓库之争？阿里提出大数据架构新概念：湖仓一体！ 14. 12. 数据仓库（离线+实时）大厂优秀案例汇总（建议收藏） 15. Good - Hive 拉链表实践 仙子紫霞 数据仓库与Python大数据 1/1 叮！致2020的一封情书，请查收！文末2019年文章精选 1. 漫谈系列 | 数仓第一篇NO.1 『基础架构』 2. 漫谈系列 | 数仓第二篇NO.2 『数据模型』 3. 漫谈系列 | 数仓第三篇NO.3 『数据ETL』 4. 漫谈系列 | 数仓第四篇NO.4 『数据应用』 5. 漫谈系列 | 数仓第五篇NO.5 『调度系统』 6. 漫谈系列 | 数仓第六篇NO.6 『数据治理』 7. 漫谈系列 | 漫谈数仓第一篇NO.7 『面试真经』 元数据管理解析以及数据仓库和主数据介绍 1. DWH, Concept OLTP (on-line transaction processing) OLAP（On-Line Analytical Processing） 数据在系统中产生 本身不产生数据，基础数据来源于产生系统 基于交易的处理系统 基于查询的分析系统 牵扯的数据量很小 牵扯的数据量庞大 (复杂查询经常使用全表扫描等) 对响应时间要求非常高 响应时间与具体查询有很大关系 用户数量大，为操作用户 用户数量少，主要有技术人员与业务人员 各种操作主要基于索引进行 业务问题不固定，数据库的各种操作不能完全基于索引进行 DW 4 大特征: Subject Oriented、Integrate、Non-Volatil、Time Variant . 数仓分层 STG Stage （不做任何加工, 禁止重复进入） ODS（Operational Data Store）不做处理，存放原始数据 (该层在stage上仅数据格式到标准格式转换) DWD（Data Warehouse Summary 明细数据层）进行简单数据清洗，降维 DWS（Data Warehouse Summary 服务数据层）进行轻度汇总（做宽表） ADS（Application Data Summary 数据应用层）为报表提供数据 1.1 DWH basic data warehouse 逻辑分层架构： 1.2 data modeling Title_Kimball 深入浅出数据模型（推荐收藏） 流程 架构是自下向上，即从数据集市(主题划分)–&gt;数据仓库–&gt; 数据抽取，是以需求为导向的，一般使用星型模型 事实表和维表 架构强调模型由事实表和维表组成，注重事实表与维表的设计 数据集市 数据仓库架构中，数据集市是一个逻辑概念，只是多维数据仓库中的主题域划分，并没有自己的物理存储，也可以说是虚拟的数据集市。是数据仓库的一个访问层，是按主题域组织的数据集合，用于支持部门级的决策。 data modeling 的几种方式: No. 数据建模方式 type 1. ER模型 三范式 2. 维度建模 1. 星型模型 2. 雪花模型 3. 星座模型 1. 事实表 事实表生于业务过程，存储业务活动或事件提炼出来的性能度量。从最低的粒度级别来看，事实表行对应一个度量事件 （1）事务事实表 （2）周期快照事实表 （3）累积快照事实表 2. 维度表 （1）退化维度（DegenerateDimension） （2）缓慢变化维（Slowly Changing Dimensions） 维度的属性并不是始终不变的，这种随时间发生变化的维度我们一般称之为缓慢变化维（SCD） 1.3 data ETL SQL分析函数，看这一篇就够了 No. Title desc 1. 数仓建模工具哪一个好? powerDesigner 勉强推一个吧 2. DWS 轻度聚合及（汇总 == group by） 是按照 Topic 划分的 3. DWD join 成宽表 by ODS 事实表基本都在 DWD 层. 4. App 层也是在 Hive 中么? 尽量不要 5. 数据仓库的数据质量如何保障? 需要从源头管控，业务系统进行细致的字段的校验 6. 如何保证你的计算的指标结果准确性？ 1. 有测试人员 2. 我们公司有做小样本数据集的抽取开发 7. 数据存储格式 orc / Parquet 8. 数据压缩方式 snappy / LZO 9. 数据存储格式 + 压缩 服务器的磁盘空间可以变为原来的 1/3 10. beeline 客户端支持远程连接 11. lzo 支持切分么？ snappy 不支持切分, 给lzo文件建立索引后，则支持切分 create_time, update_time, 使用拉链表解决历史数据变更的问题 12345678910111213# 设置输出数据格式压缩成为LZOSET hive.exec.compress.output=true;SET mapreduce.output.fileoutputformat.compress=true;set mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec;#插入数据到目标表里面去insert overwrite table ods_user_login partition(part_date)select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,op_type,online_time,operating_system,operating_version,device_brand,device_type,from_unixtime(event_time,&#x27;yyyy-MM-dd&#x27;) as part_date from tmp_ods_user_login;#给lzo文件建立索引：便于以后多个mapTask来对文件进行处理hadoop jar /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /user/hive/warehouse/game_center.db/ods_user_login/ 1.4 Tool-App No. Tool 1. Apache_Druid Druid是一个用于大数据实时查询和分析的高容错、高性能开源分布式系统，用于解决如何在大规模数据集下进行快速的、交互式的查询和分析。 2. Apache Kylin™ 一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 3. Clickhouse是一个用于在线分析处理（OLAP）的列式数据库管理系统（DBMS） 4. ADB（AnalyticDB_for_MySQL） 分析型数据库MySQL版（AnalyticDB for MySQL），是阿里巴巴自主研发的海量数据实时高并发在线分析（Realtime OLAP）云计算服务，使得您可以在毫秒级针对千亿级数据进行即时的多维分析透视和业务探索。 5. 花未全开*月未圆 1.1 presto 是Facebook开源的，完全基于内存的并⾏计算(MPP)，分布式SQL交互式查询引擎1.2 数据治理: 在ETL过程中开发人员会对数据清洗这其实就是治理的一部分1.3 元数据是记录数仓中模型的定义、各层级的映射关系、监控数仓的数据状态及 ETL 的任务运行状态 1.4 DW-DM层是采用Kimball的总线式的数据仓库架构，针对部门（比如财务部门）或者某一主题（比如商户、用户），通过维度建模（推荐星型模型），构建一致性维度，原子粒度的数据是DW层，按照实体或者主题经过一定的汇总，建设数据集市模型。数据集市可以为OLAP提供服务。 Ad-hoc 查询或报告（即席查询或报告）是 商业智能的一个次要的话题，它还经常与OLAP、数据仓库相并论. 2. SQL 廖雪峰:关系数据库概述 (1). 主键 / FOREIGN KEY 1234567身份证号、手机 这些看上去可以唯一的字段，均不可用作主键。作为主键最好是完全业务无关的字段，我们一般把这个字段命名为id。自增整数类型没有必要的情况下，我们尽量不使用联合主键，因为它给关系表带来了复杂度的上升。由于外键约束会降低数据库的性能，大部分互联网应用程序为了追求速度，并不设置外键约束，而是仅靠应用程序自身来保证逻辑的正确性。 (2). INDEX 12345可以对一张表创建多个索引。索引的优点是提高了查询效率缺点是在插入、更新和删除记录时，需要同时修改索引，因此，索引越多，插入、更新和删除记录的速度就越慢。对于主键，关系数据库会自动对其创建主键索引。使用主键索引的效率是最高的，因为主键会保证绝对唯一。 (3). SQL查询 1234567891011121314151617181920212223242526272829303132333.1 基础查询SELECT * FROM students WHERE score &gt;= 80;SELECT id, name, gender, score FROM students ORDER BY score;SELECT id, name, gender, score FROM students ORDER BY score DESC LIMIT 3 OFFSET 6; 第3页SELECT COUNT(*) boys FROM students WHERE gender = &#x27;M&#x27;;3.2 分组查询:SELECT COUNT(*) num FROM students GROUP BY class_id;SELECT class_id, gender, COUNT(*) num FROM students GROUP BY class_id, gender;3.3 多表查询:SELECT s.id sid, s.name, s.gender, s.score, c.id cid, c.name cnameFROM students s, classes cWHERE s.gender = &#x27;M&#x27; AND c.id = 1;使用多表查询可以获取M x N行记录；多表查询的结果集可能非常巨大，要小心使用。3.4 连接查询:JOIN查询需要先确定主表，然后把另一个表的数据“附加”到结果集上；INNER JOIN是最常用的一种JOIN查询，它的语法是SELECT ... FROM &lt;表1&gt; INNER JOIN &lt;表2&gt; ON &lt;条件...&gt;；JOIN查询仍然可以使用WHERE条件和ORDER BY排序。 3. Hive 1. 一篇文章让你了解Hive调优（文末赠书） 2. 再次分享！Hive调优，数据工程师成神之路 1.1 3NF vs Dim modeling 1234567893NF: 每个属性值唯一，不具有多义性； 每个非主属性必须完全依赖于整个主键，而非主键的一部分； 每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中。 维度模型: 是一个规范化的 事实表 和 反规范化的一些 维度表 组成的 (1) 一种非规范化的关系模型 (2) 表跟表之间的关系通过 keyword 和 foreign-key 来定义 Hive Input -&gt; Mappers -&gt; Sort,Shuffle -&gt; Reducers -&gt; Output Hive 系统架构 Hive 系统架构 1. Hive 原理 123456781. 用户提交查询等任务给Driver。2. 编译器获得该用户的任务Plan。3. 编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息。4. 编译器Compiler得到元数据信息，对任务进行编译，先将HiveQL转换为抽象语法树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理的计划（MapReduce）, 最后选择最佳的策略。5. 将最终的计划提交给Driver。6. Driver将计划Plan转交给ExecutionEngine去执行，获取元数据信息，提交给JobTracker或者SourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作。7. 获取执行的结果。8. 取得并返回执行结果。 2. hadoop处理数据的过程，有几个显著的特征 12341.不怕数据多，就怕数据倾斜。2．对jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次汇总，产生十几个jobs，没半小时是跑不完的。map reduce作业初始化的时间是比较长的。3.对sum，count来说，不存在数据倾斜问题。4.对count(distinct ),效率较低，数据量一多，准出问题，如果是多count(distinct )效率更低 4. Sqoop 问题 12345678910111213141516171819function import_data_hdfs() &#123; sqoop import \\ -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect $&#123;jdbc_url&#125; --username $&#123;jdbc_username&#125; --password $&#123;jdbc_passwd&#125; \\ --query &quot;$&#123;exec_sql&#125;&quot; \\ --split-by $&#123;id&#125; -m 20 \\ --target-dir $&#123;target_dir&#125; \\ --fields-terminated-by &quot;\\001&quot; --lines-terminated-by &quot;\\n&quot; \\ --hive-drop-import-delims \\ --null-string &#x27;\\\\N&#x27; --null-non-string &#x27;\\\\N&#x27; check_success echo_ex &quot;end successful import $&#123;target_dir&#125;. field.delim : \\001&quot;&#125;(1) 导入导出Null存储一致性问题 导出数据时采用–input-null-string和–input-null-non-string 导入数据时采用–null-string和–null-non-string(2). jdbc_url jdbc_url=&quot;jdbc:mysql://xxxx:3306/reportpublic?autoReconnect=true&quot;(3). Map 阶段, 只有 原理是重写了 MR： inputformat 和 outputformat 数据湖 vs 数据仓库 vs 数据中台 No. Title desc 0. https://delta.io/ 1. 数据湖如何为企业带来9%的高增长？可否取代数据仓库？ ✔️ 2. 数据湖(Data Lake)-剑指下一代数据仓库 ✔️ 3. IOTA架构、数据湖、Metric Platform，终于有人讲清楚了！ 4. Delta Lake 数据湖的诞生与案例实践 至此，我们也可以对比一下数据湖、数据仓库、数据中台，简明扼要概括为： 1）数据湖： 无为而治，目标AI 2）数据仓库：分而治之，目标BI 3）数据中台：一统天下，目标组织架构 Data Lake是一个存储库，可以存储大量结构化，半结构化和非结构化数据。它是以原生格式存储每种类型数据的地方，对帐户大小或文件没有固定限制。它提供高数据量以提高分析性能和本机集成。 Data Lake就像一个大型容器，与真正的湖泊和河流非常相似。就像在湖中你有多个支流进来一样，数据湖有结构化数据，非结构化数据，机器到机器，实时流动的日志。 数据湖相对于以往的关系型数据库、传统式数据仓库，更多体现的是一种数据存储技术上的融合。数据湖的提出，改变了用户使用数据的方式，同时，数据湖也整合了各种类型数据的分析和存储，用户不必为不同的数据构建不同数据存储库。 但是，现阶段数据湖更多是作为数据仓库的补充，它的用户一般只限于专业数据科学家或分析师。数据湖概念和技术还在不断演化，不同的解决方案供应商也在添加新的特性和功能，包括架构标准化和互操作性、数据治理要求、数据安全性等。 未来，数据湖可能会进一步发展，作为一种云服务随时按需满足对不同数据的分析、处理和存储需求，数据湖的扩展性，可以为用户提供更多的实时分析，基于企业大数据的数据湖正在向支持更多类型的实时智能化服务发展， 将会为企业现有的数据驱动型决策制定模式带来极大改变。 即席查询（Ad Hoc）用户据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表 dw No. 主题名称 主题描述 1. 客户 (USER) 当事人, 用户信息, 非常多, 人行征信信息， 个人资产信息 2. 机构 (ORG) 线下有哪些团队, 浙江区，团队长，客户经理， 有 600+ 个. 只有维度表 3. 产品 (PRD) 签协议 产生 产品, 业务流程, 只有维度表 产品维度表： 产品编号(分好几级), 产品名称, dim_code, dim_name， 上架， 下架京东金条， code， 展示给财务 4. 渠道 (CHL) 5. 事件 (EVT) 1. 业借 / 注册&amp;认证 2. 授信 3. 支用 4. 放款 5. 支付 6. 还款 6. 协议 (AGR) 合约 7. 营销 (CAMP) 营销之后的，商务经理和渠道，谈下来之后， 后端 渠道， 资产， 账务 8. 财务 (RISK) 9. 风险 (FINANCE) 风险部 Reference HDFS基本架构、原理、与应用场景、实践（附ppt） Hive存储格式对比 very good - igDataGuide/面试-all Apache Druid 简介 操作系统之堆和栈的区别 漫谈数据仓库之拉链表（原理、设计以及在Hive中的实现） 2020大数据/数仓/数开面试题真题总结(附答案)","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Spark SQL几种Join实现","slug":"spark/spark-sql-join-core","date":"2020-09-26T12:07:21.000Z","updated":"2021-06-22T06:12:17.986Z","comments":true,"path":"2020/09/26/spark/spark-sql-join-core/","link":"","permalink":"http://www.iequa.com/2020/09/26/spark/spark-sql-join-core/","excerpt":"Spark RDD Feature","text":"Spark RDD Feature 有两种方式使用SparkSQL Spark SQL性能调优以及原理图 是直接写sql语句，这个需要有元数据库支持，例如Hive等 通过Dataset/DataFrame编写Spark应用程序。 1. SparkSQL总体流程 Spark 2. Join基本要素 Spark Join基本实现流程 Spark将参与Join的两张表抽象为流式遍历表(streamIter)和查找表(buildIter)，通常streamIter为大表，buildIter为小表，我们不用担心哪个表为streamIter，哪个表为buildIter，这个spark会根据join语句自动帮我们完成. 2.1 sort merge join实现 要让两条记录能join到一起，首先需要将具有相同key的记录在同一个分区，所以通常来说，需要做一次shuffle，map阶段根据join条件确定每条记录的key，基于该key做shuffle write，将可能join到一起的记录分到同一个分区中，这样在shuffle read阶段就可以将两个表中具有相同key的记录拉到同一个分区处理. Spark 2.2 broadcast join实现 当buildIter的估计大小不超过参数spark.sql.autoBroadcastJoinThreshold设定的值(默认10M)，那么就会自动采用broadcast join，否则采用sort merge join. 2.3 hash join实现 2.4 inner join 2.5 left outer join 2.6 right outer join 2.7 full outer join full outer join仅采用sort merge join实现，左边和右表既要作为streamIter，又要作为buildIter，其基本实现流程如下图所示: Spark 2.8 left semi join Spark 2.9 left anti join 3. Spark 执行图 Join是数据库查询中一个非常重要的语法特性，在数据库领域可以说是“得join者得天下”，SparkSQL作为一种分布式数据仓库系统，给我们提供了全面的join支持，并在内部实现上无声无息地做了很多优化，了解join的实现将有助于我们更深刻的了解我们的应用程序的运行轨迹. Reference Spark shuffle内存管理与调优 每个 Spark 工程师都应该知道的五种 Join 策略 面试必知的Spark SQL几种Join实现 Spark SQL中Join常用的几种实现","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Tutorials 3 - RDD","slug":"spark/Spark-Tutorials-Part3","date":"2020-09-25T08:07:21.000Z","updated":"2021-06-22T05:41:29.731Z","comments":true,"path":"2020/09/25/spark/Spark-Tutorials-Part3/","link":"","permalink":"http://www.iequa.com/2020/09/25/spark/Spark-Tutorials-Part3/","excerpt":"","text":"very good Spark原理篇之RDD特征分析讲解 spark 基础知识整理（一） spark 基础知识整理（二）- RDD专题 Spark 分区(Partition)的认识、理解和应用 1. Create RDDs 2. RDD Persistence and Caching What is RDD persistence, Why do we need to call cache or persist on an RDD, What is the Difference between Cache() and Persist() method in Spark What is RDD Persistence and Caching The difference between cache() and persist() is that using cache() the default storage level is MEMORY_ONLY while using persist() we can use various storage levels (described below). Time efficient Cost efficient Lessen the execution time. Storage levels of Persisted RDDs MEMORY_ONLY MEMORY_AND_DISK MEMORY_ONLY_SER MEMORY_AND_DISK_SER DISK_ONLY How to Unpersist RDD in Spark? using RDD.unpersist() method. 3. RDD Features 3.1 In-memory computation 3.2 Lazy Evaluation 3.3 Fault Tolerance Fault tolerance in Apache Spark – Reliable Spark Streaming 3.4 Immutability 3.5 Persistence 3.6 Partitioning 3.7 Parallel Rdd, process the data parallelly over the cluster. 3.8 Location-Stickiness 3.9 Coarse-grained Operation 3.10 Typed We can have RDD of various types like: RDD [int], RDD [long], RDD [string]. 3.11 No limitation RDD Features good Spark基本工作原理和RDD特性 Spark RDD 特征及其依赖 4. Paired RDD Here transformation operations are: groupByKey reduceByKey join left outer join right outer Join Whereas actions like countByKey 4.1 Objective in spark is designed as each dataset in RDD is divided into logical partitions. Further, we can say here each partition may be computed on different nodes of the cluster. 4.2 Spark Paired RDD 1234567891011from pyspark import SparkConf , SparkContextfrom operator import addsc.versionlines22 = sc.textFile(&quot;/Users/blair/ghome/github/spark3.0/pyspark/spark-src/word_count.text&quot;, 2)pairs22= lines22.map(lambda x: (x, 1))#pairs22.take(2)counts22 = pairs22.reduceByKey(add)counts22 4.3 Create Spark Paired RDD a. In Python language 1pairs = lines.map(lambda x: (x.split(” “)[0], x)) 4.4 Paired RDD Operations No. Operations desc . Transformation Operations . map / flatMap / mapPartitions … 1. groupByKey The groupbykey operation generally groups all the values with the same key. rdd.groupByKey() 2. reduceByKey(fun) Here, the reduceByKey operation generally combines values with the same key. add.reduceByKey( (x, y) =&gt; x + y) 3. combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner) CombineByKey uses a different result type, then combine those values with the same key. 4. mapValues(func) Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD’s partitioning. 5. keys() Keys() Return an RDD with the keys of each tuple. 6. values() Return an RDD with the values of each tuple. 7. sortByKey(ascending=True, numPartitions=None, keyfunc=&lt;function RDD.&gt;) Similarly, the sortByKey operation generally returns an RDD sorted by the key. . Action Operations 8. countByKey() countByKey operation, we can count the number of elements for each key. 9. collectAsMap() Here, collectAsMap() operation helps to collect the result as a map to provide easy lookup. 10. lookup(key) Moreover, it returns all values associated with the provided key. PySpark 3.0.1 documentation » (1). reduceByKey(fun) &amp; groupByKey 12345678910111213141516lines = sc.textFile(&quot;/Users/blair/ghome/github/spark3.0/pyspark/spark-src/word_count.text&quot;, 2)lines.take(3)words = lines.flatMap(lambda x: x.split(&#x27; &#x27;))print(words.take(5))wco = words.map(lambda x: (x, 1))print(wco.take(5))# word_count = wco.reduceByKey(add)# print(&quot;\\nword_count:&quot;)# print(word_count.take(5))print(&quot;\\ngroupByKey:&quot;)test = wco.groupByKey()print(test.take(2))# gp = test.map(lambda x: (x[0], [i for i in x[1]]))# gp.take(2) (2). mapValues(fun) 123456rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])sorted(rdd.groupByKey().mapValues(len).collect())# [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]sorted(rdd.groupByKey().mapValues(list).collect())# [(&#x27;a&#x27;, [1, 1]), (&#x27;b&#x27;, [1])] (3). keys(), values() 1234567m = sc.parallelize([(1, 2), (3, 4)]).keys()m.collect()[1, 3]m = sc.parallelize([(1, 2), (3, 4)]).values()m.collect()[2, 4] (4). sortBykey spark combinebykey？ pyspark中combineByKey的两种理解方法 5. RDD limitations Reference Spark原理篇之RDD特征分析讲解 PySpark 3.0.1 documentation » data-flair.training/blogs Spark RDD Operations-Transformation &amp; Action with Example Spark RDD常用算子学习笔记详解(python版) Spark常用的Transformation算子的简单例子","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Tutorials 2 - SparkContext、Stage、Executor、RDD","slug":"spark/Spark-Tutorials-Part2","date":"2020-09-19T08:07:21.000Z","updated":"2021-06-20T04:12:28.347Z","comments":true,"path":"2020/09/19/spark/Spark-Tutorials-Part2/","link":"","permalink":"http://www.iequa.com/2020/09/19/spark/Spark-Tutorials-Part2/","excerpt":"","text":"1. Spark - SparkContext SparkContext allows your Spark Application to access Spark Cluster with the help of Resource Manager. The resource manager can be one of these three- Spark Standalone, YARN, Apache Mesos. The different contexts in which it can run are local, yarn-client, Mesos URL and Spark URL. 1.1 SparkContext Functions Functions of SparkContext 1.2 SparkContext Conclusion Hence, SparkContext provides the various functions in Spark like get the current status of Spark Application, set the configuration, cancel a job, Cancel a stage and much more. It is an entry point to the Spark functionality. Thus, it acts a backbone. 2. Spark - Stage Spark Stage- An Introduction to Physical Execution plan It is basically a physical unit of the execution plan. ShuffleMapstage ResultStage 2.1 What are Stages in Spark? one task per partition. In other words with the boundary of a stage in spark marked by shuffle dependencies. Ultimately, submission of Spark stage triggers the execution of a series of dependent parent stages. Although, there is a first Job Id present at every stage that is the id of the job which submits stage in Spark. 2.2 ShuffleMapStage ShuffleMapStage in Spark ResultStage in Spark 2.3 ResultStage ResultStage implies as a final stage in a job that applies a function on one or many partitions of the target RDD in Spark. It also helps for computation of the result of an action. running a function on a spark RDD Stage that executes a Spark action in a user program is a ResultStage. 3. Spark - Executor Apache Spark Executor for Executing Spark Tasks 3.1 Spark Executor Some conditions in which we create Executor in Spark is: When CoarseGrainedExecutorBackend receives RegisteredExecutor message. Only for Spark Standalone and YARN. When LocalEndpoint is created for local mode. n. 端点；[化]滴定终点 3.2 Creating Executor Instance By using the following, we can create the Spark Executor: From Executor ID. By using SparkEnv we can access the local MetricsSystem as well as BlockManager. Moreover, we can also access the local serializer by it. From Executor’s hostname. To add to tasks’ classpath, a collection of user-defined JARs. By default, it is empty. By flag whether it runs in local or cluster mode (disabled by default, i.e. cluster is preferred) Moreover, when creation is successful, the one INFO messages pop up in the logs. That is: INFO Executor: Starting executor ID [executorId] on host [executorHostname] 3.3 Heartbeat Sender Thread Basically, with a single thread, heartbeater is a daemon ScheduledThreadPoolExecutor. We call this thread pool a driver-heartbeater. 3.4 Launching Task Method 3.5 executor.taskLaunch.worker Thread Pool — ThreadPool Property executor 3.6 Conclusion we have also learned how Spark Executors are helpful for executing tasks. The major advantage we have learned is, we can have as many executors we want. Therefore, Executors helps to enhance the Spark performance of the system. 4. Spark - RDD Spark RDD – Introduction, Features &amp; Operations of RDD 4.1 resilient distributed dataset Resilient, i.e. fault-tolerant with the help of RDD lineage graph(DAG) and so able to recompute missing or damaged partitions due to node failures. Distributed, since Data resides on multiple nodes. Dataset represents records of the data you work with. The user can load the data set externally which can be either JSON file, CSV file, text file or database via JDBC with no specific data structure. 4.2 Why need RDD in Spark? Apache Spark evaluates RDDs lazily. It is called when needed, which saves lots of time and improves efficiency. The first time they are used in an action so that it can pipeline the transformation. Also, the programmer can call a persist method to state which RDD they want to use in future operations. 4.3 Features of Spark RDD 5. Spark RDD Operations Transformation Actions 5.1 Transformations a. Narrow Transformations b. Wide Transformations 5.2 Actions An Action in Spark returns final result of RDD computations. It triggers execution using lineage graph to load the data into original RDD. Conclusion – Spark RDD Because of the above-stated limitations of RDD to make spark more versatile DataFrame and Dataset evolved. 6. Limitation of Spark RDD Reference data-flair.training/blogs Spark RDD Operations-Transformation &amp; Action with Example Spark RDD常用算子学习笔记详解(python版) Spark常用的Transformation算子的简单例子","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Tutorials 1 - Introduce、Ecosysm、Features、Shell Commands","slug":"spark/Spark-Tutorials-Part1","date":"2020-09-18T23:07:21.000Z","updated":"2021-06-22T05:29:57.388Z","comments":true,"path":"2020/09/19/spark/Spark-Tutorials-Part1/","link":"","permalink":"http://www.iequa.com/2020/09/19/spark/Spark-Tutorials-Part1/","excerpt":"","text":"1. Spark - Introduction Objective – Spark Tutorial Introduction to Spark Programming Spark Tutorial – History Why Spark? Apache Spark Components a. Spark Core b. Spark SQL c. Spark Streaming Spark Tutorial – Learn Spark Programming 2. Spark - Ecosystem 3. Spark - Features 4. Spark - Shell Commands spark shell 4.1 Create a new RDD a) Read File from local filesystem and create an RDD. 1234from pyspark import SparkConf , SparkContextlines = sc.textFile(&quot;/Users/blair/ghome/github/spark3.0/pyspark/spark-src/word_count.text&quot;, 2)lines.take(3) b) Create an RDD through Parallelized Collection 12345from pyspark import SparkConf , SparkContextno = [1, 2, 3, 4, 5, 6, 8, 7]noData = sc.parallelize(no)#ParallelCollectionRDD[45] at readRDDFromFile at PythonRDD.scala:262 c) From Existing RDDs 123words= lines.map(lambda x : x + &quot;haha&quot;)words.take(3) 4.2 RDD Number of Items 12noData.count()#8 4.3 Filter Operation 1scala&gt; val DFData = data.filter(line =&gt; line.contains(&quot;DataFlair&quot;)) 4.4 Transformation and Action 1scala&gt; data.filter(line =&gt; line.contains(&quot;DataFlair&quot;)).count() 4.5 Read RDD first 5 item 12scala&gt; data.first()scala&gt; data.take(5) Let’s run some actions 123noData.count()noData.collect() 4.6 Spark WordCount 123456789101112131415161718192021222324252627282930from pyspark import SparkConf , SparkContextfrom operator import addsc.versionlines = sc.textFile(&quot;/Users/blair/ghome/github/spark3.0/pyspark/spark-src/word_count.text&quot;, 2)lines = lines.filter(lambda x: &#x27;New York&#x27; in x)#lines.take(3)words = lines.flatMap(lambda x: x.split(&#x27; &#x27;))#print(words.take(5))wco = words.map(lambda x: (x, 1))#print(wco.take(5))word_count = wco.reduceByKey(add)word_count.collect()# [# (&#x27;The&#x27;, 10),# (&#x27;of&#x27;, 33),# (&#x27;New&#x27;, 20),# (&#x27;begins&#x27;, 1),# (&#x27;around&#x27;, 4),# ...# ] 4.7 Write to HDFS 1word_count.saveAsTextFile(&quot;hdfs://localhost:9000/out&quot;) Reference data-flair.training/blogs Spark RDD Operations-Transformation &amp; Action with Example Spark RDD常用算子学习笔记详解(python版) Spark常用的Transformation算子的简单例子","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"SQL vs NoSQL","slug":"dataware/SQL_vs_NoSQL","date":"2020-09-15T01:07:21.000Z","updated":"2021-06-20T04:12:28.281Z","comments":true,"path":"2020/09/15/dataware/SQL_vs_NoSQL/","link":"","permalink":"http://www.iequa.com/2020/09/15/dataware/SQL_vs_NoSQL/","excerpt":"","text":"什么是事务？ 事务是指是程序中一系列严密的逻辑操作，而且所有操作必须全部成功完成，否则在每个操作中所作的所有更改都会被撤消。可以通俗理解为：就是把多件事情当做一件事情来处理，好比大家同在一条船上，要活一起活，要完一起完. 事物的四个特性（ACID） ● 原子性（Atomicity） ● 一致性（Consistency） ● 隔离性（Isolation） ● 持久性（Durability） 执行count(1)、count(*) 与 count(列名) 到底有什么区别 behavior questions: how your manager judge about you during your previous job? What’s your experience about a project that fails, and how would you redo the project? What’s your Advantages and disadvantages in your personality? What’s your best project and why you did better than others in your opinion? What’s your plan about your future career? tech questions: a problem of top k problem RMDB vs NoSQL DB Why distributed NoSQL DB cannot always support transaction? Level traverse a binary tree in an online white board. random questions about spark, hadoop etc How os’ thread works? General DWH concepts, Spark internals, mapreduce Explain the map reduce paradigm Reference 什么是事务？事务的四个特性以及事务的隔离级别 干货 | SQL 与 NoSQL还在傻傻分不清？ NoSql对于事务的支持 Thread in Operating System 面试必考 | 进程和线程的区别","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://www.iequa.com/tags/SQL/"}]},{"title":"HDFS 演进之路","slug":"hadoop/HDFS-Evolution-NX","date":"2020-09-14T12:07:21.000Z","updated":"2021-06-22T06:35:46.280Z","comments":true,"path":"2020/09/14/hadoop/HDFS-Evolution-NX/","link":"","permalink":"http://www.iequa.com/2020/09/14/hadoop/HDFS-Evolution-NX/","excerpt":"","text":"本节目标： HDFS 是如何实现有状态的高可用架构 HDFS 是如何从架构上解决单机内存受限问题的 揭秘 HDFS 能支撑亿级流量的核心源码设计 Hadoop HDFS HDFS1 是一个 主从式 架构, 主节点只有一个叫 NameNode. 从节点有多个叫 DataNode 1. HDFS1 架构 1.1 HDFS1 架构缺陷 单点故障问题 内存受限问题 1.2 单点故障 Solution HDFS 1.3 内存受限 Solution HDFS 2. HDFS2 结构 Solution HDFS1 Question HA 方案 (High Avaiable) 解决 HDFS1 Namenode 单点故障问题 联邦方案 解决了 HDFS1 内存受限问题 3. HDFS3 HA 方案支持多个 Namenode 引入纠删码技术 思考： 因为 NameNode 管理了元数据, 用户所有的操作请求都要操作 Namenode， 大一点的平台一天需要运行几十万上百万的任务。一个任务就会有很多个请求，这些所有的请求都打到 Namenode 这儿 (更新目录树)， 对于 Namenode 来说这就是亿级的流量. Namenode 是如何支撑亿级流量的呢？ Reference 50个Hadoop的面试问题 大数据开发系列直播课 ③","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"http://www.iequa.com/tags/hdfs/"}]},{"title":"SparkSql - 结构化数据处理 (下)","slug":"spark/spark-aura-9.2-SparkSql","date":"2020-08-27T23:07:21.000Z","updated":"2021-06-20T04:12:28.355Z","comments":true,"path":"2020/08/28/spark/spark-aura-9.2-SparkSql/","link":"","permalink":"http://www.iequa.com/2020/08/28/spark/spark-aura-9.2-SparkSql/","excerpt":"","text":"1. spark 整合 yarn rdd.aggregateByKey(init_value: U) ((C, U) =&gt; (C,C) =&gt; C) Spark 2.3 on yarn的配置安装 1.1 配置 Spark 整合 YARN 把 yarn-site.xml 复制到 $SPARK_HOME/conf 目录中 在使用 spark-submit 提交任务时候请这么执行资源调度系统： spark-submit --master yarn --deploy-mode client 但是有可能出现异常 1.2 Spark-Shell 测试 1234567只能这么启动spark-shell --master yarn --deploy-mode client不能这么启动spark-shell --master yarn --deploy-mode cluster原因： spark-shell spark-submit driver 原来： 12spark-shellspark-shell --master local[*] 1.3 Spark-Submit 测试 1234567只能这么启动spark-submit --master yarn --deploy-mode client也能这么启动spark-submit --master yarn --deploy-mode cluster原因： spark-shell spark-submit driver 2. spark 整合 hive 2.1 Spark 自带元数据库 &amp; 如果用户直接运行bin/spark-sql命令。会导致我们的元数据有两种状态： 1、in-memory状态: 如果SPARK-HOME/conf目录下没有放置hive-site.xml文件，元数据的状态就是in-memory， 也就是使用自带的 derby 在当前会话中有效 12345create table student(id int, name string, sex string, age int, department string)row format delimited fields terminated by &quot;,&quot;load data local inpath &quot;/home/..&quot; spark-sql 在 hadoop02 和 hadoop03 中启动的时候，都各自初始化了一个元数据库 所以在 hadoop02 上创建的元数据库，在 hadoop03 上启动的 spark-sql 不能共用数据. spark-sql 的使用有2种模式 2.2 Spark 整合hive配置 2、hive状态： 如果我们在SPARK-HOME/conf目录下放置了，hive-site.xml文件，那么默认情况下 spark-sql的元数据的状态就是hive. 2.3 SparkSQL 脚本使用 3. sparksql和hive的自定义函数 3.1 SparkSQL UDF 1show functions; 3.2 SparkSQL UDAF 3.3 使用测试 4. SparkSQL 常用窗口分析函数 5. 综合练习 Reference Spark实例-自定义聚合函数 Spark UDF使用详解及代码示例 看了之后不再迷糊-Spark多种运行模式,俺是亮哥 Spark SQL, DataFrame 和 Dataset 编程指南 Spark2.x学习笔记：14、Spark SQL程序设计 SparkSQL学习 1 2 3 SparkSQL在有赞大数据的实践（二） How to convert rdd object to dataframe in spark 云课堂 SparkSQL 的数据源操作 大数据资料笔记整理 HDOJ_1711_KMP 求匹配位置","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Shuffle Optimize 10 items","slug":"spark/spark-ma-public-shuffle-optimization","date":"2020-08-12T12:07:21.000Z","updated":"2021-06-22T06:12:18.005Z","comments":true,"path":"2020/08/12/spark/spark-ma-public-shuffle-optimization/","link":"","permalink":"http://www.iequa.com/2020/08/12/spark/spark-ma-public-shuffle-optimization/","excerpt":"","text":"调优: 开发调优 资源调优 DataSkew shuffle 今天的内容: （1）Spark Task 执行过程详细梳理 （2）DataSkew 发生时的现象和原因分析 （3）DataSkew 原理分析 （4）DataSkew 适用场景分析 （5）DataSkew solution 优缺点分析 （6）DataSkew solution 实践经验分享 Preface 1). 哪个是 窄依赖 ？ B (计算之前之后 2个 RDD 记录是 1对1) A、join B、filter map foreach C、sort D、group 2). 关于广播变量 ？ BCD (在Driver声明的，用来序列化到每个Executor中供Task使用) B、read-only C、存储在各个节点 （从节点：Worker负责启动和管理 Executor） D、能存储在磁盘或HDFS （默认是存储在内存里面，[存储内存 执行内存]） 3). Spark 为什么比 MapReducer 快? ABC A、基于内存计算，减少低效的磁盘交互 B、高效的调度算法, 基于 DAG C、容错机制 Linage，精华部分就是 DAG 和 Linage DAG引擎！ == mapreduce spark 能够把中间结果放到内存里面 spark 官方宣称： SPark hadoop 0.9:100 迭代计算 做一次 3：1 Distributed Computing 分布式计算，不怕数据量大，就怕 DataSkew Shuffle 调优 10点 (DataSkew Solution) 1. 使用Hive ETL预处理数据 导致 DataSkew 的是 Hive 表。如果该 Hive 表中的数据本身很不均匀(比如某个 key 对应了 100 万数据，其他 key 才对应了 10 条数据)，而业务场景需要频繁用 Spark 对 Hive 表执行某个分析操作，那么比较适合使用这种技术方案: Hive ETL 预处理数据 2. 调整shuffle操作的并行度 碰运气做法： 原来的并行度导致了倾斜，调整并行度， 如果是自定义的分区规则决定必须是n个分区，n个Task 依然使用默认的HasPartitoiner，那么这种碰运气的方案是有用的 大量不同的Key被分配到了相同的Task造成该Task数据量过大。 如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一 种方案。但是也是一种属于 碰运气的方案。因为这种方案，并不能让你一定解决数据倾斜，甚至有可能 加重。那当然，总归，你会调整到一个合适的并行度是能解决的。前提是这种方案适用于 Hash散列的 分区方式。凑巧的是，各种分布式计算引擎，比如MapReduce，Spark 等默认都是使用 Hash散列的方 式来进行数据分区。 Spark 在做 Shuffle 时，默认使用 HashPartitioner(非Hash Shuffle)对数据进行分区。如果并行度设 置的不合适，可能造成大量不相同的 Key 对应的数据被分配到了同一个 Task 上，造成该 Task 所处理的 数据远大于其它 Task，从而造成 DataSkew。 如果调整 Shuffle 时的并行度，使得原本被分配到同一 Task 的不同 Key 发配到不同 Task 上处理，则可 降低原 Task 所需处理的数据量，从而缓解 DataSkew 造成的短板效应。 并行度为2 2.5. 企业最佳实践 该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万， 那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理， 因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝 试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 如果之前的额并行度是12，现在调整成为 18 有用么?并没有多大的改善 10个 11个 13 不要拥有一样的公约数, (特别是最大公约数) 3. 过滤少数导致倾斜的key 无用数据直接过滤。 产生数据倾斜是由于部分无效数据导致的。把这部分无效数据过滤掉即可！ 12345678你们的大宽表：有些字段的值：null departmentselect department, count(*) as total from employee group by deparment;底层的分区规则，不管是什么规则，都会把所有的 null 记录分发到同一个TaskGM角色： select department, count(*) as total from employee group by deparment where role != &quot;gm&quot;; rdd = sparkContext.xxxx() rdd.filter(x =&gt; true | false) 把无效数据进行过滤 4. 将reduce join转为map join 大小表做连接, mapjoin spark中如何实现 mapjoin 的逻辑呢？ 使用 spark的广播机制！ 就是可以把小表数据当做广播变量，使用广播机制，把该变量数据，广播到所有的executor里面去。 reduceJoin: reduceJoin mapJoin: mapJoin 5. 采样倾斜 key 并分拆 join 操作 现在假设一个数据倾斜场景中的数据分为两种： 一种是导致倾斜的数据集合： 单独处理 一种是不导致倾斜的数据集合： 单独处理 最后把结果合起来！ 采样倾斜 key 并分拆 join 操作 6. 两阶段聚合(局部聚合+全局聚合) 两阶段聚合 方案六： 两阶段聚合 （聚合类逻辑的通用解决方案） 纵向切分 原来：一次hash散列导致倾斜 现在：一次随机shuffle + 一次hash散列 7. 使用随机前缀和扩容 RDD 进行 join 方案七： 增加随机字段/链接字段 + 扩容RDD 表A 表B RDD1 rdd2 两种场景： 1、如果 两张表做笛卡尔积 2、如果两张表做join，并且导致数据倾斜的某些key比较多。 拆分出来单独处理（依然可能有数据倾斜 ==&gt; 加随机前缀） 如果是导致倾斜的key只有一个，这个key的数据量非常。 加随机前缀 复制 1、笛卡尔积 2、导致倾斜的key的数据；量特别大。 不能使用单个task解决 8. 任务横切，一分为二，单独处理 9. 多种方案组合使用 10. 自定义 Partitioner 11. bitmap 求 Join 两阶段聚合 Reference Spark性能优化指南——基础篇 Spark性能优化指南——高级篇 大数据资料笔记整理 史诗级最详细10招Spark数据倾斜调优","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark dev Optimize 10 Items","slug":"spark/spark-aura-6.2-spark-dev-optimization-1-7","date":"2020-08-10T02:07:21.000Z","updated":"2021-06-22T06:12:17.993Z","comments":true,"path":"2020/08/10/spark/spark-aura-6.2-spark-dev-optimization-1-7/","link":"","permalink":"http://www.iequa.com/2020/08/10/spark/spark-aura-6.2-spark-dev-optimization-1-7/","excerpt":"","text":"调优: 开发调优 资源调优 数据倾斜 shuffle 旧知识点: 数据倾斜, 开发调优的一部分 新知识: spark的内存模型, spark的资源调优, spark的shuffle 整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。 开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础； 数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案； shuffle调优，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 今天的内容: 开发调优 数据倾斜 开发调优 10点 1. 避免创建重复的RDD mapreduce 的执行过程中, 如果有 reducer 的话, 那么就一定会进行排序. 而且这个排序, 并不是对我们最终的计算结果排序. 这个排序对我们的结果貌似没什么用处, 但是呢，又一定要有. 原因是什么呢? 最终的结论： 如果是需要对一个文件进行多次的计算, 那么注意, 最好就 only read one time. RDD: 不可变, 可分区的 弹性数据集 2. 尽可能复用同一个RDD 1234map(x =&gt; x+1)map(x =&gt; x*2)map(x =&gt; 2 * (x+1)) 3. 对多次使用的RDD进行持久化 1234567891011cachepersist val rdd2 = rdd1.map.reducerdd2.cache rdd2.sort.map()rdd2.groupByKey rdd1.map.reduce.sort.map()rdd1.map.reduce.groupByKey 程序运行过程中的 data 放置在 内存, 如程序运行 finish. 中间的数据会垃圾回收. 如果在程序执行过程中, 生成了一些中间结果是另外一个程序需要使用的数据 那么就可以把该 data persist 到内存中 或 磁盘中. 另外一个程序就可以避免重复计算, 直接从磁盘或内存中进行读取. 所以为了尽快的提交任务的执行效率, 尽量把重复利用的数据持久化到内存中. 4. 尽量避免使用 Shuffle 类算子 shuffle 到底有什么坏处? 分布式计算 决定了 一定会有 shuffle join: mapjoin, reducejoin shuffle 类算子: reduceByKey groupBy sortBy distinct 聚合类操作基本都有 shuffle. (union、join[按照hash分区, 分区数还成倍数, 就没shuffle]) xxxByKey (groupBy + xxxx) rdd1.join(rdd2) (reducejoin) // 如何避免 join 中的 shuffle mapjoin 在 mapreduce 当中, 我们知道如何定义 mapreduce 的 join 实现程序 但是在 spark中 你知道如何实现么？ 12job.addCacheFile() // 通过底层的 DistributedCache 这个组件。 来给我们 () 中的文件进行全局分发 // 全局分发： 发送文件到所有的要执行的 mapTask 节点 spark 实现伪代码 BroadCast rdd1.join(rdd2) rdd1.foreachPartition(data =&gt; { val data2 = bc.value // 小表数据 val data1 = data // 大小数据的一个 Partition data1.join(data2) // 这个无shuffle, 因为这一句代码,这个操作是在每个阶段独立执行的. }) mapper 3分钟 shuffle 2分钟 reduce 2分钟 5. 使用 map-side 预聚合(combina)的 shuffle 操作 shuffle 类算子有第3个缺点： 数据倾斜 wordcount： File block1 block1 block2 在使用 shuffle 操作的算子的时候， 如果右 map-side 预聚合操作的话 那么 shuffle 的代价还是会小很多 附带的好效果： 降低数据倾斜的程度 (1) 没有 map-side 预聚合的算子： groupByKey 有 shuffle, 没有聚合 coGroup val rdd: RDD[String, Iterable[Int], Iterable[String]] (2) 有预聚合的 shuffle 算子： 执行流程上, 执行阶段 reduceByKey = groupByKey + reduce 最终效率上： reduceByKey &gt; groupByKey + reduce reduceByKey, combineByKey, aggregateByKey 6. 使用高性能算子 6.1 使用reduceByKey/aggregateByKey替代groupByKey 详情见“原则五：使用map-side预聚合的shuffle操作”。 6.2 用 foreachPartitions 替代 foreach 需求： 如果 rdd 有 10000 条数据， 10个分区： 12345678910111213// 获取了 10000 个连接rdd.map(x =&gt; &#123; val connect = Connect.getConnect() // 模拟获取数据库连接 connect.insert(x)&#125;)// 获取了10次连接rdd.mapPartitions(data =&gt; &#123; val connect = Connect.getConnect() // 模拟获取数据库连接 for( element &lt;- data) &#123; connect.insert(element) &#125;&#125;) 原则： 如果一个操作能针对 partition 完成，就不要针对单个元素 DStream RDD 1234567dstreams.foreachRDD(rdd =&gt; &#123; rdd.foreachPartition(ptn =&gt; &#123; rdd.foreach(element =&gt; &#123; &#125;) &#125;)&#125;) 6.3 使用 filter 后 coalesce 操作 rdd.filter(奇数).coalesce(6).map(平方) rdd.map(平方).filter(奇数) 6.4 使用repartitionAndSortWithinPartitions替代repartition与sort类操作 repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 rdd.repartition.sort rdd.repartitionAndSortWithinPartitions 第一个式子效率差 1rdd.repartition = rdd2 rdd2.sort = rdd3 7. 广播大变量 有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 一句话： 目的： 让多个 task 都要使用的在 driver 中声明的变量都要维持一个独立副本， 编程让这个 executor 的内存占用量就减少了 效果： executor 的内存占用量就减少了. 网络数据传输量也减少了 原则： 要广播的数据越大, 进行广播这个操作之后得到的收益越好的. mapreduce: DistributedCache spark: BroadCastManager SparkContext 8. 使用Kryo优化序列化性能 java: mapreduce 他们的区别 java中创建对象的方式: 构造器 静态工厂方法 (私有化了构造器) 反射 克隆 反序列化 java： 实现序列化: 让参与序列化的类型 implements Serializable ObjectOutputStream oos = new ... oos.writeObject(student) 网络IO，FileIO Redis: String 把一个对象： toString, 序列化, JSON 有个缺点: 除了把当前这个对象的属性的值给存储/携带之外, 还会把当前这个对象的类型的信息都携带. 加入要传入 10000 个对象： 12341 : 类型信息 对象信息2 : 类型信息 对象信息...1000 : 类型信息 对象信息 mapreduce: 序列化，自定义规则 对于类型信息，只会携带一次 12341 : 对象信息2 : 对象信息...1000 : 对象信息 java 原生序列组件的原因 实现方式: 123456789101112131415class Student implements Writable &#123; private int id; private String name; // 序列化 write(out) &#123; out.writeInt(id); out.writeUTF(name); &#125; // 反序列化 readFields(in) &#123; this.id = in.readInt(); this.name = in.readUTF(); &#125;&#125; 他们的区别： mapreduce 的序列化机制, 只序列化要进行传输的属性的值，不重复序列化对象的类型信息 spark： 默认情况下, 是支持 java 原生序列化机制 使用 KryoSerializer 12345678使用方式:// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 9. 优化数据结构 Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用 字符串替代对象，使用原始类型（比如Int、Long）替代字符串， 数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 不要刻意去这么做, 也要注意可读性. 但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 10. 融会贯通 Reference Spark性能优化指南——基础篇 Spark性能优化指南——高级篇 大数据资料笔记整理","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Data Analysis - SQL 50 第2篇 Action","slug":"dataware/dw-sql-50-2","date":"2020-08-08T01:07:21.000Z","updated":"2021-06-22T06:03:38.734Z","comments":true,"path":"2020/08/08/dataware/dw-sql-50-2/","link":"","permalink":"http://www.iequa.com/2020/08/08/dataware/dw-sql-50-2/","excerpt":"","text":"其中重点为：1/2/5/6/7/10/11/12/13/15/17/18/19/22/23/25/31/35/36/40/41/42/45/46 共16题 超级重点 18和23、 22和25 、 41、46 (1). 查询课程编号为“01”的课程比“02”的课程成绩高的所有学生的学号（重点） 123456789101112131415SELECT t1.s_id as t1_s_id, t2.s_id as t2_s_id, t3.s_name, t1.s_score as s_score_01, t2.s_score as s_score_02FROM (select s_id, c_id, s_score from Score WHERE c_id = &#x27;01&#x27;) as t1INNER JOIN (select s_id, c_id, s_score from Score WHERE c_id = &#x27;02&#x27;) as t2ON t1.s_id = t2.s_idINNER JOIN Student as t3 ON t1.s_id = t3.s_idwhere t1.s_score &gt; t2.s_score (2). 查询平均成绩大于60分的学生的学号和平均成绩（简单，第二道重点） bilibili analysis 12345SELECT s_id, avg(s_score)FROM scoreGROUP BY s_id HAVING avg(s_score) &gt; 60 (9). 查询所有课程成绩小于60分的学生的学号、姓名 得出同学课程成绩小于 60分 的课程数 统计同学总共学了几门课 1234567891011121314151617181920212223SELECT a.s_id, t.s_nameFROM(SELECT s_id, count(c_id) as cntFROM ScoreWHERE s_score &lt; 60GROUP BY s_id) aINNER JOIN(SELECT s_id, count(c_id) as cnt FROM ScoreGROUP BY s_id) bON a.s_id = b.s_idINNER JOINStudent as t ON a.s_id=t.s_idWHERE a.cnt = b.cnt (3). 查询所有学生的学号、姓名、选课数、总成绩 学号 姓名 课程编号 这门课成绩 1 小张 1 60 1 小张 3 70 123456789SELECT t1.s_id, t1.s_name, COUNT( t2.c_id ), SUM(case when t2.s_score is NULL then 0 else t2.s_score END) FROM Student AS t1 LEFT JOIN Score AS t2 ON t1.s_id = t2.s_idGROUP BY s_id, t1.s_name (4). 查询姓“猴”的老师的个数（不重要） 1234SELECT count(t_id)FROM teacherWHERE t_name LIKE &#x27;张%&#x27; 1234SELECT count(distinct t_name)FROM teacherWHERE t_name LIKE &#x27;张%&#x27; (5). 查询没学过“张三”老师课的学生的学号、姓名（重点） 学号 课程号 成绩 教师号 教师姓名 s_1 c_1 90 t_1 张三 1234567SELECT s_id, s_name from StudentWHERE s_id not in ( SELECT s_id FROM Score s INNER JOIN Course c ON s.c_id = c.c_id INNER JOIN Teacher t ON c.t_id = t.t_id WHERE t.t_name=&#x27;张三&#x27;) (6). 查询学过“张三”老师所教的所有课的同学的学号、姓名（重点) 12345678SELECT st.s_id, st.s_name FROM Student as st INNER JOIN Score s ON s.s_id = st.s_id INNER JOIN Course c ON s.c_id = c.c_id INNER JOIN Teacher t ON c.t_id = t.t_id WHERE t.t_name=&#x27;张三&#x27;) 平时做的时候表太大，我们会先过滤用 ’张三‘ 的条件，做成 temp table 在开始做 JOIN. (7). 查询学过编号为“01”的课程并且也学过编号为“02”的课程的学生的学号、姓名（重点） 123456789SELECT * FROM STUDENT WHERE s_id IN( SELECT a.s_id FROM (SELECT s_id, c_id FROM Score WHERE c_id = &#x27;01&#x27;) a INNER JOIN (SELECT s_id, c_id FROM Score WHERE c_id = &#x27;02&#x27;) b ON a.s_id = b.s_id) (8). 查询课程编号为“02”的总成绩（不重点） 123SELECT SUM(s_score)FROM ScorewWHERE c_id = &#x27;02&#x27; (10). 查询没有学全所有课的学生的学号、姓名(重点) 12SELECT * FROM courseSELECT * FROM Score Error Version: 1234567SELECT s_id, s_name FROM Student WHERE s_id IN ( SELECT s_id FROM Score GROUP BY s_id HAVING count(distinct c_id) &lt; (SELECT COUNT(distinct c_id) FROM Course))-- 一门课都没有学，上面的 SQL 就漏掉了. Right Version: 1234567SELECT st.*, sc.*FROM Student as stLEFT JOIN Score as sc ON st.s_id=sc.s_idGROUP BY st.s_id HAVING count(distinct sc.c_id) &lt; (SELECT COUNT(distinct c_id) FROM Course))-- 一门课都没有学，上面的 SQL 就漏掉了. (11). 查询至少有一门课与学号为“01”的学生所学课程相同的学生的学号和姓名（重点） and在括号外用 distinct 不知道什么时候用？ (12). 查询和“01”号同学所学课程完全相同的其他同学的学号(重点) Reference 常见的SQL面试题：经典50题 - 知乎 SQL面试必会50题 - 知乎 bilibili【数据分析】- SQL面试50题 - 跟我一起打怪升级 一起成为数据科学家 数据分析师成长之路 【SQL】SQL面试50题 分类梳理与解答 【Python】数据分析前的入门教程 Python For Everybody P1：零基础程序设计","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Data Analysis - SQL 50 第1篇","slug":"dataware/dw-sql-50-1","date":"2020-08-07T01:07:21.000Z","updated":"2021-06-22T06:03:38.730Z","comments":true,"path":"2020/08/07/dataware/dw-sql-50-1/","link":"","permalink":"http://www.iequa.com/2020/08/07/dataware/dw-sql-50-1/","excerpt":"","text":"本文汇总了不同版本的SQL面试50题的题目，删去了重复内容，并根据涉及知识点进行分类，按照由易到难、相似题放在一起的思路进行排序并重新编 Table 关系图： SQL面试必会50题 - 知乎 建表 &amp; 准备数据： 1234567891011121314151617181920212223242526272829-- 建表-- 学生表CREATE TABLE `Student`(`s_id` VARCHAR(20),`s_name` VARCHAR(20) NOT NULL DEFAULT &#x27;&#x27;,`s_birth` VARCHAR(20) NOT NULL DEFAULT &#x27;&#x27;,`s_sex` VARCHAR(10) NOT NULL DEFAULT &#x27;&#x27;,PRIMARY KEY(`s_id`));-- 课程表CREATE TABLE `Course`(`c_id` VARCHAR(20),`c_name` VARCHAR(20) NOT NULL DEFAULT &#x27;&#x27;,`t_id` VARCHAR(20) NOT NULL,PRIMARY KEY(`c_id`));-- 教师表CREATE TABLE `Teacher`(`t_id` VARCHAR(20),`t_name` VARCHAR(20) NOT NULL DEFAULT &#x27;&#x27;,PRIMARY KEY(`t_id`));-- 成绩表CREATE TABLE `Score`(`s_id` VARCHAR(20),`c_id` VARCHAR(20),`s_score` INT(3),PRIMARY KEY(`s_id`,`c_id`)); 1234567891011121314151617181920212223242526272829303132333435363738-- 插入学生表测试数据insert into Student values(&#x27;01&#x27; , &#x27;赵雷&#x27; , &#x27;1990-01-01&#x27; , &#x27;男&#x27;);insert into Student values(&#x27;02&#x27; , &#x27;钱电&#x27; , &#x27;1990-12-21&#x27; , &#x27;男&#x27;);insert into Student values(&#x27;03&#x27; , &#x27;孙风&#x27; , &#x27;1990-05-20&#x27; , &#x27;男&#x27;);insert into Student values(&#x27;04&#x27; , &#x27;李云&#x27; , &#x27;1990-08-06&#x27; , &#x27;男&#x27;);insert into Student values(&#x27;05&#x27; , &#x27;周梅&#x27; , &#x27;1991-12-01&#x27; , &#x27;女&#x27;);insert into Student values(&#x27;06&#x27; , &#x27;吴兰&#x27; , &#x27;1992-03-01&#x27; , &#x27;女&#x27;);insert into Student values(&#x27;07&#x27; , &#x27;郑竹&#x27; , &#x27;1989-07-01&#x27; , &#x27;女&#x27;);insert into Student values(&#x27;08&#x27; , &#x27;王菊&#x27; , &#x27;1990-01-20&#x27; , &#x27;女&#x27;);-- 课程表测试数据insert into Course values(&#x27;01&#x27; , &#x27;语文&#x27; , &#x27;02&#x27;);insert into Course values(&#x27;02&#x27; , &#x27;数学&#x27; , &#x27;01&#x27;);insert into Course values(&#x27;03&#x27; , &#x27;英语&#x27; , &#x27;03&#x27;);-- 教师表测试数据insert into Teacher values(&#x27;01&#x27; , &#x27;张三&#x27;);insert into Teacher values(&#x27;02&#x27; , &#x27;李四&#x27;);insert into Teacher values(&#x27;03&#x27; , &#x27;王五&#x27;);-- 成绩表测试数据insert into Score values(&#x27;01&#x27; , &#x27;01&#x27; , 80);insert into Score values(&#x27;01&#x27; , &#x27;02&#x27; , 90);insert into Score values(&#x27;01&#x27; , &#x27;03&#x27; , 99);insert into Score values(&#x27;02&#x27; , &#x27;01&#x27; , 70);insert into Score values(&#x27;02&#x27; , &#x27;02&#x27; , 60);insert into Score values(&#x27;02&#x27; , &#x27;03&#x27; , 80);insert into Score values(&#x27;03&#x27; , &#x27;01&#x27; , 80);insert into Score values(&#x27;03&#x27; , &#x27;02&#x27; , 80);insert into Score values(&#x27;03&#x27; , &#x27;03&#x27; , 80);insert into Score values(&#x27;04&#x27; , &#x27;01&#x27; , 50);insert into Score values(&#x27;04&#x27; , &#x27;02&#x27; , 30);insert into Score values(&#x27;04&#x27; , &#x27;03&#x27; , 20);insert into Score values(&#x27;05&#x27; , &#x27;01&#x27; , 76);insert into Score values(&#x27;05&#x27; , &#x27;02&#x27; , 87);insert into Score values(&#x27;06&#x27; , &#x27;01&#x27; , 31);insert into Score values(&#x27;06&#x27; , &#x27;03&#x27; , 34);insert into Score values(&#x27;07&#x27; , &#x27;02&#x27; , 89);insert into Score values(&#x27;07&#x27; , &#x27;03&#x27; , 98); Reference 常见的SQL面试题：经典50题 - 知乎 SQL面试必会50题 - 知乎 bilibili【数据分析】- SQL面试50题 - 跟我一起打怪升级 一起成为数据科学家 数据分析师成长之路 【SQL】SQL面试50题 分类梳理与解答 【Python】数据分析前的入门教程 Python For Everybody P1：零基础程序设计","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Thin belly","slug":"tools/thin-belly","date":"2020-08-06T14:13:48.000Z","updated":"2021-06-22T06:54:43.494Z","comments":true,"path":"2020/08/06/tools/thin-belly/","link":"","permalink":"http://www.iequa.com/2020/08/06/tools/thin-belly/","excerpt":"","text":"⬇️，减肥总结的一些经验技巧 1⃣饮食 零食真的很长肉‼️用**水果 坚果**代替零食‼️不要喝饮料‼️可以用果汁或swisse胶原蛋白兑水 代替饮料排毒 柠檬蜂蜜百香果茶 如果有的话弄一点出来泡水喝 味道也很好 早饭一定要吃‼️‼️这个真的很重要 ‼️不吃早饭会拖慢你一天的新陈代谢只会越来越胖 午饭可以多吃一点 但是**不要太油腻** 可以吃少量 鸡肉 牛肉, 鸡蛋 都很不错 但是不要吃猪肉 猪肉最容易长胖 晚饭可以吃点水果 坚果 全麦面包或者喝点粥 (不要让自己营养不良) 那些蛋糕 奶油 巧克力 奶茶 冰淇淋之类的就不要碰了 这些比肉可怕多了 2⃣瘦身小习惯 饭后不要久坐‼️ 容易堆积脂肪 早上空腹喝水‼️ 有助于消除便秘 多喝绿茶‼️ 茶有燃烧脂肪的作用 饭后靠墙站‼️ 晚饭后半小时 让整个身体背着向着紧贴墙壁 夹紧臀部 让臀部、背部、腰部、头、脖子等都尽量紧贴墙面 3⃣按摩 疏通经络 按摩真的很有效‼️ 特别是肚子和腿 可选: 买个按摩霜和按摩刷 4⃣关于运动 如果只是去健身房办张卡 不请私教自己瞎练 真的是要走很多弯路 很多细节动作 发力技巧 没有人教 效果真的是大相径庭 夜跑啊什么的 跑完一定要放松要拉伸 不然就是硬邦邦的肌肉腿 个人觉得 减肥塑形最好的运动是游泳 推荐 Swimming Reference 如何瘦肚子？ 揭秘男人怎样减掉大肚子男人减肥肚子","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"LoseWeight","slug":"LoseWeight","permalink":"http://www.iequa.com/tags/LoseWeight/"}]},{"title":"Data Warehouse - 工具篇","slug":"dataware/dw-wy-3-tools","date":"2020-08-03T02:07:21.000Z","updated":"2021-06-22T06:04:44.244Z","comments":true,"path":"2020/08/03/dataware/dw-wy-3-tools/","link":"","permalink":"http://www.iequa.com/2020/08/03/dataware/dw-wy-3-tools/","excerpt":"","text":"18. Hadoop框架 19. Hive介绍 20. Hive之DDL 21. Hive之DML 22. Hive之查询语句 23. Hive函数全攻略 Reference 在写blog/Html时嵌入Pdf显示 bilibili 企业级数据仓库实战 企业级数据仓库PPT分享 讲师：南头居士 - 数据科学之企业级数据仓库 【数据分析】- SQL面试50题 - 跟我一起打怪升级 一起成为数据科学家 漫谈数据仓库之维度建模 Lambda在线 &gt; 木东居士 &gt; 如何深入浅出的理解数据仓库建模？ 木东居士谈数仓的学习方法！ 【直播回放】20200314_B站_数据仓库分享","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Data Warehouse - 理论篇 2","slug":"dataware/dw-wy-2-theory-2","date":"2020-08-03T00:07:21.000Z","updated":"2021-06-22T06:04:44.252Z","comments":true,"path":"2020/08/03/dataware/dw-wy-2-theory-2/","link":"","permalink":"http://www.iequa.com/2020/08/03/dataware/dw-wy-2-theory-2/","excerpt":"","text":"9. 维度表基本概念 10. 常见的维度表类型 11. 数据分层的意义 12. 经典分层理论详解 13. 分层的边界 14. 数据的流向 15. 数据架构Lambda与Kappa架构 16. 典型的数据流设计思路 17. 数据服务业务的几种方式 Reference 在写blog/Html时嵌入Pdf显示 bilibili 企业级数据仓库实战 企业级数据仓库PPT分享 讲师：南头居士 - 数据科学之企业级数据仓库 【数据分析】- SQL面试50题 - 跟我一起打怪升级 一起成为数据科学家 漫谈数据仓库之维度建模 Lambda在线 &gt; 木东居士 &gt; 如何深入浅出的理解数据仓库建模？ 木东居士谈数仓的学习方法！ 【直播回放】20200314_B站_数据仓库分享","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Data Warehouse - 理论篇 1","slug":"dataware/dw-wy-2-theory-1","date":"2020-08-01T04:07:21.000Z","updated":"2021-06-22T06:04:44.241Z","comments":true,"path":"2020/08/01/dataware/dw-wy-2-theory-1/","link":"","permalink":"http://www.iequa.com/2020/08/01/dataware/dw-wy-2-theory-1/","excerpt":"","text":"1. 范式建模 1.1 范式建模 1 2 3 NF 1.2 范式建模 优缺点 优点: 节约存储 结构清晰, 易于理解 适合关系型数据库 缺点： 构建比较繁琐 查询复杂 不合适构建在大数据分布式环境下 1.3 范式建模优缺点 虽然有这些缺点, 但是范式建模的理论, 仍然是需要我们去熟练掌握. 原因有如下几点: 数据仓库 的上游有相当一部分数据源是业务数据库, 而这些业务数据库基于范式理论 数据源的规范定义需要我们了解范式理论 数据仓库下游系统比如 报表 系统设计时, 可能会用到范式理论. 2. 维度建模 Kimball 2.1 维度建模优缺点 优点: 方便使用 适合大数据下的数据处理 适合进行 OLAP 操作 缺点: 维度补全造成的数据存储的浪费 维度变化造成的数据更新量大 与范式理论差异很大, 是典型的反三范式 思考摘要: 范式建模里的范式, 具体指的是什么, 哪些常见会使用到范式 维度建模理论中的反范式是指什么, 为什么会这样操作 请叙述维度建模的 4 个步骤 2. 维度建模的4个步骤 Kimball 订单表： Kimball 建模过程: Kimball 3. 事实表的基本概念 3.1 度量 事实表的度量 3.2 一致性 事实表的一致性 4. 维度建模 - 常见事实表 事务事实表 周期快照事实表 累计快照事实表 无事实的事实表 聚集事实表 Reference bilibili 企业级数据仓库实战 企业级数据仓库PPT分享 讲师：南头居士 - 数据科学之企业级数据仓库 【数据分析】- SQL面试50题 - 跟我一起打怪升级 一起成为数据科学家 漫谈数据仓库之维度建模 Lambda在线 &gt; 木东居士 &gt; 如何深入浅出的理解数据仓库建模？ 木东居士谈数仓的学习方法！ 【直播回放】20200314_B站_数据仓库分享","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"Data Warehouse - 基础篇","slug":"dataware/dw-wy-1-basic","date":"2020-08-01T02:07:21.000Z","updated":"2021-06-22T06:03:38.739Z","comments":true,"path":"2020/08/01/dataware/dw-wy-1-basic/","link":"","permalink":"http://www.iequa.com/2020/08/01/dataware/dw-wy-1-basic/","excerpt":"","text":"1. what’s data warehouse? 2. why build data warehouse? 3. 什么情况应建 data warehouse ? 当你需要集中化管理你的数据 当你希望以更高效的方式使用数据 当你的数据量和复杂度到了需要一个团队来维护时 当你希望想要数据驱动业务时 当你想要借助大数据的力量来提升产品竞争力 当你想时刻知道业务发展情况时 4. Warehouse vs Middle platform 5. OLTP vs OLAP 6. 应用关系 7. 模式设计的3个阶段 7.1 概念模型 7.2 逻辑模型 7.3 物理模型 聊聊你理解的 OLAP 和 OLTP (使用场景、技术栈等) 聊聊你对维度建模的理解 8. 技术组件 Reference bilibili 企业级数据仓库实战 企业级数据仓库PPT分享 讲师：南头居士 - 数据科学之企业级数据仓库 【数据分析】- SQL面试50题 - 跟我一起打怪升级 一起成为数据科学家","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"}]},{"title":"SparkCore 中的工作原理 - 任务执行流程 5.1","slug":"spark/spark-aura-5.1-sparkCore","date":"2020-07-31T02:07:21.000Z","updated":"2021-06-22T06:12:17.998Z","comments":true,"path":"2020/07/31/spark/spark-aura-5.1-sparkCore/","link":"","permalink":"http://www.iequa.com/2020/07/31/spark/spark-aura-5.1-sparkCore/","excerpt":"executor 一定属于某个 Application","text":"executor 一定属于某个 Application – – – – application job stage task master worker executor driver 整个 Spark 应用程序的运行分成三个阶段： 1). 编写代码，使用 spark-submit 去交任务到集群运行,一直到我们自己编写的main方法运行为止 1.1) 编写代码 1.2) 打成 Jar 1.3) 编写 spark-submit 脚本提交任务 1.4) 脚本解析 和 执行 最终转到 main 方法执行 SparkSubmit 2). sparkContext 的初始化 new SparkContext(sparkConf) 两条线: (1) 在 driver 端执行的代码 init TaskScheduler ----&gt; TaskSchedulerImpl init SchedulerBackend ----&gt; StandAloneSchedulerBackend init DAGScheduler ----&gt; DAGScheduler (2) 在 worker 和 master 端执行的各种代码 业务功能实现: 1、 master 注册 2、 worker 负责启动 executor 3). action 算子 1. SparkSubmit No. Spark 任务的提交流程 (1) 首先将程序打成 jar 包 (2) 使用 spark-submit 脚本提交任务到集群上运行 (3) 运行 sparkSubmit 的 main 方法，在这个方法中通过反射的方式创建我们编写的主类的实例对象，然后调用 main 方法，开始执行我们的代码（注意，我们的 spark 程序中的 driver就运行在 sparkSubmit 进程中） 2. SparkContext 初始化 No. SparkContext 初始化 (4) 当代码运行到创建 SparkContext 对象时，那就开始初始化 SparkContext 对象了 (5) 在初始化 SparkContext 对象的时候，会创建两个特别重要的对象，分别是： 1. DAGScheduler （RDD-&gt;Stage,—&gt; taskSet） 2. TaskScheduler (SchedulerBackEnd-&gt;Actor) 【DAGScheduler 的作用】将 RDD 的依赖切分成一个一个的 stage，然后将 stage 作为 taskSet 提交给 DriverActor 6 在构建 TaskScheduler 的同时，会创建两个非常重要的对象，分别是: 1. ClientActor： 向 master 注册用户提交的任务 ------------------ 2. DriverActor： 接受 executor 反向注册, 将任务提交给 executor (7) 当 ClientActor 启动后，会将用户提交的任务和相关的参数封装到 ApplicationDescription对象中，然后提交给 master 进行任务的注册 (8) 当 master 接受到 clientActor 提交的任务请求时，会将请求参数进行解析，并封装成 Application，然后将其持久化，然后将其加入到任务队列 waitingApps 中 (9) 当轮到我们提交的任务运行时，就开始调用 schedule()，进行任务资源的调度 (10) master 将调度好的资源封装到 launchExecutor 中发送给指定的 worker (11) worker 接受到 Master 发送来的 launchExecutor 时，会将其解压并封装到 ExecutorRunner中，然后调用这个对象的 start(), 启动 Executor (12) Executor 启动后会向 DriverActor 进行反向注册 (13) driverActor 会发送注册成功的消息给 Executor (14) Executor 接受到 DriverActor 注册成功的消息后会创建一个线程池，用于执行 DriverActor发送过来的 task 任务 (15) 当属于这个任务的所有的 Executor 启动并反向注册成功后，就意味着运行这个任务的环境已经准备好了，driver 会结束 SparkContext 对象的初始化，也就意味着 new SparkContext这句代码运行完成 3. action-job-stage-taskSet-DriverActor-launchTask-Executor-Rdd partition No. Spark 任务的提交流程 (16) 当初始化 sc 成功后，driver 端就会继续运行我们编写的代码，然后开始创建初始的 RDD，然后进行一系列转换操作，当遇到一个 action 算子时，也就意味着触发了一个 job (17) driver 会将这个 job 提交给 DAGScheduler (18) DAGScheduler 将接受到的 job，从最后一个算子向前推导，将 DAG 依据宽依赖划分成一个一个的 stage，然后将 stage 封装成 taskSet，并将 taskSet 中的 task 提交给 DriverActor (19) DriverActor 接受到 DAGScheduler 发送过来的 task，会拿到一个序列化器，对 task 进行序列化，然后将序列化好的 task 封装到 launchTask 中，然后将 launchTask 发送给指定的Executor (20) Executor 接受到了 DriverActor 发送过来的 launchTask 时，会拿到一个反序列化器，对launchTask 进行反序列化，封装到 TaskRunner 中，然后从 Executor 这个线程池中获取一个线程，将反序列化好的 task 中的算子作用在 RDD 对应的分区上 spark1.x RPC AKKA spark2.x RPC netty Spark的任务提交和执行流程详解： 马中华：Spark的任务提交和执行流程详解 Reference Spark任务的提交流程 不急吃口药: spark任务提交流程图 (马中华类) 大数据资料笔记整理","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Task-Commit 流程解析 4.2","slug":"spark/spark-aura-4.2-task-commit","date":"2020-07-29T00:07:21.000Z","updated":"2021-06-22T06:12:18.007Z","comments":true,"path":"2020/07/29/spark/spark-aura-4.2-task-commit/","link":"","permalink":"http://www.iequa.com/2020/07/29/spark/spark-aura-4.2-task-commit/","excerpt":"","text":"sparkcore 的任务执行流程分析： 构建DAG DAGScheduler —&gt; DAG —&gt; TaskSet (Task set) 补充: executor ExecutorBacked actor driver SchedulerBackend actor Spark 核心概念复习 2个重要的知识: new SparkContext (sparkConf) collect(). action算子的提交任务机制，出发任务执行核心 HDFS - File - block - mapreduce line HDFS - RDD - partition - Spark: Application Driver Program ClusterManager SparkContext 整个应用上下文 RDD DAGScheduler TaskScheduler Worker Executor Stage Job Task SparkEnv : 线程级别上下文, 存储运行时重要组件的引用 SparkEnv: MapOutPutTracker … SparkConf 课程结束: 大数据存储 大数据计算 大数据实时增删改查 MapReduce 分布式计算的鼻祖 模型 解决大数据集计算的通用思想 分而治之: 1个Application —&gt; 多个task 临时结果汇总: 多个Task的数据进行最终的汇总处理 zookeeper 议会制 , 投票 , 少数服从多数 艺术来源于生活 kylin, spark, flink ----&gt; mapreduce Spark 任务执行流程详解 现在开始介绍SparkContext，SparkContext的初始化步骤如下： 最重要的三个属性: _dagScheduler ----&gt; a _taskScheduler ----&gt; TaskSchedulerImpl _schedulerBackend ----&gt; StandaloneSchedulerBackend Spark 任务提交流程： Spark 任务提交流程 图2： Reference hustcat/spark_internal.md Spark内核分析之SparkContext初始化源码分析 spark核心概念","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"spark 基础概念复习 3.2","slug":"spark/spark-aura-3.2-spark-basic-summary","date":"2020-07-27T00:07:21.000Z","updated":"2021-06-22T06:12:17.989Z","comments":true,"path":"2020/07/27/spark/spark-aura-3.2-spark-basic-summary/","link":"","permalink":"http://www.iequa.com/2020/07/27/spark/spark-aura-3.2-spark-basic-summary/","excerpt":"","text":"1. 第1部分 任务的提交： (1). run-example SparkPi 100 (2). spark-shell (3). spark-submit --master: 123local local[2] local[*]spark://hadoop02:7077, hadoop04:7077yarn HDFS 处理 myha01 这个 nameservice 的方式非常的暴力: 所有的请求，其实都会给每个 namenode 都发送, 但是只有 active 的 namenode才会进行处理，进行回复 if (namenode.getServiceStage() == “standby”) {} else : … 2. 第2部分 核心功能: SparkContext, 存储体系, 执行引擎 DAGScheduler, 部署模式 扩展功能: SQL, Streaming, GraphX, MLlib, SparkR, Pyspark 核心概念: Application Job 切分标准: 从前往后找action的算子 Stage 切分标准: 从后往前找宽依赖的算子 Task 在spark中，Task的类型分为2种：ShuffleMapTask 和 ResultTask；简单来说，DAG的最后一个阶段会为每个结果的 partition 生成一个 ResultTask, 即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！ 而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中。 Driver Application: 客户端驱动程序, 也可以理解为客户端应用程序，用于将任何程序转换为 RDD 和 DAG, 并与 Cluster Manager 进行通信与调度. ClusterManager Driver Executor Master, Worker, Client deploy-mode 主要针对 yarn: client cluster 基本架构: 编程模型: 1). 获取编程入口 ---&gt; SparkContext 2). 通过编程入口使用不同的方式加载数据得到一个数据抽象 ---&gt; RDD 3). 针对加载得到的数据抽象调用不同的算子进行处理 ---&gt; Transformation + Action 4). 针对结果数据进行处理 RDD/Scala 对象 或 集合 ---&gt; print / save 存储 5). 关闭编程入口 ---&gt; sparkContext.stop() sparkSQL sparkStreaming &amp; sparkCore 一模一样 唯一不相同的地方就是: 编程入口, 数据抽象, 算子 RDD 摘要1： (1) 概念： 弹性分布式数据集， 不可变的， 可分区的分布式集合 (2) 属性： A list of partitions A function for computing on other RDDs A list of dependencies on other RDDs Optionally, a Partition for key-value RDDs (e.g. to say that the RDD is hash-partitioned) Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) RDD 摘要2： (1). 概念 弹性分布式数据集 不可变的，可分区的分布式集合 (2). 五大属性 分区 (A list of partitions) 作用在每个分区之上的一个函数 依赖： 宽依赖 &amp; 窄依赖 KeyValueRDD 分区器 RDD 优先位置列表 (3). 官方， 创建 RDD 的2种方式 12parallelize() makeRDD()parallelizing an existing collextion in your driver program 12textFile() 引用一个外部存储系统 referencing a dataset in an external storage systemHDFS 作用在每个RDD之上的算子: Transformation. RDD —&gt; RDD Action. RDD —&gt; Scala 对象 or 集合 WordCount 各种流程的划分： WorCount 各种流程划分 WordCount DAG 有向无环图： WorCount DAG有向无环图 Reference Spark广播变量和累加器详解 马老师-Spark的WordCount到底产生了多少个RDD 大数据技术之_19_Spark学习_02_Spark Core 应用解析 实例练习 程序员虾说:Spark Transformation算子详解 linsay Offer帮 英语学习包","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Core 中的 RDD 详解 3.1","slug":"spark/spark-aura-3.1-RDD-detail","date":"2020-07-23T00:07:21.000Z","updated":"2021-06-22T06:12:17.995Z","comments":true,"path":"2020/07/23/spark/spark-aura-3.1-RDD-detail/","link":"","permalink":"http://www.iequa.com/2020/07/23/spark/spark-aura-3.1-RDD-detail/","excerpt":"","text":"1. 广播变量 线程可以共享变量的思路 广播变量： (1) 在默认情况下，每一个task都会维持一个全局变量的副本 有一个集合：100M 在 driver 中生成， 但是在所有的task中都需要使用 那么，每一个 task 都会维持一个当前这 100M 数据的副本 如果一个 executor 中启动了 6 个 task，最终消耗 600M 内存 (2). 如果使用广播变量的话 那么可以把当前这个100M的数据，就编程一个广播变量的值 用 driver 中的 sparkcontext 进行 全局所有 executor 广播 最后的效果：每个 executor 中只存在一份这个广播变量的副本 而不是原先的每一个task都保持一个副本 所以最终的内存消耗量： 100m (3) 最后的效果： 减少了网络数据传输的量 减少了executor的内存使用 如果一个值很小，那么几乎没有广播的必要。 广播的值的大小越大，效果越明显 2. 累加器 1val a = sc.accumulator(0) 还原一个累加器 1val b = a.value spark 的累加器 和 mapreduce编程模型的全局计数器是一个道理。 3. DAG规划和基础理论 切分 Stage 是 从后往前找 shuffer类型/宽依赖的算子，遇到一个就断开，形成一个 stage 最后一个 stage： ResultStage 除此之外的stage spark中如何划分stage 因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄赖就将这个RDD加入该stage中。 在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask；简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！ 而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中。 切分stage： 从后往前找 shuffle类型/宽依赖 的算子, 遇到一个就断开, 形成一个 stage 最后一个stage: ResultStage ------&gt; ResultTask 除此之外的stage：ShffleMapStage ------&gt; ShffleTask 每一个 stage 都会切成多个同种类型的 Task 每一个 Stage 中的有可能包含多个不同的 RDD 那么一个 Stage 又有可能会划分多个 task 执行 每个 RDD 又可以指定不同的分区数 默认情况下：每一个分区，就会是一个 Task 那么现在，如果遇到了一个 stage 中有多个不同分区数的RDD， 那么请问：到底这个stage中有多少个Task执行呢？ 5 4 3 -----&gt; 3个task 以最后一个RDD的分区数来决定 切分job： 从前往后找action算子, 找到一个就形成一个 job. 3 + 2 = 5 tasks DAG 的生成 checkpoint linage 检查点 血脉 血统 容错 对于Spark任务中的宽窄依赖，我们只喜欢窄依赖 DAGScheduler： spark-submit 提交任务 初始化 DAGScheduler 和 TaskScheduler 接收到 application 之后，DAGScheduler 会首先把 application 抽象成一个 DAG DAGScheduler 对这个 DAG (DAG中的一个Job) 进行 stage 的切分 把每一个 stage 提交给 TaskScheduler rdd1.collect client 提交任务的任务节点 如果是client模式，那么 driver程序就在 client 节点 如果模式是 cluster, driver 程序在 worker 中. rdd20.countByKey() countByKey 是作用在 key-value 类型上的一个 action 算法 countByValue 一般是用来统计普通类型的RDD map reduce recudeByKey filter, json 难点： aggregate aggregate count sum max min distinct avg 100G ----&gt; 1G 20G -----&gt; 30G map mapPartitions join mapjoin reducejoin cogroup coalesce repartition repartitionAndSortWithinPartitions 重新分区, 并且分区内数据进行排序 Spark Transformations 6. RDD 持久化操作 cache, persist cache: 正常情况下： 一个RDD中是不包含真实数据的，只包含描述这个RDD的源数据信息 如果对这个RDD调用 cache 方法 那么这个rdd中的数据，现在依然还是没有真实数据 直到第一次调用一个action的算子出发了这个RDD的数据生成，那么cache 操作 cache() persist() == persist(StorageLevel.MEMORY_ONLY) persist(StorageLevel.XXXX) 12345def cache(): this.type = persist()def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)persist() ==== persist(StorageLevel.MEMORY_ONLY) 一个普通的文件 file ===》 内存 该 file 被序列化了 ===》 内存 JVM 最大的区域是 Head 内存， OffHeap 堆外内存 7. union, join, subtract, cartesian 8. 各种 byKey 操作 (重要) ----union, join, coGroup, subtract, cartesian---- groupByKey reduceByKey aggregateByKey sortByKey combineByKey 8.1 groupByKey 1val data = List((&quot;math&quot;, 89), (&quot;hadoop&quot;, 100), (&quot;math&quot;, 10), (&quot;english&quot;, 89), (&quot;math&quot;, 1000)) groupByKey把相同的key的数据分组到一个集合序列当中： 12345678910111213[ (&quot;spark&quot;,1), (&quot;hive&quot;,1), (&quot;spark&quot;,1), (&quot;hadoop&quot;,1), (&quot;hive&quot;,1)] --&gt; [ (&quot;spark&quot;,(1,1)), (&quot;hive&quot;,(1,1)), (&quot;hadoop&quot;,(1))] Spark函数讲解：aggregateByKey 过往记忆 Spark的groupByKey、reduceByKey、sortByKey算子案例 9. RDD mapPartitions, mapPartitionsWithIndex mapPartitions 每次遍历一个分区 (最小单位是分区) 10. map, flatMap, filter 11. spark 基础概念复习 11.1 第1部分 第一天spark内容的部分残留 WordCountJava7 WordCountJava8 任务的提交： (1). run-example SparkPi 100 (2). spark-shell (3). spark-submit –master: local local[2] local\\[\\*\\] spark://hadoop02:7077, hadoop04:7077 yarn HDFS 处理 myha01 这个 nameservice 的方式非常的暴力: 所有的请求，其实都会给每个 namenode 都发送, 但是只有 active 的 namenode才会进行处理，进行回复 if (namenode.getServiceStage() == &quot;standby&quot;) &#123;&#125; else : ... 11.1 第2部分 核心功能: SparkContext, 存储体系， 执行引擎 DAGScheduler, 部署魔术 扩展功能: SQL, Streaming, GraphX, MLlib, SparkR, Pyspark 核心概念: Application Job 切分标准: 从前往后找action的算子 Stage 切分标准: 从后往前找宽依赖的算子 Task Driver Application: 客户端驱动程序, 也可以理解为客户端应用程序，用于将任何程序转换为 RDD 和 DAG, 并与 Cluster Manager 进行通信与调度. ClusterManager Driver Executor Master, Worker, Client deploy-mode 主要针对 yarn: client cluster 基本架构: 编程模型: 1). 获取编程入口 ---&gt; SparkContext 2). 通过编程入口使用不同的方式加载数据得到一个数据抽象 ---&gt; RDD 3). 针对加载得到的数据抽象调用不同的算子进行处理 ---&gt; Transformation + Action 4). 针对结果数据进行处理 RDD/Scala 对象 或 集合 ---&gt; print / save 存储 5). 关闭编程入口 ---&gt; sparkContext.stop() sparkSQL sparkStreaming &amp; sparkCore 一模一样 唯一不相同的地方就是: 编程入口, 数据抽象, 算子 RDD： (1) 概念： 弹性分布式数据集， 不可变的， 可分区的分布式集合 (2) 属性： A list of partitions A function for computing on other RDDs A list of dependencies on other RDDs Optionally, a Partition for key-value RDDs (e.g. to say that the RDD is hash-partitioned) Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) 12. RDD 算子 sample, takeSample Reference Spark广播变量和累加器详解 马老师-Spark的WordCount到底产生了多少个RDD 大数据技术之_19_Spark学习_02_Spark Core 应用解析+ RDD 概念 + RDD 编程 + 键值对 RDD + 数据读取与保存主要方式 + RDD 编程进阶 + Spark Core 实例练习 Offer帮 英语学习包","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark 核心概念详述 2.2","slug":"spark/spark-aura-2.2-core-introduce","date":"2020-07-21T00:07:21.000Z","updated":"2021-06-22T06:12:18.010Z","comments":true,"path":"2020/07/21/spark/spark-aura-2.2-core-introduce/","link":"","permalink":"http://www.iequa.com/2020/07/21/spark/spark-aura-2.2-core-introduce/","excerpt":"Spark 核心概念详述 2.2","text":"Spark 核心概念详述 2.2 1. Word Count Spark 核心概念详述 2.2 data source -&gt; LineRDD -&gt; WordRDD -&gt; WordAndOneRDD -&gt; WordCountRDD -&gt; 目的地 整个 SPark 程序中的 1个Application --&gt; 1个Job --&gt; Spark 核心概念详述 2.2 Spark 核心概念详述 2.2 Spark 核心概念详述 2.2 Spark 核心概念详述 2.2 Spark 核心概念详述 2.2 storm — 细粒度 – 一条数据处理一次 sparkStreaming – 粗粒度 – 一小段时间内的所有数据处理一次 加入 1s 中 就一条数据 加入 1s 中 就一万条数据. Spark Core + MapReduce spark: master(resourcemanager) worker(nodemanager) YARN: – master yarn –deploy-mode client/cluster 上传和下载数据的流程： 7 SparkSubmit： 10步 SparkCore 任务运行流程： 20步 RDD 三句话 linage 血脉关系 一个线程一个任务 一个 Executor 会执行多个 Task Reference","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Hive Optimize 25 Items","slug":"hadoop/hive-optimization-25-cases","date":"2020-07-18T00:07:21.000Z","updated":"2021-06-22T06:38:06.698Z","comments":true,"path":"2020/07/18/hadoop/hive-optimization-25-cases/","link":"","permalink":"http://www.iequa.com/2020/07/18/hadoop/hive-optimization-25-cases/","excerpt":"","text":"调优: Hive底层执行引擎深度剖析 25条Hive性能调优实战 深度剖析Hive架构设计与工作原理 4.1 Hive 的概念 Hive依赖HDFS数据，Hive将HQL转成MapReduce执行，所以说Hive是基于Hadoop的一个数据仓库工具 实质就是一款基于HDFS的MapReduce计算框架，对存储在HDFS中的数据进行分析和管理。 4.2 Hive 的工作机制 简单总结: 1、Hive的内置四大组件(Driver, Compiler, Optimizer, Executor)完成HQL到MapReduce的转换 2、在Hive执行HQL编译过程中，会从元数据库获取表结构和数据存储目录等相关信息 3、Hive只是完成对存储在HDFS上的结构化数据的管理，并提供一种类SQL的操作方式来进行海量数据运行， 底层支持多种分布式计算引擎。 最全25条选性能调优全详解 5.1 调优概述 Hive 作为大数据领域常用的数据仓库组件，在平时设计和查询时要特别注意效率。 影响 Hive 效率的几乎从不是数据量过大，而是数据倾斜、冗余、Job或I/O过多、MapReduce 分配不合理等。 对 Hive 的调优既包含 Hive 的建表设计方面，对HiveHQL 语句本身的优化，也包含 Hive 配置参数和底层引擎 MapReduce方面的调整。 所以此次调优主要分为以下四个方面展开: 1、Hive的建表设计层面 2、HQL语法和运行参数层面 3、Hive架构层面 4、Hive数据倾斜 总之，Hive调优的作用： 在保证业务结果不变的前提下，降低资源的使用量，减少任务的执行时间。 5.2 调优须知 5.3 Hive建表设计层面 5.3.1 利用分区表优化 5.3.2 利用分桶表优化 跟分区的概念很相似, 都是把数据分成多个不同类别, 区别就是规则不一样 ! 1、分区： 按照字段值来进行: 一个分区, 就只是包含这个这一个值的所有记录 Hive Bucket, 分桶, 是指将数据以指定列的值为key进行hash, hash 到指定数据的桶中. 5.3.3 合适的文件存储格式 data warehouse ods: TextFile dw: ORC or ParquetFile 5.3.4 合适的压缩格式 5.4 HQL 语法和运行参数层面 5.4.1 查看Hive执行计划 5.4.2 列裁剪 5.4.3 谓词下推 5.4.4 分区裁剪 5.4.5 合并小文件 5.4.6 合理设置MapTask并行度 5.4.7 合理设置ReduceTask并行度 Reference Spark性能优化指南——基础篇 Spark性能优化指南——高级篇 大数据资料笔记整理","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://www.iequa.com/tags/hive/"}]},{"title":"Python 星号表达式(*) 用法详解","slug":"python/language/py-language-15-star","date":"2020-07-17T02:00:21.000Z","updated":"2021-06-22T06:52:37.039Z","comments":true,"path":"2020/07/17/python/language/py-language-15-star/","link":"","permalink":"http://www.iequa.com/2020/07/17/python/language/py-language-15-star/","excerpt":"STAR","text":"STAR 1. 函数可变参数标志以及参数解包 2. 赋值语句中作为可变变量标志 3. 对可迭代对象进行解包 4. 元组/集合/列表 以元组为例，集合与列表同理。 元组也可以比较大小，例如下面代码： 12345(1, 5) &lt; (2, 3) # True(2, 8) &lt; (2, 6) # False(1, 2) &lt; (1, 2) # False(1, 1, -1) &lt; (1, 2) # True(1, 2, -1) &lt; (1, 2) # False Reference python函数参数前面单星号（*）和双星号（**）的区别 Python星号表达式(*)用法详解 元组/集合/列表 比较大小","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Hadoop Tutorial 3 - Hive","slug":"hadoop/hadoop-hive-1","date":"2020-07-15T23:07:21.000Z","updated":"2021-06-22T06:35:52.145Z","comments":true,"path":"2020/07/16/hadoop/hadoop-hive-1/","link":"","permalink":"http://www.iequa.com/2020/07/16/hadoop/hadoop-hive-1/","excerpt":"","text":"分布式文件系统HDFS: 核心原理与操作 Hadoop HDFS Hadoop HDFS Reference 大数据开发系列直播课 ③","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://www.iequa.com/tags/Hive/"}]},{"title":"Hadoop Review 2 - MapReduce","slug":"hadoop/hadoop-mr-review-1","date":"2020-07-15T15:07:21.000Z","updated":"2021-06-22T06:34:57.642Z","comments":true,"path":"2020/07/15/hadoop/hadoop-mr-review-1/","link":"","permalink":"http://www.iequa.com/2020/07/15/hadoop/hadoop-mr-review-1/","excerpt":"Hadoop MapReduce","text":"Hadoop MapReduce 1. 什么是大数据？ 核心问题是什么？ 举个🌰: 商品的推荐 (问题1) 大量的订单数据如何存储？ (问题1) 大量的订单数据如何计算？ 大数据的核心问题 (技术上)： 存储 (HDFS) 计算 (离线 + 实时) 离线计算 与 实时计算 2.1 离线计算 - 批处理 (MapReduce, Spark Core, Flink DataSet API) 2.2 实时计算 (Spark Streaming、Flink DataStream API) MapReduce 核心思想： 先拆分，在合并 2. MapReduce 编程模式 Hadoop MapReduce 数据的处理流程： WordCount程序为例 Hadoop MapReduce 3. MapReduce 编程实战 WordCountMapper WordCountReducer WordCountMain Hadoop MapReduce Hadoop MapReduce WordCountMain 4. 分布式计算模型 MapReduce 计算模型的来源： PageRank 问题 启动 Hadoop &amp; Yarn： Hadoop & Yarn align=center 执行 MapReduce： Hadoop MapReduce 执行结果： Hadoop MapReduce 5. 其他的一些知识: Hadoop MapReduce 6. MapReduce 在 Hadoop 中的位置 Hadoop MapReduce Reference 大数据开发系列直播课 ③","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"http://www.iequa.com/tags/MapReduce/"}]},{"title":"Hadoop Tutorial - HDFS","slug":"hadoop/HDFS-Introduce","date":"2020-07-15T12:07:21.000Z","updated":"2021-06-22T06:36:10.209Z","comments":true,"path":"2020/07/15/hadoop/HDFS-Introduce/","link":"","permalink":"http://www.iequa.com/2020/07/15/hadoop/HDFS-Introduce/","excerpt":"","text":"分布式文件系统HDFS: 核心原理与操作 Hadoop HDFS Hadoop HDFS 如何学习大数据? 思想、架构、原理 (非常重要) 搭建环境 (建议: Apache版本) 1. 什么是大数据 Volume, Velocity, Variety, Value, Veracity 商品推荐： Q1： 大量的订单如何存储 ? Q2： 大量的订单如何计算 ? 大数据的核心问题是? 数据的存储 数据的计算 2. 分布式文件系统 分布式文件系统的核心原理 Q1： 硬盘不够大 多几个硬盘 Q2： 硬盘不够安全 多存几份 HDFS的默认的 数据库冗余度： 3 主从架构 HDFS、Yarn、Hbase、Storm、Spark、Flink 都是主从架构 存在的问题： 单点故障 Zookeeper: HA (Hadoop的HA实现架构) … 123456➜ hdfs dfsadmin -report➜ jps43501 DataNode43502 NameNode43503 SecondaryNameNode 3. 操作 HDFS 命令行 Web Console： 端口 9870 Java程序 使用 Java API 上传数据到HDFS 4. HDFS 进阶 回收站 安全模式 配额 权限管理 快照 Reference 大数据开发系列直播课 ③","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"http://www.iequa.com/tags/hdfs/"}]},{"title":"Spark Intrduce 2 - RDD","slug":"spark/spark-review-2","date":"2020-07-07T12:07:21.000Z","updated":"2021-06-20T04:12:28.348Z","comments":true,"path":"2020/07/07/spark/spark-review-2/","link":"","permalink":"http://www.iequa.com/2020/07/07/spark/spark-review-2/","excerpt":"Spark RDD Feature","text":"Spark RDD Feature 1. Spark 是什么? Spark SQL, Oracle, Spark Oracle == 汽车 hadoop == 飞机 SQL == 驾照 Spark 让SQL并行运行 == SQL加速器 Spark 是计算引擎 2. Spark 执行过程 Spark HiveOnSpark vs SparkSQL Store MR HiveOnSpark, Hive SQL 可以用 Spark 执行引擎, 但是这样调优起来很麻烦 SparkSQL 比 Hive on Spark 好一点. 3. Spark 的起源 Spark 4. Spark 的架构图 Spark 2.x python &amp; Java 性能 1:1 Spark Spark DataFrame 与 pandas Dataframe 没关系，但是可以互相转换 to_pandas to_spark DataSets : for Java Dataframe : for Python SQL : SQL 条条大路通罗马 Spark 适合大规模机器学习，不适合深度学习(tensorflow适合深度学习)，这样理解对么？ 5. RDD 的五个特性 Spark 6. Spark Api 如何使用 12341. PYSPARK_PYTHON2. JAVA_HOME3. SPARK_HOME4. PYLIB (os.environ[&quot;PYLIB&quot;]=os.environ[&quot;SPARK_HOME&quot;] + &quot;/python/lib&quot;) PYLIB maybe not setting export PYSPARK_PYTHON=/Users/blair/.pyenv/versions/anaconda3/envs/spark/bin/python3 export PYSPARK_DRIVER_PYTHON=“jupyter” export PYSPARK_DRIVER_PYTHON_OPTS=“notebook” 1234567以下是 Mars 的设置 （我们直接用pyspark, 则不用直接这样显示在程序中指定）：os.environ[&quot;PYSPARK_PYTHON&quot;]=&quot;C:/Users/netease/Anaconda3/python.exe&quot;os.environ[&quot;JAVA_HOME&quot;]=&quot;C:/Program Files/Java/jdk1.8.0_144&quot;os.environ[&quot;SPARK_HOME&quot;]=&quot;c:/spark&quot;os.environ[&quot;PYLIB&quot;]=os.environ[&quot;SPARK_HOME&quot;] + &quot;/python/lib&quot;sys.path.insert(0,os.environ[&quot;PYLIB&quot;] + &quot;/py4j-0.10.7-src.zip&quot; )sys.path.insert(0,os.environ[&quot;PYLIB&quot;] + &quot;/pyspark.zip&quot; ) 7. 利用 Pyspark 完成 WordCount 1234567891011121314from pyspark import SparkConf , SparkContextprint(sc.version)lines = sc.textFile(&quot;/Users/blair/ghome/github/spark3.0/pyspark/spark-src/word_count.text&quot;, 2)lines.take(3)words= lines.flatMap(lambda x : x.split(&quot; &quot;))words.take(3)wordCounts = words.countByValue()for word,count in wordCounts.items(): print(&quot;&#123;&#125; : &#123;&#125;&quot;.format(word,count)) 如果有3个节点, 则下列方法可以查看，数据分布在不同的 partition 1234def indexedFunc(parindex,pariter): return [&quot;partition : &#123;&#125; =&gt; &#123;&#125;&quot;.format(parindex,x) for x in pariter] words.mapPartitionsWithIndex(indexedFunc).collect() [‘partition : 0 =&gt; The’, ‘partition : 0 =&gt; history’, ‘partition : 0 =&gt; of’, ‘partition : 0 =&gt; New’, ‘partition : 0 =&gt; York’, ‘partition : 0 =&gt; begins’, ‘partition : 0 =&gt; around’, ‘partition : 0 =&gt; 10,000’, 查看 spark 运行状态： http://localhost:8080/ Spark 查看 spark Jobs 状态：http://localhost:4040/jobs/ Spark 知识摘要: 如果用 Hadoop Mapreduce 来完成, 则代码写起来麻烦并繁多. Spark 是 内存版的 Mapreduce, Mapreduce 可以说是所有分布式计算的鼻祖. Spark 需要做性能调优的时候，还得再看 Mapreduce 的知识来修复. 8. Spark 执行图 Spark Exec Reference 大数据入门与实战-PySpark的使用教程 Python - lru_cache和singledispatch装饰器 大数据开发系列直播课 ③ PySpark-数据操作-DataFrame 株式会社XG JAPAN 日本投资 | 过来人告诉你：日本创业移民的费用和坑","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"机考模拟解析 9.2 (Listening & Writing)","slug":"English/IELTS/KMF-Machine-Test-9.2","date":"2020-07-05T08:14:48.000Z","updated":"2021-06-22T06:33:48.647Z","comments":true,"path":"2020/07/05/English/IELTS/KMF-Machine-Test-9.2/","link":"","permalink":"http://www.iequa.com/2020/07/05/English/IELTS/KMF-Machine-Test-9.2/","excerpt":"Are you ready?","text":"Are you ready? 1. Listening 错题题号 考点 具体总结 惩罚编数 9-1-6 messy ['mesi] 不认识 messy room 10 9-1-10 ladder ['lædər] 不认识 10 9-4-2 woodland/woods ['wʊdlənd] 不认识 9-4-4 grey/gray 听不出 10 9-4-9 first year 答案: year ~ Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? 2. Writing Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready?","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"机考模拟解析 9.1 (Speaking & Reading)","slug":"English/IELTS/KMF-Machine-Test-9.1","date":"2020-07-05T00:13:48.000Z","updated":"2021-06-22T06:33:48.697Z","comments":true,"path":"2020/07/05/English/IELTS/KMF-Machine-Test-9.1/","link":"","permalink":"http://www.iequa.com/2020/07/05/English/IELTS/KMF-Machine-Test-9.1/","excerpt":"Are you ready?","text":"Are you ready? 1. Place for Speaking Are you ready? Are you ready? Are you ready? 2. Listening 错题题号 考点 具体总结 惩罚编数 9-1-6 messy ['mesi] 不认识 messy room 10 9-1-10 ladder ['lædər] 不认识 10 9-4-2 woodland/woods ['wʊdlənd] 不认识 9-4-4 grey/gray 听不出 10 9-4-9 first year 答案: year ~ 3. Reading 简单提醒先做 Are you ready? Are you ready? Are you ready? 李一老师==李老师 李一老师 == 李老师 李一老师==李老师 李一老师==李老师 李一老师==李老师 李一老师==李老师 屠龙12字: 抓名词, 去修饰, 排除法, 定动词 李一老师==李老师","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"JVM 运行时","slug":"java/java-jvm-p1","date":"2020-05-30T08:54:16.000Z","updated":"2021-06-22T06:38:22.569Z","comments":true,"path":"2020/05/30/java/java-jvm-p1/","link":"","permalink":"http://www.iequa.com/2020/05/30/java/java-jvm-p1/","excerpt":"","text":"1. JVM 的组织结构 jvm 的三个核心： jvm 类加载机制 jvm 运行时数据区 jvm 垃圾回收 2. JVM 各区域作用 heap Method Area Program Counter Register JVM Stacks Native Method Stacks 异常总结 3. JVM 垃圾回收 概述 Garbage Collector Reference JVM内存结构 JVM物理结构和在内存中的组织结构 深入理解JVM-内存模型（jmm）和GC","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://www.iequa.com/tags/jvm/"}]},{"title":"Writing - 丛昊 & 杜仕明","slug":"English/IELTS/du-writing-1","date":"2020-04-15T23:14:48.000Z","updated":"2021-06-22T06:33:48.675Z","comments":true,"path":"2020/04/16/English/IELTS/du-writing-1/","link":"","permalink":"http://www.iequa.com/2020/04/16/English/IELTS/du-writing-1/","excerpt":"1. topic 2. hand 3. other hand 4. conclusion","text":"1. topic 2. hand 3. other hand 4. conclusion 0 预习课 相邻2句话, 基本句型不重复 many people will become jobless. Cycling has been losing popularity. There has been a decrease in the popularity of cycling. 0.2 class 20:00 及格分数 6.5， CC 从 5to7 的飞跃 努力 + 一点点的运气 = 7+ 基础句型、修饰名词、状语从句 写作中必须用到的从句 状语从句 原因状语从句 最少3种从句，5个句子以上 1. 议论文核心段的扩展 一定是以解释为主，举例仅仅是锦上添花 to be subsefic 一定要越写越详细 1.1 Topic Sentence 的原则 不能随心所欲，与题目紧密相关，一般来自原题或个人观点 一定要死扣题目, 一定有扩展能力 不要选择自杀型 Topic Sentence 你要让东西成为你的 给出一个养宠物的好处： 扣题： 养宠物可以让我们健康 Reason： 因为宠物需要很多活动，主人必须陪着，避免久坐不动，避免肥胖 一个状语从句，会让你一石二鸟 video: 01:30:00 2. 议论文题型分类大全 Discuss both views and give your opinion. IELTS 90% 都是后置定语 短语词伙搭配，很漂亮，这个很给力 可以先构建中文，在转换为 IELTS 写议论文要考虑逻辑链条。 future generations, sustainable development 2种观点：偏向强的观点，放在后面，和结尾扣题更接近. 偏向的范文要重点看背. 2种共存的观点是比较少的，但是也有. 3. 议论文题型分类大全 原因 + 解决方案 最少4段， 最多5段 最简单的是介绍段：改写原题背景 + 预告 (笼统/详细) 原因分析的段落应该以 大部分人承认的事实为TS 如果遇到解决方案的问题， 必须提出切实可行的方案 注意 cause 这个词的不同意思 可能的原因TS： 互联网 - 物价上涨 - 科技发展 - 广告影响 - 出生率下降 - 女性地位提高 - 全球化 continusly rising living cost 01:23 losing popularity 4. 议论文题型分类大全 逗号，后面只能是 which go bankrupt road rage 认真读题， 理清逻辑关系，在写 5. 小作文 a significant increase could be seen in the proportion of the elderly. 对比 对比 + 变化 (变化容易说) 先总结，在拆开","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"Reading - Jack","slug":"English/IELTS/jack-reading-1","date":"2020-04-11T09:14:48.000Z","updated":"2021-06-22T06:33:48.626Z","comments":true,"path":"2020/04/11/English/IELTS/jack-reading-1/","link":"","permalink":"http://www.iequa.com/2020/04/11/English/IELTS/jack-reading-1/","excerpt":"2020.2.9jack阅读技巧班","text":"2020.2.9jack阅读技巧班 1. class 第一篇阅读最简单，第2或3最难 考点词： 能够概括归纳总结文章题干信息的 work 或 phrase 或 short sentence 考点词： 一般为可数名词的复数， 没有求其次，实意的名词或者动词 LOH 是非常简单的提醒，把握主旨就可以. NB1: 首句原则，文章某段第一句和某个heading高度一致 直接选 consists of NB2： 一般来说 85% 原词重现都是坑 不能犯这个错误 题干中是2个信息，文章中有A没B，有B没A 都不能选。 有A有B 但是AB不是两口子也不能选 Heading 是段落大意 NB3: 特殊词原则 直接选 NB4： 某一个词在一个段落中反复出现，并且仅仅在一个段落中出现 这个词或者这个词的替换词，一定会出现 Heading 里面. 作业： 812， 822， 823， 833， 841 笔记 例题 笔记放在旁边，做题 2. class 做题的功能是熟悉方法 A &amp; B 不是2口子，就是 paragraph 中的逻辑关系 与 Heading 中的逻辑关系 不一致 排除法也很重要，other item 都不对. have yet 还没 date 今天 定位词： 大写和数字 上下三行的完整句子 -【断子绝孙题】往往是 LOH 题目答案的错误. (NB2) LOH 的答案也可以是 【断子绝孙题】 的答案 (最难) 【断子绝孙题】Answer: 细节 or Heading 重复的一定有NB，没用 断子绝孙题】 7.5 分下，就舍弃， 10个paragraph 可能用6个，有就选没有就算了. LOH 全部paragraph都使用 一般题型：为非 LOH 之外的题 3. class 2020.2.9jack阅读技巧班 如果考点词，没有名次复数，则退而求其次选实意的动词和名词 预测， 如果选，那么这段话一定会讲到**? 考点词** 做 LOH 步骤： 考点词 预测 reading 文章 雅思会有很多难理解的文章，但是不影响你做题 当我们题材特别难的时候，题型往往比较人性化基础，接地气 2020.2.9jack阅读技巧班 当读到一段落，很难的时候，难以选择，可以继续读第一个段 A &amp; B 不是两口子的，难题： 2020.2.9jack阅读技巧班 C paragraph == ii B paragraph 空着，C paragraph的答案 NB1 ： E &lt;=&gt; i B paragraph 空着，C paragraph的答案 蓝色箭头，表示 有A，没有B，所以不选该项 蓝色箭头，表示 有A，没有B，所以不选该项 3.2 841 例题 这是 Section， 不是 paragraph，所以 NB1 方法是失效的. reason 确认 + 排除 4. 配对题 (2.12jack第三节) 定位词：大写和数字 短matching的定位词的第一次出现，一定是有序的 上下三行，包含三行内的完整句子 tip of the iceberg 5. 判断题 能选 T 也能学 F，那么其实就是选 NG 能选 T 也能学 F，那么其实就是选 NG 01：34","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"Transformer to BERT (by Amazon)","slug":"nlp/BERT_from_Transformer","date":"2020-04-05T03:00:21.000Z","updated":"2021-06-22T06:47:48.434Z","comments":true,"path":"2020/04/05/nlp/BERT_from_Transformer/","link":"","permalink":"http://www.iequa.com/2020/04/05/nlp/BERT_from_Transformer/","excerpt":"BERT tutorial","text":"BERT tutorial Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT的全称是: Bidirectional Encoder Representation from Transformers 1. Transformer to BERT BERT tutorial 1.1 ELMO ELMO 全称： Embeddings from Language Models BERT tutorial BERT tutorial 1.2 Transformer BERT tutorial BERT tutorial BERT tutorial BERT tutorial 1.3 Bert BERT tutorial BERT tutorial BERT tutorial E_A 代表这个 Token 属于 SentenceA 还是 Sentence B 1.4 Pre-training Bert BERT tutorial BERT tutorial BERT tutorial BERT tutorial BERT tutorial BERT tutorial BERT tutorial BERT tutorial 阅读理解是QA加难的版本 3. Recap BERT tutorial 每个word都是这句话的所有信息组成的 Bert Training 40+ times, Fine-tune 2~4 times every token: 12 * 768， 12 层的 Transformer. Bert 主要的缺陷就是太大了. Reference 一步步理解BERT 从语言模型到Seq2Seq：Transformer如戏，全靠Mask BERT完全指南 可视化Bert","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://www.iequa.com/tags/BERT/"}]},{"title":"Studying in New Zealand","slug":"world/studying_in_newzealand","date":"2020-04-04T01:31:48.000Z","updated":"2021-06-22T06:07:42.549Z","comments":true,"path":"2020/04/04/world/studying_in_newzealand/","link":"","permalink":"http://www.iequa.com/2020/04/04/world/studying_in_newzealand/","excerpt":"Auckland New Zealand","text":"Auckland New Zealand NZ 8 University 8 University in New Zealand 1. university of anckland www.auckland.ac.nz New Zealand is the most famous university in the world. The school is located in the center of Auckland, the largest city in New Zealand, with convenient life and colorful after-school activities. Postgraduate English language requirements Pearson Test of English (PTE) Academic* Overall score of 58 and no PTE Communicative score below 50 IELTS (Academic) Overall score of 6.5 and no bands below 6.0 Postgraduate fees for international students NZ$42,814 Postgraduate Diploma in Science Full-time: 1 year, 2021 Semester One – 15 February Masters degree entry requirements, Master of Science PGD -&gt; Master 1years full-time, Bachelor -&gt; Master 2years full-time 研究生课程申请截止日期 2. university of otago www.otago.ac.nz Postgraduate Diploma in Science (PGDipSci) Computer Science NZ$34,755 S1, a one-year programme which builds on an undergraduate science degree. Postgraduate English language requirements IELTS Score of 6.5 in the academic module (with no individual band below 6.0) Science Qualifications Master of Science (MSc) Computer Science NZ$34,755 S1, at least two years of full-time study Additional tests accepted for 2020 and 2021 intakes only For applicants who are unable to access the regular English language proficiency tests we accept, the University of Otago is currently accepting these additional tests for semester 2 2020, semester 1 and semester 2 2021. Duolingo English Test (DET) with the following scores: DET score of 100 for undergraduate entry DET score of 110 for postgraduate entry IELTS Indicator test This is an online version of the regular IELTS test, made available due to COVID-19 testing centre restrictions. TOEFL iBT Special Home Edition This is an online version of the regular TOEFL iBT test, made available due to COVID-19 testing centre restrictions. 3. university of canterbury www.canterbury.ac.nz Postgraduate: 2020 60 Special (Set) Programme Fee $19,200 Postgraduate Certificate in Science IELTS Academic avg score of 6.5, a minimum of 6.0 in reading, writing, listening and speaking How do I plan my certificate? The Postgraduate Certificate in Science includes 60 points of courses, with at least 45 points towards a single subject. The Certificate can be completed in 6 months full-time, or up to 2 years part-time. Master: 2years, 2020 Varies Special (Set) Programme Fee $38,400 per 120 points 4. auckland university of technology www.aut.ac.nz Postgraduate Diploma in Computer and Information Sciences Duration: 1 year full-time / part-time available, Level:8, Points:120 Starts: 13 Jul 2020, 22 Feb 2021, 12 Jul 2021, International $36,962 IELTS (Academic) 6.5 overall with all bands 6.0 or higher; or equivalent English language requirements COVID-19 situation: additional English tests accepted in 2020 We understand that completing your English language test in person is difficult during the COVID-19 pandemic. If you’re applying this year to study at AUT in 2020 or 2021, you can now also use the following online tests: IELTS Indicator TOEFL Special At Home Edition You can’t use an online English test for programmes that require IELTS results because of professional or accreditation requirements. Reference 新西兰留学扫盲: GD、PGD和Master的区别 ! 新西兰留学移民最靠谱中介推荐 | 新西兰签证移民事务所 | 史上最好沟通的留学中介 | 真诚黑米强烈推荐 我的新西兰留学经历分享 | 普天同庆成功申请到master啦 | 细说国内留学中介的坑 | 新西兰本地中介靠谱吗 小矮人Ellen 出國留學該找代辦嗎？代辦不為人知的秘密! 知乎： 奥克兰大学读硕士一年多少钱? 在新西兰当程序员是一种怎样的体验？ Jack Liu博客 — 关注技术移民政策动向，探讨华人移民热点问题 夏勇兴写一辈子代码的创业者","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"NewZealand","slug":"NewZealand","permalink":"http://www.iequa.com/tags/NewZealand/"}]},{"title":"Speaking 1 - 杨帅口语","slug":"English/IELTS/yang-speaking-1","date":"2020-03-29T06:14:48.000Z","updated":"2021-06-22T06:33:48.651Z","comments":true,"path":"2020/03/29/English/IELTS/yang-speaking-1/","link":"","permalink":"http://www.iequa.com/2020/03/29/English/IELTS/yang-speaking-1/","excerpt":"2020.3.19杨帅口语高分班","text":"2020.3.19杨帅口语高分班 雅思口语什么都考. Part3 部分什么热点社会问题都可能被讨论. 口语仅仅是和老师沟通，要用于表达自己的想法, 要敢讲就是沟通. 不知道就是不知道,答案不重要, 重要的是沟通 不要一直想内容, 要想英文如何讲. filler words and sentences 一定要完全背熟，并且逼迫自己成为习惯 I don’t know much about this, but Let me think about it. 减少思考内容的时间，我就更加琉流利 Part2 &amp; Part1 all is your personal life part1: 我们聊聊你上次什么时候收的 gift ? part2: 描述下你收到过的 gift. part2 = part1 * 4 如果你实在说不到2minutes, 你可以臭不要脸的再说另外一个. 大逻辑-布局谋篇 小逻辑-单点扩展 从上到下，逐点回答，单点扩展，不要串点 Part3 Astract concept 99句 背诵 时间，地点，人物，事件 2. Class 连贯性，逻辑 地点结尾的，说从句，有时候是 where，有时候是 which commercial 3. Class 要说具体的未来，不要说得虚无缥缈 Part1 and P2 录音回答，拿手机打开录音APP, 录一遍 Part2 做笔记，非常重要 Part3 Very Important 3.1 one mintues notes 白板横着用 一个点一行，应该不会写超. 大逻辑布局谋篇 从上到下, 单点扩展 Part2 时候可以永远看我的板子，来 Speaking， 不看考官。 也是可以的. make notes very very important ! often practice. 3.2 part 1 -&gt; part3 谈论大众、社会、抽象内容 No. part1 part3 1. 喜欢吃巧克力么? 为什么现在很多人都肥胖呢？ 2. 你快乐么？ 什么是幸福？ 3. 你经常倒垃圾么？ 全球变暖如何解决？ 4. 你朋友多不多？ 为什么现在很多国家政治都分裂呢? 有观点：扩 无观点：扯 多大众，少个人 始于宽泛，终于具体 平常 practice 时候，要多扩展，这样逼迫自己，考场上才能发挥正常 帅哥的文件夹，不要按照顺序去练习 语法的广度与准度","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"Speaking 1 - Simon312 2020 口语考试形式一览","slug":"English/IELTS/simon-speaking-1","date":"2020-03-28T09:14:48.000Z","updated":"2021-06-22T06:33:48.661Z","comments":true,"path":"2020/03/28/English/IELTS/simon-speaking-1/","link":"","permalink":"http://www.iequa.com/2020/03/28/English/IELTS/simon-speaking-1/","excerpt":"Simon312 的2020年新题库刷题班","text":"Simon312 的2020年新题库刷题班 希望大家不忘初心，每天拿出来至少半个小时的时间做到相互提问，从耳朵听到问题-脑子思考-嘴巴输出的正确方式练习，partner做好基本语法和音标的记录，以及共同找到为什么停顿的原因。双方可以自行商定练习三个部分的节奏。 Simon312 1. Part 1 高分公式 5W, since barely do I have time to take care of them. but in the future, I will think about keeping them. 说不对，停顿一定扣分. send plants as gifts. 不能在脑海中： 中翻英，要不然会很吃亏，讲错了就会扣分， 我们只是来讨要 score 的. 2. Part2 的学习 Part2在最短时间之内熟悉我们的场景内容，做到不跑题熟练即可，具体方法如下 part1 and part2 不要跑题就行 3. Part 3 七个高分公式纵览","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"IELTS Listening 2 - 雅思听力必考词汇","slug":"English/IELTS/he-listening-words-2","date":"2020-03-20T09:14:48.000Z","updated":"2021-06-22T06:33:48.693Z","comments":true,"path":"2020/03/20/English/IELTS/he-listening-words-2/","link":"","permalink":"http://www.iequa.com/2020/03/20/English/IELTS/he-listening-words-2/","excerpt":"Are you ready?","text":"Are you ready? 手抄才是复习。 多写才能真会。 1. 办理 (银行、健身房、购物中心) 6-1-1 6-2-1 6-3-1 7-2-1 8-1-1 8-2-1 2. 聚会活动 4-4-1 P. 81 3. 图书馆 5-2-1 4. 医疗健康 9-4-1 P.79 4.1 场所名称 hospital 4.2 医务工作者 4.3 受伤部位 5. 电台节目 radio programme 5-1-2 6. homework discussion 6.1 地点 chemistry lab 化学实验室 engineer room 工程师室 6.2 人物 6.3 物品 6.4 作业相关 beginning 开始，起点","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"IELTS Listening 1 - 雅思听力必考词汇","slug":"English/IELTS/he-listening-words-1","date":"2020-03-19T09:14:48.000Z","updated":"2021-06-22T06:33:48.640Z","comments":true,"path":"2020/03/19/English/IELTS/he-listening-words-1/","link":"","permalink":"http://www.iequa.com/2020/03/19/English/IELTS/he-listening-words-1/","excerpt":"Are you ready?","text":"Are you ready? 手抄才是复习。 多写才能真会。 1. 租房场景 4-3-1 (指代剑四 Test3 第一部分 有该场景习题，何琼老师建议可以自己在复习中专门 练习) 5-4-1 6-4-1 7-4-1 8-2-1 8-3-1 9-2-1 1.1 address/ area/ location 1.1.1 具体地址 16 Ocean Drive 21A Station Avenue 1.1.2 方向 east west south north northwest southeast 直接这么写就可以，不需要加连字符或者大写。 1.1.3 城郊 city center/ town center/ downtown suburb/ outskirts urban/ suburban/ rural urban area (发音连读) = city 1.2 water/ electricity water/ electricity (power)/ gas/ telephone (phone) bill/ ~ rental/ rent deposit: 可以考具体数额，百分数，一个月房租 one-month rent 结合数字(看不明白上课会有讲解) -ty -teen *80 *90 160 代表 四位以内数 £9.50/ 2800 telephone NO. extension NO. 三位或四位 分机 reference NO. 代号 post code driving license NO. flight NO. double/ triple serial NO.* 序列号 credit card NO. * master 万事达 visa 维萨 American Express 美国运通 cash check/ cheque 1.3 facility garage 车库/ garden 花园/ swimming pool 游泳池/ car park 停车场/ laundry room 洗衣房/ café 注意发音(开非 可费)/ restaurant furniture: microwave oven/ toaster/ heater/ stereo 音响/ fridge 1.4 time 1.5 people 1.6 document 1.7 personal information 2. 旅游出行 4-1-1 4-2-1 5-1-1 7-1-1 9-3-1 关键地名(红色地名必须掌握拼写，黑色地名只需要作为常识积累了解处于哪个国家即可) 3. 新生入学 orientation 5-3-2 P. 57 7-3-1 P.56 3.1 time 3.2 facility 3.3 position 3.4 service","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"IELTS Listening 6 - 复习要点","slug":"English/IELTS/he-Listening-6","date":"2020-03-16T11:14:48.000Z","updated":"2021-06-22T06:33:48.687Z","comments":true,"path":"2020/03/16/English/IELTS/he-Listening-6/","link":"","permalink":"http://www.iequa.com/2020/03/16/English/IELTS/he-Listening-6/","excerpt":"Are you ready?","text":"Are you ready? 剑14 看选项时候要横向比较 不方便看的话，就把书撕一撕 早晨不要吃米饭和馒头，这样容易发困，怕饿备一块巧克力 液体少的浓缩咖啡会帮你提神 7点之前需要起床，不要喝粥和米粉汤 到了考场7：30 不要思考的太多 看40题的全部过程 flute 各种乐器要会 全部大写能都听到 电影名字不会改，数字大写引号不改 竖向阅读，重复信息不用看了 cloakroom 单词发音语法是阅读速度的基础 corridor sitting area 客厅 controversy [ˈkɒntrəvɜːsi] 争论 taken seriously 被认真对待 distract remain fertile Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Section3 多选题难度很大 originally intend， investigate taken for granted 理所应当的 BD 正确，但是 AB 给你一半的分 选择题出题目无序 (多选题，写答案顺序也无序， BD, DB 都可以)","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"IELTS Listening 5 - 雅思陷阱","slug":"English/IELTS/he-Listening-5-trap","date":"2020-03-14T11:14:48.000Z","updated":"2021-06-22T06:33:48.671Z","comments":true,"path":"2020/03/14/English/IELTS/he-Listening-5-trap/","link":"","permalink":"http://www.iequa.com/2020/03/14/English/IELTS/he-Listening-5-trap/","excerpt":"Are you ready?","text":"Are you ready? 雅思陷阱 正确答案 不清楚 或者 替换词 干扰选项得清楚 working on 正在制作 原词听到的选项，谨慎跳过 单选题最终的答案，都是经过改写的，不改的很少 local people = the public unusual local press = newspapers sculpture == statue 总结之后至少听3遍 rather than hardly had expected 本来以为 Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Good 1 Are you ready? Good 1 Are you ready?","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"IELTS Listening 4 - 配对题","slug":"English/IELTS/he-Listening-4-SLOP","date":"2020-03-14T10:14:48.000Z","updated":"2021-06-22T06:33:48.636Z","comments":true,"path":"2020/03/14/English/IELTS/he-Listening-4-SLOP/","link":"","permalink":"http://www.iequa.com/2020/03/14/English/IELTS/he-Listening-4-SLOP/","excerpt":"Are you ready?","text":"Are you ready? 1. 考勤速递，阅读机经 提升做题能力是重要的 第二部分：生活化一些 第三部分：学术类一些 雅思出题形式不只用一次 何仙姑的必考词汇认真复习 找正确答案句的原则 SLOP, 我的任务就是 化繁而简 Are you ready? Are you ready? Are you ready? Are you ready? 2. SLOP S: Same Words L: Linking Word (But, however, and, like, so, thus, because, as, as a result of, lead to, then, another, also) O: Order P: paraphrase 配对日，替换法宝 boring 替换 tame animal (wildlife) care (look after) Are you ready? 3. 配对题表格总结 题号 听到题目 听到答案 翻译 14 I = the speaker keeping = maintanance 15 Liz now = currently (新事情开始) recruiting &amp; supporting worker = staff 16 Sarah now teaching/fed and watered =food and drink 17 Duncan = building 18 Judith retail = sales Are you ready? Are you ready? Are you ready? Are you ready? perform=performance performance=demonstration red=colored interactive=audience...question Are you ready? not headlthy=ill Tom…his book I…speaker the earth surface=the surface of the earth A for B = BA 在语境下是优秀的，就是successful 4. 第三部分难，长配对 长配对题更多时间看选项 学好找重点 抽象的词，改为具体的行为 Are you ready? www.freeice.com Are you ready?","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"IELTS Listening 3 - blank filling Map 带图题 & 地图题","slug":"English/IELTS/he-Listening-3-Maps","date":"2020-03-14T09:14:48.000Z","updated":"2021-06-22T06:33:48.707Z","comments":true,"path":"2020/03/14/English/IELTS/he-Listening-3-Maps/","link":"","permalink":"http://www.iequa.com/2020/03/14/English/IELTS/he-Listening-3-Maps/","excerpt":"Are you ready?","text":"Are you ready? 雅思地图 小写都会被改 老师我想等那个人不理我呢，因为她变了. 小写容易改. 按照题号顺序听答案 Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready?","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"IELTS Listening 2 - blank filling  填空题","slug":"English/IELTS/he-Listening-2-blank-filling","date":"2020-03-14T02:14:48.000Z","updated":"2021-06-22T06:33:48.681Z","comments":true,"path":"2020/03/14/English/IELTS/he-Listening-2-blank-filling/","link":"","permalink":"http://www.iequa.com/2020/03/14/English/IELTS/he-Listening-2-blank-filling/","excerpt":"Are you ready?","text":"Are you ready? • 复盘练习 • 不懂就问 • 时间宝贵 • 绕过七坑 • 踏实总结 • 多读多抄 Are you ready? 雅思听力考什么: Are you ready? Clarinet [ˌklærə’net]、Violin [ˌvaɪə’lɪn], piano [pi’ɑnoʊ], pianist ['pɪənɪst] 钢琴家 （注意该词） horn [hɔːrn] n. 喇叭、 route [ruːt]、root （辨意根据上下文） 辨音就得多听多读 pen: to shut an animal or a person in a small space example: At clipping time sheep need to be penned. example: a sheep pen out of bounds: out of bounds 英 [aʊt ɒv baʊndz] (某事物）被禁止;（某地）禁止进入 This place is out of bounds to students and troops. 不要指望你做了30个小时题目，就搞定了雅思，一切顺利了，做这些题目是为了暴露你的问题 我太弱了，我没听出来，这个人口音非常声优,特别 做题步骤，先画圈定位： 1). next, in the, King’s Park 2 July 2). first event, is a 3). final, players receive 出现这么多次，你还是没听出来，你多弱呢? 救命稻草： 介词是很难改的, 所以不要慌。 and 词要敏感重要, 你最最重要的是：定位到答案句 first event == begin the season with， finial == last 改写： presentation of prizes to the players = players receives prizes 13题目，考的很全面。 prizes 英 [ˈpraɪzɪz] 下面的表格，你抄10遍. 每周日复习下. Listening 本身这件事，不能控制你大脑, 所以要想办法. (听到 and, 你掐自己一下) 抓住重点，什么时候该做什么事，先考过再说. Date Date: November 23 不缩写月份, 不加后缀 23rd (写多必失) 23 November February 11 21 August Date of birth: 5.1.2020 May the first twenty twenty 27.1.1972 the twenty seven of the first Nineteen Seventy-two Time: 12小时，不加 am pm 2 to 9 == 8:58 8:30 == half past eight quarter of the hundred 四分之一 quarter to night == 8:45 格式要求：大小写，单复数，连字符 连字符： （有连字符就写单数的形式， 有连字符的词在一起，就算一个word） 10 minutes’ walk 10-minute walk 4-year-old child son-in-law 以上都算一个数字 改变自己,提升自己 以上都算一个数字 因为你连基本的都搞不定，先考到7.5分，你怎么样都行 2020.04.15 video 01:30, 开始审题: 至少有7道送分题 IELTS 不是考全程理解， 不知道他说了什么，很正常 听不懂，但是有办法得分， 定位 非常重要, 要有方法定位 direction [dɪˈrekʃn] 2:09 定位非常非常重要 定位非常非常重要","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"线性代数的本质 - 系列合集","slug":"ml/PCA-Matrix","date":"2020-03-11T14:00:21.000Z","updated":"2021-06-20T04:12:28.305Z","comments":true,"path":"2020/03/11/ml/PCA-Matrix/","link":"","permalink":"http://www.iequa.com/2020/03/11/ml/PCA-Matrix/","excerpt":"【官方双语/合集】线性代数的本质 - 系列合集","text":"【官方双语/合集】线性代数的本质 - 系列合集 01. 向量究竟是什么？ 02. 线性组合、张成的空间与基等线性代数基础概念 03. Matrics as linear transformations 矩阵与线性变换 矩阵乘法的意义就是特定的向量转换 (不同坐标系的向量转换) Are you ready? 04. Matrix multiplication as composition 09. 基变换","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"PCA","slug":"PCA","permalink":"http://www.iequa.com/tags/PCA/"}]},{"title":"IELTS Listening 1 - 雅思入门常识","slug":"English/IELTS/he-Listening-1-indoor","date":"2020-02-27T09:14:48.000Z","updated":"2021-06-22T06:33:48.633Z","comments":true,"path":"2020/02/27/English/IELTS/he-Listening-1-indoor/","link":"","permalink":"http://www.iequa.com/2020/02/27/English/IELTS/he-Listening-1-indoor/","excerpt":"Are you ready?","text":"Are you ready? 体力活：背单字, 前2后2，注意限制 Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? Are you ready? 对答案原则 programme/program, center/centre (the) kitchen three / 3 reject/refuse, flat/apartment Are you ready? frog / 45 / 657 trees Are you ready? I don’t know Sydney very well If they want to make a night of it can you just give me an idea of the location of the hotel? Are you ready? Are you ready? Are you ready? Are you ready? Month Word Week Phonetic symbol Sentence Example Monday 英 ['mʌndeɪ] I went back to work on Monday. Tuesday 英 ['tjuːzdeɪ] He phoned on Tuesday, just before you came. Wednesday 英 [ˈwenzdeɪ] Come and have supper with us on Wednesday, if you’re free. Thursday 英 ['θɜːzdeɪ] I’m usually free between Tuesday and Thursday. Friday 英 [ˈfraɪdeɪ] Mr. Cook is intending to go to the Middle East on Friday. Saturday 英 [ˈsætədɪ; -deɪ] He called her on Saturday morning at the studio. Sunday 英 [ˈsʌndeɪ; ˈsʌndi] I thought we might go for a drive on Sunday. Title Sentence Example January —— Jan. It’s quite warm for January. [ˈdʒænjuəri] February —— Feb. The second month of the year is February. March —— Mar. This term will begin on March 1st. April - Apr. May - May. June[dʒuːn] Jun, July — Jul. August — Aug. September — Sept. October — Oct. November — Nov. December — Dec. double zero seven, 26个字母发音练习准确, 单词整体发音","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"Reading - 538 考点词","slug":"English/IELTS/ji-538-1","date":"2020-02-26T14:14:48.000Z","updated":"2021-06-22T06:33:48.657Z","comments":true,"path":"2020/02/26/English/IELTS/ji-538-1/","link":"","permalink":"http://www.iequa.com/2020/02/26/English/IELTS/ji-538-1/","excerpt":"熟练掌握538,看来是很重要的","text":"熟练掌握538,看来是很重要的 1. 高频同替单词 阅读同替方式 一词多义 场景词 形近词 短语词组 我讲第一个课件，不见得内容只在第一个课件 涉及 reading 知识点，同替等等 阅读同替方式 复习： 538 要天天见 video - 5:06 练习时候： 盖住英文，看中文想英文 一天一遍 538， D1: 1 - 2 - 3 D2: 2 - 1 - 3 D3: 3 - 2 - 1 1.1 并列 并列型： 一句话的出题范围， as well as 逻辑型： 2~3句话的出题范围 538 剑9 剑9 1.2 因果 常考在： 填空题，判断题，单选偶尔出现 审题： 是否有生词 定位 考点 (判断题题干出现因果关系，因果关系就是考点) 找到考点，一定就是 T/F 1.x 难点 抽象到具体 1：00 hour 比较难点 比较对象 比较关系 比较内容 2. Class IELTS Reading 一词多义 3. Class IELTS Reading 11 类核心场景词重点词讲解","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"NZA Travel plan 🇳🇿🇦🇺","slug":"world/NewZealand_and_Australia","date":"2020-01-27T10:46:48.000Z","updated":"2021-06-22T06:07:42.543Z","comments":true,"path":"2020/01/27/world/NewZealand_and_Australia/","link":"","permalink":"http://www.iequa.com/2020/01/27/world/NewZealand_and_Australia/","excerpt":"","text":"默认段落 证件准备 Passport money exchange Travel Sim Card (data 4G+100G+call) Day Date Dest Type Time Day1 2020-12-192021-01-03 纽村土澳 南半球 16days 1. 奥克兰 纽村速览 在纽村旅行，参加当地活动是非常重要的一部分。包括 跳伞、冰川徒步、喷射快艇、峡湾渡轮等。 北岛 奥克兰 Auckland 惠灵顿 Wellington 《指环王》的取景地“霍比屯” 如果经由奥克兰－霍比特人小镇－惠灵顿，再搭船抵达南岛也是条相当理想，并且实惠的路线。 奥克兰海港夜景。By 锦衣 奥克兰是新西兰最大的城市，一方面它承载了新西兰城市的最佳面貌，是新西兰为数不多城市中的翘楚。另一方面，它是所有从国内前往新西兰旅行者的中转站，一部分人从这里直接搭乘飞机进入南岛开始旅行，另一部分人为它保留并不充裕的时间，看一看天空塔，赏一赏夜景。 奥克兰曾经做为新西兰的首都，尽管后来被地理位置更佳的惠灵顿取代，但仍旧是新西兰最发达、热闹、外向的城市。从市中心的伊丽莎白女王广场到海港大桥，或登上天空塔，用一杯咖啡的时间领略“千帆之都”的闲适与繁华。稍远一些推荐你前往伊甸山，这里风景优雅，可以眺望整个奥克兰，适合在阳光充裕的下午徒步游览。 2. 最美皇后镇 3. 凯恩斯圣诞 3. 失望墨尔本 4. 布里斯班 5. 悉尼跨年 Reference 在最美的季节畅游新西兰、澳大利亚 澳大利亚悉尼+墨尔本+新西兰基督城+皇后镇+奥克兰15日13晚自由行 在最美的季节畅游新西兰、澳大利亚 澳大利亚🇦🇺新西兰🇳🇿之旅 新西兰南岛必去小镇大解析 从一段历史开始的澳新之旅（澳大利亚＋新西兰） 澳大利亚 Australia 澳大利亚10日妙趣亲子线路 澳大利亚和新西兰经典10日线路","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"NewZealand","slug":"NewZealand","permalink":"http://www.iequa.com/tags/NewZealand/"}]},{"title":"Vietnam Travel plan 🇻🇳🇻🇳","slug":"world/Vietnam","date":"2019-12-27T10:46:48.000Z","updated":"2021-06-22T08:56:28.081Z","comments":true,"path":"2019/12/27/world/Vietnam/","link":"","permalink":"http://www.iequa.com/2019/12/27/world/Vietnam/","excerpt":"","text":"这里有大海、沙漠、童话般的小城，也有热闹年轻的大都市。 为什么要选择越南南部？ 越南越美，越南最美的地方都集中在南部: 东方巴黎胡志明 沙漠渔村美奈 避暑高原大叻 东方夏威夷芽庄 越南南部主要城市位置图 南部的越南，曾经被法国人殖民了一百多年，被美国人管理了二十多年，各种文化在这里汇集，融合，形成了万千风情的神韵，越南南部的城市，每一个地方都有自己独特的魅力和气质。 证件准备 Passport money exchange Vietnam Travel Prepaid Sim Card (data 4G+100G+call) 第一站 行程概览 Day Date Dest Type Hotel → Vietnam 东方巴黎(胡志明)→春暖花开(芽庄)→沙漠渔村(美奈)→法国花城(大叻) Ho Chi Minh→Nha Trang→Mui Ne→Dalat→Da Nang→Singapore Day1 2020-01-24 胡志明 16:55 樟宜机场T4 → 18:00 新山一国际机场T2机场6.7km→Hotel→范5老街→越式烤肉(1/2)→金融塔 宿胡志明 金融塔票是否需提前预定 grab 非常便宜，景点之间打车便可，但 traffic jam Day Date Dest Type Hotel Day2 2020-01-25 胡志明需早起 古芝+景点市区一日游 (套装)中央邮局→市政厅→圣母大教堂→粉红教堂→博物馆→统一宫→滨城市场(一个很大的集市价格要3折杀起) 越捷航空 VJ690 20:00 新山一国际机场 T1 → 21:00 金兰国际机场 T1 宿芽庄01-22 13:00 Day3 2020-01-26 芽庄 - 芽庄 8:30 芽庄市区一日环游+泥浆浴（周日教堂关闭）越南Nha Phu湾一日游Klook 宿芽庄不可取消 Day4 2020-01-27 芽庄 - 芽庄 7:30 芽庄三岛、蚕岛黑岛潜水、浮潜海钓燕岛(珊瑚岛) 宿芽庄01-24 18:00 Day5 2020-01-28 美奈一日游 7:00 白沙丘→红沙丘→仙女溪→渔村→日落 宿芽庄01-25 18:00 Day6 2020-01-29 大叻一日游 7:00 大叻火车站→大叻教堂→疯房屋Crazy house→玛丽亚大教堂→BaoDai宫→Datanla瀑布→春香湖→芽庄 宿芽庄01-26 18:00 Day6 2020-01-30 芽庄 - 芽庄 珍珠岛/天堂湾一日游 最好玩的珍珠岛乐园三岛游KLOOK 宿芽庄01-27 18:00 Day8 2020-01-31 芽庄 - 岘港 岘港市区 越捷航空 VJ580 12:10 金兰国际机场T1 - 13:10 岘港国际机场T1 宿岘港01-29 00:00 → 岘港辉煌庄园别墅酒店(Brilliant Majestic Villa Hotel Danang) 超棒4.9分 Day9 2020-02-01 岘港 - 岘港 岘港迦南岛会安古镇巴拿山→美溪海滩→山茶半岛→岘港大教堂→岘港城区→佛手桥 宿岘港01-30 00:00 Day10 2020-02-02 岘港 - 新加坡 越捷航空 VJ973 12:20 岘港国际机场T2 → 16:00 樟宜机场T4 宿新加坡 出游: 1. 注意安全 2. 注意时间 3. 注意被骗 Tmall 上旅行产品介绍不错的店铺： 福建优游国旅泉州专营店 签证方式： 落地签网申, 需要 15+25 = 40美金费用 牛车水旅行社 120 新币左右 国内淘宝 197RMB 一位 + 顺丰80RMB左右邮费 （需护照首页电子版即可） Day1. 东方巴黎-胡志明 Ho Chi Minh Day2. 东夏威夷-芽庄 芽庄是众所周知的网红城市，这座美丽的城市有许多外号，像东方夏威夷，小马尔代夫等等。 芽庄海滩 来芽庄，体验芽庄特色的热泥浆浴是必打卡的项目，泡在热热的泥浆浴里像泡温泉一样特别舒服，泡完泥浆浴和温泉，再体验一个越式按摩放松身心最合适不过了。 芽庄 矿泥浴景区 Day3. 春暖花开-芽庄 芽庄推荐景点：珍珠岛游乐园、芽庄三岛出海游、iresort泥浆浴 芽庄海滩 这里拥有大海，沙滩，椰子树，跨海缆车，世界级摩天轮，海上的迪斯尼乐园，还有遍地的咖啡厅，街头满是美美的鸡蛋花，三角梅，海风吹来，坐在沙滩的躺椅捧着椰子喝，一边放空身体发呆，这才是生活。 Day4. 沙漠渔村-美奈 这里的沙漠太迷人，这里的海鲜太好吃，因为这里的海岸线太美 美奈推荐景点：白沙丘，红沙丘，仙女溪，渔村。 （可报一个天猫套装半日游/1日游便可,赏日出日落） 越南 越南美奈如何安排行程？适合玩几天？ 白沙丘 如果你住在美奈，一定要体验一下狂野刺激的吉普车，坐在海边沙滩上看日出、日落~ 渔港是美奈的名片，风景秀丽的渔港，簸箕船渔船飘荡在海湾，傍晚和清晨特别热闹，虽然渔港弥漫着一股海鲜的腥味，但是挡不住渔港的魅力 渔港 BY deargod123 红沙滩不仅作为一个很好的拍照背景，赏日落也是最美的，红沙滩很近，从红沙滩望过去是一望无际的海岸线，蓝色的大海和红色的沙漠交相辉映，傍晚时分会有很多当地的人拿着滑沙的工具邀请你体验，从高高的沙丘上滑下来，特别的有趣，重拾童心。 Day5. 法国花城-大叻 大叻，百年前的法国人和越南末代皇帝就把这里当成度假天堂，在大叻，法国人修建了许许多多的度假别墅，百年以后，大叻的殖民风情还是被完整保留了下来，被称为越南的法国花园，家家户户法式小别墅，大街小巷鲜花盛开。 Crazy House Day6-7. 芽庄 来芽庄，有一半人会选择出海玩，满目玲琅的四岛游，三岛游，海钓游，潜水游. 芽庄黑岛 Day8-9. 岘港 岘港(Da Nang)位于越南中部，北连顺化、南接芽庄。背靠五行山，东北有山茶半岛作屏障，为天然良港. 美国《国家地理》评岘港为人生必到的50个景点之一，认为它是现代文明与自然的完美结合. 童话不在故事里——巴拿山 岘港旅行地图 Day10. 新加坡 Essentials 1. 必备 Passport VISA Hotel 确认单 往返机票行程单 WIFI 电话卡 携带黑色水笔 (入境卡填写模板) 提前订好接机 手机防水套 Airpods 2. 当地特色 长袖长裤 防蚊液 防晒霜 太阳镜 肠胃药 3. 行前注意事项 结算必要账单 设置 E-mail 自动 备忘紧急通讯 安置好宠物、植物 4. 数码产品 自拍杆 Mobile Power Adaper Power Bank 拖线板 5. 衣物类 内衣、内裤、袜子 衣服、裤子 运动鞋 6. 洗漱用品 刮胡刀 Reference 越南概况 越南超美四城详解 胡志明必访的9个景点 岘港经典4日游 岘港必玩TOP4","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"Vietnam","slug":"Vietnam","permalink":"http://www.iequa.com/tags/Vietnam/"}]},{"title":"Pycharm Keyboard Shortcuts（Mac）","slug":"python/PyCharm","date":"2019-12-22T09:00:21.000Z","updated":"2021-06-20T04:12:28.220Z","comments":true,"path":"2019/12/22/python/PyCharm/","link":"","permalink":"http://www.iequa.com/2019/12/22/python/PyCharm/","excerpt":"Pycharm Keyboard Shortcuts","text":"Pycharm Keyboard Shortcuts 1. Pycharm No. Shortcuts Function 1. Shift + option + Enter auto import 2. CMD + Opt + t try/catch 1 CMD + B 跳转到声明处（cmd加鼠标） 2 CMD + [] 光标之前/后的位置 3 SHIFT + CMD + F , 全局搜索 ✔️ 4 CMD + R / SHIFT + CMD + R 当前文件替换 / 全局替换 5 CMD + L , 指定行数跳转 ✔️ 6 SHIFT + ENTER , 直接到下一行 ✔️ 7 ALT + ENTER , auto-import ✔️ 8 CMD +/- / SHIFT CMD +/- 展开当前 / 展开所有 9 OPT + CMD + l 代码块对齐 10 CMD + D 在下一行复制本行的内容 已熟练 - - 1 CMD / 注释/取消注释一行 2 CMD + X/C 复制光标当前行,剪切同理 3 CMD + F 当前文件搜索（回车下一个 shift回车上一个） 4 CMD + F6 更改变量 结构性 1 CMD + O / SHIFT + CMD + O / OPT + CMD + O 搜索 class / files / 符号（函数等) 2 CTR + Tab 史上最NB的导航窗口 1. 工程 file 列表、文件结构列表 2. 命令行模式、代码检查、VCS等 3 ALT + F12 打开命令行栏 ✔️ 4 CMD + F12 显示文件结构 ✔️ 5 CMD + J 代码智能补全 6 ALT + F1 定位编辑文件所在位置 2. Invoke Learning 123PYTHONUNBUFFERED=1;INTERMEDIATE_DATA_PATH=/Users/blair/ghome/6E/work_project/mlar/tmp;PYTHONPATH=~/ghome/6E/work_project/mlar:~/ghome/6E/work_project/mlar/nn_framework environment Pycharm Invoke Reference 代码编辑快捷键","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"pyCharm","slug":"pyCharm","permalink":"http://www.iequa.com/tags/pyCharm/"}]},{"title":"scikit-learn 为机器学习准备文本数据","slug":"nlp/scikit-learn-text-processing","date":"2019-12-08T14:00:21.000Z","updated":"2021-06-22T06:48:39.563Z","comments":true,"path":"2019/12/08/nlp/scikit-learn-text-processing/","link":"","permalink":"http://www.iequa.com/2019/12/08/nlp/scikit-learn-text-processing/","excerpt":"scikit-learn","text":"scikit-learn 有关特征的提取，scikit-learn给出了很多方法，具体分成了图片特征提取和文本特征提取。 文本特征提取的接口是sklearn.feature_extraction.text，那么接下来学习里面封装的函数。 CountVectorizer 123456789101112131415from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer(min_df=1)corpus = [ &#x27;This is the first document.&#x27;, &#x27;This is the second second document.&#x27;, &#x27;And the third one.&#x27;, &#x27;Is this the first document?&#x27;, ]X = vectorizer.fit_transform(corpus)feature_name = vectorizer.get_feature_names()print feature_nameprint X.toarray() 程序的结果为 12345[u&#x27;and&#x27;, u&#x27;document&#x27;, u&#x27;first&#x27;, u&#x27;is&#x27;, u&#x27;one&#x27;, u&#x27;second&#x27;, u&#x27;the&#x27;, u&#x27;third&#x27;, u&#x27;this&#x27;][[0 1 1 1 0 0 1 0 1] [0 1 0 1 0 2 1 0 1] [1 0 0 0 1 0 1 1 0] [0 1 1 1 0 0 1 0 1]] Reference scikit-learn 为机器学习准备文本数据 学习sklearn之文本特征提取 Scikit-Learn Vectorizermax_features Keras Keras入门（一）搭建深度神经网络（DNN）解决多分类问题","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"scikit-learn","slug":"scikit-learn","permalink":"http://www.iequa.com/tags/scikit-learn/"}]},{"title":"Malaysia Travel plan 🇲🇾🇲🇾","slug":"world/Malaysia-West","date":"2019-12-08T10:46:48.000Z","updated":"2021-06-22T06:07:42.536Z","comments":true,"path":"2019/12/08/world/Malaysia-West/","link":"","permalink":"http://www.iequa.com/2019/12/08/world/Malaysia-West/","excerpt":"","text":"马来西亚，简称大马，位于东南亚，在我国南边。马来的国土很特殊，被南中国海分为东马和西马。 西马有著名的首都吉隆坡、文化小城马六甲，美食聚集地槟城和热带岛屿兰卡威等；东马则是海洋度假胜地。 马来西亚分成东马来西亚和西马来西亚。东马来西亚包括世界前三日落观看地之一亚庇（沙巴）、“潜水天堂”诗巴丹，还有仙本那等。距离比较远（如果你是从吉隆坡飞东马的话），时间需要比较多，因此不适合太赶的五天旅行。 西马在人文景观游览的同时，还能欣赏到 城市、小镇、海岛 等各种风情的景观： 摩登都市吉隆坡 壁画城镇槟城 热带岛屿兰卡威 热浪岛 短短的五天可以海陆结合，一举两得. 行程安排： DAY1-1： 感受首都吉隆坡的城市繁华，从热闹市井的茨厂街出发，到摩登地标双子塔。 DAY2-3： 槟城里的乔治市，边搜寻美食，边看壁画。 DAY4-5： 在兰卡威不能错过自驾环岛，陆上体验完了，可以在芭雅岛潜水，下午即可返回吉隆坡。 Day1 吉隆坡 Kuala Lumpur 路线安排：茨厂街(China Town)—马里安曼印度庙—清真寺(National Mosque)—独立广场—双子塔KLCC Day2-3 槟城 Penang 乔治市街头艺术 Mirror Georgetown 姓氏桥(姓周桥) Clan Jetty 槟城（Penang）：作为“最佳被访的岛屿之一”，吸引人的是可以在肚子撑船的美食，还有乔治市的街头艺术。噢，对了！槟城的姓氏桥还是《初恋红豆冰》的取景地，值得来看一看！ 推荐必吃美食：炒粿条、榴莲冰淇淋、亚参叻沙 Asam Laksa、煎蕊和红豆冰Cendol, Ais kacang、槟城福建面Hokkien Mee和咖哩面 Curry mee, Mee Kari 。 Day4-5 兰卡威 Langkawi 美丽的沙滩、奇特的溶洞、青翠的森林、壮观的瀑布以及种类繁多的野生动植物 2天的时间提供三种路线，供大家参考选择： 推荐线路1【天空之桥+珍南海滩/丹绒鲁海滩】 推荐线路2【红树林+巨鹰广场】 推荐线路3【芭雅岛海洋公园一日游】\u0004 珍南海滩 Pantal Chenang：是兰卡威最热闹、最出名、最成熟的公共海滩，有种似泰国“芭提雅”海滩的感觉，傍晚来这里感受一番是必不可少的体验之一。 （建议把它作为最后一站，因为“白天不懂夜的黑”） 住宿：直接打车住在珍南海滩附近，这边各种酒店，资源较多。 芭雅岛——和鱼群穿梭在珊瑚之间 ●●亮点概括●● ✮ 沙滩岸潜能看到鲨鱼宝宝 ✮ 畅游在清澈的果冻海里和上千条鱼儿浮潜 ✮ 携带水下相机拍下美好的回忆 Reference 西马来西亚五日怎么玩？有这篇就够了 盘点兰卡威基础玩法，这样玩才算来过！ 被人忽略的西马，也有多种花式玩法","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"Malaysia","slug":"Malaysia","permalink":"http://www.iequa.com/tags/Malaysia/"}]},{"title":"Linux Systemd Tutorial","slug":"devops/linux-systemd-tutorial","date":"2019-11-23T06:11:21.000Z","updated":"2021-06-20T04:12:28.198Z","comments":true,"path":"2019/11/23/devops/linux-systemd-tutorial/","link":"","permalink":"http://www.iequa.com/2019/11/23/devops/linux-systemd-tutorial/","excerpt":"www.freedesktop.org","text":"www.freedesktop.org 1. Preface 历史上，Linux 的启动一直采用init进程。 123$ sudo /etc/init.d/apache2 start# 或者$ service apache2 start 这种方法有两个缺点: 启动时间长 启动脚本复杂 init 进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 init 进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 2. Systemd (daemon) Systemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。 Systemd Author: Lennart Poettering Linux OS Daemon management tool Systemd 。它是 OS 的一部分，直接与 kernel 交互，性能棒，功能强。我们完全可以将程序交给 Systemd ，让系统统一管理，成为真正意义上的系统服务。 12查看 Systemd 的版本。$ systemctl --version 3. System management Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。 Systemd 架构图 4. Unit Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。 4.1 Unit definition Unit 一共分成12种: Service unit：系统服务 Target unit：多个 Unit 构成的一个组 Device Unit：硬件设备 Mount Unit：文件系统的挂载点 Automount Unit：自动挂载点 Path Unit：文件或路径 Scope Unit：不是由 Systemd 启动的外部进程 Slice Unit：进程组 Snapshot Unit：Systemd 快照，可以切回某个快照 Socket Unit：进程间通信的 socket Swap Unit：swap 文件 Timer Unit：定时器 systemctl list-units 命令可以查看当前系统的所有 Unit 1234567891011121314# 列出正在运行的 Unit$ systemctl list-units# 列出所有Unit，包括没有找到配置文件的或者启动失败的$ systemctl list-units --all# 列出所有没有运行的 Unit$ systemctl list-units --all --state=inactive# 列出所有加载失败的 Unit$ systemctl list-units --failed# 列出所有正在运行的、类型为 service 的 Unit$ systemctl list-units --type=service 4.2 Unit status 12345678# 显示系统状态$ systemctl status# 显示单个 Unit 的状态$ sysystemctl status bluetooth.service# 显示远程主机的某个 Unit 的状态$ systemctl -H root@rhel7.example.com status httpd.service 除了 status 命令， systemctl 还提供了三个查询状态的简单方法，主要供脚本内部的判断语句使用。 4.3 Unit management 对于用户来说，最常用的是下面这些命令，用于 start 和 stop Unit（主要是 service） 1234567891011121314151617181920212223242526# 立即启动一个服务$ sudo systemctl start apache.service# 立即停止一个服务$ sudo systemctl stop apache.service# 重启一个服务$ sudo systemctl restart apache.service# 杀死一个服务的所有子进程$ sudo systemctl kill apache.service# 重新加载一个服务的配置文件$ sudo systemctl reload apache.service# 重载所有修改过的配置文件$ sudo systemctl daemon-reload# 显示某个 Unit 的所有底层参数$ systemctl show httpd.service# 显示某个 Unit 的指定属性的值$ systemctl show -p CPUShares httpd.service# 设置某个 Unit 的指定属性$ sudo systemctl set-property httpd.service CPUShares=500 4.4 Unit dependencies Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。 systemctl list-dependencies命令列出一个 Unit 的所有依赖 1$ systemctl list-dependencies nginx.service 上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用–all参数 1$ systemctl list-dependencies --all nginx.service 5. Unit config 5.1 config overview 每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录 /etc/systemd/system/ 读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录 /usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable 命令用于在上面两个目录之间，建立符号链接关系。 123$ sudo systemctl enable clamd@scan.service# 等同于$ sudo ln -s &#x27;/usr/lib/systemd/system/clamd@scan.service&#x27; &#x27;/etc/systemd/system/multi-user.target.wants/clamd@scan.service&#x27; 配置文件的后缀名，就是该 Unit 的种类，比如 sshd.socket。如果省略，Systemd 默认后缀名为.service，所以 sshd 会被理解成 sshd.service。 5.2 config status 12345# 列出所有配置文件$ systemctl list-unit-files# 列出指定类型的配置文件$ systemctl list-unit-files --type=service 这个命令会输出一个列表 123456$ systemctl list-unit-filesUNIT FILE STATEchronyd.service enabled (已启动链接)clamd@.service static (该配置文件没有[Install]部分（无法执行），只能作为其他配置文件的依赖)clamd@scan.service disabled (没建立启动链接) 从配置文件的状态无法看出，该 Unit 是否正在运行。这必须执行前面提到的 systemctl status 命令 1$ systemctl status bluetooth.service 一旦修改配置文件，就要让 SystemD 重新加载配置文件，然后重新启动，否则修改不会生效。 12$ sudo systemctl daemon-reload$ sudo systemctl restart httpd.service 5.3 config format systemctl cat service_name 命令可以查看配置文件的内容 1234567891011$ systemctl cat atd.service[Unit]Description=ATD daemon[Service]Type=forkingExecStart=/usr/bin/atd[Install]WantedBy=multi-user.target 注意，键值对的等号两侧不能有空格 5.4 config block [Unit]区块是配置文件的第一个区块，用来定义 Unit 的元数据及配置与其他 Unit 的关系。它的主要字段如下。 field desc Description 简短描述 Documentation 文档地址 Requires 当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败 … … Assert… 当前 Unit 运行必须满足的条件，否则会报启动失败 [Install]区块是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。 field desc WantedBy 它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 + .wants后缀构成的子目录中. … … Alias 当前 Unit 可用于启动的别名 Also 当前 Unit 激活（enable）时，会被同时激活的其他 Unit [Service]区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。 field desc Type 定义启动时的进程行为。它有以下几种值。 … … Environment 指定环境变量 Unit 配置文件的完整字段清单，请参考官方文档. 6. Target (units) 启动计算机的时候，需要启动大量的 Unit。如果每一次启动，都要一一写明本次启动需要哪些 Unit，显然非常不方便。Systemd 的解决方案就是 Target。 简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。 12345678910111213141516# 查看当前系统的所有 Target$ systemctl list-unit-files --type=target# 查看一个 Target 包含的所有 Unit$ systemctl list-dependencies multi-user.target# 查看启动时的默认 Target$ systemctl get-default# 设置启动时的默认 Target$ sudo systemctl set-default multi-user.target# 切换 Target 时，默认不关闭前一个 Target 启动的进程，# systemctl isolate 命令改变这种行为，# 关闭前一个 Target 里面所有不属于后一个 Target 的进程$ sudo systemctl isolate multi-user.target Target 与 传统 RunLevel 的对应关系如下: 123456789Traditional runlevel New target name Symbolically linked to...Runlevel 0 | runlevel0.target -&gt; poweroff.targetRunlevel 1 | runlevel1.target -&gt; rescue.targetRunlevel 2 | runlevel2.target -&gt; multi-user.targetRunlevel 3 | runlevel3.target -&gt; multi-user.targetRunlevel 4 | runlevel4.target -&gt; multi-user.targetRunlevel 5 | runlevel5.target -&gt; graphical.targetRunlevel 6 | runlevel6.target -&gt; reboot.target 7. Log management Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用 journalctl 一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是 /etc/systemd/journald.conf。 123456# 查看某个 Unit 的日志$ sudo journalctl -u nginx.service$ sudo journalctl -u nginx.service --since today# 实时滚动显示某个 Unit 的最新日志$ sudo journalctl -u nginx.service -f 它与init进程的主要差别如下 12345（1）默认的 RunLevel（在/etc/inittab文件设置）现在被默认的 Target 取代，位置是/etc/systemd/system/default.target，通常符号链接到graphical.target（图形界面）或者multi-user.target（多用户命令行）。（2）启动脚本的位置，以前是/etc/init.d目录，符号链接到不同的 RunLevel 目录 （比如/etc/rc3.d、/etc/rc5.d等），现在则存放在/lib/systemd/system和/etc/systemd/system目录。（3）配置文件的位置，以前init进程的配置文件是/etc/inittab，各种服务的配置文件存放在/etc/sysconfig目录。现在的配置文件主要存放在/lib/systemd目录，在/etc/systemd目录里面的修改可以覆盖原始设置。 Reference 阮一峰: 计算机是如何启动的？ ✔️ 阮一峰: Linux 的启动流程 ✔️ 阮一峰: Linux daemon (守护进程)的启动方法 ✔️ 阮一峰: Systemd 入门教程：命令篇 ✔️ 阮一峰: Kiss (Keep it sample stupid) Unix哲学 ✔️ Systemd Service Manager Great: Linux 系统开机启动项清理 Understanding And Using Systemd sudo sudo 命令情景分析 Linux 系统中 sudo 命令的 10 个技巧 鸟哥的私房菜： 第十四章、Linux 账号管理与 ACL 权限配置","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Systemd","slug":"Systemd","permalink":"http://www.iequa.com/tags/Systemd/"}]},{"title":"Anaconda + Tensorflow 环境搭建","slug":"devops/tensorflow_env_build","date":"2019-11-08T14:00:21.000Z","updated":"2021-06-20T04:12:28.196Z","comments":true,"path":"2019/11/08/devops/tensorflow_env_build/","link":"","permalink":"http://www.iequa.com/2019/11/08/devops/tensorflow_env_build/","excerpt":"Anaconda+Tensorflow","text":"Anaconda+Tensorflow 1. Anaconda 1.1 Anaconda 常用命令 12conda --versionconda update conda 123456# 帮助命令conda -h# 更新所有包conda update --allconda upgrade --all 1.2 Anaconda 管理环境 1conda create --name &lt;env_name&gt; &lt;package_names&gt; 如： conda create -n python3 python=3.5 numpy pandas 即创建一个名为“python3”的环境，环境中安装版本为3.5的python，同时也安装了numpy和pandas。 12345conda info --envsconda env listsource activate &lt;env_name&gt; 12345# 复制环境conda create --name &lt;new_env_name&gt; --clone &lt;copied_env_name&gt;# 删除环境conda remove --name &lt;env_name&gt; --all 1234pip list / conda list# 当使用 conda install 无法进行安装时，可以使用pip进行安装pip install &lt;package_name&gt; 2. Tensorflow 1conda install pandas xlrd 12conda install keras==2.2.4pip install keras-bert 12conda install tensorflow==&#x27;1.11.0&#x27;conda install tensorflow-gpu==&#x27;1.11.0&#x27; 3. GPU 1234567891011nvidia-smi+-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 6 7847 C python 11615MiB || 7 6412 C python 4219MiB || 8 36257 C .../anaconda2/envs/bert-serving/bin/python 11615MiB || 9 17293 C /home/xxx/.conda/envs/aspect/bin/python 11613MiB |+-----------------------------------------------------------------------------+ 解决 cuda10 因为显卡驱动不支持的 1conda install cudatoolkit=9.0 用如下代码可检测tensorflow的能使用设备情况： 12from tensorflow.python.client import device_libprint(device_lib.list_local_devices()) 4. CPU Keras以及Tensorflow强制使用CPU 使用CUDA_VISIBLE_DEVICES命令行参数，代码如下： 1CUDA_VISIBLE_DEVICES=&quot;&quot; python3 train.py Reference Anaconda介绍、安装及使用教程 Tensorflow检验GPU是否安装成功 及 使用GPU训练注意事项 报错：cudaGetDevice() failed. Status: CUDA driver version is https://tensorflow.google.cn/install/source tensorflow 使用CPU而不使用GPU的问题解决 Keras以及Tensorflow强制使用CPU Good! 教程 | 使用 PyCharm 连接服务器进行远程开发和调试","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Anaconda","slug":"Anaconda","permalink":"http://www.iequa.com/tags/Anaconda/"}]},{"title":"Bert 最简单的打开姿势","slug":"nlp/bert_keras-tutorial","date":"2019-11-08T03:00:21.000Z","updated":"2021-06-22T06:48:39.557Z","comments":true,"path":"2019/11/08/nlp/bert_keras-tutorial/","link":"","permalink":"http://www.iequa.com/2019/11/08/nlp/bert_keras-tutorial/","excerpt":"bert 遇见 keras","text":"bert 遇见 keras 1. 当Bert遇上Keras 在Keras下对Bert最好的封装是： keras-bert：https://github.com/CyberZHG/keras-bert 12345678910111213141516171819202122232425262728293031323334353637383940414243#! -*- coding:utf-8 -*-import jsonimport numpy as npimport pandas as pdfrom random import choicefrom keras_bert import load_trained_model_from_checkpoint, Tokenizerimport re, osimport codecsmaxlen = 100config_path = &#x27;../bert/chinese_L-12_H-768_A-12/bert_config.json&#x27;checkpoint_path = &#x27;../bert/chinese_L-12_H-768_A-12/bert_model.ckpt&#x27;dict_path = &#x27;../bert/chinese_L-12_H-768_A-12/vocab.txt&#x27;token_dict = &#123;&#125;# dict_path = &#x27;./bert/chinese_L-12_H-768_A-12/vocab.txt&#x27;with codecs.open(dict_path, &#x27;r&#x27;, &#x27;utf8&#x27;) as reader: for line in reader: token = line.strip() token_dict[token] = len(token_dict)class OurTokenizer(Tokenizer): def _tokenize(self, text): R = [] for c in text: if c in self._token_dict: print(c) R.append(c) elif self._is_space(c): R.append(&#x27;[unused1]&#x27;) # space类用未经训练的[unused1]表示 else: R.append(&#x27;[UNK]&#x27;) # 剩余的字符是[UNK] return Rtokenizer = OurTokenizer(token_dict)tokenizertokenizer = OurTokenizer(token_dict)tokenizer.tokenize(u&#x27;今天天 气不错&#x27;) output: 1[&#x27;[CLS]&#x27;, &#x27;今&#x27;, &#x27;天&#x27;, &#x27;天&#x27;, &#x27;[unused1]&#x27;, &#x27;气&#x27;, &#x27;不&#x27;, &#x27;错&#x27;, &#x27;[SEP]&#x27;] 这里简单解释一下Tokenizer的输出结果。首先，默认情况下，分词后句子首位会分别加上[CLS]和[SEP]标记，其中[CLS]位置对应的输出向量是能代表整句的句向量（反正Bert是这样设计的），而[SEP]则是句间的分隔符，其余部分则是单字输出（对于中文来说） 2. Sentiment classification 文本情感分类（一）：传统模型 文本情感分类（二）：深度学习模型 做一个最基本的文本分类任务: 12345678910111213141516171819bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)for l in bert_model.layers: l.trainable = Truex1_in = Input(shape=(None,))x2_in = Input(shape=(None,))x = bert_model([x1_in, x2_in])x = Lambda(lambda x: x[:, 0])(x) # 取出[CLS]对应的向量用来做分类p = Dense(1, activation=&#x27;sigmoid&#x27;)(x)model = Model([x1_in, x2_in], p)model.compile( loss=&#x27;binary_crossentropy&#x27;, optimizer=Adam(1e-5), # metrics=[&#x27;accuracy&#x27;])model.summary() 在Keras中调用Bert来做情感分类任务就这样写完了～写完了～～ 3. 运行效果 所有的 params train 12for l in bert_model.layers: l.trainable = True bert keras Sentiment analysis 加载的 bert params non-train bert keras Sentiment analysis 4. 指导原则 有什么原则来指导Bert后面应该要接哪些层？ 答案是：用尽可能少的层来完成你的任务。 比如上述情感分析只是一个二分类任务，你就取出第一个向量然后加个Dense(1)就好了，不要想着多加几层Dense，更加不要想着接个LSTM再接Dense； 如果你要做序列标注（比如NER），那你就接个Dense+CRF就好，也不要多加其他东西。 总之，额外加的东西尽可能少。一是因为Bert本身就足够复杂，它有足够能力应对你要做的很多任务；二来你自己加的层都是随即初始化的，加太多会对Bert的预训练权重造成剧烈扰动，容易降低效果甚至造成模型不收敛 Reference 《Attention is All You Need》-苏神 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 深度学习教程-苏神 、 当Bert遇上Keras-苏神 Keras中文文档 fit_generator 开启NLP的大魔法阵——通过Keras-Bert操纵Bert 超牛推荐 123456alberthttps://arxiv.org/pdf/1909.11942.pdfELECTRAhttps://openreview.net/pdf?id=r1xMH1BtvBbert attention analysishttps://nlp.stanford.edu/pubs/clark2019what.pdf","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://www.iequa.com/tags/BERT/"}]},{"title":"ElasticSearch 各种查询关键字的区别","slug":"elasticsearch/es-keyword-for-search","date":"2019-10-31T03:00:21.000Z","updated":"2021-06-22T06:33:48.664Z","comments":true,"path":"2019/10/31/elasticsearch/es-keyword-for-search/","link":"","permalink":"http://www.iequa.com/2019/10/31/elasticsearch/es-keyword-for-search/","excerpt":"ElasticSearch","text":"ElasticSearch Reference ElasticSearch各种查询关键字的区别 终于有人把elasticsearch原理讲通了！ 深入搜索 Function Score 查询 ElasticSearch 评分排序 深入搜索 » 控制相关度 19 个很有用的 ElasticSearch 查询语句 ElasticSearch查询 第四篇：匹配查询（Match） Elasticsearch bool query小结 (解决should失效) 深入搜索 » 多字段搜索 » 多数字段 Stemmer Token Filter Elasticsearch Reference [6.2] » Analysis » Analyzers Elasticsearch Reference [6.2] » Analysis » Analyzers » Standard Analyzer Elasticsearch修改分词器以及自定义分词器 ES 09 - Elasticsearch如何定制分词器 (自定义分词策略) 处理人类语言 » 将单词还原为词根 » 原形词干提取 elasticsearch should实现or功能，设置minimum_should_match Kibana 用户手册 » 数据探索 Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 组合查询 Elasticsearch Reference [6.4] » Search APIs » Request Body Search » Field Collapsing ES Field Collapsing 字段折叠使用详解","categories":[{"name":"elastic","slug":"elastic","permalink":"http://www.iequa.com/categories/elastic/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.iequa.com/tags/ElasticSearch/"}]},{"title":"Kubernetes tutorial","slug":"devops/kubernetes-tutorial","date":"2019-10-28T10:11:21.000Z","updated":"2021-06-22T06:31:05.531Z","comments":true,"path":"2019/10/28/devops/kubernetes-tutorial/","link":"","permalink":"http://www.iequa.com/2019/10/28/devops/kubernetes-tutorial/","excerpt":"kubernetes.io","text":"kubernetes.io 1. Kubernetes what? Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes minikube install @Mac 12345sysctl -a | grep -E --color &#x27;machdep.cpu.features|VMX&#x27; brew cask install minikube同時它也會一起安裝 kubectl 這個 Kubernetes 指令操作工具 kubectl minikube version 123456# /tmp/fluentd/etc [9:47:26]➜ minikube versionminikube version: v1.5.0commit: d1151d93385a70c5a03775e166e94067791fe2d9minikube 主要是用在練習和教學使用, 非生产环境. Kubernetes is growing rapidly, has become a leader in Container Orchestration。 2. Kubernetes why？ 生产环境应用会包含多个 Containers，而这些 Containers 还很可能会跨越多个服务器主机部署。 Kubernetes 提供了为那些工作负载大规模部署 Container 的编排与管理能力. Kubernetes 编排能构建多容器应用服务，在集群上调度或伸缩这些容器，及管理它们随时间变化的健康状态. Kubernetes 也需要与网络、存储、安全、监控等其它服务集成才能提供综合性的容器基础设施。 有了 Kubernetes，你可以： 跨主机编排 Container; 控制与自动化应用的部署与升级。 为有状态的 Application 挂载和添加存储器。 线上扩展或裁剪 Containerized applications 与它们的资源。 声明式的容器管理，保证所部署的应用按照我们部署的方式运作。 通过自动布局、自动重启、自动复制、自动伸缩实现应用的状态检查与自我修复。 Kubernetes 依赖其它项目来提供完整的编排服务。结合其它必要开源项目作为其组件： 仓库：Atomic Registry、Docker Registry 等。 网络：OpenvSwitch 和智能边缘路由等。 监控：heapster、kibana、hawkular 和 elastic。 安全：LDAP、SELinux、 RBAC 与 支持多租户的 OAUTH。 自动化：通过 Ansible 的 playbook 进行集群的安装和生命周期管理。 服务：大量事先创建好的常用应用模板。 红帽 OpenShift 为容器部署预先集成了上面这些组件。 3. Kubernetes architecture Kubernetes 支持在多种环境下的安装: 本地主机（Fedora） 云服务（Google GAE、AWS） 3.1 K8s architecture kubernetes.io 3.2 Master Master有三个组件：API Server、Scheduler、Controller: API Server 是整个系统的对外接口，提供 RESTful 方式供客户端和其它组件调用； Scheduler 负责对资源进行调度，分配某个 pod 到某个节点上； Controller-manager 负责管理控制器，包括 endpoint-controller（刷新服务和 pod 的关联信息）和 replication-controller（维护某个 pod 的复制为配置的数值）。 3.3 Node Kubernetes Node 集群中的每个非 master 节点都运行两个进程： kubelet，和 master 节点进行通信。 kube-proxy，一种网络代理，将 Kubernetes 的网络服务代理到每个节点上。 3.4 Kubernetes 术语 Master（主节点）： 控制 Kubernetes 节点的机器，也是创建作业任务的地方。 Node（节点）： 这些机器在 Kubernetes 主节点的控制下执行被分配的任务。 Pod： 由一个或多个容器构成的集合，作为一个整体被部署到一个单一节点。 同一个 pod 中的容器共享 IP 地址、进程间通讯（IPC）、主机名以及其它资源。 Pod 将底层容器的网络和存储抽象出来，使得集群内的容器迁移更为便捷。 Replication controller： 控制一个 pod 在集群上运行的实例数量。 Service： 将服务内容与具体的 pod 分离。Kubernetes 服务代理负责自动将服务请求分发到正确的 pod 处. Kubelet： 这个守护进程运行在各个工作节点上，负责获取容器列表，保证声明的容器已经启动且正常运行。 kubectl： 这是 Kubernetes 的命令行配置工具。 更多内容请查看 Kubernetes 术语表 4. Quickstart minikube 启动 Kubernetes cluster 4.1 Start Minik &amp; create cluster Start Minikube and create a cluster 12345# /tmp/fluentd/etc [14:03:57]➜ minikube start😄 minikube v1.5.0 on Darwin 10.14.6✨ Automatically selected the &#x27;hyperkit&#x27; driver💾 Downloading driver docker-machine-driver-hyperkit: 4.2 create a k8s Deployment Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using --port. 12➜ kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.10deployment.apps/hello-minikube created 4.3 hello-minikube Deployment To access the hello-minikube Deployment, expose it as a Service: 12➜ kubectl expose deployment hello-minikube --type=NodePort --port=8080service/hello-minikube exposed 4.4 view hello-minikube Pod The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. If the output shows the STATUS as Running, the Pod is now up and running: 1234➜ kubectl get podNAME READY STATUS RESTARTS AGEhello-minikube-797f975945-nrb6r 1/1 Running 0 2m14s(anaconda3) (base) kubectl get cmd： 12kubectl get nodekubectl get po,svc -n kube-system 4.5 view the Service details Get the URL of the exposed Service to view the Service details: 123➜ minikube service hello-minikube --urlhttp://192.168.64.2:30799(anaconda3) (base) Output: 12345678910111213141516171819202122232425262728Hostname: hello-minikube-797f975945-nrb6rPod Information: -no pod information available-Server values: server_version=nginx: 1.13.3 - lua: 10008Request Information: client_address=172.17.0.1 method=GET real path=/ query= request_version=1.1 request_scheme=http request_uri=http://192.168.64.2:8080/Request Headers: accept=text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3 accept-encoding=gzip, deflate accept-language=zh-CN,zh;q=0.9,zh-TW;q=0.8,en-US;q=0.7,en;q=0.6 connection=keep-alive host=192.168.64.2:30799 upgrade-insecure-requests=1 user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36Request Body: -no body in request- 4.6 delete hello-minik service 1kubectl delete services hello-minikube 4.7 delete hello-minik deploym 销毁该 Deployment（和它的 pod） 1kubectl delete deployment hello-minikube 4.8 stop Minikube cluster 1minikube stop 4.9 delete Minikube cluster 1minikube delete 5. Kubernetes hexo Docker 用户使用 kubectl 命令指南 Kubernetes 支持两种方式创建资源： 用 kubectl 命令直接创建，比如： 123kubectl run nginx-deployment --image=nginx:1.7.9 --replicas=2在命令行中通过参数指定资源的属性. 通过配置文件和 kubectl apply 创建，要完成前面同样的工作，可执行命令： 123kubectl apply -f nginx.ymlkubectl apply 不但能够创建 Kubernetes 资源，也能对资源进行更新，非常方便。 Kubernets 还提供了几个类似的命令，例如: kubectl create, kubectl replace kubectl edit, kubectl patch 5.1 kubectl cluster-info cmd kubectl cluster-info 123➜ kubectl cluster-infoKubernetes master is running at https://192.168.64.2:8443KubeDNS is running at https://192.168.64.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubectl get po,svc -n kube-system 1234567891011121314➜ kubectl get po,svc -n kube-systemNAME READY STATUS RESTARTS AGEpod/coredns-5644d7b6d9-2qtds 1/1 Running 0 25hpod/coredns-5644d7b6d9-w8442 1/1 Running 0 25hpod/etcd-minikube 1/1 Running 0 25hpod/kube-addon-manager-minikube 1/1 Running 0 25hpod/kube-apiserver-minikube 1/1 Running 0 25hpod/kube-controller-manager-minikube 1/1 Running 1 25hpod/kube-proxy-k2pgh 1/1 Running 0 25hpod/kube-scheduler-minikube 1/1 Running 2 25hpod/storage-provisioner 1/1 Running 0 25hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 25h 5.2 Create deployment hexo4 1kubectl run --image=blair101/ubuntu-hexo-blog:v1.4 hexo4 --port=4000 --env=&quot;DOMAIN=cluster&quot; 创建一个 deployment, name: hexo4, 也会生成一个 pod, 可通过 kubectl edit deploy hexo4 修改副本数为2 1234567➜ kubectl get deploy,svcNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/hexo4 2/2 2 2 8m34sNAME READY STATUS RESTARTS AGEpod/hexo4-67f54d9bd9-8gzt9 1/1 Running 0 8m34spod/hexo4-67f54d9bd9-dvzht 1/1 Running 0 3s k8s 端口映射 查看详细 deploy hexo4 -o yaml 1kubectl get deploy hexo4 -o yaml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849apiVersion: apps/v1kind: Deploymentmetadata: annotations: deployment.kubernetes.io/revision: &quot;1&quot; creationTimestamp: null generation: 1 labels: run: hexo4 name: hexo4 selfLink: /apis/apps/v1/namespaces/default/deployments/hexo4spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: run: hexo4 strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: run: hexo4 spec: containers: - env: - name: DOMAIN value: cluster image: blair101/ubuntu-hexo-blog:v1.4 imagePullPolicy: IfNotPresent name: hexo4 ports: - containerPort: 4000 protocol: TCP resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30status: &#123;&#125;(anaconda3) (base) 一些常用命令记录： 12345678kubectl describe po hexo4-67f54d9bd9-ffnspkubectl exec -it hexo4-67f54d9bd9-ffnsp bashkubectl get po hexo4-5b97779b7c-8z8ns -o yaml --exportkubectl get deploy hexo4 -o yaml --export | tee deploy-v1.yamlkubectl apply -f deploy-v5.yaml 5.3 Create service hexo4 1kubectl expose deployment hexo4 --port=4000 --name=hexo4 查看 service, kubectl edit svc hexo4 1234567891011➜ kubectl get deploy, po, svcNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/hexo4 2/2 2 2 54mNAME READY STATUS RESTARTS AGEpod/hexo4-67f54d9bd9-8gzt9 1/1 Running 0 54mpod/hexo4-67f54d9bd9-dvzht 1/1 Running 0 45mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/hexo4 ClusterIP 10.97.151.174 &lt;none&gt; 4000/TCP 35sservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 26h 查看 service hexo4 详情: 12345678910111213141516171819202122232425➜ kubectl get svc -o yaml hexo4apiVersion: v1kind: Servicemetadata: creationTimestamp: &quot;2019-10-30T03:56:49Z&quot; labels: run: hexo4 name: hexo4 namespace: default resourceVersion: &quot;48479&quot; selfLink: /api/v1/namespaces/default/services/hexo4 uid: e1c9c30b-d3b8-4625-9817-a14ea3677bd3spec: clusterIP: 10.97.151.174 ports: - port: 4000 protocol: TCP targetPort: 4000 selector: run: hexo4 sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125;(anaconda3) (base) 5.4 kubectl edit service hexo4 1kubectl edit service hexo4 type: ClusterIP --&gt; NodePort 且 spec.ports 增加 nodePort: 30001 123456789101112spec: clusterIP: 10.107.236.48 externalTrafficPolicy: Cluster ports: - nodePort: 30001 port: 4000 protocol: TCP targetPort: 4000 selector: run: hexo4 sessionAffinity: None type: NodePort 5.5 build hexo5, deploy ervice create deployment hexo5 1kubectl run --image=blair101/ubuntu-hexo-blog:v1.5 hexo5 --port=4000 --env=&quot;DOMAIN=cluster&quot; create service hexo5 1kubectl expose deployment hexo5 --port=4000 --name=hexo5 kubectl edit service hexo5 type: ClusterIP --&gt; NodePort 且 spec.ports 增加 nodePort: 30001 5.6 modify service hexo4 更改 selector.run: hexo4 为 hexo5 123456789101112spec: clusterIP: 10.107.236.48 externalTrafficPolicy: Cluster ports: - nodePort: 30001 port: 4000 protocol: TCP targetPort: 4000 selector: run: hexo5 sessionAffinity: None type: NodePort 接下来是一些 delete 操作. Reference kubernetes.io Kubernetes with Minikube Install Minikube Github kubernetes Kubernetes 项目· Docker —— 从入门到实践 - yeasy 从0到1使用Kubernetes系列 知乎： Kubernetes 是什么？ 郑建勋（jonson）K8S 官网 k8s 概念 &amp; 对象 Docker 用户使用 kubectl 命令指南 Get a Shell to a Running Container Label， Deployment， Service 和 健康检查","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.iequa.com/tags/Kubernetes/"}]},{"title":"Tech Stack","slug":"devops/tech_stack","date":"2019-10-28T09:11:21.000Z","updated":"2021-06-22T06:33:48.701Z","comments":true,"path":"2019/10/28/devops/tech_stack/","link":"","permalink":"http://www.iequa.com/2019/10/28/devops/tech_stack/","excerpt":"Tech Stack","text":"Tech Stack 1. Python env conda python &gt; 3.6 2. Data Process numpy pandas scikit-learn scipy matplotlib 3. Deep Learning tensorflow pytorch keras 4. NLP nltk spacy bert 5. System Dev 5.1 Dev tensorflow serving flask flask-restplus pytest pyinvoke gunicorn 5.2 Libraries pydash dotenv fabric pathlib 5.2 Tools Cyberduck 6. System Test pytest nose locust mitmproxy 7. Log Collection fluentd elastic 8. CI/CD gitlab docker kubernetes Reference","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Stack","slug":"Stack","permalink":"http://www.iequa.com/tags/Stack/"}]},{"title":"Fluentd tutorial","slug":"devops/fluentd_td-agent","date":"2019-10-23T09:11:21.000Z","updated":"2021-06-22T06:31:05.536Z","comments":true,"path":"2019/10/23/devops/fluentd_td-agent/","link":"","permalink":"http://www.iequa.com/2019/10/23/devops/fluentd_td-agent/","excerpt":"Fluentd","text":"Fluentd 1. Fluentd what? Fluentd is an open source data collector for unified logging layer. Fluentd allows you to unify data collection and consumption for a better use and understanding of data. 2. Fluentd docker Docker 提供很多 logging driver，默认下用的 json-file，docker logs 看到的日志就是来自于这些json文件. 当有多个docker host的时候你会希望能够把日志汇集起来，集中存放到一处. 本文讲的是如何通过 fluentd logging driver 配合 fluentd 来达成这一目标。 Target： 将standalone容器打到stdout/stderror的日志收集起来 收集的日志根据容器名分开存储 日志文件根据每天滚动 2.1 配置 Fluentd 实例 fluent.conf 12345678910111213141516171819202122&lt;source&gt; @type forward&lt;/source&gt; &lt;match *&gt; @type file path /fluentd/log/$&#123;tag&#125;/$&#123;tag&#125; append true &lt;format&gt; @type single_value message_key log &lt;/format&gt; &lt;buffer tag,time&gt; @type file timekey 1d timekey_wait 10m flush_mode interval flush_interval 30s &lt;/buffer&gt;&lt;/match&gt; 新建目录 container-logs， 设置 777 权限 1234567# ~/ghome/home/ub [17:57:07]➜ lltotal 0drwxrwxrwx 5 blair staff 160B Oct 24 17:50 container-logsdrwxr-xr-x 4 blair staff 128B Oct 23 18:06 fluentd(base)# ~/ghome/home/ub [17:57:07] 启动Fluentd实例，这里使用的Docker方式： 123456docker run -it \\ -d \\ -p 24224:24224 \\ -v /Users/blair/ghome/home/ub/fluentd/conf/fluent.conf:/fluentd/etc2/fluent.conf \\ -v /Users/blair/ghome/home/ub/container-logs:/fluentd/log \\ fluent/fluentd:v1.7.3-debian-1.0 2.2 指定 logging driver 1234567docker run \\ -d \\ --log-driver=fluentd \\ --log-opt fluentd-address=&lt;fluentdhost&gt;:24224 \\ --log-opt mode=non-blocking \\ --log-opt tag=&#123;&#123;.Name&#125;&#125; \\ --rm -p 4000:4000 blair101/ubuntu-hexo-blog:v1.4 &lt;fluentdhost&gt; 参数不写则是指本机. 2.3 观察日志 12345678# ~/ghome/home/ub [18:00:10]➜ ll container-logstotal 16-rw-r--r-- 1 blair staff 470B Oct 24 17:47 data.b595a4ec02f220b1ce9d68e14ca1fc303.log-rw-r--r-- 1 blair staff 74B Oct 24 17:47 data.b595a4ec02f220b1ce9d68e14ca1fc303.log.metalrwxrwxrwx 1 blair staff 55B Oct 24 17:47 data.log -&gt; /fluentd/log/data.b595a4ec02f220b1ce9d68e14ca1fc303.log(base)# ~/ghome/home/ub [18:00:12] 3. Fluentd docker + nginx 本机 /tmp/fluentd/etc 下创建fluentd.conf 12345678910&lt;source&gt;@type forward&lt;/source&gt;&lt;match *&gt; @type file path /fluentd/log/app.log append true&lt;/match&gt; 3.1 Fluentd Container 12345docker run -d \\-p 24224:24224 \\-v /tmp/fluentd/etc:/fluentd/etc -e FLUENTD_CONF=fluentd.conf \\-v /tmp/container-logs:/fluentd/log \\fluent/fluentd 查看日志 1docker logs 2e6d50875a07 3.2 Nginx Container 1docker run -d --log-driver fluentd --log-opt fluentd-address=localhost:24224 --log-opt tag=&quot;nginx-test&quot; --log-opt fluentd-async-connect --name nginx-test -p 9080:80 nginx 3.3 Curl test 1curl -X GET http://localhost:9080 3.4 查看日志 123456789# /tmp/container-logs/app.log [19:02:53]➜ cat buffer.b595a5b11618618c6459a69df40de5e79.log2019-10-24T10:42:36+00:00 nginx-test &#123;&quot;container_id&quot;:&quot;53929d5422b0d74cd47f9fa3f6a21e63dd0a559b570631034118b4185435f266&quot;,&quot;container_name&quot;:&quot;/nginx-test&quot;,&quot;source&quot;:&quot;stdout&quot;,&quot;log&quot;:&quot;172.17.0.1 - - [24/Oct/2019:10:42:36 +0000] \\&quot;GET / HTTP/1.1\\&quot; 200 612 \\&quot;-\\&quot; \\&quot;curl/7.55.1\\&quot; \\&quot;-\\&quot;&quot;&#125;2019-10-24T10:46:04+00:00 nginx-test &#123;&quot;container_id&quot;:&quot;35dcb86564324032d5a4736d5e7bcfe78e6c2ead1408b9be85c60ac113d0de43&quot;,&quot;container_name&quot;:&quot;/nginx-test&quot;,&quot;source&quot;:&quot;stdout&quot;,&quot;log&quot;:&quot;172.17.0.1 - - [24/Oct/2019:10:46:04 +0000] \\&quot;GET / HTTP/1.1\\&quot; 200 612 \\&quot;-\\&quot; \\&quot;curl/7.55.1\\&quot; \\&quot;-\\&quot;&quot;&#125;2019-10-24T10:47:03+00:00 nginx-test &#123;&quot;log&quot;:&quot;172.17.0.1 - - [24/Oct/2019:10:47:03 +0000] \\&quot;GET / HTTP/1.1\\&quot; 200 612 \\&quot;-\\&quot; \\&quot;curl/7.55.1\\&quot; \\&quot;-\\&quot;&quot;,&quot;container_id&quot;:&quot;35dcb86564324032d5a4736d5e7bcfe78e6c2ead1408b9be85c60ac113d0de43&quot;,&quot;container_name&quot;:&quot;/nginx-test&quot;,&quot;source&quot;:&quot;stdout&quot;&#125;2019-10-24T10:48:12+00:00 nginx-test &#123;&quot;log&quot;:&quot;172.17.0.1 - - [24/Oct/2019:10:48:12 +0000] \\&quot;GET / HTTP/1.1\\&quot; 200 612 \\&quot;-\\&quot; \\&quot;curl/7.55.1\\&quot; \\&quot;-\\&quot;&quot;,&quot;container_id&quot;:&quot;35dcb86564324032d5a4736d5e7bcfe78e6c2ead1408b9be85c60ac113d0de43&quot;,&quot;container_name&quot;:&quot;/nginx-test&quot;,&quot;source&quot;:&quot;stdout&quot;&#125;(base)# /tmp/container-logs/app.log [19:02:55]➜ 4. Fluent-Logger-Python fluentd.conf 1234567&lt;source&gt; @type forward port 24224&lt;/source&gt;&lt;match fluentd.test.**&gt; @type stdout&lt;/match&gt; Please restart your agent once these lines are in place. 123456docker run --rm \\-d \\-p 24224:24224 \\-v /tmp/fluentd/etc:/fluentd/etc -e FLUENTD_CONF=fluentd.conf \\-v /tmp/container-logs:/fluentd/log \\fluent/fluentd 4.1 Event-Based Interface First, install the fluent-logger library via pip. 1pip install fluent-logger Next, initialize and post the records as shown below. 12345678# test.pyfrom fluent import senderfrom fluent import eventsender.setup(&#x27;fluentd.test&#x27;, host=&#x27;localhost&#x27;, port=24224)event.Event(&#x27;follow&#x27;, &#123; &#x27;from&#x27;: &#x27;userA&#x27;, &#x27;to&#x27;: &#x27;userB&#x27;&#125;) Executing the script will send the logs to Fluentd 12# /tmp/fluentd/etc [14:21:01]➜ python test.py show docker fluentd log 12# /tmp/fluentd/etc [14:21:11]➜ docker logs cda923986a28 1234562019-10-25 06:19:57 +0000 [info]: parsing config file is succeeded path=&quot;/fluentd/etc/fluentd.conf&quot;.........2019-10-25 06:21:11.000000000 +0000 fluentd.test.follow: &#123;&quot;from&quot;:&quot;userA&quot;,&quot;to&quot;:&quot;userB&quot;&#125;(anaconda3) (base) #fluentsender-interface 4.2 FluentSender Interface fluentd.conf 123456789101112&lt;source&gt; @type forward port 24224&lt;/source&gt;&lt;match fluentd.test.**&gt; @type stdout&lt;/match&gt;&lt;match app.**&gt; type stdout&lt;/match&gt; app.py 1234567891011121314151617181920212223from fluent import senderfrom fluent import eventimport time&#x27;&#x27;&#x27; FluentSender Interface sender.FluentSender is a structured event logger for Fluentd. By default, the logger assumes fluentd daemon is launched locally. You can also specify remote logger by passing the options.&#x27;&#x27;&#x27;# for local fluentlogger = sender.FluentSender(&#x27;app&#x27;)# for remote fluentlogger = sender.FluentSender(&#x27;app&#x27;, host=&#x27;localhost&#x27;, port=24224)# Specify optional timecur_time = int(time.time())logger.emit_with_time(&#x27;follow&#x27;, cur_time, &#123;&#x27;from&#x27;: &#x27;userA&#x27;, &#x27;to&#x27;:&#x27;userB&#x27;&#125;) 4.3 Handler for buffer overflow You can inject your own custom proc to handle buffer overflow in the event of connection failure. This will mitigate the loss of data instead of simply throwing data away. 12345678910111213141516171819202122232425262728293031323334from fluent import senderfrom fluent import eventimport timeimport msgpackfrom io import BytesIOdef overflow_handler(pendings): unpacker = msgpack.Unpacker(BytesIO(pendings)) for unpacked in unpacker: print(unpacked)# for local fluentlogger = sender.FluentSender(&#x27;app&#x27;)# for remote fluentlogger = sender.FluentSender(&#x27;app&#x27;, host=&#x27;localhost&#x27;, port=24224)# Specify optional timecur_time = int(time.time())# logger.emit_with_time(&#x27;follow&#x27;, cur_time, &#123;&#x27;from&#x27;: &#x27;userA&#x27;, &#x27;to&#x27;:&#x27;userB&#x27;&#125;)# Use nanosecondlogger = sender.FluentSender(&#x27;app&#x27;, nanosecond_precision=True)logger = sender.FluentSender(&#x27;app&#x27;, host=&#x27;localhost&#x27;, port=24224, buffer_overflow_handler=overflow_handler)logger.emit(&#x27;follow&#x27;, &#123;&#x27;from&#x27;: &#x27;userA&#x27;, &#x27;to&#x27;: &#x27;userB&#x27;&#125;)logger.emit_with_time(&#x27;follow&#x27;, time.time(), &#123;&#x27;from&#x27;: &#x27;userA&#x27;, &#x27;to&#x27;: &#x27;userB&#x27;&#125;)logger.close() 4.4 Python logging.Handler interface This client-library also has FluentHandler class for Python logging module. 1234567891011121314151617181920212223242526272829303132import loggingfrom fluent import handlerimport msgpackfrom io import BytesIOcustom_format = &#123; &#x27;host&#x27;: &#x27;%(hostname)s&#x27;, &#x27;where&#x27;: &#x27;%(module)s.%(funcName)s&#x27;, &#x27;type&#x27;: &#x27;%(levelname)s&#x27;, &#x27;stack_trace&#x27;: &#x27;%(exc_text)s&#x27;&#125;logging.basicConfig(level=logging.INFO)def overflow_handler(pendings): unpacker = msgpack.Unpacker(BytesIO(pendings)) for unpacked in unpacker: print(unpacked)l = logging.getLogger(&#x27;fluent.test&#x27;)h = handler.FluentHandler(&#x27;app.follow&#x27;, host=&#x27;localhost&#x27;, port=24224, buffer_overflow_handler=overflow_handler)formatter = handler.FluentRecordFormatter(custom_format)h.setFormatter(formatter)l.addHandler(h)l.info(&#123; &#x27;from&#x27;: &#x27;userA&#x27;, &#x27;to&#x27;: &#x27;userB&#x27;&#125;)l.info(&#x27;&#123;&quot;from&quot;: &quot;userC&quot;, &quot;to&quot;: &quot;userD&quot;&#125;&#x27;)l.info(&quot;This log entry will be logged with the additional key: &#x27;message&#x27;.&quot;) 使用步骤总结： FluentHandler (并可设置 formatter) logging instance , l.addHandler(h) 自定义格式化程序 你还可以通过 logging.config.dictConfig 自定义格式化程序 123import logging.configimport yamlwithopen(&#x27;logging.yaml&#x27;) as fd: conf = yaml.load(fd)logging.config.dictConfig(conf[&#x27;logging&#x27;]) 你可以在连接失败时插入自己的自定义过程来处理缓冲区溢出。 这将减少数据的丢失，而不是简单地丢弃数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344logging: version: 1 formatters: brief: format: &#x27;%(message)s&#x27; default: format: &#x27;%(asctime)s %(levelname)-8s %(name)-15s %(message)s&#x27; datefmt: &#x27;%Y-%m-%d %H:%M:%S&#x27; fluent_fmt: &#x27;()&#x27;: fluent.handler.FluentRecordFormatter format: level: &#x27;%(levelname)s&#x27; hostname: &#x27;%(hostname)s&#x27; where: &#x27;%(module)s.%(funcName)s&#x27; handlers: console: class : logging.StreamHandler level: DEBUG formatter: default stream: ext://sys.stdout fluent: class: fluent.handler.FluentHandler host: localhost port: 24224 tag: app.follow buffer_overflow_handler: overflow_handler formatter: fluent_fmt level: DEBUG none: class: logging.NullHandler loggers: amqp: handlers: [none] propagate: False conf: handlers: [none] propagate: False &#x27;&#x27;: # root logger handlers: [console, fluent] level: DEBUG propagate: False Reference docs.fluentd.org fluentd docker Fluentd入门教程 fluentd之mac install &amp; test fluentd-不負責任的學習筆記 使用Fluentd收集Docker容器日志 fluentd收集docker容器日志 Docker安装Fluentd并管理 Docker 日志 fluent-logger-python github.com/fluent/fluent-logger-python fluent-logger-python, 用於Fluentd的結構化記錄器( python ) fluentd中buffer配置 fluentd插件介绍","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Fluentd","slug":"Fluentd","permalink":"http://www.iequa.com/tags/Fluentd/"}]},{"title":"Developer's manual","slug":"devops/developer-manual","date":"2019-10-20T09:11:21.000Z","updated":"2021-06-22T06:31:05.521Z","comments":true,"path":"2019/10/20/devops/developer-manual/","link":"","permalink":"http://www.iequa.com/2019/10/20/devops/developer-manual/","excerpt":"Developer's manual","text":"Developer's manual 阮一峰: Docker 入门教程 阮一峰: Docker 微服务教程 阮一峰: developer 手册 阮一峰: RESTful API 最佳实践 阮一峰: RESTful API 设计指南 阮一峰: MVC，MVP 和 MVVM 的图示 阮一峰: Linux的五个查找命令 阮一峰: curl 的用法指南 CoolShell: 打造高效的工作环境 – SHELL 篇 CoolShell: 记一次KUBERNETES/DOCKER网络排障 CoolShell: 程序员技术练级攻略 廖雪峰的官方网站 devops 阮一峰: awk 入门教程 阮一峰: xargs 命令教程 阮一峰: Vim 配置入门 阮一峰: 命令行通配符教程 阮一峰: 为什么文件名要小写？ 阮一峰: YAML 语言教程 阮一峰: Linux Server的初步配置流程 阮一峰: 读懂diff Steve Yegge 程序员的呐喊 other 我的Tweet档案 感谢廖雪峰帮助 陈皓读过的书(72)","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"developer","slug":"developer","permalink":"http://www.iequa.com/tags/developer/"}]},{"title":"怎么理解 Python3 的 cmp_to_key函数？","slug":"python/language/py3_sort_vs_sorted","date":"2019-10-20T02:07:21.000Z","updated":"2021-06-22T06:52:37.034Z","comments":true,"path":"2019/10/20/python/language/py3_sort_vs_sorted/","link":"","permalink":"http://www.iequa.com/2019/10/20/python/language/py3_sort_vs_sorted/","excerpt":"","text":"12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-&quot;&quot;&quot; @file: tt_sort.py @date: 2020-10-20 10:41 AM&quot;&quot;&quot;import copyfrom functools import cmp_to_keydef rule(a, b): if a &gt; b: return 1 if a &lt; b: return -1 return 0L = [7, 4, 8, 2, 9, 6]L2 = copy.copy(L)L.sort(key=cmp_to_key(rule))print(sorted(L2, key=cmp_to_key(rule)))print(L)# print(L2)print(&#x27;&#x27;.join([str(num) for num in L2])) Reference python sort vs sorted python的getattr（）函数 python类的继承 为Python类使用”get函数”有什么好处？","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python3","slug":"python3","permalink":"http://www.iequa.com/tags/python3/"}]},{"title":"Docker + Hexo","slug":"devops/docker-hexo","date":"2019-10-19T09:11:21.000Z","updated":"2021-06-22T06:31:05.511Z","comments":true,"path":"2019/10/19/devops/docker-hexo/","link":"","permalink":"http://www.iequa.com/2019/10/19/devops/docker-hexo/","excerpt":"Docker","text":"Docker 为了更好的引出 Docker, 先简单介绍下 Linux 容器: 1. docker what? Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。 2. dockerfile 1234567891011FROM blair101/ubuntu-hexo-blog:v1.3COPY ./source /blog/sourceWORKDIR /blog#RUN hexo sEXPOSE 4000CMD hexo s 3. markdown 适配渲染问题 1234npm uninstall hexo-renderer-markdown-itnpm un hexo-renderer-markednpm install hexo-renderer-markdown-it-plus --savenpm install hexo-renderer-kramed --save Reference hexo.io, hexo-server 阮一峰: Docker 入门教程 阮一峰: Docker 微服务教程 荒野之萍: Docker 最简教程 荒野之萍: Hexo+Github-Dockerfile自动搭建 Docker 容器从入门到入魔 plugins: Hexo + hexo-asset-image Hexo + hexo-deployer-git Hexo + WordCount Hexo + sitemap、rss 標籤外掛（Tag Plugins）","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.iequa.com/tags/Docker/"}]},{"title":"Docker microservices","slug":"devops/docker-wordpress-tutorial","date":"2019-10-14T09:11:21.000Z","updated":"2021-06-22T06:31:05.527Z","comments":true,"path":"2019/10/14/devops/docker-wordpress-tutorial/","link":"","permalink":"http://www.iequa.com/2019/10/14/devops/docker-wordpress-tutorial/","excerpt":"Docker Microservices","text":"Docker Microservices 站在 Docker 的角度，Software is the combination of Containers： 业务逻辑容器 数据库容器 储存容器 … Docker 使得软件可以拆分成若干个标准化容器，然后像搭积木一样组合起来。 这正是微服务（microservices）的思想： 软件把任务外包出去，让各种外部服务完成这些任务，软件本身只是底层服务的调度中心和组装层。 如何在一台计算机上实现多个 Services，让它们互相配合，组合出一个 Application: Docker Microservices 为了加深理解，采用三种方法，演示如何架设 WordPress 网站 方法 A：自建 WordPress 容器 方法 B：采用官方的 WordPress 容器 方法 C：采用 Docker Compose 工具 1. 自建 WordPress Container 方法 A：自建 WordPress Container 1.1 官方 PHP image 123456docker container run \\ -p 8080:80 \\ -it \\ --rm \\ --name wordpress \\ --volume &quot;$PWD/&quot;:/var/www/html php:5.6-apache 1.2 拷贝 WordPress 安装包 12$ wget https://cn.wordpress.org/wordpress-4.9.4-zh_CN.tar.gz$ tar -xvf wordpress-4.9.4-zh_CN.tar.gz 1.3 官方 MySQL Container 1234567docker container run \\ -d \\ --rm \\ --name wordpressdb \\ --env MYSQL_ROOT_PASSWORD=123456 \\ --env MYSQL_DATABASE=wordpress \\ mysql:5.7 查看正在运行的容器 123456➜ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES228857116d2d mysql:5.7 &quot;docker-entrypoint.s…&quot; 4 minutes ago Up 4 minutes 3306/tcp, 33060/tcp wordpressdba4c7f3d045a3 php:5.6-apache &quot;docker-php-entrypoi…&quot; 23 minutes ago Up 23 minutes 0.0.0.0:8080-&gt;80/tcp wordpress(anaconda3) (base)➜ 其中，wordpressdb是后台运行的，前台看不见它的输出，必须使用下面的命令查看 1docker container logs wordpressdb 1.4 定制 PHP Container PHP 的官方 image 不带有mysql扩展，必须自己新建 image 文件 1docker container stop wordpress Create Dockerfile in docker-demo dir 123FROM php:5.6-apacheRUN docker-php-ext-install mysqliCMD apache2-foreground 基于这个 Dockerfile 文件，新建一个名为 phpwithmysql 的 image 文件. 1docker build -t phpwithmysql . 1.5 WordpresspC 连接 MySQL 123456docker container run \\ --rm \\ --name wordpress \\ --volume &quot;$PWD/&quot;:/var/www/html \\ --link wordpressdb:mysql \\ phpwithmysql WordPressC 要连到 wordpressdbC，冒号表示该 Container 的别名是 mysql . wp 看到以上界面，自建WPC 演示完毕。 关闭 Containers。 1➜ docker container stop wordpress wordpressdb 2. Official WordPress Container 方法 B：官方 WordPress Container 2.1 mysql container 1234567docker container run \\ -d \\ --rm \\ --name wordpressdb \\ --env MYSQL_ROOT_PASSWORD=123456 \\ --env MYSQL_DATABASE=wordpress \\ mysql:5.7 2.2 wordpress container 12345678➜ docker container run \\ -p 8080:80 \\ -d \\ --rm \\ --name wordpress \\ --env WORDPRESS_DB_PASSWORD=123456 \\ --link wordpressdb:mysql \\ wordpress -p 127.0.0.1:8080:80：将容器的 80 端口 映射到 127.0.0.2 的 8080 端口 –volume “$PWD/wordpress”:/var/www/html：将容器的/var/www/html目录映射到当前目录的wordpress子目录 2.3 docker container ls 12345➜ docker container ls --allCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES99e93c4dfc7c wordpress &quot;docker-entrypoint.s…&quot; 2 seconds ago Up 1 second 80/tcp wordpress373fcc5e1b94 mysql:5.7 &quot;docker-entrypoint.s…&quot; 2 minutes ago Up 2 minutes 3306/tcp, 33060/tcp wordpressdb(anaconda3) (base) 浏览器 http://localhost:8080 就能看到 WordPress 的安装提示. 2.4 stop containers 1➜ docker container stop wordpress wordpressdb 3. Docker Compose Tool Compose 一个工具软件, 可以管理多个 Docker 容器组成一个应用. 定义 docker-compose.yml，写好多容器的调用关系。然后，只要一个命令，就能同时启动/关闭这些容器. 1docker-compose --version 3.1 WordPress example 12345678910111213141516mysql: image: mysql:5.7 environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=wordpressweb: image: wordpress links: - mysql environment: - WORDPRESS_DB_PASSWORD=123456 ports: - &quot;127.0.0.3:8080:80&quot; working_dir: /var/www/html volumes: - wordpress:/var/www/html 启动 2个 Container 1docker-compose up 关闭 2个 Container 1docker-compose stop 关闭以后，这两个容器文件还是存在的，写在里面的数据不会丢失。下次启动的时候，还可以复用。下面的命令可以把这两个容器文件删除（容器必须已经停止运行）。 1docker-compose rm Reference 阮一峰: Docker 入门教程 阮一峰: Docker 微服务教程 阮一峰: developer 手册 荒野之萍: Docker最简教程 Hexo+Github博客最简教程-Dockerfile自动搭建","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.iequa.com/tags/Docker/"}]},{"title":"Docker tutorial","slug":"devops/docker-tutorial","date":"2019-10-07T09:11:21.000Z","updated":"2021-06-22T06:31:05.524Z","comments":true,"path":"2019/10/07/devops/docker-tutorial/","link":"","permalink":"http://www.iequa.com/2019/10/07/devops/docker-tutorial/","excerpt":"Docker","text":"Docker 由于环境配置的难题，所以开发者常常会说：It works on my machine 为了更好的引出 Docker, 先简单介绍下 Linux 容器: Linux Container 不是模拟一个完整的 OS，而是对进程进行隔离。或者说，在正常进程的外面套了一个保护层。对于 Container 里面的进程来说，它接触到的各种资源都是 virtual，从而实现与底层系统的隔离。 Container 是进程级别的，相比 virtual machine 有很多优势。 Container 有点像轻量级的 virtual machine，能够提供 virtual_env，但是成本开销小得多 1. docker what? Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。 Docker 将 Application 与该程序的依赖，打包在一个 image_file 里面。运行这个文件，就会生成一个虚拟容器。 程序在这个 Container 里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。 2. docker function 提供一次性的环境 提供弹性的云服务 组建微服务架构 3. docker install Install Docker Desktop on Mac 123$ docker version# 或者$ docker info 4. docker image file Docker 把 Application 及依赖，打包在 image file 里. 只有通过这个 image file，才能生成 Docker Container, image file 可以看作是 template of container. Docker 根据 image file 生成 instance of container. 同一 image 文件，可以生成多个同时运行的 instance of container. 12345# 列出本机的所有 image 文件。$ docker image ls# 删除 image 文件$ docker image rm [imageName] image 文件是通用的。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。 为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 Docker Hub 是最重要、最常用的 image 仓库。 5. example：hello world 我们通过最简单的 image 文件&quot;hello world&quot;，感受一下 Docker (1). 将 image 文件从仓库抓取到本地 123$ docker image pull library/hello-world#or$ docker image pull hello-world (2). 本机看到这个 image 文件 1$ docker image ls (3). 运行这个 image 文件 1$ docker container run hello-world 运行成功: 123456$ docker container run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.... ... 输出这段提示以后，hello world就会停止运行，容器自动终止。 (4). 手动终止容器 对于那些不会自动终止的容器，必须使用 docker container kill 手动终止. 1$ docker container kill [containID] 6. container file image file 生成的 instance of container，本身也是一个文件，称为 container file。 也就是说，一旦 container 生成，就会同时存在 2 files： image file 和 container file。 且 close container 并不会 delete container file，只是 container stop run. 12345# 列出本机正在运行的容器$ docker container ls# 列出本机所有容器，包括终止运行的容器$ docker container ls --all docker 的常用命令: 12345678docker --version #查看Docker版本docker info #查看Docker安装有关的所有细节信息docker version #查看Docker安装有关的所有细节信息docker image ls #列出镜像清单docker container ls #列出容器清单（列出运行中的容器）docker container ls --all #列出容器清单（列出所有容器）docker container ls --aq #列出容器清单（列出所有容器，简单模式，只有容器ID）docker run hello-world #执行Docker镜像，镜像名字为hello-world 7. Dockerfile 学会使用 image 文件以后，接下来的问题就是，如何生成 image 文件？ 这需要用到 Dockerfile 文件。它是一个文本文件，用来配置 image。 Docker 根据 Dockerfile 生成二进制的 image file . 8. Custom docker container 8.1 make Dockerfile 123.gitnode_modulesnpm-debug.log 8.2 create image 8.3 generate Container 8.4 CMD 8.5 Release image 在 hub.docker.com 或 cloud.docker.com 注册一个账户。然后，用下面的命令登录。 1$ docker login 接着，为本地的 image 标注用户名和版本： 123$ docker image tag [imageName] [username]/[repository]:[tag]# 实例$ docker image tag koa-demos:0.0.1 ruanyf/koa-demos:0.0.1 也可以不标注用户名，重新构建一下 image 文件也是可以的: 1$ docker image build -t [username]/[repository]:[tag] . 最后，发布 image 文件: 1$ docker image push [username]/[repository]:[tag] 9. other docker command docker 的主要用法就是上面这些，此外还有几个命令，也非常有用 12345docker container start [containerID]docker container stop [containerID]docker container logs [containerID]docker container exec [containerID]docker container cp [containerID] 9.1 container start 命令是新建容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。 1$ docker container start [containerID] 9.2 container stop docker container kill 命令终止容器运行，相当于向容器里面的主进程发出 SIGKILL 信号。 1$ docker container kill [containerID] docker container stop 命令也是用来终止容器运行，相当于向容器里面的主进程发出 SIGTERM 信号，然后过一段时间再发出 SIGKILL 信号 1$ docker container stop [containerID] 9.3 container logs docker container logs 命令用来查看 docker 容器的输出，即容器里面 Shell 的标准输出。 1$ docker container logs [containerID] 9.4 container exec docker container exec 命令用于进入一个正在运行的 docker Container. 1$ docker container exec -it [containerID] /bin/bash 9.5 container cp docker container cp 命令用于从正在运行的 Docker 容器里面，将文件拷贝到本机. 1$ docker container cp [containID]:[/path/to/file] . Reference 阮一峰: Docker 入门教程 阮一峰: Docker 微服务教程 Dockerfile文件详解 Docker 容器从入门到入魔 GO语言、DOCKER 和新技术","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.iequa.com/tags/Docker/"}]},{"title":"Python WSGI 协议详解","slug":"python/WSGI","date":"2019-10-06T01:11:21.000Z","updated":"2021-06-22T06:48:57.965Z","comments":true,"path":"2019/10/06/python/WSGI/","link":"","permalink":"http://www.iequa.com/2019/10/06/python/WSGI/","excerpt":"WSGI","text":"WSGI Web应用程序的本质: User 通过 浏览器 访问 互联网上指定的 网页文件 展示到浏览器上。 WSGI 技术角度，以下3个步骤： 浏览器，将要请求的内容按照HTTP协议发送服务端 服务端，根据请求内容找到指定的HTML页面 浏览器，解析请求到的HTML内容展示出来 WEB开发——Python WSGI协议详解 1. Web DEV 静态开发 动态开发 动态开发 CGI WSGI CGI 流程 CGI 流程 WSGI 流程 WSGI 流程 2. What’s WSGI WSGI全称是Web Server Gateway Interface，其主要作用是Web服务器与Python Web应用程序或框架之间的建议标准接口，以促进跨各种Web服务器的Web应用程序可移植性。 WSGI 协议 分成三个组件 Application、Server、Middleware 和 协议中传输的内容。 Application：Django，Flask等 Server：常用的有uWSGI，gunicorn等 Middleware： Flask等框架中的装饰器 2.1 Application 应用程序，是一个可重复调用的可调用对象，在Python中可以是一个函数，也可以是一个类，如果是类的话要实现__call__方法，要求这个可调用对象接收2个参数，返回一个内容结果。 2.2 Server Web服务器，主要是实现相应的信息转换，将网络请求中的信息，按照HTTP协议将内容拿出，同时按照WSGI协议组装成新的数据，同时将提供的start_response传递给Application。最后接收Application返回的内容，按照WSGI协议解析出。最终按照HTTP协议组织好内容返回就完成了一次请求。 2.3 Middleware Middleware 中间件，可以理解为对应用程序的一组装饰器。 在 Application 端看来，它可以提供一个类start_response函数，可以像start_response函数一样接收HTTP STATU和Headers；和environ。 在 Server 看来，他可以接收2个参数，并且可以返回一个类 Application对象。 2.4 总结 WSGI 对于 application 对象有如下三点要求 必须是一个可调用的对象 接收两个必选参数 environ、start_response。 返回值必须是可迭代对象，用来表示 http body。 3. Python Web 应用是什么? 一个 Python Web 应用包含两个部分： 应用的开发：实现 Web 应用的逻辑，读数据库，拼装页面，用户登录 应用的部署：将 Web 应用跑起来，多线程，多进程，异步，监听端口，所有服务器该做的事情 我们希望用各种不同的技术来开发应用，用各种不同的服务器程序来跑应用. 这就要求 开发 和 部署 遵循统一的 通信接口，这接口叫 Web Server Gateway Interface，简称“WSGI”。 WSGI 接口 就是一个满足特定要求的函数 1234567def application(environ, start_response): “”“ environ：包含所有 HTTP 请求信息的 dict start_response：发送 HTTP 响应的函数 ”“” start_response(&#x27;200 OK&#x27;, [(&#x27;Content-Type&#x27;, &#x27;text/html&#x27;)]) return &#x27;&lt;h1&gt;Hello, web!&lt;/h1&gt;&#x27; WSGI 完整说明：PEP 333 : Python Web Server Gateway Interface v1.0.1 WSGI 简单说明： 廖雪峰的官方网站 Py Web 应用概览：The Hitchhiker’s Guide To Python 4. Flask Web 应用开发 基本形态：URL 路由，获取请求，构造响应 Flask 作为一个 Web 框架，它遵循 WSGI 接口，并且在其之上整合了 Werkzeug 的 URL 路由以及 WSGI 工具库，SQLAlchemy 的数据库访问，jinja2 的模版渲染等功能。 app 对象实现了 WSGI 接口 Flask，从简单开始 5. Flask-RESTPlus Flask-RESTPlus 是对Flask的扩展，它增加了对快速开发REST API的支持. Flask-RESTPlus 鼓励以最小的设置来实现功能的开发. Flask-RESTPlus 既包含 Flask-Restful包 的功能，又包括 swagger 文档化功能（其实是封装了swagger）. 5.1 quickstart hello 123456789from flask import Flaskfrom flask_restplus import Api# Flask实例app = Flask(__name__)# 在使用Flask-RESTPlus之前，需要通过传入Flask实例进行初始化, 进行初始化.api = Api(app) 最简单的打开 Flask-RESTPlus, API 示例: 1234567891011121314151617# file:simple_api.pyfrom flask import Flaskfrom flask_restplus import Resource, Apiapp = Flask(__name__)api = Api(app)@api.route(&#x27;/hello&#x27;)class HelloWorld(Resource): def get(self): return &#123;&#x27;hello&#x27;: &#x27;world&#x27;&#125;if __name__ == &#x27;__main__&#x27;: app.run(debug=True) # 开启了Flask的调试模式 # 生产环境绝对不要开启调试模式，因为它会使你的后台服务处于被攻击的风险之中！ run ➜ python simple_api.py , then: http://localhost:5000/hello： 123&#123;&quot;hello&quot;: &quot;world&quot;&#125; 5.2 Resourceful Routing 通过在资源上定义方法来很容易地访问多个 HTTP 方法. 12345678910111213141516171819from flask import Flask, requestfrom flask_restplus import Resource, Apiapp = Flask(__name__)api = Api(app)todos = &#123;&#125;@api.route(&#x27;/&lt;string:todo_id&gt;&#x27;)class TodoSimple(Resource): def get(self, todo_id): return &#123;todo_id: todos[todo_id]&#125; def put(self, todo_id): todos[todo_id] = request.form[&#x27;data&#x27;] return &#123;todo_id: todos[todo_id]&#125;if __name__ == &#x27;__main__&#x27;: app.run(debug=True) 5.3 Endpoints 大多数情况下，某个资源都会有多个URL。 我们可以用 route()装饰器 中传入多个URL，这样每个URL都将会路由到该资源上. 5.4 Data Formatting 123456789101112131415161718192021222324252627from flask import Flaskfrom flask_restplus import fields, Api, Resourceapp = Flask(__name__)api = Api(app)model = api.model(&#x27;Model&#x27;, &#123; &#x27;task&#x27;: fields.String, &#x27;uri&#x27;: fields.Url(&#x27;todo_ep&#x27;,absolute=True) # absolute参数表示生成的url是否是绝对路径&#125;)class TodoDao(object): def __init__(self, todo_id, task): self.todo_id = todo_id self.task = task # 该字段不会发送到响应结果中 self.status = &#x27;active&#x27;@api.route(&#x27;/todo&#x27;,endpoint=&#x27;todo_ep&#x27;)class Todo(Resource): @api.marshal_with(model) def get(self, **kwargs): return TodoDao(todo_id=&#x27;my_todo&#x27;, task=&#x27;Remember the milk&#x27;)if __name__ == &#x27;__main__&#x27;: app.run(debug=True) marshal_with()装饰器就是用来对结果按照model的结构进行转换的. flask-restplus quickstart flask-restplus example.html 5.5 Order Preservation Api全局保留：api = Api(ordered = True) Namespace全局保留：ns = Namespace(ordered=True) marshal()局部保留：return marshal(data, fields, ordered=True) 5.6 Full example 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697from flask import Flaskfrom flask_restplus import Api, Resource, fieldsfrom werkzeug.contrib.fixers import ProxyFixapp = Flask(__name__)app.wsgi_app = ProxyFix(app.wsgi_app)api = Api(app, version=&#x27;1.0&#x27;, title=&#x27;TodoMVC API&#x27;, description=&#x27;A simple TodoMVC API&#x27;,)# 定义命名空间ns = api.namespace(&#x27;todos&#x27;, description=&#x27;TODO operations&#x27;)todo = api.model(&#x27;Todo&#x27;, &#123; &#x27;id&#x27;: fields.Integer(readOnly=True, description=&#x27;The task unique identifier&#x27;), &#x27;task&#x27;: fields.String(required=True, description=&#x27;The task details&#x27;)&#125;)# DAOclass TodoDAO(object): def __init__(self): self.counter = 0 self.todos = [] def get(self, id): for todo in self.todos: if todo[&#x27;id&#x27;] == id: return todo api.abort(404, &quot;Todo &#123;&#125; doesn&#x27;t exist&quot;.format(id)) def create(self, data): todo = data todo[&#x27;id&#x27;] = self.counter = self.counter + 1 self.todos.append(todo) return todo def update(self, id, data): todo = self.get(id) todo.update(data) return todo def delete(self, id): todo = self.get(id) self.todos.remove(todo)DAO = TodoDAO()DAO.create(&#123;&#x27;task&#x27;: &#x27;Build an API&#x27;&#125;)DAO.create(&#123;&#x27;task&#x27;: &#x27;?????&#x27;&#125;)DAO.create(&#123;&#x27;task&#x27;: &#x27;profit!&#x27;&#125;)# 对 all todo 操作@ns.route(&#x27;/&#x27;)class TodoList(Resource): &#x27;&#x27;&#x27;获取所有todos元素，并允许通过POST来添加新的task&#x27;&#x27;&#x27; @ns.doc(&#x27;list_todos&#x27;) @ns.marshal_list_with(todo) def get(self): &#x27;&#x27;&#x27;返回所有task&#x27;&#x27;&#x27; return DAO.todos @ns.doc(&#x27;create_todo&#x27;) @ns.expect(todo) @ns.marshal_with(todo, code=201) def post(self): &#x27;&#x27;&#x27;创建一个新的task&#x27;&#x27;&#x27; return DAO.create(api.payload), 201# 对其中 某个id 的 实体 操作@ns.route(&#x27;/&lt;int:id&gt;&#x27;)@ns.response(404, &#x27;Todo not found&#x27;)@ns.param(&#x27;id&#x27;, &#x27;The task identifier&#x27;)class Todo(Resource): &#x27;&#x27;&#x27;获取单个todo项，并允许删除操作&#x27;&#x27;&#x27; @ns.doc(&#x27;get_todo&#x27;) @ns.marshal_with(todo) def get(self, id): &#x27;&#x27;&#x27;获取id指定的todo项&#x27;&#x27;&#x27; return DAO.get(id) @ns.doc(&#x27;delete_todo&#x27;) @ns.response(204, &#x27;Todo deleted&#x27;) def delete(self, id): &#x27;&#x27;&#x27;根据id删除对应的task&#x27;&#x27;&#x27; DAO.delete(id) return &#x27;&#x27;, 204 @ns.expect(todo) @ns.marshal_with(todo) def put(self, id): &#x27;&#x27;&#x27;更新id指定的task&#x27;&#x27;&#x27; return DAO.update(id, api.payload)if __name__ == &#x27;__main__&#x27;: app.run(debug=True) get all 1curl &quot;http://localhost:5000/todos/&quot; get id 1curl -X GET &quot;http://localhost:5000/todos/1&quot; -H &quot;accept: application/json&quot; delete: 1curl -X DELETE &quot;http://localhost:5000/todos/2&quot; -H &quot;accept: application/json&quot; Reference 花了两个星期，我终于把 WSGI 整明白了 尝试理解Flask源码 之 搞懂WSGI协议 WEB开发——Python WSGI协议详解 Flask，从简单开始 HackHan技术博客:【Flask-RESTPlus系列】Flask-RESTPlus系列译文开篇","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"WSGI","slug":"WSGI","permalink":"http://www.iequa.com/tags/WSGI/"}]},{"title":"Singapore Travel plan 🇸🇬🇸🇬","slug":"world/Singapore","date":"2019-10-05T10:46:48.000Z","updated":"2021-06-22T06:07:42.538Z","comments":true,"path":"2019/10/05/world/Singapore/","link":"","permalink":"http://www.iequa.com/2019/10/05/world/Singapore/","excerpt":"","text":"证件准备 Passport &amp; Visa @Taobao Singapore 4G流量卡 @Taobao EZ-link Card @MRT 坡图览 Day1: 市区经典（鱼尾狮公园 →） Marina Bay Sands Casino [金沙赌场娱乐城] Merlion Park [鱼尾狮公园] Gardens by the Bay [滨海湾花园] 若想看坡市区风景，建议搭乘摩天轮，每圈循环约30分钟。 海湾花园，除了能够观看新加坡著名的超级树[SuperTree]，也可以付费上空中走廊(OCBC Skywalk)，或参观云雾林[Cloud Forest]和花穹[Flower Dome]这两个超大温室。 Day2: 文化背景（克拉码头 →） Day3: 动物河川海洋 (S.E.A.海洋館 →) Day4: 圣淘沙小岛（环球影城 →） Day5: 种族文化之旅（小印度街区 →） Day6: 植物花草购物（乌节路 →） Day7: 新加坡校园（NUS、NTU →） Reference Singapore 7days","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"Singapore","slug":"Singapore","permalink":"http://www.iequa.com/tags/Singapore/"}]},{"title":"Summary 20 dynamic programming","slug":"leetcode/summary_dp","date":"2019-08-31T02:54:16.000Z","updated":"2021-06-20T04:12:28.266Z","comments":true,"path":"2019/08/31/leetcode/summary_dp/","link":"","permalink":"http://www.iequa.com/2019/08/31/leetcode/summary_dp/","excerpt":"","text":"使用动态规划解决问题一般分为三步： 表示状态 找出状态转移方程 边界处理 表示状态 分析问题的状态时，不要分析整体，只分析最后一个阶段即可！因为动态规划问题都是划分为多个阶段的，各个阶段的状态表示都是一样，而我们的最终答案在就是在最后一个阶段。 爬楼梯 climbing-stairs ， ✔️ 新建{}or[] ,滚动数组 不同路径 II ， ✔️ 编辑距离 ， ✔️ 扔鸡蛋 连续子数组的最大和 // dp: F[i] = max(a[i], F[i-1]+a[i]); Longest Palindromic Substring/最长回文子串 Edit Distance/编辑距离 Distinct Subsequences/不同子序列 Interleaving String/交错字符串 把数字翻译成字符串 Leetcode 64. Minimum Path Sum, 最小路径和 115. Distinct Subsequences I (Hard) 940. 不同的子序列 II (Hard) 不同路径 II： 如果当前没有障碍物，dp[m][n] = dp[m - 1][n] + dp[m][n - 1] 如果有障碍物，则dp[m][n] = 0 12345678910111213141516171819202122232425输入:[ [0,0,0], [0,1,0], [0,0,0]]输出: 2class Solution: def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -&gt; int: #新建矩阵版 height, width = len(obstacleGrid),len(obstacleGrid[0]) store = [[0]*width for i in range(height)] #从上到下，从左到右 for m in range(height):#每一行 for n in range(width):#每一列 if not obstacleGrid[m][n]: #如果这一格没有障碍物 if m == n == 0: #或if not(m or n) store[m][n] = 1 else: a = store[m-1][n] if m!=0 else 0 #上方格子 b = store[m][n-1] if n!=0 else 0 #左方格子 store[m][n] = a+b return store[-1][-1] 编辑距离： 1234567891011121314给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。你可以对一个单词进行如下三种操作：插入一个字符删除一个字符替换一个字符输入：word1 = &quot;horse&quot;, word2 = &quot;ros&quot;输出：3解释：horse -&gt; rorse (将 &#x27;h&#x27; 替换为 &#x27;r&#x27;)rorse -&gt; rose (删除 &#x27;r&#x27;)rose -&gt; ros (删除 &#x27;e&#x27;) 如果单词1第i+1个字符和单词2第j+1个字符相同，那么就不用操作，则DP[i + 1][j + 1] = DP[i][j]; 如果不相同,则有三种可能操作，即增，删，替换。则取这三种操作的最优值，即dp[i + 1][j + 1] = 1 + Math.min(dp[i][j], Math.min(dp[i][j + 1], dp[i + 1][j])); 12345678910111213141516171819202122232425262728293031323334class Solution: def minDistance(self, word1, word2): &quot;&quot;&quot; :type word1: str :type word2: str :rtype: int &quot;&quot;&quot; n = len(word1) m = len(word2) # 有一个字符串为空串 if n * m == 0: return n + m # dp 数组 dp = [ [0] * (m + 1) for _ in range(n + 1)] # 边界状态初始化 for i in range(n + 1): dp[i][0] = i for j in range(m + 1): dp[0][j] = j # 计算所有 DP 值 for i in range(1, n + 1): for j in range(1, m + 1): left = dp[i - 1][j] + 1 right = dp[i][j - 1] + 1 left_right = dp[i - 1][j - 1] if word1[i - 1] != word2[j - 1]: left_right += 1 dp[i][j] = min(left, right, left_right) return dp[n][m] 二维DP 布尔数组 (1). Longest Palindromic Substring/最长回文子串 ， P(i,j)=P(i+1,j−1)∧(Si==Sj)P(i,j)=P(i+1,j−1)∧(Si == Sj)P(i,j)=P(i+1,j−1)∧(Si==Sj) 12345678910111213141516171819202122class Solution: def longestPalindrome(self, s: str) -&gt; str: n = len(s) dp = [[False] * n for _ in range(n)] ans = &quot;&quot; # 枚举子串的长度 l+1 for l in range(n): sub_len = l+1 # 枚举子串的起始位置 i，这样可以通过 j=i+l 得到子串的结束位置 for i in range(n): j = i + l if j &gt;= len(s): break if sub_len == 1: dp[i][j] = True elif sub_len == 2: dp[i][j] = (s[i] == s[j]) else: dp[i][j] = (dp[i + 1][j - 1] and s[i] == s[j]) if dp[i][j] and sub_len &gt; len(ans): ans = s[i:j+1] # &quot;abcd&quot;[1:2] = b return ans (2). Interleaving String/交错字符串 给定三个字符串 s1, s2, s3, 验证 s3 是否是由 s1 和 s2 交错组成的。 12345678示例 1：输入：s1 = &quot;aabcc&quot;, s2 = &quot;dbbca&quot;, s3 = &quot;aadbbcbcac&quot;输出：true示例 2：输入：s1 = &quot;aabcc&quot;, s2 = &quot;dbbca&quot;, s3 = &quot;aadbbbaccc&quot;输出：false 解决这个问题的正确方法是动态规划。 首先如果 ∣s1∣+∣s2∣≠∣s3∣|s_1| + |s_2| \\neq |s_3|∣s1​∣+∣s2​∣​=∣s3​∣, 那 s3s_3s3​ 必然不可能由 s1s_1s1​ 和 s2s_2s2​ 交错组成。在 ∣s1∣+∣s2∣=∣s3∣|s_1| + |s_2| = |s_3|∣s1​∣+∣s2​∣=∣s3​∣时，我们可以用动态规划来求解。我们定义 f(i,j)f(i,j)f(i,j) 表示 s1s_1s1​ 的前 iii 个元素和 s2s_2s2​ 的前 jjj 个元素是否能交错组成 s3s_3s3​ 的前 i+ji + ji+j 个元素。如果 s1s_1s1​ 的第 iii 个元素和 s3s_3s3​ 的第 i+ji+ji+j 个元素相等，那么 s1s_1s1​ 的前 iii 个元素和 s2s_2s2​ 的前 jjj 个元素是否能交错组成 s3s_3s3​ 的前 i+ji+ji+j 个元素取决于 s1s_1s1​ 的前 i−1i−1i−1 个元素和 s2s_2s2​ 的前 jjj 个元素是否能交错组成 s3s_3s3​ 的前 i+j−1i + j - 1i+j−1 个元素，即此时 f(i,j)f(i, j)f(i,j) 取决于 f(i−1,j)f(i - 1, j)f(i−1,j)，在此情况下如果 f(i−1,j)f(i - 1, j)f(i−1,j) 为真，则 f(i,j)f(i, j)f(i,j) 也为真。同样的，如果 s2s_2s2​ 的第 jjj 个元素和 s3s_3s3​ 的第 i+ji + ji+j 个元素相等并且 f(i,j−1)f(i, j - 1)f(i,j−1) 为真，则 f(i,j)f(i, j)f(i,j) 也为真。于是我们可以推导出这样的动态规划转移方程： f(i,j)=[f(i−1,j) and s1(i−1)=s3(p)] or [f(i,j−1) and s2(j−1)=s3(p)]f(i, j) = [f(i - 1, j) \\, {\\rm and} \\, s_1(i - 1) = s_3(p)] \\, {\\rm or} \\, [f(i, j - 1) \\, {\\rm and} \\, s_2(j - 1) = s_3(p)] f(i,j)=[f(i−1,j)ands1​(i−1)=s3​(p)]or[f(i,j−1)ands2​(j−1)=s3​(p)] 其中 p=i+j−1p = i + j - 1p=i+j−1。边界条件为 f(0,0)=Truef(0, 0) = {\\rm True}f(0,0)=True。至此，我们很容易可以给出这样一个实现： 1234567891011121314151617class Solution: def isInterleave(self, s1: str, s2: str, s3: str) -&gt; bool: len1=len(s1) len2=len(s2) len3=len(s3) if(len1+len2!=len3): return False dp=[[False]*(len2+1) for i in range(len1+1)] dp[0][0]=True for i in range(1,len1+1): dp[i][0]=(dp[i-1][0] and s1[i-1]==s3[i-1]) for i in range(1,len2+1): dp[0][i]=(dp[0][i-1] and s2[i-1]==s3[i-1]) for i in range(1,len1+1): for j in range(1,len2+1): dp[i][j]=(dp[i][j-1] and s2[j-1]==s3[i+j-1]) or (dp[i-1][j] and s1[i-1]==s3[i+j-1]) return dp[-1][-1] (3). 剑指 Offer 46. 把数字翻译成字符串 给定一个数字，我们按照如下规则把它翻译为字符串：0 翻译成 “a” ，1 翻译成 “b”，……，11 翻译成 “l”，……，25 翻译成 “z”。一个数字可能有多个翻译。请编程实现一个函数，用来计算一个数字有多少种不同的翻译方法。 示例 1: 123输入: 12258输出: 5解释: 12258有5种不同的翻译，分别是&quot;bccfi&quot;, &quot;bwfi&quot;, &quot;bczi&quot;, &quot;mcfi&quot;和&quot;mzi&quot; 分析摘要： 要的是种类总数，并不是要的是详细列表，这类问题，常见方案是： 动态规划 分类计数： dp[i] 表示 nums[0…i] dp[i] = dp[i-1] + dp[i-2] 12345678910class Solution: def translateNum(self, num: int) -&gt; int: s = str(num) a = b = 1 for i in range(2, len(s) + 1): tmp = s[i - 2:i] c = a + b if &quot;10&quot; &lt;= tmp &lt;= &quot;25&quot; else a b = a a = c return a 数字数组 Leetcode 64. Minimum Path Sum, 最小路径和 矩阵左上角出发到右下角，只能向右或向下走，找出哪一条路径上的数字之和最小。 给定一个包含非负整数的 m x n 网格，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。 说明：每次只能向下或者向右移动一步。 示例: 12345678输入:[ [1,3,1], [1,5,1], [4,2,1]]输出: 7解释: 因为路径 1→3→1→1→1 的总和最小。 12. 不同的子序列 123456789101112131415示例 1：输入：S = &quot;rabbbit&quot;, T = &quot;rabbit&quot;输出：3解释：如下图所示, 有 3 种可以从 S 中得到 &quot;rabbit&quot; 的方案。(上箭头符号 ^ 表示选取的字母)rabbbit^^^^ ^^rabbbit^^ ^^^^rabbbit^^^ ^^^ dp[i][j] 代表 T 前 i 字符串可以由 S j 字符串组成最多个数. 12345[1, 1, 1, 1, 1, 1, 1, 1][0, 1, 1, 2, 2, 3, 3, 3][0, 0, 1, 1, 1, 1, 4, 4][0, 0, 0, 0, 1, 1, 1, 5]5 所以动态方程: 当 S[j] == T[i] , dp[i][j] = dp[i-1][j-1] + dp[i][j-1]; 当 S[j] != T[i] , dp[i][j] = dp[i][j-1] 举个例子,如示例的 Reference 知乎： [Leetcode][动态规划]相关题目汇总/分析/总结 简书： 2019 算法面试相关(leetcode)–动态规划(Dynamic Programming) CSDN leetcode DP 刷完700多题后的首次总结：LeetCode应该怎么刷？ 小白一路走来，连续刷题三年，谈谈我的算法学习经验","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"dynamic programming","slug":"dynamic-programming","permalink":"http://www.iequa.com/tags/dynamic-programming/"}]},{"title":"RNN classifier in Keras","slug":"tensorflow/keras-1.5-RNN","date":"2019-08-22T12:17:21.000Z","updated":"2021-06-20T04:12:28.323Z","comments":true,"path":"2019/08/22/tensorflow/keras-1.5-RNN/","link":"","permalink":"http://www.iequa.com/2019/08/22/tensorflow/keras-1.5-RNN/","excerpt":"RNN, Recurrent Neural Networks 进行分类（classification），采用 MNIST 数据集，用 SimpleRNN 层。","text":"RNN, Recurrent Neural Networks 进行分类（classification），采用 MNIST 数据集，用 SimpleRNN 层。 LSTM in Keras 123456789101112131415161718192021import numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import SimpleRNN, Activation, Densefrom keras.optimizers import AdamTIME_STEPS = 28 # same as the height of the imageINPUT_SIZE = 28 # same as the width of the imageBATCH_SIZE = 50BATCH_INDEX = 0OUTPUT_SIZE = 10CELL_SIZE = 50LR = 0.001 1. data pre-processing MNIST里面的图像分辨率是28×28，为用RNN，将图像理解为序列化数据。 每一行作为一个输入单元，所以输入数据大小 INPUT_SIZE = 28； 先是第1行输入，再是第2行，…，第28行输入， 这就是一张图片也就是一个序列，所以步长 TIME_STEPS = 28。 训练数据要进行 normalize，因为原始数据是 8bit 灰度图像, 所以需要除以 255。 12345678910# download the mnist to the path &#x27;~/.keras/datasets/&#x27; if it is the first time to be called# X shape (60,000 28x28), y shape (10,000, )(X_train, y_train), (X_test, y_test) = mnist.load_data()# data pre-processingX_train = X_train.reshape(-1, 28, 28) / 255. # normalizeX_test = X_test.reshape(-1, 28, 28) / 255. # normalizey_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10) 12345print(X_train.shape)print(y_train.shape)(60000, 28, 28)(60000, 10) 2. build model 1234567891011# build RNN modelmodel = Sequential()# RNN cellmodel.add(SimpleRNN( # for batch_input_shape, if using tensorflow as the backend, we have to put None for the batch_size. # Otherwise, model.evaluate() will get error. batch_input_shape=(None, TIME_STEPS, INPUT_SIZE), # Or: input_dim=INPUT_SIZE, input_length=TIME_STEPS, output_dim=CELL_SIZE, unroll=True,)) 1234567891011# output layermodel.add(Dense(OUTPUT_SIZE))model.add(Activation(&#x27;softmax&#x27;))# optimizeradam = Adam(LR)model.compile(optimizer=adam, loss=&#x27;categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_1 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_1 (Dense) (None, 10) 510 _________________________________________________________________ activation_1 (Activation) (None, 10) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 110 _________________________________________________________________ activation_2 (Activation) (None, 10) 0 ================================================================= Total params: 4,570 Trainable params: 4,570 Non-trainable params: 0 _________________________________________________________________ 设置优化方法，loss函数 和 metrics 方法之后就可以开始训练了。 每次训练的时候并不是取所有的数据，只是取 BATCH_SIZE个序列，或者称为 BATCH_SIZE 张图片，这样可以大大降低运算时间，提高训练效率。 3. training &amp; evaluate 输出 test 上的 loss 和 accuracy 结果 123456789101112131415# trainingfor step in range(4001): # data shape = (batch_num, steps, inputs/outputs) X_batch = X_train[BATCH_INDEX: BATCH_INDEX+BATCH_SIZE, :, :] Y_batch = y_train[BATCH_INDEX: BATCH_INDEX+BATCH_SIZE, :] cost = model.train_on_batch(X_batch, Y_batch) BATCH_INDEX += BATCH_SIZE BATCH_INDEX = 0 if BATCH_INDEX &gt;= X_train.shape[0] else BATCH_INDEX if step % 500 == 0: cost, accuracy = model.evaluate(X_test, y_test, batch_size=y_test.shape[0], verbose=False) print(&#x27;test cost: &#x27;, cost, &#x27;test accuracy: &#x27;, accuracy) test cost: 2.311124086380005 test accuracy: 0.0957999974489212 test cost: 1.6327736377716064 test accuracy: 0.5228999853134155 test cost: 1.3161704540252686 test accuracy: 0.559499979019165 test cost: 1.1487971544265747 test accuracy: 0.5494999885559082 test cost: 1.0471760034561157 test accuracy: 0.5713000297546387 test cost: 1.0110148191452026 test accuracy: 0.5630999803543091 test cost: 0.9520753622055054 test accuracy: 0.5877000093460083 test cost: 0.8796814680099487 test accuracy: 0.604200005531311 test cost: 0.858435869216919 test accuracy: 0.6585999727249146 Reference keras-cn、 keras.io 莫烦 Keras","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"keras","slug":"keras","permalink":"http://www.iequa.com/tags/keras/"}]},{"title":"CNN classifier in Keras","slug":"tensorflow/keras-1.4-CNN","date":"2019-08-22T07:17:21.000Z","updated":"2021-06-20T04:12:28.325Z","comments":true,"path":"2019/08/22/tensorflow/keras-1.4-CNN/","link":"","permalink":"http://www.iequa.com/2019/08/22/tensorflow/keras-1.4-CNN/","excerpt":"Convolutional Neural Networks，CNN 也是一种前馈神经网络，其特点是每层的神经元节点只响应前一层局部区域范围内的神经元（全连接网络中每个神经元节点响应前一层的全部节点）","text":"Convolutional Neural Networks，CNN 也是一种前馈神经网络，其特点是每层的神经元节点只响应前一层局部区域范围内的神经元（全连接网络中每个神经元节点响应前一层的全部节点） Convolutional Neural Network in Keras pooling Convolutional Neural Network in Keras 研究发现, 在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, pooling 就可以很好地解决这一问题. 而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析. 同时也减轻了神经网络的计算负担. 也就是说在卷集的时候, 我们不压缩长宽, 尽量保留更多信息, 压缩的工作就交给池化了,这样的一项附加工作能够很有效的提高准确性. 有了这些技术,我们就可以搭建一个 CNN. Convolutional Neural Network in Keras 1234567import numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flattenfrom keras.optimizers import Adam 数据集 MNIST 123# download the mnist to the path &#x27;~/.keras/datasets/&#x27; if it is the first time to be called# training X shape (60000, 28x28), Y shape (60000, ). test X shape (10000, 28x28), Y shape (10000, )(X_train, y_train), (X_test, y_test) = mnist.load_data() 1. data pre-processing 12345X_train = X_train.reshape(-1, 1,28, 28)/255.X_test = X_test.reshape(-1, 1,28, 28)/255.y_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10) 2. build model 123456789101112131415161718192021# Another way to build your CNNmodel = Sequential()# Conv layer 1 output shape (32, 28, 28)model.add(Convolution2D( batch_input_shape=(None, 1, 28, 28), filters=32, kernel_size=5, strides=1, padding=&#x27;same&#x27;, # Padding method data_format=&#x27;channels_first&#x27;,))model.add(Activation(&#x27;relu&#x27;))# Pooling layer 1 (max pooling) output shape (32, 14, 14)model.add(MaxPooling2D( pool_size=2, strides=2, padding=&#x27;same&#x27;, # Padding method data_format=&#x27;channels_first&#x27;,)) 再添加第二, 卷积层和池化层 123456# Conv layer 2 output shape (64, 14, 14)model.add(Convolution2D(64, 5, strides=1, padding=&#x27;same&#x27;, data_format=&#x27;channels_first&#x27;))model.add(Activation(&#x27;relu&#x27;))# Pooling layer 2 (max pooling) output shape (64, 7, 7)model.add(MaxPooling2D(2, 2, &#x27;same&#x27;, data_format=&#x27;channels_first&#x27;)) Fully connected layer 1 input shape (64 * 7 * 7) = (3136), output shape (1024) 123model.add(Flatten())model.add(Dense(1024))model.add(Activation(&#x27;relu&#x27;)) Fully connected layer 2 to shape (10) for 10 classes 12model.add(Dense(10))model.add(Activation(&#x27;softmax&#x27;)) define your optimizer 12# Another way to define your optimizeradam = Adam(lr=1e-4) keras.layers.Conv1D 1D 卷积层 (例如时序卷积) Keras Convolution1D与Convolution2D区别 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_3 (Conv2D) (None, 32, 28, 28) 832 _________________________________________________________________ activation_5 (Activation) (None, 32, 28, 28) 0 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 32, 14, 14) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 64, 14, 14) 51264 _________________________________________________________________ activation_6 (Activation) (None, 64, 14, 14) 0 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 64, 7, 7) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 3136) 0 _________________________________________________________________ dense_3 (Dense) (None, 1024) 3212288 _________________________________________________________________ activation_7 (Activation) (None, 1024) 0 _________________________________________________________________ dense_4 (Dense) (None, 10) 10250 _________________________________________________________________ activation_8 (Activation) (None, 10) 0 ================================================================= Total params: 3,274,634 Trainable params: 3,274,634 Non-trainable params: 0 _________________________________________________________________ 3. compile model 设置adam优化方法，loss函数, metrics方法来观察输出结果 1234# We add metrics to get more results you want to seemodel.compile(optimizer=adam, loss=&#x27;categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;]) 4. train model 123print(&#x27;Training ------------&#x27;)# Another way to train the modelmodel.fit(X_train, y_train, epochs=1, batch_size=64,) 5. evaluate model 123456print(&#x27;\\nTesting ------------&#x27;)# Evaluate the model with the metrics we defined earlierloss, accuracy = model.evaluate(X_test, y_test)print(&#x27;\\ntest loss: &#x27;, loss)print(&#x27;\\ntest accuracy: &#x27;, accuracy) Reference keras-cn、 keras.io 莫烦 Keras","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://www.iequa.com/tags/CNN/"}]},{"title":"Classifier in Keras","slug":"tensorflow/keras-1.3-classifier","date":"2019-08-22T06:17:21.000Z","updated":"2021-06-22T06:54:43.492Z","comments":true,"path":"2019/08/22/tensorflow/keras-1.3-classifier/","link":"","permalink":"http://www.iequa.com/2019/08/22/tensorflow/keras-1.3-classifier/","excerpt":"Classifier in Keras","text":"Classifier in Keras data preprocessing 123456789101112131415161718192021222324from keras.datasets import mnist# download the mnist to the path &#x27;~/.keras/datasets/&#x27; if it is the first time to be called# X shape (60,000 28x28), y shape (10,000, )(X_train, y_train), (X_test, y_test) = mnist.load_data()# data pre-processingX_train = X_train.reshape(X_train.shape[0], -1) / 255. # normalizeX_test = X_test.reshape(X_test.shape[0], -1) / 255. # normalizey_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10)print(X_train[1].shape)&quot;&quot;&quot;(784,)&quot;&quot;&quot;#print(y_train[:3])&quot;&quot;&quot;[[ 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]&quot;&quot;&quot; 1. build model 1234567891011import numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Activationfrom keras.optimizers import RMSprop Another way to build your neural net 12345678# Another way to build your neural netmodel = Sequential([ Dense(32, input_dim=784), Activation(&#x27;relu&#x27;), Dense(10), Activation(&#x27;softmax&#x27;),])model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_5 (Dense) (None, 32) 25120 _________________________________________________________________ activation_5 (Activation) (None, 32) 0 _________________________________________________________________ dense_6 (Dense) (None, 10) 330 _________________________________________________________________ activation_6 (Activation) (None, 10) 0 ================================================================= Total params: 25,450 Trainable params: 25,450 Non-trainable params: 0 _________________________________________________________________ 12# Another way to define your optimizerrmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) 2. compile model 1234# We add metrics to get more results you want to seemodel.compile(optimizer=rmsprop, loss=&#x27;categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;]) 3. train model 1234567891011print(&#x27;Training ------------&#x27;)# Another way to train the modelmodel.fit(X_train, y_train, epochs=2, batch_size=32)# &quot;&quot;&quot;# Training ------------# Epoch 1/2# 60000/60000 [==============================] - 2s - loss: 0.3506 - acc: 0.9025 # Epoch 2/2# 60000/60000 [==============================] - 2s - loss: 0.1995 - acc: 0.9421 # &quot;&quot;&quot; 4. evaluate model 1234567891011121314print(&#x27;\\nTesting ------------&#x27;)# Evaluate the model with the metrics we defined earlierloss, accuracy = model.evaluate(X_test, y_test)print(&#x27;test loss: &#x27;, loss)print(&#x27;test accuracy: &#x27;, accuracy)&quot;&quot;&quot;Testing ------------ 9760/10000 [============================&gt;.] - ETA: 0stest loss: 0.1724540345test accuracy: 0.9489&quot;&quot;&quot; Reference keras-cn、 keras.io 莫烦 Keras","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"keras","slug":"keras","permalink":"http://www.iequa.com/tags/keras/"}]},{"title":"Japan Travel plan 🇯🇵🇯🇵","slug":"world/Japan","date":"2019-08-21T14:46:48.000Z","updated":"2021-06-22T06:08:24.606Z","comments":true,"path":"2019/08/21/world/Japan/","link":"","permalink":"http://www.iequa.com/2019/08/21/world/Japan/","excerpt":"","text":"证件准备 Passport China Identity Card Japan 电话卡4G无限高速流量卡 10天 交通介绍, JR Pass 关西图览 行程总概 Name Date Time Place Checking Luggage 乐桃航空 MM080 上海-大阪 2019.09.07 06:15 ~ 09:25 浦东机场 T2 - 关西国际机场 T2 Nothing 2121921611766 穷游网：如何从关西机场到大阪市区 bset: 南海电车 关西空港－－－难波（namba） 南海快线不需要换乘 消费1130日元（约66人民币） 关西空港－－新今宫－－－大阪或梅田（umeda) 需要换乘一次 到新今宫换乘大坂环线 消费约1310日元（1130是南海电车 180 是大阪环线） 超详细关西七日游路线，玩转大阪京都奈良~ DAY1. 大阪（大阪城 →） Day Date Dest Type Desc 关西 大阪 → 京都 → 奈良 → 神户 → 大阪 Day1 2019-09-07 上海 - 大阪 大阪城公园→天守阁→道顿崛→法善寺→道顿崛水上观光船→心斋桥→海游馆→吃大阪烧 (一日券) →(环影待定) 宿大阪 路线： ・四天王寺（1小时）→ 大阪城公园、天守阁（2小时）→ 道顿堀（1.5小时）→. 法善寺（30分钟, 闹中取静）→ 道顿堀水上观光船（20分钟， 傍晚, 感受南大阪的繁华）→ 梅田蓝天大厦（1小时， 看夜景） 上午游览大阪城公园，时间充裕可以深度游览里面的天守阁与西之丸庭园。中午可以前往道顿崛附近用餐，连着心斋桥，法善寺小巷等，可以逛上一下午。晚上前往梅田展望台看大阪的夜景。 Day2. 京都（清水寺 →） 大阪→京都，电车40分钟–1小时20分钟 Day Date Dest Type Desc Day2 2019-09-08 大阪 - 京都 清水坂→清水寺→产宁坂,二宁坂,宁宁之道→八坂塔→高台寺→金阁寺→八坂神社→祗园 宿京都 上午游玩清水寺及其周边，清水寺的参观结束后可以逛逛二年坂三年坂，然后往祗园方向步行，沿途经过二宁坂（二年坂）、产宁坂（三年坂）、宁宁之道、八坂塔、高台寺等著名景点。 傍晚可以步行至八坂神社，京都重要的活动祭典很多都会在这里举办，6点还会亮灯，景色精致独具韵味，晚上顺便逛逛祇园，走走花见小路，夏季的时候尤其热闹。 Route： ・清水坂→清水寺（2小时）→ 产宁坂、二宁坂、宁宁之道→ ・八坂塔→高台寺（40分钟）→ 金阁寺（40分钟）→ 八坂神社（30分钟）→ 祗园 Topic: 穿上和服，漫步在清水寺或祗园的街道，体验日式风情 Traffic： 推荐购买京都巴士1日券，500日元，无需出示护照。 巴士车内也可购买，但是不附带京都巴士交通图。 Day3. 京都（伏见稻荷 →） Day Date Dest Type Desc Day3 2019-09-09 京都 - 京都 伏见稻荷→千本鸟居→宇治上神社→平等院→平等院表参道→祗园四条岚山嵯峨野竹林→天龙寺→二条城→锦市场→鸭川 宿京都 上午游览伏见稻荷大社，走一走千本鸟居，下午游览三十三间堂，参观完后有时间可以逛一逛锦市场，不过部分商铺会在17:00左右打烊，这是一处专卖京都特产的地方，有“京都厨房”的美称，傍晚时分去到京都著名的花街——先斗町，这里紧邻鸭川，吃过饭后可以沿着鸭川逛一圈。 前往岚山，想要体验小火车的蜂蜂可以在龟冈站搭乘小火车穿越保津峡，岚山站下后可步行至竹林的入口。在渡月桥景区也可以租借自行车游览 相比两条线地铁而言，京都市巴士的使用频率更多，单程240日元起，推荐500日元的巴士1日券，现场购买. Day4. 奈良（萌萌小鹿 →） Day Date Dest Type Desc Day4 2019-09-10 京都 - 奈良 近铁奈良站→奈良公园→东大寺→春日大社→唐招提寺→奈良町 宿大阪 Day5. 神戶（神户港 →） Day Date Dest Type Desc Day5 2019-09-11 大阪 - 神户 坐JR→神户→神户港→北野异人馆→六甲山→六甲山牧场→六甲山夜景→坐缆车去有马泡温泉海游馆→吃大阪烧 (一日券) 宿大阪 上午抵达神户港，感受充满现代设计感的港区建筑，还可以乘坐神户港地标马赛克摩天轮，从高空欣赏整座城市；随后前往北野异人馆街，多数为当年外国商人的私人住宅或外国使领馆。下午前往日本最古老的三大温泉乡之一有马温泉，泡温泉的同时感受历史文化；最后前往日本三大夜景地之一六甲山，眺望神户、大阪和京都 的绝美夜景～ 神户经典1日游 Day6. 大阪（去东京 →） Checking Luggage : 20公斤 Day Date Dest Type Desc 关东 东京 → 箱根 → 镰仓 → 东京 Day6 2019-09-12 大阪 - 东京 浅草寺→尾張屋,蕎麥麵店→隅田川 晴空塔明治神宫→原宿、表参道→彩虹桥 钢弹→东京塔 宿东京 Day7. 东京（富士山 →） 暑期预售 新仓山浅间公园+河口湖天上山公园+忍野八海+本栖湖 Day Date Dest Type Desc Day7 2019-09-13 富士山B套装 富士回游→河口湖(缆车待定)→仓山浅间公园(浅间神灶)→山中湖→六本本之丘室外展52F 宿东京 河口湖是富士五湖中最繁华的湖，湖中的富士山倒影为一奇景。 Day8. 东京（灌篮高手 →） Day Date Dest Type Desc Day8 2019-09-14 镰仓 套装 大佛→鹤冈八幡宫→江之岛→镰仓高校前站→高中→江之电 宿东京 Day9. 东京 Tokyo Day Date Dest Type Desc Day9 2019-09-15 东京 - 东京 bills→新宿车站→台场景点10选→皇居→银座→Akomeya→樂町→上野公园→阿美横滨→秋叶原→新宿歌舞伎町 宿东京 超详细关西七日游路线 第一次東京旅遊 一定要去的10個地方｜東京自由行必看 淺草雷門走到晴空塔，遠眺吾妻橋河岸城市之美，一日淺草玩樂戰略 京城上野站是交通枢纽 Matters Attention JP电车： 轻声细语 + 先下后上 (非常挤) + 后背包往前背 + 可吃喝(无味) Reference 知乎： 🇯🇵日本 关西 自由行 7日游 🇯🇵 大阪（➕神户）-京都-奈良 马蜂窝： 关西京都大阪奈良7日线路 超详细关西七日游路线，玩转大阪京都奈良~ 10天的日本游，从关东到关西（东京、镰仓、大阪、奈良、进度） 关东东京镰仓箱根5日线路 「富士回遊」新宿到河口湖直接省30分鐘！富士山回遊列車超方便《阿倫去旅行》 從東京到成田機場最便宜舒適的巴士坐法【2018最新版】《阿倫去旅行》","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"Japan","slug":"Japan","permalink":"http://www.iequa.com/tags/Japan/"}]},{"title":"Regressor in Keras","slug":"tensorflow/keras-1.2-Regressor","date":"2019-08-21T05:17:21.000Z","updated":"2021-06-20T04:12:28.329Z","comments":true,"path":"2019/08/21/tensorflow/keras-1.2-Regressor/","link":"","permalink":"http://www.iequa.com/2019/08/21/tensorflow/keras-1.2-Regressor/","excerpt":"NN 可用来模拟 regression，给一组数据，用一条线对数据进行拟合，并可预测新输入 x 的输出值。","text":"NN 可用来模拟 regression，给一组数据，用一条线对数据进行拟合，并可预测新输入 x 的输出值。 Regressor in Keras 创建数据 123456789101112import numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.models import Sequentialfrom keras.layers import Denseimport matplotlib.pyplot as plt # 可视化模块# create some dataX = np.linspace(-1, 1, 200)np.random.shuffle(X) # randomize the dataY = 0.5 * X + 2 + np.random.normal(0, 0.05, (200, )) 123456# plot dataplt.scatter(X, Y)plt.show()X_train, Y_train = X[:160], Y[:160] # train 前 160 data pointsX_test, Y_test = X[160:], Y[160:] # test 后 40 data points 1. build model 12model = Sequential()model.add(Dense(output_dim=1, input_dim=1)) 用 Sequential 建立 model， 再用 model.add 添加神经层，添加的是 Dense FC 层 参数有两个，一个是输入数据和输出数据的维度，本代码的例子中 x 和 y 是一维的。 如果需要添加下一个神经层的时候，不用再定义输入的纬度，因为它默认就把前一层的输出作为当前层的输入。在这个例子里，只需要一层就够了。 2. compile model 1234# choose loss function and optimizing methodmodel.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;sgd&#x27;)# mse 均方误差； optimizer sgd 随机梯度下降法. 3. train model 1234567891011121314# trainingprint(&#x27;Training -----------&#x27;)for step in range(301): cost = model.train_on_batch(X_train, Y_train) if step % 100 == 0: print(&#x27;train cost: &#x27;, cost)&quot;&quot;&quot;Training -----------train cost: 4.111329555511475train cost: 0.08777070790529251train cost: 0.007415373809635639train cost: 0.003544030711054802&quot;&quot;&quot; 训练的时候用 model.train_on_batch 一批一批的训练 X_train, Y_train。默认的返回值是 cost. 4. evaluate model 1234567891011121314# testprint(&#x27;\\nTesting ------------&#x27;)cost = model.evaluate(X_test, Y_test, batch_size=40)print(&#x27;test cost:&#x27;, cost)W, b = model.layers[0].get_weights()print(&#x27;Weights=&#x27;, W, &#x27;\\nbiases=&#x27;, b)&quot;&quot;&quot;Testing ------------40/40 [==============================] - 0stest cost: 0.004269329831Weights= [[ 0.54246825]] biases= [ 2.00056005]&quot;&quot;&quot; 用到的函数是 model.evaluate，输入测试集的x和y， 输出 cost，weights 和 biases。其中 weights 和 biases 是取在模型的第一层 model.layers[0] 学习到的参数。 从学习到的结果可以看到, weights 比较接近0.5，bias 接近 2。 5. Visualization 123456# plotting the predictionY_pred = model.predict(X_test)plt.scatter(X_test, Y_test)plt.plot(X_test, Y_pred)plt.show() 6. Gaussian Distribution 先看伟大的高斯分布（Gaussian Distribution）的概率密度函数（probability density function） f(x)=12πσexp⁡(−(x−μ)22σ2)f(x)=\\frac1{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}) f(x)=2π​σ1​exp(−2σ2(x−μ)2​) 对应于numpy中： 1numpy.random.normal(loc=0.0, scale=1.0, size=None) Reference keras-cn、 keras.io 从np.random.normal()到正态分布的拟合 莫烦 Keras 莫烦 Keras Model, Save &amp; reload","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"keras","slug":"keras","permalink":"http://www.iequa.com/tags/keras/"}]},{"title":"Keras Introduce","slug":"tensorflow/keras-1.1-intro","date":"2019-08-19T05:17:21.000Z","updated":"2021-06-22T06:54:43.498Z","comments":true,"path":"2019/08/19/tensorflow/keras-1.1-intro/","link":"","permalink":"http://www.iequa.com/2019/08/19/tensorflow/keras-1.1-intro/","excerpt":"Keras 开发重点是支持快速的实验。能够以最小的时延把你的想法转换为实验结果，是做好研究的关键。 Keras 并不处理如张量乘法、卷积等底层操作。这些操作依赖于某种特定的、优化良好的张量操作库。","text":"Keras 开发重点是支持快速的实验。能够以最小的时延把你的想法转换为实验结果，是做好研究的关键。 Keras 并不处理如张量乘法、卷积等底层操作。这些操作依赖于某种特定的、优化良好的张量操作库。 Keras + Tensorflow 1. Keras install 123456789pip install --upgrade pippip3 install keraspip3 install ipythonpip3 install notebookpip3 install tensorflowpip3 install --upgrade tensorflowpip3 install astkitpip3 install pandaspip3 install matplotlib 2. Basic concepts keras-cn.readthedocs.io 一些基本概念 符号计算 tensor 张量 data_format functional model API batch epochs 规模最小的张量是0阶张量，即标量，也就是一个数。 Keras 模型有一种叫 Sequential，也就是单输入单输出，一条路通到底，跨层连接统统没有。 Keras 中用的优化器SGD是stochastic gradient descent的缩写，不是一样本更新，还是基于mini-batch的. 廖雪峰的Python教程 123456789101112131415import numpy as npa = np.array([[1,2],[3,4]])# 1 2# 3 4sum0 = np.sum(a, axis=0)sum1 = np.sum(a, axis=1)print(sum0)print(sum1)# [4 6]# [3 7] 3. Quickstart in 30s 30s上手Keras Sequential模型如下 123from keras.models import Sequentialmodel = Sequential() 将一些网络层通过.add()堆叠起来，就构成了一个模型： 123456from keras.layers import Dense, Activationmodel.add(Dense(units=64, input_dim=100))model.add(Activation(&quot;relu&quot;))model.add(Dense(units=10))model.add(Activation(&quot;softmax&quot;)) 完成模型的搭建后，我们需要使用.compile()方法来编译模型： 1234model.compile(loss=&#x27;categorical_crossentropy&#x27;, optimizer=&#x27;sgd&#x27;, metrics=[&#x27;accuracy&#x27;])# from keras.optimizers import SGD# model.compile(loss=&#x27;categorical_crossentropy&#x27;, optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True)) 完成模型编译后，我们在训练数据上按batch进行一定次数的迭代来训练网络 1model.fit(x_train, y_train, epochs=5, batch_size=32) 也可以手动将一个个batch的数据送入网络中训练，这时候需要使用： 1model.train_on_batch(x_batch, y_batch) 随后，我们可以使用一行代码对我们的模型进行评估，看看模型的指标是否满足我们的要求： 1loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128) 使用模型，对新的数据进行预测： 1classes = model.predict(x_test, batch_size=128) 4. Functional model 快速开始函数式（Functional）模型 第一个模型：全连接网络 多输入和多输出模型 共享层, 层“节点”的概念 5. Sequential model Sequential model 是多个网络层的线性堆叠，也就是“一条路走到黑”。 123model = Sequential()model.add(Dense(32, input_shape=(784,)))model.add(Activation(&#x27;relu&#x27;)) 5.1 input data shape 1234model = Sequential()model.add(Dense(32, input_dim=784))model = Sequential()model.add(Dense(32, input_shape=(784,))) 5.2 compile and train (fit) Sequential model methods_cn、Sequential model methods_en compile Arguments optimizer loss metrics 1234# For a multi-class classification problemmodel.compile(optimizer=&#x27;rmsprop&#x27;, loss=&#x27;categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;]) Keras 以 Numpy数组 作为 input_data 和 label 的数据类型。训练模型一般使用**fit函数**. 12345678910111213141516# For a single-input model with 2 classes (binary classification):model = Sequential()model.add(Dense(32, activation=&#x27;relu&#x27;, input_dim=100))model.add(Dense(1, activation=&#x27;sigmoid&#x27;))model.compile(optimizer=&#x27;rmsprop&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])# Generate dummy dataimport numpy as npdata = np.random.random((1000, 100))labels = np.random.randint(2, size=(1000, 1))# Train the model, iterating on the data in batches of 32 samplesmodel.fit(data, labels, epochs=10, batch_size=32) 6. Pre-knowledge 6.1 python language Object-oriented, class, object, encapsulation, polymorphism, inheritance, scope, etc. Python 的科学计算包有一定了解，numpy, scipy, scikit-learn, pandas… generator，以及如何编写 generator。什么是匿名函数（lambda） 6.2 deep learning Supervised Learning, Unsupervised Learning, Classification, Clustering, Regression Neuron model, multilayer perceptron，BP algorithm loss function，activation function，Gradient descent Fully Connected NN、CNN、RNN、LSTM Training set, test set, cross validation, under-fitting, over-fitting Reference keras-cn keras.io","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"keras","slug":"keras","permalink":"http://www.iequa.com/tags/keras/"}]},{"title":"数据清理中，处理缺失值的方法是?","slug":"ml/data_clean_process","date":"2019-08-10T02:06:16.000Z","updated":"2021-06-22T06:39:45.833Z","comments":true,"path":"2019/08/10/ml/data_clean_process/","link":"","permalink":"http://www.iequa.com/2019/08/10/ml/data_clean_process/","excerpt":"data pre-processing","text":"data pre-processing 由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。 1.1 estimation 估算(estimation)。最简单的办法就是用某个变量的样本 均值、中位数或众数 代替无效值和缺失值。 这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。另一种办法就是根据调查对象对其他问题的答案，通过变量之间的相关分析或逻辑推论进行估计。例如，某一产品的拥有情况可能与家庭收入有关，可以根据调查对象的家庭收入推算拥有这一产品的可能性。 1.2 casewise deletion 整例删除(casewise deletion)是剔除含有缺失值的样本。 由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。 1.3 variable deletion 变量删除(variable deletion)。 如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。 1.4 pairwise deletion 成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。 但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。这是一种保守的处理方法，最大限度地保留了数据集中的可用信息。 采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在调查中应当尽量避免出现无效值和缺失值，保证数据的完整性。 2. GBDT vs XGBoost GBDT 原理 GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型 XGBoost 原理 XGBoost的性能在GBDT上又有一步提升，而其性能也能通过各种比赛管窥一二。坊间对XGBoost最大的认知在于其能够自动地运用CPU的多线程进行并行计算，同时在算法精度上也进行了精度的提高。 由于GBDT在合理的参数设置下，往往要生成一定数量的树才能达到令人满意的准确率，在数据集较复杂时，模型可能需要几千次迭代运算。但是XGBoost利用并行的CPU更好的解决了这个问题。 GBDT vs XGBoost (1). GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这时XGBoost相当于L1和L2正则化的LR； (2). GBDT优化时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数； (3). XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统GBDT的一个特性； (4). shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost 在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）； (5) 列抽样。XGBoost 借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算； (6) 对缺失值的处理。对于特征的值有缺失的样本，XGBoost 还可以自动 学习出它的分裂方向； (7) XGBoost 工具支持并行。 Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。 XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 Reference 牛客网 处理缺失值的方法 GBDT和XGBoost区别","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"estimation","slug":"estimation","permalink":"http://www.iequa.com/tags/estimation/"}]},{"title":"2019 Leetcode","slug":"leetcode/2019-leetcode","date":"2019-07-01T02:54:16.000Z","updated":"2021-06-22T06:05:07.536Z","comments":true,"path":"2019/07/01/leetcode/2019-leetcode/","link":"","permalink":"http://www.iequa.com/2019/07/01/leetcode/2019-leetcode/","excerpt":"","text":"1、七种常见的数组排序算法整理(C语言版本) 2、2019 算法面试相关(leetcode)–数组和链表 3、2019 算法面试相关(leetcode)–字符串 4、2019 算法面试相关(leetcode)–栈和队列 5、2019 算法面试相关(leetcode)–优先队列 6、2019 算法面试相关(leetcode)–哈希表 7、2019 算法面试相关(leetcode)–树、二叉树、二叉搜索树 8、2019 算法面试相关(leetcode)–递归与分治 9、2019 算法面试相关(leetcode)–贪心算法 10、2019 算法面试相关(leetcode)–动态规划(Dynamic Programming) Hot100 hot easy 两数之和， ✔️ 反转链表， 回文链表(1.快慢p拆，2.翻转 3. 判断)， 相交链表（f1: set() f2: A+B和B+A f3:快慢p） ✔️ 最大子序和 （动态规划/分治法）， ✔️ 爬楼梯， 打家劫舍 &amp; 环形链表 , 合并2个有序链表， ✔️ 有效的括号， 最小栈 ✔️ 买卖股票的最佳时机-3, 两阶段， ✔️ 买卖股票的最佳时机-4, 超难 翻转二叉树 , 把二叉搜索树转换为累加树 ， ✔️ 只出现一次的数字， 汉明距离 x&amp;(x-1)来统计二进制数x中1的个数 ✔️ 二叉树的直径 &amp; 二叉树的最大深度 &amp; 对称二叉树, 合并二叉树 ， ✔️ 求众数 ，移动零 ，找到所有数组中消失的数字 最短无序连续子数组,找到字符串中所有字母异位词 , 路径总和 III hot 中等 Page1 两数之和（方法：一遍哈希表） Map &lt; Integer, Integer &gt; ， ✔️ 最长回文子串 反转链表 ， ✔️ LRU缓存机制 编辑距离 无重复字符的最长子串 戳气球 接雨水 两数相加 (链表+进位carry)， ✔️ 寻找两个有序数组的中位数 三数之和 （2方法： hash &amp; 对撞指针） ， ✔️ 最大子序和 （动态规划/分治法）， ✔️ 字符串解码 删除无效的括号 最大矩形 有难度 nok 全排列 (交换法，类比字符串思路)， ✔️ 合并两个有序链表 ， ✔️ 正则表达式匹配 零钱兑换 ， ✔️ Page3 单词搜索 (DFS vector &lt; vector &lt; bool &gt; &gt; visit(row, vector &lt; bool &gt; (col, false))) 打家劫舍之三 (DFS) 最小路径和 电话号码的字母组合 每日温度 回文子串 验证二叉搜索树 单词拆分 颜色分类 在排序数组中查找元素的第一个和最后一个位置 字母异位词分组 完全平方数 环形链表 II 除法求值 课程表 根据身高重建队列 除自身以外数组的乘积 目标和 比特位计数 123456789101112131415public class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; HashMap&lt;Integer, Integer&gt; m = new HashMap&lt;Integer, Integer&gt;(); int[] res = new int[2]; for (int i = 0; i &lt; nums.length; ++i) &#123; if (m.containsKey(target - nums[i])) &#123; res[0] = i; res[1] = m.get(target - nums[i]); break; &#125; m.put(nums[i], i); &#125; return res; &#125;&#125; 爬楼梯 1. Array 1.1 easy 二维数组中的查找 bool Find(int b[][4], int rows, int cols, int value)， ✔️ 替换空格 char* replace(char* str, int len) ‘ ’-&gt;%20 在源数组总长度，从后向前，逐个赋值， ✔️ 数字在排序数组中出现的次数 biSea(arr,k+0.5)-biSea(arr,k-0.5); / bina(*a, len, num, isLeft)， ✔️ 旋转数组的最小元素 while(low &lt; high) { if(a[m] &gt; a[high]) min[m+1,high], else [low,m]} ✔️ 调整数组位数使奇数位于前面 void odds(int[] arr) ， ✔️ 次数超过一半的次数 * int core(int *a, int len)， ✔️ 丑数, 只包含质因子2、3和5的数称作丑数, 1, 2, 3, 5, 6, … ， ✔️ 和为S的两个数字(双指针思想) ， ✔️ 扑克牌顺子 (排序后，统计大小王数量 + 间隔)， ✔️ 构建乘积数组 (A数组，从前向后，再从后向前j-2,构造 B)， ✔️ 求1+2+3+…+n (判断 &amp;&amp; 递归)， ✔️ 1.2 medium 八皇后， void dfs(int n) for(int i = 0; i &lt; 8; i++) { pos[n] = i; ， ✔️ 和S连续正数序 (3fun，mid = (1+sum)/2; while(start&lt;mid), Sum(int start, int end), 双vector)， ✔️ 约瑟夫环 LinkedList; (index = (index + m) %link.size();link.remove(index--)😉 link.get(0); ， ✔️ 数组排成最小的数 Arrays.sort(str, new Comparator(){ public int compare(String s1, String s2)，✔️ 数组中只出现一次的数字 , 划分2数组，num &amp; (-num);二者与后得到的数，将num最右边的1保留下来，✔️ 1.3 important 数组中的逆序对(归并排序). void mergeSort(int a[], int l, int r) ， ✔️ 最小的K个数(堆排序)， 数据流中的中位数 （2 PriorityQueue）. 最小的K个数 （2fun）快排思想 part return l, 外else return left; void set_k(int* input, n, k) ， ✔️ quickSort(a[],left,right), while(1) { (双while, if(l &gt;= r) break; swap) } swap(a[left], a[l]);， ✔️ 最短路Floyd， ✔️ quickSort (1 while + [2while + break + swap] + swap + 2 quickSort) mergeSort (2 mergeSort + new *arr + 3 while + 1 for) 1.1 八皇后 123456789101112131415161718192021222324252627const int N = 105;int pos[N];int num = 8, cnt = 0;bool ok(int n) &#123; for(int i = 0; i &lt; n; i++) &#123; if(pos[i] == pos[n]) return false; if(abs(pos[n] - pos[i]) == n-i) return false; &#125; return true;&#125;void dfs(int n) &#123; if(n == num) &#123; cnt++; return; &#125; for(int i = 0; i &lt; num; i++) &#123; pos[n] = i; if(ok(n)) &#123; dfs(n+1); &#125; &#125;&#125;int main() &#123; res = dfs(0); cout &lt;&lt; res &lt;&lt; endl; return 0;&#125; 1.2 二维数组中的查找 123456789101112131415161718192021222324252627int a[][4] = &#123; &#123; 1, 2, 3, 4 &#125;, &#123; 5, 6, 7, 8 &#125;, &#123; 9, 10, 11, 12&#125;, &#123; 13, 14, 15, 16&#125;, &#123; 17, 18, 19, 20&#125;&#125;;bool Find(int b[][4], int rows, int cols, int value) &#123; int row = 0, col = cols - 1; while (b != NULL &amp;&amp; row &lt; rows &amp;&amp; col &gt;= 0) &#123; if (b[row][col] == value) &#123; cout &lt;&lt; &quot;row: &quot; &lt;&lt; row &lt;&lt; &quot; col: &quot; &lt;&lt; col &lt;&lt; end; return true; &#125; else if (b[row][col] &gt; value) &#123; col--; &#125; else &#123; row++; &#125; &#125; return false;&#125; 1.3 替换空格 123456789char* replace(char* str, int len) &#123; while(str[len] != &#x27;\\0&#x27;)&#123; if(str[len] == &#x27; &#x27;)&#123; konglen++; &#125; len++; &#125; return &#x27;\\0&#x27;;&#125; 1.4 旋转数组的最小元素 1234567891011121314void quickSort(int a[], int left, int right) &#123; if(left &lt; right) &#123; // exit. good idea! int l = left, r = right, x = a[l]; while(1) &#123; while(l &lt; r &amp;&amp; a[r] &gt;= x) r--; while(l &lt; r &amp;&amp; a[l] &lt;= x) l++; if(l &gt;= r) break; swap(a[r], a[l]); &#125; swap(a[left], a[l]); quickSort(a, left, l-1); quickSort(a, l+1, right); &#125;&#125; 1.5 调整数组位数使奇数位于前面 123456789101112131415161718void reorderOddEven(int[] arr, len) &#123; if (arr == NULL || len &lt;= 1 ) &#123; return; &#125; int l = 0, r = len - 1; while (l &lt; r) &#123; while (l &lt; r &amp;&amp; arr[r] % 2 == 0) &#123; r--; &#125; while (l &lt; r &amp;&amp; arr[l] % 2 == 1) &#123; l++; &#125; if (l &lt; r) &#123; swap(arr[l], arr[r]); l++, r--; &#125; &#125;&#125; 1.6 出现次数超过一半的次数 1234567891011121314151617181920int core(int *a, int len) &#123; if (arr == NULL || len &lt;= 0 ) &#123; return; &#125; int target = a[0], cnt = 1; for (int i = 1; i &lt; len; i++) &#123; if (a[i] == target) &#123; cnt++; &#125; else &#123; cnt--; if (cnt == 0) &#123; target = a[i]; cnt = 1; &#125; &#125; &#125; return target;&#125; 1.7 最小的K个数 part 快排思想 123456789101112131415161718192021222324252627282930313233343536373839404142434445const int N = 105;int a[N] = &#123;4, 5, 1, 6, 2, 7, 3, 8&#125;;int part (int *a, int left, int right) &#123; if(left &lt; right) &#123; int x = a[0], l = left, r = right; while(l &lt; r) &#123; while(l &lt; r &amp;&amp; a[r] &gt;= x) r--; while(l &lt; r &amp;&amp; a[l] &lt;= x) l++; if(l &gt;= r) break; swap(a[l], a[r]); &#125; swap(x, a[l]); return l; &#125; else return left;&#125;void set_k(int *input, int n, int k) &#123; if(input == NULL || k &gt; n || k &lt;= 0 || n &lt;= 0) &#123; return; &#125; int start = 0, end = n - 1; int index = part(input, start, end); while(index != k-1) &#123; if(index &gt; k-1) &#123; end = index - 1; &#125; else if(index &lt; k - 1) &#123; start = index + 1; &#125; else &#123; break; &#125; index = part(input, start, end); &#125;&#125; 1.8 数组中的逆序对 &amp; 归并排序 1234567891011121314151617181920212223void mergeSort(int a[], int l, int r) &#123; // 8, 5, 4, 9, 2, 3, 6 if(l &gt;= r) return; // exit. int mid = (l+r) / 2; // overflow &lt;-&gt; l + (r-l)/2 mergeSort(a, l, mid); mergeSort(a, mid+1, r); int *arr = new int[r-l+1]; int k = 0; int i = l, j = mid + 1; while(i &lt;= mid &amp;&amp; j &lt;= r) &#123; if(a[i] &lt;= a[j]) &#123; arr[k++] = a[i++]; &#125; else &#123; arr[k++] = a[j++]; // ans += (mid-i+1); &#125; &#125; while(i &lt;= mid) arr[k++] = a[i++]; while(j &lt;= r) arr[k++] = a[j++]; for(int i = l; i &lt;= r; i++) &#123; a[i] = arr[i-l]; &#125; delete []arr;&#125; 1.15 约瑟夫环 1234567891011121314151617import java.util.LinkedList;public class Solution &#123; public int LastRemaining_Solution(int n, int m) &#123; if(n &lt; 1 || m &lt; 1) return -1; LinkedList&lt;Integer&gt; link = new LinkedList&lt;Integer&gt;(); for(int i = 0; i &lt; n; i++) link.add(i); int index = -1; //起步是 -1 不是 0 while(link.size() &gt; 1)&#123; index = (index + m) % link.size(); //对 link的长度求余不是对 n link.remove(index); index --; &#125; return link.get(0); &#125;&#125; 1.17 排序数组中某数字出现的次数 12345678910111213int bina(int *a, int len, int data) &#123; if(a == NULL || len &lt;= 0) return -1; int l = 0, r = len - 1; while(l &lt;= r) &#123; int mid = (l + r) / 2; if(a[mid] == data) return mid; else if(data &lt; a[mid]) &#123; r = mid - 1; &#125; else l = mid+1; &#125; return -1; &#125; 2. LinkedList linkedlist_summary 2.1 easy: 在 O(1) 时间删除链表节点， ✔️ 删除单链表倒数第 n 个节点， ✔️ 求单链表的中间节点， ✔️ 判断单链表是否存在环， ✔️ 从尾到头打印链表, 递归 ok.， ✔️ 链表中倒数第k个结点 ok.， ✔️ 判断两个无环单链表是否相交， ✔️ 两个链表相交扩展：求两个无环单链表的第一个相交点， ✔️ 两个链表的第一个公共结点 ， ✔️ 旋转单链表 题目描述：给定一个单链表，设计一个算法实现链表向右旋转 K 个位置。 举例： 给定 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;NULL, K=3 则4-&gt;5-&gt;6-&gt;1-&gt;2-&gt;3-&gt;NULL 环路的入口点 在第 4 题两个指针相遇后，让其中一个指针回到链表的头部，另一个指针在原地，同时往前每次走一步，当它们再次相遇时，就是在环路的入口点。 2.2 medium: 反转链表 next=head-&gt;next, head-&gt;next=pre, pre=head, head=next; 4步 ok， ✔️ 翻转部分单链表 举例：1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;null, from = 2, to = 4 结果：1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;null 复杂链表的复制 ok， ✔️ 链表划分 （题目描述： 给定一个单链表和数值x，划分链表使得小于x的节点排在大于等于x的节点之前） 单链表排序 合并两个或k个有序链表 ok， 递归 (三元运算符). 删除链表重复结点 链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5. first-&gt;next=head, last, p 三针， ✔️ 链表中环的入口结点， ✔️ 单链表排序 or 合并两个或k个有序链表 1234567891011121314151617181920212223242526272829303132struct Node &#123; Node *next; int value; &#125;; /* 你一定要相信递归是一个强大的思想 */Node* mergeList(Node* head1, Node* head2) &#123; // 有序链表的合并 if(head1 == NULL) return head2; if(head2 == NULL) return head1; Node* tmp; if(head1-&gt;value &lt; head2-&gt;value) &#123; tmp = head1; head1 = head1-&gt;next; &#125; else &#123; tmp = head2; head2 = head2-&gt;next; &#125; tmp-&gt;next = mergeList(head1, head2); return tmp;&#125;Node* mergeSort(Node* head) &#123; if(head == NULL) return NULL; Node* r_head = head; Node* head1 = head; Node* head2 = head; while(head2-&gt;next != NULL &amp;&amp; head2-&gt;next-&gt;next != NULL) &#123; head1 = head1-&gt;next; head2 = head2-&gt;next-&gt;next; &#125; if(head1-&gt;next == NULL) return r_head; // 只有一个节点 head2 = head1-&gt;next; head1 = head; r_head = mergeList(mergeSort(head1), mergeSort(head2)); return r_head;&#125; 2.3 difficul: 链表求和 — 反转链表: 123456789101112public ListNode reverseList(ListNode head) &#123; ListNode next = null; ListNode pre = null while (head != null) &#123; next = head.next; (保存当前头结点的下个节点) head.next = pre; (将当前头结点的下一个节点指向“上一个节点”，这一步是实现了反转) pre = head; (将当前头结点设置为“上一个节点”) head = next; (将保存的下一个节点设置为头结点) &#125; return pre;&#125; 复杂链表的复制: 1234567891011/*public class RandomListNode &#123; int label; RandomListNode next = null; RandomListNode random = null; RandomListNode(int label) &#123; this.label = label; &#125;&#125;*/ 12345678910111213141516171819202122232425262728293031public class Solution &#123; public RandomListNode Clone(RandomListNode pHead) &#123; if(pHead == null) return null; //复制节点 A-&gt;B-&gt;C 变成 A-&gt;A&#x27;-&gt;B-&gt;B&#x27;-&gt;C-&gt;C&#x27; RandomListNode head = pHead; while(head != null)&#123; RandomListNode node = new RandomListNode(head.label); node.next = head.next; head.next = node; head = node.next; &#125; //复制random head = pHead; while(head != null)&#123; head.next.random = head.random == null ? null : head.random.next; head = head.next.next; &#125; //折分 head = pHead; RandomListNode chead = head.next; while(head != null)&#123; RandomListNode node = head.next; head.next = node.next; node.next = node.next == null ? null : node.next.next; head = head.next; &#125; return chead; &#125;&#125; 链表划分： 给定一个单链表和数值x，划分链表使得所有小于x的节点排在大于等于x的节点之前。 你应该保留两部分内链表节点原有的相对顺序。 样例 给定链表 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;2-&gt;null，并且 x=3 返回** 1-&gt;2-&gt;2-&gt;4-&gt;3-&gt;5-&gt;null** 1234567891011121314151617181920212223 public ListNode partition(ListNode head, int x) &#123; // write your code here if(head == null) return null; ListNode leftDummy = new ListNode(0); ListNode rightDummy = new ListNode(0); ListNode left = leftDummy, right = rightDummy; while (head != null) &#123; if (head.val &lt; x) &#123; left.next = head; left = head; &#125; else &#123; right.next = head; right = head; &#125; head = head.next; &#125; right.next = null; left.next = rightDummy.next; return leftDummy.next; &#125;&#125; 3. String 13 道题搞定 BAT 面试——字符串 字符串 123456789101112131415# include &lt;iostream&gt;# include &lt;vector&gt;# include &lt;queue&gt;# include &lt;cstring&gt;using namespace std;int main() &#123; string x; cin &gt;&gt; x; sort(x.begin(), x.end()); reverse(x.begin(), x.end()); cout &lt;&lt; x &lt;&lt; endl; return 0;&#125; 3.1 easy: 替换空格， ✔️ 反转字符串 abcd -&gt; dcba， ✔️ 翻转单词顺序列， I am a student. -&gt; student. a am I， ✔️ 旋转字符串， abcde --2位–&gt; cdeab, 若干次旋转操作之后，A 能变成B，那么返回True， ✔️ 左旋转字符串 string LeftRotateString(string str, int n),string Reverse(string str)， ✔️ 反转字串单词 string ReverseSentence(string str), reverse(str.begin(), str.end()); in lib algorithm 3.2 medium 字符流中第一个不重复的字符 (哈希来存每个字符及其出现的次数，另用一字符串 s 来保存字符流中字符顺序) 第一个只出现一次的字符 字符串全排列 void res(char *str, char *pStr), scanf(&quot;%s&quot;, str); #include &lt; utility&gt; 字符串转整型 int StrToInt(char* str) ok 字符串的排列 (给定两个字符串 s1 和 s2，第一个字符串的排列之一是第二个字符串的子串) 123456789101112131415161718192021222324import java.util.HashMap;public class Solution &#123; HashMap&lt;Character, Integer&gt; map = new HashMap&lt;Character, Integer&gt;(); StringBuffer s = new StringBuffer(); //Insert one char from stringstream public void Insert(char ch) &#123; s.append(ch); if(map.containsKey(ch))&#123; map.put(ch, map.get(ch)+1); &#125;else&#123; map.put(ch, 1); &#125; &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; for(int i = 0; i &lt; s.length(); i++)&#123; if(map.get(s.charAt(i)) == 1) return s.charAt(i); &#125; return &#x27;#&#x27;; &#125;&#125; 3.2 difficult KMP 算法 最长公共前缀 最长回文串 (3.1) 验证回文串 (3.2) 最长回文子串 (3.3) 最长回文子序列 表示数值的字符串 剑指offer: 表示数值的字符 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100″,”5e2″,”-123″,”3.1416″和”-1E-16″都表示数值。 但是”12e”,”1a3.14″,”1.2.3″,”±5″和”12e+4.3″都不是。 3.1 easy code 3.1.1 替换空格 1234567891011for(int i=tail; i&gt;len &amp;&amp; i&gt;=0; i--)&#123; if(str[len] == &#x27; &#x27;) &#123; str[i--] = &#x27;0&#x27;; str[i--] = &#x27;2&#x27;; str[i] = &#x27;%&#x27;; &#125; else &#123; str[i] = str[len]; &#125; len--;&#125; 3.1.4 旋转字符串 12345class Solution &#123; public boolean rotateString(String A, String B) &#123; return A.length() == B.length() &amp;&amp; (A+A).contains(B); &#125;&#125; 3.1.5 左旋转字符串 字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ 在第 n 个字符后面将切一刀，将字符串分为两部分，再重新并接起来即可。注意字符串长度为 0 的情况。 1234567891011public class Solution &#123; public String LeftRotateString(String str,int n) &#123; int len = str.length(); if(len == 0) return &quot;&quot;; n = n % len; String s1 = str.substring(n, len); String s2 = str.substring(0, n); return s1+s2; &#125;&#125; 3.2 medium code 123456789101112void res(char *str, char *pStr) &#123; if(*pStr == &#x27;\\0&#x27;) cout &lt;&lt; str &lt;&lt; endl; for(char *p = pStr; *p != &#x27;\\0&#x27;; ++p) &#123; char tmp = *p; *p = *pStr; *pStr = tmp; res(str, pStr+1); tmp = *p; *p = *pStr; *pStr = tmp; &#125;&#125; 字符流中第一个不重复的字符 C++ 123456789101112131415161718192021222324252627/class Solution&#123;public: //Insert one char from stringstream // vector用来记录字符流 vector&lt;char&gt; vec; // map用来统计字符的个数 map&lt;char, int&gt; table; void Insert(char ch) &#123; table[ch]++; vec.push_back(ch); &#125; //return the first appearence once char in current stringstream char FirstAppearingOnce() &#123; // 遍历字符流，找到第一个为1的字符 for (char c : vec) &#123; if (table[c] == 1) return c; &#125; return &#x27;#&#x27;; &#125;&#125;; 字符流中第一个不重复的字符 Java 12345678910111213141516171819202122import java.util.HashMap;public class Solution &#123; HashMap&lt;Character, Integer&gt; map = new HashMap&lt;Character, Integer&gt;(); StringBuffer s = new StringBuffer(); //Insert one char from stringstream public void Insert(char ch) &#123; s.append(ch); if(map.containsKey(ch))&#123; map.put(ch, map.get(ch)+1); &#125;else&#123; map.put(ch, 1); &#125; &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; for(int i = 0; i &lt; s.length(); i++)&#123; if(map.get(s.charAt(i)) == 1) return s.charAt(i); &#125; return &#x27;#&#x27;; &#125;&#125; 4. Binary Tree 算法&amp;数据结构 ， 20Tree 4.1 easy 递归： 求二叉树中的节点个数 ， ✔️ 递归： 求二叉树的最大层数(最大深度) &amp; (最小深度) 最小深度特殊情况：left || right==0 ， ✔️ 递归： 求二叉树第K层的节点个数 get_k(root.left, k-1) + get_k(root.right, k-1); good ， ✔️ 递归： 求二叉树第K层的叶子节点个数 if(k==1 and root.left and root.right is null) return 1; ， ✔️ 递归： 二叉树先序遍历/前序遍历 (fIno(Node* root) { while(1) {if else} 递归： 判断两棵二叉树是否结构相同 ， ✔️ 递归： 求二叉树的镜像（反转二叉树） ， （左右递归交换）✔️ 递归： 对称二叉树 （双函数，承接上题二叉树的镜像， good） ， ✔️ 递归： 求二叉树中两个节点的最低公共祖先节点 good ， ✔️ 递归： 求二叉搜索树的最近公共祖先 good ， ✔️ 递归： 根据前序和中序重建二叉树 ， ✔️ 双函数，递归 树的子结构,遍历+判断, bool f5(Node* root1, Node* root2), bool son(Node* p1, Node* p2) ， ✔️ 判断二叉树是不是平衡二叉树 bool isBalance(Node* root)， int maxHigh(Node* root) ， ✔️ 求二叉树的直径 （直径长度是任意两个结点路径长度中的最大值）， ✔️ 4.2 medium 分层遍历 (判断二叉树是不是完全二叉树) （遍历到了NULL结点，如后续还有非NULL结点）， ✔️ 分层遍历 (自下而上分层遍历) bfs + vector&lt; vector &lt; int &gt; &gt;， ✔️ 分层遍历 (按之字形顺序打印二叉树)， ✔️ 4.3 difficult: 二叉树中和为某一值的路径 void f4(Node* root, int exSum, int curSum, vecotr&lt; int &gt;&amp; path)， ✔️ 二叉树下一结点:3情况 (1.有right.child 2.没有right.child,父left.child 3.没有right.child,父right.child)✔️ 序列化二叉树， String serialize(TreeNode root), TreeNode deserialize(String data) Queue queue = new LinkedList&lt;&gt;(); ✔️ 二叉搜索树的后序遍历序列 bool f6(int* sec, int len)， ✔️ 二叉搜索树与双向链表 void convert(Node* root, Node*&amp; pLast) ， ✔️ 二叉搜索树的第k个结点 ， ✔️ 二叉查找树节点的删除. 重要 4.1 easy code 4.1.9 二叉树两节点的最低公共祖先 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# include &lt;iostream&gt;# include &lt;vector&gt;# include &lt;cstring&gt;using namespace std;struct Node &#123; Node* lchild; Node* rchild; int value; Node(): value(0), lchild(NULL), rchild(NULL) &#123;&#125; &#125;;vector&lt;Node*&gt; vec1;vector&lt;Node*&gt; vec2;/** 1. 找到目标节点1，寻找路径，存放在 vec1 中. 2. 找到目标节点2，寻找路径，存放在 vec2 中. 3. 同时遍历 2 个 */bool getNodePath(Node* root, Node* target, vector&lt;Node*&gt;&amp; vec) &#123; if (root == NULl) &#123; return false; &#125; vec.push_back(root); if (root == target) &#123; return true; &#125; bool flag = false; flag = getNodePath(root-&gt;lchild, target, vec) if (!flag &amp;&amp; root-&gt;rchild) &#123; flag = getNodePath(root-&gt;rchild, target, vec) &#125; if (!flag) &#123; vec.pop_back(); &#125; return flag;&#125;Node* getCom(const vector&lt;Node*&gt;&amp; v1, const vector&lt;Node*&gt;&amp; v2) &#123; vector&lt;Node*&gt;::const_iterator it1 = v1.begin(); vector&lt;Node*&gt;::const_iterator it2 = v2.begin(); Node* pLast = NULL; while (it1 != v1.end() &amp;&amp; it2 != v2.end()) &#123; if (*it1 != *it2) &#123; break; &#125; pLast = *it1; it1++; it2++; &#125; return pLast;&#125;int main() &#123; retunr 0;&#125; 4.1.10 求二叉搜索树的最近公共祖先 123456789101112Node* lowestCommonAncestor(Node* root, Node* p, Node* q) &#123; if (root == NULL || p == NULL || q == NULL) &#123; return NULL; &#125; if (root-&gt;value &gt; p-&gt;value &amp;&amp; root &gt; q-&gt;value) &#123; return lowestCommonAncestor(root-&gt;lchild, p, q); &#125; if (root-&gt;value &lt; p-&gt;value &amp;&amp; root &lt; q-&gt;value) &#123; return lowestCommonAncestor(root-&gt;rchild, p, q); &#125; return root;&#125; 4.1.11 前序中序重建二叉树 12345678910111213Node* f3(int* pre, int* ino, int len) &#123; // pre : 1, 2, 4, 7, 3, 5, 6, 8 ino : 4, 7, 2, 1, 5, 3, 8, 6 if(pre == NULL || ino == NULL || len &lt;= 0) return NULL; int r_v = pre[0]; Node* root = new Node(); root-&gt;value = r_v; int i; for(i = 0; ; i++) &#123; if(ino[i] == r_v) break; &#125; root-&gt;lchild = f3(pre+1, ino, i); root-&gt;rchild = f3(pre+i+1, ino+i+1, len-1-i); return root;&#125; 双函数 + 递归 4.1.12 树2是否是树1的子结构 12345678910111213141516171819202122232425262728bool son(Node* p1, Node* p2) &#123; if (p2 == NULL) &#123; return true; &#125; if (p1 == NULL) &#123; return false; &#125; if (p1-&gt;value == p2-&gt;value) &#123; return son(p1-&gt;lchild, p2-lchild); &#125;&#125;bool son_tree(Node* root1, Node* root2) &#123; if (root2 == NULL) &#123; return true; &#125; if (root1 == NULL) &#123; return false; &#125; if (root1-&gt;value == root2-&gt;child) &#123; return son(root1, root2); &#125; bool flag = false flag = son_tree(root1-&gt;lchild, root2); if (!flag) &#123; return son_tree(root1-&gt;rchild, root2); &#125;&#125; 4.1.13 平衡二叉树 123456789101112bool isBalanced(Node* root) &#123; if(root == NULL) return true; return (-1 &lt;= (maxHigh(root-&gt;lchild) - maxHigh(root-&gt;rchild)) &lt;= 1) &amp;&amp; isBalanced(root-&gt;lchild) &amp;&amp; isBalanced(root-&gt;lchild);&#125;int maxHigh(Node* root)&#123; if(root == NULL) return 0; return max(maxHigh(root-&gt;lchild), maxHigh(root-&gt;rchild))+1;&#125; 4.1.14 求二叉树的直径 12345678private int diamHelper(TreeNode root)&#123; if(root == null) return 0; int left = diamHelper(root.left); int right = diamHelper(root.right); path = Math.max(path, left + right); return Math.max(left, right) + 1;&#125; 4.2 medium code 4.2.1 判断二叉树是不是完全二叉树 123456789101112131415161718192021222324252627bool checkCompleteTree(Node* root) &#123; bool flag = true; queue&lt;Node*&gt; q; if (root == null) return true; q.push(root); while(!q.empty())&#123; for (int i = 0; i &lt; q.size(); ++i) &#123; Node* tmp = q.front(); q.pop(); if (tmp-&gt;lchild == NULL &amp;&amp; tmp-&gt;rchild != NULL)&#123; flag = false; break; &#125; if (tmp-&gt;left != NULL) que.push(tmp-&gt;left); if (tmp-&gt;right != NULL) que.push(tmp-&gt;right); &#125; &#125; return flag; &#125; 4.2.2 分层遍历 (自下而上分层遍历) 1234567891011121314151617181920212223242526272829303132333435vector&lt;vector&lt;int&gt;&gt; bfs(Node* root) &#123; vector &lt;vector&lt;int&gt; &gt; ans; if (root == NULL) return ans; queue&lt;Node*&gt; q; q.push(root); while(!q.empty()) &#123; vector&lt;int&gt; tv; for (int i = 0; i &lt; q.size(); ++i) &#123; Node* tmp = q.front(); q.pop(); if (tmp-&gt;lchild == NULL &amp;&amp; tmp-&gt;rchild != NULL)&#123; flag = false; break; &#125; if (tmp-&gt;left != NULL) que.push(tmp-&gt;left); if (tmp-&gt;right != NULL) que.push(tmp-&gt;right); tv.push_back(tmp-&gt;value); &#125; ans.push_back(tv) &#125; return flag; &#125;// reverse(res[i].begin(), res[i].end()); 4.3 difficult code 4.3.1 二叉树中和为某一值的路径 123456789101112131415161718void f4(Node*, int, int, vector&lt;int&gt;&amp;);void f4(Node* root, int exSum) &#123; if(root == NULL) return; vector&lt;int&gt; V; int curSum = 0; f4(root, exSum, curSum, V);&#125;void f4(Node* root, int exSum, int curSum, vecotr&lt;int&gt;&amp; path) &#123; curSum += root-&gt;value; path.push_back(root-&gt;value); if(curSum == exSum &amp;&amp; root-&gt;lchild == NULL &amp;&amp; root-&gt;rchild == NULL) &#123; //; 打印vector中的路径 &#125; if(root-&gt;lchild) f4(root-&gt;lchild, exSum, curSum, path); if(root-&gt;rchild) f4(root-&gt;rchild, exSum, curSum, path); curSum -= root-&gt;value; path.pop_back();&#125; 4.3.3 序列化二叉树 12345678910111213141516171819202122232425262728public String serialize(TreeNode root) &#123; if(root == null) return &quot;#,&quot;; StringBuffer res = new StringBuffer(root.val + &quot;,&quot;); res.append(serialize(root.left)); res.append(serialize(root.right)); return res.toString();&#125;// Decodes your encoded data to tree.public TreeNode deserialize(String data) &#123; String [] d = data.split(&quot;,&quot;); Queue&lt;String&gt; queue = new LinkedList&lt;&gt;(); for(int i = 0; i &lt; d.length; i++)&#123; queue.offer(d[i]); &#125; return pre(queue);&#125;public TreeNode pre(Queue&lt;String&gt; queue)&#123; String val = queue.poll(); if(val.equals(&quot;#&quot;)) return null; TreeNode node = new TreeNode(Integer.parseInt(val)); node.left = pre(queue); node.right = pre(queue); return node;&#125; 4.3.4 二叉搜索树后序遍历的结果 123456789101112bool f6(int* sec, int len) &#123; if(sec == NULL) return false; if(len &lt;= 1) return true; int i, rv = sec[len-1]; for(i = 0; i &lt; len-1; i++) &#123; if(sec[i] &gt; rv) break; &#125; for(int j = i; j &lt; len-1; j++) &#123; if(sec[j] &lt; rv) return false; &#125; return f6(sec, i) &amp;&amp; f6(sec+i, len-i-1);&#125; 4.3.5 二叉搜索树与双向链表 123456789101112void convert(Node* root, Node*&amp; pLast) &#123; if(root == NULL) return; if(root-&gt;lchild) convert(root-&gt;lchild, pLast); Node* pCur = root; pCur-&gt;lchild = pLast; if(pLast) pLast-&gt;rchild = pCur; pLast = pCur; if(root-&gt;rchild) convert(root-&gt;rchild, pLast);&#125; 4.3.6 二叉搜索树的第k个结点 给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8）中，按结点数值大小顺序第三小结点的值为4。 因为二叉搜索树按照中序遍历的顺序打印出来就是排好序的，所以，我们按照中序遍历找到第k个结点就是题目所求的结点。 12345678910111213141516171819202122class Solution &#123; public int kthSmallest(TreeNode root, int k) &#123; if(root == null) return Integer.MIN_VALUE; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); int count = 0; TreeNode p = root; while(p != null || !stack.isEmpty())&#123; if(p != null)&#123; stack.push(p); p = p.left; &#125;else&#123; TreeNode node = stack.pop(); count ++; if(count == k) return node.val; p = node.right; &#125; &#125; return Integer.MIN_VALUE; &#125;&#125; 5. 递归 / 回溯 5.1 斐波拉契 斐波拉契数列 &amp; 跳台阶 &amp; 变态跳台阶 2 * Fib(n-1). ， ✔️ 矩形覆盖 ， ✔️ 5.2 回溯 矩阵中的路径(BFS) 机器人的运动范围(DFS) 12345678910public int count(int threshold, int rows, int cols, int i, int j, int[][] flag)&#123; if(i&lt;0 || j&lt;0 || i&gt;=rows || j&gt;=cols || sum(i)+sum(j) &gt; threshold || flag[i][j] == 1)&#123; return 0; &#125; flag[i][j] = 1; return 1 + count(threshold, rows, cols, i - 1, j, flag) + count(threshold, rows, cols, i + 1, j, flag) + count(threshold, rows, cols, i, j - 1, flag) + count(threshold, rows, cols, i, j + 1, flag);&#125; 5.3 位运算 二进制中1的个数 n &amp; n-1.， ✔️ 数值的整数次方 dp. ， ✔️ 数组中只出现一次的数字 ok. ， ✔️ 6. Stack &amp; Queue &amp; heap Stack &amp; Queue 用两个栈实现队列 ， ✔️ 包含min函数的栈 ， ✔️ 栈的压入、弹出序列 6.1 用两个栈实现队列 12345678910111213141516171819202122232425class MyQueue &#123; Stack&lt;Integer&gt; input = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; output = new Stack&lt;Integer&gt;(); /** Push element x to the back of queue. */ public void push(int x) &#123; input.push(x); &#125; /** Removes the element from in front of queue and returns that element. */ public int pop() &#123; peek(); return output.pop(); &#125; /** Get the front element. */ public int peek() &#123; if(output.isEmpty())&#123; while(!input.isEmpty()) output.push(input.pop()); &#125; return output.peek(); &#125; /** Returns whether the queue is empty. */ public boolean empty() &#123; return input.isEmpty() &amp;&amp; output.isEmpty(); &#125;&#125; 6.2 包含min函数的栈 12345678910111213141516171819202122232425class MinStack &#123; Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; temp = new Stack&lt;Integer&gt;(); public void push(int x) &#123; stack.push(x); if(temp.isEmpty() || temp.peek() &gt;= x) temp.push(x); &#125; public void pop() &#123; int x = stack.pop(); int min = temp.peek(); if(x == min) temp.pop(); &#125; public int top() &#123; return stack.peek(); &#125; public int getMin() &#123; return temp.peek(); &#125;&#125; 6.3 栈的 push pop 序列 栈的 push pop 序列 1 2 3 4 5 4 3 5 1 2 1234567891011121314151617181920import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public boolean IsPopOrder(int [] pushA, int [] popA) &#123; if(pushA.length != popA.length || pushA.length == 0 || popA.length == 0) return false; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int index = 0; for(int i = 0; i &lt; pushA.length; i++)&#123; stack.push(pushA[i]); while(!stack.empty() &amp;&amp; stack.peek() == popA[index])&#123; stack.pop(); index++; &#125; &#125; return stack.empty(); &#125;&#125; 7. PriorityQueue 最小的K个数 数据流中的第K大元素 滑动窗口最大值 前K个高频单词 优先队列 8. Dynamic Programming 爬楼梯 ， ✔️ 不同路径 II ， ✔️ 编辑距离 ， ✔️ 不同路径 II 如果当前没有障碍物，dp[m][n] = dp[m - 1][n] + dp[m][n - 1] 如果有障碍物，则dp[m][n] = 0 编辑距离 如果单词1第i+1个字符和单词2第j+1个字符相同，那么就不用操作，则DP[i + 1][j + 1] = DP[i][j]; 如果不相同,则有三种可能操作，即增，删，替换。则取这三种操作的最优值，即dp[i + 1][j + 1] = 1 + Math.min(dp[i][j], Math.min(dp[i][j + 1], dp[i + 1][j])); 8.1 一维DP 连续子数组的最大和 // dp: F[i] = max(a[i], F[i-1]+a[i]); 8.2 二维DP 布尔数组 Longest Palindromic Substring/最长回文子串 给出一个字符串S，找到一个最长的连续回文串。 Interleaving String/交错字符串 输入三个字符串s1、s2和s3，判断第三个字符串s3是否由前两个字符串s1和s2交替而成且不改变s1和s2中各个字符原有的相对顺序。 数字数组 Unique Paths II/不同路径 (初始化很重要) ， 起点到终点有多少条不同路径，向右或向下走。 Minimum Path Sum 矩阵左上角出发到右下角，只能向右或向下走，找出哪一条路径上的数字之和最小。 Edit Distance/编辑距离 求两个字符串之间的最短编辑距离，即原来的字符串至少要经过多少次操作才能够变成目标字符串，操作包括删除一个字符、插入一个字符、更新一个字符。 Distinct Subsequences/不同子序列 给定S和T两个字符串，问把通过删除S中的某些字符，把S变为T有几种方法？ 补充：京东2019实习编程题-删除0或部分字符使其成为回文串 见笔试整理总结 8.3 三维DP 9. 剑指offer 算法学习5 数据流中的中位数 滑动窗口的最大值 (双向队列) 正则表达式匹配 数值的整数次方 两个链表的第一个公共节点（2个stack or 长短链表相减） diffcult 整数中1出现的次数 (判断每一位， 比如百位分别为 1, 0, 2~9, 后2种情况可合并) LRU Cache 需要深入学习Java的Map的内部实现 vector: vector::iterator, Modifiers (push_back, pop_back, insert) array : len = sizeof(arr)/sizeof(int) 1234567891011121314151617181920import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; maxInWindows(int [] num, int size)&#123; ArrayList&lt;Integer&gt; res = new ArrayList&lt;Integer&gt;(); LinkedList&lt;Integer&gt; deque = new LinkedList&lt;Integer&gt;(); if(num.length == 0 || size == 0) return res; for(int i = 0; i &lt; num.length; i++)&#123; if(!deque.isEmpty() &amp;&amp; deque.peekFirst() &lt;= i - size) deque.poll(); while(!deque.isEmpty() &amp;&amp; num[deque.peekLast()] &lt; num[i]) deque.removeLast(); deque.offerLast(i); if(i + 1 &gt;= size) res.add(num[deque.peekFirst()]); &#125; return res; &#125;&#125; 10. shopee &amp; sina shopee 下一个更大元素 (Stack &lt; Integer &gt; (), Map &lt; Int, Int &gt; map, map.getOrDefault(nums1[i], -1); 序列为 9 2 1 4 借助栈实现，判断栈顶 和 下一个元素的大小 ， ✔️ 鸡蛋掉落 (DP问题，难) google 扔鸡蛋，原题是 100 层楼，鸡蛋无限，答案 14 次。， ✔️ 二叉树的右视图 (层次遍历) res.push_back(q.back()-&gt;val);， ✔️ 复杂链表指针 ， ✔️ K 个一组翻转链表 1-&gt;2-&gt;3-&gt;4-&gt;5， 当 k = 2 时，应返: 2-&gt;1-&gt;4-&gt;3-&gt;5 ， ✔️ 不同的二叉搜索树 ， ✔️ 零钱兑换 完全背包问题 ，i=coins[j];i&lt;=amount;，dp[i]=Math.min(dp[i],dp[i-coins[j]]+1); ✔️ 相交链表 有效的括号 （Stack来解决） ， ✔️ 两数相加 [LeetCode] 887. Super Egg Drop 超级鸡蛋掉落 ， ✔️ 1dp[i][j] = min(dp[i][j], max(dp[i - 1][k - 1], dp[i][j - k]) + 1) ( 1 &lt;= k &lt;= j ) 之后可以再优化. 1234567891011121314151617181920212223class Solution: def isValid(self, s): &quot;&quot;&quot; :type s: str :rtype: bool &quot;&quot;&quot; stack = list() match = &#123;&#x27;&#123;&#x27;:&#x27;&#125;&#x27;, &#x27;[&#x27;:&#x27;]&#x27;, &#x27;(&#x27;:&#x27;)&#x27;&#125; for i in s: if i == &#x27;&#123;&#x27; or i == &#x27;(&#x27; or i == &#x27;[&#x27;: stack.append(i) else: if len(stack) == 0: return False top = stack.pop() if match[top] != i: return False if len(stack) != 0: return False return True sina 两数之和 (链表 carry) ， ✔️ 最大子序和 ， ✔️ 分治？ 搜寻名人 (if (knows(res, i)) res = i;) ， ✔️ 连续出现的数字 ， ✔️ 搜索二维矩阵 ， ✔️ 排序链表 翻转二叉树 ， ✔️ 买卖股票的最佳时机 系列 字符串转整型 无重复字符的最长子串 ， ✔️ （借助hashmap） Tencent Minimum Factorization 最小因数分解 [LeetCode 最小基因变化（广度优先搜索）] 1234567891011121314151617181920212223class Solution &#123;public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123; int carry = 0; ListNode* fakeHead = new ListNode(-1); ListNode* curr = fakeHead; while (l1 != NULL || l2 != NULL | carry != 0) &#123; if (l1 != NULL) &#123; carry += l1-&gt;val; l1 = l1-&gt;next; &#125; if (l2 != NULL) &#123; carry += l2-&gt;val; l2 = l2-&gt;next; &#125; ListNode* node = new ListNode(carry % 10); carry /= 10; curr-&gt;next = node; curr = node; &#125; return fakeHead-&gt;next; &#125;&#125;; 11. 海量数据 m_15 二叉树非递归的 先根遍历和中序遍历 (必会数据结构之一) 1234567891011121314151617181920212223242526272829303132void fPre(Node* root) &#123; // 先根遍历 根-&gt;左-&gt;右 Node* p = root; stack&lt;Node*&gt; S; while(1) &#123; if(p != NULL) &#123; cout &lt;&lt; p-&gt;value &lt;&lt; &#x27; &#x27;; S.push(p); p = p-&gt;lchild; &#125; else &#123; if(S.empty()) return; p = S.top(); S.pop(); p = p-&gt;rchild; &#125; &#125;&#125;void fIno(Node* root) &#123; Node* p = root; stack&lt;Node*&gt; S; while(1) &#123; if(p != NULL) &#123; S.push(p); p = p-&gt;lchild; &#125; else &#123; if(S.empty()) return; p = S.top(); S.pop(); cout &lt;&lt; p-&gt;value &lt;&lt; &#x27; &#x27;; p = p-&gt;rchild; &#125; &#125;&#125; leetcode CN leetcode EN Reference 知乎： [Leetcode][动态规划]相关题目汇总/分析/总结 简书： 2019 算法面试相关(leetcode)–动态规划(Dynamic Programming) CSDN leetcode DP","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/tags/leetcode/"}]},{"title":"Fine-grained Sentiment Analysis of User Online Reviews","slug":"nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews","date":"2019-06-25T02:16:21.000Z","updated":"2021-06-22T06:48:39.551Z","comments":true,"path":"2019/06/25/nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews/","link":"","permalink":"http://www.iequa.com/2019/06/25/nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews/","excerpt":"AI-Challenger","text":"AI-Challenger Challenger.AI Online reviews have become the critical factor to make consumption decision in recent years. They not only have a profound impact on the incisive understanding of shops, users, and the implied sentiment, but also have been widely used in Internet and e-commerce industry, such as personalized recommendation, intelligent search, product feedback, and business security. In this challenge, we provide a dataset of user reviews for fine-grained sentiment analysis from the catering industry, containning 335K public user reviews from Dianping.com. The dataset builds a two-layer labeling system according to the granularity, which contains 6 categories and 20 fine-grained elements. Training set: 105K Verification set: 15K Test set A: 15K Test set B: 200K There are four sentimental types for every fine-grained element: Positive, Neutral, Negative and Not mentioned, which are labelled as 1, 0, -1 and-2. The meaning of these four labels are listed below. An example of one labelled review: “味道不错的面馆，性价比也相当之高，分量很足～女生吃小份，胃口小的，可能吃不完呢。环境在面馆来说算是好的，至少看上去堂子很亮，也比较干净，一般苍蝇馆子还是比不上这个卫生状况的。中午饭点的时候，人很多，人行道上也是要坐满的，隔壁的冒菜馆子，据说是一家，有时候也会开放出来坐吃面的人。“ 「AI Challenger」是面向全球人工智能人才的开源数据集和编程竞赛平台。AI Challenger 2018 由创新工场、搜狗、美团点评、美图公司联合主办。有上万支团队参赛， 覆盖 81 个国家、1100 所高校、990 家公司。 min_count设置为2貌似也有一些负向影响， word_ngrams 2， epoch 10 . 1.3 baseline 效果 1234567service_wait_time:0.5247890022873511service_waiters_attitude:0.6781093513108542service_parking_convenience:0.5828932335474249service_serving_speed:0.6146828053320519......f1_score: 0.5513 调参： 1python main_train.py -mn fasttext_model_wn2.pkl -wn 2 约跑15分钟左右，存储的模型大约在17G，验证集 macro F1值结果如下： 12345678service_wait_time:0.5247890022873511service_waiters_attitude:0.6881098513108542service_parking_convenience:0.5828935095474249service_serving_speed:0.6168828054420539...f1_score: 0.5783 这个结果看起来还不错，我们可以基于这个fasttext多分类模型进行测试集的预测： 1python main_predict.py -mn fasttext_wn2_model.pkl 优化方法： 去停用词和去一些标点符号，调参，learning_rate的影响是比较直接的，min_count 1.4 fastText 速度快 能够做到效果好，速度快，主要依靠两个秘密武器： 利用了 词内的n-gram信息 (subword n-gram information) 用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick. 2. Attention RNN、RCNN 2.1 预处理 data 粗暴使用char模型，用到的停用词也不多。 trainsets lines： 501132， 合法例子 ： 105000 validationset lines： 70935, 合法例子 ： 15000 testsets lines： 72028， 合法例子 ： 15000 数据预处理，生成 train_char.csv、test_char.csv、test_char.csv 三个文件: 123-rw-r--r-- 1 blair 10:36 test_char.csv-rw-r--r-- 1 blair 10:09 train_char.csv-rw-r--r-- 1 blair 10:32 validation_char.csv word2vec： 维度 100， 窗口 10， 过滤掉次数小于 1~2 的字 13.1M chars.vector 过滤掉低频词之后： 1word2vec/chars.vector 为 7983 * 100 2.2 Attention RCNN Attention-RNN (0.637)、Attention-RCNN (0.669) Attention 参考自 Kaggle 的 Attention Model Kaggle 常见文本分类结构: 2层GRU 接Attention层，然后和 avgpool、maxpool concat 接起来. 1def model(self, embeddings_matrix, maxlen, word_index, num_class): 为了之后 summary 看清楚网络结构，所以我们一些参数先写死看一下： 123456789101112131415161718import kerasfrom keras import Modelfrom keras.layers import *# from JoinAttLayer import Attentionmaxlen=1200inp = Input(shape=(maxlen,)) # 当输入序列的长度固定时，该值为其长度 1200 （一个文档doc的最大长度）encode = Bidirectional(CuDNNGRU(128, return_sequences=True))encode2 = Bidirectional(CuDNNGRU(128, return_sequences=True))# attention = Attention(maxlen)x_4 = Embedding(7555+ 1,#7983+1 # 词汇表大小， 即，最大整数 index + 1, len(word_index) + 1, # input_dim 100, # output_dim: int &gt;= 0。词向量的维度。 input_length=maxlen, # maxlen=1200, 一个 doc 最大长度 trainable=True)(inp) 接下来： 1234567891011121314151617181920212223242526x_3 = encode(x_4)x_3 = encode2(x_3)# 输入shape， 形如（samples，steps，features）的3D张量# 输出shape， 形如(samples, features)的2D张量avg_pool_3 = GlobalAveragePooling1D()(x_3) # GlobalAveragePooling1D 为时域信号施加全局平均值池化max_pool_3 = GlobalMaxPooling1D()(x_3) # 对于时间信号的全局最大池化# attention_3 = attention(x_3)x = keras.layers.concatenate([avg_pool_3, max_pool_3], name=&quot;fc&quot;)x = Dense(4, activation=&quot;softmax&quot;)(x)adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, amsgrad=True)rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)model = Model(inputs=inp, outputs=x)model.compile( loss=&#x27;categorical_crossentropy&#x27;, optimizer=adam)# categorical_crossentropy 用来做多分类问题# binary_crossentropy 用来做多标签分类问题model.summary() RNN： RCNN： 1234567891011121314151617181920212223x_4 = Embedding(7555+ 1,#7983+1 # 词汇表大小， 即，最大整数 index + 1 100, input_length=maxlen, trainable=True)(inp)x_3 = encode(x_4)x_3 = encode2(x_3)x_3 = Conv1D(64, kernel_size=3, padding=&quot;valid&quot;, kernel_initializer=&quot;glorot_uniform&quot;)(x_3)# 输入shape， 形如（samples，steps，features）的3D张量# 输出shape， 形如(samples, features)的2D张量avg_pool_3 = GlobalAveragePooling1D()(x_3) # GlobalAveragePooling1D 为时域信号施加全局平均值池化max_pool_3 = GlobalMaxPooling1D()(x_3) # 对于时间信号的全局最大池化# attention_3 = attention(x_3)x = keras.layers.concatenate([avg_pool_3, max_pool_3], name=&quot;fc&quot;)x = Dense(4, activation=&quot;softmax&quot;)(x)...model.summary() Input 一个网络层次，输入层 在 keras SpatialDropout1D ，那么常规的 dropout 将无法使激活正则化，且导致有效的学习速率降低。 SpatialDropout1D ，在这种情况下，SpatialDropout1D 将有助于提高特征图之间的独立性，应该使用它来代替 Dropout。 CuDNNGRU 是 基于CuDNN的快速GRU实现，只能在GPU上运行，只能使用 tensoflow 为后端 CuDNNLSTM 是 基于CuDNN的快速LSTM实现，只能在GPU上运行，只能使用 tensoflow 为后端 attention = Attention(maxlen) Embedding嵌入层将正整数（下标）转换为具有固定大小的向量，如[4, [20]]-&gt;[[0.25, 0.1], [0.6, -0.2]] 12345678keras.layers.embeddings.Embedding( input_dim, output_dim, embeddings_initializer=&#x27;uniform&#x27;, # embeddings_regularizer=None, activity_regularizer=None, # embeddings_constraint=None, mask_zero=False, input_length=None) Embedding 的一些参数解释： Embedding层只能作为模型的第一层 input_dim: int &gt; 0。词汇表大小， 即，最大整数 index + 1。 output_dim: int &gt;= 0。词向量的维度。 embeddings_initializer: 嵌入矩阵的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。 Convolutional Neural Networks (week1) - CNN , 运用 Padding TensorFlow中CNN的两种padding方式“SAME”和“VALID” word2vec : 7983 100 word2vec/chars.vector 过滤掉低频词 循环卷积神经网络(RCNN)，并将其应用于文本分类的任务。首先，我们应用一个双向的循环结构，与传统的基于窗口的神经网络相比，它可以大大减少噪声，从而最大程度地捕捉上下文信息。此外，该模型在学习文本表示时可以保留更大范围的词序。其次，我们使用了一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。我们的模型结合了RNN的结构和最大池化层，利用了循环神经模型和卷积神经模型的优点。此外，我们的模型显示了O(n)的时间复杂度，它与文本长度的长度是线性相关的。 RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。 RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。 CNN 优点： 提取数据中的局部位置的特征，然后再拼接池化层。 CNN可以更好地捕捉文本的语义。是O(n) CNN 优点： 一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。 首先我们来理解下什么是卷积操作？卷积，你可以把它想象成一个应用在矩阵上的滑动窗口函数。 卷积网络也就是对输入样本进行多次卷积操作，提取数据中的局部位置的特征，然后再拼接池化层（图中的Pooling层）做进一步的降维操作 我们可以把CNN类比N-gram模型，N-gram也是基于词窗范围这种局部的方式对文本进行特征提取，与CNN的做法很类似 自然语言处理系列（8）：RCNN Keras之文本分类实现 12345678910111213141516171819202122232425from keras.backend.tensorflow_backend import set_sessionimport tensorflow as tfconfig = tf.ConfigProto()config.gpu_options.allow_growth = Trueset_session(tf.Session(config=config))import randomrandom.seed = 42import pandas as pdfrom tensorflow import set_random_seedset_random_seed(42)from keras.preprocessing import text, sequencefrom keras.callbacks import ModelCheckpoint, Callbackfrom sklearn.metrics import f1_score, recall_score, precision_scorefrom keras.layers import *from classifier_bigru import TextClassifierfrom gensim.models.keyedvectors import KeyedVectorsimport pickleimport gc 2.3 loss function 多分类和多标签分类, gensim训练word2vec及相关函数 多分类：类别数目大于2个，类别之间是互斥的。比如是猫，就不能是狗、猪 categorical crossentropy 用来做多分类问题 binary crossentropy 用来做多标签分类问题 sigmoid,softmax,binary/categorical crossentropy的联系？ Binary cross-entropy 常用于二分类问题，当然也可以用于多分类问题，通常需要在网络的最后一层添加sigmoid进行配合使用 Categorical cross-entropy 适用于多分类问题，并使用softmax作为输出层的激活函数的情况。 2.4 Early Stop 需要在每个 epoch 结束之后去计算模型的 F1 值，这样可以更好的掌握模型的训练情况。 Tips 如果我们在训练中设置 metric 的话，得到是每个 batch 的 F1 值, 是不靠谱的. 类似这样: 1234567891011121314151617181920def getClassification(arr): arr = list(arr) if arr.index(max(arr)) == 0: return -2 elif arr.index(max(arr)) == 1: return -1 elif arr.index(max(arr)) == 2: return 0 else: return 1class Metrics(Callback): def on_train_begin(self, logs=&#123;&#125;): self.val_f1s = [] self.val_recalls = [] self.val_precisions = [] def on_epoch_end(self, epoch, logs=&#123;&#125;): ... return early_stop，就是在训练模型的时候，当在验证集上效果不再提升的时候，就提前停止训练，节约时间。 2.6 Max Length (padding) 所有评论平均的长度是 200 左右，max_length 取 2 * 200，效果一直不给力. 将 max_length 改为 1200 ，macro f-score 效果明显提升 Tips： 多分类问题中，那些长度很长的评论可能会有部分属于那些样本数很少的类别，padding过短会导致这些长评论无法被正确划分。 3. ELMO-Like ai-challenger-2018-文本挖掘类竞赛相关解决方案及代码汇总 QA问答系统中的深度学习技术实现 深度学习代码复现之减少随机性的影响","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Sentiment","slug":"Sentiment","permalink":"http://www.iequa.com/tags/Sentiment/"}]},{"title":"User Credit Score","slug":"datascience/credit-score","date":"2019-06-23T05:28:21.000Z","updated":"2021-06-22T06:27:34.129Z","comments":true,"path":"2019/06/23/datascience/credit-score/","link":"","permalink":"http://www.iequa.com/2019/06/23/datascience/credit-score/","excerpt":"Credit Score Card","text":"Credit Score Card 信用评分卡模型是最常见的金融风控手段之一，它是指根据客户的各种属性和行为数据，利用一定的信用评分模型，对客户进行信用评分，据此决定是否给予授信以及授信的额度和利率，从而识别和减少在金融交易中存在的交易风险。 按照借贷用户的借贷时间，评分卡模型可以划分为以下三种： 贷前： Application score card， 又称为A卡 贷中： Behavior score card， 又称为B卡 贷后： Collection score card， 又称为C卡 申请评分卡要求最为严格，也最为重要，可解释性也要求最强，一般主流模型为 LR. 如需了解更多关于 互联网金融风控 的相关背景知识，请参阅： Internet Financial Risk Control (part1) ： The Fraud Risk of Financial Technology Enterprises Internet Financial Risk Control (part2) ： model strategy Internet Financial Risk Control (part3) ： Lending Club Data dev 1. 数据获取 金融机构自身字段： 年龄，户籍，性别，收入，…； 第三方机构的数据： 消费行为… 数据情况： 23W+ 去掉一些灰用户，剩余 M1标准： 好/坏: 13W+ 坏 4K 30~50 : 1 都是正常的 2. EDA Exploratory Data Analysis 每个字段的缺失值情况、异常值情况、平均值、中位数、最大值、最小值、分布情况等 牛顿插值的几何解释是怎么样的？ 如何直观地理解拉格朗日插值法？ 从牛顿插值法到泰勒公式 泰勒公式一句话描述：就是用多项式函数去逼近光滑函数 缺失值的处理，还可以用 RF 去拟合. 3. 数据预处理 (1). 数据清洗 (2). 变量分箱 (3). WOE 编码 缺失值太多 非数值变量多 (emp_title…) id, member_id 等 loan_amnt != df.funded_amnt 空值填充为 0 带 % 的浮点，去掉 % 好坏比是 34:1 是非常难以处理的样本了. 3.1 数据清洗 缺失值太多 123456789# 处理对象类型的缺失，uniquedf.select_dtypes(include=[&#x27;O&#x27;]).describe().T.\\assign(missing_pct=df.apply(lambda x : (len(x)-x.count())/float(len(x))))# 缺失值特别高的可以删除掉# 我们可以对 非数值型 变量，做一个循环，发现 zip_code 有 873 个 unique 的值，那么先不处理for col in df.select_dtypes(include=[&#x27;object&#x27;]).columns: print (&quot;Column &#123;&#125; has &#123;&#125; unique instances&quot;.format( col, len(df[col].unique())) ) 3.2 变量分箱 对连续变量进行分段离散化； 将多状态的离散变量进行合并，减少离散变量的状态数。 先可以粗分箱，之后在合并。 比如 年龄取值数有 30个，那么分为 30 箱，最后合为 5 箱。 其他变量可能粗分箱为 100 箱，之后合并. 分箱方法很多，最常见的方法之一： Merge分箱中的 Chimerge 分箱. Chimerge 其基本思想是如果两个相邻的区间具有类似的类分布，则这两个区间合并； 否则，它们应保持分开。Chimerge通常采用卡方值来衡量两相邻区间的类分布情况。 连续值按升序排列，离散值先转化为坏客户的比率，然后再按升序排列； 为了减少计算量，对于状态数大于某一阈值 (建议为100) 的变量，利用等频分箱进行粗分箱。 若有缺失值，则缺失值单独作为一个分箱。 分箱的最大区间数 &amp; 分箱初始化 这里我取： n = 5 连续值按升序排列，离散值先转化为坏客户的比率，然后再按升序排列； 为了减少计算量，对于状态数大于某一阈值 (建议为100) 的变量，利用等频分箱进行粗分箱。 若有缺失值，则缺失值单独作为一个分箱。 合并区间： 将卡方值最小的一对区间合并 分箱后处理： 对于坏客户比例为 0 或 1 的分箱进行合并 (一个分箱内不能全为好客户或者全为坏客户)。 对于分箱后某一箱样本占比超过 95% 的箱子进行删除。 检查缺失分箱的坏客户比例是否和非缺失分箱相等，如果相等，进行合并。 总结一下特征分箱的优势： 特征分箱可以有效处理特征中的缺失值和异常值。 特征分箱后，数据和模型会更稳定。 特征分箱可以简化逻辑回归模型，降低模型过拟合的风险，提高模型的泛化能力。 将所有特征统一变换为类别型变量。 分箱后变量才可以使用标准的评分卡格式，即对不同的分段进行评分。 列表内容特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 变量分箱实践 One-Hot编码与哑变量 方差、标准差和均方根误差的区别总结 基于卡方分箱的评分卡建模 good 基于卡方分箱的评分卡建模 , 卡方分布—chi-square distribution, χ2-distribution 卡方值的意义,举个例子： 某医院对某种病症的患者使用了 AAA，BBB 两种不同的疗法，结果如表1，问两种疗法有无差别？ 组别 有效 无效 合计 有效率（%） A组 19 24 43 44.2 B组 34 10 44 77.3 合计 53 34 87 60.9 可以计算出各格内的期望频数： 第1行1列： 43×53/87=26.2 , 第1行2列： 43×34/87=16.8 , 第2行1列： 44×53/87=26.8 , 第2行2列： 44×34/87=17.2 A箱 (26.2-19)*(26.2-19) / 26.2 -&gt; (26.2 是 正类的期望频数， 19 是真实频数) (16.8-24)*(16.8-24) / 16.8 (16.8 是 负类的期望频数) B箱 (26.8-34)*(26.8-34) / 26.8 (26.8 是 正类的期望频数) (17.2-10)*(17.2-10) / 17.2 (17.2 是 负类的期望频数) 先建立原假设：A、B两种疗法没有区别。根据卡方值的计算公式，计算：卡方值=10.01。得到卡方值以后，接下来需要查询卡方分布表来判断p值，从而做出接受或拒绝原假设的决定。 点击时的区域标题： 卡方分布—chi-square distribution, χ2-distribution χ2-distribution: 若 $k$ 个独立的随机变量 $Z\\_1, Z\\_2, ..., Z\\_k$ 满足标准正态分布 $N(0,1)$ , 则这 $k$ 个随机变量的平方和： 卡方检验—χ2检验是以χ2分布为基础的一种假设检验方法，主要用于分类变量之间的独立性检验 思想是根据样本数据推断 总体分布与期望分布 是否有显著性差异，或者推断两个分类变量是否相关或者独立。 一般可以设原假设为 ：观察频数与期望频数没有差异，或者两个变量相互独立不相关。 实际应用中，我们先假设原假设成立，计算出卡方值，卡方表示观察值与理论值间的偏离程度。 卡方值只是一个中间过程，通过卡方值计算出p值，p值才是我们最重要需要的。p小于0.05意味着存在显著差异。 卡方值是非参数检验中的一个统计量，主要用于非参数统计分析中。它的作用是检验数据的相关性。如果卡方值的显著性（即SIG.）小于0.05，说明两个变量是显著相关的。 3.3 WOE (weight of evidence) WOE 称为证据权重(weight of evidence) , 将离散变量转化为连续变量。 WOE 将预测类别的集中度的属性作为编码的数值。对于自变量第 iii 箱的WOE值为： p_i1p\\_{i1}p_i1 是第 iii 箱中坏客户占所有坏客户比例 p_i0p\\_{i0}p_i0 是第 iii 箱中好客户占所有好客户比例 WOE: 当前分箱中坏客户和好客户的比值，和所有样本中这个比值的差异 当分箱中坏客户和好客户的比例等于随机坏客户和好客户的比值时，说明这个分箱没有预测能力，即WOE=0。 (WOE为0，说明该箱出的特征对结果没有区分度) 实际上WOE编码相当于把分箱后的特征从非线性可分映射到近似线性可分的空间内: 总结一下WOE编码的优势： 可提升模型的预测效果 将自变量规范到同一尺度上 WOE能反映自变量取值的贡献情况 有利于对变量的每个分箱进行评分 转化为连续变量之后，便于分析变量与变量之间的相关性 与独热向量编码相比，可以保证变量的完整性，同时避免稀疏矩阵和维度灾难 4. 变量筛选 主要衡量标准: 变量的预测能力和变量的线性相关性。 挑选入模变量: 变量的预测能力 变量之间的线性相关性 变量在业务上的可解释性 变量两两相关性分析，变量的多重共线性分析。 4.1 单变量筛选 IV 信息价值(information value)，自变量的IV值越大，表示自变量的预测能力越强。 类似指标还有信息增益等。 变量对应的IV值为所有分箱对应的 IV 值之和： IV 值实际上式变量各个分箱的加权求和。且和决策树中的交叉熵有异曲同工之妙。以下为交叉熵公式： IV值的具体的计算流程如下： IV排序后，选择IV&gt;0.02的变量，共58个变量IV&gt;0.02 4.2 多变量分析 保留相关性低于阈值0.6的变量，剩余27个变量 为什么要进行相关性分析？ 理想状态下，系数权重会有无数种取法，使系数权重变得无法解释，导致变量的每个分段的得分也有无数种取法（后面我们会发现变量中不同分段的评分会用到变量的系数） 总结一下变量筛选的意义： 剔除跟目标变量不太相关的特征 消除由于线性相关的变量，避免特征冗余 减轻后期验证、部署、监控的负担 保证变量的可解释性 特征相关度筛选 1234cor = df.corr()cor.loc[:,:] = np.tril(cor, k=-1) # below main lower triangle of an arraycor = cor.stack()cor[(cor &gt; 0.55) | (cor &lt; -0.55)] # 特征相关度筛选 4.3 显著性分析 删除P值不显著的变量，剩余12个变量了。 GBDT, GBRT, Xgboost, RF grid search 123456param_grid = &#123; &#x27;learning_rate&#x27;: [0.1, 0.05, 0.02, 0.01], &#x27;max_depth&#x27;: [1,2,3,4], &#x27;min_samples_split&#x27;: [50,100,200,400], &#x27;n_estimators&#x27;: [100,200,400,800]&#125; 特征重要度： 1234# Top Ten， GBRT top 10 的参数有哪些，可以作为参考， GBRT 不仅是一个分类器，还能帮你筛选变量# 比如 feature_importance = 0 的话，那么下一次 这个特征，你就去掉就可以了# 重新用模型GBRT重新跑，或者用其他模型LR跑# 或者其他的 RF 也可以帮助你做特征筛选 最后选择出 84 个变量， 然后在放到不同的模型中做训练，在 ensemble 应该效果还是不错的。 特征不稳定的，不可以作为入模变量： 挑选变量的时候，开始每个月我一直在看它的均值和方差的变化是否在容忍的范围内，超过50%舍 5. 转化为评分卡 将 odds 带入可得： 评分卡的分值可以定义为比率对数的线性表达来，即： 最终得到评分卡模型： 需要设定两个假设： 某个特定的违约概率下的预期评分，即比率 即比率 odds\\text{odds}odds 为 θ_0θ\\_0θ_0 时的分数为 P_0P\\_0P_0 该odds为2θ_02θ\\_02θ_0情况下评分的减少量（PDO） 6. KS 评估 K-S曲线其实数据来源和本质和ROC曲线是一致的，只是ROC曲线是把真正率和假正率当作横纵轴，而K-S曲线是把真正率和假正率都当作是纵轴，横轴则由选定的阈值来充当。 由于ks值能找出模型中差异最大的一个分段，因此适合用于cut_off，像评分卡这种就很适合用ks值来评估。但是ks值只能反映出哪个分段是区分最大的，而不能总体反映出所有分段的效果，因果AUC值更能胜任。 KS值表示了模型将+和-区分开来的能力。值越大，模型的预测准确性越好。 KS 值表示了模型区分好坏客户的能力。 等分 10 份，两条洛伦兹曲线， TPR 与 FPR 的差值. 好坏客户的区程度. 其实质是 TPR−FPRTPR - FPRTPR−FPR 随好坏客户阈值变化的最大值。KS 值越大，模型的预测准确性越好。一般，KS &gt; 0.4 即认为模型有很好的预测性能。 Reference Python三大评分卡之行为评分卡 玩转逻辑回归之金融评分卡模型 拍拍贷教你如何用GBDT做评分卡 模型区分度指标 KS 值 分类模型评判指标 ROC，AUC，GINI，KS 深入理解KS AUC和KS指标 一文完全理解模型ks指标含义并画出ks曲线","categories":[{"name":"data-science","slug":"data-science","permalink":"http://www.iequa.com/categories/data-science/"}],"tags":[{"name":"Credit-Score","slug":"Credit-Score","permalink":"http://www.iequa.com/tags/Credit-Score/"}]},{"title":"Chatbot Project","slug":"nlp/chatbot/chatbot-project","date":"2019-06-20T11:16:21.000Z","updated":"2021-06-22T06:47:48.431Z","comments":true,"path":"2019/06/20/nlp/chatbot/chatbot-project/","link":"","permalink":"http://www.iequa.com/2019/06/20/nlp/chatbot/chatbot-project/","excerpt":"chatbot","text":"chatbot ����������������聊天机器人（chatbot），也被称为会话代理或对话系统，现已成为了一个热门话题。微软在聊天机器人上押上了重注，Facebook（M）、苹果（Siri）、谷歌 和 Slack 等公司也是如此。 新一波创业者们正在尝试改变消费者与服务的交互方式。 Chatbot Research 1 - 聊天机器人的行业综述 Chatbot Research 2 - NLP 的基础知识回顾 Chatbot Research 3 - 机器学习构建 chatbot Chatbot Research 4 - 深度学习知识回顾 Chatbot Research 5 - 基于深度学习的检索聊天机器人 Chatbot Research 6 - 更多论文 (感谢 PaperWeekly) Chatbot Research 7 - Dialog_Corpus 常用数据集 Chatbot Research 8 - 理论 seq2seq+Attention 机制模型详解 Chatbot Research 11 - 第二个版本 (新版实现) Chatbot Research 12 - 理论篇： 评价指标介绍 Chatbot Research 13 - 理论篇： MMI 模型理论 Chatbot Useful Links 预备知识 1.1 词嵌入（word2vec） 1.2 近似训练 1.3 Word2vec 的实现 1.4 子词嵌入（fastText） 1.5 全局向量的词嵌入（GloVe） 1.6 求近义词和类比词 1.7 文本情感分类：使用 RNN 1.8 文本情感分类：使用 CNN（textCNN） 1.9 编码器—解码器（seq2seq） 1.10 束搜索 beam-search 1.11 Attention机制 1.12 tensorflow模型线上部署 2. TensorFlow TensorFlow 用于机器学习和神经网络方面的研究，采用数据流图来进行数值计算的开源软件库. Keras 开发重点是支持快速的实验。能够以最小的时延把你的想法转换为实验结果，是做好研究的关键。 2.1 TensorFlow 简介 1.1 TensorFlow Why ? 1.2 TensorFlow 快速学习 &amp; 文档 2.2 Tensorflow 基础构架 2.1 处理结构: 计算图 2.2 完整步骤 例子2 🌰（创建数据、搭建模型、计算误差、传播误差、训练） 2.3 Session 会话控制 2.4 Variable 变量 2.5 Placeholder 传入值 2.6 什么是激励函数 (Activation Function) 2.7 激励函数 Activation Function 2.8 TensorFlow 基本用法总结 🌰🌰🌰 2.3 建造我们第一个神经网络 3.1 添加层 def add_layer() 3.2 建造神经网络 🌰🌰🌰 3.3 Speed Up Training &amp; Optimizer (转载自莫烦) 2.4 Tensorboard 4.1 Tensorboard 可视化好帮手 1 2.5 Estimator 5.1 tf.contrib.learn 快速入门 5.2 tf.contrib.learn 构建输入函数 5.3 tf.contrib.learn 基础的记录和监控教程 5.4 tf.contrib.learn 创建 Estimator 5.5 TF 保存和加载模型 - 简书 2.6 Language model 介绍 语言模型是自然语言处理问题中一类最基本的问题，它有着非常广泛的应用。 1.1 RNN 循环神经网络 简介 1.2 LSTM &amp; Bi-RNN &amp; Deep RNN 1.3 Language model 介绍 / 评价方法 perplexity (not finish) 2.7 NNLM (神经语言模型) 2.2 PTB 数据的 batching 方法 2.3 RNN 的语言模型 TensorFlow 实现 2.8 MNIST 数字识别问题 3.1 简单前馈网络实现 mnist 分类 3.3 name / variable_scope 3.4 多层 LSTM 通俗易懂版 4. Python Python 哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码. Python 数据分析模块: Numpy &amp; Pandas, 及强大的画图工具 Matplotlib Python Numpy &amp; Pandas Matplotlib 5. Scikit-Learn Sklearn 机器学习领域当中最知名的 Python 模块之一 why 1.1 : Sklearn Choosing The Right Estimator 1.2 : Sklearn General Learning Model 1.3 : Sklearn DataSets 1.4 : Sklearn Common Attributes and Functions 1.5 : Normalization 1.6 : Cross-validation 1 1.7 : Cross-validation 2 1.8 : Cross-validation 3 1.9 : Sklearn Save Model","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"}]},{"title":"BERT tutorial 1","slug":"nlp/BERT_tutorial_1","date":"2019-06-20T03:00:21.000Z","updated":"2021-06-20T04:12:28.333Z","comments":true,"path":"2019/06/20/nlp/BERT_tutorial_1/","link":"","permalink":"http://www.iequa.com/2019/06/20/nlp/BERT_tutorial_1/","excerpt":"bert","text":"bert 2018.10 google 发布 BERT 模型. 引爆整个AI圈的 NLP 模型. 在 NLP领域 刷新 11 项记录. BERT 其实是 language_encoder，把输入的 sentence 或 paragraph 转成 feature_vector（embedding）. Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT 创新点在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的 NLP 任务. 1. NLP 的发展 NLP 神经网络发展历史中最重要的 8 个里程碑 Language Model (语言模型就是要看到上文预测下文, So NNLM) n-gram model（n元模型）（基于 马尔可夫假设 思想）上下文相关的特性 建立数学模型。 2001 - NNLM , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。 2008 - Multi-task learning 2013 - Word2Vec (Word Embedding的工具word2vec : CBOW 和 Skip-gram) 2014 - sequence-to-sequence 2015 - Attention 2015 - Memory-based networks 2018 - Pretrained language models good 张俊林: 深度学习中的注意力模型（2017版） good 张俊林: 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史 NNLM vs Word2Vec NNLM 目标： 训练语言模型， 语言模型就是要看上文预测下文， word embedding 只是无心的一个副产品。 Word2Vec目标： 它单纯就是要 word embedding 的，这是主产品。 2018 年之前的 Word Embedding 有个缺点就是无法处理 多义词 的问题, 静态词嵌. ELMO: Embedding from Language Models ELMO的论文题目：“Deep contextualized word representation” NAACL 2018 最佳论文 - ELMO： Deep contextualized word representation ELMO 本身是个根据当前上下文对Word Embedding动态调整的思路。 ELMO 有什么缺点？ LSTM 抽取特征能力远弱于 Transformer 拼接方式双向融合特征能力偏弱 **GPT (Generative Pre-Training) ** 第一个阶段是利用 language 进行 Pre-Training. 第二阶段通过 Fine-tuning 的模式解决下游任务。 GPT: 有什么缺点？ 要是把 language model 改造成双向就好了 不太会炒作，GPT 也是非常重要的工作. Bert 亮点 : 效果好 和 普适性强 Transformer 特征抽取器 Language Model 作为训练任务 (双向) Bert 采用和 GPT 完全相同的 两阶段 模型： Pre-Train Language Model； Fine-&gt; Tuning模式解决下游任务。 NLP 的 4大任务 4 NLP task description 序列标注 特点是句子中每个单词要求模型根据上下文都要给出一个 分类label； 分类任务 特点是不管文章有多长，总体给出一个分类label 即可； 句子关系判断 特点是给定两个句子，模型判断出两个句子 是否具备某种语义关系； 生成式任务 特点是输入文本内容后，需要自主生成另外一段文字。 1. 简介 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Bert的预训练和微调（图片来自Bert的原论文) 2. 原理篇 本章将会先给大家介绍BERT的核心transformer，而transformer又是由attention组合而成. 2.1 Attention机制讲解 苏神《Attention is All You Need》浅读（简介+代码） Attention机制讲解 张俊林: 深度学习中的注意力模型（2017版） 2.2 Transrofmer模型讲解 Transformer模型详解 BERT大火却不懂Transformer？读这一篇就够了 Jay Alammar’s Blog Transformer The Illustrated Transformer【译】 2.3 BERT原理 图示详解BERT模型的输入与输出 bert input More Reading： 张俊林: 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史 张俊林: 放弃幻想，全面拥抱Transformer：三大特征抽取器（CNN/RNN/TF）比较 3. 代码篇 4. 实践篇 当Bert遇上Keras：这可能是Bert最简单的打开姿势 hanxiao/bert-as-service PaperWeekly: 两行代码玩转Google BERT句向量词向量 5. BERT hanxiao大佬开源出来的bert-as-service框架很适合初学者 Netycc’s blog 利用Bert构建句向量并计算相似度 BERT使用详解(实战) BERT中文文本相似度计算与文本分类 Reference 一步步理解BERT [从语言模型到Seq2Seq：Transformer如戏，全靠Mask][w1] BERT完全指南 张俊林: 天空之城：拉马努金式思维训练法 互联网人到了 30 岁，大部分都去干什么了？ AINLP BERT相关论文、文章和代码资源汇总 自然语言处理中的Transformer和BERT RNN和LSTM弱！爆！了！注意力模型才是王道 NLP突破性成果 BERT 模型详细解读 一步步理解BERT 当Bert遇上Keras：这可能是Bert最简单的打开姿势 good 张俊林: 放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://www.iequa.com/tags/BERT/"}]},{"title":"Seq2Seq and Attention","slug":"deeplearning/Seq2Seq-Attention","date":"2019-06-17T02:00:21.000Z","updated":"2021-06-22T06:31:05.518Z","comments":true,"path":"2019/06/17/deeplearning/Seq2Seq-Attention/","link":"","permalink":"http://www.iequa.com/2019/06/17/deeplearning/Seq2Seq-Attention/","excerpt":"Attention 和人类的选择性视觉注意力机制类似","text":"Attention 和人类的选择性视觉注意力机制类似 我们先结合上篇文章的内容，将 language model 和 Machine translation model 做一个对比： 可以看到，机器翻译模型的后半部分其实就是语言模型，Andrew 将其称之为 “条件语言模型”. P(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;)P(y^{&lt;1&gt;},…,y^{&lt;{T\\_y}&gt;}|x^{&lt;1&gt;},…,x^{&lt;{T\\_x}&gt;}) P(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;) 1. Encoder-Decoder Encoder-Decoder Source 和 Target 分别由各自的单词序列构成： Source=(x_1,x_2,...,x_m)Source = ({x}\\_1, {x}\\_2, ..., {x}\\_m) Source=(x_1,x_2,...,x_m) Target=(y_1,y_2,...,y_n)Target = ({y}\\_1, {y}\\_2, ..., {y}\\_n) Target=(y_1,y_2,...,y_n) Encoder 顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C： C=F(x_1,x_2,...,x_m)C = F({x}\\_1, {x}\\_2, ..., {x}\\_m) C=F(x_1,x_2,...,x_m) 对于 Decoder 来说，其任务是根据句子 Source 的 中间语义表示 C 和 之前已经生成的历史信息 (y_1,y_2,...,y_i−1)({y}\\_1, {y}\\_2, ..., {y}\\_{i-1}) (y_1,y_2,...,y_i−1) 来生成 i时刻 要生成的单词 y_i{y}\\_{i}y_i y_i=g(C,y_1,y_2,...,y_i−1)y\\_{i} = g(C, {y}\\_1, {y}\\_2, ..., {y}\\_{i-1}) y_i=g(C,y_1,y_2,...,y_i−1) 每个 y_iy\\_iy_i 都依次这么产生，那么看起来就是整个系统根据输入 句子Source 生成了目标句子Target。 (1). 如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题； (2). 如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要； (3). 如果Source是一句问句，Target是一句回答，那么这是问答系统。 Encoder-Decoder框架 不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation 1.1 encoder 用函数 fff 表达 RNN 隐藏层的变换： h_t=f(x_t,h_t−1).\\boldsymbol{h}\\_t = f(\\boldsymbol{x}\\_t, \\boldsymbol{h}\\_{t-1}). h_t=f(x_t,h_t−1). 然后 Encoder 通过自定义函数 qqq 将各个时间步的隐藏状态变换为背景变量 c=q(h_1,…,h_T).\\boldsymbol{c} = q(\\boldsymbol{h}\\_1, \\ldots, \\boldsymbol{h}\\_T). c=q(h_1,…,h_T). 例如，当选择 q(h_1,…,h_T)=h_Tq(\\boldsymbol{h}\\_1, \\ldots, \\boldsymbol{h}\\_T) = \\boldsymbol{h}\\_Tq(h_1,…,h_T)=h_T 时，背景变量是输入序列最终时间步的隐藏状态 h_T\\boldsymbol{h}\\_Th_T。 以上描述的编码器是一个单向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用 Bi-RNN 构造编码器。 这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。 1.2 decoder 上小节 Encode 编码器输出的背景变量 ccc 编码了整个输入序列 x_1,…,x_Tx\\_1, \\ldots, x\\_Tx_1,…,x_T 的信息。 给定 train sample 的 input sequence： y_1,y_2,…,y_T′y\\_1, y\\_2, \\ldots, y\\_{T&#x27;}y_1,y_2,…,y_T′，对每个时间步 t′t&#x27;t′（符号与 input sequence 或 encoder 的时间步 ttt 有区别）， decoder 输出 y_t′y\\_{t&#x27;}y_t′ 的条件概率将基于之前的 output sequence： y_1,…,y_t′−1y\\_1,\\ldots,y\\_{t&#x27;-1}y_1,…,y_t′−1 和 ccc. 即: P(y_t′∣y_1,…,y_t′−1,c)P(y\\_{t&#x27;} \\mid y\\_1, \\ldots, y\\_{t&#x27;-1}, \\boldsymbol{c}) P(y_t′∣y_1,…,y_t′−1,c) 为此，我们可以使用另一个RNN作为解码器。 在输出序列的时间步 t′t^\\primet′，解码器将上一时间步的输出 y_t′−1y\\_{t^\\prime-1}y_t′−1 以及背景变量 ccc 作为输入，并将它们与上一时间步的隐藏状态 h_t′−1\\boldsymbol{h}\\_{t^\\prime-1}h_t′−1 变换为当前时间步的隐藏状态 h_t′\\boldsymbol{h}\\_{t^\\prime}h_t′。因此，我们可以用函数 ggg 表达解码器隐藏层的变换： h_t′=g(y_t′−1,c,h_t′−1).\\boldsymbol{h}\\_{t^\\prime} = g(y\\_{t^\\prime-1}, \\boldsymbol{c}, \\boldsymbol{h}\\_{t^\\prime-1}). h_t′=g(y_t′−1,c,h_t′−1). 可使用自定义的 output layer 和 softmax 计算 P(y_t′∣y_1,…,y_t′−1,c){P}(y\\_{t^\\prime} \\mid y\\_1, \\ldots, y\\_{t^\\prime-1}, \\boldsymbol{c})P(y_t′∣y_1,…,y_t′−1,c)，计算当前时间步输出 y_t′y\\_{t^\\prime}y_t′ 的概率分布. 1.3 decoder greedy search 在语言模型之前有一 个条件也就是被翻译的句子: P(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;)P(y^{&lt;1&gt;},…,y^{&lt;{T\\_y}&gt;}|x^{&lt;1&gt;},…,x^{&lt;{T\\_x}&gt;}) P(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;) 但是我们知道翻译是有很多种方式的，同一句话可以翻译成很多不同的句子，那么如何判断哪一句子是最好的呢？ 还是翻译上面那句话，有如下几种翻译结果： “Jane is visiting China in September.” “Jane is going to visit China in September.” “In September, Jane will visit China” “Jane’s Chinese friend welcomed her in September.” … 得到最好的翻译结果，转换成数学公式就是: argmaxP(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;)argmax P(y^{&lt;1&gt;},…,y^{&lt;{T\\_y}&gt;}|x^{&lt;1&gt;},…,x^{&lt;{T\\_x}&gt;}) argmaxP(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;) 那么 Greedy Search 就是每次输出的那个都必须是最好的。还是以翻译那句话为例。 现在假设通过贪婪搜索已经确定最好的翻译的前两个单词是：&quot;Jane is &quot; 然后因为 “going” 出现频率较高和其它原因，所以根据贪婪算法得出此时第三个单词的最好结果是 “going”。 所以据贪婪算法最后的翻译结果可能是下图中的第二个句子，但第一句可能会更好. 所以 Greedy Search 的缺点是局部最优并不代表全局最优. Greedy Search 更加短视，看的不长远。 1.4 decoder beam search Beam Search 是 greedy search 的加强版本，首先要预设一个值 beam width，这里等于 3 (如果等于 1 就是 greedy search)。然后在每一步保存最佳的 3 个结果进行下一步的选择，以此直到遇到句子的终结符. 1.4.1 step 1 如下图示，因为beam width=3，所以根据输入的需要翻译的句子选出 3 个 y&lt;1&gt;y^{&lt;1&gt;}y&lt;1&gt;最可能的输出值。 即选出 P(y&lt;1&gt;∣x)P(y^{&lt;1&gt;}|x)P(y&lt;1&gt;∣x) 最大的前3个值。 假设分别是 “in”, “jane”, &quot;september&quot; 1.4.2 step 2 以&quot;in&quot;为例进行说明，其他同理. 如下图示，在给定被翻译句子 xxx 和确定 y&lt;1&gt;y^{&lt;1&gt;}y&lt;1&gt; = “in” 的条件下，下一个输出值的条件概率是 P(y&lt;2&gt;∣x,&quot;in&quot;)P(y^{&lt;2&gt;}|x,&quot;in&quot;)P(y&lt;2&gt;∣x,&quot;in&quot;)。 此时需要从 10000 种可能中找出条件概率最高的前 3 个. 又由公式: P(y&lt;1&gt;,y&lt;2&gt;∣x)=P(y&lt;1&gt;∣x)P(y&lt;2&gt;∣x,y&lt;1&gt;)P(y^{&lt;1&gt;},y^{&lt;2&gt;}|x)=P(y^{&lt;1&gt;}|x) P(y^{&lt;2&gt;}|x, y^{&lt;1&gt;}) P(y&lt;1&gt;,y&lt;2&gt;∣x)=P(y&lt;1&gt;∣x)P(y&lt;2&gt;∣x,y&lt;1&gt;) 我们此时已经得到了给定输入数据，前两个输出值的输出概率比较大的组合了. 另外 2 个单词也做同样的计算 此时我们得到了 9 组 P(y&lt;1&gt;,y&lt;2&gt;∣x)P(y^{&lt;1&gt;},y^{&lt;2&gt;}|x)P(y&lt;1&gt;,y&lt;2&gt;∣x), 此时我们再从这 9组 中选出概率值最高的前 3 个。 如下图示，假设是这3个： “in september” “jane is” “jane visits” 1.4.3 step 3 继续 step 2 的过程，根据 P(y&lt;3&gt;∣x,y&lt;1&gt;,y&lt;2&gt;)P(y^{&lt;3&gt;}|x,y^{&lt;1&gt;},y^{&lt;2&gt;})P(y&lt;3&gt;∣x,y&lt;1&gt;,y&lt;2&gt;) 选出 P(y&lt;1&gt;,y&lt;2&gt;,y&lt;3&gt;∣x)P(y^{&lt;1&gt;},y^{&lt;2&gt;},y^{&lt;3&gt;}|x)P(y&lt;1&gt;,y&lt;2&gt;,y&lt;3&gt;∣x) 最大的前3个组合. 后面重复上述步骤得出结果. 1.4.4 summary 总结一下上面的步骤就是： (1). 经过 encoder 以后，decoder 给出最有可能的三个开头词依次为 “in”, “jane”, “september” P(y&lt;1&gt;∣x)P(y^{&lt;1&gt;}|x) P(y&lt;1&gt;∣x) (2). 经 step 1 得到的值输入到 step 2 中，最可能的三个翻译为 “in september”, “jane is”, “jane visits” P(y&lt;2&gt;∣x,y&lt;1&gt;)P(y^{&lt;2&gt;}|x,y^{&lt;1&gt;}) P(y&lt;2&gt;∣x,y&lt;1&gt;) (这里，september开头的句子由于概率没有其他的可能性大，已经失去了作为开头词资格) (3). 继续这个过程… P(y&lt;3&gt;∣x,y&lt;1&gt;,y&lt;2&gt;)P(y^{&lt;3&gt;}|x,y^{&lt;1&gt;},y^{&lt;2&gt;}) P(y&lt;3&gt;∣x,y&lt;1&gt;,y&lt;2&gt;) 1.5 refinements to beam search P(y&lt;1&gt;,….,P(yT_y)∣x)=P(y&lt;1&gt;∣x)P(y&lt;2&gt;∣x,y&lt;1&gt;)…P(y&lt;T_y&gt;∣x,y&lt;1&gt;,…y&lt;T_y−1&gt;)P(y^{&lt;1&gt;},….,P(y^{T\\_y})|x)=P(y^{&lt;1&gt;}|x)P(y^{&lt;2&gt;}|x,y^{&lt;1&gt;})…P(y^{&lt;{T\\_y}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{T\\_y-1}&gt;}) P(y&lt;1&gt;,….,P(yT_y)∣x)=P(y&lt;1&gt;∣x)P(y&lt;2&gt;∣x,y&lt;1&gt;)…P(y&lt;T_y&gt;∣x,y&lt;1&gt;,…y&lt;T_y−1&gt;) 所以要满足 argmaxP(y&lt;1&gt;,….,P(yT_y)∣x)argmax P(y^{&lt;1&gt;},….,P(y^{T\\_y})|x)argmaxP(y&lt;1&gt;,….,P(yT_y)∣x), 也就等同于要满足 argmax∏_t=1T_yP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;)argmax \\prod\\_{t=1}^{T\\_y}P(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;}) argmax∏_t=1T_yP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;) 但是上面的公式存在一个问题，因为概率都是小于1的，累乘之后会越来越小，可能小到计算机无法精确存储，所以可以将其转变成 log 形式（因为 log 是单调递增的，所以对最终结果不会有影响），其公式如下： argmax∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;)argmax \\sum\\_{t=1}^{T\\_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;}) argmax∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;) But！！！上述公式仍然存在bug，观察可以知道，概率值都是小于1的，那么log之后都是负数，所以为了使得最后的值最大，那么只要保证翻译的句子越短，那么值就越大，所以如果使用这个公式，那么最后翻译的句子通常都是比较短的句子，这显然不行。 所以我们可以通过归一化的方式来纠正，即保证平均到每个单词都能得到最大值。其公式如下： argmax1T_y∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;)argmax \\frac{1}{T\\_y}\\sum\\_{t=1}^{T\\_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;}) argmaxT_y1​∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;) 归一化的确能很好的解决上述问题，但是在实际运用中，会额外添加一个参数 ααα, 其大小介于 0 和 1 之间 argmax1T_yα∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;)argmax \\frac{1}{T\\_y^α}\\sum\\_{t=1}^{T\\_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;}) argmaxT_yα1​∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;) T_yT\\_yT_y 为输出句子中单词的个数，ααα 是一个超参数 (可以设置为 0.7) ααα == 1. 则代表 完全用句子长度归一化 ααα == 0. 则代表 没有归一化 ααα == 0~1. 则代表 在 句子长度归一化 与 没有归一化 之间的折中程度. beam width = B = 3**10**100 是会有一个明显的增长，但是 B 从 1000 ~ 3000 是并没有一个明显增长的. 1.6 train seq2seq model 根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率 \\begin{split}\\begin{aligned} {P}(y\\_1, \\ldots, y\\_{T&#039;} \\mid x\\_1, \\ldots, x\\_T) &amp;= \\prod\\_{t&#039;=1}^{T&#039;} {P}(y\\_{t&#039;} \\mid y\\_1, \\ldots, y\\_{t&#039;-1}, x\\_1, \\ldots, x\\_T)\\\\\\\\ &amp;= \\prod\\_{t&#039;=1}^{T&#039;} {P}(y\\_{t&#039;} \\mid y\\_1, \\ldots, y\\_{t&#039;-1}, \\boldsymbol{c}), \\end{aligned}\\end{split} 并得到该输出序列的损失 −log⁡P(y_1,…,y_T′∣x_1,…,x_T)=−∑_t′=1T′log⁡P(y_t′∣y_1,…,y_t′−1,c), - \\log{P}(y\\_1, \\ldots, y\\_{T&#x27;} \\mid x\\_1, \\ldots, x\\_T) = -\\sum\\_{t&#x27;=1}^{T&#x27;} \\log {P}(y\\_{t&#x27;} \\mid y\\_1, \\ldots, y\\_{t&#x27;-1}, \\boldsymbol{c}), −logP(y_1,…,y_T′∣x_1,…,x_T)=−∑_t′=1T′logP(y_t′∣y_1,…,y_t′−1,c), 在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图中所描述的模型预测中，我们需要将decode在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列在上一个时间步的标签作为decode在当前时间步的输入。这叫做强制教学（teacher forcing）。 1.7 summary 编码器 - 解码器（seq2seq）可以输入并输出不定长的序列。 编码器—解码器使用了两个 RNN。 在编码器—解码器的训练中，我们可以采用 teacher forcing。(这也是 Seq2Seq 2 的内容) 2. Seq2Seq 框架2 Seq2Seq model 来自于 “Sequence to Sequence Learning with Neural Networks” 其模型结构图如下所示： 与上面模型最大的区别在于其source编码后的 向量CCC 直接作为 Decoder RNN 的 init state，而不是在每次decode时都作为 RNN cell 的输入。此外，decode 时 RNN 的输入是 label，而不是前一时刻的输出。 Encoder 阶段： 每个词经过 RNN 都会编码为 hidden (e0,e1,e2), source序列 的编码向量e 就是 最终的 hidden state e2 Tips： 这里 e_0,e_1,e_2e\\_0, e\\_1, e\\_2e_0,e_1,e_2 是 hidden state， 并没有经过 g 和 softmax . Decoder 阶段： e向量 仅作为 RNN 的 init state 传入decode模型，每一时刻输入都是前一时刻的正确label。直到最终输入符号截止. 3. Seq2Seq Attention 请务必要阅读： 张俊林 深度学习中的注意力模型（2017版） decode 在各个时间步依赖相同的 背景变量 ccc 来获取输入序列信息。当 encode 为 RNN 时，背景变量ccc 来自它最终时间步的 hidden state。 英语输入：“They”、“are”、“watching”、“.” 法语输出：“Ils”、“regardent”、“.” 翻译例子：输入为英语序列“They”、“are”、“watching”、“.”，输出为法语序列“Ils”、“regardent”、“.”。，decode 在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步 1，解码器可以主要依赖“They”、“are”的信息来生成“Ils”，在时间步 2 则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步 3 则直接映射句号“.”。这看上去就像是在 decode 的每一时间步对输入序列中不同时间步的编码信息分配不同的注意力一样。这也是注意力机制的由来 1。 仍以 RNN 为例，Attention 通过对 Encode 所有时间步的隐藏状态做加权平均来得到背景变量ccc。Decode 在每一时间步调整这些权重，即 Attention weight，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量ccc。本节我们将讨论 Attention机制 是怎么工作的。 在“encoder-decoder（seq2seq）”, Decoder 在时间步 t′t&#x27;t′ 的 hidden state s_t′=g(y_t′−1,c,s_t′−1)\\boldsymbol{s}\\_{t&#x27;} = g(\\boldsymbol{y}\\_{t&#x27;-1}, \\boldsymbol{c}, \\boldsymbol{s}\\_{t&#x27;-1}) s_t′=g(y_t′−1,c,s_t′−1) 在 Attention机制 中, Decoder 的每一时间步将使用可变的背景变量ccc s_t′=g(y_t′−1,c_t′,s_t′−1).\\boldsymbol{s}\\_{t&#x27;} = g(\\boldsymbol{y}\\_{t&#x27;-1}, \\boldsymbol{c}\\_{t&#x27;}, \\boldsymbol{s}\\_{t&#x27;-1}). s_t′=g(y_t′−1,c_t′,s_t′−1). 关键是如何计算背景变量 c_t′\\boldsymbol{c}\\_{t&#x27;}c_t′ 和如何利用它来更新隐藏状态 s_t′\\boldsymbol{s}\\_{t&#x27;}s_t′。以下将分别描述这两个关键点。 3.1 计算背景变量 c c_t′=∑_t=1Tα_t′th_t,\\boldsymbol{c}\\_{t&#x27;} = \\sum\\_{t=1}^T \\alpha\\_{t&#x27; t} \\boldsymbol{h}\\_t, c_t′=∑_t=1Tα_t′th_t, 其中给定 t′t&#x27;t′ 时，权重 α_t′t\\alpha\\_{t&#x27; t}α_t′t 在 t=1,…,Tt=1,\\ldots,Tt=1,…,T 的值是一个概率分布。为了得到概率分布，可以使用 softmax 运算: α_t′t=exp⁡(e_t′t)∑_k=1Texp⁡(e_t′k),t=1,…,T.\\alpha\\_{t&#x27; t} = \\frac{\\exp(e\\_{t&#x27; t})}{ \\sum\\_{k=1}^T \\exp(e\\_{t&#x27; k}) },\\quad t=1,\\ldots,T. α_t′t=∑_k=1Texp(e_t′k)exp(e_t′t)​,t=1,…,T. 现在，我们需要定义如何计算上式中 softmax 运算的输入 e_t′te\\_{t&#x27; t}e_t′t。由于 e_t′te\\_{t&#x27; t}e_t′t 同时取决于decode的时间步 t′t&#x27;t′ 和encode的时间步 ttt，我们不妨以解码器在时间步 t′−1t&#x27;−1t′−1 的隐藏状态 s_t′−1\\boldsymbol{s}\\_{t&#x27; - 1}s_t′−1 与编码器在时间步 ttt 的隐藏状态 h_th\\_th_t 为输入，并通过函数 aaa 计算 e_t′te\\_{t&#x27; t}e_t′t： e_t′t=a(s_t′−1,h_t).e\\_{t&#x27; t} = a(\\boldsymbol{s}\\_{t&#x27; - 1}, \\boldsymbol{h}\\_t). e_t′t=a(s_t′−1,h_t). 这里函数 a 有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积 a(s,h)=s⊤ha(\\boldsymbol{s}, \\boldsymbol{h})=\\boldsymbol{s}^\\top \\boldsymbol{h}a(s,h)=s⊤h。而最早提出Attention机制的论文则将输入连结后通过含单隐藏层的多层感知机MLP 变换 a(s,h)=v⊤tanh⁡(W_ss+W_hh),a(\\boldsymbol{s}, \\boldsymbol{h}) = \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}\\_s \\boldsymbol{s} + \\boldsymbol{W}\\_h \\boldsymbol{h}), a(s,h)=v⊤tanh(W_ss+W_hh), 其中 v、W_s、W_hv、W\\_s、W\\_hv、W_s、W_h 都是可以学习的模型参数。 3.2 update hidden state 以 GRU 为例，在解码器中我们可以对 GRU 的设计稍作修改。解码器在时间步 t′t&#x27;t′ 的隐藏状态为 s_t′=z_t′⊙s_t′−1+(1−z_t′)⊙s~_t′,\\boldsymbol{s}\\_{t&#x27;} = \\boldsymbol{z}\\_{t&#x27;} \\odot \\boldsymbol{s}\\_{t&#x27;-1} + (1 - \\boldsymbol{z}\\_{t&#x27;}) \\odot \\tilde{\\boldsymbol{s}}\\_{t&#x27;}, s_t′=z_t′⊙s_t′−1+(1−z_t′)⊙s~_t′, 其中的重置门、更新门和候选隐含状态分别为 : \\begin{split}\\begin{aligned} \\boldsymbol{r}\\_{t&#039;} &amp;= \\sigma(\\boldsymbol{W}\\_{yr} \\boldsymbol{y}\\_{t&#039;-1} + \\boldsymbol{W}\\_{sr} \\boldsymbol{s}\\_{t&#039; - 1} + \\boldsymbol{W}\\_{cr} \\boldsymbol{c}\\_{t&#039;} + \\boldsymbol{b}\\_r),\\\\\\\\ \\boldsymbol{z}\\_{t&#039;} &amp;= \\sigma(\\boldsymbol{W}\\_{yz} \\boldsymbol{y}\\_{t&#039;-1} + \\boldsymbol{W}\\_{sz} \\boldsymbol{s}\\_{t&#039; - 1} + \\boldsymbol{W}\\_{cz} \\boldsymbol{c}\\_{t&#039;} + \\boldsymbol{b}\\_z),\\\\\\\\ \\tilde{\\boldsymbol{s}}\\_{t&#039;} &amp;= \\text{tanh}(\\boldsymbol{W}\\_{ys} \\boldsymbol{y}\\_{t&#039;-1} + \\boldsymbol{W}\\_{ss} (\\boldsymbol{s}\\_{t&#039; - 1} \\odot \\boldsymbol{r}\\_{t&#039;}) + \\boldsymbol{W}\\_{cs} \\boldsymbol{c}\\_{t&#039;} + \\boldsymbol{b}\\_s), \\end{aligned}\\end{split} 其中含下标的 W 和 b 分别为 GRU 的权重参数和偏差参数。 3.3 attention summary 可以在decode的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。 Attention机制可以采用更为高效的矢量化计算。 除此之外模型为了取得比较好的效果还是用了下面三个小技巧来改善性能： 深层次的LSTM：作者使用了4层LSTM作为encoder和decoder模型，并且表示深层次的模型比shallow的模型效果要好（单层，神经元个数多）。 将source进行反序输入：输入的时候将“ABC”变成“CBA”，这样做的好处是解决了长序列的long-term依赖，使得模型可以学习到更多的对应关系，从而达到比较好的效果。 注意力机制是一种思想，可以有多种不同的实现方式，在 Seq2Seq 模型以外的场景也有不少应用 4. Attention 本质思想 把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易懂: 4.1 Attention 的三阶段 第一个阶段根据Query和Key计算两者的相似性或者相关性； 第二个阶段对第一阶段的原始分值进行归一化处理； 根据权重系数对Value进行加权求和。 4.2 Self Attention Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。 引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。 请务必要阅读： 张俊林 深度学习中的注意力模型（2017版） Reference 动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制 seq2seq+Attention机制模型详解 深度学习前沿笔记 百面 seq2seq模型 百面 注意力机制 Bert遇上Keras Sequence-Models-week3 seq2seq中的beam search算法过程 深度学习中的注意力模型（2017版）","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"Seq2Seq","slug":"Seq2Seq","permalink":"http://www.iequa.com/tags/Seq2Seq/"}]},{"title":"Language Model and Perplexity","slug":"nlp/Language-Model-and-Word-Embedding","date":"2019-06-16T03:00:21.000Z","updated":"2021-06-22T06:48:39.565Z","comments":true,"path":"2019/06/16/nlp/Language-Model-and-Word-Embedding/","link":"","permalink":"http://www.iequa.com/2019/06/16/nlp/Language-Model-and-Word-Embedding/","excerpt":"2001 NNLM","text":"2001 NNLM 计算机很多事情比人类做得好，那么机器是否能懂 Natural Language? 一些 NLP 技术的应用: 简单的任务：拼写检查，关键词检索，同义词检索等 复杂的任务：信息提取、情感分析、文本分类等 更复杂任务：机器翻译、人机对话、QA系统 Natural Language 逐渐演变成一种 上下文信息表达 和 传递 的方式。 让计算机处理自然语言，一个基本的问题就是为 自然语言 这种 上下文相关的特性 建立数学模型。 1. Language Model 美联储主席昨天告诉媒体 7000 亿美金的救助资金将借给上百家银行、汽车公司。 美联储主席昨天 7000 亿美金的救助资金告诉媒体将借给上百家银行、汽车公司。 美联储主席昨天 告媒诉体 70 亿00美金的救助资金上百家银行将借给、汽车公司。 上世纪70年代科学家们试图用规则文法判断句子是否合理。贾里尼克用统计模型解决方法更有效。 如果 S 表示一连串特定顺序排列的词 w1w_1w1​， w2w_2w2​，…， wnw_nwn​ ，换句话说，S 表示的是一个有意义的句子。机器对语言的识别从某种角度来说，就是想知道 S 在文本中出现的可能性，也就是数学上所说的 S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是 P(S) 可展开为： P(S)=P(w1)P(w2∣w1)P(w3∣w1w2)…P(wn∣w1w2…wn−1)P(S) = P(w_1)P(w_2|w_1)P(w_3| w_1 w_2)…P(w_n|w_1 w_2…w_{n-1}) P(S)=P(w1​)P(w2​∣w1​)P(w3​∣w1​w2​)…P(wn​∣w1​w2​…wn−1​) 1.1 Markov assumption 假定文本中的每个词 wiw_iwi​ 和 前面N-1个词有关，而和更前面的词无关，这样当前词 wiw_iwi​ 的概率值取决于前面 N-1个词 P(wi−N+1,wi−N+2,...,wi−1)P(w_{i-N+1}, w_{i-N+2}, ..., w_{i-1})P(wi−N+1​,wi−N+2​,...,wi−1​) P(wi∣w1,w2,...,wi−1)=P(wi∣wi−N+1,wi−N+2,...,wi−1)P(w_{i}|w_{1}, w_{2}, ..., w_{i-1}) = P(w_i | w_{i-N+1}, w_{i-N+2}, ..., w_{i-1}) P(wi​∣w1​,w2​,...,wi−1​)=P(wi​∣wi−N+1​,wi−N+2​,...,wi−1​) N元模型， N=2 时，为二元模型。 在实际中应用最多的是 N=3 的三元模型. 1.2 n-gram, n=2 P(S)=P(w1)P(w2∣w1)P(w3∣w2)…P(wi∣wi−1)…P(S) = P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_i|w_{i-1})… P(S)=P(w1​)P(w2​∣w1​)P(w3​∣w2​)…P(wi​∣wi−1​)… 接下来如何估计 P(wi∣wi−1)P (w_i|w_{i-1})P(wi​∣wi−1​)。只要机器数一数这对词 (wi−1,wi)(w_i{-1}, w_i)(wi​−1,wi​) 在统计的文本中出现了多少次，以及 wi−1w_{i-1}wi−1​ 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了, P(wi∣wi−1)=P(wi−1,wi)P(wi−1)P(w_i|w_{i-1}) = \\frac {P(w_{i-1}, w_i)} {P(w_{i-1})} P(wi​∣wi−1​)=P(wi−1​)P(wi−1​,wi​)​ 2. Perplexity, PPL 语言模型效果的常用指标 perplexity， 在测试集上 perplexity 越低，说明建模效果越好. 计算perplexity的公式如下： perplexity 刻画的是语言模型预测一个语言样本的能力. 比如已经知道 (w1,w2,w3,…,wm) 这句话会出现在语料库之中，那么通过语言模型计算得到的这句话的概率越高，说明语言模型对这个语料库拟合得越好。 perplexity 实际是计算每一个单词得到的概率倒数的几何平均，因此 perplexity 可以理解为平均分支系数（average branching factor），即模型预测下一个词时的平均可选择数量。 例如，考虑一个由0~9这10个数字随机组成的长度为m的序列，由于这10个数字出现的概率是随机的，所以每个数字出现的概率是 。因此，在任意时刻，模型都有10个等概率的候选答案可以选择，于是perplexity就是10（有10个合理的答案）。 perplexity的计算过程如下： 在语言模型的训练中，通常采用 perplexity 的对数表达形式： 相比较乘积求平方根的方式，加法的形式可加速计算，同时避免概率乘积数值过小而导致浮点数向下溢出的问题. 在数学上，log perplexity 可以看作真实分布与预测分布之间的交叉熵 Cross Entropy, 交叉熵描述了两个概率分布之间的一种距离. log perplexity 和 Cross Entropy 是等价的 在神经网络模型中，P(wi∣w1,,...,wi−1)P(w_i | w_{1}, , ..., w_{i-1})P(wi​∣w1​,,...,wi−1​) 分布通常是由一个 softmax层 产生的，TensorFlow 中提供了两个方便计算交叉熵的函数，可以将 logits 结果直接放入输入，来帮助计算 softmax 然后再进行计算 Cross Entropy. 12cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y)cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = y) 知乎_习翔宇 3. NNLM NNLM,直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程. 既然离散的表示有辣么多缺点，于是有小伙伴就尝试着用模型最优化的过程去转换词向量了. 计算复杂度： (N \\* D + N \\* D \\* H + H \\* V) 相当之高, 于是有了 CBOW 和 Skip-Gram . NN 训练 语言模型， 会顺带产生一个 Word Embedding 矩阵. 词嵌矩阵 * 单词的独热编码 = 单词的词嵌 (300, 10000) * (10000, 1) = (300, 1) 可以通过训练神经网络的方式构建词嵌表 E . 下图展示了预测单词的方法，即给出缺少一个单词的句子： “I want a glass of orange ___” 计算方法是将已知单词的特征向量都作为输入数据送到神经网络中去，然后经过一系列计算到达 Softmax分类层，在该例中输出节点数为 10000个。经过计算 juice 概率最高，所以预测为 “I want a glass of orange juice” 在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表 EEE 假设要预测的单词为 WWW，词嵌表仍然为 EEE，需要注意的是训练词嵌表和预测 WWW 是两个不同的任务。 如果任务是预测 WWW，最佳方案是使用 WWW 前面 nnn 个单词构建语境。 如果任务是训练 EEE，除了使用 WWW 前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 Skip Gram 算法），这些方法都能提供很好的结果。 3.1 Word Representation 单词与单词之间是有很多共性的，或在某一特性上相近，比如“苹果”和“橙子”都是水果；或者在某一特性上相反，比如“父亲”在性别上是男性，“母亲”在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率，这里用一个简化的表格说明: Man (5391) Woman (9853) Apple (456) Orange (6257) 性别 -1 1 0 年龄 0.01 0.02 -0.01 食物 0.04 0.01 0.95 颜色 0.03 0.01 0.70 在表格中可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。括号里的数字代表这个单词在独热编码中的位置，可以用这个数字代表这个单词比如 Man = ，Man 的特性用 ，也就是那一纵列。 在实际的应用中，特性的数量远不止 4 种，可能有几百种，甚至更多。对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。 压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个就是 “Word Embeddings” 的概念. 上图通过聚类将词性相类似的单词在二维空间聚为一类. 3.2 Word Embeddings 先下一个非正规定义 “词嵌 - 描述了词性特征的总量，也是在高维词性空间中嵌入的位置，拥有越多共性的词，词嵌离得越近，反之则越远”。值得注意的是，表达这个“位置”，需要使用所有设定的词性特征，假如有 300 个特征（性别，颜色，…），那么词嵌的空间维度就是 300. 3.3 使用词嵌三步 获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库 应用词嵌：将获得的词嵌应用在我们的训练任务中 可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了） 词嵌实用场景: No. sencentce replace word target 1 Sally Johnson is an orange farmer. orange Sally Johnson 2 Robert Lin is an apple farmer. apple Robert Lin 3 Robert Lin is a durian cultivator. durian cultivator Robert Lin 我们继续替换，我们将 apple farmer 替换成不太常见的 durian cultivator (榴莲繁殖员)。此时词嵌入中可能并没有 durian 这个词，cultivator 也是不常用的词汇。这个时候怎么办呢？我们可以用到迁移学习。 词嵌入迁移学习步骤如下： 学习含有大量文本语料库的词嵌入 (一般含有 10亿 到 1000亿 单词)，或者下载预训练好的词嵌入 将学到的词嵌入迁移到相对较小规模的训练集 (例如 10万 词汇). (可选) 这一步骤就是对新的数据进行 fine-tune。 4. word2vec word2vec 并不是一个模型， 而是一个 2013年 google 发表的工具. 该工具包含2个模型： Skip-Gram 和 CBOW. 及两种高效训练方法： negative sampling 和 hierarchicam softmax. CBOW Continous Bag of Words Model Skip-Gram Model 词向量（词的特征向量）既能够降低维度，又能够capture到当前词在本句子中上下文的信息. Word2Vec Word2Vec词嵌入矩阵 4.1 CBOW 纠错 : 上图”目标函数“的第一个公式，应该是 连乘 公式，不是 连加 运算。 理解 : 背景词向量与 中心词向量 内积 等部分，你可考虑 softmax w \\* x+b 中 xxx 和 www 的关系来理解. 4.2 Skip-Gram 跳字模型假设基于某个词来生成它在文本序列周围的词。举个例子，假设文本序列是“the”“man”“loves”“his”“son”。以“loves”作为中心词，设背景窗口大小为2。如图10.1所示，跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词“the”“man”“his”“son”的条件概率，即 P(the,man,his,son∣loves).P(\\textrm{the},\\textrm{man},\\textrm{his},\\textrm{son}\\mid\\textrm{loves}). P(the,man,his,son∣loves). 假设给定中心词的情况下，背景词的生成是相互独立的，那么上式可以改写成 P(the∣loves)⋅P(man∣loves)⋅P(his∣loves)⋅P(son∣loves).P(\\textrm{the}\\mid\\textrm{loves})\\cdot P(\\textrm{man}\\mid\\textrm{loves})\\cdot P(\\textrm{his}\\mid\\textrm{loves})\\cdot P(\\textrm{son}\\mid\\textrm{loves}). P(the∣loves)⋅P(man∣loves)⋅P(his∣loves)⋅P(son∣loves). 训练 Skip-Gram 跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数： −∑t=1T∑−m≤j≤m, j≠0log P(w(t+j)∣w(t)). - \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\text{log}\\, P(w^{(t+j)} \\mid w^{(t)}). −t=1∑T​−m≤j≤m, j​=0∑​logP(w(t+j)∣w(t)). 如果使用随机梯度下降，那么在每一次迭代里我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，首先看到 log⁡P(wo∣wc)=uo⊤vc−log⁡(∑i∈Vexp(ui⊤vc))\\log P(w_o \\mid w_c) = \\boldsymbol{u}_o^\\top \\boldsymbol{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)\\right) logP(wo​∣wc​)=uo⊤​vc​−log(i∈V∑​exp(ui⊤​vc​)) 它的计算需要词典中所有词以 wcw_cwc​ 为中心词的条件概率。有关其他词向量的梯度同理可得。 训练结束后，对于词典中的任一索引为 iii 的词，我们均得到该词作为中心词和背景词的两组词向量 viv_ivi​ 和 uiu_iui​ 。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。 两个向量越相似，他们的点乘也就越大. 小结： 最大似然估计 MLE 最小化损失函数（与第一步等价），损失函数对数联合概率的相反数 描述概率函数，该函数的自变量是词向量（u和v），词向量也是模型参数 对第二步中每一项求梯度。有了梯度就可以优化第二步中的损失函数，从而迭代学习到模型参数，也就是词向量。 4.3 高效近似训练 hierarchicam softmax negative sampling 5. fastText FastText是一个快速文本分类算法，在使用标准多核CPU的情况下，在10分钟内可以对超过10亿个单词进行训练。 不需要使用预先训练好的词向量，因为FastText会自己训练词向量。 文本分类： 情感分类: fastText 能够做到效果好，速度快，主要依靠两个秘密武器： 利用了 词内的n-gram信息 (subword n-gram information) 用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick. fastText 和 word2vec 的区别: 两者表面的不同： 模型的输出层： word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用； 模型的输入层： word2vec的输出层，是 context window 内的term；而fasttext对应的整个sentence的内容，包括term，也包括 n-gram的内容； 两者本质的不同，体现在 h-softmax 的使用： Wordvec的目的是得到词向量，该词向量最终是在输入层得到，输出层对应的 h-softmax也会生成一系列的向量，但最终都被抛弃，不会使用。 fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个） Reference 《数学之美》 读书笔记 word2vec前世今生 CS224N NLP with Deep Learning: Lecture 1 课程笔记 good, sklearn 中 CountVectorizer、TfidfTransformer 和 TfidfVectorizer","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"PPL","slug":"PPL","permalink":"http://www.iequa.com/tags/PPL/"}]},{"title":"Recurrent Neural Networks","slug":"deeplearning/RNN-LSTM-GRU","date":"2019-06-14T02:06:16.000Z","updated":"2021-06-20T04:12:28.250Z","comments":true,"path":"2019/06/14/deeplearning/RNN-LSTM-GRU/","link":"","permalink":"http://www.iequa.com/2019/06/14/deeplearning/RNN-LSTM-GRU/","excerpt":"","text":"作为生物体，我们的视觉和听觉不断地获得带有序列的声音和图像信号，并交由大脑理解； 互联网数据中，很多是以序列形式存在的，例如文本、语音、视频、点击流等等。 RNN 基础知识，详参本博： Sequence Models 1. RNN Basic 在介绍 RNN 之前，首先解释一下为什么之前的标准网络不再适用了。因为它有两个缺点： 输入和输出的长度不尽相同 无法共享从其他位置学来的特征 传统方法文本分类： 用一篇文章的 TF-IDF 向量作为输入，其中 TF-IDF 向量是词汇表大小. Typical RNN Structure: 在 h_Th\\_Th_T 后面直接接一个 Softmax 层，输出文本所属类别的预测概率 yyy，就可以实现文本分类. 可应用于多种具体任务： net_t=Ux_t+Wh_t−1net\\_{t}=U x\\_{t}+W h\\_{t-1} net_t=Ux_t+Wh_t−1 h_t=f(net_t)h\\_{t}=f\\left(\\text {net}\\_{t}\\right) h_t=f(net_t) y=g(Vh_T)y=g\\left(V h\\_{T}\\right) y=g(Vh_T) 其中 fff 和 ggg 为激活函数，UUU 为输入层到隐含层的权重矩阵，WWW 为隐含层从上一时刻到下一时刻状态转移的权重矩阵。在文本分类任务中，fff 可以选取 Tanh 函数或者 ReLU 函数，ggg 可以采用 Softmax 函数。 1.1 TensorFlow RNN Forward Propagation 更多详情参见本博： TensorFlow：第8章 Recurrent Neural Networks 1 1.2 Forward Propagation a&lt;0&gt;=0⃗a^{&lt;0&gt;}=\\vec{0}a&lt;0&gt;=0 a&lt;1&gt;=g_1(W_aaa&lt;0&gt;+W_axx&lt;1&gt;+b_a)a^{&lt;1&gt;}=g\\_1(W\\_{aa}a^{&lt;0&gt;}+W\\_{ax}x^{&lt;1&gt;}+b\\_a)a&lt;1&gt;=g_1(W_aaa&lt;0&gt;+W_axx&lt;1&gt;+b_a) y&lt;1&gt;=g_2(W_yaa&lt;1&gt;+b_y)y^{&lt;1&gt;}=g\\_2(W\\_{ya}a^{&lt;1&gt;}+b\\_y)y&lt;1&gt;=g_2(W_yaa&lt;1&gt;+b_y) a&lt;t&gt;=g_1(W_aaa&lt;t−1&gt;+W_axx&lt;t&gt;+b_a)a^{&lt;{t}&gt;}=g\\_1(W\\_{aa}a^{&lt;{t-1}&gt;}+W\\_{ax}x^{&lt;{t}&gt;}+b\\_a)a&lt;t&gt;=g_1(W_aaa&lt;t−1&gt;+W_axx&lt;t&gt;+b_a) y&lt;t&gt;=g_2(W_yaa&lt;t&gt;+b_y)y^{&lt;{t}&gt;}=g\\_2(W\\_{ya}a^{&lt;{t}&gt;}+b\\_y)y&lt;t&gt;=g_2(W_yaa&lt;t&gt;+b_y) 激活函数：g_1g\\_1g_1 一般为 tanh函数 (or Relu函数)，g_2g\\_2g_2 一般是 Sigmod or softmax 函数. 1.3 RNN vs CNN RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。 RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。 CNN 优点： 提取数据中的局部位置的特征，然后再拼接池化层。 CNN可以更好地捕捉文本的语义。是O(n) CNN 优点： 一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。 2. Language model 此时就需要通过语言模型来预测每句话的概率： 2.1 RNN Language Model 首先我们需要一个很大的语料库 (Corpus) 将每个单词字符化 (Tokenize，即使用One-shot编码) 得到词典,，假设有 10000 个单词 还需要添加两个特殊的单词 end of sentence. 终止符，表示句子结束. UNknown, 之前的笔记已介绍过 2.2 Language Model Example 假设要对这句话进行建模：Cats average 15 hours of sleep a day. 1. 初始化 这一步比较特殊，即 x&lt;1&gt;x^{&lt;1&gt;}x&lt;1&gt; 和 a&lt;0&gt;a^{&lt;0&gt;}a&lt;0&gt; 都需要初始化为 0⃗\\vec{0}0 . 此时 y^&lt;1&gt;\\hat{y}^{&lt;1&gt;}y^​&lt;1&gt; 将会对第一个字可能出现的每一个可能进行概率的判断,即 y^&lt;1&gt;=[p(a),…,p(cats),…]\\hat{y}^{&lt;1&gt;}=[p(a),…,p(cats),…]y^​&lt;1&gt;=[p(a),…,p(cats),…]. 当然在最开始的时候没有任何的依据，可能得到的是完全不相干的字，因为只是根据初始的值和激活函数做出的取样。 2. 将真实值作为输入值: 之所以将真实值作为输入值很好理解，如果我们一直传错误的值，将永远也无法得到字与字之间的关系 如下图示，将 y&lt;1&gt;y^{&lt;1&gt;}y&lt;1&gt; 所表示的真实值 Cats 作为输入，即 x&lt;2&gt;=y&lt;1&gt;x^{&lt;2&gt;}=y^{&lt;1&gt;}x&lt;2&gt;=y&lt;1&gt; 得到 y^&lt;2&gt;\\hat{y}^{&lt;2&gt;}y^​&lt;2&gt; 此时的 y^&lt;2&gt;=[p(a∣cats),…,p(average∣cats),…]\\hat{y}^{&lt;2&gt;}=[p(a|cats),…,p(average|cats),…]y^​&lt;2&gt;=[p(a∣cats),…,p(average∣cats),…] 同理有 y^&lt;3&gt;=[p(a∣cats average),…,p(average∣cats average),…]\\hat{y}^{&lt;3&gt;}=[p(a|cats\\, average),…,p(average|cats\\,average),…]y^​&lt;3&gt;=[p(a∣catsaverage),…,p(average∣catsaverage),…] 另外输入值满足： x&lt;t&gt;=y&lt;t−1&gt;x^{&lt;{t}&gt;}=y^{&lt;{t-1}&gt;}x&lt;t&gt;=y&lt;t−1&gt; 3. 计算出损失值: 下图给出了构建模型的过程以及损失值计算公式: 随着训练的次数的增多，或者常用词出现的频率的增多，语言模型便慢慢的会开始掌握简单的词语比如“平均”，“每天”，“小时”。一个完善的语言模型看到类似“ 10 个小”的时候，应该就能准确的判定下一个字是“时”。 （当然也许实际情况是“ 10 个小朋友”，所以通常会有更多的判断因素，这里只是一个例子） 3. Vanishing gradients with RNNs 目前这种基本的 RNN 也不擅长捕获这种长期依赖效应. 梯度爆炸可以用梯度消减解决、梯度消失就有点麻烦了，需要用 GRU 来解决. gradient value 在 RNN 中也可能因为反向传播的层次太多导致 过小 或 过大 当梯度值过小的时候，网络无法有效调整自己权重矩阵致训练效果不佳，称为 gradient vanishing； 过大时直接影响到程序的运作因为程序已无法存储那么大的值，会返回 NaN ，称为 gradient exploding. 当 gradient 过大时, 可以每次将返回的梯度值进行检查，超出预定范围，则手动设为范围的边界值： 123if (gradient &gt; max) &#123; gradient = max&#125; 但梯度值过小的解决方案要稍微复杂一点，比如下面两句话： “The cat，which already ate apple，yogurt，banana，…, was full.” “The cats，which already ate apple，yogurt，banana，…, were full.” 重点标出的 cat(s) 和 be 动词（was, were） 是有重要关联的，但是中间隔了一个 which 引导的定语从句，对于前面所介绍的基础的 RNN网络 很难学习到这个信息，尤其是当出现梯度消失时，而且这种情况很容易发生. 神经网络层次很多时，反向传播很难影响前面层次的参数。解决 gradient vanishing，提出了 GRU 单元. 将在接下来的两个章节介绍两种方法来解决 梯度过小 问题，目标是当一些重要的单词离得很远的时候，比如例子中的 “cat” 和 “was”，能让语言模型准确的输出单数人称过去时的 “was”，而不是 “is” 或者 “were”. 两个方法都将引入“记忆”的概念，也就是为 RNN 赋予一个记忆的功能. 4. GRU - Gated Recurrent Unit GRU 是一种用来解决梯度值过小的方法，首先来看下在一个时刻下的 RNN单元，激活函数为 tanh 4.1 回顾 RNN 输入数据为 a&lt;t−1&gt;a^{&lt;{t-1}&gt;}a&lt;t−1&gt; 和 x&lt;t&gt;x^{&lt;{t}&gt;}x&lt;t&gt;, 与参数 W_aW\\_aW_a 进行线性运算后再使用 tanhtanhtanh 函数 转化得到 a&lt;t&gt;a^{&lt;{t}&gt;}a&lt;t&gt;. 当然 a&lt;t&gt;a^{&lt;{t}&gt;}a&lt;t&gt;, 再使用 softmax 函数处理可以得到预测值. 4.2 GRU结构 在 GRU中 会用到 “记忆细胞(Memory cell)” 这个概念, 我们用变量C表示。这个记忆细胞提供了记忆功能，例如它能够帮助记住 cat 对应 was, cats 对应 were. 而在 ttt 时刻，记忆细胞所包含的值其实就是 Activation function 值，即 c&lt;t&gt;=a&lt;t&gt;c^{&lt;{t}&gt;}=a^{&lt;{t}&gt;}c&lt;t&gt;=a&lt;t&gt; 注意：在这里两个变量的值虽然一样，但是含义不同。 另外在下节将介绍的 LSTM 中，二者值的大小有可能是不一样的，所以有必要使用这两种变量进行区分. 为了更新 memory_cell 的值，我们引入 c~\\tilde{c}c~ 来作为候选值从而来更新 c&lt;t&gt;c^{&lt;{t}&gt;}c&lt;t&gt;，其公式为： c~=tanh(W_c[c&lt;t−1&gt;,x&lt;t&gt;]+b_c)\\tilde{c}=tanh(W\\_c [c^{&lt;{t-1}&gt;}, x^{&lt;{t}&gt;}]+b\\_c) c~=tanh(W_c[c&lt;t−1&gt;,x&lt;t&gt;]+b_c) 更新门 (update gate): 更新门是 GRU 的核心概念，它的作用是用于判断是否需要进行更新. 更新门用 Γ_u\\Gamma\\_uΓ_u 表示，其公式为： Γ_u=σ(W_u[c&lt;t−1&gt;,x&lt;t&gt;]+b_u)\\Gamma\\_u=σ(W\\_u [c^{&lt;{t-1}&gt;}, x^{&lt;{t}&gt;}]+b\\_u) Γ_u=σ(W_u[c&lt;t−1&gt;,x&lt;t&gt;]+b_u) 如上图示，Γ_u\\Gamma\\_uΓ_u 值的大小大多分布在 0 或者 1，所以可以将其值的大小粗略的视为 0 或者 1。 这就是为什么我们就可以将其理解为一扇门，如果 Γ_u=1\\Gamma\\_u=1Γ_u=1 , 就表示此时需要更新值，反之不用. ttt 时刻记忆细胞: 有了更新门公式后，我们则可以给出 ttt 时刻 memory_cell 的值的计算公式: c^{&lt;{t}&gt;} = \\Gamma\\_u \\* \\tilde{c} + (1-\\Gamma\\_u) \\* c^{&lt;{t-1}&gt;} 公式很好理解，如果 Γ_u=1\\Gamma\\_u=1Γ_u=1，那么 ttt 时刻 记忆细胞的值就等于候选值 c~\\tilde{c}c~, 反之等于前一时刻记忆细胞的值. 注：上面公式中的 * 表示元素之间进行乘法运算，而其他公式是 矩阵运算. 下图给出了该公式很直观的解释： 在读到 “cat” 时候，其他时候一直为 0，知道要输出 “was” 的时刻，我们知道 “cat” 的存在，也就知道它为单数 GRU 结构示意图 4.3 完整版 GRU 上面简化了 GRU，在完整版中还存在另一个符号 ，这符号的意义是控制 c~\\tilde{c}c~ 和 c&lt;t−1&gt;c^{&lt;{t-1}&gt;}c&lt;t−1&gt; 之间的联系强弱，完整版如下： 注意，完整公式中多出了一个 Γ_r\\Gamma\\_rΓ_r, 这个符号的作用是控制 c~&lt;t&gt;\\tilde{c}^{&lt;{t}&gt;}c~&lt;t&gt; 和 c&lt;t&gt;c^{&lt;{t}&gt;}c&lt;t&gt; 之间联系的强弱. 5. LSTM - Long Short Term 介绍完 GRU 后，再介绍 LSTM 会更加容易理解。 5.1 GRU and LSTM GRU 只有两个门，而 LSTM 有三个门，分别是更新门 Γ_u\\Gamma\\_uΓ_u (是否需要更新为 c~&lt;t&gt;\\tilde{c}^{&lt;{t}&gt;}c~&lt;t&gt;)，遗忘门 Γ_f\\Gamma\\_fΓ_f (是否需要丢弃上一个时刻的值)，输出门 Γ_o\\Gamma\\_oΓ_o (是否需要输出本时刻的值) 下图是 LSTM 的结构示意图： 5.2 LSTM Structure 1997年 LSTM 仍是 x_tx\\_tx_t 和 h_t−1h\\_{t−1}h_t−1 来计算 h_th\\_th_t，但对内部的结构进行了更加精心的设计，加入 3 Gate 和 1 memory_cell. 输入门 Γ_u\\Gamma\\_uΓ_u： 控制 当前计算的新状态 以多大程度更新到记忆单元中 （也叫更新门）; 遗忘门 Γ_f\\Gamma\\_fΓ_f： 控制 前一步记忆单元 中的信息有多大程度被遗忘掉; 输出门 Γ_o\\Gamma\\_oΓ_o： 控制当前的输出有多大程度上取决于 当前的记忆单元; 记忆单元 memory cell c_tc\\_tc_t. h_t=o_t⊙Tanh⁡(c_t)h\\_{t}=o\\_{t} \\odot \\operatorname{Tanh}\\left(c\\_{t}\\right) h_t=o_t⊙Tanh(c_t) 5.3 Activation function 在 LSTM 中, 关于 activation function 的选取： Γ_f\\Gamma\\_fΓ_f、Γ_iu\\Gamma\\_i{u}Γ_iu 和 Γ_o\\Gamma\\_oΓ_o 使用 Sigmoid 函数作为激活函数； 在生成候选记忆时，使用 Tanh 作为激活函数。 注： 这两个激活函数都是饱和的，也就是说在输入达到一定值的情况下，输出就不会发生明显变化了。如果是用非饱和的激活函数，例如 ReLU，那么将难以实现门控的效果。 使用这个激活函数的原因如下： (1). Sigmoid 输出在 0～1 之间。且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关。 (2). 在生成候选记忆时，使用 Tanh 函数，是因为其输出在 −1~1 之间，这与大多数场景下特征分布是 0 中心的吻合。此外，Tanh 函数在输入为 0 附近相比 Sigmoid 函数有更大的梯度，通常使模型收敛更快。 总而言之，LSTM 经历了 20 年的发展，其核心思想一脉相承，但各个组件都发生了很多演化。 GRU vs LSTM GRU 和 LSTM 的效果在很多任务上不分伯仲。 GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达效果更好，但计算量更大。 从结构上来说： GRU 只有两个门（update和reset），LSTM 有三个门（forget，input，output） GRU 直接将 hidden state 传给下一个单元，而 LSTM 则用 memory cell 把hidden state 包装起来。 6. Bidirectional RNN 前面介绍的都是单向的 RNN 结构，在处理某些问题上得到的效果不尽人意 如下面两句话，我们要从中标出人名： He said, “Teddy Roosevelt was a great President”. He said, “Teddy bears are on sale”. 第一句中的 Teddy Roosevelt 是人名 第二句中的 Teddy bears 是泰迪熊，同样都是单词 Teddy 对应的输出在第一句中应该是 1，第二句中应该是 0 像这样的例子如果想让我们的序列模型明白就需要借助不同的结构比如 - 双向递归神经网络(Bidirectional RNN). 该神经网络首先从正面理解一遍这句话，再从反方向理解一遍. 下图摘自大数据文摘整理 7. Deep RNNs 深层，顾名思义就是层次增加。如下图是深层循环神经网络的示意图 横向表示时间展开，纵向则是层次展开。 注意激活值的表达形式有所改变，以 a^{\\[1\\]&lt;0&gt;} 为例进行解释： [1] 表示第一层 &lt;0&gt; 表示第一个激活值 另外各个激活值的计算公式也略有不同，以 a^{\\[2\\]&lt;3&gt;} 为例，其计算公式如下： Reference 《百面机器学习》 RNN and CNN LSTM 长短期记忆网络","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://www.iequa.com/tags/RNN/"}]},{"title":"Convolutional Neural Networks","slug":"deeplearning/CNN","date":"2019-06-13T02:06:16.000Z","updated":"2021-06-22T06:28:17.301Z","comments":true,"path":"2019/06/13/deeplearning/CNN/","link":"","permalink":"http://www.iequa.com/2019/06/13/deeplearning/CNN/","excerpt":"Convolutional Neural Networks","text":"Convolutional Neural Networks CNN 基础知识，详参本博： Convolutional-Neural-Networks Convolutional Neural Networks，CNN 也是一种前馈神经网络，其特点是每层的神经元节点只响应前一层局部区域范围内的神经元（全连接网络中每个神经元节点响应前一层的全部节点）。 一个 DCNN 通常由若干 Convolutional-Layer 叠加若干 Fully-Connected 组成，中间也包含各种 Non-Linear 操作以及 Pooling 操作。 Convolution operation 的**参数共享特性**使得需要优化的参数数目大大缩减，提高了模型的训练效率以及可扩展性。 LeCun Yann 在 1998 年提出 1. Convolutional Function 1.1 Sparse Interaction 稠密的连接结构, 神经元 s_is\\_is_i 与输入的所有神经元 x_jx\\_jx_j 均有连接： 对于全连接网络，任意一对输入与输出神经元之间都产生交互，形成`稠密`的连接结构 卷积， 每个输出神经元仅与前一层特定局部区域内的神经元存在连接： 卷积神经网络中，卷积核尺度远小于输入的维度，我们称这种特性为稀疏交互 神经元 s_is\\_is_i 仅与前一层中的 x_i−1x\\_{i−1}x_i−1, x_ix\\_ix_i 和 x_i+1x\\_{i+1}x_i+1 相连。具体来讲如果限定每个输出与前一层神经元的连接数为 k ，那么该层的参数总量为 k×nk×nk×n。在实际应用中，一般 kkk 值远小于 mmm 就可以取得较为可观的效果；复杂度将会减小几个数量级，过拟合的情况改善. 稀疏交互的物理意义： 1.2 Parameter Sharing Fully-Connected Networks，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次； CNN，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。 根据参数共享的思想，我们只需要学习一组参数集合，而不需要针对每个位置的每个参数都进行优化，从而大大降低了模型的存储需求。 参数共享的物理意义是使得卷积层具有平移等变性。什么意思？假如图像中有一只猫，那么无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。 2. Pooling Function mean pooling max pooling pooling 的本质是降采样. pooling 除了能显著降低参数量外，还能够保持对平移、伸缩、旋转操作的不变性。 3. CNN 文本分类任务 CNN 的核心思想是捕捉局部特征，起初在图像领域取得了巨大的成功，后来在文本领域也得到了广泛的应用。对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于 N-gram. CNN 的优势在于能够自动地对 N-gram 特征进行组合和筛选，获得不同抽象层次的语义信息。 (1). 输入层是一个 N×KN×KN×K 的矩阵，其中 NNN 为文章所对应的单词总数，KKK 是每个词对应的表示向量的维度. (2). 卷积层。在输入的 N×KN×KN×K 维矩阵上，我们定义不同大小的滑动窗口进行卷积操作. (3). 池化层，网络采用了 1-MaxPool, 达到的效果都是将不同长度的句子通过池化得到一个定长的向量表示。 (4). 得到文本的向量表示之后，接一个全连接层，并使用 Softmax 激活函数输出每个类别的概率。 &lt;img src=&quot;/images/nlp/textcnn-3.webp&quot; width=“700” /img&gt; 整个模型由四部分构成： 输入层、卷积层、池化层、全连接层。 更多资料详见： TextCNN文本分类 针对海量的文本多分类数据，也可以尝试一下浅层的深度学习模型 FastText模型，该模型的分类效率更高. 4. ResNet 深度神经网络的层数决定了模型的容量，然而随着神经网络层数的加深： 优化函数越来越陷入局部最优解。 同时，随着网络层数的增加，梯度消失的问题更加严重，这是因为梯度在反向传播时会逐渐衰减。特别是利用 Sigmoid 激活函数时，使得远离输出层（即接近输入层）的网络层不能够得到有效的学习，影响了模型泛化的效果。 Deep Residual Network，ResNet 的提出背景和核心理论是什么？ ResNet 的提出背景是解决或缓解 Deep Neural Networks 训练中的 Gradients Vanishing 问题 ResNet在 ImageNet 竞赛和 AlphaGo Zero 的应用中都取得了非常好的效果. Reference 《百面机器学习》 迭代自己 DCNN","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://www.iequa.com/tags/CNN/"}]},{"title":"Deep Feedforward Networks","slug":"deeplearning/Deep-Feedforward-Networks","date":"2019-06-12T02:06:16.000Z","updated":"2021-06-20T04:12:28.252Z","comments":true,"path":"2019/06/12/deeplearning/Deep-Feedforward-Networks/","link":"","permalink":"http://www.iequa.com/2019/06/12/deeplearning/Deep-Feedforward-Networks/","excerpt":"","text":"**更多基础知识系列详情参见 : Deep Learning Notes ** Feedforward Networks 是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。 Feedforward Networks 是一类网络的统称: MLP、Autoencoder、RBM、CNN 等都属于这类. 对于中间层来说, 往往是 ReLU 的效果最好. 虽然 z &lt; 0, 的时候，斜率为0， 但在实践中，有足够多的隐藏单元 令 z &gt; 0, 对大多数训练样本来说是很快的. so the one place you might use as linear activation function others usually in the output layer. 1. Neural Networks Basics 2. Activation functions 四种常用的激活函数: Sigmoid, Tanh, ReLU, Leaky ReLU. 其中 sigmoid 我们已经见过了, 它的输出可看成一个概率值, 往往用在输出层. 对于中间层来说, 往往ReLU效果最好. Tanh 数据平均值为 0，具有数据中心化的效果，几乎在任何场合都优于 Sigmoid 为什么需要激活函数? 如果没有激活函数, 那么不论多少层的神经网络都只相当于一个LR: it turns out that if you use a linear activation function or alternatively if you don’t have an activation function, then no matter how many layers your neural network has, always doing just computing a linear activation function, so you might as well not have any hidden layers. so unless you throw a non-linearity in there, then you’re not computing more interesting functions ReLU (rectified linear unit 矫正线性单元) tanh 和 sigmoid 都有一个缺点，就是 z 非常大或者非常小，函数的斜率(导数梯度)就会非常小, 梯度下降很慢. the slope of the function you know ends up being close to zero, and so this can slow down gradient descent ReLU (rectified linear unit) is well, z = 0 的时候，你可以给导数赋值为 0 or 1，虽然这个点是不可微的. 但实现没有影响. 虽然 z &lt; 0, 的时候，斜率为0， 但在实践中，有足够多的隐藏单元 令 z &gt; 0, 对大多数训练样本来说是很快的. 激活函数的对比: Sigmoid 和 Tanh 为什么会导致 Vanishing/Exploding gradients ? Tanh 值域 (-1,1) Sigmoid 值域 (0,1) ReLU 的优点，和局限性分别是什么? 谈谈激活函数 Sigmoid,Tanh,ReLu,softplus,softmax 3. Initialization weights 在 LR 中我们的参数 www 初始化为 0, 如果在神经网络中也是用相同的初始化, 那么一个隐藏层的每个节点都是相同的, 不论迭代多少次. 这显然是不合理的, 所以我们应该 随机地初始化 www 从而解决这个 sysmmetry breaking problem. 破坏对称问题 具体初始化代码可参见下图, 其中 乘以 0.01 是为了让参数 www 较小, 加速梯度下降 如激活为 tanh 时, 若参数较大则 zzz 也较大, 此时梯度接近于 0, 更新缓慢. 如不是 tanh or sigmoid 则问题不大. this is a relatively shallow neural network without too many hidden layers, so 0.01 maybe work ok. finally it turns out that sometimes there can be better constants than 0.01. 4. Improving DNN 深度学习的实用层面 ： 能够高效地使用神经网络通用的技巧，包括 初始化、L2和dropout正则化、Batch归一化、梯度检验。 能够实现并应用各种优化算法，例如 Mini-batch、Momentum、RMSprop、Adam，并检查它们的收敛程度。 理解深度学习时代关于如何 构建训练/开发/测试集 以及 偏差/方差分析 最新最有效的方法. 4.1 Train/dev/test 传统的机器学习中： 通常按照 70/30 来数据集, 或者按照 60/20/20 的比例分为 Train/Validation/Test. 深度学习问题中: 我们可用的数据集的量级非常大. 这时我们就不需要给验证集和测试集太大的比例, 例如 98/1/1. 4.2 Regularization 为什么正则化没有加 λ2mb2\\frac{\\lambda}{2m} b^22mλ​b2: 因为 www 通常是一个高维参数矢量, 已经可以表达 High bias 的问题, www 可能含有很多参数，我们不可能拟合所有参数, 而 bbb 只是单个数字, 所以 www 几乎覆盖了所有参数，而不是 bbb, 如果加了 bbb 也没有影响，因为 bbb 只是众多参数中的一个. L1 regularization : 如果用的是 L1 regularization, then www will end up being sprase 稀疏的, 也就是说 www 向量中有很多 0. 有人说这样有利于压缩模型，但是我觉得不是很合适. 越来越多的人使用 L2. Notes: 不称为:矩阵 L2 范数， 按照惯例我们称为: Frobenius norm of a matrix, 其实就是 : 矩阵 L2. L2 regularization : L2 regularization 下的 Cost Function 如下所示, 只需要添加正则项 λ2m∑_l=1L∣∣w[l]∣∣2_F\\frac{\\lambda}{2m}\\sum\\_{l=1}^L||w^{[l]}||^2\\_F2mλ​∑_l=1L∣∣w[l]∣∣2_F, 其中 F 代表 Frobenius Norm. 在添加了正则项之后, 相应的梯度也要变化, 所以在更新参数的时候需要加上对应的项. 这里注意一点, 我们只对参数 www 正则, 而不对 bbb. 因为对于每一层来说, www 有很高的维度, 而 bbb 只是一个标量. www 对整个模型的影响远大于 bbb. 下面给出添加 regularization 为什么能防止过拟合给出直观的解释: 当我们的 λ 比较大的时候, 模型就会加大对 w 的惩罚, 这样有些 w 就会变得很小 (L2 Regularization 也叫权重衰减, weights decay). 效果就是整个神经网络变得简单了(一些隐藏层甚至 www 趋向于 0), 从而降低了过拟合的风险. 那些 隐藏层 并没有被消除，只是影响变得更小了，神经网络变得简单了. 从另一个角度来看. 以 tanh激活函数 为例, 当 λλλ 增加时, www 会偏小, 这样 z=wa+bz = wa +bz=wa+b 也会偏小, 此时的激活函数大致是线性的. 这样模型的复杂度也就降低了, 即降低了过拟合的风险. 如果神经网络每层都是线性的，其实整个还是一个线性的, 即使是一个很深的网络，因为线性激活函数的特征，最终我们只能计算线性函数. Other Regularization Data augmentation Early stopping W 开始是变小的，之后会随着迭代越来越大. early stopping 就是在中间点停止迭代过程. 其实可以设置在 J 不在明显下降的时候，设置 Early Stopping. Notes: 1. early stopping 缺点是 提早停止，w 是防止了过拟合，但是 J 没有被继续下降. 2. L2 正则化 的缺点是，要用大量精力搜索合适的 λ . 4.3 Dropout dropout 也是一种正则化的手段, 在训练时以 1-keep_prob 随机地”丢弃”一些节点. 如下图所示. dropout 将产生收缩权重的平方范数的效果, 和 L2 类似，实施 dropout 的结果是它会压缩权重，并完成一些预防过拟合的外层正则化，事实证明 dropout 被正式地作为一种正则化的替代形式 L2 对不同权重的衰减是不同的，它取决于倍增的激活函数的大小. dropout 的功能类似于 L2 正则化. 甚至 dropout 更适用于不同的输入范围. 其他 : 计算机视觉的人员非常钟情 dropout 函数. Notes: dropout 的一大缺点就是 J 不会被明确定义. 每次迭代都会被随机删除一些节点. 如果再三检查梯度下降的性能，实际上是很难复查的. 定义明确的代价函数，每次迭代都会下降. 因为 dropout 使得 J 没有被明确定义，或者在某种程度上很难计算. 所以我们失去了调试工具，我通常会关闭 dropout. keep_prob 设置为 1， 运行代码，确保 J 函数单调递减, 然后在打开 dropout, 在 dropout 的过程中，代码并未引入bug. 4.4 Normalization 0 均值化 归一化 方差 上图2， 特征 x1 的方差 比 特征 x2 的方差 大很多 上图3， 特征 x1 和 特征 x2 的 方差 都是 1 注意: 训练集 和 测试集，都是通过相同的 μ\\muμ 和 σ2{\\sigma}^2σ2 定义的相同数据转换, 其中 μ\\muμ 和 σ2{\\sigma}^2σ2 是由训练数据计算而来. 张俊林 - Batch Normalization导读 、 张俊林 - 深度学习中的Normalization模型 Internal Covariate Shift &amp; Independent and identically distributed，缩写为 IID Batch Normalization 可以有效避免复杂参数对网络训练产生的影响，也可提高泛化能力. 神经网路的训练过程的本质是学习数据分布，如果训练数据与测试数据分布不同，将大大降低网络泛化能力， BN 是针对每一批数据，在网络的每一层输入之前增加 BN，(均值0，标准差1)。 Dropout 可以抑制过拟合，作用于每份小批量的训练数据，随机丢弃部分神经元机制. bagging 原理. ML算法： 关于防止过拟合，整理了 8 条迭代方向 4.5 Vanishing/Exploding Vanishing/Exploding gradients 指的是随着前向传播不断地进行, 激活单元的值会逐层指数级地增加或减小, 从而导致梯度无限增大或者趋近于零, 这样会严重影响神经网络的训练. 如下图. 可以减小这种情况发生的方法, 就是用有效的参数初始化 (该方法并不能完全解决这个问题). 但是也是有意义的 设置合理的权重，希望你设置的权重矩阵，既不会增长过快，也不会下降过快到 0. 想更加了解如何初始化权重可以看下这篇文章 神经网络权重初始化问题，其中很详细的介绍了权重初始化问题。 5. Optimization &amp; Hyperparam 关于优化算法与超参数调优，以及 BN 等更多，详情参阅本博下面链接： Improving DNN (week2) - Optimization Algorithm Improving DNN (week3) - Hyperparameter、Batch Regularization Reference Andrew Ng Notes","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"MLP","slug":"MLP","permalink":"http://www.iequa.com/tags/MLP/"}]},{"title":"SVM 和 LR 的区别与联系？","slug":"ml/SVM-vs-LR","date":"2019-06-10T02:06:16.000Z","updated":"2021-06-22T06:46:36.122Z","comments":true,"path":"2019/06/10/ml/SVM-vs-LR/","link":"","permalink":"http://www.iequa.com/2019/06/10/ml/SVM-vs-LR/","excerpt":"SVM vs LR","text":"SVM vs LR 关于 SVM 的详细原理与推导过程，详见： Support Vecor Machine (六部曲) 其实我认为 SVM 的使用率已经明显在下降趋势了，主要原因是随着神经网络的兴起之后，所以我不建议一定要花非常多的经历去学习 SVM 的原理和推导了，可以简单了解下。 主要还是了解下适用范围和场景等就可以了。 1. LR vs SVM 要说有什么本质区别，那就是 loss function 不同，两者对数据和参数的敏感程度不同 (1). 对非线性表达上，LR 只能通过人工的特征组合来实现，而 SVM 引入核函数来实现非线性表达。 (2). LR 产出的是概率值，而 SVM 只能产出是正类还是负类，不能产出概率。 (3). Linear SVM 依赖数据表达的距离测度，所以需要对数据先做 normalization；LR不受其影响. (4). SVM 不直接依赖数据分布，而LR则依赖, SVM 主要关注的是“支持向量”，也就是和分类最相关的少数点，即关注局部关键信息；而 LR 是在全局进行优化的。这导致 SVM 天然比 LR 有更好的泛化能力，防止过拟合。 LR则受所有数据点的影响，如果数据不同类别 strongly unbalance 一般需要先对数据做 balancing。 (5). 损失函数的优化方法不同，LR 是使用 GD 来求解 对数似然函数 的最优解；SVM 使用 (Sequnential Minimal Optimal) 顺序最小优化，来求解条件约束损失函数的对偶形式。 一般用线性核和高斯核，也就是Linear核与RBF核需要注意的是需要对 数据归一化处理. 一般情况下RBF效果是不会差于Linear但是时间上RBF会耗费更多 扩展点： 注：不带正则化的LR，其做 normalization 的目的是为了方便选择优化过程的起始值，不代表最后的解的 performance 会跟 normalization 相关，而其线性约束是可以被放缩的（等式两边可同时乘以一个系数），所以做 normalization 只是为了求解优化模型过程中更容易选择初始值 2. Andrew Ng 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况 如何量化 feature number 和 sample number： n 是feature的数量, m是样本数 1). feature number &gt;&gt; sample number，则使用LR算法或者不带核函数的SVM（线性分类） feature number = 1W， sample number = 1K 2). fn 小， sample number 一般1W，使用带有 kernel函数 的 SVM算法. 3). fn 小， sample number 很大5W+（n=1-1000，m=50000+） 增加更多的 feature 然后使用LR 算法或者 not have kernel 的 SVM Reference LR 与 SVM的异同 懒死骆驼 - 统计学习方法笔记(一) 知乎 : 最小二乘、极大似然、梯度下降有何区别？ 知乎 : Linear SVM 和 LR 有什么异同？ 白开水加糖 SVM与LR的比较 懒死骆驼 - 口述模型整理 支持向量机(SVM)硬核入门-基础篇 scikit-learn 逻辑回归类库使用小结 LR 正负样本不均衡问题","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"http://www.iequa.com/tags/SVM/"}]},{"title":"L1、L2 Regularization","slug":"ml/ERM-SRM-L1-L2","date":"2019-06-09T02:06:16.000Z","updated":"2021-06-22T06:41:39.435Z","comments":true,"path":"2019/06/09/ml/ERM-SRM-L1-L2/","link":"","permalink":"http://www.iequa.com/2019/06/09/ml/ERM-SRM-L1-L2/","excerpt":"L1 L2 Regularization","text":"L1 L2 Regularization Supervised Learning: “minimizeyour error while regularizing your parameters” w∗=argminw∑iL(yi,f(xi;w))+λΩ(w)w^*=argmin_w\\sum_iL(y_i,f(x_i;w))+\\lambda\\Omega(w) w∗=argminw​i∑​L(yi​,f(xi​;w))+λΩ(w) 我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小. 第二项 λΩ(w)\\lambda\\Omega(w)λΩ(w)，也就是对参数www的规则化函数 Ω(w)Ω(w)Ω(w) 去约束我们的模型尽量的简单. 第一项对应模型的训练损失函数 (Square Loss、Hinge loss、Exp loss、Log loss) 第二项对应模型的正则化项 （模型参数向量的范数） 经验风险最小化 empirical risk minimization, 结构风险最小化 structural risk minimization 李沐曾经说过： model是用离散特征还是连续特征，其实是“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。 既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾 feature 还是折腾 model 了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 结构风险最小化: 规则化项是结构风险最小化策略的实现，即在经验风险上加一个正则化项或惩罚项 最小化误差是为了让我们的模型拟合我们的训练数据，regularized parameters是防止我们的模型过分拟合我们的训练数据。 因为参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。 我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小）. 而模型**“简单”就是通过规则函数来实现的**。regularized item的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如 稀疏、低秩、平滑 等等。 人的先验是非常重要的。前人的经验会让你少走很多弯路，这就是为什么我们平时学习最好找个大牛带带的原因。一句点拨可以为我们拨开眼前乌云，还我们一片晴空万里，醍醐灌顶。对机器学习也是一样，如果被我们人稍微点拨一下，它肯定能更快的学习相应的任务。只是由于人和机器的交流目前还没有那么直接的方法，目前只能由规则项来担当了. 1. Supervised Learning Obj 第一项对应模型的训练损失函数: Square Loss –&gt; 最小二乘 Hinge Loss –&gt; SVM Exp Loss –&gt; AdaBoost Log Loss –&gt; LR 第二项对应模型的正则化项: (一般是模型复杂度的单调递增函数) 模型参数向量的范数，不同的选择对参数的约束不同，取得的效果也不同 论文中常都聚集在：零范数、一范数、二范数、核范数等等。这么多范数，到底它们表达啥意思？具有啥能力？ 2. Norm 在机器学习中，我们经常使用称为范数(norm)的函数来衡量向量大小. LpL^pLp 范数定义如下： ∥x∥_p=(∑_i∣x_i∣p)1p\\|x\\|\\_{p}=\\left(\\sum\\_{i}\\left|x\\_{i}\\right|^{p}\\right)^{\\frac{1}{p}} ∥x∥_p=(∑_i∣∣​x_i∣∣​p)p1​ L0L^0L0 范数：∥x∥_0\\|x\\|\\_{0}∥x∥_0 为 xxx 向量各个非零元素的个数。 L1L^1L1 范数：∥x∥_1\\|x\\|\\_{1}∥x∥_1 为 xxx 向量各个元素绝对值之和，也叫“稀疏规则算子”（Lasso Regularization）。 L2L^2L2 范数：∥x∥_2\\|x\\|\\_{2}∥x∥_2 为 xxx 向量各个元素平方和的 1/21/21/2 次方，L2L^2L2 范数又称 Euclidean、 Frobenius 范数。 3. L1 Regularization L1 可以实现稀疏，为什么要稀疏？ ∥x∥_1=∑_i∣x_i∣\\|\\boldsymbol{x}\\|\\_{1}=\\sum\\_{i}\\left|x\\_{i}\\right| ∥x∥_1=∑_i∣∣​x_i∣∣​ 让我们的参数稀疏有什么好处呢？这里扯两点： 特征选择(Feature Selection)： 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。x_ix\\_ix_i 的大部分元素（也就是特征）都是和最终的输出 y_iy\\_iy_i 没有关系或者不提供任何信息的，在最小化目标函数的时候考虑x_ix\\_ix_i这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确y_iy\\_iy_i的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 可解释性(Interpretability)： 模型更容易解释。例如患某种病的概率是yyy，我们收集到的数据xxx是 1000 维的，也就是我们需要寻找这 1000种 因素到底是怎么影响患上这种病的概率的。 假设我们这个是个回归模型：y=w_1x_1+w_2x_2+w_1000x_1000+by=w\\_1 x\\_1 + w\\_2 x\\_2 + w\\_{1000} x\\_{1000} + by=w_1x_1+w_2x_2+w_1000x_1000+b（当然了，为了让 yyy 限定在[0,1]的范围，一般还得加个Logistic函数）。 通过学习，如果最后学习到的 w\\* 就只有很少的非零元素，例如只有5个非零的 w_iw\\_iw_i，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。 患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个 w_iw\\_iw_i 都非0，医生面对这1000种因素，累觉不爱. 4. L2 Regularization 4.1 L2 能防止过拟合？ 通过 L2范数 的规则项最小来使参数值都较小、甚至趋于0(但不会为0)，模型参数值越小则对应的特征对于模型的影响就比较小，这样相当于对这部分无关特征做了一个惩罚，即使他们的值波动比较大，受限于参数值很小，也不会对模型的输出结果造成太大影响，也就使得模型不会习得这部分特征而发生过拟合 4.2 L2 范数的好处 学习理论的角度：可以防止过拟合，提升模型的泛化能力 优化、数值计算的角度：L2范数能够让我们的优化求解变得稳定和快速. Reference 懒死骆驼 机器学习中的范数规则化之（一）L0、L1与L2范数 机器学习算法系列（28）：L1、L2正则化 L1 / L2 正规化 什么是过拟合 (Overfitting) 迭代自己: 范数","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"L1-L2","slug":"L1-L2","permalink":"http://www.iequa.com/tags/L1-L2/"}]},{"title":"Probabilistic Graphical Model","slug":"ml/Probabilistic_Graphical_Model","date":"2019-06-08T02:06:16.000Z","updated":"2021-06-22T06:46:36.098Z","comments":true,"path":"2019/06/08/ml/Probabilistic_Graphical_Model/","link":"","permalink":"http://www.iequa.com/2019/06/08/ml/Probabilistic_Graphical_Model/","excerpt":"Probabilistic Graphical Model","text":"Probabilistic Graphical Model 对于一个实际问题，目标： 能够挖掘隐含在数据中的知识。 怎样才能使用概率图模型挖掘这些隐藏知识呢？ 用观测结点表示观测到的数据，用隐含结点表示潜在的知识，用边来描述知识与数据的相互关系，获一概率分布。 1. Probabilistic Graphical Model 概率图中的**节点**分为： 隐含节点 观测节点 概率图中的**边** 分为： 有向边 无向边 常见的概率图模型 ： Native Bayes、最大熵、隐马尔科夫模型、CRF、LDA 等. PGM 联合概率: 概率图模型最为“精彩”的部分就是能够用简洁清晰的图示形式表达概率生成的关系: 在给定A的条件下B和C是条件独立的，基于条件概率的定义可得： P(C∣A,B)=P(B,C∣A)P(BA)=P(B∣A)P(C∣A)P(B∣A)=P(C∣A)\\begin{aligned} P(C | A, B) &amp;=\\frac{P(B, C | A)}{P(B A)}=\\frac{P(B | A) P(C | A)}{P(B | A)} \\\\\\\\ &amp;=P(C | A) \\end{aligned} P(C∣A,B)​=P(BA)P(B,C∣A)​=P(B∣A)P(B∣A)P(C∣A)​=P(C∣A)​ 同理，在给定B和C的条件下A和D是条件独立的，可得： P(D∣A,B,C)=P(A,D∣B,C)P(A∣B,C)=P(A∣B,C)P(D∣B,C)P(A∣B,C)=P(D∣B,C)\\begin{aligned} P(D | A, B, C) &amp;= \\frac{P(A, D | B, C)}{P(A | B, C)}=\\frac{P(A | B, C) P(D | B, C)}{P(A | B, C)} \\\\\\\\ &amp;= P(D | B, C) \\end{aligned} P(D∣A,B,C)​=P(A∣B,C)P(A,D∣B,C)​=P(A∣B,C)P(A∣B,C)P(D∣B,C)​=P(D∣B,C)​ 结合上面的两个表达式可得联合概率： P(A,B,C,D)=P(A)P(B∣A)P(C∣A,B)P(D∣A,B,C)=P(A)P(B∣A)P(C∣A)P(D∣B,C)(6.3)\\begin{aligned} P(A,B,C,D)&amp;=P(A)P(B|A)P(C|A,B)P(D|A,B,C) \\\\\\\\ &amp;= P(A)P(B|A)P(C|A)P(D|B,C) \\end{aligned}\\tag{6.3} P(A,B,C,D)​=P(A)P(B∣A)P(C∣A,B)P(D∣A,B,C)=P(A)P(B∣A)P(C∣A)P(D∣B,C)​(6.3) 2. PGM Expression 解释朴素贝叶斯模型的原理，并给出概率图模型表示: 通过预测指定样本属于特定类别的概率 y=max⁡_y_iP(y_i∣x)y=\\max \\_{y\\_{i}} P\\left(y\\_{i} | x\\right) y=max_y_iP(y_i∣x) 可以写成： P(y_i∣x)=P(x∣y_i)P(y_i)P(x)P\\left(y\\_{i} | x\\right)=\\frac{P\\left(x | y\\_{i}\\right) P\\left(y\\_{i}\\right)}{P(x)} P(y_i∣x)=P(x)P(x∣y_i)P(y_i)​ 其中 x=(x_1,x_2,……,x_n)x=\\left(x\\_{1}, x\\_{2}, \\ldots \\ldots, x\\_{n}\\right)x=(x_1,x_2,……,x_n), 为样本对应的特征向量， P(x)P(x)P(x) 为样本的先验概率。 3. Generative vs Discriminative 3.1 Generative generative approach 由数据学习到联合概率分布 P(X,Y)P(X,Y)P(X,Y)，然后求出条件概率分布 P(Y∣X)P(Y\\mid X)P(Y∣X) 作为预测的模型： P(Y∣X)=P(X,Y)P(X)P(Y\\mid X)=\\frac{P(X,Y)}{P(X)} P(Y∣X)=P(X)P(X,Y)​ 典型的生成式模型包括： Native Bayes、HMM、Bayes Net 3.2 Discriminative discriminative approach 由数据直接学到决策函数 f(X)f(X)f(X) 或条件概率分布 P(Y∣X)P(Y\\mid X)P(Y∣X) 作为预测的模型： f(X),P(Y∣X)f(X), P(Y\\mid X) f(X),P(Y∣X) 判别式模型关心的是对于给定的输入 XXX, 应该预测什么样的输出 YYY, 判别模型就是判别数据输出量的模型。 典型的判别式模型包括： LR、NN、SVM、CRF、CART 3.3 generative vs discriminative vs generative approach discriminative approach 定义 由数据学习联合概率分布P(X,Y)P(X,Y)P(X,Y) 然后,求出在XXX情况下，P(Y)P(Y)P(Y)作为预测的模型 决策函数f(x)f(x)f(x)或条件概率分布P(X)P(X)P(X)作为预测模型 特点 1. 可还原出P(X,Y)P(X,Y)P(X,Y)； 2. 学习收敛速度更快； 3. 存在隐变量时仍可用 1. 直接面对预测，准确率更高些; 2. 便于数据抽象，特征定义使用; 模型 native bayes、hidden markov Logistic Regression、SVM、Gradient Boosting、CRF… Note 给定输入 XXX 产生输出 YYY 的生成关系 对给定的输入 XXX，应预测什么样的输出 YYY Reference 产生式模型与判别式模型 迭代自己 概率图表示","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"entropy","slug":"entropy","permalink":"http://www.iequa.com/tags/entropy/"}]},{"title":"Information Entropy","slug":"ml/Entropy","date":"2019-06-07T08:06:16.000Z","updated":"2021-06-22T06:41:11.152Z","comments":true,"path":"2019/06/07/ml/Entropy/","link":"","permalink":"http://www.iequa.com/2019/06/07/ml/Entropy/","excerpt":"Claude Shannon","text":"Claude Shannon 1. Infomation Entropy 2014 年举行了世界杯足球赛，大家都很关心谁会是冠军。假如我错过了看世界杯，赛后我问一个知道比赛结果的观众“哪只球队是冠军”？他不愿意直接告诉我，而让我猜，并且我每猜一次，他要收我一元钱才肯告诉我是否猜对了，那么我要掏多少钱才能知道谁是冠军呢？我可以把球队编上号，从 1 到 32，然后提问：“冠军在 1~16 号中吗？”假如他告诉我猜对了，我会接着提问：“冠军在 1~8 号中吗？”假如他告诉我猜错了，我自然知道冠军在 9~16 号中。这样我只需要 5 次，就能知道哪只球队是冠军。所以，谁是世界杯冠军这条消息的信息量只值 5 块钱。 H=−(p_1⋅log⁡p_1+p_2⋅log⁡p_2+⋯+p_32⋅log⁡p_32)H=-(p\\_1\\cdot\\log p\\_1+p\\_2\\cdot\\log p\\_2+\\cdots+p\\_{32}\\cdot\\log p\\_{32}) H=−(p_1⋅logp_1+p_2⋅logp_2+⋯+p_32⋅logp_32) 其中 p_1,p_2,⋯ ,p_32p\\_1,p\\_2,\\cdots,p\\_{32}p_1,p_2,⋯,p_32 分别是这 32 支球队夺冠的概率。香农把它称为信息熵(Entropy)，一般用符号 HHH 表示，单位比特。很容易计算出，当 32 支球队夺冠概率相同时，对应的信息熵等于 5 比特。 1.1 信息熵的定义 对于任意一个离散型随机变量 XXX，它的熵 H(X)H(X)H(X) 定义为: H(X)=−∑_x∈Xp(x)log⁡p(x)H(X)=-\\sum\\_{x\\in X}p(x)\\log p(x) H(X)=−∑_x∈Xp(x)logp(x) 其中，约定 0log⁡0=00\\log0=00log0=0。H(X)H(X)H(X) 可以写为 H(p)H(p)H(p)。 变量的不确定性越大，熵也就越大，要把它搞清楚，所需信息量也就越大。 信息熵的物理含义是对一个信息系统不确定性的度量，和热力学中熵有相似之处，后者就是一个系统无序的度量。 从另一角度讲也是对一种 不确定性的度量。 1.2 信息熵的例子 设一次随机事件（用随机变量XXX表示）它可能会有 x1，x2，⋯，xmx_1，x_2，⋯，x_mx1​，x2​，⋯，xm​ 共 m 个不同的结果，每个结果出现的概率分别为 p1，p2，⋯，pmp_1，p_2，⋯，p_mp1​，p2​，⋯，pm​，那么 XXX 的不确定度，即信息Entropy为： H(X)=∑_i=1mp_i⋅log⁡_21p_i=−∑_i=1mp_i⋅log⁡_2p_iH(X) =\\sum\\_{i=1}^{m} p\\_i \\cdot \\log\\_{2} \\frac{1}{p\\_i} = - \\sum\\_{i=1}^{m} p\\_i \\cdot \\log\\_{2} p\\_i H(X)=∑_i=1mp_i⋅log_2p_i1​=−∑_i=1mp_i⋅log_2p_i 将一个立方体A抛向空中，记落地时着地的面为 ccc, ccc 的取值为{1,2,3,4,5,6} info(c)=−(1/6⋅log_2(1/6)+...+1/6⋅log_2(1/6))=−1⋅log(1/6)=2.58；info(c) = - (1/6 \\cdot log\\_{2}(1/6)+...+1/6 \\cdot log\\_{2}(1/6)) = -1 \\cdot log(1/6) = 2.58； info(c)=−(1/6⋅log_2(1/6)+...+1/6⋅log_2(1/6))=−1⋅log(1/6)=2.58； 四面体抛入空中 : info(c)=−(1/4⋅log_2(1/4)+...+1/4⋅log_2(1/4))=−1⋅log(1/4)=2；info(c) = - (1/4 \\cdot log\\_{2}(1/4)+...+1/4 \\cdot log\\_{2}(1/4)) = -1 \\cdot log(1/4) = 2； info(c)=−(1/4⋅log_2(1/4)+...+1/4⋅log_2(1/4))=−1⋅log(1/4)=2； 球体抛入空中 : info(c)=−1⋅log(1)=0；info(c) = -1 \\cdot log(1) = 0； info(c)=−1⋅log(1)=0； 此时表示不确定程度为0，也就是着地时向下的面是确定的。 1.3 最大熵 “最大熵”这个名词听起来很深奥，但是它的原理很简单，我们每天都在用。说白了，就是要保留全部的不确定性，将风险降到最小。 p^=arg⁡max⁡_p∈CH(p)\\hat{p}=\\arg\\max\\_{p\\in C}H(p) p^​=argmax_p∈CH(p) 最大熵原理指出，对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们把这种模型称作“最大熵模型”。我们常说的，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定时，就要保留各种可能性。 2. Conditional Entropy 自古以来，信息和消除不确定性是相联系的。在英语里，信息和情报是同一个词（Infomation），而我们知道情报的作用就是排除不确定性。 (二战时期，斯大林无兵可派到欧洲战场，但在远东有60万大军，情报得知 Japan 不会北上) 一个事物内部会存有随机性，也就是不确定性，假定为 UUU, 而从外部消除这个不确定性的唯一的方法是引入信息 III， 而引入的信息量取决于这个不确定性的大小, 当 I&lt;UI &lt; UI&lt;U 时，这些信息可以消除一部分不确定性. 新的不确定性： U′=U−IU&#x27;=U-I U′=U−I 几乎所有的自然语言处理、信息与信号处理的应用都是一个消除不确定性的过程。 自然语言的统计模型，其中的一元模型就是通过某个词本身的概率分布，来消除不确定性；二元及更高阶的语言模型则还使用了上下文的信息，那就能准确预测一个句子中当前的词汇了。在数学上可以严格地证明为什么这些“相关的”信息也能够消除不确定性。为此，需要引入条件熵(Conditional Entropy)的概念。 2.1 条件熵的定义 假定 XXX 和 YYY 是两个随机变量，XXX 是我们需要了解的。假定我们现在知道了 XXX 的随机分布 P(X)P(X)P(X)，那么也就知道了 XXX 的熵： H(X)=−∑_x∈XP(x)⋅log⁡P(x)H(X)=-\\sum\\_{x\\in X}P(x)\\cdot\\log P(x) H(X)=−∑_x∈XP(x)⋅logP(x) 那么它的不确定性就是这么大。现在假定我们还知道 YYY 的一些情况，包括它和 XXX 一起出现的概率，在数学上称为联合概率分布(Joint Probability)，以及在 YYY 取不同值的前提下 XXX 的概率分布，在数学上称为条件概率分布(Conditional Probability)。定义在 YYY 条件下 XXX 的条件熵为： \\begin{align}H(X \\mid Y) &amp;= \\sum\\_{y\\in Y}P(y)H(X \\mid Y=y) \\\\\\&amp;=\\sum\\_{y\\in Y}P(y)\\big[-\\sum\\_{x\\in X}P(x\\mid y)\\log P(x\\mid y)\\big]\\\\\\&amp;=-\\sum\\_{x\\in X,y \\in Y}P(x,y)\\log P(x\\mid y)\\end{align} 可以证明 H(X)≥H(X∣Y)H(X)\\ge H(X\\mid Y)H(X)≥H(X∣Y)，也就是说，多了 YYY 的信息之后，关于 XXX 的不确定性下降了！在统计语言模型中，如果把 YYY 看成是前一个字，那么在数学上就证明了二元模型的不确定性小于一元模型。同理，可以定义有两个条件的条件熵： H(X∣Y,Z)=−∑_x∈X,y∈Y,z∈ZP(x,y,z)log⁡P(x∣y,z)H(X\\mid Y,Z)=-\\sum\\_{x\\in X,y\\in Y,z\\in Z}P(x,y,z)\\log P(x\\mid y,z) H(X∣Y,Z)=−∑_x∈X,y∈Y,z∈ZP(x,y,z)logP(x∣y,z) 还可以证明 H(X∣Y)≥H(X∣Y,Z)H(X\\mid Y)\\ge H(X\\mid Y,Z)H(X∣Y)≥H(X∣Y,Z)。也就说，三元模型应该比二元的好。 2.2 一个有意思的问题 思考一下，上述式子中的等号什么时候成立？等号成立说明增加了信息，不确定性却没有降低，这可能吗？ 答案是肯定的，如果我们获取的信息与要研究的事物毫无关系，等号就成立。那么如何判断事物之间是否存在关系，或者说我们应该如何去度量两个事物之间的关联性呢？这自然而然就引出了互信息(Mutual Infomation)的概念。 3. Mutual Infomation 获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能够量化地度量“相关性”。为此，香农在信息论中提出了一个互信息(Mutual Infomation)的概念作为两个随机事件“相关性”的量化度量。 假定有两个随机事件 XXX 和 YYY，它们之间的互信息定义如下： I(X;Y)=∑x∈X,y∈YP(x,y)log⁡P(x,y)P(x)P(y)I(X;Y)=\\sum_{x\\in X,y\\in Y}P(x,y)\\log\\frac{P(x,y)}{P(x)P(y)} I(X;Y)=x∈X,y∈Y∑​P(x,y)logP(x)P(y)P(x,y)​ 可以证明，互信息 I(X;Y)I(X;Y)I(X;Y) 就是随机事件 XXX 的不确定性（熵 H(X)H(X)H(X) 与在知道随机事件 YYY 条件下 XXX 的不确定性（条件熵 H(X∣Y)H(X∣Y)H(X∣Y)）之间的差异，即： I(X;Y)=H(X)−H(X∣Y)I(X;Y)=H(X)-H(X\\mid Y) I(X;Y)=H(X)−H(X∣Y) 所谓两个事件相关性的量化度量，就是在了解了其中一个 YYY 的前提下，对消除另一个 XXX 不确定性所提供的信息量。互信息是一个取值在 0 到 min(H(X),H(Y))min(H(X),H(Y))min(H(X),H(Y)) 之间的函数，当 XXX 和 YYY 完全相关时，它的取值是 1；当二者完全无关时，它的取值是 0。 4. Relative Entropy 相对熵也用来衡量相关性，但和变量的互信息不同，它用来衡量两个取值为正数的函数的相似性。它的定义如下： KL(f(x)∣∣g(x))=∑_x∈Xf(x)⋅log⁡f(x)g(x)KL(f(x) || g(x))=\\sum\\_{x\\in X}f(x)\\cdot\\log \\frac{f(x)}{g(x)} KL(f(x)∣∣g(x))=∑_x∈Xf(x)⋅logg(x)f(x)​ 大家不必关心公式本身，只需记住下面三条结论就好： (1). 对于两个完全相同的函数，它们的相对熵等于零。 (2). 相对熵越大，两个函数差异越大；反之，相对熵越小，两个函数差异越小。 (3). 对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性。 第 (3) 条： 可以对标，CrossEntropy 的意义. 注意，相对熵是不对称的，即： KL(f(x)∣∣g(x))≠KL(g(x)∣∣f(x))KL(f(x)||g(x))\\ne KL(g(x)||f(x)) KL(f(x)∣∣g(x))​=KL(g(x)∣∣f(x)) 为了让它对称，詹森和香农提出一种新的相对熵的计算方法 JS(f(x)∣∣g(x))=12[KL(f(x)∣∣g(x))+KL(g(x)∣∣f(x))]JS(f(x)||g(x))=\\frac{1}{2}\\big[KL(f(x)||g(x))+KL(g(x)||f(x))\\big] JS(f(x)∣∣g(x))=21​[KL(f(x)∣∣g(x))+KL(g(x)∣∣f(x))] 相对熵最早用在信号处理上。如两个随机信号，它们相对熵越小，说明这两个信号越接近，否则信号的差异越大。 Reference 吴军《数学之美》 宗成庆《统计自然语言处理》 信息的度量和作用","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"entropy","slug":"entropy","permalink":"http://www.iequa.com/tags/entropy/"}]},{"title":"Evaluation Metric","slug":"ml/Evaluation_Metric","date":"2019-06-03T02:01:21.000Z","updated":"2021-06-22T06:42:09.169Z","comments":true,"path":"2019/06/03/ml/Evaluation_Metric/","link":"","permalink":"http://www.iequa.com/2019/06/03/ml/Evaluation_Metric/","excerpt":"机器学习性能评估指标","text":"机器学习性能评估指标 选择与问题相匹配的评估方法，才能快速发现在 模型选择和训练过程 中可能出现的问题，迭代地对模型进行优化. 针对 分类、排序、回归、序列预测 等不同类型的机器学习问题，评估指标的选择也有所不同: 本文将谈谈机器学习中，常用的性能评估指标： &lt;img class=“img-fancy” src=&quot;/images/ml/metric/metric-2.jpg&quot;, width=“600” border=“0px”, alt=&quot;&quot;%} 关于模型评估的基础概念: 【误差(error)】：学习器的预测输出与样本的真实输出之间的差异。根据产生误差的数据集，可分为： Training error：又称为经验误差(empirical error)，学习器在训练集上的误差。 Test error：学习器在测试集上的误差。 Generalization error：学习器在未知新样本上的误差。 需要注意的是，上述所说的“误差”均指误差期望，排除数据集大小的影响。 应该从训练样本中尽可能学出适用于所有 潜在样本的“普遍规律”，这样才能在遇到新样本时做出正确的判别。因为，泛化误差无法测量，因此，通常我们会将 Test error 近似等同于 Generalization error。 1. Classification Metric 1.1 Accuracy 准确率：指的是分类正确的样本数量占样本总数的比例，定义如下： Accuracy=n_correctn_total,Error=n_errorn_totalAccuracy = \\frac{n\\_{correct}}{n\\_{total}}, Error = \\frac{n\\_{error}}{n\\_{total}} Accuracy=n_totaln_correct​,Error=n_totaln_error​ 正反比例严重失衡，则没意义，存在 accuracy paradox 现象. accuracy 准确率 = (TP+TN)/(TP+TN+FP+FN), 准确率可以判断总的正确率 1.2 Precision precision 查准率 (80% = 你一共预测了100个正例，80个是对的正例) Precision = TP/(TP+FP) 1.3 Recall recall (样本中的正例有多少被预测正确 TPR = TP/(TP+FN)) 1.4 F1-score multi-class classification 如果非要用一个综合考量的 metric 的话， macro-average（宏平均）- 分布计算每个类别的F1，然后做平均（各类别F1的权重相同） micro-average（微平均）- 通过先计算总体的TP，FN和FP的数量，再计算F1 macro-average（宏平均） 会比 micro-average（微平均）好一些哦，因为 macro 会受 minority class 影响更大，也就是说更能体现在 small class 上的 performance. sklearn中 F1-micro 与 F1-macro 区别和计算原理 precision &amp; recall precision 是相对你自己的模型预测而言 recall 是相对真实的答案而言 1.5 P-R curve P-R曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说，其P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本， 小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。 整条 P-R 曲线是通过将阈值从高到低移动而生成的。 实线：A model， 虚线：B model 上面说的阈值从高到低原因：阈值高，精确率高，阈值低，召回率高 1.6 ROC Q1: 什么是ROC曲线？ ROC curve （TPR 纵轴，FPR 横轴，TP（真正率）和 FP（假正率），设一个阈值） ROC（Receiver Operating Characteristic Curve）曲线。 ROC 曲线 是基于混淆矩阵得出的。 TPR = recall = 灵敏度 = P（X=1 | Y=1）， FPR = 特异度 = P（X=0 | Y=0） 注意： ROC 曲线是 FPR 越小，TPR 越大 最好. (也就是曲线 x 轴越小，y轴越大 最好) ROC曲线的适用场景更多，被广泛用于排序、推荐、广告等领域 Q2: 如何绘制ROC曲线？ 与前面的P-R曲线类似， ROC曲线是通过不断移动分类器的“截断点”来生成曲线上的一组关键点的, 截断点指的是区分正负预测结果的阈值: (1). 还有一种更直观地绘制ROC的方法。根据样本标签统计正负样本数量，假设正样本数量 P，负样本数量 N. (2). 接下来把横轴刻度间隔设置为 1 / N, 纵轴的刻度间隔设置为 1 / P。 (3). 再根据模型输出的预测概率对样本进行排序（从高到低）；依次遍历样本，同时从零点开始绘制ROC曲线，每遇到一个正样本就沿着纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在（1，1）整个ROC曲线绘制完成。这样就很好理解为什么面积越大，分类性能越好了，想象这个过程即可。 ROC曲线无视样本不平衡 精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ 机器学习之类别不平衡问题 (2) —— ROC和PR曲线 1.7 AUC AUC = 0.5，跟随机猜测一样， ROC 纵轴 TPR 越大， 横轴 FPR 越小 模型越好 0.5 - 0.7：效果较低，但用于预测股票已经很不错了 0.7 - 0.85：效果一般 0.85 - 0.95：效果很好 real world data 经常会面临 class imbalance 问题，即正负样本比例失衡。 根据计算公式可以推知，在 testing set 出现 imbalance 时 ROC曲线 能保持不变，而 PR 则会出现大变化。 2 Regression Metric 2.1 MSE MSE （Mean Squared Error）称为均方误差，，又被称为 L2范数损失: MSE=1n∑i=1n(f_i−y_i)2MSE=\\frac{1}{n}\\sum_{i=1}^n{(f\\_i - y\\_i)^2} MSE=n1​i=1∑n​(f_i−y_i)2 2.2 RMSE 均方根误差(Root Mean Squared Error, RMSE)，定义如下： RMSE=MSERMSE=\\sqrt{MSE} RMSE=MSE​ 2.3 MAE MAE=1n∑_i=1n∣f_i−y_i∣MAE = \\frac{1}{n}\\sum\\_{i=1}^n|f\\_i-y\\_i| MAE=n1​∑_i=1n∣f_i−y_i∣ 缺点：因为它使用的是平均误差，而平均误差对异常点较敏感，如果回归器对某个点的回归值很不合理，那么它的误差则比较大，从而会对RMSE的值有较大影响，即平均值是非鲁棒的。 2.4 MAPE 全称是 Mean Absolute Percentage Error（WikiPedia）, 也叫 mean absolute percentage deviation (MAPD)，在统计领域是一个预测准确性的衡量指标。 MAPE=100n∑_t=1n∣y_i−f_iy_i∣MAPE=\\frac{100}{n}\\sum\\_{t=1}^{n}|\\frac{y\\_i-f\\_i}{y\\_i}| MAPE=n100​∑_t=1n∣y_iy_i−f_i​∣ 3. 余弦相似度 vs 欧式距离 余弦相似度 ： 坐标系中两个向量，来计算两向量之间的夹角, 值域 [-1, 1] 余弦距离 ： 值域 [0, 1] 1−cos1 - cos 1−cos 欧式距离 ： 坐标系中两个点，来计算两点之间的距离； 假设二维空间两个点： 然后归一化为单位向量： 余弦相似度就是： 欧式距离就是： 化简后就是： 很明显，是一个单调函数（图像类似于单位元的第一象限部分），也就意味着，两者在归一化为单位向量的时候计算相似度结果完全一样。只不过余弦相似度是值越大月相似，欧式距离是值越小越相似。 知识点 ： 余弦相似度、余弦距离、欧式距离、距离的定义 4. A/B 测试的陷阱 Q1：在对模型进行充分的离线评估之后，为什么还要进行在线A/B测试？ （1）离线评估无法完全消除模型过拟合的影响。 （2）离线评估无法完全还原线上的工程环境。线上的工程环境包括数据延迟、数据缺失、标签缺失等情况。 （3）线上系统的某些商业指标在离线评估中无法计算。 离线模型评估的指标包括准确率，召回率和ROC曲线等。 而线上评估可以全面了解该推荐算法带来的用户点击率、留存时长、PV访问量等的变化。这些都要由A/B测试来进行全面的评估。 Q2：如何进行线上AB测试？ 答：进行AB测试的主要手段是进行用户分桶，即将用户分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型。 分桶的过程中，要注意 样本的独立性 和 采样方式的无偏性. 5. 模型评估方法 Q1: 在模型评估过程中，有哪些主要的验证方法，它们的优缺点是什么？ Holdout检验 交叉检验 (k-fold交叉验证, 在实际实验中，经常取10。) 自助法 Q2：在自助法的采样过程中，对n个样本进行n次自助抽样，当n趋于无穷大时，最终有多少数据从未被选择过? 根据重要极限，当样本量很大时，大约有 0.368 的样本从未被选择过，可做为验证集。 6. 超参数调优 网格搜索 随机搜索 贝叶斯优化 (未研究) 7. 过拟合/欠拟合 防止 overfiting 的 8 条 1). get more data 2). Data augmentation 3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验) 4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；) 5). Choosing Right Network Structure 6). Early stopping 7). Model Ensumble 8). Batch Normalization 8. 其他评价指标 计算速度：模型训练和预测需要的时间； 鲁棒性：处理缺失值和异常值的能力； 可拓展性：处理大数据集的能力； 可解释性：模型预测标准的可理解性，比如决策树产生的规则就很容易理解，而神经网络被称为黑盒子的原因就是它的大量参数并不好理解。 Reference 精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ 机器学习之类别不平衡问题 (2) —— ROC和PR曲线 sklearn中 F1-micro 与 F1-macro 区别和计算原理 Bias-Variance Tradeoff 程序员大本营-百面 CSDN 模型评估 简单聊聊模型的性能评估标准 从0开始机器学习-为什么要做A／B Test 百面模型评估","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"Metric","slug":"Metric","permalink":"http://www.iequa.com/tags/Metric/"}]},{"title":"Rondom Forest vs GBDT","slug":"ml/Random_Forest_and_GBDT","date":"2019-06-02T02:01:21.000Z","updated":"2021-06-22T06:46:36.119Z","comments":true,"path":"2019/06/02/ml/Random_Forest_and_GBDT/","link":"","permalink":"http://www.iequa.com/2019/06/02/ml/Random_Forest_and_GBDT/","excerpt":"","text":"1. Random Forest Rondom Forest 是一个典型的多个决策树的组合分类器. Rondom Forest, 的 Random 体现在 2 个方面： data random feature random RF、bagging、boosting、GBDT、xgboost算法总结 Sample data Random： &lt;img class=“img-fancy” src=&quot;/images/ml/ensumble/ensumble-2.jpg&quot;, width=“600” border=“0px” %} Feature Random： 与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。 &lt;img class=“img-fancy” src=&quot;/images/ml/ensumble/ensumble-3.jpg&quot;, width=“600” border=“0px” %} 2. GBDT GBDT 是以决策树（CART）为基学习器的 GB算法，是迭代树，而不是分类树。 一般 Boosting 算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。 &lt;img class=“img-fancy” src=&quot;/images/ml/ensumble/ensumble-4.jpg&quot;, width=“600px” %} GBDT 的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。 &lt;img class=“img-fancy” src=&quot;/images/ml/ensumble/ensumble-5.jpg&quot;, width=“850px” %} 3. RF vs GBDT 组成 RF 的树可以是分类树，也可以是回归树；而GBDT只由回归树组成 组成 RF 的树可以并行生成；而 GBDT 只能是串行生成. 对于最终的输出结果而言，随机森林采用多数投票等；而 GBDT 则是将所有结果累加起来，或者加权累加起来. RF 对异常值不敏感，GBDT 对异常值非常敏感. RF 对训练集一视同仁，GBDT 是基于权值的弱分类器的集成. RF 是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能. 4. Xgboost vs GBDT Xgboost相比于GBDT来说，更加有效应用了数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂。目标函数依然是所有树的预测值相加等于预测值。 二阶泰勒展开，同时用到了一阶和二阶导数 xgboost在代价函数里加入了正则项，用于控制模型的复杂度 Shrinkage（缩减），相当于学习速率（xgboost中的eta） 列抽样（column subsampling）。xgboost借鉴RF做法，支持列抽样（即每次的输入特征不是全部特征) 并行化处理： 预先对每个特征内部进行了排序找出候选切割点.各个feature的增益计算就可以开多线程进行. &lt;img class=“img-fancy” src=&quot;/images/ml/ensumble/ensumble-6.jpg&quot;, width=“600px” %} 好的模型需要具备两个基本要素： 是要有好的精度（即好的拟合程度） 是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定） 因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项） 常用的误差项有平方误差和逻辑斯蒂误差，常见的惩罚项有l1，l2正则，l1正则是将模型各个元素进行求和，l2正则是对元素求平方。 ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结 5. Bagging vs Boosting 两种方法的采样和处理方式都不同，主要代表算法便是： Rondom Forest、 GBDT. 5.1 Bagging Bagging 的思想比较简单，即每一次从原始数据中根据 均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。 5.2 Boosting Boosting 的每一次抽样的 样本分布都是不一样 的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到 难以分类的样本，这是一个 不断学习的过程，也是一个不断提升 的过程，这也就是boosting思想的本质所在。 迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。 6. 特征重要度 6.1 RF 特征度量 (1）对每一颗决策树，选择相应的袋外数据（out of bag，OOB）​计算袋外数据误差，记为errOOB1. 所谓袋外数据是指，每次建立决策树时，通过重复抽样得到一个数据用于训练​决策树，这时还有大约1/3的数据没有被利用，没有参与决策树的建立。这部分数据可以用于对决策树的性能进行评估，计算模型的预测错误率，称为袋外数据误差。 ​这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。 ​(2）随机对袋外数据OOB所有样本的特征XXX加入噪声干扰（可以随机改变样本在特征X处的值），再次计算袋外数据误差，记为errOOB2。 (3）​假设森林中有N棵树，则特征X的重要性=∑（errOOB2-errOOB1）/N。这个数值之所以能够说明特征的重要性是因为，如果加入随机噪声后，袋外数据准确率大幅度下降（即errOOB2上升），说明这个特征对于样本的预测结果有很大影响，进而说明重要程度比较高。 6.2 GBDT 特征度量 主要是通过计算特征i在单棵树中重要度的平均值，计算公式如下： 其中，M是树的数量。特征i在单棵树的重要度主要是通过计算按这个特征i分裂之后损失的减少值. 其中，L是叶子节点的数量，L-1就是非叶子结点的数量。 6.3 Xgboost XGboost是通过该特征每棵树中分裂次数的和去计算的，比如这个特征在第一棵树分裂1次，第二棵树2次……，那么这个特征的得分就是(1+2+…)。 Reference 随机森林进行特征重要性度量的详细说明 什么是无偏估计？ GBDT PDF 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"Rondom_Forest","slug":"Rondom-Forest","permalink":"http://www.iequa.com/tags/Rondom-Forest/"}]},{"title":"Logistic Regression","slug":"ml/Logistic_Regression","date":"2019-05-31T02:01:21.000Z","updated":"2021-06-22T06:46:36.114Z","comments":true,"path":"2019/05/31/ml/Logistic_Regression/","link":"","permalink":"http://www.iequa.com/2019/05/31/ml/Logistic_Regression/","excerpt":"","text":"Logistic Regression 是一种用于解决二分类（0 or 1）问题的机器学习方法. 举个🌰 : (1). 用户购买某商品的可能性 (2). 某病人患有某种疾病的可能性 (3). 某广告被用户点击的可能性等 1. Logistic vs Linear Regression Logistic Regression 和 Linear Regression 都是一种广义线性模型（generalized linear model）。 逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。 因此与线性回归有很多相同之处，去除 Sigmoid 映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过 Sigmoid函数 引入了非线性因素，因此可以轻松处理 0/1 分类问题。 1.1 Linear Regression z=θ_0+θ_1x_1+θ_2x_2+θ_3x_3...+θ_nx_n=θTxz={\\theta\\_{0}}+{\\theta\\_{1}x\\_{1}}+{\\theta\\_{2}x\\_{2}+{\\theta\\_{3}x\\_{3}}...+{\\theta\\_{n}x\\_{n}}}=\\theta^Tx z=θ_0+θ_1x_1+θ_2x_2+θ_3x_3...+θ_nx_n=θTx 使用最小二乘法求解 Linear Regression 时，我们认为因变量 y 服从正态分布。 线性回归假设因变量 y 服从高斯分布: 真实值y与拟合值Y之间的差值是不是符合正态分布。 1.2 Logistic Regression h_θ(x)=11+e−z=11+e−θTxh\\_{\\theta}(x)=\\frac{1}{1+e^{-z}}=\\frac{1}{1+e^{-\\theta^Tx}} h_θ(x)=1+e−z1​=1+e−θTx1​ Logistic Regression 将线性函数的结果映射到了 sigmoid 函数 中. 取值在 [0 logp1−p=θTxlog\\frac{p}{1-p} = \\theta^Tx log1−pp​=θTx 一件事发生的几率 odds=p1−podds = \\frac{p}{1-p}odds=1−pp​， p=P(y=1∣x)p = P(y=1|x)p=P(y=1∣x) Logistic Regression 可以看作是对于 “y=1|x” 这一事件的对数几率的线性回归. 逻辑回归通过对似然函数 $ L(\\theta)=\\prod_{i=1}{N}P(y_i|x_i;\\theta)=\\prod_{i=1}{N}(\\pi(x_i)){y_i}(1-\\pi(x_i)){1-y_i} $ 的学习，得到最佳参数 θ. Logistic 与 Linear Regression 二者都使用了极大似然估计来对训练样本进行建模 迭代自己 Logistic Regression 2. LR Hypothesis function 其函数曲线如下： 取值在[0 Logistic Regression 假设函数形式如下： 所以： 一个机器学习的模型，实际上是把决策函数限定在某一组条件下，这组限定条件就决定了模型的假设空间。当然，我们还希望这组限定条件简单而合理。而逻辑回归模型所做的假设是： 这个函数的意思就是在给定 x 和 0 的条件下 y=1 的概率 3. Cost Function Cost Function： Loss Function： 上面的方程等价于： 选择 Cost_Function 时，最好挑选对参数 θ\\thetaθ 可微的函数（全微分存在，偏导数一定存在） 对于每种算法来说，Cost_Function 不是唯一的； Cost_Function 是参数 θ\\thetaθ 的函数； Cost Function 是对所有样本而言, Loss Function 是对单一样本而言. J(θ)J(\\theta)J(θ) 是一个标量, 我们需要 min 最小化它. LR 中，代价函数是交叉熵 (Cross Entropy)，交叉熵是一个常见的代价函数: good 简单的交叉熵损失函数，你真的懂了吗？ 4. Cross Entropy Cross Entropy 是信息论中的一个概念，要想了解交叉熵的本质，需要先从最基本的概念讲起。 信息量、熵、相对熵（KL散度）、交叉熵 同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)， 用 KL散度 来衡量这两个分布的差异。 D\\_{KL}(p||q)=\\sum\\_{i=1}^np(x\\_i)log(\\frac{p(x\\_i)}{q(x\\_i)}) \\tag{3.1} \\begin{eqnarray} D\\_{KL}(p||q) &amp;=&amp; \\sum\\_{i=1}^np(x\\_i)log(p(x\\_i))-\\sum\\_{i=1}^np(x\\_i)log(q(x\\_i))\\end{eqnarray} 等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵： \\begin{eqnarray} =&amp; -H(p(x))+[-\\sum\\_{i=1}^np(x\\_i)log(q(x\\_i))] \\end{eqnarray} 在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即 D_KL(y∣∣y^)D\\_{KL}(y||\\hat{y})D_KL(y∣∣y^​) . 由于KL散度中的前一部分 −H(y)-H(y)−H(y) 不变，故在优化过程中，只需要关注 Cross Entropy 就可以了。 所以一般在机器学习中直接用用 Cross Entropy Loss，评估模型。 D_KLD\\_{KL}D_KL 的值越小，表示 q分布 和 p分布 越接近. 一文搞懂交叉熵在机器学习中的使用 交叉熵在多分类问题中的使用, sigmoid 1）CrossEntropy lossFunction 二分类: 意义：能表征 真实样本标签 和 预测概率 之间的差值 2）最小化交叉熵的本质就是对数似然函数的最大化； 3）对数似然函数的本质就是衡量在某个参数下，整体的估计和真实情况一样的概率，越大代表越相近； 4）损失函数的本质就是衡量预测值和真实值之间的差距，越大代表越不相近。 4.1 Log 设计理念 预测输出与 y 差得越多，L 的值越大，也就是说对当前模型的 “ 惩罚 ” 越大，而且是非线性增大，是一种类似指数增长的级别。这是由 log 函数本身的特性所决定的。这样的好处是 模型会倾向于让预测输出更接近真实样本标签 y。 我们希望 log P(y|x) 越大越好，反过来，只要 log P(y|x) 的负值 -log P(y|x) 越小就行了。那我们就可以引入损失函数，且令 Loss = -log P(y|x)即可。则得到损失函数为： 图可以帮助我们对 CrossEntropy lossFunction 有更直观的理解。无论真实样本标签 y 是 0 还是 1，L 都表征了预测输出与 y 的差距。 4.2 MLE 最大似然 就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的 模型参数值！ 输入有两个：xxx 表示某一个具体的数据；θθθ 表示模型的参数。 Probability Function： 对于这个函数： P(x∣θ)P(x|θ)P(x∣θ) ， 如果 θθθ 是已知确定的，xxx 是变量，这个函数叫做概率函数 (probability function)，它描述对于不同的样本点 xxx，其出现概率是多少。 Likelihood Function： 如果 xxx 是已知确定的，θθθ 是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现 xxx 这个样本点的概率是多少。 MLE 提供了一种 给定观察数据来评估模型参数 的方法，即：“模型已定，参数未知”。 MLE 中 采样 需满足一个重要的假设，就是所有的采样都是 独立同分布 的. 一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。 5. 最小二乘法 vs 最大似然估计 总结一句话： 最小二乘法的核心是权衡，因为你要在很多条线中间选择，选择出距离所有的点之和最短的； 而极大似然的核心是自恋，要相信自己是天选之子，自己看到的，就是冥冥之中最接近真相的。_ Reference 逻辑回归（Logistic Regression）（一） 广义线性模型（Generalized Linear Model） GLM(广义线性模型) 与 LR(逻辑回归) 详解 怎样用通俗易懂的文字解释正态分布及其意义？ 最大似然估计和最小二乘法怎么理解？ 知乎：一文搞懂极大似然估计 CSDN：详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解 一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"Logistic_Regression","slug":"Logistic-Regression","permalink":"http://www.iequa.com/tags/Logistic-Regression/"}]},{"title":"IELTS Learning Experience","slug":"English/IELTS/ielts-learning-exp","date":"2019-05-25T09:14:48.000Z","updated":"2021-06-22T06:33:48.644Z","comments":true,"path":"2019/05/25/English/IELTS/ielts-learning-exp/","link":"","permalink":"http://www.iequa.com/2019/05/25/English/IELTS/ielts-learning-exp/","excerpt":"Are you ready?","text":"Are you ready? 复习材料推荐： 剑桥雅思 8～13 雅思王听力，真题语料库，王陆网课太拖， 时间紧，你就练习，5-11-8-3-4 考点词538. 非常有用，公开在置顶评论 Simon 口语翻转课堂，非常逻辑清晰, 太阳法对我非常有用. 网课好，顾家北写作书，非常有名, 后面词伙要背下. 实用网站： YouTube &amp; Bilibili 大B站，应有尽有. www.chinaielts.org 蛮多有用的东西. ielts.kmf.com/home 考满分网站, 其中 听力词汇必备练习 非常好，空了就去刷一个小时，锻炼你的手感. 1. Listening Skills 听力填空词 90% 都是名词 所听即所得 看题顺序技巧 出现地图记住街道的位置 听到的一定要写对 不要试图将整篇全部听明白 精听 &gt; 听写单字 APP: 《每日英语听力》、《BBC学英语》，《雅思王听力》 有定时功能，要保证自己长时间沉侵在英文环境. 《雅思王听力》： 2019年雅思王听力语料库学习方法 Saturday Sunday Monday Tuesday Wednesday Thursday Friday 全天听写5，11，3，4 改错，计算正确率整理错题本 背诵错词3.1-3.5背诵11章 S1S4 背诵错词3.6-4.1 背诵错词4.2-5.2 背诵错词5.3-5.7 背诵错词5.8-5.12背诵11章 S2S3 first/second/third == 1.0/1.4/1.6 (Chapter 3, 4, 11) (Chapter 5 == 1.0) 背诵秘诀： 眼看单字+手抄单字+嘴读单字(跟CD) 同步进行 背诵错字的时间 ： (周一到周五，21:00 ~ 22:30) Target : 2019.06.22 ~ 2019.07.22 听写正确率在 85% 以上. 2. Reading Skills 填空最容易 看题干，画重要词 总分总，顺序出题 时间分配，第一篇要15分其他20分 同义词替换是考察的重中之重 快速提高阅读： 先看题目，了解主题，再看文章 雅思出现的答案，是按照顺序出现的 看完问题，在去找答案，不需要全部读完 哪个问题，答案在哪段是可以预知的 APP： 《墨墨背单字》、《知米背单词》、《Timing》， Reading 一共 40 题. 2019.10.01 ~ 2019.11.15 3. Writing Skills Simon 句型: 上升下降 倍数、对比、最值… 小作文： 诀窍 总分总 大作文： 总分总 总： 讲下背景 （一些人认为，另一些人认为，我认为…） 分： 扯出东西，证明你的观点（可以是名人故事也可以是自己） 总： 总结下观点 Writing 一个小时，一个小一个大 2019.07.22 ~ 2019.08.31 小矮人Ellen IELTS Writing Task 1 小矮人Ellen IELTS Writing Task 2 雅思写作7分+, 只需9个句型,10分钟教会你 L329 雅思写作保7分,必备4个技能-资料下载 L299 4. Oral language Skills 发音问题不用担心，重要交流 表达清楚意思，不要背范文，要有思考肢体动作，与考官有眼神交流 教育，医疗，社会环境 准备口语同一个话题可以重复 catch that 语速可以慢一点，但是不要停太久 in fact，How can I put it，in addition ，what else 不要说 of course ，说 Yes 给明确的观点，不要两种观点 《雅思口语应急300句》 有一些很地道的俚语, 在于你记忆的多准确. 喜马拉雅上有音带. Part1, 用 《雅思哥》 录音，听你是否有语法错误，练习你的流利度 Part2, 不要整段写下来, 关键还是你和 topic 相关的词，能提醒你的词. Part3, 提前30天开始复习。 APP： 《雅思哥》 2019.08.01 ~ 2019.09.01 ~ 2019.11.01 Reference www.aut.ac.nz 雅思 | 雅思自学 | 5.5到7学习经验分享 | ILTES | 雅思考试技巧！听力8分 首战7分 心得全面分享！- 渣渣自学2周的经验 Bilibili 雅思考试技巧！听力8分 首战7分 雅思考试, 全球哪里, 给分高？,泰国清迈IDP 小矮人Ellen 雅思口說技巧 ＋ 里茲螞蟻書推薦 | IELTS SPEAKING TIPS","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"Auckland Study Abroad","slug":"world/auckland_study_abroad","date":"2019-05-17T13:31:48.000Z","updated":"2021-06-22T06:07:42.552Z","comments":true,"path":"2019/05/17/world/auckland_study_abroad/","link":"","permalink":"http://www.iequa.com/2019/05/17/world/auckland_study_abroad/","excerpt":"Auckland New Zealand","text":"Auckland New Zealand 新西兰留学答疑申请互助微信交流群 奥克兰大学:奥克兰大学是新西兰的“国宝级”大学，是新西兰顶级的教育学府。工程专业在新西兰数 一数二，值得有志于在工程方向发展的学生申请。 奥克兰大学:奥克兰大学的计算机专业涉及到计算机相关的设计开发以及计算机系统的使用，学生可以学习计算机编程、人工智能、数据运算等知识，毕业后就业前景广阔。 奥克兰大学硕士专业语言直升课程(英文全称为 EnglishPathway for Postgraduate Studies，缩写为 EPPS) EPPS 课程主要是针对那些已经满足了奥克兰硕士专业学术要求，但语言方面略有不足的学生。 EPPS 课程能够确保学生获得未来本科，硕士甚至博士专业学习所需的英语语言能力与学术写作能力。 成功通过 EPPS 课程并达到相应的分数后，无需参加任何雅思考试即可进入奥克兰大学的专业课程进行学习。 新西兰八大的语言中心入学要求 : 奥克兰大学 本科/研究生最低要求:比正课低 0.5 分 奥克兰大学 2019 新生讨论群 新西兰高校申请文件分享群 入驻顾问有:新西兰各大学校校方代理，教育顾问及课程顾问 硕士学位 L9 专业定位表、MBTI 性格测试题库、新西兰可移民职业列表及清单、坎特伯雷地区紧缺清单、在读证明和成绩单中英文模板、PS 个人陈述中英文模板、CV 简历中英文模板、推荐信中英文模板。 奥克兰大学国际留学生奖学金(University of Auckland International Student Scholarships) 学生的生源国也会作为评选委员会的参考因素, maybe China 不会被选中. 我们为大家提供了奥克兰大学申请交流群和校友群，请大家加我们工作人员微信，或者 关注我们的微信公众号。 Reference www.auckland.ac.nz 小矮人Ellen 出國留學該找代辦嗎？代辦不為人知的秘密! The University of Auckland The University of Auckland Chinese Tuition fees 知乎： 奥克兰大学读硕士一年多少钱? 新加坡国立大学申请 Tuition fees 本地学生学费","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"NewZealand","slug":"NewZealand","permalink":"http://www.iequa.com/tags/NewZealand/"}]},{"title":"Dimensionality Reduction：PCA","slug":"ml/PCA","date":"2019-04-21T09:01:21.000Z","updated":"2021-06-22T06:46:36.094Z","comments":true,"path":"2019/04/21/ml/PCA/","link":"","permalink":"http://www.iequa.com/2019/04/21/ml/PCA/","excerpt":"PCA","text":"PCA 常见的 Dimensionality Reduction 方法： PCA、LDA 等。 PCA主成分分析 Machine Learning 中的数据维数 与 现实世界中的空间维度本同末离。在 Machine Learning 中，数据通常需要被表示成向量形式以输入模型进行训练。用一个 低维度的向量 表示原始 高纬度的特征 显得尤为重要。 降维的好处很明显，它不仅可以数据减少对内存的占用，而且还可以加快学习算法的执行 PCA 是一种经典的降维方法，是一种线性、非监督、全局的降维方法，是面试常见问题. 1. Motivation 如果我们有许多冗余的数据，我们可能需要对特征量进行降维(Dimensionality Reduction) 1.1 Data Compression 如图10-2所示的3维到2维的例子，通过对x1,x2,x3的可视化，发现虽然样本处于3维空间，但是他们大多数都分布在同一个平面中，所以我们可以通过投影，将3维降为2维。 1.2 Visualization 特征量维数大于3时，我们几乎不能对数据进行可视化。所以，有时为了对数据进行可视化，我们需要对其进行降维。我们可以找到2个或3个具有代表性的特征量，他们(大致)可以概括其他的特征量。 例如，描述一个国家有很多特征量，比如GDP，人均GDP，人均寿命，平均家庭收入等等。 2. Principal Component Analysis 主成分分析(Principal Component Analysis : PCA)是最常用的降维算法。 2.1 Problem formulation 首先我们思考如下问题，对于正交属性空间(对2维空间即为直角坐标系)中的样本点，如何用一个超平面(直线/平面的高维推广)对所有样本进行恰当的表达？ 事实上，若存在这样的超平面，那么它大概应具有这样的性质： 最近重构性 : 样本点到这个超平面的距离都足够近； 最大可分性 : 样本点在这个超平面上的投影能尽可能分开。 对当前样本而言，s1平面比s2平面的最近重构性要好（样本离平面的距离更近）； 对当前样本而言，s1平面(左边) 比s2平面的最大可分性要好(样本点更分散)。 总结: 让上面的例子也说明了 最近重构性 和 最大可分性 可以同时满足。分别以最近重构性和最大可分性为目标，能够得到 PCA 的两种等价推导。 丢失信息对比： s2平面 进行投影降维，我们会丢失更多（相当多）的特征量信息，因为它的投影结果甚至可以在转化为1维。 而 s1平面 上的投影包含更多的信息(丢失的更少)。 将特征量从n维降到k维： 以最近重构性为目标，PCA的目标是找到k个向量，将所有样本投影到这k个向量构成的超平面，使得投影的距离最小（或者说投影误差projection error最小）。 以最大可分性为目标，PCA的目标是找到k个向量，将所有样本投影到这k个向量构成的超平面，使得样本点的投影能够尽可能的分开，也就是使投影后的样本点方差最大化。 摘要： PCA 与 线性回归 有一点相似，但是他们是2种不同的算法. good CSDN 主成分分析PCA之协方差矩阵的理解 知乎: 如何直观地理解「协方差矩阵」？ PCA为什么要用协方差矩阵？ 线性降维方法（代码篇）| 机器学习你会遇到的“坑” 主成分分析（PCA）原理总结 刘建平Pinard 用scikit-learn学习主成分分析(PCA) PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？为神马啊为神马？ 降维的目的就是“降噪”和“去冗余”。“降噪”的目的就是使保留下来的维度间的相关性尽可能小，而“去冗余”的目的就是使保留下来的维度含有的“能量”即方差尽可能大。那首先的首先，我们得需要知道各维度间的相关性以及个维度上的方差啊！那有什么数据结构能同时表现不同维度间的**相关性以及各个维度上的方差**呢？自然是协方差矩阵！ 协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。协方差矩阵的主对角线上的元素是各个维度上的方差(即能量)，其他元素是两两维度间的协方差(即相关性)。我们要的东西协方差矩阵都有了，先来看“降噪”，让保留下的不同维度间的相关性尽可能小，也就是说让协方差矩阵中非对角线元素都基本为零。达到这个目的的方式自然不用说，线代中奖的很明确——矩阵对角化。 方差是用来度量单个随机变量的离散程度，而协方差则一般用来刻画两个随机变量的相似程度. 2.2 PCA Algorithm 输入： 训练集 x(1),x(2),...,x(m)x^{(1)}, x^{(2)}, ..., x^{(m)}x(1),x(2),...,x(m) 和 低维空间维数 kkk 过程： (1) 数据预处理：对所有样本进行中心化(即使得样本和为0) (2) 计算样本的协方差矩阵 (3) 对2中求得的协方差矩阵Sigma进行特征值分解 在实践中通常对协方差矩阵进行 SVD分解 代替 特征值分解. [U,S,V]=svd(Sigma); [U, S, V] = svd(Sigma); [U,S,V]=svd(Sigma); (4) 取最大的k个特征值所对应的特征向量 u(1),u(2),...,u(k)u^{(1)}, u^{(2)}, ..., u^{(k)}u(1),u(2),...,u(k) 2.3 Choosing the Number of Principal Components 如何选择k（又称为主成分的个数）的值？ 首先，试想我们可以使用PCA来压缩数据，我们应该如何解压？或者说如何回到原本的样本值？事实上我们可以利用下列等式计算出原始数据的近似值Xapprox： 2.4 Advice for Applying PCA PCA 通常用来加快监督学习算法. PCA 应该只是通过 训练集的特征量 来获取 投影矩阵 Ureduce，而不是交叉检验集或测试集。 但是获取到 Ureduce 之后可以应用在交叉检验集和测试集。 避免使用 PCA 来防止 overfiting，PCA 只是对 特征 XXX 进行降维，并没有考虑 YYY 的值； 不应在项目开始就用PCA: 花大量时间来选择k值，可能项目并不需用PCA来降维。同时降维定会丢失一些信息。 仅在需用 PCA 的时使用 PCA: 降维丢失的信息可能在一定程度上是噪声，使用 PCA 可以起到一定的去噪效果。 PCA 通常用来 Data Compression 以加快算法，减少内存使用或磁盘占用，或者用于可视化(k=2, 3)。 Reference article 百面机器学习 Stanford Machine Learning 网易云课堂","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"PCA","slug":"PCA","permalink":"http://www.iequa.com/tags/PCA/"}]},{"title":"台湾大陆成长经历对比","slug":"world/taiwanese_accent_nice","date":"2019-01-12T00:26:48.000Z","updated":"2021-06-22T06:07:42.545Z","comments":true,"path":"2019/01/12/world/taiwanese_accent_nice/","link":"","permalink":"http://www.iequa.com/2019/01/12/world/taiwanese_accent_nice/","excerpt":"台湾 vs 大陆","text":"台湾 vs 大陆 介紹 台灣和大陸 90左右時代的人成長經歷 以及 台灣腔和大陸腔的不同 成长经历 (小学篇) 计划生育(一胎制) vs 无出生限制 好好念书/要听话/考名校 vs 好好讀書/要乖/做家務 推广普通话(纠正) vs 国语/闽南语(不特意纠正) 拼音/五笔输入法 vs 注音/仓颉输入法 儿歌/民歌/红歌 vs 两只老虎/茉莉花/妈妈的眼睛 美术课/语文课/品德课 vs 美劳课/国语/道德课/乡土课 数学语言作业 vs 读书心得/日记活动(拍照) 班长/学习委员/体育委员/纪律委员/严格红领巾 vs 班長/體育股長/風紀股長/無紅領巾 大陸校服 很逗超丑 vs 台灣校服 稍微好一點 (現在說穿校服是檢驗顏值最好的辦法) 轉學校 (借读费、迁户籍) vs 轉學 （戶籍處登錄即可，無借讀費） 不用服兵役 vs 需要服兵役 台湾腔 然後咧 (nango le) 喔是喔 (oh s~o) 真的假的 (zhenn jia de) 女儿、儿子、耳朵 (无儿话儿，没有卷舌) 你知道吗? (ni zao ma) 不好意思 (bao is) 這樣 (醬) 那樣 (釀 nian) 大家試試看 (Daa s~~ kan no) 很多地方粘在一起 老師我要上廁所 （老師和上廁所你能聽出來，其他弱化連讀） 啊你現在是想怎樣？ 這樣(醬)會不會被罵啊？ eng en 分不清楚 學生 f 和 h 分不清楚 你的頭髮ha怎麼那麼多，你在玩等一下叫你是罰(ha)站哦 这水(sui)好深(sheng)喔 是(si)不是(si) 阿你吃(ci)飽沒啊？ 問問題前加一個（啊） 乳液ye4, vs 乳液yi4 期待 qi1 vs 期待 qi2 垃圾 vs 垃圾 le4se4 悬崖ya2 vs 悬崖ai2 包括kuo4 vs 包括kua4 阿a1姨 vs 阿a3/a1姨 认识shi(轻声) vs 认识shi4 企qi3业 vs 企qi4业 相xiang1亲 vs 相xiang4亲 亚ya4洲 vs 亚ya3/ya4洲 俄e2罗斯 vs 俄e4罗斯 法fa3国 vs 法fa4国 成绩ji4 vs 成绩ji1 质zhi4量 vs 质zhi2量 危wei1险 vs 危wei2险 攻击ji1 vs 攻击ji2 我 和 han 你 Reference 超強分析「台灣腔」特徵 網友：居然全命中！ 教你如何快速學會台灣腔～ 大陆普通话发音VS.台湾的国语发音 大陆中文用词VS.台湾中文用词 于文文【體面】 - 蔡佩軒 Ariel Tsai 翻唱 旺福【糙級給力】Cover - 蔡佩軒 Ariel Tsai","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"Taiwan","slug":"Taiwan","permalink":"http://www.iequa.com/tags/Taiwan/"}]},{"title":"FastText 用于高效文本分类的技巧","slug":"nlp/fastText","date":"2018-12-18T23:00:21.000Z","updated":"2021-06-22T06:48:39.560Z","comments":true,"path":"2018/12/19/nlp/fastText/","link":"","permalink":"http://www.iequa.com/2018/12/19/nlp/fastText/","excerpt":"fasttext word2vec","text":"fasttext word2vec fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。 fastText 是 智慧与美貌并重的 文本分类 and 向量化工具 的项目，它是有两部分组成的。 论文1链接： Bag of Tricks for Efficient Text Classification 论文2链接： Enriching Word Vectors with Subword Information github链接： facebookresearch/fastText fastText 能够做到效果好，速度快，主要依靠两个秘密武器： 利用了词内的n-gram信息(subword n-gram information) 用到了层次化Softmax回归(Hierarchical Softmax) 的训练 trick. fastText 背景 英语单词通常有其内部结构和形成方式。例如我们可以从“dog”、“dogs”和“dogcatcher”的字面上推测他们的关系。这些词都有同一个词根“dog”，这个关联可以推广至其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。很多词根据场景不同有多种不同的形态。构词学（morphology）作为语言学的一个重要分支，研究的正是词的内部结构和形成方式。 在 word2vec 中，我们并没有直接利用构词学中的信息。无论是在 skip-gram 还是 CBOW 中，我们将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。有鉴于此，fastText 提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入 word2vec 中的 skip-gram。 子词嵌入 subword embedding 在 fastText 中，每个中心词被表示成子词的集合。下面我们用单词“where”作为例子来了解子词是如何产生的。首先，我们在单词的首尾分别添加特殊字符“&lt;”和“&gt;”以区分作为前后缀的子词。然后，将单词当成一个由字符构成的序列来提取 nnn 元语法。例如当 n=3n=3n=3 时，我们得到所有长度为 3 的子词： &lt;.wh，whe，her，ere，re&gt;&lt;.wh ， whe ， her ， ere ， re&gt; &lt;.wh，whe，her，ere，re&gt; 以及特殊子词 “&lt;.where&gt;”。 在 fastText 中，对于一个词 www，将它所有长度在 3 到 6 的子词和特殊子词的并集记为 G_w\\mathcal{G}\\_wG_w。那么词典则是所有词的子词集合的并集。假设词典中子词 ggg 的向量为 z_g\\boldsymbol{z}\\_gz_g，那么跳字模型中词 www 的作为中心词的向量 v_w\\boldsymbol{v}\\_wv_w 则表示成 v_w=∑_g∈G_wz_g.\\boldsymbol{v}\\_w = \\sum\\_{g\\in\\mathcal{G}\\_w} \\boldsymbol{z}\\_g. v_w=∑_g∈G_wz_g. FastText 的其余部分同 skip-gram 一致，不在此重复。可以看到，同 skip-gram 相比，fastText 中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。但与此同时，较生僻的复杂单词，甚至是词典中没有的单词，可能会从同它结构类似的其他词那里获取更好的词向量表示。 1. 前置知识 1.1 Softmax Regression Softmax Regression (回归) 又被称作多项LR（multinomial logistic regression），它是LR在多类别任务上的推广。 &lt;img src=&quot;/images/nlp/fastText2.jpg&quot; width=“850” /img&gt; 1.2 Hierarchical Softmax 1.3 n-gram’s feature 在文本特征提取中，常常能看到 n-gram 的身影。它是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为 N 的滑动窗口操作，最终形成长度为 N 的字节片段序列。看下面的例子： 字粒度 我来到达观数据参观 相应的bigram特征为：我来 来到 到达 达观 观数 数据 据参 参观 相应的trigram特征为：我来到 来到达 到达观 达观数 观数据 数据参 据参观 词粒度 我 来到 达观数据 参观 相应的bigram特征为：我/来到 来到/达观数据 达观数据/参观 相应的trigram特征为：我/来到/达观数据 来到/达观数据/参观 小结： n-gram中的gram根据粒度不同。它可以是字粒度，也可以是词粒度的。 n-gram 产生的特征只是作为文本特征的候选集，你后面可能会采用信息熵、卡方统计、IDF等文本特征选择方式筛选出比较重要特征。 2. word2vec 架构原理 2.1 CBOW 模型架构 2.2 前向传播 2.3 反向传播 3. fastText 核心思想 3.1 字符级 n-gram 3.2 模型架构 3.3 核心思想 仔细观察模型的后半部分，即从隐含层输出到输出层输出，会发现它就是一个softmax线性多类别分类器，分类器的输入是一个用来表征当前文档的向量；模型的前半部分，即从输入层输入到隐含层输出部分，主要在做一件事情：生成用来表征文档的向量。那么它是如何做的呢？叠加构成这篇文档的所有词及n-gram的词向量，然后取平均。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。 于是fastText的核心思想就是：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。 3.4 分类效果 还有个问题，就是为何fastText的分类效果常常不输于传统的非线性分类器？ 假设我们有两段文本： 我 来到 达观数据 俺 去了 达而观信息科技 这两段文本意思几乎一模一样，如果要分类，肯定要分到同一个类中去。但在传统的分类器中，用来表征这两段文本的向量可能差距非常大。传统的文本分类中，你需要计算出每个词的权重，比如tfidf值， “我”和“俺” 算出的tfidf值相差可能会比较大，其它词类似，于是，VSM（向量空间模型）中用来表征这两段文本的文本向量差别可能比较大。 但是fastText就不一样了，它是用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度，于是，在fastText模型中，这两段文本的向量应该是非常相似的，于是，它们很大概率会被分到同一个类中。 fastText效果好的原因： 使用词embedding而非词本身作为特征 字符级n-gram特征的引入对分类效果会有一些提升 4. fastText keras 实战 6. 小结 FastText 提出了子词嵌入方法。在 word2vec skip-gram 基础上，将中心词向量表示成单词的子词向量之和。 子词嵌入（subword embedding）利用构词上的规律，通常可以提升生僻词表示的质量。 fastText 训练时复杂度 采用层次化 softmax 之后，减少为 O(hlogK) 级别, 预测时还是 O(Kh) Reference 子词嵌入（fastText） fastText，智慧与美貌并重的文本分类及向量化工具 NLP︱高级词向量表达（二）——FastText（简述、学习笔记） 如何在python 非常简单训练FastText 我爱自然语言处理-fastText原理及实践 FastText文本分类算法学习笔记（好文） FastText的内部机制 利用skift实现fasttext模型 CSDN 层次softmax","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"fastText","slug":"fastText","permalink":"http://www.iequa.com/tags/fastText/"}]},{"title":"K-means","slug":"ml/K-Means","date":"2018-12-18T02:01:21.000Z","updated":"2021-06-22T06:46:36.125Z","comments":true,"path":"2018/12/18/ml/K-Means/","link":"","permalink":"http://www.iequa.com/2018/12/18/ml/K-Means/","excerpt":"","text":"在数据挖掘中， k-Means 算法是一种 cluster analysis 的算法，其主要是来计算数据聚集的算法，主要通过不断地取离种子点最近均值的算法。 1. 问题 K-Means算法主要解决的问题如下图所示。我们可以看到，在图的左边有一些点，我们用肉眼可以看出来有四个点群，但是我们怎么通过计算机程序找出这几个点群来呢？于是就出现了我们的K-Means算法. K均值聚类的基本思想是，通过迭代方式寻找K个簇(Cluster)的一种划分方案，使得聚类结果对应的代价函数最小。特别的，代价函数可以定义为各个样本距离所属簇中心点的误差平方和 2. 算法概要 这个算法其实很简单，如下图所示： 从上图中，我们可以看到，A, B, C, D, E 是五个在图中点。而灰色的点是我们的种子点，也就是我们用来找点群的点。有两个种子点，所以K=2。 然后，K-Means的算法如下： 1). 随机在图中取K（这里K=2）个种子点。 2). 然后对图中的所有点求到这K个种子点的距离，假如点Pi离种子点Si最近，那么Pi属于Si点群。（上图中，我们可以看到A,B属于上面的种子点，C,D,E属于下面中部的种子点）. 3). 接下来，我们要移动种子点到属于他的“点群”的中心。（见图上的第三步）. 4). 然后重复 第2）和 第3）步，直到，种子点没有移动（我们可以看到图中的第四步上面的种子点聚合了A,B,C，下面的种子点 聚合了D，E）。 这个算法很简单。 3. K-Means 具体步骤 (1). 数据预处理、归一化、离群点处理。 (2). 随机选择 K 个簇中心，记为 μ_1(0),μ_2(0),μ_3(0),...,μ_k(0){\\mu\\_1}^{(0)}, {\\mu\\_2}^{(0)}, {\\mu\\_3}^{(0)}, ..., {\\mu\\_k}^{(0)}μ_1(0),μ_2(0),μ_3(0),...,μ_k(0) (3). 定义代价函数 : (4). 令 t=0,1,2,… 为迭代轮数，重复下面的过程知道 J 收敛: 对于每一个样本 x_ix\\_ix_i, 将其分配到距离最近的簇. 对于每一个类簇 kkk, 重新计算该类簇的中心 4. K-Means 优缺点 Advantage： 对于大数据集，K均值 高效且可伸缩，它的复杂度是 O(NKt)O(NKt)O(NKt)，接近于线性。 其中 t是迭代轮数。 Disadvantage： (1) 需要人工预设K值，且该值和真实数据分布未必吻合； (2) 受初值和离群点的影响，每次的结果不稳定； (3) 受初值影响，结果通常是局部最优； (4) 无法很好地解决数据簇分布差别比较大的情况(如一类是另一类样本数量的100倍)； (5) 不太适用于离散分布；样本点只能被划分到单一的类中。 5. K-Means++ K-means 最开始是随机选取数据集中的K个点作为聚类中心. K-means++ 改进了初始值的选择，会尽量使聚类中心越远越好. 6. 扩展 ISODATA算法(迭代自组织数据分析法) 高斯混合模型、EM 自组织映射神经网络 (SOM) 聚类算法的评估 Reference article 百面机器学习 、 网易云课堂 COOLSHELL K-MEANS 算法 K NEAREST NEIGHBOR 单位矩阵和逆矩阵","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"Kmeans","slug":"Kmeans","permalink":"http://www.iequa.com/tags/Kmeans/"}]},{"title":"TextCNN 文本情感分类的卷积神经网络","slug":"nlp/textCNN","date":"2018-12-15T23:00:21.000Z","updated":"2021-06-22T06:48:39.568Z","comments":true,"path":"2018/12/16/nlp/textCNN/","link":"","permalink":"http://www.iequa.com/2018/12/16/nlp/textCNN/","excerpt":"","text":"textCNN 是 2014年 提出的用来做文本分类的卷积神经网络，结构简单、效果好. 论文链接： Convolutional Neural Networks for Sentence Classification 在文本分类等 NLP 领域应用广泛. 一般结构： 降维 -&gt; conv -&gt; 最大池化 -&gt; 完全连接层 -&gt; softmax . 将文本当做是一维图像，从而可以用一维卷积神经网络来捕捉临近词之间的关联。 1. 一维卷积层 在介绍模型前我们先来解释一维卷积层的工作原理。和二维卷积层一样，一维卷积层使用一维的互相关运算。在一维互相关运算中，卷积窗口从输入数组的最左方开始，按从左往右的顺序，依次在输入数组上滑动。 当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。 1.1 多输入通道 * 一维互相关 如下图所示，输入是一个宽为 7 的一维数组，核数组的宽为 2。可以看到输出的宽度为 7−2+1=6，且第一个元素是由输入的最左边的宽为 2 的子数组与核数组按元素相乘后再相加得到的。 &lt;img src=&quot;/images/nlp/conv1d.svg&quot; width=“650” /img&gt; 一维互相关运算。阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：0×1+1×2=2。 1.2 多输入通道 * 一维互相关 多输入通道的一维互相关运算也与多输入通道的二维互相关运算类似：在每个通道上，将核与相应的输入做一维互相关运算，并将通道之间的结果相加得到输出结果。下图展示了含 3 个输入通道的一维互相关运算。 &lt;img src=&quot;/images/nlp/conv1d-channel.svg&quot; width=“650” /img&gt; 含3个输入通道的一维互相关运算。阴影部分为第一个输出元素及其计算所使用的输入和核数组元素： 0×1+1×2+1×3+2×4+2×(−1)+3×(−3)=20×1+1×2+1×3+2×4+2×(−1)+3×(−3)=2 0×1+1×2+1×3+2×4+2×(−1)+3×(−3)=2 1.3 单输入通道 * 二维互相关 &lt;img src=&quot;/images/nlp/conv1d-2d.svg&quot; width=“650” /img&gt; 2×(−1)+3×(−3)+1×3+2×4+0×1+1×2=22×(−1)+3×(−3)+1×3+2×4+0×1+1×2=2 2×(−1)+3×(−3)+1×3+2×4+0×1+1×2=2 结论 ： 多输入通道 一维互相关 &lt;=&gt; 单输入通道 二维互相关 1.4 多输入通道 * 二维互相关 &lt;img src=&quot;/images/nlp/conv_multi_in.svg&quot; width=“650” /img&gt; (1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56(1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56 (1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56 1.5 多输出通道 1×1 卷积层 当输入通道有多个时，由于我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为 1 &lt;img src=&quot;/images/nlp/conv_1x1.svg&quot; width=“650” /img&gt; 1.6 小结 使用多通道可以拓展卷积层的模型参数. 假设将通道维当做是特征维，将高和宽维度上的元素当成数据样本，那么 1×1 卷积层的作用与全连接层等价. 1×1 卷积层通常用来调整网络层之间的通道数，并控制模型复杂度. 2. 时序最大池化层 类似地，我们有一维池化层。TextCNN 中使用的时序最大池化层（max-over-time pooling）实际上对应一维全局最大池化层：假设输入包含多个通道，各通道由不同时间步上的数值组成，各通道的输出即该通道所有时间步中最大的数值。因此，时序最大池化层的输入在各个通道上的时间步数可以不同。 为提升计算性能，我们常常将不同长度的时序样本组成一个小Batch，并通过在较短序列后附加**特殊字符（例如 0）**令批量中各时序样本长度相同。这些人为添加的特殊字符当然是无意义的。由于时序最大池化的主要目的是抓取时序中最重要的特征，它通常能使模型不受人为添加字符的影响。 3. TextCNN 模型 TextCNN 主要使用了一维卷积层和时序最大池化层。假设输入的文本序列由 n 个词组成，每个词用 d 维的词向量表示。那么输入样本的宽为 n，高为 1，输入通道数为 d。 textCNN 的计算主要分为以下几步： (1). 卷积层： 定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。 (2). 池化层： 对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。 (3). 全连层： 通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合。 &lt;img src=&quot;/images/nlp/conv_textcnn.svg&quot; width=“700” /img&gt; 用一个例子解释了 textCNN 的设计。这里的输入是一个有 11 个词的句子，每个词用 6 维词向量表示。因此输入序列的宽为 11，输入通道数为 6。给定 2 个一维卷积核，核宽分别为 2 和 4，输出通道数分别设为 4 和 5。因此，一维卷积计算后，4 个输出通道的宽为 11−2+1=10，而其他 5 个通道的宽为 11−4+1=8。尽管每个通道的宽不同，我们依然可以对各个通道做时序最大池化，并将 9 个通道的池化输出连结成一个 9 维向量。最终，我们使用全连接将 9 维向量变换为 2 维输出：正面情感和负面情感的预测。 4. CNN 的特点 CNN的三个优点： sparse interaction(稀疏的交互) parameter sharing(参数共享) equivalent respresentation(等价表示)。 经典的简化卷积公式表示如下： &lt;img src=&quot;/images/nlp/textcnn-1.webp&quot; width=“700” /img&gt; 假设每个词用三维向量表示，左边是4个词，右边是卷积矩阵，那么得到输出为： &lt;img src=&quot;/images/nlp/textcnn-2.webp&quot; width=“700” /img&gt; 如果基于这个结果做1-MaxPool池化(最大值池化)，那么就取卷积层结果 o 中的最大值，即提取最显著的特征。 针对海量的文本多分类数据，也可以尝试一下浅层的深度学习模型FastText模型，该模型的分类效率更高。 &lt;img src=&quot;/images/nlp/textcnn-3.webp&quot; width=“700” /img&gt; 整个模型由四部分构成： 输入层、卷积层、池化层、全连接层。 5. 小结 我们可以使用一维卷积来处理和分析时序数据。 多输入通道的一维互相关运算可以看作是单输入通道的二维互相关运算。 时序最大池化层的输入在各个通道上的时间步数可以不同。 TextCNN 主要使用了一维卷积层和时序最大池化层。 Reference 文本情感分类：使用卷积神经网络（textCNN） 我爱自然语言处理 吾爱NLP(4)—基于Text-CNN模型的中文文本分类实战 fastText、TextCNN、TextRNN……这里有一套NLP文本分类深度学习方法库供你选择 大规模文本分类网络TextCNN介绍","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"TextCNN","slug":"TextCNN","permalink":"http://www.iequa.com/tags/TextCNN/"}]},{"title":"Chatbot Research 12 - 理论篇： 评价指标介绍","slug":"nlp/chatbot/chatbot-research12","date":"2018-12-01T14:00:21.000Z","updated":"2021-06-20T04:12:28.339Z","comments":true,"path":"2018/12/01/nlp/chatbot/chatbot-research12/","link":"","permalink":"http://www.iequa.com/2018/12/01/nlp/chatbot/chatbot-research12/","excerpt":"对话系统之所以没有取得突破性的进展，很大程度是因为没有一个可以准确表示回答效果好坏的评价标准。对话系统中大都使用机器翻译、摘要生成领域提出来的评价指标，但是很明显对话系统的场景和需求与他们是存在差别的.","text":"对话系统之所以没有取得突破性的进展，很大程度是因为没有一个可以准确表示回答效果好坏的评价标准。对话系统中大都使用机器翻译、摘要生成领域提出来的评价指标，但是很明显对话系统的场景和需求与他们是存在差别的. 1. 评价指标*概览 对于某一轮对话而言: 可使用响应的适当性、流畅度、相关性； 对于多轮对话而言: 关注流畅性、对话深度、多样性、一致连贯性等指标 对于整个对话系统: 我们则希望他可以涵盖更多的话题、回复真实可信等等。 这些都是我们想要对话系统所拥有的能力，但是往往在一个具体的任务中我们只能关注某一项或者几项指标，这里我们主要针对开放域生成式对话模型的评价指标进行总结。 2. 词重叠评价指标 首先来看词重叠评价指标，他们认为有效地回答应该和真实回答之间存在大量的词重叠 （但是对话系统的答案空间往往是发散的，也就是一个问题的答案可能是完全不同的两句话，这种情况下该评价指标效果不好），也就是说这是一个非常强的假设。（以下环节中r表示真是响应，r^表示系统生成响应） 3. BLEU 该评价指标有IBM在2002年提出，参考论文“BLEU: a Method for Automatic Evaluation of Machine Translation”，常作为机器翻译系统评价指标。其实就是统计生成响应和真实响应中的n-gram词组在整个训练语料中出现次数。公式如下所示： &lt;img src=&quot;/images/chatbot/chatbot-12.1.jpg&quot; width=“400” /img&gt; ROUGE : 该指标常用于文本摘要领域 METEOR: BLEU 的升级版 4. 词向量评价指标 上面的词重叠评价指标基本上都是n-gram方式，去计算生成响应和真实响应之间的重合程度，共现程度等指标。而词向量则是通过Word2Vec等方法将句子转换为向量表示，这样一个句子就被映射到一个低维空间，句向量在一定程度上表征了其含义，在通过余弦相似度等方法就可以计算两个句子之间的相似程度。使用词向量的好处是，可以一定程度上增加答案的多样性，因为这里大多采用词语相似度进行表征，相比词重叠中要求出现完全相同的词语，限制降低了很多。 5. perplexity困惑度 perplexity是语言模型中的指标，用于评价语言模型的好坏，其实就是估算一句话出现的概率，看一句话是否通顺。也经常会在对话系统中出现评价生成的响应是否符合语言规则，计算方法也很简单，如下图所示： &lt;img src=&quot;/images/chatbot/chatbot-12.3.jpg&quot; width=“400” /img&gt; 所以当我们使用tf.contrib.seq2seq.sequence_loss()函数计算模型loss的时候，perplexity的计算就显得很简单了，直接对计算出来的loss取个指数就行了，命令如下所示： 1train_perp = math.exp(float(mean_loss)) if mean_loss &lt; 300 else math.inf 现在我训练的对话系统，一般都只是用了perplexity来评判模型的效果，最终perplexity可以降到20左右（越小越好，说明越接近于自然语言）。 6. 人工指标 最后说一下人工评价，首先来讲，上面说了这么多的评价指标，并没有一个可以很好的解决对话系统的问题，就像“How NOT To Evaluate Your Dialogue System”论文中说到的那样，当下的这些评价指标都跟人工评价成弱相关或者完全没有关系，相关程度跟具体的数据集有关。 以下摘自徐阿衡的回答： 在闲聊性质的数据集上，上述 metric 和人工判断有一定微弱的关联 (only a small positive correlation on chitchat oriented Twitter dataset) 在技术类的数据集上，上述 metric 和人工判断完全没有关联(no correlation at all on the technical UDC) 当局限于一个特别具体的领域时，BLEU 会有不错的表现 随着发展，还逐渐有了一些别的评价方法，比如使用GAN网络来评价生成的回复是否跟人类回复相似等等。。。 Reference","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"}]},{"title":"Tensorflow Sequence_loss","slug":"tensorflow/sequence_loss","date":"2018-12-01T05:36:21.000Z","updated":"2021-06-20T04:12:28.328Z","comments":true,"path":"2018/12/01/tensorflow/sequence_loss/","link":"","permalink":"http://www.iequa.com/2018/12/01/tensorflow/sequence_loss/","excerpt":"sequence_loss 是 nlp算法 中非常重要的一个函数. rnn,lstm,attention都要用到这个函数.看下面代码:","text":"sequence_loss 是 nlp算法 中非常重要的一个函数. rnn,lstm,attention都要用到这个函数.看下面代码: 1. 特殊的🌰 123456789101112131415161718192021222324# coding: utf-8import numpy as npimport tensorflow as tffrom tensorflow.contrib.seq2seq import sequence_losslogits_np = np.array([ [[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]], [[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]]])targets_np = np.array([ [0, 0, 0], [0, 0, 0]], dtype=np.int32)logits = tf.convert_to_tensor(logits_np)targets = tf.convert_to_tensor(targets_np)cost = sequence_loss(logits=logits, targets=targets, weights=tf.ones_like(targets, dtype=tf.float64))init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) r = sess.run(cost) print(r) 先对每个[0.5,0.5,0.5,0.5]取softmax. softmax([0.5,0.5,0.5,0.5])=(0.25,0.25,0.25,0.25)然后再计算-ln(0.25)*6/6=1.38629436112. 2. 一般的🌰 1234567891011121314151617181920212223242526272829303132333435# coding:utf-8from __future__ import unicode_literalsfrom __future__ import print_functionfrom __future__ import divisionfrom tensorflow.contrib.seq2seq import sequence_lossimport tensorflow as tfimport numpy as np# 2个句子，3个时刻，4个值(词汇表)output_np = np.array( [ [[0.6, 0.5, 0.3, 0.2], [0.9, 0.5, 0.3, 0.2], [1.0, 0.5, 0.3, 0.2]], [[0.2, 0.5, 0.3, 0.2], [0.3, 0.5, 0.3, 0.2], [0.4, 0.5, 0.3, 0.2]] ])print(output_np.shape)# 2个句子，target_np = np.array([[0, 1, 2], [3, 0, 1]], dtype=np.int32)print(target_np.shape)output = tf.convert_to_tensor(output_np, np.float32)target = tf.convert_to_tensor(target_np, np.int32)cost = sequence_loss(output, target, tf.ones_like(target, dtype=np.float32))init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) cost_r = sess.run(cost) print(cost_r) 这个代码作用和下面的tf.reduce_mean(softmax_cross_entropy_with_logits)作用一致. 123456789101112131415161718192021222324252627282930313233# coding:utf-8from __future__ import unicode_literalsfrom __future__ import print_functionfrom __future__ import divisionimport tensorflow as tfimport numpy as npdef to_onehot(a): max_index = np.max(a) b = np.zeros((a.shape[0], max_index + 1)) b[np.arange(a.shape[0]), a] = 1 return blogits_ph = tf.placeholder(tf.float32, shape=(None, None))labels_ph = tf.placeholder(tf.float32, shape=(None, None))output_np = np.array([ [0.6, 0.5, 0.3, 0.2], [0.9, 0.5, 0.3, 0.2], [1.0, 0.5, 0.3, 0.2], [0.2, 0.5, 0.3, 0.2], [0.3, 0.5, 0.3, 0.2], [0.4, 0.5, 0.3, 0.2]])cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_ph, logits=logits_ph))target_np = np.array([0, 1, 2, 3, 0, 1])init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) cost_r = sess.run(cost, feed_dict=&#123;logits_ph: output_np, labels_ph: to_onehot(target_np)&#125;) print(cost_r) 再取交叉熵,再取平均. seq2seq 的应用 chatbot 应用 seq2seq 需要用到 sequence_loss Reference","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"sequence_loss","slug":"sequence-loss","permalink":"http://www.iequa.com/tags/sequence-loss/"}]},{"title":"Chatbot Research 11 - Chatbot 的第二个版本 (新API实现)","slug":"nlp/chatbot/chatbot-research11","date":"2018-11-29T14:00:21.000Z","updated":"2021-06-22T06:47:48.428Z","comments":true,"path":"2018/11/29/nlp/chatbot/chatbot-research11/","link":"","permalink":"http://www.iequa.com/2018/11/29/nlp/chatbot/chatbot-research11/","excerpt":"我们使用 tf.contrib.legacy_seq2seq 下的 API 构建了一个简单的 chatbot 对话系统. 代码是1.0之前旧版. 本篇我们学习新版本灵活的的API，这里先来说一下二者的不同：","text":"我们使用 tf.contrib.legacy_seq2seq 下的 API 构建了一个简单的 chatbot 对话系统. 代码是1.0之前旧版. 本篇我们学习新版本灵活的的API，这里先来说一下二者的不同： 新版本API： 用 dynamic_rnn 来构造 RNN模型，这样就避免了数据长度不同所带来的困扰，不需要再使用 model_with_buckets 这种方法来构建模型，使得我们数据处理和模型代码都简洁很多。 新版本将 Attention、 Decoder 等几个主要的功能都分别进行封装，直接调用相应的 Wapper函数 进行封装即可，更加灵活方便，而且只需要写几个简单的函数既可以自定义的各个模块以满足我们个性化的需求。 实现了beam_search功能，可直接调用。 tensor flow dynamic_rnn 与rnn有啥区别？ dynamic_rnn 只在一个 batch 内部进行自动 padding， 不同 batch padding 长度可以不同 1. 数据处理 word2id is : { ‘decorations’: 12002, ‘scraps’: 4599, …} id2word is : { 0: ‘’, 1: ‘’, 2: ‘’, 3: ‘’, 4: ‘can’, 5: ‘we’, 6: ‘make’, … } trainingSamples is : [ [ [793, 138, 65], [35, 209, 110, 9016, 208, 382, 35, 22] ], [ [35, 209, 110, 9016, 208, 382, 35, 22], [26, 92, 1906, 47, 254, 65] ], … ] 2. 模型构建 代码主要是从 tensorflow官网 给出的nmt例子的代码简化而来，实现了最基本的 attention 和 beam_search 等功能，同时有将nmt代码中繁杂的代码逻辑进行简化。这里参考nmt中所提到的构建train、eval、inference , 三个图进行模型构建，好处在于 nmt官方文档 Building Training, Eval, and Inference Graphs inference图 往往与 train 和 eval结构 存在较大差异，所以往往需要单独进行构建 . （没有decoder输入和目标，需要使用 greedy 或 beam_search 进行 decode，batch_size 也不同等等） eval图，不需要进行反向传播，只需要得到一个 loss 和 acc值 数据进行 feed，简化数据操作 变量重用变得简单，因为 train、eval 存在一些公用变量和代码块，就不需要我们重复定义 只需要在 train 时不断保存模型参数，然后在 eval 和 infer 的时候 restore参数 即可 以上，所以我们构建了 train、eval、infer 三个函数来实现上面的功能。在看代码之前我们先来简单说一下新版API几个主要的模块以及相互之间的调用关系。tf.contrib.seq2seq文件夹下面主要有下面6个文件，除了loss文件和之前的sequence_loss函数没有很大区别，这里不介绍之外，其他几个文件都会简单的说一下，这里主要介绍函数和类的功能，源码会放在下篇文章中介绍。 decoder basic_decoder helper attention_wrapper beam_search_decoder loss 2.1 BasicDecoder 类和 dynamic_decode decoder文件中定义了 Decoder抽象类 dynamic_decode函数 dynamic_decode 可以视为整个解码过程的入口，需要传入的参数就是 Decoder 的一个实例，他会动态的调用 Decoder 的 step函数 按步执行 decode，可以理解为Decoder类定义了单步解码（根据输入求出输出，并将该输出当做下一时刻输入），而dynamic_decode则会调用control_flow_ops.while_loop这个函数来循环执行直到输出结束编码过程. 2.2 cell类型（Attention类型） 12345678910111213# 分为3步，#. 1. 定义attention机制#. 2. 定义要是用的基础的RNNCell#. 3. 使用AttentionWrapper进行封装# 1. 定义要使用的attention机制。attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=self.rnn_size, memory=encoder_outputs, memory_sequence_length=encoder_inputs_length)# 2. 定义decoder阶段要是用的LSTMCell，然后为其封装attention wrapperdecoder_cell = self._create_rnn_cell()# 3. 使用AttentionWrapper进行封装decoder_cell = tf.contrib.seq2seq.AttentionWrapper(cell=decoder_cell, attention_mechanism=attention_mechanism, attention_layer_size=self.rnn_size, name=&#x27;Attention_Wrapper&#x27;) 2.3 helper类型 helper其实就是decode阶段如何根据预测结果得到下一时刻的输入，比如train训练过程中应该直接使用上一时刻的真实值作为下一时刻输入，预测过程中可以使用贪婪的方法选择概率最大的那个值作为下一时刻等等。 Reference Tensorflow新版Seq2Seq接口使用 tensorflow官网API指导 DeepQA Neural_Conversation_Models","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"}]},{"title":"工程师应该如何注意身体健康？","slug":"tools/body-health","date":"2018-11-26T09:26:48.000Z","updated":"2021-06-22T06:52:37.110Z","comments":true,"path":"2018/11/26/tools/body-health/","link":"","permalink":"http://www.iequa.com/2018/11/26/tools/body-health/","excerpt":"IT 工程师群体是职业病高发人群（不说了，都是泪…）","text":"IT 工程师群体是职业病高发人群（不说了，都是泪…） 程序员常见的职业病 颈椎病 腰椎病 久坐对前列腺的危害以及肥胖问题 眼疲劳、用眼过度 饮食、作息不规律导致的胃病等一系列问题 全文目录： 颈椎、腰椎病防治、久坐对前列腺、肛门的危害以及肥胖问题 换一把人体工学椅，附不同价位品牌推荐 站立式办公 人体工学椅和站立式办公，应该选择哪个？ 用眼过度、眼疲劳 护眼宝 软件 Gunnar 防蓝光眼镜 f.lux 软件 Reference 程序员应该如何注意身体健康？可能患哪些职业病？如何防治？","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"body","slug":"body","permalink":"http://www.iequa.com/tags/body/"}]},{"title":"Chatbot Research 8 - 理论 seq2seq+Attention 机制模型详解","slug":"nlp/chatbot/chatbot-research8","date":"2018-11-17T14:00:21.000Z","updated":"2021-06-20T04:12:28.338Z","comments":true,"path":"2018/11/17/nlp/chatbot/chatbot-research8/","link":"","permalink":"http://www.iequa.com/2018/11/17/nlp/chatbot/chatbot-research8/","excerpt":"从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。","text":"从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。 Seq-to-Seq 框架1 Seq-to-Seq 框架2（teacher forcing） Seq-to-Seq with Attention（NMT） Seq-to-Seq with Attention 各种变形 Seq-to-Seq with Beam-Search 当输入输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder）1 或者 seq2seq 模型 2。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。 1. Seq2Seq 框架1 Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation 1.1 encode 编码器 编码器的作用是把一个不定长的输入序列变换成一个定长的背景变量 ccc，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。 让我们考虑批量大小为 1 的时序数据样本。假设输入序列是 x_1,…,x_Tx\\_1,\\ldots,x\\_Tx_1,…,x_T, 例如 x_ix\\_ix_i 是输入句子中的第 iii 个词。在时间步 ttt，循环神经网络将输入 x_tx\\_tx_t 的特征向量 x_tx\\_tx_t 和上个时间步的隐藏状态 h_t−1\\boldsymbol{h}\\_{t-1}h_t−1 变换为当前时间步的隐藏状态 h_th\\_th_t。我们可以用函数 fff 表达循环神经网络隐藏层的变换： h_t=f(x_t,h_t−1).\\boldsymbol{h}\\_t = f(\\boldsymbol{x}\\_t, \\boldsymbol{h}\\_{t-1}). h_t=f(x_t,h_t−1). 接下来编码器通过自定义函数 qqq 将各个时间步的隐藏状态变换为背景变量 c=q(h_1,…,h_T).\\boldsymbol{c} = q(\\boldsymbol{h}\\_1, \\ldots, \\boldsymbol{h}\\_T). c=q(h_1,…,h_T). 例如，当选择 q(h_1,…,h_T)=h_Tq(\\boldsymbol{h}\\_1, \\ldots, \\boldsymbol{h}\\_T) = \\boldsymbol{h}\\_Tq(h_1,…,h_T)=h_T 时，背景变量是输入序列最终时间步的隐藏状态 h_T\\boldsymbol{h}\\_Th_T。 以上描述的编码器是一个单向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用 Bi-RNN 构造编码器。这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。 1.2 decode 解码器 Encode 编码器输出的背景变量 ccc 编码了整个输入序列 x_1,…,x_Tx\\_1, \\ldots, x\\_Tx_1,…,x_T 的信息。给定训练样本中的输出序列 y_1,y_2,…,y_T′y\\_1, y\\_2, \\ldots, y\\_{T&#x27;}y_1,y_2,…,y_T′，对每个时间步 t′t&#x27;t′（符号与输入序列或编码器的时间步 ttt 有区别）， 解码器输出 y_t′y\\_{t&#x27;}y_t′ 的条件概率将基于之前的输出序列 y_1,…,y_t′−1y\\_1,\\ldots,y\\_{t&#x27;-1}y_1,…,y_t′−1 和背景变量 ccc，即 P(y_t′∣y_1,…,y_t′−1,c)\\mathbb{P}(y\\_{t&#x27;} \\mid y\\_1, \\ldots, y\\_{t&#x27;-1}, \\boldsymbol{c})P(y_t′∣y_1,…,y_t′−1,c)。 为此，我们可以使用另一个RNN作为解码器。 在输出序列的时间步 t′t^\\primet′，解码器将上一时间步的输出 y_t′−1y\\_{t^\\prime-1}y_t′−1 以及背景变量 ccc 作为输入，并将它们与上一时间步的隐藏状态 h_t′−1\\boldsymbol{h}\\_{t^\\prime-1}h_t′−1 变换为当前时间步的隐藏状态 h_t′\\boldsymbol{h}\\_{t^\\prime}h_t′。因此，我们可以用函数 ggg 表达解码器隐藏层的变换： h_t′=g(y_t′−1,c,h_t′−1).\\boldsymbol{h}\\_{t^\\prime} = g(y\\_{t^\\prime-1}, \\boldsymbol{c}, \\boldsymbol{h}\\_{t^\\prime-1}). h_t′=g(y_t′−1,c,h_t′−1). 有了decode的隐藏状态后，我们可以使用自定义的输出层和 softmax 运算来计算 P(y_t′∣y_1,…,y_t′−1,c)\\mathbb{P}(y\\_{t^\\prime} \\mid y\\_1, \\ldots, y\\_{t^\\prime-1}, \\boldsymbol{c})P(y_t′∣y_1,…,y_t′−1,c)，例如基于当前时间步的解码器隐藏状态 h_t′\\boldsymbol{h}\\_{t^\\prime}h_t′、上一时间步的输出 y_t′−1y\\_{t^\\prime-1}y_t′−1 以及背景变量 ccc 来计算当前时间步输出 y_t′y\\_{t^\\prime}y_t′ 的概率分布。 1.3 train 模型训练 根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率 \\begin{split}\\begin{aligned} \\mathbb{P}(y\\_1, \\ldots, y\\_{T&#039;} \\mid x\\_1, \\ldots, x\\_T) &amp;= \\prod\\_{t&#039;=1}^{T&#039;} \\mathbb{P}(y\\_{t&#039;} \\mid y\\_1, \\ldots, y\\_{t&#039;-1}, x\\_1, \\ldots, x\\_T)\\\\\\\\ &amp;= \\prod\\_{t&#039;=1}^{T&#039;} \\mathbb{P}(y\\_{t&#039;} \\mid y\\_1, \\ldots, y\\_{t&#039;-1}, \\boldsymbol{c}), \\end{aligned}\\end{split} 并得到该输出序列的损失 −log⁡P(y_1,…,y_T′∣x_1,…,x_T)=−∑_t′=1T′log⁡P(y_t′∣y_1,…,y_t′−1,c), - \\log\\mathbb{P}(y\\_1, \\ldots, y\\_{T&#x27;} \\mid x\\_1, \\ldots, x\\_T) = -\\sum\\_{t&#x27;=1}^{T&#x27;} \\log \\mathbb{P}(y\\_{t&#x27;} \\mid y\\_1, \\ldots, y\\_{t&#x27;-1}, \\boldsymbol{c}), −logP(y_1,…,y_T′∣x_1,…,x_T)=−∑_t′=1T′logP(y_t′∣y_1,…,y_t′−1,c), 在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图中所描述的模型预测中，我们需要将decode在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列在上一个时间步的标签作为decode在当前时间步的输入。这叫做强制教学（teacher forcing）。 1.4 小结 编码器 - 解码器（seq2seq）可以输入并输出不定长的序列。 编码器—解码器使用了两个循环神经网络。 在编码器—解码器的训练中，我们可以采用强制教学。 （这也是 Seq2Seq 2 的内容） 2. Seq2Seq 框架2 第二个要讲的Seq-to-Seq模型来自于 “Sequence to Sequence Learning with Neural Networks”，其模型结构图如下所示： 与上面模型最大的区别在于其source编码后的 向量CCC 直接作为Decoder阶段RNN的初始化state，而不是在每次decode时都作为RNN cell的输入。此外，decode时RNN的输入是目标值，而不是前一时刻的输出。首先看一下编码阶段： 就是简单的RNN模型，每个词经过RNN之后都会编码为hidden state（e0,e1,e2），并且source序列的编码向量e就是最终的hidden state e2。接下来再看一下解码阶段： e向量仅作为RNN的初始化状态传入decode模型。接下来就是标准的循环神经网络，每一时刻输入都是前一时刻的正确label。直到最终输入符号截止滚动。 3. Seq2Seq Attention decode 在各个时间步依赖相同的 背景变量 ccc 来获取输入序列信息。当 encode 为 RNN 时，背景变量ccc 来自它最终时间步的隐藏状态。 英语输入：“They”、“are”、“watching”、“.” 法语输出：“Ils”、“regardent”、“.” 翻译例子：输入为英语序列“They”、“are”、“watching”、“.”，输出为法语序列“Ils”、“regardent”、“.”。，decode 在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步 1，解码器可以主要依赖“They”、“are”的信息来生成“Ils”，在时间步 2 则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步 3 则直接映射句号“.”。这看上去就像是在 decode 的每一时间步对输入序列中不同时间步的编码信息分配不同的注意力一样。这也是注意力机制的由来 1。 仍以 RNN 为例，Attention 通过对 Encode 所有时间步的隐藏状态做加权平均来得到背景变量ccc。Decode 在每一时间步调整这些权重，即 Attention weight，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量ccc。本节我们将讨论 Attention机制 是怎么工作的。 在“编码器—解码器（seq2seq）”, 解码器在时间步 t′t&#x27;t′ 的隐藏状态 s_t′=g(y_t′−1,c,s_t′−1)\\boldsymbol{s}\\_{t&#x27;} = g(\\boldsymbol{y}\\_{t&#x27;-1}, \\boldsymbol{c}, \\boldsymbol{s}\\_{t&#x27;-1}) s_t′=g(y_t′−1,c,s_t′−1) 在 Attention机制 中, 解码器的每一时间步将使用可变的背景变量ccc s_t′=g(y_t′−1,c_t′,s_t′−1).\\boldsymbol{s}\\_{t&#x27;} = g(\\boldsymbol{y}\\_{t&#x27;-1}, \\boldsymbol{c}\\_{t&#x27;}, \\boldsymbol{s}\\_{t&#x27;-1}). s_t′=g(y_t′−1,c_t′,s_t′−1). 关键是如何计算背景变量 c_t′\\boldsymbol{c}\\_{t&#x27;}c_t′ 和如何利用它来更新隐藏状态 s_t′\\boldsymbol{s}\\_{t&#x27;}s_t′。以下将分别描述这两个关键点。 3.1 计算背景变量 c c_t′=∑_t=1Tα_t′th_t,\\boldsymbol{c}\\_{t&#x27;} = \\sum\\_{t=1}^T \\alpha\\_{t&#x27; t} \\boldsymbol{h}\\_t, c_t′=∑_t=1Tα_t′th_t, 其中给定 t′t&#x27;t′ 时，权重 α_t′t\\alpha\\_{t&#x27; t}α_t′t 在 t=1,…,Tt=1,\\ldots,Tt=1,…,T 的值是一个概率分布。为了得到概率分布，可以使用 softmax 运算: α_t′t=exp⁡(e_t′t)∑_k=1Texp⁡(e_t′k),t=1,…,T.\\alpha\\_{t&#x27; t} = \\frac{\\exp(e\\_{t&#x27; t})}{ \\sum\\_{k=1}^T \\exp(e\\_{t&#x27; k}) },\\quad t=1,\\ldots,T. α_t′t=∑_k=1Texp(e_t′k)exp(e_t′t)​,t=1,…,T. 现在，我们需要定义如何计算上式中 softmax 运算的输入 e_t′te\\_{t&#x27; t}e_t′t。由于 e_t′te\\_{t&#x27; t}e_t′t 同时取决于decode的时间步 t′t&#x27;t′ 和encode的时间步 ttt，我们不妨以解码器在时间步 t′−1t&#x27;−1t′−1 的隐藏状态 s_t′−1\\boldsymbol{s}\\_{t&#x27; - 1}s_t′−1 与编码器在时间步 ttt 的隐藏状态 h_th\\_th_t 为输入，并通过函数 aaa 计算 e_t′te\\_{t&#x27; t}e_t′t： e_t′t=a(s_t′−1,h_t).e\\_{t&#x27; t} = a(\\boldsymbol{s}\\_{t&#x27; - 1}, \\boldsymbol{h}\\_t). e_t′t=a(s_t′−1,h_t). 这里函数 a 有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积 a(s,h)=s⊤ha(\\boldsymbol{s}, \\boldsymbol{h})=\\boldsymbol{s}^\\top \\boldsymbol{h}a(s,h)=s⊤h。而最早提出Attention机制的论文则将输入连结后通过含单隐藏层的多层感知机MLP 变换 a(s,h)=v⊤tanh⁡(W_ss+W_hh),a(\\boldsymbol{s}, \\boldsymbol{h}) = \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}\\_s \\boldsymbol{s} + \\boldsymbol{W}\\_h \\boldsymbol{h}), a(s,h)=v⊤tanh(W_ss+W_hh), 其中 v、W_s、W_hv、W\\_s、W\\_hv、W_s、W_h 都是可以学习的模型参数。 3.2 更新隐藏状态 以门控循环单元为例，在解码器中我们可以对门控循环单元的设计稍作修改。解码器在时间步 t′t&#x27;t′ 的隐藏状态为 s_t′=z_t′⊙s_t′−1+(1−z_t′)⊙s~_t′,\\boldsymbol{s}\\_{t&#x27;} = \\boldsymbol{z}\\_{t&#x27;} \\odot \\boldsymbol{s}\\_{t&#x27;-1} + (1 - \\boldsymbol{z}\\_{t&#x27;}) \\odot \\tilde{\\boldsymbol{s}}\\_{t&#x27;}, s_t′=z_t′⊙s_t′−1+(1−z_t′)⊙s~_t′, 其中的重置门、更新门和候选隐含状态分别为 : \\begin{split}\\begin{aligned} \\boldsymbol{r}\\_{t&#039;} &amp;= \\sigma(\\boldsymbol{W}\\_{yr} \\boldsymbol{y}\\_{t&#039;-1} + \\boldsymbol{W}\\_{sr} \\boldsymbol{s}\\_{t&#039; - 1} + \\boldsymbol{W}\\_{cr} \\boldsymbol{c}\\_{t&#039;} + \\boldsymbol{b}\\_r),\\\\\\\\ \\boldsymbol{z}\\_{t&#039;} &amp;= \\sigma(\\boldsymbol{W}\\_{yz} \\boldsymbol{y}\\_{t&#039;-1} + \\boldsymbol{W}\\_{sz} \\boldsymbol{s}\\_{t&#039; - 1} + \\boldsymbol{W}\\_{cz} \\boldsymbol{c}\\_{t&#039;} + \\boldsymbol{b}\\_z),\\\\\\\\ \\tilde{\\boldsymbol{s}}\\_{t&#039;} &amp;= \\text{tanh}(\\boldsymbol{W}\\_{ys} \\boldsymbol{y}\\_{t&#039;-1} + \\boldsymbol{W}\\_{ss} (\\boldsymbol{s}\\_{t&#039; - 1} \\odot \\boldsymbol{r}\\_{t&#039;}) + \\boldsymbol{W}\\_{cs} \\boldsymbol{c}\\_{t&#039;} + \\boldsymbol{b}\\_s), \\end{aligned}\\end{split} 其中含下标的 W 和 b 分别为门控循环单元的权重参数和偏差参数。 3.3 小结 可以在decode的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。 Attention机制可以采用更为高效的矢量化计算。 4. Seq2Seq Attention各种变形 第四个Seq-to-Seq模型，来自于论文 Effective Approaches to Attention-based Neural Machine Translation 这篇论文提出了两种 Seq2Seq模型 分别是global Attention 和 local Attention。 5. Seq2Seq with Beam-Search 上面讲的几种Seq2Seq模型都是从模型结构上进行的改进，也就说为了从训练的层面上改善模型的效果，但这里要介绍的beam-search是在测试的时候才用到的技术。 Reference 动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制 门控循环单元（GRU） seq2seq+Attention机制模型详解 三分钟带你对 Softmax 划重点 Softmax 回归","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"},{"name":"Attention","slug":"Attention","permalink":"http://www.iequa.com/tags/Attention/"}]},{"title":"Glove 和 fastText","slug":"nlp/glove_and_fastText","date":"2018-11-15T23:00:21.000Z","updated":"2021-06-20T04:12:28.336Z","comments":true,"path":"2018/11/16/nlp/glove_and_fastText/","link":"","permalink":"http://www.iequa.com/2018/11/16/nlp/glove_and_fastText/","excerpt":"本节介绍两种更新一点的词向量。分别是2014年Stanford发表的Glove和2017年由Facebook发表的fastText.","text":"本节介绍两种更新一点的词向量。分别是2014年Stanford发表的Glove和2017年由Facebook发表的fastText. 让我们先回顾一下 word2vec 中的跳字模型。将跳字模型中使用 softmax 运算表达的条件概率 P(w_j∣w_i)\\mathbb{P}(w\\_j\\mid w\\_i)P(w_j∣w_i). 记作 q_ijq\\_{ij}q_ij，即 q_ij=exp⁡(u_j⊤v_i)∑_k∈Vexp(u_k⊤v_i),q\\_{ij}=\\frac{\\exp(\\mathbf{u}\\_j^\\top \\mathbf{v}\\_i)}{ \\sum\\_{k \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}\\_k^\\top \\mathbf{v}\\_i)}, q_ij=∑_k∈Vexp(u_k⊤v_i)exp(u_j⊤v_i)​, GloVe 模型 有鉴于此，作为在 word2vec 之后提出的词嵌入模型，GloVe 采用了平方损失，并基于该损失对跳字模型做了三点改动 从条件概率比值理解 GloVe 我们还可以从另外一个角度来理解 GloVe 词嵌入。沿用本节前面的符号，P(w_j∣w_i)\\mathbb{P}(w\\_j \\mid w\\_i)P(w_j∣w_i) 表示数据集中以 w_iw\\_iw_i 为中心词生成背景词 w_jw\\_jw_j 的条件概率，并记作 p_ijp\\_{ij}p_ij。作为源于某大型语料库的真实例子，以下列举了两组分别以“ice”（“冰”）和“steam”（“蒸汽”）为中心词的条件概率以及它们之间的比值 [1]： w_kw\\_kw_k= “solid” “gas” “water” “fashion” p_1=P(w_k∣&quot;ice&quot;)p\\_1=\\mathbb{P}(w\\_k\\mid\\text{&quot;ice&quot;})p_1=P(w_k∣&quot;ice&quot;) 0.00019 0.000066 0.003 0.000017 p_2=P(w_k∣&quot;steam&quot;)p\\_2=\\mathbb{P}(w\\_k\\mid\\text{&quot;steam&quot;})p_2=P(w_k∣&quot;steam&quot;) 0.000022 0.00078 0.0022 0.000018 p_1/p_2p\\_1/p\\_2p_1/p_2 8.9 0.085 1.36 0.96 我们可以观察到以下现象： 对于与“ice”相关而与“steam”不相关的词 w_kw\\_kw_k，例如 w_k=w\\_k=w_k=“solid”（“固体”），我们期望条件概率比值较大，例如上表最后一行中的值 8.9； 对于与“ice”不相关而与 steam 相关的词 w_kw\\_kw_k，例如 w_k=w\\_k=w_k=“gas”（“气体”），我们期望条件概率比值较小，例如上表最后一行中的值 0.085； 对于与“ice”和“steam”都相关的词 w_kw\\_kw_k，例如 w_k=w\\_k=w_k=“water”（“水”），我们期望条件概率比值接近 1，例如上表最后一行中的值 1.36； 对于与“ice”和“steam”都不相关的词 w_kw\\_kw_k，例如 w_k=w\\_k=w_k=“fashion”（“时尚”），我们期望条件概率比值接近 1，例如上表最后一行中的值 0.96。 由此可见，条件概率比值能比较直观地表达词与词之间的关系。我们可以构造一个词向量函数使得它能有效拟合条件概率比值。我们知道，任意一个这样的比值需要三个词 w_iw\\_iw_i、w_jw\\_jw_j 和 w_kw\\_kw_k。以 w_iw\\_iw_i 作为中心词的条件概率比值为 p_ij/p_ik{p\\_{ij}}/{p\\_{ik}}p_ij/p_ik。我们可以找一个函数，它使用词向量来拟合这个条件概率比值 f(u_j,u_k,v_i)≈p_ijp_ik.f(\\boldsymbol{u}\\_j, \\boldsymbol{u}\\_k, {\\boldsymbol{v}}\\_i) \\approx \\frac{p\\_{ij}}{p\\_{ik}}. f(u_j,u_k,v_i)≈p_ikp_ij​. 这里函数 fff 可能的设计并不唯一，我们只需考虑一种较为合理的可能性。注意到条件概率比值是一个标量，我们可以将 fff 限制为一个标量函数：f(u_j,u_k,v_i)=f((u_j−u_k)⊤v_i)f(\\boldsymbol{u}\\_j, \\boldsymbol{u}\\_k, {\\boldsymbol{v}}\\_i) = f\\left((\\boldsymbol{u}\\_j - \\boldsymbol{u}\\_k)^\\top {\\boldsymbol{v}}\\_i\\right)f(u_j,u_k,v_i)=f((u_j−u_k)⊤v_i)。交换索引 jjj 和 kkk 后可以看到函数 fff 应该满足 f(x)f(−x)=1f(x)f(-x)=1f(x)f(−x)=1，因此一个可能是 f(x)=exp⁡(x)f(x)=\\exp(x)f(x)=exp(x)，于是 f(u_j,u_k,v_i)=exp⁡(u_j⊤v_i)exp⁡(u_k⊤v_i)≈p_ijp_ik.f(\\boldsymbol{u}\\_j, \\boldsymbol{u}\\_k, {\\boldsymbol{v}}\\_i) = \\frac{\\exp\\left(\\boldsymbol{u}\\_j^\\top {\\boldsymbol{v}}\\_i\\right)}{\\exp\\left(\\boldsymbol{u}\\_k^\\top {\\boldsymbol{v}}\\_i\\right)} \\approx \\frac{p\\_{ij}}{p\\_{ik}}. f(u_j,u_k,v_i)=exp(u_k⊤v_i)exp(u_j⊤v_i)​≈p_ikp_ij​. 满足最右边约等号的一个可能是 exp⁡(u_j⊤v_i)≈αp_ij\\exp\\left(\\boldsymbol{u}\\_j^\\top {\\boldsymbol{v}}\\_i\\right) \\approx \\alpha p\\_{ij}exp(u_j⊤v_i)≈αp_ij，这里 α\\alphaα 是一个常数。考虑到 p_ij=xij/x_ip\\_{ij}=x_{ij}/x\\_ip_ij=xij​/x_i，取对数后 u_j⊤v_i≈log⁡ α+log⁡ x_ij−log⁡ x_i\\boldsymbol{u}\\_j^\\top {\\boldsymbol{v}}\\_i \\approx \\log\\,\\alpha + \\log\\,x\\_{ij} - \\log\\,x\\_iu_j⊤v_i≈logα+logx_ij−logx_i。我们使用额外的偏差项来拟合 −log⁡ α+log⁡ x_i- \\log\\,\\alpha + \\log\\,x\\_i−logα+logx_i，例如中心词偏差项 b_ib\\_ib_i 和背景词偏差项 c_jc\\_jc_j： u_j⊤v_i+b_i+c_j≈log⁡(x_ij).\\boldsymbol{u}\\_j^\\top \\boldsymbol{v}\\_i + b\\_i + c\\_j \\approx \\log(x\\_{ij}). u_j⊤v_i+b_i+c_j≈log(x_ij). 对上式左右两边取平方误差并加权，我们可以得到 GloVe 的损失函数。 &lt;img src=&quot;/images/nlp/glove-1.jpeg&quot; width=“900” /img&gt; 小结 在有些情况下，交叉熵损失函数有劣势。GloVe 采用了平方损失，并通过词向量拟合预先基于整个数据集计算得到的全局统计信息。 任意词的中心词向量和背景词向量在 GloVe 中是等价的。 Reference 动手学深度学习第十七课：GloVe、fastText和使用预训练的词向量","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Glove","slug":"Glove","permalink":"http://www.iequa.com/tags/Glove/"}]},{"title":"TensorFlow：第8章 LSTM & Bi-RNN & Deep RNN","slug":"tensorflow/tf-google-8-rnn-2","date":"2018-11-10T05:00:21.000Z","updated":"2021-06-20T04:12:28.330Z","comments":true,"path":"2018/11/10/tensorflow/tf-google-8-rnn-2/","link":"","permalink":"http://www.iequa.com/2018/11/10/tensorflow/tf-google-8-rnn-2/","excerpt":"LSTM 可以学习到距离很远的信息，解决了RNN无法长期依赖的问题。 Bidirectional RNN 解决的是 当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关。 Deep RNNs 是 为了增强模型的表达能力，可以在网络中设置多个循环层，将每层 RNN 的输出传给下一层处理。","text":"LSTM 可以学习到距离很远的信息，解决了RNN无法长期依赖的问题。 Bidirectional RNN 解决的是 当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关。 Deep RNNs 是 为了增强模型的表达能力，可以在网络中设置多个循环层，将每层 RNN 的输出传给下一层处理。 1. LSTM 单层LSTM结构实现 Tensorflow中实现了以下模块 :tf.nn.rnn_cell，包括了10个类： class BasicLSTMCell: Basic LSTM recurrent network cell. class BasicRNNCell: The most basic RNN cell. class DeviceWrapper: Operator that ensures an RNNCell runs on a particular device. class DropoutWrapper: Operator adding dropout to inputs and outputs of the given cell. class GRUCell: Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078). class LSTMCell: Long short-term memory unit (LSTM) recurrent network cell. class LSTMStateTuple: Tuple used by LSTM Cells for state_size, zero_state, and output state. class MultiRNNCell: RNN cell composed sequentially of multiple simple cells. class RNNCell: Abstract object representing an RNN cell. class ResidualWrapper: RNNCell wrapper that ensures cell inputs are added to the outputs. 在基本的 LSTM cell 中我们用第一个类来进行实现，他是 tf.contrib.rnn.BasicLSTMCell 同名类，定义在 tensorflow/python/ops/rnn_cell_impl.py 中 12345678_init__( num_units, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None, name=None) 其中参数表示： num_units 表示神经元的个数 forget_bias 就是LSTM们的忘记系数，如果等于1，就是不会忘记任何信息。如果等于0，就都忘记 state_is_tuple 默认就是True，表示返回的状态是一个 2-tuple (c_state, m_state) activation 表示内部状态的激活函数，默认是 tanh name 表示这一层的名字，同样名字的层会共享权重，如果为了避免这样的情况需要设置reuse=True 采用BasicLSTMCell来声明LSTM结构如下所示，我们用伪代码和注释来进行说明。 123456789101112131415161718192021222324import tensorflow as tf# 定义一个lstm结构，在tensorflow中通过一句话就能实现一个完整的lstm结构# lstm_hidden_size 表示 LSTM cell 中神经元的数量。 cell其实就是一个RNN的网络。lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_hidden_size)# 将lstm中的状态初始化为全0数组，BasicLSTMCell提供了zero_state来生成全0数组# 在优化RNN时每次也会使用一个batch的训练样本，batch_size给出了一个batch的大小state = lstm.zero_state(batch_size, tf.float32)# 定义损失函数loss = 0.0# 为了在训练中避免梯度弥散的情况，规定一个最大的序列长度num_stepsfor i in range(num_steps): # 在第一个时刻声明lstm结构中使用的变量，在之后的时刻都需要重复使用之前定义好的变量 if i&gt;0: tf.get_variable_scope().reuse_variables() # 每一步处理时间序列中的一个时刻，将当前输入current_input和前一时刻状态state传入LSTM结构 # 就可以得到当前lstm结构的输出lstm_output和更新后的状态state lstm_output, state = lstm(current_input, state) # 将当前时刻lstm输出传入一个全连接层得到最后的输出 final_output = fully_connected(lstm_output) # 计算当前时刻输出的损失 loss += calc_loss(final_output, expected_output) 2. Bidirectional RNN Bidirectional RNN 双向递归神经网络. 该神经网络首先从正面理解一遍这句话，再从反方向理解一遍. 3. Deep RNNs Deep RNNs 深层，顾名思义就是层次增。 横向表示时间展开，纵向则是层次展开。","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://www.iequa.com/tags/RNN/"}]},{"title":"TensorFlow： 第8章 循环神经网络 1","slug":"tensorflow/tf-google-8-rnn-1","date":"2018-11-08T14:00:21.000Z","updated":"2021-06-20T04:12:28.327Z","comments":true,"path":"2018/11/08/tensorflow/tf-google-8-rnn-1/","link":"","permalink":"http://www.iequa.com/2018/11/08/tensorflow/tf-google-8-rnn-1/","excerpt":"实战Google深度学习框架 笔记-第8章 循环神经网络-1-前向传播。 Github: RNN-1-Forward_Propagation.ipynb","text":"实战Google深度学习框架 笔记-第8章 循环神经网络-1-前向传播。 Github: RNN-1-Forward_Propagation.ipynb 运算的流程图可参考下面这张图 RNN Forward Propagation RNN 前向传播知识回顾 a&lt;0&gt;=0⃗a^{&lt;0&gt;}=\\vec{0}a&lt;0&gt;=0 a&lt;1&gt;=g_1(W_aaa&lt;0&gt;+W_axx&lt;1&gt;+b_a)a^{&lt;1&gt;}=g\\_1(W\\_{aa}a^{&lt;0&gt;}+W\\_{ax}x^{&lt;1&gt;}+b\\_a)a&lt;1&gt;=g_1(W_aaa&lt;0&gt;+W_axx&lt;1&gt;+b_a) y&lt;1&gt;=g_2(W_yaa&lt;1&gt;+b_y)y^{&lt;1&gt;}=g\\_2(W\\_{ya}a^{&lt;1&gt;}+b\\_y)y&lt;1&gt;=g_2(W_yaa&lt;1&gt;+b_y) a&lt;t&gt;=g_1(W_aaa&lt;t−1&gt;+W_axx&lt;t&gt;+b_a)a^{&lt;{t}&gt;}=g\\_1(W\\_{aa}a^{&lt;{t-1}&gt;}+W\\_{ax}x^{&lt;{t}&gt;}+b\\_a)a&lt;t&gt;=g_1(W_aaa&lt;t−1&gt;+W_axx&lt;t&gt;+b_a) y&lt;t&gt;=g_2(W_yaa&lt;t&gt;+b_y)y^{&lt;{t}&gt;}=g\\_2(W\\_{ya}a^{&lt;{t}&gt;}+b\\_y)y&lt;t&gt;=g_2(W_yaa&lt;t&gt;+b_y) 激活函数：g_1g\\_1g_1 一般为 tanh函数 (或者是 Relu函数)，g_2g\\_2g_2 一般是 Sigmod函数. 注意: 参数的下标是有顺序含义的，如 W_axW\\_{ax}W_ax 下标的第一个参数表示要计算的量的类型，即要计算 aaa 矢量，第二个参数表示要进行乘法运算的数据类型，即需要与 xxx 矢量做运算。如 W_axxt→aW\\_{ax} x^{t}\\rightarrow{a}W_axxt→a 1. 定义RNN的参数 这个例子是用np写的，没用到tensorflow 123456789101112131415import numpy as np# 初始化， state = a^&#123;&lt;0&gt;&#125; 与 定义 X 时间序列参数X = [1, 2]state = [0.0, 0.0] # a^&#123;&lt;0&gt;&#125;# 分开定义不同输入部分的权重以方便操作w_cell_state = np.asarray([[0.1, 0.2], [0.3, 0.4]]) # W_&#123;aa&#125;w_cell_input = np.asarray([[0.5, 0.6]]) # W_&#123;ax&#125;b_cell = np.asarray([0.1, -0.1])# 定义用于输出的全连接层参数， 与 state = a^&#123;&lt;i&gt;&#125; 的 shape 相反置w_output = np.asarray([[0.1], [2.0]])b_output = 0.1 2. 执行前向传播的过程 12345678910111213# 按照时间顺序执行循环审计网络的前向传播过程for i in range(len(X)): # 计算循环体中的全连接层神经网络 before_activation = np.dot(state, w_cell_state) + X[i] * w_cell_input + b_cell state = np.tanh(before_activation) final_output = np.dot(state, w_output) + b_output print(&quot;iteration round:&quot;, i+1) print(&quot;before activation: &quot;, before_activation) print(&quot;state: &quot;, state) print(&quot;output: &quot;, final_output) output: 12345678iteration round: 1before activation: [[0.95107374 1.0254142 ]]state: [[0.74026877 0.7720626 ]]output: [[1.71815207]]iteration round: 2before activation: [[1.40564566 1.55687879]]state: [[0.88656589 0.91491336]]output: [[2.0184833]] 和其他神经网络类似，在定义完损失函数之后，套用第4章中介绍的优化框架TensorFlow就可以自动完成模型训练的过程。这里唯一需要特别指出的是，理论上循环神经网络可以支持任意长度的序列，然而在实际中，如果序列过长会导致优化时出现梯度消散的问题（the vanishing gradient problem） (8) ，所以实际中一般会规定一个最大长度，当序列长度超过规定长度之后会对序列进行截断。 Reference 知乎：《TensorFlow：实战Google深度学习框架》笔记、代码及勘误-第8章 循环神经网络-1-前向传播 7天时间： 循环神经网络简介 (1)","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://www.iequa.com/tags/RNN/"}]},{"title":"大数据平台CDH6.0集群在线安装","slug":"hadoop/ops-install-CDH6.0.1","date":"2018-11-02T06:16:21.000Z","updated":"2021-06-20T04:12:28.318Z","comments":true,"path":"2018/11/02/hadoop/ops-install-CDH6.0.1/","link":"","permalink":"http://www.iequa.com/2018/11/02/hadoop/ops-install-CDH6.0.1/","excerpt":"介绍了 CDH 集群的搭建与安装 标签： Cloudera-Manager CDH Hadoop 部署 集群","text":"介绍了 CDH 集群的搭建与安装 标签： Cloudera-Manager CDH Hadoop 部署 集群 目前Hadoop比较流行的主要有2个版本，Apache和Cloudera版本。 Apache Hadoop：维护人员比较多，更新频率比较快，但是稳定性比较差。 Cloudera Hadoop（CDH）：CDH：Cloudera公司的发行版本，基于Apache Hadoop的二次开发，优化了组件兼容和交互接口、简化安装配置、增加Cloudera兼容特性。 1. 操作环境 CentOS 7.3 x64 （4C/10G/50G） Cloudera Manager：6.0.1 CDH: 6.0.1 相关包地址 Cloudera Manager下载地址：https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/ cloudera-manager-agent-6.0.0-530873.el7.x86_64.rpm cloudera-manager-daemons-6.0.0-530873.el7.x86_64.rpm cloudera-manager-server-6.0.0-530873.el7.x86_64.rpm cloudera-manager-server-db-2-6.0.0-530873.el7.x86_64.rpm oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm CDH安装包地址：https://archive.cloudera.com/cdh6/6.0.0/parcels/ CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256 manifest.json 注意：以下操作均用root用户操作。 2. 网络配置(所有节点) 在所有节点上把IP和主机名的对应关系写入 123456vim /etc/hosts# 注释掉原有的语句, 增加：192.192.0.25 server192.192.0.26 chdagent1192.192.0.27 chdagent2 在相应的节点主机上修改主机名 123456vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=cdhserver# 修改或者添加 HOSTNAME=cdhserver cdhserver 是你起的的主机名字 执行命令 1# hostname cdhserver CentOS7要多执行以下这步： 1hostnamectl set-hostname cdhserver 3. 打通SSH 设置ssh无密码登陆（所有节点） 4. 关闭防火墙和SELinux 注意： 需要在所有的节点上执行，因为涉及到的端口太多了，临时关闭防火墙是为了安装起来更方便，安装完毕后可以根据需要设置防火墙策略，保证集群安全。 关闭防火墙并关闭自启动： 12systemctl stop firewalldsystemctl disable firewalld 5. 所有节点配置NTP服务 集群中所有主机必须保持时间同步，如果时间相差较大会引起各种问题。 具体思路如下： master节点作为ntp服务器与外界对时中心同步时间，随后对所有datanode节点提供时间同步服务。 所有datanode节点以master节点为基础同步时间。 所有节点安装相关组件： 1yum install ntp 启动服务： 1systemctl start ntpd 配置开机启动： 1systemctl enable ntpd 6. 安装 python 2.7 必须是python2.7版本，CentOS 7 系统可以不用装，系统自带的。 1234#下载并安装EPEL，安装python-pip，psycopg2有依赖[root@localhost ~]# wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm[root@localhost ~]# rpm -ivh epel-release-latest-7.noarch.rpm[root@localhost ~]# yum repolist #检查是否已添加至源列表 升级软件依赖版本 Starting with CDH 6, PostgreSQL-backed Hue requires the Psycopg2 version to be at least 2.5.4 首先安装epel扩展源： 123yum -y install epel-releaseyum -y install python-pippip install --upgrade psycopg2 7. 准备Parcels，用以安装CDH6 将CHD6相关的Parcel包放到主节点的/opt/cloudera/parcel-repo/目录中，如果没有此目录，可以自己创建。 CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256 manifest.json 注意：最后将CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256，重命名为CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha 安装repo: 1wget https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/ 导入GPG key: 1rpm --import https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPM-GPG-KEY-cloudera JDK install: yum install oracle-j2sdk1.8 注意 ： 使用 yum 下载，需要确定版本与安装CDH6官方要求的需要的版本一致 也可不使用 yum 安装，使用自己下载 JDK，然后手动绿色安装配置 也可在安装 CM 的时候，再根据提示来安装需要的 JDK 三种方式任选其一便可 yum安装CM: 1yum install cloudera-manager-server 8. 安装MySql 123456wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum updateyum install mysql-serversystemctl start mysqldsystemctl enable mysqld 初始化Mysql 1/usr/bin/mysql_secure_installation 配置JDBC 12345wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gztar zxvf mysql-connector-java-5.1.46.tar.gzmkdir -p /usr/share/java/cd mysql-connector-java-5.1.46cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar 建库：根据官方文档提供的命名建库，方便记忆。(在CM配置CDH的时候会用到这些库名) Set up the Cloudera Manager Database：/opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm 出现如下日志： 123456JAVA_HOME=/usr/java/jdk1.8.0_141-clouderaVerifying that we can write to /etc/cloudera-scm-serverCreating SCM configuration file in /etc/cloudera-scm-serverExecuting: /usr/java/jdk1.8.0_141-cloudera/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.[main] DbCommandExecutor INFO Successfully connected to database.All done, your SCM database is configured correctly! 9. 启动CM服务 启动： 1systemctl start cloudera-scm-server 查看日志： 1tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log 出现：INFO WebServerImpl:com.cloudera.server.cmf.WebServerImpl: Started Jetty server.则表示服务正常启动 登录 http://&lt;server_host&gt;:7180 账号：admin Reference CDH 6.0.0 搭建 CDH6.0.0详细安装教程及所遇到的问题 官方文档 - Cloudera Installation Guide","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"http://www.iequa.com/tags/CDH/"}]},{"title":"多层 LSTM 通俗版","slug":"tensorflow/tf-simple-lstms","date":"2018-10-07T05:10:21.000Z","updated":"2021-06-20T04:12:28.329Z","comments":true,"path":"2018/10/07/tensorflow/tf-simple-lstms/","link":"","permalink":"http://www.iequa.com/2018/10/07/tensorflow/tf-simple-lstms/","excerpt":"官方给出的例子，用多层 LSTM 来实现 PTBModel 语言模型，比如： tensorflow笔记：多层LSTM代码分析 感觉这些例子还是太复杂了，所以这里写了个比较简单的版本","text":"官方给出的例子，用多层 LSTM 来实现 PTBModel 语言模型，比如： tensorflow笔记：多层LSTM代码分析 感觉这些例子还是太复杂了，所以这里写了个比较简单的版本 声明： 本文部分内容转自 永永夜 Tensorflow学习之路 自己做了一个示意图，希望帮助初学者更好地理解 多层RNN. 通过本例，你可以了解到单层 LSTM 的实现，多层 LSTM 的实现。输入输出数据的格式。 RNN 的 dropout layer 的实现。 MNIST 背景 准备数据 MNIST 是在机器学习领域中的一个经典问题。该问题解决的是把 28x28像素 的灰度手写数字图片识别为相应的数字，其中数字的范围从 0到9. MNIST 数据集 包含了 60000 张图片来作为训练数据，10000 张图片作为测试数据。每张图片都代表了 0~9 中的一个数字。图片大小都为 28*28，处理后的每张图片是一个长度为 784 的一维数组，这个数组中的元素对应图片像素矩阵提供给神经网络的输入层，像素矩阵中元素的取值范围 [0, 1]， 它代表了颜色的深浅。其中 0 表示白色背景(background)，1 表示黑色前景(foreground)。 1234567891011121314# -*- coding:utf-8 -*-import tensorflow as tfimport numpy as npfrom tensorflow.contrib import rnnfrom tensorflow.examples.tutorials.mnist import input_data# 设置 GPU 按需增长config = tf.ConfigProto()config.gpu_options.allow_growth = Truesess = tf.Session(config=config)# 首先导入数据，看一下数据的形式mnist = input_data.read_data_sets(&#x27;MNIST_data&#x27;, one_hot=True)print(mnist.train.images.shape) 12345Extracting MNIST_data/train-images-idx3-ubyte.gzExtracting MNIST_data/train-labels-idx1-ubyte.gzExtracting MNIST_data/t10k-images-idx3-ubyte.gzExtracting MNIST_data/t10k-labels-idx1-ubyte.gz(55000, 784) # 训练集图片 - 55000 张 * 784维一维数组 执行 input_data.read_data_sets 后自动创建一个目录 MNIST_data，并开始下载数据 12345678910111213141516(anaconda3)# ~/ghome/github/TensorFlowExamples [master ✗ (98591d9)] [16:15:03]➜ lltotal 24drwxr-xr-x 6 blair staff 192B Oct 9 16:14 MNIST_data-rw-r--r-- 1 blair staff 2.3K Oct 9 16:13 simple-lstms.ipynb(anaconda3)# ~/ghome/github/TensorFlowExamples [master ✗ (98591d9)] [16:15:06]➜ ll MNIST_datatotal 22672-rw-r--r-- 1 blair staff 1.6M Oct 9 16:14 t10k-images-idx3-ubyte.gz-rw-r--r-- 1 blair staff 4.4K Oct 9 16:14 t10k-labels-idx1-ubyte.gz-rw-r--r-- 1 blair staff 9.5M Oct 9 16:14 train-images-idx3-ubyte.gz-rw-r--r-- 1 blair staff 28K Oct 9 16:14 train-labels-idx1-ubyte.gz(anaconda3)# ~/ghome/github/TensorFlowExamples [master ✗ (98591d9)] [16:15:12] 文件 内容 train-images-idx3-ubyte.gz 训练集图片 - 55000 张 训练图片, 5000 张 验证图片 train-labels-idx1-ubyte.gz 训练集图片对应的数字标签 t10k-images-idx3-ubyte.gz 测试集图片 - 10000 张 图片 t10k-labels-idx1-ubyte.gz 测试集图片对应的数字标签 12345print(&#x27;training data shape &#x27;, mnist.train.images.shape)print(&#x27;training label shape &#x27;, mnist.train.labels.shape)# training data shape (55000, 784)# training label shape (55000, 10) 1. 首先设置好模型用到的各个超参数 12345678910111213141516171819lr = 1e-3 # 0.001# 在训练和测试的时候，我们想用不同的 batch_size.所以采用占位符的方式batch_size = tf.placeholder(tf.int32, []) # 注意类型必须为 tf.int32keep_prob = tf.placeholder(tf.float32, [])# 每个时刻的输入特征是28维的，就是每个时刻输入一行，一行有 28 个像素input_size = 28# 时序持续长度为28，即每做一次预测，需要先输入28行timestep_size = 28# 每个隐含层的节点数hidden_size = 256# LSTM layer 的层数layer_num = 2# 最后输出分类类别数量，如果是回归预测的话应该是 1class_num = 10_X = tf.placeholder(tf.float32, [None, 784])y = tf.placeholder(tf.float32, [None, class_num]) 2. 开始搭建 LSTM 模型，其实普通 RNNs 模型也一样 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 把784个点的字符信息还原成 28 * 28 的图片# 下面几个步骤是实现 RNN / LSTM 的关键#################################################################### tf.reshape(tensor, shape, name=None) 函数的作用是将 tensor 变换为参数shape的形式## **步骤1：RNN 的输入shape = (batch_size, timestep_size, input_size) X = tf.reshape(_X, [-1, 28, 28]) # **步骤2：定义一层 LSTM_cell，只需要说明 hidden_size, 它会自动匹配输入的 X 的维度lstm_cell = rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=1.0, state_is_tuple=True)# **步骤3：添加 dropout layer, 一般只设置 output_keep_problstm_cell = rnn.DropoutWrapper(cell=lstm_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)# **步骤4：调用 MultiRNNCell 来实现多层 LSTMmlstm_cell = rnn.MultiRNNCell([lstm_cell] * layer_num, state_is_tuple=True)# **步骤5：用全零来初始化stateinit_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)# **步骤6：方法一，调用 dynamic_rnn() 来让我们构建好的网络运行起来# ** 当 time_major==False 时， outputs.shape = [batch_size, timestep_size, hidden_size] # ** 所以，可以取 h_state = outputs[:, -1, :] 作为最后输出# ** state.shape = [layer_num, 2, batch_size, hidden_size], # ** 或者，可以取 h_state = state[-1][1] 作为最后输出# ** 最后输出维度是 [batch_size, hidden_size]# outputs, state = tf.nn.dynamic_rnn(mlstm_cell, inputs=X, initial_state=init_state, time_major=False)# h_state = outputs[:, -1, :] # 或者 h_state = state[-1][1]# *************** 为了更好的理解 LSTM 工作原理，我们把上面 步骤6 中的函数自己来实现 ***************# 通过查看文档你会发现， RNNCell 都提供了一个 __call__()函数（见最后附），我们可以用它来展开实现LSTM按时间步迭代。# **步骤6：方法二，按时间步展开计算 (暂时没有运行通过)outputs = list()state = init_statewith tf.variable_scope(&#x27;RNN&#x27;): for timestep in range(timestep_size): if timestep &gt; 0: tf.get_variable_scope().reuse_variables() # 这里的state保存了每一层 LSTM 的状态 (cell_output, state) = mlstm_cell(X[:, timestep, :], state) outputs.append(cell_output)h_state = outputs[-1]# X[:, timestep, :] 就是取第timestep个时刻的特征 x_t 输入 mlstm_cell 中计算，因为每次用 batch_size 个样本来训练，所以相当于（并行）输入 batch_size 个 x_t 到 mlstm_cell 中计算。 3. 设置 loss function 和 优化器，展开训练并完成测试 1234567891011121314151617181920212223242526272829303132# 上面 LSTM 部分的输出会是一个 [hidden_size] 的 tensor，我们要分类的话，还需要接一个 softmax 层# 首先定义 softmax 的连接权重矩阵和偏置# out_W = tf.placeholder(tf.float32, [hidden_size, class_num], name=&#x27;out_Weights&#x27;)# out_bias = tf.placeholder(tf.float32, [class_num], name=&#x27;out_bias&#x27;)# 开始训练和测试W = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)bias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)y_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)# 损失和评估函数cross_entropy = -tf.reduce_mean(y * tf.log(y_pre))train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))sess.run(tf.global_variables_initializer())for i in range(2000): _batch_size = 128 batch = mnist.train.next_batch(_batch_size) sess.run(train_op, feed_dict=&#123;_X: batch[0], y: batch[1], keep_prob: 0.5, batch_size: _batch_size&#125;) if (i+1)%200 == 0: train_accuracy = sess.run(accuracy, feed_dict=&#123; _X:batch[0], y: batch[1], keep_prob: 1.0, batch_size: _batch_size&#125;) # 已经迭代完成的 epoch 数: mnist.train.epochs_completed print &quot;Iter%d, step %d, training accuracy %g&quot; % ( mnist.train.epochs_completed, (i+1), train_accuracy) # 计算测试数据的准确率print &quot;test accuracy %g&quot;% sess.run(accuracy, feed_dict=&#123; _X: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0, batch_size:mnist.test.images.shape[0]&#125;) 1234567891011Iter0, step 200, training accuracy 0.851562Iter0, step 400, training accuracy 0.960938Iter1, step 600, training accuracy 0.984375Iter1, step 800, training accuracy 0.960938Iter2, step 1000, training accuracy 0.984375Iter2, step 1200, training accuracy 0.9375Iter3, step 1400, training accuracy 0.96875Iter3, step 1600, training accuracy 0.984375Iter4, step 1800, training accuracy 0.992188Iter4, step 2000, training accuracy 0.984375test accuracy 0.9858 我们一共只迭代不到 5 个 epoch，在测试集上就已经达到了 0.98 的准确率，可以看出来 LSTM 在做这个字符分类的任务上还是比较有效的，而且我们最后一次性对 10000 张测试图片进行预测，才占了 725 MiB 的显存。而我们在之前的两层 CNNs 网络中，预测 10000 张图片一共用了 8721 MiB 的显存，差了整整 12 倍呀！！ 这主要是因为 RNN/LSTM 网络中，每个时间步所用的权值矩阵都是共享的，可以通过前面介绍的 LSTM 的网络结构分析一下，整个网络的参数非常少。 Reference 大学之道，在明明德 永永夜 Tensorflow学习之路 tensorflow笔记：多层LSTM代码分析 极客学院 MNIST 数据下载 隔壁小王 LSTM 神经网络输入输出究竟是怎样的？ colab.research.google zh.gluon.ai 动手学深度学习 discuss.gluon.ai 论坛 1234567roll_jj： 博主你好， outputs, state = tf.nn.dynamic_rnn(mlstm_cell, inputs=X, initial_state=init_state, time_major=False) h_state = outputs[:, -1, :] 这两句话里，outputs的三个维度是什么意思，为什么把中间那个维度去掉就是我们要的输出结果了？(1年前#6楼)收起回复举报回复Jerr__yCQU_HYX回复 roll_jj： 是的(1年前)roll_jjroll_jj回复 CQU_HYX： 谢谢博主解答。我看官方的那个PTB例子里，没有取[-1]的这个操作，而是用了output = tf.reshape(tf.concat(1, outputs), [-1, size])操作，这是因为预测目标的不同么？(1年前)Jerr__yCQU_HYX回复 roll_jj： 原文注释上面有说了，outputs.shape = [batch_size, timestep_size, hidden_size]。 因为是分类问题，所有只需要在看完最后一行像素后才输出分类结果。-1 表示取最后一个 timestep 的结果， 而不是说把中间维度去掉","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"简单前馈网络实现 mnist 分类","slug":"tensorflow/tf-mnist-1-beginners","date":"2018-10-04T13:10:21.000Z","updated":"2021-06-20T04:12:28.326Z","comments":true,"path":"2018/10/04/tensorflow/tf-mnist-1-beginners/","link":"","permalink":"http://www.iequa.com/2018/10/04/tensorflow/tf-mnist-1-beginners/","excerpt":"我们来实现一个非常简单的两层 FC 全连接网络来完成 MNIST数据 的分类","text":"我们来实现一个非常简单的两层 FC 全连接网络来完成 MNIST数据 的分类 输入 [-1,28*28]， FC1 有 1024 个neurons， FC2 有 10 个neurons。 这么简单的一个全连接网络，结果测试准确率达到了 0.98。还是非常棒的！！！ MNIST 数据集 包含了 60000 张图片来作为训练数据，10000 张图片作为测试数据。每张图片都代表了 0~9 中的一个数字。图片大小都为 28*28，处理后的每张图片是一个长度为 784 的一维数组，这个数组中的元素对应图片像素矩阵提供给神经网络的输入层，像素矩阵中元素的取值范围 [0, 1]， 它代表了颜色的深浅。其中 0 表示白色背景(background)，1 表示黑色前景(foreground)。 为了方便使用随机梯度下降， input_data.read_data_sets 函数生成的类还提供了 mnist.train.next.batch 函数，它可以从所有训练数据中读取一小部分作为一个训练 batch。 MNIST 数据下载地址和内容 内容 Extracting MNIST_data/train-images-idx3-ubyte.gz 训练数据图片 Extracting MNIST_data/train-labels-idx1-ubyte.gz 训练数据答案 Extracting MNIST_data/t10k-images-idx3-ubyte.gz 测试数据图片 Extracting MNIST_data/t10k-labels-idx1-ubyte.gz 测试数据答案 1234567import numpy as npimport tensorflow as tf# 设置按需使用 GPUconfig = tf.ConfigProto()config.gpu_options.allow_growth = Truesess = tf.InteractiveSession(config=config) 1. 导入数据 123456789# 用tensorflow 导入数据from tensorflow.examples.tutorials.mnist import input_data# input_data.read_data_sets 自动将 MNIST 数据集划分为 train、validation、test 三个数据集mnist = input_data.read_data_sets(&#x27;MNIST_data&#x27;, one_hot=True)# train 集合有 55000 张图片# validation 集合有 5000 张图片# test 集合有 10000 张图片，图片来自 MNIST 提供的测试数据集 12345print(&#x27;training data shape &#x27;, mnist.train.images.shape)print(&#x27;training label shape &#x27;, mnist.train.labels.shape)# training data shape (55000, 784)# training label shape (55000, 10) 2. 构建网络 12345678910111213141516171819202122232425# 权值初始化def weight_variable(shape): # 用正态分布来初始化权值 initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): # 本例中用relu激活函数，所以用一个很小的正偏置较好 initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)# input_layerX_ = tf.placeholder(tf.float32, [None, 784])y_ = tf.placeholder(tf.float32, [None, 10])# FC1W_fc1 = weight_variable([784, 1024])b_fc1 = bias_variable([1024])h_fc1 = tf.nn.relu(tf.matmul(X_, W_fc1) + b_fc1)# FC2W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_pre = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2) 3. 训练和评估 12345678910111213141516171819202122# 1.损失函数：cross_entropycross_entropy = -tf.reduce_sum(y_ * tf.log(y_pre))# 2.优化函数：AdamOptimizer, 优化速度要比 GradientOptimizer 快很多train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)# 3.预测结果评估# 预测值中最大值（１）即分类结果，是否等于原始标签中的（１）的位置。argmax()取最大值所在的下标correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.arg_max(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# 开始运行sess.run(tf.global_variables_initializer())# 这大概迭代了不到 10 个 epoch， 训练准确率已经达到了0.98for i in range(5000): X_batch, y_batch = mnist.train.next_batch(batch_size=100) train_step.run(feed_dict=&#123;X_: X_batch, y_: y_batch&#125;) if (i+1) % 200 == 0: train_accuracy = accuracy.eval(feed_dict=&#123;X_: mnist.train.images, y_: mnist.train.labels&#125;) print(&quot;step %d, training acc %g&quot; % (i+1, train_accuracy)) if (i+1) % 1000 == 0: test_accuracy = accuracy.eval(feed_dict=&#123;X_: mnist.test.images, y_: mnist.test.labels&#125;) print(&quot;= &quot; * 10, &quot;step %d, testing acc %g&quot; % (i+1, test_accuracy)) Output: 123456789101112131415161718192021222324252627282930step 200, training acc 0.937364step 400, training acc 0.965818step 600, training acc 0.973364step 800, training acc 0.977709step 1000, training acc 0.981528= = = = = = = = = = step 1000, testing acc 0.9688step 1200, training acc 0.988437step 1400, training acc 0.988728step 1600, training acc 0.987491step 1800, training acc 0.993873step 2000, training acc 0.992527= = = = = = = = = = step 2000, testing acc 0.9789step 2200, training acc 0.995309step 2400, training acc 0.995455step 2600, training acc 0.9952step 2800, training acc 0.996073step 3000, training acc 0.9964= = = = = = = = = = step 3000, testing acc 0.9778step 3200, training acc 0.996709step 3400, training acc 0.998109step 3600, training acc 0.997455step 3800, training acc 0.995055step 4000, training acc 0.997291= = = = = = = = = = step 4000, testing acc 0.9808step 4200, training acc 0.997746step 4400, training acc 0.996073step 4600, training acc 0.998564step 4800, training acc 0.997946step 5000, training acc 0.998673= = = = = = = = = = step 5000, testing acc 0.98 Reference 大学之道，在明明德 永永夜 Tensorflow学习之路 W3cschool MNIST数据集 來龍去脈講解的清清楚楚 Visual-Information 交叉熵","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"RNN 的语言模型 TensorFlow 实现","slug":"tensorflow/tf-nlp-9.2.3","date":"2018-10-02T13:00:21.000Z","updated":"2021-06-20T04:12:28.326Z","comments":true,"path":"2018/10/02/tensorflow/tf-nlp-9.2.3/","link":"","permalink":"http://www.iequa.com/2018/10/02/tensorflow/tf-nlp-9.2.3/","excerpt":"上篇 PTB 数据集 batching 中我们介绍了如何对 PTB 数据集进行 连接、切割 成多个 batch，作为 NNLM 的输入。 本文将介绍如何采用 TensorFlow 实现 RNN-based NNLM。","text":"上篇 PTB 数据集 batching 中我们介绍了如何对 PTB 数据集进行 连接、切割 成多个 batch，作为 NNLM 的输入。 本文将介绍如何采用 TensorFlow 实现 RNN-based NNLM。 1. Embedding 层 将 word 编号 转化为 word embedding 两大作用 : No. function desc 1. 降低输入的维度 词向量的维度通常在 200 ~ 1000 之间, 大大减少 RNN 网络的参数数量 与 计算量 2. 增加语义信息 简单的单词编号是不包含任何语义信息的. 词向量维度: EMB_SIZE，词汇表大小: VOCAB_SIZE 所有单词的词向量可以放入一个大小为 (EMB_SIZE, VOCAB_SIZE) 的矩阵内 在读取词向量时，可以调用 tf.nn.embedding_lookup 方法。 用 tf.Variable 来表示词向量，这样就可以采用任意初始化的词向量，学习过程中也会优化词向量。 12345# 定义单词的词向量矩阵embedding = tf.get_variable(&quot;embedding&quot;, [VOCAB_SIZE, EMB_SIZE])# 将数据转化为词向量表示inputs = tf.nn.embedding_lookup(embedding, input_data) 其中输入数据 input_data 的维度是 (batch_size * num_steps) 而输出的 input_embedding 的维度成为 (batch_size * num_steps * EMB_SIZE). 在本文中，我们输入数据维度是 (20×3520 \\times 3520×35) ，EMB_SIZE = 300, 输入词向量维度时 (20×35×30020 \\times 35 \\times 30020×35×300) . 2. Softmax 层 Softmax层 的作用是将 RNN 的输出 转化为一个单词表中每个单词的输出概率，为此需要两个步骤： 2.1 第一步 使用一个线性映射将 RNN 的输出映射为一个维度与词汇表大小相同的向量，这一步的输出叫做 logits. 代码如下所示： 1234567# 首先定义映射用到的参数# HIDDEN_SIZE 是 RNN 的隐藏状态维度，VOCAB_SIZE 是词汇表大小weight = tf.get_variable(&quot;weight&quot;, [HIDDEN_ZIZE, VOCAB_SIZE])bias = tf.get_variable(&quot;bias&quot;, [VOCAB_SIZE])# 计算线性映射logits = tf.matmul(output, weight) + bias 其中 output 是 RNN 的输出，维度是 [batch_size * num_steps, HIDDEN_SIZE] 经过线性映射后，输出结果是 [batch_size * num_steps, VOCAB_SIZE]. Reference tensorflow.org tf.nn.embedding_lookup函数原理？ 求通俗讲解下tensorflow的embedding_lookup接口的意思？ Tomas Mikolov PTB 数据 如何理解深度学习源码里经常出现的logits？ 基于循环神经网络的语言模型的介绍与TensorFlow实现(4)：TensorFlow实现RNN-based语言模型","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"PTB 数据的 batching 方法","slug":"tensorflow/tf-nlp-9.2.2","date":"2018-10-01T03:00:21.000Z","updated":"2021-06-20T04:12:28.325Z","comments":true,"path":"2018/10/01/tensorflow/tf-nlp-9.2.2/","link":"","permalink":"http://www.iequa.com/2018/10/01/tensorflow/tf-nlp-9.2.2/","excerpt":"PTB 数据集 batching 介绍, 如何对 PTB 数据集进行 连接、切割 成多个 batch。 重点了解 batch_size、num_batch、num_step 这三个概念。","text":"PTB 数据集 batching 介绍, 如何对 PTB 数据集进行 连接、切割 成多个 batch。 重点了解 batch_size、num_batch、num_step 这三个概念。 batching 先看下面这段，摘取自 知乎 作者：dalida 画图很弱，直接就徒手画了。以朱自清的《背影》中的节选段落为例。字有点难看，请忽略～(￣▽￣～)~ 这是单个字符级别的 RNN，换成词语也一样。 num_batch 其实就是 batch 的数量，这里只画了三个，可以一直接下去…这样一次完成 6个字符 (num_step=6) 的处理，而且能保证 batch 之间的连续性。但是行与行之间的连续性确实是丢失了。 结论 : batch_size = 3 num_batch = 3 num_step = 6 batch_size 也是一次输入的句子数 朱自清《背影》 我们过了江，进了车站。我买票，他忙着照看行李。行李太多了，得向脚夫行些小费，才可过去。他便又忙着和他们讲价钱。我那时真是聪明过分，总觉他说话不大漂亮，非自己插嘴不可。但他终于讲定了价钱；就送我上车。他给我拣定了靠车门的一张椅子；我将他给我做的紫毛大衣铺好坐位。他嘱我路上小心，夜里警醒些，不要受凉。又嘱托茶房好好照应我。我心里暗笑他的迂；他们只认得钱，托他们直是白托！而且我这样大年纪的人，难道还不能料理自己么？唉，我现在想想，那时真是太聪明了！ 我说道，“爸爸，你走吧。”他望车外看了看，说，“我买几个橘子去。你就在此地，不要走动。”我看那边月台的栅栏外有几个卖东西的等着顾客。走到那边月台，须穿过铁道，须跳下去又爬上去。父亲是一个胖子，走过去自然要费事些。我本来要去的，他不肯，只好让他去。我看见他戴着黑布小帽，穿着黑布大马褂，深青布棉袍，蹒跚地走到铁道边，慢慢探身下去，尚不大难。可是他穿过铁道，要爬上那边月台，就不容易了。他用两手攀着上面，两脚再向上缩；他肥胖的身子向左微倾，显出努力的样子。这时我看见他的背影，我的泪很快地流下来了。我赶紧拭干了泪，怕他看见，也怕别人看见。我再向外看时，他已抱了朱红的橘子望回走了。过铁道时，他先将橘子散放在地上，自己慢慢爬下，再抱起橘子走。到这边时，我赶紧去搀他。他和我走到车上，将橘子一股脑儿放在我的皮大衣上。于是扑扑衣上的泥土，心里很轻松似的，过一会说，“我走了；到那边来信！”我望着他走出去。他走了几步，回过头看见我，说，“进去吧，里边没人。”等他的背影混入来来往往的人里，再找不着了，我便进来坐下，我的眼泪又来了。 近几年来，父亲和我都是东奔西走，家中光景是一日不如一日。他少年出外谋生，独力支持，做了许多大事。那知老境却如此颓唐！他触目伤怀，自然情不能自已。情郁于中，自然要发之于外；家庭琐屑便往往触他之怒。他待我渐渐不同往日。但最近两年的不见，他终于忘却我的不好，只是惦记着我，惦记着我的儿子。我北来后，他写了一信给我，信中说道，“我身体平安，惟膀子疼痛利害，举箸提笔，诸多不便，大约大去之期不远矣。”我读到此处，在晶莹的泪光中，又看见那肥胖的，青布棉袍，黑布马褂的背影。唉！我不知何时再能与他相见！ 再看这个图： Reference 关于batching多句子切割batch疑问？ [L2]使用LSTM实现语言模型-数据batching TensorFlow入门（五）多层 LSTM 通俗易懂版","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"TensorFlow： 第6章 图片识别与CNN","slug":"tensorflow/tf-google-6-cnn-1","date":"2018-09-28T02:00:21.000Z","updated":"2021-06-20T04:12:28.328Z","comments":true,"path":"2018/09/28/tensorflow/tf-google-6-cnn-1/","link":"","permalink":"http://www.iequa.com/2018/09/28/tensorflow/tf-google-6-cnn-1/","excerpt":"实战Google深度学习框架 笔记-第6章 图片识别 与 CNN 介绍 CNN 在图片识别的应用 和 CNN 基本原理 以及 如何使用 TensorFlow 来实现 CNN .","text":"实战Google深度学习框架 笔记-第6章 图片识别 与 CNN 介绍 CNN 在图片识别的应用 和 CNN 基本原理 以及 如何使用 TensorFlow 来实现 CNN . 1. 图像识别介绍与经典数据集 视觉是人类认识世界非常重要的一种知觉. 经典数据集 MNIST CIFAR ImageNet CIFAR 数据集是一个影响力很大的图像分类数据集, 是图像词典项目（Visual Dictionary） (7) 中800万张图片的一个子集， 都是 32×32的彩色图片, 由Alex Krizhevsky教授、Vinod Nair博士和Geoffrey Hinton教授整理的. CIFAR-10 问题收集了来自 10个 不同种类的 60000张图片, Cifar-10中的图片大小都是固定的且每一张图片中仅包含一个种类的实体 ImageNet 是为了更加贴近真实环境下的图像识别问题，基于 WordNet， 由斯坦福大学（Stanford University）的李飞飞（Feifei Li）带头整理的ImageNet很大程度地解决了这两个问题。 ILSVRC2012 包含来自 1000 个类别 120万 张图片. 2. CNN 介绍与常用结构 这部分内容详见 Convolutional Neural Networks 如何搭建一个神经网络，包括最新的变体，如: ResNet 3. 经典 CNN 模型 LeNet-5 Inception ResNet 3.1 LeNet-5 模型 该网络 1980s 提出，主要针对灰度图像训练的，用于识别手写数字。 3.2 Inception-v3 模型 4. CNN 迁移学习 4.1 迁移学习介绍 Reference 知乎：《TensorFlow：实战Google深度学习框架》笔记、代码及勘误-第6章 图像识别与卷积神经网络-1 7天时间读书 Tensorflow 实战 Google 深度学习框架 MNIST识别自己手写的数字–进阶篇（CNN）","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://www.iequa.com/tags/CNN/"}]},{"title":"Chatbot Research 7 - Dialog_Corpus 常用数据集","slug":"nlp/chatbot/chatbot-research7","date":"2018-09-26T06:00:21.000Z","updated":"2021-06-20T04:12:28.339Z","comments":true,"path":"2018/09/26/nlp/chatbot/chatbot-research7/","link":"","permalink":"http://www.iequa.com/2018/09/26/nlp/chatbot/chatbot-research7/","excerpt":"一些用于 Dialog 对话系统的数据集资料汇总","text":"一些用于 Dialog 对话系统的数据集资料汇总 对话系统常用数据集 介绍一下公开的数据集 : 可以参考“A Survey of Available Corpora for Building Data-Driven Dialogue Systems”这篇论文，而且作者把所有的数据集按照不同类别进行分类总结，里面涵盖了很多数据集，详情请戳 Dialogue datasets 英文数据集 Cornell Movie Dialogs Ubuntu Dialogue Corpus OpenSubtitles：电影字幕 Twitter：twitter数据集 Papaya Conversational Data Set：基于Cornell、Reddit等数据集重新整理 相关数据集的处理代码可参见下面两个github项目： DeepQA chat_corpus 中文数据集 dgk_shooter_min.conv：中文电影台词数据集 白鹭时代中文问答语料：白鹭时代论坛问答数据 微博数据集：华为李航实验室发布 也是论文“Neural Responding Machine for Short-Text Conversation”使用的数据集 新浪微博数据集，评论回复短句 Reference Github 用于对话系统的中英文语料 汇总合集 美剧 ： 近1GB的三千万聊天语料 - 10元 chatterbot-corpus chinese 简书 ： 高质量的对话语料 知乎 ： 现在有哪些中文的聊天语料库？ 深度学习对话系统理论篇–数据集和评价指标介绍","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"}]},{"title":"Tensorboard 可视化好帮手 1","slug":"tensorflow/tf-4.1-tensorboard1","date":"2018-09-12T07:00:21.000Z","updated":"2021-06-20T04:12:28.327Z","comments":true,"path":"2018/09/12/tensorflow/tf-4.1-tensorboard1/","link":"","permalink":"http://www.iequa.com/2018/09/12/tensorflow/tf-4.1-tensorboard1/","excerpt":"学会用 Tensorflow 自带的 tensorboard 去可视化我们所建造出来的神经网络是一个很好的学习理解方式. 用最直观的流程图告诉你, 你的神经网络是长怎样,有助于你发现编程中间的问题和疑问.","text":"学会用 Tensorflow 自带的 tensorboard 去可视化我们所建造出来的神经网络是一个很好的学习理解方式. 用最直观的流程图告诉你, 你的神经网络是长怎样,有助于你发现编程中间的问题和疑问. 效果 这次我们会介绍如何可视化神经网络。因为很多时候我们都是做好了一个神经网络，但是没有一个图像可以展示给大家看。 TensorFlow 的可视化工具 Tensorboard : 通过使用这个工具我们可以很直观的看到整个神经网络的结构、框架。 今天要显示的神经网络差不多是这样子的 同时我们也可以展开看每个layer中的一些具体的结构： 好，通过阅读代码和之前的图片我们大概知道了此处是有: 一个输入层（inputs） 一个隐含层（layer） 一个输出层（output） 现在可以看看如何进行可视化. 搭建图纸 首先从 Input 开始： 123# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1]) 对于 input 我们进行如下修改： 首先，可以为xs指定名称为x_in, ys 同样。 : 12xs= tf.placeholder(tf.float32, [None, 1],name=&#x27;x_in&#x27;)ys= tf.placeholder(tf.loat32, [None, 1],name=&#x27;y_in&#x27;) 这里指定的名称将来会在可视化的图层 inputs 中显示出来 使用 with tf.name_scope('inputs') 可以将 xs 和 ys 包含进来，形成一个大的图层，图层的名字就是 with tf.name_scope() 方法里的参数 1234with tf.name_scope(&#x27;inputs&#x27;): # define placeholder for inputs to network xs = tf.placeholder(tf.float32, [None, 1]) ys = tf.placeholder(tf.float32, [None, 1]) 接下来开始编辑 layer ， 请看编辑前的程序片段 ： 12345678910def add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b, ) return outputs 2.1 编辑后 add_layer 编辑后， 这里的名字应该叫 layer, 下面是编辑后的: 12345def add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer with tf.name_scope(&#x27;layer&#x27;): Weights= tf.Variable(tf.random_normal([in_size, out_size])) # and so on... 在定义完大的框架layer之后，同时也需要定义每一个’框架‘里面的小部件：(Weights biases 和 activation function): 现在现对 Weights 定义： 定义的方法同上，可以使用tf.name.scope() 方法，同时也可以在 Weights 中指定名称 W 。 即为： 接着继续定义 biases ， 定义方式同上。 activation_function 的话，可以暂时忽略。因为当你自己选择用 tensorflow 中的激励函数（activation function）的时候，tensorflow会默认添加名称。 最终，layer形式如下： 1234567891011121314151617181920def add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer with tf.name_scope(&#x27;layer&#x27;): with tf.name_scope(&#x27;weights&#x27;): Weights = tf.Variable( tf.random_normal([in_size, out_size]), name=&#x27;W&#x27;) with tf.name_scope(&#x27;biases&#x27;): biases = tf.Variable( tf.zeros([1, out_size]) + 0.1, name=&#x27;b&#x27;) with tf.name_scope(&#x27;Wx_plus_b&#x27;): Wx_plus_b = tf.add( tf.matmul(inputs, Weights), biases) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b, ) return outputs 效果如下：（有没有看见刚才定义 layer 里面的“内部构件”呢？） 最后编辑 loss 部分：将 with tf.name_scope() 添加在 loss 上方，并为它起名为 loss 1234567# the error between prediciton and real datawith tf.name_scope(&#x27;loss&#x27;): loss = tf.reduce_mean( tf.reduce_sum( tf.square(ys - prediction), eduction_indices=[1] )) 这句话就是“绘制” loss 了， 如下： 使用 with tf.name_scope() 再次对 train_step 部分进行编辑, 如下： 12with tf.name_scope(&#x27;train&#x27;): train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) 我们需要使用 tf.summary.FileWriter() 将上面‘绘画’出的图保存到一个目录中，以方便后期在浏览器中可以浏览。 这个方法中的第二个参数需要使用 sess.graph ， 因此我们需要把这句话放在获取 session 的后面。 这里的 graph 是将前面定义的框架信息收集起来，然后放在 logs/ 目录下面。 123sess = tf.Session() # get session# tf.train.SummaryWriter soon be deprecated, use followingwriter = tf.summary.FileWriter(&quot;logs/&quot;, sess.graph) 最后在你的terminal（终端）中 ，使用以下命令 1tensorboard --logdir logs 同时将终端中输出的网址复制到浏览器中，便可以看到之前定义的视图框架了。 tensorboard 还有很多其他的参数，希望大家可以多多了解, 可以使用 tensorboard --help 查看tensorboard的详细参数 最终的全部代码在这里 可能会遇到的问题 (1) 与 Tensorboard 兼容的浏览器是 “Google Chrome”. 使用其他的浏览器不保证所有内容都能正常显示. (2) 请使用 http://localhost:6006, 大多数朋友都是这个问题. (3) 请确保你的 tensorboard 指令是在你的 logs 文件根目录执行的. 如果在其他目录下, 比如 Desktop 等, 可能不会成功看到图. 比如在下面这个目录, 你要 cd 到 project 这个地方执行 /project &gt; tensorboard --logdir logs 1234- project - logs model.py env.py 有的朋友使用 anaconda 下的 python3.5 的虚拟环境, 如果你输入 tensorboard 的指令, 出现报错: &quot;tensorboard&quot; is not recognized as an internal or external command... 解决方法的关键就是需要激活TensorFlow. 管理员模式打开 Anaconda Prompt, 输入 activate tensorflow, 接着按照上面的流程执行 tensorboard 指令. Reference tensorflow.org 莫烦Python 莫烦代码 tf14_tensorboard/full_code.py","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"numpy.random.normal 函数","slug":"python/numpy-pandas/py-numpy-9-numpy.random.normal","date":"2018-09-11T08:16:21.000Z","updated":"2021-06-20T04:12:28.211Z","comments":true,"path":"2018/09/11/python/numpy-pandas/py-numpy-9-numpy.random.normal/","link":"","permalink":"http://www.iequa.com/2018/09/11/python/numpy-pandas/py-numpy-9-numpy.random.normal/","excerpt":"numpy.random.normal 函数，有三个参数（loc, scale, size），代表生成的高斯分布随机数的均值、方差以及输出的 size.","text":"numpy.random.normal 函数，有三个参数（loc, scale, size），代表生成的高斯分布随机数的均值、方差以及输出的 size. 例子： 1np.random.normal(0, 0.05, (7,1)).astype(np.float32) Output : 1234567array([[-0.05229944], [ 0.01754326], [ 0.01764081], [-0.03058357], [-0.05406121], [-0.07284269], [ 0.00289147]], dtype=float32) Scipy Help: numpy.random.normal 12345678910111213Parameters: loc : float or array_like of floatsMean (“centre”) of the distribution.scale : float or array_like of floatsStandard deviation (spread or “width”) of the distribution.size : int or tuple of ints, optionalOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.Returns: out : ndarray or scalarDrawn samples from the parameterized normal distribution. Reference numpy.random.normal","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"numpy","slug":"numpy","permalink":"http://www.iequa.com/tags/numpy/"}]},{"title":"numpy.newaxis 转变矩阵的形狀","slug":"python/numpy-pandas/py-numpy-9-newaxis","date":"2018-09-11T07:37:21.000Z","updated":"2021-06-20T04:12:28.208Z","comments":true,"path":"2018/09/11/python/numpy-pandas/py-numpy-9-newaxis/","link":"","permalink":"http://www.iequa.com/2018/09/11/python/numpy-pandas/py-numpy-9-newaxis/","excerpt":"有一個一維陣列 x1，我分別想要把它變成一個 3*1 的矩陣 x2，以及 1*3 的矩陣 x3，作法如下。","text":"有一個一維陣列 x1，我分別想要把它變成一個 3*1 的矩陣 x2，以及 1*3 的矩陣 x3，作法如下。 1234567891011121314151617181920import numpy as npx1 = np.array([10, 20, 30], float)# 有一個一維陣列x1，我分別想要把它變成一個 3*1 的矩陣x2，以及 1*3 的矩陣x3，作法如下。print(&quot;shape of x1 is &quot;, x1.shape)print(x1)print(&quot;-------------&quot;)x2 = x1[:, np.newaxis]print(&quot;shape of x2 is &quot;, x2.shape)print(x2)print(&quot;-------------&quot;)x3 = x1[np.newaxis, :]print(&quot;shape of x3 is &quot;, x3.shape)print(x3) output: 12345678910shape of x1 is (3,)[ 10. 20. 30.]-------------shape of x2 is (3, 1)[[ 10.] [ 20.] [ 30.]]-------------shape of x3 is (1, 3)[[ 10. 20. 30.]] Reference 利用numpy的newaxis轉變矩陣的形狀","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"newaxis","slug":"newaxis","permalink":"http://www.iequa.com/tags/newaxis/"}]},{"title":"Tensorflow 例子3 ： 建造神经网络","slug":"tensorflow/tf-3.2-create-NN","date":"2018-09-11T06:37:21.000Z","updated":"2021-06-20T04:12:28.324Z","comments":true,"path":"2018/09/11/tensorflow/tf-3.2-create-NN/","link":"","permalink":"http://www.iequa.com/2018/09/11/tensorflow/tf-3.2-create-NN/","excerpt":"这次提到了怎样建造一个完整的神经网络, 包括添加 神经层, 计算误差, 训练步骤, 判断是否在学习.","text":"这次提到了怎样建造一个完整的神经网络, 包括添加 神经层, 计算误差, 训练步骤, 判断是否在学习. 1. add_layer 功能 首先，我们导入本次所需的模块 1234567891011121314151617import tensorflow as tfimport numpy as np# 构造添加一个神经层的函数def add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs TensorFlow随机值: tf.random_normal函数： 将返回一个指定形状的张量，通过随机的正常值填充 tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 2. 导入数据 构建所需的数据。 这里的 x_data 和 y_data 并不是严格的一元二次函数的关系，因为我们多加了一个 noise, 这样看起来会更像真实情况 1234567891011121314x_data = np.linspace(-1,1,300, dtype=np.float32)[:, np.newaxis]# numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)# 得到 300 个大小的一维数组， 通过 [:, np.newaxis] 行变列，变为 300 行，1 列 的二维 数组# numpy.random.normal(loc=0.0 均值, scale=1.0 标准差, size=None 形状)noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)y_data = np.square(x_data) - 0.5 + noise# 利用占位符定义我们所需的神经网络的输入。 `tf.placeholder()` 就是代表占位符# 这里的 `None` 代表无论输入有多少都可以，因为输入只有一个特征，所以这里是 `1`。xs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1]) 接下来，我们就可以开始定义神经层了。 通常神经层都包括 input输入层、hide隐藏层 和 output输出层。这里的输入层只有一个属性， 所以我们就只有一个输入；隐藏层我们可以自己假设，这里我们假设隐藏层有10个神经元； 输出层和输入层的结构是一样的，所以我们的输出层也是只有一层。 所以，我们构建的是——输入层1个、隐藏层10个、输出层1个的神经网络。 3. 搭建网络 12345678910111213141516171819202122# 定义 hide隐藏层， 利用之前的 `add_layer()` 函数，这里使用 Tensorflow 自带的激励函数 `tf.nn.relu`。l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# 接着，定义输出层。此时的输入就是隐藏层的输出 —— `l1`，输入有 10 层（隐藏层的输出层），输出有 1 层。prediction = add_layer(l1, 10, 1, activation_function=None)# 计算预测值 `prediction` 和 真实值的误差，对二者差的平方求和再取平均。loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))# 接下来，是很关键的一步，如何让机器学习提升它的准确率。# `tf.train.GradientDescentOptimizer()` 中的值通常都小于 `1`，这里取的是 `0.1`，代表以 `0.1` 的效率来最小化误差 `loss`。train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)# 使用变量时，都要对它进行初始化，这是必不可少的。init = tf.global_variables_initializer() # 替换成这样就好# 定义 `Session`，并用 `Session` 来执行 `init` 初始化步骤。 #（注意：在 `tensorflow` 中，只有session.run()才会执行我们定义的运算。）sess = tf.Session()sess.run(init) 4. 训练 下面，让机器开始学习。 比如这里，我们让机器学习1000次。机器学习的内容是 train_step, 用 Session 来 run 每一次 training 的数据，逐步提升神经网络的预测准确性。 (注意：当运算要用到 placeholder 时，就需要 feed_dict 这个字典来指定输入。) 1234567for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) # 每 50 步 我们输出一下机器学习的误差。 if i % 50 == 0: print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;)) 在电脑上运行本次代码的结果为： 0.0587868 0.00416427 0.00312624 0.00291327 0.00282026 0.0027577 0.00270546 0.00266943 0.00265278 0.00263559 通过上图可以看出，误差在逐渐减小，这说明机器学习是有积极的效果的 Reference tensorflow.org 莫烦Python Tensorflow 提供的一些 激励函数 利用numpy的newaxis轉變矩陣的形狀","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"Tensorflow 例子3 ： 添加层 def add_layer()","slug":"tensorflow/tf-3.1-add-layer","date":"2018-09-09T01:37:21.000Z","updated":"2021-06-20T04:12:28.326Z","comments":true,"path":"2018/09/09/tensorflow/tf-3.1-add-layer/","link":"","permalink":"http://www.iequa.com/2018/09/09/tensorflow/tf-3.1-add-layer/","excerpt":"在 Tensorflow 里定义一个 添加层的函数， 可以很容易的 添加神经层, 为之后的添加省下不少时间.","text":"在 Tensorflow 里定义一个 添加层的函数， 可以很容易的 添加神经层, 为之后的添加省下不少时间. 定义 add_layer() 神经层里常见的参数通常有 weights、biases 和激励函数。 首先，我们需要导入 tensorflow 模块。 1import tensorflow as tf 然后定义添加神经层的函数 def add_layer(), 它有四个参数：输入值、输入的大小、输出的大小和激励函数，我们设定默认的激励函数是 None。 1def add_layer(inputs, in_size, out_size, activation_function=None): 接下来，我们开始定义 weights 和 biases。 因为在生成初始参数时，随机变量(normal distribution)会比全部为0要好很多，所以我们这里的 weights 为一个 in_size 行, out_size 列的随机变量矩阵。 1Weights = tf.Variable(tf.random_normal([in_size, out_size])) 在机器学习中，biases 的推荐值不为0，所以我们这里是在0向量的基础上又加了0.1。 1biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) 下面，我们定义 Wx_plus_b, 即神经网络未激活的值。其中，tf.matmul() 是矩阵的乘法。 1Wx_plus_b = tf.matmul(inputs, Weights) + biases 当 activation_function 为 None 时，输出就是当前的预测值 Wx_plus_b，不为 None 时，就把 Wx_plus_b 传到 activation_function() 函数中得到输出。 1234if activation_function is None: outputs = Wx_plus_belse: outputs = activation_function(Wx_plus_b) 最后，返回输出，添加一个神经层的函数 def add_layer() 就定义好了。 1return outputs Reference tensorflow.org 莫烦Python Tensorflow 提供的一些 激励函数","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"CNN (week4) - Face recognition & Neural style transfer","slug":"deeplearning/Convolutional-Neural-Networks-week4","date":"2018-09-08T07:00:21.000Z","updated":"2021-06-20T04:12:28.249Z","comments":true,"path":"2018/09/08/deeplearning/Convolutional-Neural-Networks-week4/","link":"","permalink":"http://www.iequa.com/2018/09/08/deeplearning/Convolutional-Neural-Networks-week4/","excerpt":"Face recognition &amp; Neural style transfer 能够在图像、视频以及其他 2D 或 3D 数据上应用这些算法。","text":"Face recognition &amp; Neural style transfer 能够在图像、视频以及其他 2D 或 3D 数据上应用这些算法。 1. What is face recognition? 这一节中的人脸识别技术的演示的确很NB…, 演技不错，😄 2. One Shot Learning 作为老板希望与时俱进，所以想使用人脸识别技术来实现打卡。 假如我们公司只有4个员工，按照之前的思路我们训练的神经网络模型应该如下： 如图示，输入一张图像，经过CNN，最后再通过 Softmax 输出 5 个可能值的大小 (4个员工中的一个，或者都不是，所以共5种可能性)。 看起来好像没什么毛病，但是我们要相信我们的公司会越来越好啊，所以难道公司每增加一个人就要重新训练CNN 及 最后一层的输出数量吗 ？ one-shot： 这显然有问题，所以有人提出了一次学习(one-shot)，更具体地说是通过一个函数来求出输入图像与数据库中的图像的差异度，用 d(img1,img2)d(img1,img2)d(img1,img2) 表示。 如上图示，如果两个图像之间的差异度不大于某一个阈值 τ，那么则认为两张图像是同一个人。反之，亦然。 下一小节介绍了如何计算差值。 3. Siamese Network 注意：下图中两个网络参数是一样的。 先看上面的网络。记输入图像为 x(1)x^{(1)}x(1)，经过卷积层，池化层 和 全连接层 后得到了箭头所指位置的数据 (一般后面还会接上 softmaxsoftmaxsoftmax 层，但在这里暂时不用管)，假设有 128 个节点，该层用 f(x(1))f(x^{(1)})f(x(1)) 表示，可以理解为输入 x(1)x^{(1)}x(1) 的编码。 那么下一个网络同理，不再赘述。 因此上一节中所说的差异度函数即为 d(x(1),x(2))=∣∣f(x(1))−f(x(2))∣∣2d(x^{(1)},x^{(2)})=||f(x^{(1)})-f(x^{(2)})||^2 d(x(1),x(2))=∣∣f(x(1))−f(x(2))∣∣2 问题看起来好像解决了，但感觉还漏了点什么。。神经网络的参数咋确定啊？也就是说 f(x(i))f(x^{(i)})f(x(i)) 的参数怎么计算呢？ 首先可以很明确的是如果两个图像是同一个人，那所得到的参数应该使得 ∣∣f(x(1))−f(x(2))∣∣2||f(x^{(1)})-f(x^{(2)})||^2∣∣f(x(1))−f(x(2))∣∣2 的值较小，反之较大。 4. Triplet Loss 4.1 Learning Objective 这里首先介绍一个三元组，即 (Anchor, Positive, Negative)，简写为(A,P,N) (A,P,N) 三元组 各个含义 Anchor 可以理解为用于识别的图像 （锚） Positive 表示是这个人 Negative 表示不是同一个人 由上一节中的思路，我们可以得到如下不等式： d(A,P)≦d(A,N)d(A,P)\\leqq d(A,N)d(A,P)≦d(A,N), 即 ∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2≦0||f(A)-f(P)||^2-||f(A)-f(N)||^2\\leqq0∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2≦0 (如下图示) 但是这样存在一个问题，即如果神经网络什么都没学到，返回的值是0，也就是说如果 f(x)=0⃗f(x)=\\vec{0}f(x)=0 的话，那么这个不等式是始终成立的。(如下图示) 为了避免上述特殊情况，且左边值必须小于0，所以在右边减去一个变量α，但按照惯例是加上一个值，所以将α加在左边。 综上，所得到的参数需要满足如下不等式 ∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2+α≦0||f(A)-f(P)||^2-||f(A)-f(N)||^2+α\\leqq0 ∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2+α≦0 4.2 Lost function 介绍完三元组后，我们可以对单个图像定义如下的损失函数(如下图示) L(A,P,N)=max(∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2+α,0)L(A,P,N)=max(||f(A)-f(P)||^2-||f(A)-f(N)||^2+α,0) L(A,P,N)=max(∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2+α,0) 解释一下为什么用max函数，因为如果只要满足 ∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2+α≦0||f(A)-f(P)||^2-||f(A)-f(N)||^2+α\\leqq0∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2+α≦0，我们就认为已经正确识别出了图像中的人，所以对于该图像的损失值是 0. 所以总的损失函数是 : J=∑L(A(i),P(i),N(i))J=\\sum{L(A^{(i)},P^{(i)},N^{(i)})}J=∑L(A(i),P(i),N(i)) 要注意的是使用这种方法要保证每一个人不止有一张图像，否则无法训练。另外要注意与前面的 One-shot 区分开来，这里是在训练模型，所以训练集的数量要多一些，每个人要有多张照片。而One-shot是进行测试了，所以只需一张用于输入的照片即可。 4.3 Choosing the triplets(A,P,N) 还有一个很重要的问题就是如何选择三元组 (A,P,N)。因为实际上要满足不等式 d(A,P)+α≦d(A,N)d(A,P)+α\\leqq d(A,N)d(A,P)+α≦d(A,N) 是比较简单的,即只要将 Negative 选择的比较极端便可，比如 Anchor 是一个小女孩，而 Negative 选择一个老大爷。 所以还应该尽量满足 d(A,N)≈d(A,N)d(A,N)\\approx{d(A,N)}d(A,N)≈d(A,N) 5. Face Verification and Binary Classification 通过以上内容，我们可以确定下图中的网络的参数了，那么现在开始进行面部验证了。 上面的是测试图，下面的是数据库中的一张照片 和之前一样假设 f(x(i))f(x^{(i)})f(x(i)) 有 128个节点，之后这两个数据作为输入数据输入到后面的逻辑回归模型中去，即 y^=σ(∑_k=1128w_i∣f(x(i))_k−f(x(j))_k∣+b_i)\\hat{y} = σ(\\sum\\_{k=1}^{128}w\\_i|f(x^{(i)})\\_k-f(x^{(j)})\\_k|+b\\_i) y^​=σ(∑_k=1128w_i∣f(x(i))_k−f(x(j))_k∣+b_i) 若 y^=1\\hat{y}=1y^​=1, 为同一人。反之，不是。 如下图示，绿色下划线部分可以用其他公式替换，即有 y^=σ(∑_k=1128w_i(f(x(i))_k−f(x(j))_k)2f(x(i))_k+f(x(j))_k+b_i)\\hat{y}=σ(\\sum\\_{k=1}^{128}w\\_i \\frac{(f(x^{(i)})\\_k-f(x^{(j)})\\_k)^2}{f(x^{(i)})\\_k+f(x^{(j)})\\_k}+b\\_i) y^​=σ(∑_k=1128w_if(x(i))_k+f(x(j))_k(f(x(i))_k−f(x(j))_k)2​+b_i) 当然数据库中的图像不用每次来一张需要验证的图像都重新计算，其实可以提前计算好，将结果保存起来，这样就可以加快运算的速度了。 6. What is neural style transfer? 7. What are deep ConvNets learning? 第一层只能看到小部分卷积神经. 你选择一个隐藏单元，发现有9个图片，最大化了单元激活，你可能找到类似这样的图片浅层区域. 8. Cost Function 如下图示： 左上角的包含 Content 的图片简称为 C，右上角包含 Style 的简称 S，二者融合后得到的图片简称为 G。 我们知道计算问题须是有限的，所以融合的标准是什么？也就是说 Content 的保留程度和 Style 的运用程度如何取舍呢？ 此时引入损失函数，并对其进行最优化，这样便可得到最优解。 J(G)=αJ_Content(C,G)+βJ_Style(S,G)J(G)=αJ\\_{Content}(C,G)+βJ\\_{Style}(S,G) J(G)=αJ_Content(C,G)+βJ_Style(S,G) J(G)J(G)J(G) 定义用来生成图像的好坏，J_Content(C,G)J\\_{Content}(C,G)J_Content(C,G) 表示 图像CCC 和 图像GGG 之间的差异，J_Style(S,G)J\\_{Style}(S,G)J_Style(S,G) 同理。 计算过程示例： 随机初始化图像 GGG，假设为 100*100*3 （maybe 500*500*3） (如下图右边四个图像最上面那个所示) 使用梯度下降不断优化 J(G)J(G)J(G)。 (优化过程如下图右边下面3个图像所示) 下面一小节将具体介绍 Cost Function 的计算。 9. Content Cost Function 首先假设我们使用 第 lll 层 隐藏层 来计算 J_Content(C,G)J\\_{Content}(C,G)J_Content(C,G)，注意这里的 lll 一般取在中间层，而不是最前面的层，或者最后层 原因如下： 假如取第1层，那么得到的 GGG 图像 将会与 图像CCC 像素级别的相似，这显然不行。 假如取很深层，那么该层已经提取出了比较重要的特征，例如 图像CCC 中有一条狗，那么得到的 图像GGG 会过度的保留这个特征。 然后使用预先训练好的卷积神经网络，如 VGG网络。这样我们就可以得到 图像CCC 和 图像GGG 在第lll层的激活函数值，分别记为 a[l][C],a[l][G]a^{[l][C]},a^{[l][G]}a[l][C],a[l][G] 内容损失函数 J_Content(C,G)=12∣∣a[l][C]−a[l][G]∣∣2J\\_{Content}(C,G) = \\frac{1}{2} || a^{[l][C]} - a^{[l][G]} ||^2J_Content(C,G)=21​∣∣a[l][C]−a[l][G]∣∣2 10. Style Cost Function 10.1 什么是“风格” 要计算风格损失函数，我们首先需要知道“风格(Style)”是什么。 我们使用 lll 层的激活来度量“Style”，将“Style”定义为通道间激活值之间的相关系数。(Define style as correlation between activation across channels) 那么我们如何计算这个所谓的相关系数呢？ 下图是我们从上图中所标识的第 lll 层，为方便说明，假设只有 5 层通道。 如上图示，红色通道和黄色通道对应位置都有激活项，而我们要求的便是它们之间的相关系数。 但是为什么这么求出来是有效的呢？为什么它们能够反映出风格呢？ 继续往下看↓ 10.2 图像风格的直观理解 如图风格图像有 5 层通道，且该图像的可视化特征如 左下角图 所示。 其中红色通道可视化特征如图中箭头所指是垂直条纹，而黄色通道的特征则是橘色背景。","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"TensorFlow 基本用法总结","slug":"tensorflow/tf-2.8-tensorflow-basic-summary","date":"2018-09-08T06:27:21.000Z","updated":"2021-06-20T04:12:28.329Z","comments":true,"path":"2018/09/08/tensorflow/tf-2.8-tensorflow-basic-summary/","link":"","permalink":"http://www.iequa.com/2018/09/08/tensorflow/tf-2.8-tensorflow-basic-summary/","excerpt":"tensorflow 中文文档学习 tensorflow 的基本用法。","text":"tensorflow 中文文档学习 tensorflow 的基本用法。 按照文档说明，一点知识点小总结： 就是 Session() 和 InteractiveSession() 的用法。后者用 Tensor.eval() 和 Operation.run() 来替代了 Session.run(). 其中更多的是用Tensor.eval()，所有的表达式都可以看作是 Tensor. tf 表达式中所有的 var变量 或 constant常量 都应该是 tf 的类型。 只要是声明了 var变量，就得用 sess.run(tf.global_variables_initializer()) 方法来初始化才能用。 1. 平面拟合 通过本例可以看到机器学习的一个通用过程： 准备数据 构造模型（设置求解目标函数） 求解模型 1234567891011121314151617181920212223242526272829303132333435363738394041import tensorflow as tfimport numpy as np# 1.准备数据：使用 NumPy 生成假数据(phony data), 总共 100 个点.x_data = np.float32(np.random.rand(2, 100)) # 随机输入y_data = np.dot([0.100, 0.200], x_data) + 0.300# 2.构造一个线性模型b = tf.Variable(tf.zeros([1]))W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))y = tf.matmul(W, x_data) + b# 3.求解模型# 设置损失函数：误差的均方差loss = tf.reduce_mean(tf.square(y - y_data))# 选择梯度下降的方法optimizer = tf.train.GradientDescentOptimizer(0.5)# 迭代的目标：最小化损失函数train = optimizer.minimize(loss)############################################################# 以下是用 tf 来解决上面的任务# 1.初始化变量：tf 的必备步骤，主要声明了变量，就必须初始化才能用init = tf.global_variables_initializer()# 设置tensorflow对GPU的使用按需分配config = tf.ConfigProto()config.gpu_options.allow_growth = True# 2.启动图 (graph)sess = tf.Session(config=config)sess.run(init)# 3.迭代，反复执行上面的最小化损失函数这一操作（train op）,拟合平面for step in xrange(0, 201): sess.run(train) if step % 20 == 0: print step, sess.run(W), sess.run(b)# 得到最佳拟合结果 W: [[0.100 0.200]], b: [0.300] 12345678910110 [[ 0.27467242 0.81889796]] [-0.13746099]20 [[ 0.1619305 0.39317462]] [ 0.18206716]40 [[ 0.11901411 0.25831661]] [ 0.2642329]60 [[ 0.10580806 0.21761954]] [ 0.28916073]80 [[ 0.10176832 0.20532639]] [ 0.29671678]100 [[ 0.10053726 0.20161074]] [ 0.29900584]120 [[ 0.100163 0.20048723]] [ 0.29969904]140 [[ 0.10004941 0.20014738]] [ 0.29990891]160 [[ 0.10001497 0.20004457]] [ 0.29997244]180 [[ 0.10000452 0.20001349]] [ 0.29999167]200 [[ 0.10000138 0.2000041 ]] [ 0.29999748] 2. 两个数求和 123456789101112input1 = tf.constant(2.0)input2 = tf.constant(3.0)input3 = tf.constant(5.0)intermd = tf.add(input1, input2)mul = tf.multiply(input2, input3)with tf.Session() as sess: result = sess.run([mul, intermd]) # 一次执行多个op print(result) print(type(result)) print(type(result[0])) 123[15.0, 5.0]&lt;class &#x27;list&#x27;&gt;&lt;class &#x27;numpy.float32&#x27;&gt; 3. 变量，常量 3.1 tensorflow 实现计数器 主要是设计了在循环中调用加法实现计数 123456789101112131415161718# 创建变量，初始化为0state = tf.Variable(0, name=&quot;counter&quot;)# 创建一个 op , 其作用是时 state 增加 1one = tf.constant(1) # 直接用 1 也就行了new_value = tf.add(state, 1)update = tf.assign(state, new_value)# 启动图之后， 运行 update opwith tf.Session() as sess: # 创建好图之后，变量必须经过‘初始化’ sess.run(tf.global_variables_initializer()) # 查看state的初始化值 print(sess.run(state)) for _ in range(3): sess.run(update) # 这样子每一次运行state 都还是1 print(sess.run(state)) output: 12340123 3.2 用 tf 来实现对一组数求和，再计算平均 123456789101112131415161718192021222324252627import tensorflow as tfimport numpy as nph_sum = tf.Variable(0.0, dtype=tf.float32)# h_vec = tf.random_normal(shape=([10]))h_vec = tf.constant([1.0,2.0,3.0,4.0])# 把 h_vec 的每个元素加到 h_sum 中，然后再除以 10 来计算平均值# 待添加的数h_add = tf.placeholder(tf.float32)# 添加之后的值h_new = tf.add(h_sum, h_add)# 更新 h_new 的 opupdate = tf.assign(h_sum, h_new)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 查看原始值 print(&#x27;s_sum =&#x27;, sess.run(h_sum)) print(&quot;vec = &quot;, sess.run(h_vec)) # 循环添加 for _ in range(4): sess.run(update, feed_dict=&#123;h_add: sess.run(h_vec[_])&#125;) print(&#x27;h_sum =&#x27;, sess.run(h_sum))# print &#x27;the mean is &#x27;, sess.run(sess.run(h_sum) / 4) # 这样写 4 是错误的， 必须转为 tf 变量或者常量 print(&#x27;the mean is &#x27;, sess.run(sess.run(h_sum) / tf.constant(4.0))) 3.3 只用一个变量来实现计数器 上面的计数器是 TensorFlow 官方文档的例子，但是觉得好臃肿，所以下面这个是更加简单的，只需要定义一个变量和一个 加 1 的操作（op）。通过for循环就能够实现了。 12345678910# 如果不是 assign() 重新赋值的话，每一次 sess.run()都会把 state再次初始化为 0.0state = tf.Variable(0.0, tf.float32)# 通过 assign 操作来改变state的值。add_op = tf.assign(state, state+1)sess.run(tf.global_variables_initializer())print &#x27;init state &#x27;, sess.run(state)for _ in xrange(3): sess.run(add_op) print(sess.run(state)) 1234init state 0.01.02.03.0 这样子和我们平时实现计数器的方法基本上就一致了。我们要重点理解的是， TensorFlow 中通过 tf.assign(ref, value) 的方式来把 value 值赋给 ref 变量。这样子，每一次循环的时候，ref 变量才不会再做定义时候的初始化操作。 4. InteractiveSession() 的用法 InteractiveSession() 主要是避免 Session（会话）被一个变量持有 1234567891011a = tf.constant(1.0)b = tf.constant(2.0)c = a + b# 下面的两种情况是等价的with tf.Session(): # 不用 close() print(c.eval())sess = tf.InteractiveSession()print(c.eval())sess.close() output: 123.03.0 4.1 InteractiveSession()、eval、init 1234567891011121314151617181920212223242526a = tf.constant(1.0)b = tf.constant(2.0)c = tf.Variable(3.0)d = a + bsess = tf.InteractiveSession()sess.run(tf.global_variables_initializer())#################### 这样写是错误的# print(a.run())# print(d.run())##################### 这样才是正确的print(a.eval())print(d.eval())# run() 方法主要用来x = tf.Variable(1.2)# print(x.eval()) # 还没初始化，不能用x.initializer.run() # x.initializer 就是一个初始化的 op， op 才调用run() 方法print(x.eval())sess.close() output: 1231.03.01.2 4.2 使用 tf.InteractiveSession() 来完成上面 求和、平均 的操作呢? 1234567891011121314151617181920212223h_sum = tf.Variable(0.0, dtype=tf.float32)# h_vec = tf.random_normal(shape=([10]))h_vec = tf.constant([1.0,2.0,3.0,4.0])# 把 h_vec 的每个元素加到 h_sum 中，然后再除以 10 来计算平均值# 待添加的数h_add = tf.placeholder(tf.float32)# 添加之后的值h_new = tf.add(h_sum, h_add)# 更新 h_new 的 opupdate = tf.assign(h_sum, h_new)sess = tf.InteractiveSession()sess.run(tf.global_variables_initializer())print(&#x27;s_sum =&#x27;, h_sum.eval())print(&quot;vec = &quot;, h_vec.eval())print(&quot;vec = &quot;, h_vec[0].eval())for _ in range(4): update.eval(feed_dict=&#123;h_add: h_vec[_].eval()&#125;) print(&#x27;h_sum =&#x27;, h_sum.eval())sess.close() output: 1234567s_sum = 0.0vec = [ 1. 2. 3. 4.]vec = 1.0h_sum = 1.0h_sum = 3.0h_sum = 6.0h_sum = 10.0 4.3 使用 feed 来对变量赋值 这些需要用到 feed 来赋值的操作可以通过 tf.placeholder() 说明，以创建占位符。 下面的例子中可以看出 session.run([output], …) 和 session.run(output, …) 的区别。前者输出了 output 的类型等详细信息，后者只输出简单结果。 🌰🌰1：feed 123456input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.multiply(input1, input2)with tf.Session() as sess: print(sess.run([output], feed_dict=&#123;input1:[7.0], input2:[2.0]&#125;)) output: 1[array([ 14.], dtype=float32)] 🌰🌰2： input1:[7.0], input2:[2.0] 1234with tf.Session() as sess: result = sess.run(output, feed_dict=&#123;input1:[7.0], input2:[2.0]&#125;) print(type(result)) print(result) output: 12&lt;type &#x27;numpy.ndarray&#x27;&gt;[ 14.] 🌰🌰3： input1:7.0, input2:2.0 1234with tf.Session() as sess: result = sess.run(output, feed_dict=&#123;input1:7.0, input2:2.0&#125;) print(type(result)) print(result) output: 12&lt;type &#x27;numpy.float32&#x27;&gt;14.0 🌰🌰4： [output], output 1234with tf.Session() as sess: print(sess.run([output], feed_dict=&#123;input1:[7.0, 3.0], input2:[2.0, 1.0]&#125;)) print() print(sess.run(output, feed_dict=&#123;input1:[7.0, 3.0], input2:[2.0, 1.0]&#125;)) output: 123[array([ 14., 3.], dtype=float32)][ 14. 3.] Reference tensorflow.org TensorFlow入门（一）基本用法","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"Activation Function 激励函数","slug":"tensorflow/tf-2.6-B-activation-function","date":"2018-09-07T06:27:21.000Z","updated":"2021-06-20T04:12:28.327Z","comments":true,"path":"2018/09/07/tensorflow/tf-2.6-B-activation-function/","link":"","permalink":"http://www.iequa.com/2018/09/07/tensorflow/tf-2.6-B-activation-function/","excerpt":"Tensorflow 提供的一些 Activation Function tensorflow/api_guides/python/nn","text":"Tensorflow 提供的一些 Activation Function tensorflow/api_guides/python/nn 激励函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经系统。 激励函数的实质是非线性方程。 Tensorflow 的神经网络 里面处理较为复杂的问题时都会需要运用 activation function 。 下面是一个 TensorFlow 搭建的 简单版神经网络 数据流图 : Layer2 展开部分，Layer1 出来的数据，再输入到 Layer2 中 详细介绍请前往 What’s Activation Function Reference tensorflow.org 莫烦Python Tensorflow 提供的一些 激励函数","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"What's Activation Function","slug":"tensorflow/tf-2.6-A-activation-function","date":"2018-09-07T06:15:21.000Z","updated":"2021-06-20T04:12:28.328Z","comments":true,"path":"2018/09/07/tensorflow/tf-2.6-A-activation-function/","link":"","permalink":"http://www.iequa.com/2018/09/07/tensorflow/tf-2.6-A-activation-function/","excerpt":"现代神经网络中 必不可少的一个组成部分, 激励函数, activation function.","text":"现代神经网络中 必不可少的一个组成部分, 激励函数, activation function. 本文大部分内容转载自 莫烦Python /Tensorflow-Tutorial 非线性方程 我们为什么要使用激励函数? 用简单的语句来概括. 就是因为, 现实并没有我们想象的那么美好。激励函数也就是为了解决我们日常生活中 不能用线性方程所概括的问题. 说到线性方程, 我们不得不提到另外一种方程, 非线性方程 (nonlinear function). 我们假设, 女生长得越漂亮, 越多男生爱. 这就可以被当做一个线性问题. 但是如果我们假设这个场景是发生在校园里. 校园里的男生数是有限的, 女生再漂亮, 也不可能会有无穷多的男生喜欢她. 所以这就变成了一个非线性问题.再说…女生也不可能是无穷漂亮的. 这个问题我们以后有时间私下讨论. 然后我们就可以来讨论如何在神经网络中达成描述非线性的任务了. 我们可以把整个网络简化成这样一个式子. Y=WXY = WXY=WX, WWW 就是我们要求的参数, yyy 是预测值, xxx 是输入值. 用这个式子, 我们很容易就能描述刚刚的那个线性问题, 因为 WWW 求出来可以是一个固定的数. 不过这似乎并不能让这条直线变得扭起来 , 激励函数见状, 拔刀相助, 站出来说道: “让我来掰弯它!”. 激励函数 这里的 AF 就是指激励函数. 激励函数拿出自己最擅长的”掰弯利器”, 套在原函数上用力一扭, 原来的 WXWXWX 结果就被扭弯了. 其实这个 AF, 掰弯利器, 也不是什么触不可及的东西. 它其实就是另外一个非线性函数. 比如说relu, sigmoid, tanh. 将这些掰弯利器嵌套在原有的结果之上, 强行把原有的线性结果给扭曲了. 使得输出结果 yyy 也有了非线性的特征. 举个例子, 比如我使用了 relu 这个掰弯利器, 如果此时 WxWxWx 的结果是 1, yyy 还将是 1, 不过 WXWXWX 为 -1 的时候, yyy 不再是 -1, 而会是 0. 你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在 backpropagation 误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去. 常用选择 想要恰当使用这些激励函数, 还是有窍门的. 比如当你的神经网络层只有两三层, 不是很多的时候, 对于隐藏层, 使用任意的激励函数, 随便掰弯是可以的, 不会有特别大的影响. 不过, 当你使用特别多层的神经网络, 在掰弯的时候, 玩玩不得随意选择利器. 因为这会涉及到梯度爆炸, 梯度消失的问题. 因为时间的关系, 我们可能会在以后来具体谈谈这个问题. 最后我们说说, 在具体的例子中, 我们默认首选的激励函数是哪些. 在少量层结构中, 我们可以尝试很多种不同的激励函数. 在卷积神经网络 Convolutional neural networks 的卷积层中, 推荐的激励函数是 relu. 在循环神经网络中 Recurrent neural networks, 推荐的是 tanh 或者是 relu (这个具体怎么选, 我会在以后 循环神经网络 的介绍中在详细讲解). Reference tensorflow.org 莫烦Python Tensorflow 提供的一些 激励函数","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"深度学习与计算机视觉 - 历史回顾与介绍","slug":"deeplearning/deeplearning-CS231n-P1","date":"2018-09-01T12:00:21.000Z","updated":"2021-06-20T04:12:28.251Z","comments":true,"path":"2018/09/01/deeplearning/deeplearning-CS231n-P1/","link":"","permalink":"http://www.iequa.com/2018/09/01/deeplearning/deeplearning-CS231n-P1/","excerpt":"1966年是计算机视觉的诞生年 CVPR、ICCV","text":"1966年是计算机视觉的诞生年 CVPR、ICCV 第一领悟 : Hubel 和 Wiesel 从简单的形状开始 第二领悟 : David Marr 《视觉》 视觉是分层的 深度学习架构的基石 感知分组 ImageNet 2012年是历史性时刻的一年，提出了 CNN . ImageNet 竞赛 2010年 开始举办竞赛 ImageNet 竞赛 2012年 7层 CNN 获胜 ImageNet 竞赛 2015年 151层 CNN 获胜， MS 深度残差网络. 计算机视觉智能比物体识别要更为任重而道远 计算机视觉的愿景是 ： 可以看图讲故事 Reference 网易云课堂 - deeplearning DeepLearning.ai学习笔记汇总 斯坦福CS231N课程学习笔记（一）.课程简介与准备 李飞飞CS231n2017课程双语字幕版上线 !（附课程链接）","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"http://www.iequa.com/tags/CS231n/"}]},{"title":"Convolutional Neural Networks (week3) - Object detection","slug":"deeplearning/Convolutional-Neural-Networks-week3","date":"2018-09-01T07:00:21.000Z","updated":"2021-06-20T04:12:28.266Z","comments":true,"path":"2018/09/01/deeplearning/Convolutional-Neural-Networks-week3/","link":"","permalink":"http://www.iequa.com/2018/09/01/deeplearning/Convolutional-Neural-Networks-week3/","excerpt":"知道如何将卷积网络应用到视觉检测和识别任务。 知道如何使用神经风格迁移生成艺术。","text":"知道如何将卷积网络应用到视觉检测和识别任务。 知道如何使用神经风格迁移生成艺术。 1. Object Localization 这一小节视频主要介绍了我们在实现目标定位时标签该如何定义 输出 softmax 的同时，在输出4个值 b_x,b_y,b_w,b_hb\\_x,b\\_y,b\\_w,b\\_hb_x,b_y,b_w,b_h, 边界框的具体位置 （图片左上角坐标为 (0, 0)， 右下角为 (1, 1) ) 在上面的例子中，b_xb\\_xb_x 的值大约为 0.5，b_yb\\_yb_y 的值大约为 0.7，最佳位置. how we define the target label yyy for this as a supervised learning task. 上图左下角给出了损失函数的计算公式 (这里使用的是平方差) 注意: 我们假设图像中存在这三者 (pedestrian，car，motorcycles) 中的一种 或者 都不存在，所以共有四种可能. P_c=1P\\_c=1P_c=1 表示有三者中的一种 C_1=1C\\_1=1C_1=1 表示有 pedestrian，反之没有 C_2=1C\\_2=1C_2=1 表示有 car C_3=1C\\_3=1C_3=1 表示有 motorcycles b\\_\\* 用于标识所识别食物的位置 b_x,b_yb\\_x,b\\_yb_x,b_y: 表示识别物体的中心坐标 b_w,b_hb\\_w,b\\_hb_w,b_h: 表示识别物体的宽和高 注意: P_c=0P\\_c=0P_c=0 表示三者都没有，所以此时 C\\_\\*,b\\_\\* 的值我们并不在乎了. 为了让大家便于了解对象定位的细节，这里我用平方误差简化了描述过程. (实际应用中 P_cP\\_cP_c 更多是应用逻辑回归函数) 2. Landmark Detection 特征点检测，这一节的内容和上一节感觉很类似. 要输出眼角的位置，你可以让神经网络输出的最后一层，多输出 2 个数字 l_x,l_yl\\_x, l\\_yl_x,l_y. 假设 脸部 有 64 个特征点. 具体做法是准备一个 CNN 和 一些关键特征集. 将人脸图片输入 CNN. 输出 1 或者 0. 1 表示有 人脸。 这里一共有 128 + 1 个输出单元，因为有 64 个特征 64 * 2， 来实现对人脸的检测和定位. Snapchat 应用，AR (Augmented Reality) 增强现实 Filter 有所了解，Snapchat Filter 实现了在人脸上画皇冠 还有一些其他效果。检测脸部特征也是计算机图形学的关键构造模块. 最后一个例子是，检测人体姿态. 3. Object Detection 假设你想构造一个汽车检测算法: 首先，创建一个标签训练集 Training Set， 也就是 xxx 和 yyy 选定一个大小确定的窗口，输入 CNN，CNN 开始预测红色方框中是否有汽车. 然后，滑动窗口继续, 红色方框稍向右滑动之后的区域, 处理下一个，输入 CNN… 依次重复操作…, 知道这个红色的窗口滑过每一个角落。 如果上面到最后角落还是没有检测到汽车， 那么选用更大的窗口，然后还是以固定步长…依次重复…滑过…检测 这个算法叫做 : 滑动窗口目标检测 目标检测常使用的是滑动窗口技术检测，即使用一定大小的窗口按照指定的步长对图像进行遍历 因为图像中车辆的大小我们是不知道的，所以可以更改窗口大小，从而识别并定位出车辆的位置。 滑动窗口目标检测 算法的缺点就是计算成本的问题. 如果你把窗口调大，显然会减少输入CNN的窗口个数，但是粗粒度可能会影响性能. 如果采用小粒度，那么输入给CNN的窗口个数就会特别多。这意味着超高计算成本. 人们通常采用更简单的分类器做对象检测. 比如简单的线性分类器. 计算成本就会很低. 滑动窗口算法表现良好. 然而 CNN 运行单个分类任务的成本确高得多。 这样的滑动窗口太慢了。 庆幸的是，人们已经找到了方法解决计算成本，下一节见. 4. Convolutional Implementation of Sliding Windows 注意：该节视频的例子和上一节一样，都是识别图像中是否有 pedestrian，car，motorcycles，background，所以最后输出y是 4个节点 4.1 全连接层→卷积层 在介绍卷积滑动窗口之前我们首先要知道如何把神经网络的全连接层转化成卷积层，下面是使用了全连接层的网络结构 那么如何将全连接层转化成卷积层呢？如下图示 我们可以看到经过 Max Pooling 之后的数据大小是 (5, 5, 16), 第一个FC层是 400 个节点。我们可以使用 400 个 5*5 的过滤器进行卷积运算，随后我们就得到了 (1, 1, 400) 的矩阵。 第二个 FC层 也是 400 个节点，由之前的 1*1 过滤器的特点，我们可以使用 400 个 1*1 的过滤器，也可以得到 (1,1,400) 的矩阵。至此，我们已经成功将全连接层转化成了卷积层。 以上就是用卷积层代替全连接层的过程， 下面再看如何通过卷积实现滑动窗口对象检测算法，借鉴 OverFeat Paper。 4.2 卷积滑动窗口实现 目标检测一节中介绍了滑动窗口。要实现窗口遍历，那么就需要很大的计算量，But 大神们自然已经有了解决办法。 注意: 下面的第一幅图，Andrew Ng 为了方便只花了平面图，就没有画出3D的效果了。 首先我们先看下图，这就是上面提到的将全连接层转化成卷积层的示意图: 下面，假设我们的测试图大小是 16*16，并令滑动窗口大小是 14*14 的 (为了方便理解，下图用蓝色清楚地表明了 14*14 窗口的大小), 步长是 2，所以这个测试图可以被窗口划分成 4 个部分。随后和上面执行一样的操作，最后可以得到 (2,2,4) 的矩阵（这样发现很多计算部分是重复的），此时我们不难看出测试图被滑动窗口选取的左上角部分对应的结果也是输出矩阵的左上角部分，其他3个部分同理。 所以这说明了什么？ 说明我们没有必要用滑动窗口截取一部分，然后带入卷积网络运算。相反我们可以整体进行运算，这样速度就快很多了。 下图很清楚的展示了卷积滑动窗口的实现。我们可以看到图片被划分成了 64 块 目前这个算法还有一个缺点，就是边界框的位置可能不够准确，下一节，我们将学习解决这个缺点。 5. Bounding Box Predictions 上面介绍的滑动窗口方法存在一个问题就是很多情况下滑动窗口并不能很好的切割出车体，如下图示： 上面中的蓝色框可能是滑动窗口的时候，最佳的匹配位置了，但是其实我们人看后，其实很明显发现，这并不是，这就是我们要解决的问题所在. 其实最佳的匹配位置应该是红色的框，稍微有点长方形，长宽比有点向水平方向延伸. 为了解决这个问题，就有了 YOLO (you only look once) 算法，即只需要计算一次便可确定需要识别物体的位置的大小。 原理如下： 首先将图像划分成 3*3 (即9份)，每一份最后由一个向量表示，这个向量在本文最前面介绍过，即 y=[P_c,b_x,b_y,b_h,b_w,c_1,c_2,c_3]y=[P\\_c,b\\_x,b\\_y,b\\_h,b\\_w,c\\_1,c\\_2,c\\_3] y=[P_c,b_x,b_y,b_h,b_w,c_1,c_2,c_3] 如果一个格子中有两个对象，那么就当做这个格子中不存在对象. 按照 P_c=0P\\_c = 0P_c=0 处理. (如果是 19 * 19 的情况，两个对象的中点分配到一个格子的情况，其实特别低) 因为有 9 份，所以最后输出矩阵大小是 (3,3,8) , 如下图示： 那么如何构建卷积网络呢？ 输入矩阵是 (100, 100, 3), 然后是 Conv，Maxpool 层，……，最后只要确保输出矩阵大小是 (3,3,8) 即可。 这是一个卷积实现，你并没有在 3 * 3 网格上跑 9 次算法，19 * 19 同样你不需要网格上跑 361 次，相反这是单次卷积的实现，但你使用了一个卷积网络，有很多共享计算步骤，比如在处理这 3 * 3 计算中很多计算步骤是共享的. 所以这个算法的效率很高. YOLO 算法有一个好处是，它的计算特别快，可以达到实时识别。 下图是以右边的车辆作为示例介绍该车辆所在框的输出矩阵 很显然 P_c=1P\\_c=1P_c=1 然后 b_x,b_yb\\_x,b\\_yb_x,b_y 的值是右边车辆的中心点相对于该框的位置,所以它们的值是一定小于 1 的，我们可以很容易的得到近似值 b_x=0.4,b_y=0.3b\\_x=0.4, b\\_y=0.3b_x=0.4,b_y=0.3 b_h,b_wb\\_h,b\\_wb_h,b_w 的值同理也是车辆的宽高相对于其所在框的比例，但是要注意的是这两个值是可以大于 1 的，因为有可能部分车身在框外。但是也可以使用 sigmoid函数 将值控制在 1 以内。 这里我用 3 * 3 的网格说明，在实践中可能 19 * 19 的网格会精细的多. 避免把 多个 对象分配到同一个格子中，观察这个对象的中点，然后将图像对象分配到其中点所在的格子. 19 * 19 的网格, 两个对象的中点处于同一个格子的概率会更低. 上面吴大大给出的是一个合理的约定，使用起来应该没什么问题. 其实还有其他更复杂的参数化形式. YOLO 的论文是相对难度较高的论文, Andrew Ng 看的时候也看不懂很多. 6. Intersection Over Union 如何评价对象检测算法呢，IOU 交并比函数可以用来评价对象检测算法。 前面说到了实现目标定位时可能存在 滑动窗口 与 真实边框 存在出入，如下图示： 红色框是车身边界，紫色框是滑动窗口，那么此窗口返回的值是有车还是无车呢？ 为了解决上面的问题引入了交并比(IoU)，也就是两个框之间的交集与并集之比，依据这个值可以评价定位算法是否精准。 上图黄色区域表示紫色框和红色框的交集，绿色区域表示紫色框和红色框的并集，交并比(IoU)就等于 黄色区域大小 比上 绿色区域大小。 如果 IoU≥0.5IoU\\geq0.5IoU≥0.5，则表示紫色框中有车辆，反之没有。 当然 0.5 这个阈值是人为设定的，没有深入的科学探究，所以如果希望结果更加精确，也可以用 0.6 或 0.7 设为阈值，但是不建议用小于 0.5 的阈值。 下一节讨论这个 Non-max Suppression 工具，可以让 YOLO 工作的更好 7. Non-max Suppression 非极大值抑制可以确保你对每个对象只检测一次。 算法大致思路 前面 Bounding Box 一节中介绍到将图片划分成若干等分，例如 3*3，那么一共就有9块，如下图示，我们可以很清楚的看到第二行第一块和第三块都有车，所以可以标出一个中心点坐标 (b_x,b_y)(b\\_x,b\\_y)(b_x,b_y)，这样我们就能通过最终的输出结果知道这两个框中有车。 但是如果我们划分的数量变多之后呢？如下图示划分成了 19*19，图中标出的 3 个黄框和 3 个绿框最终结果都会都会返回[P_x=1,b_x=,b_y=……P\\_x=1,b\\_x=,b\\_y=……P_x=1,b_x=,b_y=……]，但是最后我们该信谁的呢？是这三个框真的有车，而且还不是同一辆车？还是只是同一辆车？所以就有了非极大值抑制来解决这个问题。 Non-max Suppression 做的就是清理这些检测结果，这样一辆车只检测一次，而不是每辆车都出发多次检测。 其思路大致如下 (为了方便说明和理解，我们不使用 19*19 的方框)： 首先每个框会对是否有目标返回一个 P_cP\\_cP_c 的概率值 (也可以是 P\\_c\\*C\\_1\\*C\\_2\\*C\\_3 的概率之积)，如下图示： 然后找到 P_cP\\_cP_c 最大的一个框，显然 0.9 的框有车的概率最大，所以该边框颜色高亮 然后算法遍历其他边框，找出与上一个边框的交并比大于 0.5 的边框，很显然右边剩余两个边框符合条件，所以这两个边框变暗 左边的车同理，不加赘述 下面结合一个例子总结一下 非极大值抑制 算法的实现步骤： 在这里假设只需要识别定位车辆即可，所以输出格式为 P_c,b_x,b_y,b_h,b_wP\\_c,b\\_x,b\\_y,b\\_h,b\\_wP_c,b_x,b_y,b_h,b_w 这个例子中将图像划分成 19*19 方格，假设每个方格都已经计算出 P_cP\\_cP_c 的概率值 去掉所有满足 P_c≤0.6P\\_c ≤ 0.6P_c≤0.6 的方格 (0.6也可以进行人为修改) 对剩下的方格进行如下循环操作： 从剩下的方格中选取 P_cP\\_cP_c 最大的一个作为预测值输出，假设这个方格为 AAA 将与 AAA 方格交并比大于 0.5 的剔除 8. Anchor Boxes 前面介绍了那么多，都只是识别单个物体，如果要同时识别多个物体该怎么办呢？而且识别的不同物体的中心点在同一个框中又该怎么呢 (如下图示，人和车的中心都在红点位置，处于同一个框中)？这时就需要使用 Anchor Boxes 了。 Anchor Boxes 思路是对于不同物体事先采用不同的框，例如人相对于车属于瘦高的，所以使用下图中的 Anchor Box 1，相反车辆就使用** Anchor Box 2**. 之前的输出值的格式都是 y=[P\\_x,b\\_x,b\\]_y,b\\_h,b\\_w,C\\_1,C\\_2,C\\_3]，最后输出的矩阵大小(以该图为例) 是 (3,3,8), 但是这样只能确定一个物体。 所以为了同时检测不同物体，很自然的我们可以重复输出这个上面的值即可，即 y=[P_x,b_x,b_y,b_h,b_w,C_1,C_2,C_3,P_x,b_x,b_y,b_h,b_w,C_1,C_2,C_3]y = [P\\_x, b\\_x, b\\_y, b\\_h, b\\_w, C\\_1, C\\_2, C\\_3, P\\_x, b\\_x, b\\_y, b\\_h, b\\_w, C\\_1, C\\_2, C\\_3]y=[P_x,b_x,b_y,b_h,b_w,C_1,C_2,C_3,P_x,b_x,b_y,b_h,b_w,C_1,C_2,C_3], 所以输出矩阵是 (3,3,16), 也可以是(3,3,2,8)。 要注意的是我们需要提前设定好输出值前面的值对应 Anchor Box 1，后面的对应 Anchor Box 2. 例如我们得到了图中人的边框信息值，然后经过计算发现其边框与 Anchor Box 1更为接近，所以最后将人的边框信息对应在前面，同理车辆边框信息对应在后面。 总结起来 Anchor Box算法 和 之前的算法区别如下： 之前的算法： 对于训练集图像中的每个对象，都根据那个对象的中点位置分配到对应的格子中,所以在上面的示例中输出y就是(3,3,8) Anchor Boxes 算法 现在每个对象都和之前一样分配到同一个格子中, 即对象中心所在的格子。不同的是也需要分配到和对象形状交并比最高的 Anchor Box。 例如下图中的红色框不仅要分配到其中心所在的图像上的格子中，而且还需要分配到与其交并比最大的 Anchor Box 中，即竖条的紫色方格。 回到本小节最开始的例子，最后的输出值如下图示： 图中人的对应 Anchor Box 1， 输出值对应图中的黄色字体；车辆同理，对应绿色字体 如果两个对象所在一个格子，且两个对象圈出来的框也是一样的，这种情况非常非常少见，我们用其他方法来特殊处理. 9. YOLO Algorithm YOLO Algorithm, that also encompasses many of the best ideas across the entire computer vision literature the relate to object detection. YOLO 计算机视觉对象检测领域文献中最精妙的思路。 10. Region Proposals (Optional) 候选区域，这些目前工作其实用的很少，但是还是非常有意义的，所以我作为可选视频可学习.​​​​ 吴大大认为 YOLO 这种只看一次的算法，长远而言是CV领域更有希望的方向 Reference 网易云课堂 - deeplearning DeepLearning.ai学习笔记汇总","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Python 中 4 个魔法方法， __len__,__getitem__,__setitem__,__iter__","slug":"python/language/py-language-9-class","date":"2018-09-01T02:00:21.000Z","updated":"2021-06-20T04:12:28.225Z","comments":true,"path":"2018/09/01/python/language/py-language-9-class/","link":"","permalink":"http://www.iequa.com/2018/09/01/python/language/py-language-9-class/","excerpt":"python中除了可以使用内建的类型，如 list,tuple,dict，还可以创建自己的对象来实现像这些内建类型的访问 不过需要在定义类的时候对这些魔法方法实现: __len__, __getitem__, __setitem__, __iter__","text":"python中除了可以使用内建的类型，如 list,tuple,dict，还可以创建自己的对象来实现像这些内建类型的访问 不过需要在定义类的时候对这些魔法方法实现: __len__, __getitem__, __setitem__, __iter__ Reference python的迭代器为什么一定要实现__iter__方法？ python liaoxuefeng","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Tensorflow 入门例子","slug":"tensorflow/tf-2.2-example","date":"2018-08-27T03:27:21.000Z","updated":"2021-06-20T04:12:28.324Z","comments":true,"path":"2018/08/27/tensorflow/tf-2.2-example/","link":"","permalink":"http://www.iequa.com/2018/08/27/tensorflow/tf-2.2-example/","excerpt":"Tensorflow 是非常重视结构的, 我们建立好了神经网络的结构, 才能将数字放进去, 运行这个结构.","text":"Tensorflow 是非常重视结构的, 我们建立好了神经网络的结构, 才能将数字放进去, 运行这个结构. 这个例子简单的阐述了 tensorflow 当中如何用代码来运行我们搭建的结构. 1. 创建数据 首先, 我们这次需要加载 tensorflow 和 numpy 两个模块, 并且使用 numpy 来创建我们的数据. 123456import tensorflow as tfimport numpy as np# create datax_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.1 + 0.3 接着, 我们用 tf.Variable 来创建描述 y 的参数. 我们可以把 y_data = x_data*0.1 + 0.3 想象成 y=Weights * x + biases, 然后神经网络也就是学着把 Weights 变成 0.1, biases 变成 0.3. 2. 搭建模型 1234Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))biases = tf.Variable(tf.zeros([1]))y = Weights*x_data + biases 3. 计算误差 接着就是计算 y 和 y_data 的误差: 1loss = tf.reduce_mean(tf.square(y-y_data)) 4. 传播误差 反向传递误差的工作就教给 optimizer 了, 我们使用的误差传递方法是梯度下降法: Gradient Descent 然后我们使用 optimizer 来进行参数的更新. 12optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss) 5. 训练 到目前为止, 我们只是建立了神经网络的结构, 还没有使用这个结构. 在使用这个结构之前, 我们必须先初始化所有之前定义的 Variable, 所以这一步是很重要的! 1init = tf.global_variables_initializer() # 替换成这样就好 接着,我们再创建会话 Session. 我们会在下一节中详细讲解 Session. 我们用 Session 来执行 init 初始化步骤. 并且, 用 Session 来 run 每一次 training 的数据. 逐步提升神经网络的预测准确性. 1234567sess = tf.Session()sess.run(init) # Very importantfor step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases)) 6. Reference tensorflow.org 莫烦Python","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"Tensorflow 处理结构","slug":"tensorflow/tf-2.1-structure","date":"2018-08-25T03:17:21.000Z","updated":"2021-06-20T04:12:28.329Z","comments":true,"path":"2018/08/25/tensorflow/tf-2.1-structure/","link":"","permalink":"http://www.iequa.com/2018/08/25/tensorflow/tf-2.1-structure/","excerpt":"Tensorflow 首先要定义神经网络的结构, 然后再把数据放入结构当中去运算 和 training.","text":"Tensorflow 首先要定义神经网络的结构, 然后再把数据放入结构当中去运算 和 training. 计算图 因为 TensorFlow 是采用数据流图（data flow graphs）来计算, 所以首先我们得创建一个数据流图, 然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算. Nodes 在图中表示数学操作 Edges 在图中则表示在节点间相互联系的多维数据数组，即张量（tensor） 训练模型时 tensor 会不断的从数据流图中的一个节点 flow 到另一节点, 这就是 TensorFlow 名字的由来. Tensor 张量意义 张量（Tensor): 张量有多种. 零阶张量为 纯量或标量 (scalar) 也就是一个数值. 比如 [1] 一阶张量为 向量 (vector), 比如 一维的 [1, 2, 3] 二阶张量为 矩阵 (matrix), 比如 二维的 [[1, 2, 3],[4, 5, 6],[7, 8, 9]] 以此类推, 还有 三阶 三维的 … Reference tensorflow.org","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"Convolutional Neural Networks (week2) - deep CNN","slug":"deeplearning/Convolutional-Neural-Networks-week2","date":"2018-08-24T12:00:21.000Z","updated":"2021-06-20T04:12:28.260Z","comments":true,"path":"2018/08/24/deeplearning/Convolutional-Neural-Networks-week2/","link":"","permalink":"http://www.iequa.com/2018/08/24/deeplearning/Convolutional-Neural-Networks-week2/","excerpt":"理解如何搭建一个神经网络，包括最新的变体，例如残余网络。","text":"理解如何搭建一个神经网络，包括最新的变体，例如残余网络。 1. Why look at case studies? 通过他人的实例可以更好的理解如何构建卷积神经网络，本周课程主要会介绍如下网络 LeNet-5 AlexNet VGG ResNet (有152层) Inception 2. Classic networks 2.1 LeNet-5 该网络 1980s 提出，主要针对灰度图像训练的，用于识别手写数字。 当时很少用到 Padding，所以看到随着网络层次增加，图像的高度和宽度都是逐渐减小的，深度则不断增加. 当时人们会更倾向于使用 Average Pooling，但是现在则更推荐使用 Max Pooling. 最后的预测没有使用 softmax，而是使用了一般的方法. 论文中你会发现，过去人们使用 Sigmoid函数 和 Tanh函数，而不是 ReLu， 这种网路结构的特别之处还在于各网络层之间是有关联的. 2.2 AlexNet AlexNet 其实和 LetNet-5 有很多相似的地方，如大致的网络结构。不同的地方主要有如下： 激活函数使用的是 Relu，最后一层使用的是 Softmax 参数更多，有6000万个参数，而 LeNet-5 只有6万个左右 使用 Max Pooling Local Response Normalization 局部响应归一化 - LRN层，不重要划掉. 这篇论文之后，深度学习逐渐在 CV 方面的关注，与日俱增. AlexNet 比较复杂，包含大量超参数. 2.3 VGG-16 这个网络太牛了，因为它有将近 1.38亿个参数，即使放到现在也是一个很大的网络，但是这个网络的结构并不复杂。下面主要介绍一下上图网络。 首先该网络使用的是 Same卷积，即保证高度和宽度不变，另外因为总共有16层卷积操作，所以就不把每一层都用图像的方式表现出来了，例如 [CONV 64 X2] 表示的是用 64个 过滤器进行 Same卷积 操作2次，即右上角所画的示意图，(224,224,3) -&gt; (224,224,64) -&gt; (224,224,64) Andrew Ng : 我最喜欢它的一点是，随着网络的加深，图像的 Height 和 Width 都在以一定的规律不断缩小，每次池化之后刚好缩小一半，而信道数量在不断增加. 而刚好也是在每组卷积操作后增加一倍. 也就是说 ： 图像缩小的比例和信道增加的比例是有规律的. 上面三个是比较经典的网络，可阅读其论文，Ng吴大师 建议的阅读顺序是 AlexNet-&gt;VGG-&gt;LeNet。 3. Residual Network (ResNets) ResNets 发明者是 何恺明、张翔宇、任少卿、孙剑 吴大师表示 “非常深的网络是很难训练的，因为存在梯度消失和梯度爆炸的问题”，为了解决这个问题，引入了 Skip Connection (跳远链接)，残差网络正是使用了这个方法。 3.1 残差块 (Residual Block) 首先介绍组成残差网络的单元：残差块(Residual Block)，如下图示： 残差块是由两层网络节点组成的, a[l]a^{[l]}a[l] 经过线性变化，再通过Relu激活函数后得到 a[l+1]a^{[l+1]}a[l+1]， a[l+2]a^{[l+2]}a[l+2] 也同理，具体过程如下图示： 特别注意上图中的紫色线连接，a[l]a^{[{l}]}a[l] 通过这条线直接将数据传递给 a[l+2]a^{[l+2]}a[l+2]， 所以 a[l+2]=g(z[l+1]+a[l])a^{[l+2]}=g(z^{[l+1]}+a^{[l]})a[l+2]=g(z[l+1]+a[l]) ，这条紫色线也叫作short cut(或skip connection) 3.2 残差网络 如图示，残差网络每两层网络节点组成一个残差块，这也就是其与普通网络(Plain Network)的差别。 随着网络深度的加深，优化算法会越来越难训练，训练错误会越来越多，但是有了 ResNets 就不一样了. 也可以在一定程度上缓解梯度消失和梯度爆炸问题. 另一角度，网络越深会比较臃肿，但是 ResNet 确实在训练深度网络方面非常有效. 结合之前的课程我们知道如果使用普通网络训练模型，训练误差会随着网络层次加深先减小，而后会开始增加，而残差网络则不会有这种情况，反而它会随着层次增加，误差也会越来越小，这与理论相符。 随着网络深度的加深，优化算法会越来越难训练，训练错误会越来越多，但是有了 ResNets 就不一样了. 也可以在一定程度上缓解梯度消失和梯度爆炸问题. 另一角度，网络越深会比较臃肿，但是 ResNet 确实在训练深度网络方面非常有效. 4. Why ResNets work 吴大表示: 网络在训练集上表现好，才能 Hold-out 交叉验证集 或 dev集、测试集上表现好，所以训练集上表现好是第一步. 为了直观解释残差网络为什么有用，假设我们已经通过一个很大的神经网络得到了 a[l]a^{[l]}a[l]。 而现在我们又需要添加两层网络进去，我们看看如果添加的是残差块会有什么效果。如下图示： 由 残差块Residual Block 的特点我们知道 a[l+2]=g(z[l+1]+a[l])=g(W[l+1]a[l]+b[l+1]+a[l])a^{[l+2]}=g(z^{[l+1]}+a^{[l]})=g(W^{[l+1]}a^{[l]}+b^{[l+1]}+a^{[l]})a[l+2]=g(z[l+1]+a[l])=g(W[l+1]a[l]+b[l+1]+a[l]) 我们先考虑一个极端情况，即 W[l+1]=0,b[l+1]=0W^{[l+1]}=0,b^{[l+1]}=0W[l+1]=0,b[l+1]=0， 那么 a[l+2]=g(a[l])=a[l]a^{[l+2]}=g(a^{[l]})=a^{[l]}a[l+2]=g(a[l])=a[l] (因为激活函数是Relu)，所以在添加了额外的两层网络后，即使最坏情况也是保持和之前结果一样。(而如果只是加上普通的两层网络，可能结果会更好，但是也很有可能结果会越来越糟糕, 因为普通网络就算是选择用来学习恒等函数的参数都很困难)，残差网络起作用的主要原因是这些残差块学习恒等函数非常容易, 这也就是为什么残差网络能够保证深度网络依旧有用的原因了。 注意 ： 各层网络的维度，因为 a[l+2]=g(z[l+1]+a[l])a^{[l+2]}=g(z^{[l+1]}+a^{[l]})a[l+2]=g(z[l+1]+a[l]), 那么就要求 z[l+1]z^{[l+1]}z[l+1] 要和 a[l]a^{[l]}a[l] 保持相同的维度所以残差网络使用的是Same卷积。 但是如果唯独不一样也没关系，可以给 a[l]a^{[l]}a[l] 乘上一个 W_sW\\_sW_s 来保持相同维度。 W_sW\\_sW_s 的值可以通过学习获得 普通网络 和 ResNets 常用的结构是 Conv -&gt; Conv -&gt; Conv -&gt; Pool -&gt; Conv -&gt; Conv -&gt; Conv -&gt; Pool 依次重复之… 直到有一个通过 Softmax 进行预测的全连接层. 5. in Network and 1×1 convolutions 1 \\* 1 卷积乍看起来好像很没用，如下图上，​但是如果这个 1 \\* 1 的卷积有深度呢？​ 说个更加直观的理解就是使用 1\\*1 卷积可以很方便的减少深度，而不改变高度和宽度，如下图所示： 只需要用 32 个 (1\\*1\\*192) 的 Filter 即可, 如果不用 1 \\* 1 卷积，例如采用 2\\*2 卷积,要想实现只改变深度，那么还需要使用 padding，相比起来更加麻烦了. 6. Inception network motivation 如上图示，我们使用了各种过滤器，也是用了 Max Pooling。但是这些并不需要人工的选择其个数，这些都可以通过学习来确定下来。所以这种方法很好的帮助我们选择何种 Filter 的问题，这也就是 Inception网络。 6.1 计算成本 注意随之而来的计算成本，尤其是 5\\*5 的 Filter，下面以这个 Filter 举例进行说明： 如上图示，使用 32 个 (5\\*5\\*192) 的 Filter，对 (28,28,192)(28,28,192)(28,28,192) 进行 Same卷积 运算得到 (28,28,32)(28,28,32)(28,28,32) 的输出矩阵，该卷积需要执行的乘法运算有多少次呢？ 输出矩阵中的一个数据是经过 (5\\*5\\*192) 次乘法得到的，那么总共的乘法运算次数则是 (5\\*5\\*192\\*28\\*28\\*32=1.2) 亿 6.2 瓶颈层(Bottleneck layer) 上面运算次数多大1.2亿次，运算量相当大，因此有另一种网络结构对此进行优化，且可以达到同样效果，即采用 1 * 1 卷积 如图示进行了两次卷积，我们计算一下总共的乘法次数。 第一次卷积：28 * 28 * 16 * 192 = 2.4 million 第二次卷积：28 * 28 * 32 * 5 * 5 * 16 = 10 million 总共乘法次数是 12.4 million，这与上面直接用 5 * 5 Filter 的运算次数整整少了十倍。 7. Inception network 为了可以防止过拟合，还有个特别的 inception network，是一个 Google 员工开发的叫做 GoogLeNet，这个名字是为了向 LeNet 致敬. 这样非常好，深度学习的研究人员如何重视协作，深度学习工作者对彼此的工作成果，都有一种强烈的敬意. 8. Using open-source impl Practical advice for using ConvNets 9. Transfer Learning 简单说就是在他人的基础上实现自己想要的模型，举个🌰，假如我们现在需要识别家里养的两只猫，分别叫 小花 和 小白，但是我们只有比较少的图片。幸运的是网上已经有一个已经训练好的模型，是用来区分1000个不同事物的(包括猫)，其网络模型如下： 我们的需求是最后结果有三种：是 小花，or 小白，or 都不是。​所以需要对 softmax 做如下图修改. 由于数据较少，所以可以对他人的模型的前面的结构中的参数进行冻结，即 权重 weight 和 偏差 bias 不做改动。 ​当然，如果我们有一定量的数据，那么 freeze 的范围也可以随之减少，即拿来训练的层次可以增多 ​ ​ you find that for a lot of computer vision applications, you just do much better if you download someone else’s open source weights and use that as initialization for your problem. I think that computer vision is one where transfer learning is something that you should almost always do. （unless you actually have a very very large data set…） 10. Data augmentation 10.1 Common augmentation 旋转(rotation) 修剪(shearing) 局部变形(local warping) 镜像(mirroring) ​ 以上介绍的方法，同时使用并没有什么坏处，但是在实践中，因为太复杂了，所以使用的很少。 更经常使用的方法可能下面要介绍的 Color shifting 。 10.2 Color shifting 我们都知道图像是由 RGB 三种颜色构成的，所以该数据扩充方法常采用 PCA color augmentation，即假如一个图片的 R 和 G 成分较多，那么该算法则 R,G 的值会减少很多，而 B 的值变化会少一些，所以使得总体的颜色保持一致. ​ 如果你看不懂这些，那么没关系，可以看看 AlexNet 论文中的细节，你也能找到 PCA 颜色增强的开源实现方法. 11. The state of CV ​ ​ ​ ​ Reference 网易云课堂 - deeplearning DeepLearning.ai学习笔记汇总","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Convolutional Neural Networks (week1) - CNN","slug":"deeplearning/Convolutional-Neural-Networks-week1","date":"2018-08-21T02:00:21.000Z","updated":"2021-06-20T04:12:28.251Z","comments":true,"path":"2018/08/21/deeplearning/Convolutional-Neural-Networks-week1/","link":"","permalink":"http://www.iequa.com/2018/08/21/deeplearning/Convolutional-Neural-Networks-week1/","excerpt":"理解如何搭建一个神经网络，包括最新的变体，例如残余网络。 知道如何将卷积网络应用到视觉检测和识别任务。 知道如何使用神经风格迁移生成艺术。 能够在图像、视频以及其他2D或3D数据上应用这些算法。","text":"理解如何搭建一个神经网络，包括最新的变体，例如残余网络。 知道如何将卷积网络应用到视觉检测和识别任务。 知道如何使用神经风格迁移生成艺术。 能够在图像、视频以及其他2D或3D数据上应用这些算法。 1. Computer vision 如图示，之前课程中介绍的都是 64 * 64 * 3的图像 (3 代表:因为每个图片都有3个颜色通道 channels, 12288 So XXX, the input features has dimension 12288)，而一旦图像质量增加，例如变成 1000 * 1000 * 3 的时候那么此时的神经网络的计算量会巨大，显然这不现实。所以需要引入其他的方法来解决这个问题. 2. Edge detection example 使用边缘检测作为入门样例, you see how the convolution operation works. 边缘检测可以是垂直边缘检测，也可以是水平边缘检测，如上图所示. 至于算法如何实现，下面举一个比较直观的例子： 可以很明显的看出原来 6 * 6 的矩阵有明显的垂直边缘，通过 3 * 3 的过滤器 filter (也叫做 “核”)卷积之后，仍然保留了原来的垂直边缘特征，虽然这个边缘貌似有点粗，这是因为数据不够大的原因，如果输入数据很大的话这个不是很明显了. 关于用编程语言实现：python / tensorflow / keras 等，都有一些函数来实现卷积运算. 3. More edge detection 除了上面的垂直，水平边缘检测，其实也可以检测初颜色过度变化，例如是亮变暗，还是暗变亮？ **在计算机视觉的历史上，层曾经公平的争论过哪些什么样的 filter 数字组合才是最好的: ** 下面是一些常见的过滤器，第二个是Sobel filter，具有较强的鲁棒性，第三个是Schoss filter. Filter desc Sobel Filter it puts a little bit more weight to the central row 增加了中间一行的权重 Schoss Filter 实际它也是一种垂直边缘检测 翻转90度，它就变为水平边缘检测 Other Filter 9个参数也可以通过学习(反向传播)的方式获得 其实过滤器的9个参数也可以通过学习(反向传播)的方式获得，虽然比较费劲，但是可能会学到很多其他除了垂直，水平的边缘特征，例如 45°，70° 等各种特征. 4. Padding 由前面的例子, 卷积的方法，有2个缺点: 每经过一次卷积计算，原数据都会减小，但有时我们并不希望这样。举个比较极端的例子：假设原数据是 30 * 30 的一只猫的图像，经过10次卷积 (过滤器是3 * 3) 后，最后图像只剩下了 10 * 10 了 😳😳 由卷积的计算方法可知，图像边缘特征计算次数显然少于图像中间位置的像素点，如下图示 (绿色的位置明显是冷宫), 图像边缘的大部分信息都丢失了. 4.1 运用 Padding 的原因 原来的 6 * 6 填充后变成了 8 * 8，此时在经过一次卷积得到的仍旧是 6 * 6 的矩阵。 下面总结一下卷积之后得到矩阵大小的计算方法，假设： Title Size desc 原数据 n \\* n 矩阵 n \\* n Filter f \\* f 过滤器 Padding p \\* p 填充数量 综上： n+2p−f+1n+2p-f+1n+2p−f+1 得到的矩阵大小 padding 后，虽然边缘像素点仍旧计算的比较少，但是这个缺点至少一定程度上被削弱了. 4.2 如何 padding 的大小 Type desc Size Valid convolutions 不添加 padding n−f+1n - f + 1n−f+1 Same convolutions Pad so that output size is the same as the input size. 保持原图像矩阵的大小 满足 n+2p−f+1=nn+2p-f+1 = nn+2p−f+1=n 即 p=f−12p=\\frac{f-1}{2}p=2f−1​, 为了满足上式，fff 一般奇数 5. Strided convolutions 过滤器 纵向、横向 都需要按 步长 SSS 来移动，如图示: 结合之前的内容，输出矩阵大小计算公式方法为，假设： Title Size desc 原数据 n \\* n 矩阵 n \\* n Filter f \\* f 过滤器 Padding p \\* p 填充数量 Stride s \\* s 步长 综上： 得到的矩阵大小是 ⌊n+2p−fs\\frac{n+2p-f}{s}sn+2p−f​⌋ * ⌊n+2p−fs\\frac{n+2p-f}{s}sn+2p−f​⌋ ⌊⌋: 向下取整符号 ⌊59/60⌋ = 0 ⌈⌉: 向上取整符号 ⌈59/60⌉ = 1 6. Convolutions Over Volumes 这一节用立体卷积来解释 如图: 输入矩阵是 6 \\* 6 \\* 3 (height * width * channels), 过滤器是 3 \\* 3 \\* 3，计算方法是一一对应相乘相加 最后得到 4 \\* 4 的二维矩阵. 有时可能需要检测 水平边缘 或 垂直边缘，或 其他特征，所以我们可以使用多个过滤器。上图则使用了两个过滤器 (黄色和橘黄色)，得到的特征矩阵大小为 4 \\* 4 \\* 2. Filter 数字组合参数的选择不同，你可以得到不同的特征检测器. 7. One Layer of CNN Example 如图示得到 4 \\* 4 的矩阵后还需要加上一个偏差 b_nb\\_nb_n (Python 广播机制)，之后还要进行非线性转换，即用 ReLU 函数. 因此假如在某一卷积层中使用了 10 个 3 \\* 3 的 Filter 过滤器，那么一共有 (3\\*3+1)\\*10=280 个参数. 下面总结了各项参数的大小和表示方法： Title Formula desc 输入矩阵 n\\_H^{l-1} \\* n\\_W^{l-1} \\* n\\_c^{l-1} height \\* width \\* channels = 6 \\* 6 \\* 3 过滤器大小 f[l]f^{[l]}f[l] Padding p[l]p^{[l]}p[l] Stride sls^lsl 步长 Each Filter is f^{l} \\* f^{l} \\* n\\_c^{[l-1]} 3 \\* 3 \\* 3 , n_c[l−1]=3n\\_c^{[l-1]} = 3n_c[l−1]=3 每一卷积层的过滤器的通道的大小 = 输入层的通道大小 n_c[l−1]n\\_c^{[l-1]}n_c[l−1] Activations 激活函数 ala^{l}al = n\\_H^{l} \\* n\\_W^{l} \\* n\\_c^{l} AlA^{l}Al = m \\* n\\_H^{l} \\* n\\_W^{l} \\* n\\_c^{l}，(mmm个例子) 权重 weight f^{l} \\* f^{l} \\* n\\_c^{[l-1]} \\* n\\_c^{[l]} Filter \\* 过滤器个数 过滤器的个数 = 输出层的通道的大小 n_cln\\_c^{l}n_cl 偏差 bias 1 \\* 1 \\* 1 \\* n\\_c^{[l]} 输出矩阵 n\\_H^{l} \\* n\\_W^{l} \\* n\\_c^{l} height \\* width \\* channels Output n_H/W[l]=[n_H/W[l−1]+2p[l]−f[l]s[l]+1]n\\_{H/W}^{[l]}=[\\frac{n\\_{H/W}^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]n_H/W[l]=[s[l]n_H/W[l−1]+2p[l]−f[l]​+1] 输出层与输入层计算公式 8. A Simple Convolution Network 上图简单介绍了卷积网络的计算过程，需要再介绍的一点是最后一层的全连接层，即将 7 \\* 7 \\* 40 的输出矩阵展开，得到 196019601960 个节点，然后再采用 Softmax 来进行预测. 一般的 CNN 中，每一层矩阵的 height 和 width 是逐渐减小的，而 channel 则是增加的 (n_cn\\_cn_c 在增加，n_Hn\\_Hn_H 和 n_Wn\\_Wn_W 在减少). CNN 中常见的 3 种类型的 layer： Convolution (Conv 卷积层) Pooling (Pool 池化层) Fully connected (FC 全连接层) 9. Pooling Layers 使用池化层来缩减模型的大小，提高计算速度，同时提高提取特征的鲁棒性. 最大池化的直观理解: 你可以把上面 4 \\* 4 输入看做是某些特征的集合 (也许不是)，也就是神经网络中某一层的反激活值，数字大意味着可能提取了某些特定特征. 左上象限具有这个特征，可能是一个垂直边缘. or maybe an eye， 显然左上象限具有这个特征. 最大化操作的功能就是只要在任何一个象限内提取到某个特征，它就会保留在最大池化的输出里. 最大化操作的实际作用就是：如果在 Filter 中提取到某个特征 那么保留其最大值. 如果没有提取到这个特征，比如说 右上象限 中，那么其最大值也还是很小. Max Pooling 的超级参数 f=2/3f=2 / 3f=2/3，s=2s=2s=2 是最常用的，效果相当于高度和宽度缩减一半. Max Pooling 很少用 padding， by far, is p=0p = 0p=0，Average Pooling 这个用的不多，这个也会加入更多的计算量. 10. A CNN Example 在 Andrew Ng 的课件中将 Conv layer 和 Pooling layer 合并在一起视为一层，因为池化层没有参数 (因为池化层的过滤器 无参数，有超参数，而且其大小可以事先确定好)。 但是在其他文献中有可能会把池化层算成单独的层，所以视情况而定。 池化层参数个数为 0，卷积层参数个数一般多，fully connected layer 全连接层 参数很多. 11. Why Convolutions ? 卷积 相比于 FC全连接 的好处最直观的就是使用的参数更少，参数共享 和 稀疏连接： 11.1 Parameter sharing 特征检测如垂直边缘检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域，也就是说如果你用一个 3 \\* 3 的 Filter 检测垂直边缘. 那么图片的左上角区域以及旁边的各个区域都可以使用这个 3 \\* 3 的过滤器. 每个特征检测器以及输出都可以在输入图片的不同区域中使用同样的参数，以便提取垂直边缘或其他特征，它不仅适用于边缘特征这样的低阶特征，同样适用于高阶特征. 例如：提取脸上的眼睛 等或者其他对象. 这种是整张图片共享特征检测器 提取效果也很好 11.2 Sparsity of Connections 右边图 的边缘，仅与 36 个输入特征中的 9 个相连接, 而且其他像素值都不会对输出产生任何影响, 这就是稀疏连接的概念. 某个输出点，看上去只有这 9 个输入特征与输出相连接. 其他像素对输出没有任何影响. 神经网络可以通过这两种机制减少参数. 以便于我们用更小的训练集训练它，从而预防过度拟合. (卷积有一个平移不变属性) 综上这些就是卷积神经网络 CNN 表现良好的原因. Reference 网易云课堂 - deeplearning DeepLearning.ai学习笔记汇总 CNN入门讲解：卷积层是如何提取特征的？","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Chatbot Research 6 - 更多论文 (感谢 PaperWeekly)","slug":"nlp/chatbot/chatbot-research6","date":"2018-08-16T06:00:21.000Z","updated":"2021-06-20T04:12:28.339Z","comments":true,"path":"2018/08/16/nlp/chatbot/chatbot-research6/","link":"","permalink":"http://www.iequa.com/2018/08/16/nlp/chatbot/chatbot-research6/","excerpt":"Chatbot 更多 Paper 论文和参考资料 (感谢 PaperWeekly)","text":"Chatbot 更多 Paper 论文和参考资料 (感谢 PaperWeekly) Paper 1 《A Neural Conversational Model》 代码: https://github.com/Conchylicultor/DeepQA 2128 star 作者: 来自 Google Brain，毕业于 UC Berkeley 的 Oriol Vinyals博士 对比 cleverbot (第二代基于检索的聊天机器人)，部分回答 更智能。 如何客观地评价生成的效果? 有一些问题没有标准答案 来说，自动评价 VS 用户评价 作者对一些评估方法提出了一些自己的思考方式 DeepQA chatbot 训练的部分 chatbot_website 服务的部分，网页端 DeepQA/chatbot/ 这几个文件比较重要 12345from chatbot.corpus.cornelldata import CornellDatafrom chatbot.corpus.opensubsdata import OpensubsDatafrom chatbot.corpus.scotusdata import ScotusDatafrom chatbot.corpus.ubuntudata import UbuntuDatafrom chatbot.corpus.lightweightdata import LightweightData 语料太大的情况下，做一些采样或者层次化的 softmax 可行 12345678if 0 &lt; self.args.softmaxSamples &lt; self.textData.getVocabularySize(): outputProjection = ProjectionOp( (self.textData.getVocabularySize(), self.args.hiddenSize), scope=&#x27;softmax_projection&#x27;, dtype=self.dtype ) def sampledSoftmax(labels, inputs): Paper 2 《A Diversity-Promoting Objective Function for Neural Conversation Models》 作者： Jiwei Li， Jiwei Li’s Github 关于《A Persona-Based Neural Conversation Model》的pre-paper Seq2seq 容易产出”呵呵”，”都可以”，”我不知道”这种 safe 但无意义的回答 NLG 问题，常使用 MLE 作为目标函数，产出的结果通畅，但 diversity 差，可考虑 decoder 产出 n-best, 再 rank 提出 Maximum Mutual Information(MMI) 作为目标函数， 有 MMI-antiLM 和MMI-bidi 2种 Paper 3 《A Persona-Based Neural Conversation Model》 Speaker Model 和 Speaker-Addressee Model 作者： Jiwei Li， Jiwei Li’s Github 代码： https://github.com/jiweil/Neural-Dialogue-Generation 解决多轮对话回答不一致问题 Model 中融入 user identity (比如背景信息、用户画像，年龄等信息)，构建出个性化的 seq2seq 模型，为不同的 user， 以及 同一个user 对不同的对象对话生成不同风格的 response Paper 4 《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》 代码: https://github.com/julianser/hed-dlg-truncated 作者: 来自蒙特利尔大学和Maluuba公司 意在解决语言模型生成部分存在的问题 整个 seq2seq 框架中 decoder生成部分的问题，不仅是 bot领域对话生成的问题，都可以尝试用这个方式。 Paper 5 更多论文和参考资料(感谢PaperWeekly) 《End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning》 http://rsarxiv.github.io/2016/07/17/End-to-end-LSTM-based-dialog-control-optimized-with-supervised-and-reinforcement-learning-PaperWeekly/ 《A Network-based End-to-End Trainable Task-oriented Dialogue System》 http://rsarxiv.github.io/2016/07/12/A-Network-based-End-to-End-Trainable-Task-oriented-Dialogue-System-PaperWeekly 《A Neural Network Approach to Context-Sensitive Generation of Conversational Responses》 http://rsarxiv.github.io/2016/07/15/A-Neural-Network-Approach-to-Context-Sensitive-Generation-of-Conversational-Responses-PaperWeekly/ Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation http://rsarxiv.github.io/2016/07/09/Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation-PaperWeekly/ Reference 2015 DeepQA 聊天机器人深度学习应用-part1：引言 dennybritz/chatbot-retrieval 更多论文和参考资料(感谢PaperWeekly)","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"}]},{"title":"Chatbot Research 5 - 基于深度学习的检索聊天机器人","slug":"nlp/chatbot/chatbot-research5","date":"2018-08-15T06:00:21.000Z","updated":"2021-06-20T04:12:28.337Z","comments":true,"path":"2018/08/15/nlp/chatbot/chatbot-research5/","link":"","permalink":"http://www.iequa.com/2018/08/15/nlp/chatbot/chatbot-research5/","excerpt":"介绍基于检索式机器人。检索式架构有预定好的语料答复库。 检索式模型的输入是上下文潜在的答复。模型输出对这些答复的打分，可以选择最高分的答案作为回复。","text":"介绍基于检索式机器人。检索式架构有预定好的语料答复库。 检索式模型的输入是上下文潜在的答复。模型输出对这些答复的打分，可以选择最高分的答案作为回复。 既然生成式的模型更弹性，也不需要预定义的语料，为何不选择它呢？ 生成式模型的问题就是实际使用起来并不能好好工作，至少现在是。因为答复比较自由，容易犯语法错误和不相关、不合逻辑的答案，并且需要大量的数据且很难做优化。 大量的生产系统上还是采用 检索模型 或者 检索模型 和 生成模型 结合的方式。 例如 google 的 smart reply。 生成模型是研究的热门领域，但是我们还没到应用它的程度。如果你想要做一个聊天机器人，最好还是选用检索式模型 更聪明的聊天机器人 ： 生成式模型 VS 检索匹配模型 Chatterbot的进化: 深度学习提高智能度 模型构建 ： 问题的分析与转化 数据集与样本构造方法 网络结构的构建 模型的评估 代码实现与解析 1. 聊天机器人 1.1 基于检索的 chatbot 根据 input 和 context，结合知识库的算法得到合适回复 从一个固定的数据集中找到合适的内容作为回复 检索和匹配的方式有很多种 数据和匹配方法对质量有很大影响 1.2 基于生成模型的chatbot 典型的是 seq2seq 的方法 生成的结果需要考虑通畅度和准确度 以前者为主(可控度高)，后者为辅 2. 回顾 chatterbot 2.1 chatterbot 的问题 应答模式的匹配方式太粗暴 编辑距离无法捕获深层语义信息 核心词 + word2vec 无法捕获整句话语义 LSTM 等 RNN模型 能捕获序列信息 … 用深度学习来提高匹配阶段准确率!! 心得 : Open Domain 的 chatbot 很难做，话题太广，因为无法预知用户会问到什么问题. 你想吃什么 ： 随便 你感觉怎么样 : 还好 没问题其实 所以针对一个 Closed Domain + 检索 + 知识库，还应该可以做一个可以用的机器人. 2.2 应该怎么做 匹配本身是一个模糊的场景 转成排序问题 排序问题怎么处理? 转成能输出概率的01分类问题 Q1 -&gt; { R1: 0.8, R2: 0.1, R3: 0.05, R4: 0.2 } Query &lt;&gt; Response 数据构建? 我们需要正样本(正确的回答) 和 负样本(不对的回答) { 正样本 : Q1-R1 1 }, { 负样本 : Q1-R3 0 } Loss function? 回忆一下 logistic regression 心得 : 定义问题 和 解决问题 很重要 有一个问题，可以转换为 机器学习 或 深度学习 可以解决的问题，这非常重要。 3. 用深度学习来完成 不管网络结构如何，你抓住最好的 loss function 100 W 样本，50W+， 50W-， 这样的数据集 我们拿来做训练，这样的网络结构，不管如何搭建，都不要太担心，你就抓住 loss function，你的损失函数由 c 和 r 决定的。 c 和 r 是由于上面的结构产生的，所以我们就可以用 BPTT 做训练了. Query 和 Respon 都是我们分词后用的 word embmming，灌入 RNN 中，我们把 LSTM 顺着捕捉下来，当做问题和回答，两个捕捉的信息来做匹配，我找了个参数 M，来做 c 和 r 的匹配。 M 是一定的，匹配程度和方式一致. M 初始化的时候可以由 radom 是生成. M 之后是可以通过训练做更新的. 2016 Google Brain deep-learning-for-chatbots-2-retrieval-based-model-tensorflow, wildml blog 4. 数据 - Ubuntu 对话语料库 我们将使用Ubuntu对话数据集（论文来源 github地址）。这个数据集（Ubuntu Dialog Corpus, UDC）是目前最大的公开对话数据集之一，它是来自 Ubuntu 的 IRC网络 上的对话日志。这篇论文介绍了该数据集生成的具体细节。下面简单介绍一下数据的格式。 训练数据有 100W 条实例，其中一半是正例（label为1），一半是负例（label为0，负例为随机生成）。每条实例包括一段上下文信息（context），即Query；和一段可能的回复内容，即Response；Label为1表示该Response确实是Query的回复，Label为0则表示不是。下面是数据示例： 数据集生成脚本已用NLTK做了一系列的语料处理包括（分词，stemmed，lemmatized）等文本预处理步骤； 使用了NER技术，将文本中的实体，如 姓名、地点、组织、URL 等替换成特殊字符。 这些预处理不是严格必要的，但是能改善一些系统的表现。 语料的上下文平均有86个词语，答复平均有17个词语长。有人做了语料的统计分析：data analysis 数据集也包括了 Test / Validation sets，但这两部分的数据和训练数据在格式上不太一样。 在 Test / Validation sets 中，对于每一条实例，有一个正例和九个负例数据（也称为干扰数据）。 模型的目标在于给正例的得分尽可能的高，而给负例的得分尽可能的低。下面是数据示例： 4.1 Train sets 4.2 Test / Validation sets 每个样本，有一个正例和九个负例数据 (也称为干扰数据)。 建模的目标在于给正例的得分尽可能的高，而给负例的得分尽可能的低。(有点类似分类任务) 语料做过分词、stemmed、lemmatized 等文本预处理。 NLTK stemmed 123from nltk.stem.porter import PorterStemmerp = PorterStemmer()p.stem(&#x27;wenting&#x27;) NLTK Lemma 1234from nltk.stem import WordNetLemmatizerwordnet_lemmatizer = WordNetLemmatizer()wordnet_lemmatizer.lemmatize(‘dogs’)u’dog’ 用 NER(命名实体识别) 将文本中的 实体，如姓名、地点、组织、URL等 替换成特殊字符 5. 评估准则 BASELINE Recall@K 常见的 Kaggle 比赛评判准则 经模型对候选的 response 排序后，前 k 个候选中 存在正例数据(正确的那个)的占比。 让 K=10，这就得到一个 100% 的召回率，因最多就 10 个备选。如果 K=1，模型只一次机会选中正确答案。 K 值 越大，指标值越高，对模型性能的要求越松。 9个干扰项目怎么选出来 这个数据集里是随机的方法选择的。 但是现实世界里你可能数百万的可能答复，并且你并不知道答复是否合理正确。你没能力从数百万的可能的答复里去挑选一个得分最高的正确答复。成本太高了！ google 的 smart reply 用分布式集群技术计算一系列的可能答复去挑选,. 可能你只有百来个备选答案，可以去评估每一个。 1234567def evaluate_recall(y, y_test, k=1): num_examples = float(len(y)) num_correct = 0 for predictions, label in zip(y, y_test): if label in predictions[:k]: num_correct += 1 return num_correct/num_examples 其中，y 是所预测的以降序排列的模型预测分值，y_test 是实际的 label 值。举个例子，假设 y 的值为 [0,3,1,2,5,6,4,7,8,9]，这说明 第0号 的候选的预测分值最高、作为回复的可能性最高，而9号则最低。这里的 第0号 同时也是正确的那个，即正例数据，标号为 1-9 的为随机生成的负例数据。 5.1 基线模型:random guess 理论上，最base的随机模型（Random Predictor）的recall@1的值为10%，recall@2的值为20%. 相应的代码如下： 12345678910# Random Predictordef predict_random(context, utterances): return np.random.choice(len(utterances), 10, replace=False) #np.random.choice(5, 3) array([0, 3, 4]) # 可以从一个 int数字 或 1维 array 里随机选取内容，并将选取结果放入 n维 array 中返回# Evaluate Random predictory_random = [predict_random(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]y_test = np.zeros(len(y_random))for n in [1, 2, 5, 10]: print(&quot;Recall @ (&#123;&#125;, 10): &#123;:g&#125;&quot;.format(n, evaluate_recall(y_random, y_test, n))) 实际的模型结果如下： 1234Recall @ (1, 10): 0.0937632Recall @ (2, 10): 0.194503Recall @ (5, 10): 0.49297Recall @ (10, 10): 1 这与理论预期相符，但这不是我们所追求的结果。 5.2 基线模型:TF-IDF检索 另外一个 baseline 的模型为 tfidf predictor。直观上，两篇文档对应的 tfidf 向量 越接近，两篇文章的内容也越相似。同样的，对于一个 QR pair，它们语义上接近的词共现的越多，也将越可能是一个正确的 QR pair（这句话存疑，原因在于 Q R 之间也有可能不存在语义上的相似，一个Q对应的 R 是多样的。）。tfidf predictor 对应的代码如下（利用scikit-learn工具能够轻易实现）： tfidf表示词频（term frequency）和逆文档词频（inverse document frequency），它衡量了一个词在一篇文档中的重要程度（基于整个语料库）。 123456789101112131415161718192021222324class TFIDFPredictor: def __init__(self): self.vectorizer = TfidfVectorizer() def train(self, data): self.vectorizer.fit(np.append(data.Context.values,data.Utterance.values)) def predict(self, context, utterances): # Convert context and utterances into tfidf vector vector_context = self.vectorizer.transform([context]) vector_doc = self.vectorizer.transform(utterances) # The dot product measures the similarity of the resulting vectors result = np.dot(vector_doc, vector_context.T).todense() result = np.asarray(result).flatten() # Sort by top results and return the indices in descending order return np.argsort(result, axis=0)[::-1]# Evaluate TFIDF predictorpred = TFIDFPredictor()pred.train(train_df)y = [pred.predict(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]for n in [1, 2, 5, 10]: print(&quot;Recall @ (&#123;&#125;, 10): &#123;:g&#125;&quot;.format(n, evaluate_recall(y, y_test, n))) 模型结果如下： 1234Recall @ (1, 10): 0.495032Recall @ (2, 10): 0.596882Recall @ (5, 10): 0.766121Recall @ (10, 10): 1 显然这比 Random 的模型要好得多，但这还不够。之前的假设并不完美，首先 query 和 response 之间并不一定要是语义上的相近；其次 tfidf模型 忽略了词序这一重要的信息。使用 NN模型 我们能做得更好一些。 6. LSTM 建立的 NN模型 为两层 Encoder 的 LSTM模型（Dual Encoder LSTM Network），这种形式的网络被广泛应用 chatbot 中。 seq2seq模型 常用于机器翻译领域，并取得了较大的效果。使用 Dual LSTM模型 的原因在于这个模型被证明在这个数据集有较好的效果（详情见这里）, 这可以作为我们后续模型效果的验证。 两层 Encoder 的 LSTM模型 的结构图如下（论文来源）： 大致流程： (1). Query 和 Response 都是经过分词的，分词后每个词 embedding 为向量形式。初始的词向量使用 GloVe / Word2vec，之后词向量随着模型的训练会进行 fine-tuned 。 (2). 分词且向量化的 Query 和 Response 经过相同的 RNN（word by word）。RNN 最终生成一个向量表示，捕捉了 Query 和 Response 之间的[语义联系]（图中的ccc和rrr）；这个向量的维度是可以指定的，这里指定为 256维。 (3). 将 向量c 与一个 矩阵M 相乘，来预测一个可能的 回复r’r’r’。如果 ccc 为一个256维的向量，M维 256*256 的矩阵，两者相乘的结果为另一个256维的向量，我们可以将其解释为[一个生成式的回复向量]。矩阵M 是需要训练的参数。 (4). 通过点乘的方式来预测生成的 回复r’r’r’ 和 候选的 回复rrr 之间的相似程度，点乘结果越大表示候选回复作为回复的可信度越高；之后通过 sigmoid 函数归一化，转成概率形式。 (sigmoid作为压缩函数经常使用) 图中把第(3)步和第(4)步结合在一起了。 (5). 损失函数（loss function）。这里使用二元的交叉熵（binary cross-entropy）作为损失函数。我们已知实例的真实 label yyy， 值为 0 或 1； 通过上面的第(4)步可以得到一个概率值 y′y&#x27;y′；因此，交叉熵损失值为 L = -y \\* ln(y&#039;) - (1 - y) \\* ln(1 - y&#039;)。 这个公式意义是直观的，即当 y=1y=1y=1 时，L=−ln(y′)L = -ln(y&#x27;)L=−ln(y′)，期望 y′y&#x27;y′ 尽量接近 1 使得损失函数的值越小；反之亦然。 实现过程中使用了 numpy、pandas、TensorFlow 和 TF Learn 等工具。 6.1. 数据预处理 数据集的原始格式为csv格式，我们需要先将其转为 TensorFlow 专有的格式，这种格式的好处在于能够直接从输入文件中 load tensors，并让 TensorFlow 来处理洗牌(shuffling)、批量(batching) 和 队列化(queuing) 等操作。预处理中还包括创建一个字典库，将词进行标号，TFRecord 文件将直接存储这些词的标号。 每个实例包括如下几个字段： Query：表示为一串词标号的序列，如 [231, 2190, 737, 0, 912]； Query 的长度； Response：同样是一串词标号的序列； Response 的长度； Label； Distractor_[N]：表示负例干扰数据，仅在验证集和测试集中有，N 的取值为 0-8； Distractor_[N]的长度； 数据预处理的 Python脚本，生成了3个文件：train.tfrecords, validation.tfrecords 和 test.tfrecords。你可以尝试自己运行程序，或者直接下载和使用预处理后的数据。 6.2. 创建输入函数 为了使用 TensoFlow内置 的训练和评测模块，我们需要创建一个输入函数：这个函数返回输入数据的 batch。 因为训练数据和测试数据的格式不同，我们需要创建不同的输入函数。 输入函数需要返回批量(batch)的特征和标签值(如果有的话)。类似于如下： 123def input_fn(): # TODO Load and preprocess data here return batched_features, labels 因为我们需要在模型训练和评测过程中使用不同的输入函数，为了防止重复书写代码，我们创建一个包装器(wrapper)，名称为create_input_fn，针对不同的mode使用相应的code，如下： 12345def create_input_fn(mode, input_files, batch_size, num_epochs=None): def input_fn(): # TODO Load and preprocess data here return batched_features, labels return input_fn 完整的code见udc_inputs.py。整体上，这个函数做了如下的事情： (1) 定义了示例文件中的 feature字段； (2) 使用 tf.TFRecordReader 来读取 input_files 中的数据； (3) 根据 feature字段 的定义对数据进行解析； (4) 提取训练数据的标签 label； (5) 产生批量化的训练数据 batch train_datasets； (6) 返回批量的特征数据及对应标签 label； 6.3. 定义评测指标 之前已经提到用 recall@k 这个指标来评测模型，TensorFlow 中已经实现了许多标准指标（包括 recall@k）。为了使用这些指标，需要创建一个字典，key 为指标名称，value 为对应的计算函数。如下 12345678def create_evaluation_metrics(): eval_metrics = &#123;&#125; for k in [1, 2, 5, 10]: eval_metrics[&quot;recall_at_%d&quot; % k] = functools.partial( tf.contrib.metrics.streaming_sparse_recall_at_k, k=k ) return eval_metrics 如上，我们使用了 functools.partial 函数，这个函数的输入参数有两个。不要被 streaming_sparse_recall_at_k 所困惑，其中的 streaming 的含义是表示指标的计算是增量式的。 训练和测试所使用的评测方式是不一样的，训练过程中我们对 每个case 可能作为正确回复的概率进行预测，而测试过程中我们对每组数据（包含10个case，其中1个是正确的，另外9个是生成的负例/噪音数据）中的case进行逐条概率预测，得到例如 [0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11] 这样格式的输出，这些输出值的和并不要求为 1（因为是逐条预测的，有单独的预测概率值，在 0 到 1 之间）； 而对于这组数据而言，因为数据 index=0 对应的为正确答案，这里 recall@1 为 0，因为 0.34 是其中第二大的值，所以 recall@2 是 1（表示这组数据中预测概率值在前二的中有一个是正确的）。 6.4. 训练程序样例 首先，给一个模型训练和测试的程序样例，这之后你可以参照程序中所用到的标准函数，来快速切换和使用其他的网络模型。假设我们有一个函数 model_fn，函数的输入参数有 batched features，label 和 mode(train/evaluation)，函数的输出为预测值。程序样例如下： 12345678910111213141516171819202122232425262728293031323334estimator = tf.contrib.learn.Estimator( model_fn=model_fn, model_dir=MODEL_DIR, config=tf.contrib.learn.RunConfig())input_fn_train = udc_inputs.create_input_fn( mode=tf.contrib.learn.ModeKeys.TRAIN, input_files=[TRAIN_FILE], batch_size=hparams.batch_size)input_fn_eval = udc_inputs.create_input_fn( mode=tf.contrib.learn.ModeKeys.EVAL, input_files=[VALIDATION_FILE], batch_size=hparams.eval_batch_size, num_epochs=1)eval_metrics = udc_metrics.create_evaluation_metrics()# We need to subclass theis manually for now. The next TF version will# have support ValidationMonitors with metrics built-in.# It&#x27;s already on the master branch.class EvaluationMonitor(tf.contrib.learn.monitors.EveryN): def every_n_step_end(self, step, outputs): self._estimator.evaluate( input_fn=input_fn_eval, metrics=eval_metrics, steps=None)eval_monitor = EvaluationMonitor(every_n_steps=FLAGS.eval_every)estimator.fit(input_fn=input_fn_train, steps=None, monitors=[eval_monitor]) 这里创建了一个 model_fn 的 estimator(评估函数)； 两个输入函数，input_fn_train 和 input_fn_eval，以及计算评测指标的函数； 完整的code见udc_train.py。 6.5. 创建模型 到目前为止，我们创建了模型的 输入、解析、评测和训练 的样例程序。现在我们来写 LSTM 的程序，create_model_fn函数 用以处理不同格式的训练和测试数据；它的输入参数为 model_impl，这个函数表示实际作出预测的模型，这里就是用的LSTM，当然你可以替换成任意的其他模型。程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def dual_encoder_model( hparams, mode, context, context_len, utterance, utterance_len, targets): # Initialize embedidngs randomly or with pre-trained vectors if available embeddings_W = get_embeddings(hparams) # Embed the context and the utterance context_embedded = tf.nn.embedding_lookup( embeddings_W, context, name=&quot;embed_context&quot;) utterance_embedded = tf.nn.embedding_lookup( embeddings_W, utterance, name=&quot;embed_utterance&quot;) # Build the RNN with tf.variable_scope(&quot;rnn&quot;) as vs: # We use an LSTM Cell cell = tf.nn.rnn_cell.LSTMCell( hparams.rnn_dim, forget_bias=2.0, use_peepholes=True, state_is_tuple=True) # Run the utterance and context through the RNN rnn_outputs, rnn_states = tf.nn.dynamic_rnn( cell, tf.concat(0, [context_embedded, utterance_embedded]), sequence_length=tf.concat(0, [context_len, utterance_len]), dtype=tf.float32) encoding_context, encoding_utterance = tf.split(0, 2, rnn_states.h) with tf.variable_scope(&quot;prediction&quot;) as vs: M = tf.get_variable(&quot;M&quot;, shape=[hparams.rnn_dim, hparams.rnn_dim], initializer=tf.truncated_normal_initializer()) # &quot;Predict&quot; a response: c * M generated_response = tf.matmul(encoding_context, M) generated_response = tf.expand_dims(generated_response, 2) encoding_utterance = tf.expand_dims(encoding_utterance, 2) # Dot product between generated response and actual response # (c * M) * r logits = tf.batch_matmul(generated_response, encoding_utterance, True) logits = tf.squeeze(logits, [2]) # Apply sigmoid to convert logits to probabilities probs = tf.sigmoid(logits) # Calculate the binary cross-entropy loss losses = tf.nn.sigmoid_cross_entropy_with_logits(logits, tf.to_float(targets)) # Mean loss across the batch of examples mean_loss = tf.reduce_mean(losses, name=&quot;mean_loss&quot;) return probs, mean_loss 完整的程序见 dual_encoder.py。基于这个，我们能够实例化 model函数 在我们之前定义的 udc_train.py，如下： 123model_fn = udc_model.create_model_fn( hparams=hparams, model_impl=dual_encoder_model) 这样我们就可以直接运行 udc_train.py文件，来开始模型的训练和评测了，你可以设定–eval_every参数 来控制模型在验证集上的评测频率。更多的命令行参数信息可见 tf.flags 和 hparams，你也可以运行 python udc_train.py --help 来查看。 运行程序的效果如下： 1234567INFO:tensorflow:training step 20200, loss = 0.36895 (0.330 sec/batch).INFO:tensorflow:Step 20201: mean_loss:0 = 0.385877INFO:tensorflow:training step 20300, loss = 0.25251 (0.338 sec/batch).INFO:tensorflow:Step 20301: mean_loss:0 = 0.405653...INFO:tensorflow:Results after 270 steps (0.248 sec/batch): recall_at_1 = 0.507581018519, recall_at_2 = 0.689699074074, recall_at_5 = 0.913020833333, recall_at_10 = 1.0, loss = 0.5383... 6.6. 模型的评测 在训练完模型后，你可以将其应用在 测试集 上，使用： 1python udc_test.py --model_dir=$MODEL_DIR_FROM_TRAINING 例如： 1python udc_test.py --model_dir=~/github/chatbot-retrieval/runs/1467389151 这将得到模型在 测试集 上的 recall@k 的结果，注意在使用 udc_test.py文件 时，需要使用与训练时相同的参数。 在训练模型的次数大约 2w 次时(在GPU上大约花费1小时)，模型在测试集上得到如下的结果： 123recall_at_1 = 0.507581018519recall_at_2 = 0.689699074074recall_at_5 = 0.913020833333 其中，recall@1的值与tfidf模型的差不多，但是recall@2和recall@5的值则比tfidf模型的结果好太多。原论文中的结果依次是0.55,0.72和0.92，可能通过模型调参或者预处理能够达到这个结果。 6.7. 使用模型进行预测 对于新的数据，你可以使用 udc_predict.py 来进行预测；例如： 1python udc_predict.py --model_dir=./runs/1467576365/ 结果如下： 123Context: Example contextResponse 1: 0.44806Response 2: 0.481638 你可以从候选的回复中，选择预测分值最高的那个作为回复。 6.8. 总结 这篇博文中，我们实现了一个基于检索的 NN模型，它能够对候选的回复进行预测和打分，通过输出分值最高（或者满足一定阈值）的候选回复已完成聊天的过程。后续可以尝试其他更好的模型，或者通过调参来取得更好的实验结果。 Reference About ChatterBot 2016 Google Brain deep-learning-for-chatbots-part-1-introduction, wildml blog 2016 Google Brain deep-learning-for-chatbots-2-retrieval-based-model-tensorflow, wildml blog 聊天机器人中的深度学习技术之二：基于检索模型的实现 聊天机器人中的深度学习技术之一：导读 Tensorflow搞一个聊天机器人 Eric，基于多搜索引擎的自动问答机器人 测试人机问答系统智能性的3760个问题 中国版的聊天机器人地图 Chatbots Landscape 条件随机场简介 知识图谱学习系列之二：命名实体识别1（技术及代码）","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"}]},{"title":"Sequence Models (week3) - Attention mechanism","slug":"deeplearning/Sequence-Models-week3","date":"2018-08-14T02:00:21.000Z","updated":"2021-06-20T04:12:28.254Z","comments":true,"path":"2018/08/14/deeplearning/Sequence-Models-week3/","link":"","permalink":"http://www.iequa.com/2018/08/14/deeplearning/Sequence-Models-week3/","excerpt":"能够将序列模型应用到自然语言问题、音频应用 等，包括文字合成、语音识别和音乐合成。","text":"能够将序列模型应用到自然语言问题、音频应用 等，包括文字合成、语音识别和音乐合成。 1. Basic models 假设需要翻译下面这句话 “简将要在 9 月访问中国” 我们希望得到的结果是 &quot;Jane is visiting China in September” 在这个例子中输入的数量是 10 个中文汉字，输出为 6 个单词， T_xT\\_xT_x 与 TyT_yTy​ 数量不一致，就需要用到 Sequence to sequence model RNN 类似的例子还有用机器为下面这张图片生成描述 只需要将 encoder 部分用一个 CNN模型 替换就可以了，比如 AlexNet，就可以得到“一只（可爱的）猫躺在楼梯上” 2. Picking the most likely sentence 下面将之前学习的语言模型和机器翻译模型做一个对比, P 为概率 语言模型: 机器翻译模型: 可以看到，机器翻译模型的后半部分其实就是语言模型，Andrew 将其称之为 “条件语言模型”，在语言模型之前有一 个条件也就是被翻译的句子: P(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;)P(y^{&lt;1&gt;},…,y^{&lt;{T\\_y}&gt;}|x^{&lt;1&gt;},…,x^{&lt;{T\\_x}&gt;}) P(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;) 但是我们知道翻译是有很多种方式的，同一句话可以翻译成很多不同的句子，那么我们如何判断哪一个句子是最好的呢？ 还是翻译上面那句话，有如下几种翻译结果： “Jane is visiting China in September.” “Jane is going to visit China in September.” “In September, Jane will visit China” “Jane’s Chinese friend welcomed her in September.” … 与语言模型不同的是，机器模型在输出部分不再使用 softmax 随机分布的形式进行取样，因为很容易得到一个不准确的翻译，取而代之的是使用 Beam Search 做最优化的选择。这个方法会在后下一小节介绍，在此之前先介绍一下**贪婪搜索(Greedy Search)**及其弊端，这样才能更好地了解 Beam Search 的优点。 2.1 Greedy Search 得到最好的翻译结果，转换成数学公式就是: argmaxP(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;)argmax P(y^{&lt;1&gt;},…,y^{&lt;{T\\_y}&gt;}|x^{&lt;1&gt;},…,x^{&lt;{T\\_x}&gt;}) argmaxP(y&lt;1&gt;,…,y&lt;T_y&gt;∣x&lt;1&gt;,…,x&lt;T_x&gt;) 那么 Greedy Search 是什么呢？ 通俗解释就是每次输出的那个都必须是最好的。还是以翻译那句话为例。 现在假设通过贪婪搜索已经确定最好的翻译的前两个单词是：&quot;Jane is &quot; 然后因为 “going” 这个单词出现频率较高和其它原因，所以根据贪婪算法得出此时第三个单词的最好结果是 “going”。 所以据贪婪算法最后的翻译结果可能是下图中的第二个句子，但第一句可能会更好(不服气的话，我们就假设第一句更好). 所以贪婪搜索的缺点是局部最优并不代表全局最优，就好像五黑，一队都是很牛逼的，但是各个都太优秀，就显得没那么优秀了，而另一队虽然说不是每个都是最优秀，但是凑在一起就是能 carry 全场。 更形象的理解可能就是 Greedy Search 更加短视，看的不长远，而且也更加耗时。假设字典中共有 10000 个单词，如果使用 Greedy Search，那么可能的组合有 1000010 种，所以还是挺恐怖的 2333~~ 3. Beam Search Beam Search 是 greedy search 的加强版本，首先要预设一个值 beam width，这里等于 3 (如果等于 1 就是 greedy search)。然后在每一步保存最佳的 3 个结果进行下一步的选择，以此直到遇到句子的终结符. 3.1 步骤一 如下图示，因为beam width=3，所以根据输入的需要翻译的句子选出 3 个 y&lt;1&gt;y^{&lt;1&gt;}y&lt;1&gt;最可能的输出值，即选出P(y&lt;1&gt;∣x)P(y^{&lt;1&gt;}|x)P(y&lt;1&gt;∣x)最大的前3个值。假设分别是&quot;in&quot;,“jane”,“september” 3.2 步骤二 以&quot;in&quot;为例进行说明，其他同理. 如下图示，在给定被翻译句子 xxx 和确定 y&lt;1&gt;y^{&lt;1&gt;}y&lt;1&gt; = “in” 的条件下，下一个输出值的条件概率是 P(y&lt;2&gt;∣x,&quot;in&quot;)P(y^{&lt;2&gt;}|x,&quot;in&quot;)P(y&lt;2&gt;∣x,&quot;in&quot;)。此时需要从 10000 种可能中找出条件概率最高的前 3 个. 又由公式 P(y&lt;1&gt;,y&lt;2&gt;∣x)=P(y&lt;1&gt;∣x)P(y&lt;2&gt;∣x,y&lt;1&gt;)P(y^{&lt;1&gt;},y^{&lt;2&gt;}|x)=P(y^{&lt;1&gt;}|x) P(y^{&lt;2&gt;}|x, y^{&lt;1&gt;})P(y&lt;1&gt;,y&lt;2&gt;∣x)=P(y&lt;1&gt;∣x)P(y&lt;2&gt;∣x,y&lt;1&gt;), 我们此时已经得到了给定输入数据，前两个输出值的输出概率比较大的组合了. 另外 2 个单词也做同样的计算 此时我们得到了 9 组 P(y&lt;1&gt;,y&lt;2&gt;∣x)P(y^{&lt;1&gt;},y^{&lt;2&gt;}|x)P(y&lt;1&gt;,y&lt;2&gt;∣x), 此时我们再从这 9组 中选出概率值最高的前 3 个。如下图示，假设是这3个： “in september” “jane is” “jane visits” 3.3 步骤三 继续步骤2的过程，根据 P(y&lt;3&gt;∣x,y&lt;1&gt;,y&lt;2&gt;)P(y^{&lt;3&gt;}|x,y^{&lt;1&gt;},y^{&lt;2&gt;})P(y&lt;3&gt;∣x,y&lt;1&gt;,y&lt;2&gt;) 选出 P(y&lt;1&gt;,y&lt;2&gt;,y&lt;3&gt;∣x)P(y^{&lt;1&gt;},y^{&lt;2&gt;},y^{&lt;3&gt;}|x)P(y&lt;1&gt;,y&lt;2&gt;,y&lt;3&gt;∣x) 最大的前3个组合. 后面重复上述步骤得出结果. 3.4 总结 总结一下上面的步骤就是： (1). 经过 encoder 以后，decoder 给出最有可能的三个开头词依次为 “in”, “jane”, “september” P(y&lt;1&gt;∣x)P(y^{&lt;1&gt;}|x) P(y&lt;1&gt;∣x) (2). 经过将第一步得到的值输入到第二步中，最有可能的三个翻译为 “in september”, “jane is”, “jane visits” P(y&lt;2&gt;∣x,y&lt;1&gt;)P(y^{&lt;2&gt;}|x,y^{&lt;1&gt;}) P(y&lt;2&gt;∣x,y&lt;1&gt;) (这里，september开头的句子由于概率没有其他的可能性大，已经失去了作为开头词资格) (3). 继续这个过程… P(y&lt;3&gt;∣x,y&lt;1&gt;,y&lt;2&gt;)P(y^{&lt;3&gt;}|x,y^{&lt;1&gt;},y^{&lt;2&gt;}) P(y&lt;3&gt;∣x,y&lt;1&gt;,y&lt;2&gt;) 4. Refinements to beam search P(y&lt;1&gt;,….,P(yT_y)∣x)=P(y&lt;1&gt;∣x)P(y&lt;2&gt;∣x,y&lt;1&gt;)…P(y&lt;T_y&gt;∣x,y&lt;1&gt;,…y&lt;T_y−1&gt;)P(y^{&lt;1&gt;},….,P(y^{T\\_y})|x)=P(y^{&lt;1&gt;}|x)P(y^{&lt;2&gt;}|x,y^{&lt;1&gt;})…P(y^{&lt;{T\\_y}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{T\\_y-1}&gt;}) P(y&lt;1&gt;,….,P(yT_y)∣x)=P(y&lt;1&gt;∣x)P(y&lt;2&gt;∣x,y&lt;1&gt;)…P(y&lt;T_y&gt;∣x,y&lt;1&gt;,…y&lt;T_y−1&gt;) 所以要满足 argmaxP(y&lt;1&gt;,….,P(yT_y)∣x)argmax P(y^{&lt;1&gt;},….,P(y^{T\\_y})|x)argmaxP(y&lt;1&gt;,….,P(yT_y)∣x), 也就等同于要满足 argmax∏_t=1T_yP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;)argmax \\prod\\_{t=1}^{T\\_y}P(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;}) argmax∏_t=1T_yP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;) 但是上面的公式存在一个问题，因为概率都是小于1的，累乘之后会越来越小，可能小到计算机无法精确存储，所以可以将其转变成 log 形式（因为 log 是单调递增的，所以对最终结果不会有影响），其公式如下： argmax∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;)argmax \\sum\\_{t=1}^{T\\_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;}) argmax∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;) But！！！上述公式仍然存在bug，观察可以知道，概率值都是小于1的，那么log之后都是负数，所以为了使得最后的值最大，那么只要保证翻译的句子越短，那么值就越大，所以如果使用这个公式，那么最后翻译的句子通常都是比较短的句子，这显然不行。 所以我们可以通过归一化的方式来纠正，即保证平均到每个单词都能得到最大值。其公式如下： argmax1T_y∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;)argmax \\frac{1}{T\\_y}\\sum\\_{t=1}^{T\\_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;}) argmaxT_y1​∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;) 通过归一化的确能很好的解决上述问题，但是在实际运用中，会额外添加一个参数 ααα, 其大小介于 0 和 1 之间，公式如下: argmax1T_yα∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;)argmax \\frac{1}{T\\_y^α}\\sum\\_{t=1}^{T\\_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;}) argmaxT_yα1​∑_t=1T_ylogP(y&lt;t&gt;∣x,y&lt;1&gt;,…y&lt;t−1&gt;) T_yT\\_yT_y 为输出句子中单词的个数，ααα 是一个超参数 (可以设置为 0.7) ααα == 1. 则代表 完全用句子长度归一化 ααα == 0. 则代表 没有归一化 ααα == 0~1. 则代表 在 句子长度归一化 与 没有归一化 之间的折中程度. beam width = B = 3**10**100 是会有一个明显的增长，但是 B 从 1000 ~ 3000 是并没有一个明显增长的. 5. Error analysis on beam search 仔细想想 beam search，我们会发现其实它是近似搜索，也就是说可能使用这种方法最终得到的结果并不是最好的。当然也有可能是因为使用的 RNN 模型有缺陷导致结果不是最好的。 所以我们如何判断误差是出在哪个地方呢？ 还是以翻译这句话为例：“简在9月访问中国”。 假设按照人类的习惯翻译成英文是“Jane visits China in September.”,该结果用 y^\\* 表示。 假设通过算法得出的翻译结果是：“Jane visited China in September.”,该结果用 y^\\hat{y}y^​ 表示。 要判断误差出在哪，只需要比较 P(y^\\*|x) 和 P(y^∣x)P(\\hat{y}|x)P(y^​∣x) 的大小即可. 下面分两种情况讨论： RNN 实际上是 encode 和 decode 的过程. 两种情况： (1). P(y∗∣x)&gt;P(y^∣x) P(y^*|x)&gt;P(\\hat{y}|x) P(y∗∣x)&gt;P(y^​∣x) 上面的不等式的含义是 beam search 最后选出的结果不如人类，也就是 beam search 并没有选出最好的结果，所以问题出在 beam search (2). P(y∗∣x)≤P(y^∣x) P(y^*|x)≤P(\\hat{y}|x) P(y∗∣x)≤P(y^​∣x) 上面不等式表示 beam search 最后选出的结果要比人类的更好，也就是说 beam search 已经选出了最好的结果，但是模型对各个组合的预测概率值并不符合人类的预期，所以 RNN模型 at fault. 上面已经介绍了误差分析的方式，但时仅凭一次误差分析就判定谁该背锅肯定也不行，所以还需要进行多次误差分析多次。 如下图示已经进行了多次的误差分析，每次分析之后都判定了锅该谁背，最后计算出beam search和模型背锅的比例，根据比例作出相应的调整。 例如: 如果 beam search 更高，可以相应调整 beam width. 如果模型背锅比例更高，那么可以考虑增加正则化，增加数据等操作. 6. Bleu score (optional) 主要介绍了如何给机器翻译结果打分，因为是选修内容, 所以 balabala… 7. Attention model intuition 之前介绍的 RNN 翻译模型存在一个很明显的问题就是: 机器翻译的翻译过程是首先将所有需要翻译的句子输入到 Encoder 中，之后再通过 Decoder 输出翻译语句. 7.1 Why Attention model 如下图示机器算法将法语翻译成英语的模型. 机器翻译与人类的翻译过程不太相同。因为人类翻译一般是逐句翻译，或者是讲一段很长的句子分解开来进行翻译。 所以上述模型的翻译结果的 Bleu评分 与被翻译句子的长短有很大关系，句子较短时，模型可能无法捕捉到关键信息，所以翻译结果不是很高；但是当句子过长时，模型又抓不到重点等原因使得结果也不是很高。 ​见上图，如果机器能像人一样逐句或者每次将注意力只集中在一小部分进行翻译，那么翻译结果将不受句子长度的影响。下图中的绿色线即为使用了注意力模型后的翻译句子得分。 7.2 Attention model intro 下图展示了普通的翻译模型双向 RNN 结构，该结构可根据输入 x&lt;t&gt;x^{&lt;{t}&gt;}x&lt;t&gt; 直接得到输出 y&lt;t&gt;y^{&lt;{t}&gt;}y&lt;t&gt;. Attention model 在此基础上做进一步处理。 为避免误解，使用另一个符号 sss 来表示节点。 如下图示，根据下面一层的 双向RNN 计算结果可得到节点 s&lt;1&gt;s^{&lt;1&gt;}s&lt;1&gt; 与其他节点权重 α&lt;1,1&gt;,α&lt;1,2&gt;,…α^{&lt;1,1&gt;},α^{&lt;1,2&gt;},…α&lt;1,1&gt;,α&lt;1,2&gt;,… 通过这些权重可以知道该节点与其他节点的相关联程度，从而可以达到将注意力集中到部分区域的效果。 ​其他节点同理。整个注意力模型结构如下图示 8. Attention model 特别要区分 aaa (字母a) 和 ααα (alpha)。前者表示特征节点，后者表示注意力权重。 8.1 参数介绍 如下图示，注意力模型采用双向 RNN 结构，所以每个节点有两个值，用 a→&lt;t′&gt;,a←&lt;t′&gt;\\overrightarrow{a}^{&lt;{t&#x27;}&gt;},\\overleftarrow{a}^{&lt;{t&#x27;}&gt;}a&lt;t′&gt;,a&lt;t′&gt; 表示，为了使公式更简化，令 a&lt;t′&gt;=(a→&lt;t′&gt;,a←&lt;t′&gt;)a^{&lt;{t&#x27;}&gt;}=(\\overrightarrow{a}^{&lt;{t&#x27;}&gt;},\\overleftarrow{a}^{&lt;{t&#x27;}&gt;})a&lt;t′&gt;=(a&lt;t′&gt;,a&lt;t′&gt;) 。其中 t′t&#x27;t′ 表示输入数据的索引。 上一节已经介绍了注意力权重 α&lt;t,t′&gt;α^{&lt;{t,t&#x27;}&gt;}α&lt;t,t′&gt;，以第一个节点为例，它的权重值可以用 α&lt;1,t′&gt;α^{&lt;{1,t&#x27;}&gt;}α&lt;1,t′&gt; 表示，且所有权重值满足 ∑α&lt;1,t′&gt;=1\\sum{α^{&lt;{1,t&#x27;}&gt;}}=1∑α&lt;1,t′&gt;=1 所有权重与对应节点的线性之和用 c&lt;t′&gt;c^{&lt;{t&#x27;}&gt;}c&lt;t′&gt; 表示（为方便书写，用 ccc 表示）,ccc 表示 context，即上下文变量. 还是以第一个节点为例，c 的计算公式如下： c&lt;1&gt;=∑_t′α&lt;1,t′&gt;a&lt;t′&gt;c^{&lt;1&gt;}=\\sum\\_{t&#x27;}α^{&lt;{1,t&#x27;}&gt;}a^{&lt;{t&#x27;}&gt;} c&lt;1&gt;=∑_t′α&lt;1,t′&gt;a&lt;t′&gt; 8.2 注意力权值计算公式 α&lt;t,t′&gt;=exp(e&lt;t,t′&gt;)∑_t′′=1T_xexp(et,t′′)\\alpha^{&lt;{t,t&#x27;}&gt;}=\\frac{exp(e^{&lt;{t,t&#x27;}&gt;})}{\\sum\\_{t&#x27;&#x27;=1}^{T\\_x}{exp(e^{t,t&#x27;&#x27;})}} α&lt;t,t′&gt;=∑_t′′=1T_xexp(et,t′′)exp(e&lt;t,t′&gt;)​ 上面公式中的 e&lt;t,t′&gt;e^{&lt;{t,t&#x27;}&gt;}e&lt;t,t′&gt; 计算图如下： 其中 s&lt;t−1&gt;s^{&lt;{t-1}&gt;}s&lt;t−1&gt; 表示上一个状态的值, a&lt;t′&gt;a^{&lt;{t&#x27;}&gt;}a&lt;t′&gt; 表示第 t′t&#x27;t′ 个特征节点. Andrew Ng 并没有详细的介绍上面的网络，只是一笔带过，说反向传播和梯度下降会自动学习，emmm。。那就这样吧。 结合下图可以独自参考一下上面的公式是什么意思. 8.3 大数据文摘 下面的笔记是《大数据文摘》的笔记，感觉他写的清楚一些: 通过之前的学习可以看到机器翻译是将所有要翻译的内容统一输入然后再开始生成结果，但这样有一个弊端就是在句子特别长的时候后面的内容有的时候无法翻译的特别的准确。通过搭建 attention model 可以解决这个问题: 如图所示，这是一个 BRNN，并且在普通 RNN 的基础上增加 attention层，将阶段性的输入部分转化为输出，这样的方式也更符合人类的翻译过程。 让我们拿出细节部分仔细的理解一下，首先是 attention 层，也就是下图中 context&lt;t&gt;context^{&lt;{t}&gt;}context&lt;t&gt; ，每一个 attention 单元接受 三个单词的输入所以也称作语境单元（context）， α 是每单个输入词在语境单元中占得权重。对每一个语境单元 t 来说，因为 α 是通过 softmax 决定的，所以 ∑_i=1T_xαt,i=1\\sum\\_{i=1}^{T\\_x}α^{t,i}=1∑_i=1T_xαt,i=1. 这里决定终每一个单词占得语境权重仍然是通过一 个小型的神经网络来进行计算并且后得到的。 输出的 context&lt;t&gt;context^{&lt;{t}&gt;}context&lt;t&gt; 进入到下一层 Post LSTM 这一步就和之前学习过的那样子，将前一步的输出与这一步经过重重分析的输入综合到一起产生这一步的输出。 让我们评估一下 attention model： 由于结构的复杂，计算量与时间比普通的语言模型要多和慢许多。不过对于机器翻译来说，由于每一句话并不会特别特比的长，所以有的时候稍微慢一点也不是完全无法接受. 一个重要 attention model 应用就是语音识别，人通过麦克风输入一句话让机器来翻译输入的内容，来看一下是如何实现的 9. Speech recognition 一般语音识别过程是如下图示的，即首先将原音频 (黑白的，纵轴表示振幅) 转化成纵轴为频率的音谱图，并且通过人工预先设定的音素(phonemes)再来识别. 当人对着麦克风录入一句话，麦克风记录下来的是空气细微的震动的强度，以及频率。人耳在听到一句话的时候其 实做的是类似的处理。在深度学习没有特别流行之前，比较流行的是用音节做语音识别，但现在因为有了强大的 attention model，得到的结果比音节的效果更好。 CTC(connectionist temporal classiﬁcation)是之前较为常用的方法。 具体原理如下： 假设每秒音频可以提取出 100 个特征值，那么假设 10秒 的音频就有 1000 个特征值，那么输出值也有 1000 个，但是说出的话并没有这么多啊，那该怎么处理呢？ 方法很简单，只需要把“”进行压缩即可，注意需要将 &quot;&quot;和空额区分开来，因为空格也是占一个字符的。 10. Trigger word detection 假设下图式训练集中的一段音频，其中包含了两次唤醒词: 搭建一个 attention model，在听到唤醒词之前一直输出的是 0，在听到唤醒词以后输出 1，但因为一个唤醒词会持续半秒左右所以我们也不仅仅只输出一次 1，而是将输出的 1 持续一段时间，通过这样的方式训练出的 RNN 就可以很 有效的检测到唤醒词了。 11. Summary and thank you 终于学完了。虽然并不能说明什么~~~233333 感谢吴恩达和他的团队给我们带来这么好的教程 Reference 网易云课堂 - deeplearning DeepLearning.ai学习笔记汇总 deeplearning.ai深度学习课程字幕翻译项目 seq2seq学习笔记","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Chatbot Research 3 - 机器学习构建 chatbot","slug":"nlp/chatbot/chatbot-research3","date":"2018-08-13T06:00:21.000Z","updated":"2021-06-20T04:12:28.340Z","comments":true,"path":"2018/08/13/nlp/chatbot/chatbot-research3/","link":"","permalink":"http://www.iequa.com/2018/08/13/nlp/chatbot/chatbot-research3/","excerpt":"Chatterbot 聊天机器人框架 检索与匹配 &amp; 分类与朴素贝叶斯","text":"Chatterbot 聊天机器人框架 检索与匹配 &amp; 分类与朴素贝叶斯 关于聊天机器人的思考 工程考量 机器学习角度考虑 预备知识 检索与匹配 分类与朴素贝叶斯 chatterbot 架构与使用方法 源码分析 1. 传统聊天机器人 NLP 基础知识 基本分词 (jieba) 关键词抽取 (tf-idf等) 正则表达式模式匹配 … Machine Learning相关知识 文本表示与匹配 分类 (文本场景分析) 数据驱动 (特征工程) … 2. 聊天机器人的一些思考 工程考量 架构设计清晰、模块化 功能分拆，解耦，部件可插拔与扩展 算法与机器学习角度考量 算法简单，数据(特征)驱动 场景化与垂直领域 3. 预备知识 基于检索与匹配 知识库 (存储了问题与回复内容) 检索: 搜寻相关问题 匹配: 对结果进行排序 编辑距离 编辑距离/Levenshtein距离，是指两个字符串之间，由一个转成另一个所需要的最少编辑操作次数。 递归 &amp; 动态规划 DP TFIDF QA pair 的 TFIDF 的相近度 S1: “你喜欢什么书” S2: “你喜欢什么电影” python 编辑距离 Python在string 类型中，默认的 utf-8 编码下，一个中文字符是用三个字节来表示的。用unicode。 1234567# -*- coding:utf-8 -*-import Levenshteintexta = &quot;u关键时刻&quot;textb = &quot;u关键·时刻&quot;print(Levenshtein.distance(texta,textb)) # 1 4. Chatterbot 聊天机器人 每个部分都设计了不同的 “适配器”(Adapter) 机器人应答逻辑 =&gt; Logic Adapters Closest Match Adapter 字符串模糊匹配(编辑距离) Closest Meaning Adapter 借助 nltk 的 WordNet，近义词评估 Time Logic Adapter 处理涉及时间的提问 Mathematical Evaluation Adapter 涉及数学运算 存储器后端 =&gt; Storage Adapters Read Only Mode 只读模式，当有输入数据到 chatterbot 的时候，数据库并不会发生改变 Json Database Adapter 用以存储对话数据的接口，对话数据以 Json格式 进行存储. (载入数据特别慢，工业界不可行) Mongo Database Adapter 以 MongoDB database 方式来存储对话数据 输入形式 =&gt; Input Adapters Variable input type adapter 允许 chatterbot 接收不同类型的输入的，如 strings, dictionaries 和Statements Terminal adapter 使得 ChatterBot 可以通过终端进行对话 Speech recognition 语音识别输入，详见 chatterbot-voice 5. Bayes 分类 预备知识:场景分类与NB Reference github ChatterBot Docs » About ChatterBot 使用chatterbot构建自己的中文chat(闲聊)机器人/ 自然语言处理 Chatterbot聊天机器人 精品: 开源ChatterBot工作原理","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"}]},{"title":"Chatbot Research 2 - NLP基础知识回顾","slug":"nlp/chatbot/chatbot-research2","date":"2018-08-12T06:00:21.000Z","updated":"2021-06-20T04:12:28.338Z","comments":true,"path":"2018/08/12/nlp/chatbot/chatbot-research2/","link":"","permalink":"http://www.iequa.com/2018/08/12/nlp/chatbot/chatbot-research2/","excerpt":"NLTK Python上著名的自然语言处理库。 自带语料库，词性分类库， 还有强大的社区支持。","text":"NLTK Python上著名的自然语言处理库。 自带语料库，词性分类库， 还有强大的社区支持。 文本处理流程 分词 归一化 停止词 **NLP经典三案例 ** 情感分析 文本相似度 文本分类 斯坦佛 CoreNLP (英文、中文、西班牙语) 1. NLTK Python 著名的自然语言处理库 自带语料库、词性分类库 自带分类、分词 等功能 强大的社区支持 12sudo pip install -U nltksudo pip install -U numpy 12import nltknltk.download()","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"},{"name":"NLTK","slug":"NLTK","permalink":"http://www.iequa.com/tags/NLTK/"}]},{"title":"Chatbot Research 1 - 聊天机器人的行业综述","slug":"nlp/chatbot/chatbot-research1","date":"2018-08-11T06:00:21.000Z","updated":"2021-06-20T04:12:28.337Z","comments":true,"path":"2018/08/11/nlp/chatbot/chatbot-research1/","link":"","permalink":"http://www.iequa.com/2018/08/11/nlp/chatbot/chatbot-research1/","excerpt":"英文原文： 《Deep Learning For Chatbots, Part 1 - Introduction》 中文翻译： 《聊天机器人中的深度学习导读》","text":"英文原文： 《Deep Learning For Chatbots, Part 1 - Introduction》 中文翻译： 《聊天机器人中的深度学习导读》 这篇博文主要概述了目前聊天机器人主要用到的技术，从宏观上进行介绍，不涉及具体的技术细节。 聊天机器人 (Chatbot)，也被称为对话引擎或者对话系统，是目前的热点之一。微软公司在聊天机器人上的投入巨大（链接），著名产品有小冰 (xiaoice)、bot framework等，其他公司也纷纷在这个领域发力，如Facebook的M，苹果公司的Siri等等。此外，一大批的创业公司发布了类似的产品，例如客户应用Operator，x.ai，bot framework Chatfuel，bot开发工具 Howdy’s Botkit。微软也发布了供开发者使用的 bot developer framework 还有一些应用案例，列举如下： 案例:语音助手 Siri 案例:餐饮 Pizza Hut 案例:健身 Fitness Tips 案例:旅游 expedia 案例:医疗 healthtap 案例:新闻 wordpress 案例:财经 meetcleo 许多公司希望能开发出与用户进行自然语言式对话的机器人，并且声称使用了NLP和深度学习相关技术使之成为可能，然而这并不容易实现。在这个博文系列中，我将会重温一些被用于聊天机器人中的深度学习技术，披露出目前技术能够解决或者可能解决的问题以及几乎难以解决的问题。这篇文章是个概述，在接下来的博文中将介绍具体的技术细节。 1. 模型分类 1.1 检索技术模型 VS 生成式模型 基于检索技术的模型较为简单，主要是根据用户的输入和上下文内容，使用了知识库（存储了事先定义好的回复内容）和一些启发式方法来得到一个合适的回复。启发式方法简单的有基于规则的表达式匹配，复杂的有一些机器学习里的分类器。这些系统不能够生成任何新的内容，只是从一个固定的数据集中找到合适的内容作为回复。 生成式模型则更加复杂，它不依赖于预定义好的回复内容，而是通过抓取(Scratch)的方法生成新的回复内容。生成式模型典型的有基于机器翻译模型的，与传统机器翻译模型不同的是，生成式模型的任务不是将一句话翻译成其他语言的一句话，而是将用户的输入[翻译]为一个回答(response) 1.2 模型分类总结 以上两种模型均有优缺点。对于基于检索技术的模型，由于使用了知识库且数据为预先定义好的，因此进行回复的内容语法上较为通顺，较少出现语法错误；但是基于检索技术的模型中没有会话概念，不能结合上下文给出更加[智能]的回复。而生成式模型则更加[智能]一些，它能够更加有效地利用上下文信息从而知道你在讨论的东西是什么；然而生成式模型比较难以训练，并且输出的内容经常存在一些语法错误（尤其对于长句子而言），以及模型训练需要大规模的数据。 深度学习技术都能够用于基于检索技术的模型和生成式模型中，但是目前的研究热点在生成式模型上。深度学习框架例如Sequence to Sequence非常适合用来生成文本，非常多的研究者希望能够在这个领域取得成功。然而目前这一块的研究还在初期阶段，工业界的产品更多的还是使用基于检索计算的模型。 2. 问题分类 2.1 短对话 VS 长对话 直观上处理长对话内容将更加困难，这是因为你需要在当前对话的情境下知道之前的对话说过什么。如果是一问一答的形式，技术上这将简单的多。通常对于客服对话而言，长对话更加常见，一次对话中往往会伴随着多个关联问题。 2.2 开放域 VS 特定领域 面向开放域的聊天机器人技术面临更多困难，这是因为会话可能涉及的面太广，没有一个清晰的目标和意图。在一些社交网站例如Twitter和Reddit上的会话是属于开放域的，会话涉及的主题多种多样，需要的知识量也将非常巨大。 面向特定领域的相关技术则相对简单一些，这是因为特定领域给会话的主题进行了限制，目标和意图也更加清晰，典型的例子有客服系统助手和购物助手。这些系统通常是为了完成某些特定任务，尽管用户在该系统中也能够问些其他方面的东西，但是系统并不会给出相应的回复。 3. 面临的挑战 下面介绍一下聊天机器人技术所面临的挑战。 3.1 如何结合上下文信息 为了产生质量更高的回复，聊天机器人系统通常需要利用一些上下文信息(Context)，这里的上下文信息包括了对话过程中的语言上下文信息和用户的身份信息等。在长对话中人们关注的是之前说了什么内容以及产生了什么内容的交换，这是语言上下文信息的典型。常见的方法是将一个会话转化为向量形式，但这对长会话而言是困难的。论文Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models和Attention with Intention for a Neural Network Conversation Model中的实验结果表明了这一点。另外，可以结合的上下文信息还包括会话进行时的日期地点信息、用户信息等。 3.2 语义一致性 理论上来说，机器人面对相同语义而不同形式的问题应该给予一致的回复，例如这两个问题[How old are you?]和[What’s your age?]。这理解起来是简单的，但却是学术界目前的难题之一（如下图）。许多系统都试图对相同语义而不同形式的问题给予语义上合理的回复，但却没有考虑一致性，最大的原因在于训练模型的数据来源于大量不同的用户，这导致机器人失去了固定统一的人格。论文A Persona-Based Neural Conversation Model中提及的模型旨在创建具有固定统一人格的机器人。 3.3 对话模型的评测 评价一个对话模型的好坏在于它是否很好地完成了某项任务，例如在对话中解决了客户的问题。这样的训练数据需要人工标注和评测，所以获取上需要一定人力代价。有时在开放域中的对话系统也没有一个清晰的优化目标。用于机器翻译的评测指标BLEU不能适用于此，是因为它的计算基础是语言表面上的匹配程度，而对话中的回答可以是完全不同词型但语义通顺的语句。论文How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation中给出结论，目前常用的评测指标均与人工评测无关。 3.4 意图和回复多样性 生成式模型中的一个普遍问题是，它们都想要生成一些通用的回答，例如[That’s great!]和[I don’t know]这样的可以应付许多的用户询问。早期的Google智能回复基本上以[I love you]回复所有的东西链接，这是一些模型最终训练出来的结果，原因在于训练数据和训练的优化目标。因此，有些研究学者开始关注如何提升机器人的回复的多样性，然而人们在对话过程中的回复与询问有一定特定关系，是有一定意图的，而许多面向开放域的机器人不具备特定的意图。 3.5 实际效果 以目前的研究水平所制造的机器人能够取得的效果如何？使用基于检索技术的显然无法制作出面向开放域的机器人，这是因为你不能编写覆盖所有领域的语料；而生成式的面向开放域的机器人还属于通用人工智能(Artifical General Intelligence, AGI)水平，距离理想状态还相距甚远，但相关研究学者还在致力于此。 对于特定领域的机器人，基于检索的技术和生成式模型都能够利用。但是对于长对话的情境，也面临许多困难。 在最近对Andrew NG的采访中，NG提到： 目前深度学习的价值主要体现在能够获取大量数据的特定领域。 目前一个无法做的事情是产生一个有意义的对话。 详细： 当今深度学习的价值在你可以获得许多数据的狭窄领域内。有一件事它做不到：进行有意义的对话。存在一些演示，并且如果你仔细挑选这些对话，看起来就像它正在进行有意义的对话，但是如果你亲自尝试，它就会快速偏离轨道。 许多创业公司声称只要有足够多的数据，就能够产生自动智能的对话系统。然而，目前的水平生产出面向一个特定的子领域的对话应用（如利用Uber打车），而对于一个稍微开放点的领域就难以实现了（如自动销售）。但是，帮助用户提供自动回复建议以及语法纠正还是可行的。 使用基于检索技术的对话系统更加可控和稳定，给出的回复出现语法错误的几率更低。而使用生成式模型的风险在于回复不可控，且容易出现一些风险，例如微软的Tay。 4. 即将到来的事情和阅读列表 在之后的博文中将具体介绍深度学习方面的技术细节。提前阅读下面的文章将会对后面的学习更加有帮助。 Neural Responding Machine for Short-Text Conversation (2015-03) A Neural Conversational Model (2015-06) A Neural Network Approach to Context-Sensitive Generation of Conversational Responses (2015-06) The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems (2015-06) Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models (2015-07) A Diversity-Promoting Objective Function for Neural Conversation Models (2015-10) Attention with Intention for a Neural Network Conversation Model (2015-10) Improved Deep Learning Baselines for Ubuntu Corpus Dialogs (2015-10) A Survey of Available Corpora for Building Data-Driven Dialogue Systems (2015-12) Incorporating Copying Mechanism in Sequence-to-Sequence Learning (2016-03) A Persona-Based Neural Conversation Model (2016-03) How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation (2016-03) 数据驱动的意义是： 算法越简单，解释性越好 数据量足够大，覆盖的真实世界的大部分场景","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"}]},{"title":"Sequence Models (week2) - NLP - Word Embeddings","slug":"deeplearning/Sequence-Models-week2","date":"2018-08-02T08:00:21.000Z","updated":"2021-06-20T04:12:28.254Z","comments":true,"path":"2018/08/02/deeplearning/Sequence-Models-week2/","link":"","permalink":"http://www.iequa.com/2018/08/02/deeplearning/Sequence-Models-week2/","excerpt":"能够将序列模型应用到自然语言问题中，包括文字合成。","text":"能够将序列模型应用到自然语言问题中，包括文字合成。 1. Word Representation 上周的学习中，学习了如何用独热编码来代表一个词，这一节我们来探究一下词和词之间的联系。比如有下面这句话： 1“I want a glass of orange ________” 假如我们的 RNN 的模型通过训练已经学会了短语 “orange juice”，并准确的预测了这句话的空格部分，那么如果遇到了另一句话时，比如： 1“I want a glass of apple _________” 是否需要从头学习短语 “apple juice” 呢？能否通过构建 “apple” 与 “orange” 的联系让它不需要重学就能进行判断呢？ 能否通过构建 “apple” 与 “orange” 的联系让它不需要重学就能进行判断呢？ 所以下面给出了一种改进的表示方法，称之为“词嵌入(Word Embedding)” 1.1 词汇的特性 单词与单词之间是有很多共性的，或在某一特性上相近，比如“苹果”和“橙子”都是水果；或者在某一特性上相反，比如“父亲”在性别上是男性，“母亲”在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率，这里用一个简化的表格说明: Man (5391) Woman (9853) Apple (456) Orange (6257) 性别 -1 1 0 年龄 0.01 0.02 -0.01 食物 0.04 0.01 0.95 颜色 0.03 0.01 0.70 在表格中可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。括号里的数字代表这个单词在独热编码中的位置，可以用这个数字代表这个单词比如 Man = ，Man 的特性用 ，也就是那一纵列。 在实际的应用中，特性的数量远不止 4 种，可能有几百种，甚至更多。对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。 这里还介绍了一个 t-SNE 算法，因为词性表本身是一个很高维度的空间，通过这个算法压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个就是 “Word Embeddings” 的概念. 上图通过聚类将词性相类似的单词在二维空间聚为一类. 2. Using Word Embeddings 先下一个非正规定义 “词嵌 - 描述了词性特征的总量，也是在高维词性空间中嵌入的位置，拥有越多共性的词，词嵌离得越近，反之则越远”。值得注意的是，表达这个“位置”，需要使用所有设定的词性特征，假如有 300 个特征（性别，颜色，…），那么词嵌的空间维度就是 300. 2.1 使用词嵌三步 获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库 应用词嵌：将获得的词嵌应用在我们的训练任务中 可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了） 2.2 词嵌实用场景 No. sencentce replace word target 1 Sally Johnson is an orange farmer. orange Sally Johnson 2 Robert Lin is an apple farmer. apple Robert Lin 3 Robert Lin is a durian cultivator. durian cultivator Robert Lin 我们继续替换，我们将 apple farmer 替换成不太常见的 durian cultivator (榴莲繁殖员)。此时词嵌入中可能并没有 durian 这个词，cultivator 也是不常用的词汇。这个时候怎么办呢？我们可以用到迁移学习。 词嵌入迁移学习步骤如下： 学习含有大量文本语料库的词嵌入 (一般含有 10亿 到 1000亿 单词)，或者下载预训练好的词嵌入 将学到的词嵌入迁移到相对较小规模的训练集 (例如 10万 词汇). (可选) 这一步骤就是对新的数据进行 fine-tune。 3. Properties of Word Embeddings 假设有如下的问题： 1&quot;Man&quot; -&gt; &quot;Woman&quot; 那么 &quot;King&quot; -&gt; ？ 这个问题被称作词汇的类比问题，通过研究词嵌的特征可以解决这样的问题. 数学的表达式为： e_man−e_woman ≈ e_king−e_we\\_{man} - e\\_{woman} \\, \\approx \\, e\\_{king}-e\\_w e_man−e_woman≈e_king−e_w e_we\\_we_w 是什么呢？ 在高纬度空间中（300D） argmax_w Similarity(e_w,e_king−e_man+e_woman)argmax\\_w \\;\\, Similarity(e\\_w, e\\_{king}-e\\_{man}+e\\_{woman}) argmax_wSimilarity(e_w,e_king−e_man+e_woman) 这个公式相当于在算两个向量(vector)的cos相似度 Similarity(u,v)=uTv∣∣u∣∣_2∣∣v∣∣_2Similarity(u,v) = \\frac {u^Tv} {||u||\\_2||v||\\_2} Similarity(u,v)=∣∣u∣∣_2∣∣v∣∣_2uTv​ 当然也可以用其他距离公式, 但是多数是用这个余弦相似度. 如下图用几何方式能够更容易理解，即只要找到与向量 AB⃗\\vec{AB}AB 最接近平行的向量 CD⃗\\vec{CD}CD 即可. 4. Embedding Matrix 这一节中主要讲了词嵌矩阵的shape，如果词嵌（词性特征的总量）是300，独热编码的长度是10000，那么词嵌矩阵的的shape就是 300 * 10000 。所以就有了下面的式子： 词嵌矩阵 * 单词的独热编码 = 单词的词嵌 (300, 10000) * (10000, 1) = (300, 1) 5. Learning Word Embeddings 可以通过训练神经网络的方式构建词嵌表 E . 下图展示了预测单词的方法，即给出缺少一个单词的句子： “I want a glass of orange ___” 计算方法是将已知单词的特征向量都作为输入数据送到神经网络中去，然后经过一系列计算到达 Softmax分类层，在该例中输出节点数为10000个。经过计算juice概率最高，所以预测为 “I want a glass of orange juice” 在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表 EEE 假设要预测的单词为 WWW，词嵌表仍然为 EEE，需要注意的是训练词嵌表和预测 WWW 是两个不同的任务。 如果任务是预测 WWW，最佳方案是使用 WWW 前面 nnn 个单词构建语境。 如果任务是训练 EEE，除了使用 WWW 前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 Skip Gram 算法），这些方法都能提供很好的结果。 6. Word2Vec 视频中一直没有给 Word2Vec 下一个明确的定义，我们再次下一个非正式定义便于理解: “word2vec” 是指将词语 word 变成向量vector 的过程，这一过程通常通过浅层的神经网络完成，例如 CBOW 或者skip gram，这一过程同样可以视为构建词嵌表 EEE 的过程”。 6.1 Skip-grams 这里着重介绍了skip gram model，这是一个用一个随机词预测其他词的方法。比如下面这句话中 “I want a glass of orange juice.” 我们可以选 orange作为随机词 c(Context)，通过设置窗口值例如前后 5 个单词以监督学习的方式去预测其中的词t(Target) 例如 “juice, glass, a, of” 但需要注意的是，这个过程仍然是为了搭建（更新）词嵌表 EEE 而不是为了真正 的去预测，所以如果预测效果不好并不用担心，表达式： O_c→E→e_c→OutputSoftmax→y^O\\_{c}\\rightarrow E \\rightarrow e\\_{c} \\rightarrow \\underset{Softmax}{Output} \\rightarrow \\hat{y} O_c→E→e_c→SoftmaxOutput​→y^​ Softmax公式为(假设输出节点数为10000)： p(t∣c)=eθ_tTe_c∑_j=110000eθ_jTe_cp(t|c)=\\frac{e^{θ\\_t^Te\\_c}}{\\sum\\_{j=1}^{10000}e^{θ\\_j^Te\\_c}} p(t∣c)=∑_j=110000eθ_jTe_ceθ_tTe_c​ θ_tθ\\_tθ_t 表示与ttt有关的参数 损失函数： l(y^,y)=∑_i=110000y_ilogy_i^l(\\hat{y},y)=\\sum\\_{i=1}^{10000}y\\_ilog\\hat{y\\_i} l(y^​,y)=∑_i=110000y_ilogy_i^​ 在skip gram中有一个不足是 softmax 作为激活函数需要的运算量太大，在上限为10000个单词的词库中就已经比较慢了。一种补救的办法是用一个它的变种 “Hierachical Softmax (分层的Softmax)”，通过类似二叉树的方法提高训练的效率 例如一些常见的单词，如the、of等就可以在很浅的层次得到，而像durian这种少用的单词则在较深的层次得到 7. Negative Sampling 负采样 对于skip gram model而言，还要解决的一个问题是如何取样（选择）有效的随机词 ccc 和目标词 ttt 呢？如果真的按照自然随机分布的方式去选择，可能会大量重复的选择到出现次数频率很高的单词比如说 “the, of, a, it, I, …” 重复的训练这样的单词没有特别大的意义. 如何有效的去训练选定的词如 orange 呢？在设置训练集时可以通过“负取样”的方法, 下表中第一行是通过和上面一样的窗口法得到的“正”（1）结果，其他三行是从字典中随机得到的词语，结果为“负”（0）。通过这样的负取样法可以更有效地去训练 skip gram model. context word target? orange juice 1 orange king 0 orange book 0 orange the 0 负取样的个数 k 由数据量的大小而定，上述例子中为3. 实际中数据量大则 k = 2 ~ 5，数据量小则可以相对大一些 k = 5 ~ 20 通过负取样，我们的神经网络训练从 softmax 预测每个词出现的频率变成了经典 binary logistic regression 问题，概率公式用 sigmoid 代替 softmax 从而大大提高了速度. x_1=(orange,juice)→y_1=1x_2=(orange,king)→y_2=0...P(y=1∣c,t)=σ(θ_tTe_c)x\\_1=(orange, juice) \\rightarrow y\\_1=1 \\\\\\\\ x\\_2=(orange, king) \\rightarrow y\\_2=0 \\\\\\\\ ... \\\\\\\\ P(y=1|c,t)=\\sigma(\\theta\\_t^Te\\_c) x_1=(orange,juice)→y_1=1x_2=(orange,king)→y_2=0...P(y=1∣c,t)=σ(θ_tTe_c) 最后我们通过一个并没有被理论验证但是实际效果很好的方式来确定每个被负选样选中的概率为： P(w_i)=f(w_i34)∑_j=110000f(w_j34)P(w\\_i)=\\frac{f(w\\_i^{\\frac{3}{4}})} {\\sum\\_{j=1}^{10000}f(w\\_j^{\\frac{3}{4}})} P(w_i)=∑_j=110000f(w_j43​)f(w_i43​)​ 8. GloVe Word Vectors GloVe(Global vectors for word representation)虽不像Word2Vec模型那样流行，但是它也有自身的优点. 9. Sentiment Classification 平时上淘宝我们都会对买的东西给出文字评价和对应的星级评价，如下图示。 商家可以通过对这些数据来构建一个情绪分类器，从而可以在一些社交平台上如微博、QQ等大家的文字评论然后对应输出相应的星级等级，这样就可以更容易知道自家店是蒸蒸日上还是日落西山了,hehehe。 可以看到下图中的模型先将评语中各个单词通过 词嵌表(数据量一般比较大，例如有100Billion的单词数) 转化成对应的特征向量，然后对所有的单词向量做求和或者做平均，然后构建Softmax分类器，最后输出星级评级。 但是上面的模型存在一个问题，一般而言如果评语中有像&quot;good、excellent&quot;这样的单词，一般都是星级评分较高的评语，但是该模型对下面这句评语就显得无能为力了： “Completely lacking in good taste, good service, and good ambience.” 该评语中出现大量的good，如果直接做求和或者平均运算，经过分类器得到的输出很大概率上是高星级评分的，但这显然与该评语的本意不符. 之所以上面的模型存在那样的缺点，就是因为它没有把单词的时序考虑进去，所以我们可以使用RNN构建模型来解决这种问题。RNN模型如下图示： 另外使用RNN模型还有另一个好处，假设测试集中的评语是这样的 “Completely absent of good taste, good service, and good ambience.” 该评语只是将lacking in替换成了absent of，而且我们即使假设absent并没有出现在训练集中，但是因为词嵌表很庞大，所以词嵌表中包含absent，所以算法依旧可以知道absent和lacking有相似之处，最后输出的结果也依然可以保持正确。 10. Debiasing Word Embeddings 现如今机器学习已经被用到了很多领域，例如银行贷款决策，简历筛选。但是因为机器是向人们学习，所以好的坏的都会学到. 因为 RNN 通常是通过大量的网络数据文本集进行训练得到的，所以很多时候文本集中的偏见会反映在词嵌以及最终 的结果中，例如 当说到Man：程序员的时候，算法得出Woman：家庭主妇，这显然存在偏见。 又如Man：Doctor，算法认为Woman：Nurse。这显然也存在其实和偏见。 这种带有偏见的结果是应该尽力避免的，这类偏见大量存在于网络数据文本中，包括 性别偏见，种族偏见，年龄偏见，等等… 人类在这方面已经做的不对了，所以机器应当做出相应的调整来减少歧视. 给词嵌去偏见主要分三步(在词嵌的高维空间中完成): 找到偏见的方向(确定偏见的x，y轴) 将非定义化的词平移到x=0(父亲，母亲这类词就是定义化的词，本身就带有了性别的暗示) 使定义化的词据离移动的词距离相等 So word embeddings can reflect the gender, ethnicity, age, sexual, orientation, and other biases of the text used to train the model. One that I’m especially passionate about is bias relating to socioeconomic status. I think that every person, whether you come from a wealthy family, or a low income family, or anywhere in between, I think everyone should have a great opportunities. 下面将主要从性别歧视上来举例说明如何让机器学习消除偏见。 下图展示了一些单词，你可以在心里先想想你看到这些单词的第一时间认为他们所对应的性别是什么吧~~~ 1. 识别偏见方向 因为该例子是以消除性别歧视为目的，所以我们需要计算出图中这些单词之间的距离的平均值，进而作为偏见方向(bias direction) e_he−e_shee_boy−e_girle_grandmother−e_grandfathere\\_{he}-e\\_{she} \\\\\\\\ e\\_{boy}-e\\_{girl} \\\\\\\\ e\\_{grandmother}-e\\_{grandfather} e_he−e_shee_boy−e_girle_grandmother−e_grandfather 将上面所求做平均运算，得到的向量方向即为偏见方向 为方便理解，已在图中画出偏见方向，其余299D(除gender以外的其他单词特征)向量与偏见方向正交，也在下图中画出. 2. 词性中和 像“ boy, girl ”这类词在性别词性上是很明确的，而且不存在歧视，所以无需中和(Neutralize). 而图中的 babysister、doctor 则需要中和，具体方法就是将该词像非偏见方向投影得到一个新的坐标. 3. 单词对等距离化 如下图示，虽然 babysister 中和化，但是它还是离 grandmother 更近，所以依旧带有偏见 所以我们还需要将grandmother、grandfather这类与性别有关的对应词等距分布在非偏见方向的两侧(红色剪头表示移动方向，红色点表示移动后的新坐标)，如下图示。 11. Reference 网易云课堂 - deeplearning DeepLearning.ai学习笔记汇总 deeplearning.ai深度学习课程字幕翻译项目 seq2seq学习笔记","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Skin Care for Male","slug":"tools/male-skin-care-1","date":"2018-08-02T00:13:48.000Z","updated":"2021-06-22T06:54:43.488Z","comments":true,"path":"2018/08/02/tools/male-skin-care-1/","link":"","permalink":"http://www.iequa.com/2018/08/02/tools/male-skin-care-1/","excerpt":"","text":"有几个知识点关注我的朋友应该比较熟悉了： 1.防晒是最好的美白。 2.不用纠结一个硬币或者2mg/cm2涂抹量，这是测试极限值，不是日常用的值，但是尽可能多涂防晒，是没毛病的，是硬指标。 3.美白是系统工程，即使顶好顶好的美白产品，你拿30人做实验，实验效果也是要30天以上，所以不要求快，求安全就好 4.洗面奶这些不可能美白的，身体乳也不太可能，不然几百上千块的美白精华我们研发出来的意义何在？ 5.再好的美白产品，也抵不过你在太阳底下呆半个小时，防晒是美白的前提，最好硬防晒＋防晒霜。 6.如果你长年不见太阳的屁股或者大腿内侧都是黑色的朋友，你就不用纠结美白了，白不了的，最白也只能白到屁股或者大腿内侧的白度。 ---- 言雨潇 1. 防晒 1.1 防晒伞 1.2 防晒霜 2. 美白护肤 2.1 洗面奶 2.2 烟酰胺 Olay prox方程式 216¥/40ml 总结： 1.想要变白就注意防晒，不做好防晒涂再多的美白精华液都是白搭； 2.要注意选择适合自己肤质的美白产品； 3.刷酸不适合敏感肌和角质层薄的人，浓度太高建议在医生的指导下使用； 4.变白是一个需要时间的过程，周期在1-3个月，短时间内让人变白的护肤品大多数都是对身体有害的 5.审美是主观的，男生不只有白才是好看的，有时候偶尔黑一点更显得有阳刚气息。 Reference 男士面部肤色偏黑，怎么美白？用什么产品好？ ✨緊．亮．淨✨ProX by OLAY科研級精華系列💎 80%的人有颈椎病，好枕头胜过10个老中医","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"SkinCare","slug":"SkinCare","permalink":"http://www.iequa.com/tags/SkinCare/"}]},{"title":"Sequence Models (week1) - Recurrent Neural Networks","slug":"deeplearning/Sequence-Models-week1","date":"2018-07-26T11:00:21.000Z","updated":"2021-06-20T04:12:28.256Z","comments":true,"path":"2018/07/26/deeplearning/Sequence-Models-week1/","link":"","permalink":"http://www.iequa.com/2018/07/26/deeplearning/Sequence-Models-week1/","excerpt":"这次我们要学习专项课程中第五门课 Sequence Models. 第一周: Recurrent Neural Networks 已被证明在时间数据上表现好，它有几个变体，包括 LSTM、GRU 和双向神经网络.","text":"这次我们要学习专项课程中第五门课 Sequence Models. 第一周: Recurrent Neural Networks 已被证明在时间数据上表现好，它有几个变体，包括 LSTM、GRU 和双向神经网络. 1. Why sequence models? 为什么要学习序列模型呢? 序列模型, 普遍称为 RNN (递归神经网络 - Recurrent Neural Network), 做为深度学习中非常重要的一环，有着比普通神经网络更广的宽度与更多的可能性，其应用领域包括但不限于“语音识别”， “NLP”， “DNA序列分析”，“Machine Translation”， “视频动作分析”，等等… 有这样一种说法，也许并不严谨，但有助于我们理解RNN，大意是这样的: 普通神经网络处理的是一维的数据，CNN 处理的是二维的数据，RNN 处理的是三维的数据 最直观的理解是在 CNN 对图片的分析基础上，RNN 可以对视频进行分析，这里也就引入了第三维“时间”的概念 这一小节通过一个小例子为我们打开序列模型的大门，例子如下: 给出这样一个句子 “Harry Potter and Herminone Granger invented a new spell.&quot;(哈利波特与赫敏格兰杰发明了 一个新的咒语。)， 我们的任务是在这个句子中准确的定位到人名 Harry Potter 和 Herminone Granger. 用深度学习的语言来描述如下图 - 每一个单词对应一个输出 0 或者 1，1 代表着是人名，0 代表不是。 接下来我们要解决的一个问题是如何才能代表一个单词?，比如我们例子中的 “Harry”，这里我们介绍一种新的编码方式， 就是用另一种方式来代表每一个单词 - 独热编码（One-Hot Encoding）。 具体流程是这样，假设我们有 10000 个常用词，为其构建一个10000*1 的矩阵(column matrix)，假如第一个词是苹果(apple), 那么对应的第一个位置为 1，其他都为 0，所以称之为独热。这样每个单词都有对应的矩阵进行表示，如果这个词没有出现在我们的字典中，那么我们可以给一个特殊的符号代替，常用的是 (unknown) 2. Notation 为了后面方便说明，先将会用到的数学符号进行介绍. 以下图为例，假如我们需要定位一句话中人名出现的位置. 红色框中的为输入、输出值。可以看到人名输出用 1 表示，反之用 0 表示； 绿色框中的 x&lt;t x^{&lt; t \\&gt;}x&lt;t,y&lt;t y^{&lt; t \\&gt;}y&lt;t 表示对应红色框中的输入输出值的数学表示，注意从 1 开始. 灰色框中的 T_x,T_yT\\_x,T\\_yT_x,T_y 分别表示输入输出序列的长度，在该例中，T_x=9,T_y=9T\\_x=9,T\\_y=9T_x=9,T_y=9 黄色框中 X(i)&lt;t X^{(i)&lt; t \\&gt;}X(i)&lt;t 上的表示第 iii 个输入样本的第 ttt 个输入值，T_x(i)T\\_x^{ (i) }T_x(i) 则表示第 iii 个输入样本的长度。输出 yyy 也同理. 输入值中每个单词使用 One-Hot 来表示。即首先会构建一个字典(Dictionary), 假设该例中的字典维度是 10000*1 (如图示)。第一个单词 “Harry” 的数学表示形式即为 [0,0,0,……,1 (在第4075位) ,0,……,0]，其他单词同理。 但是如果某一个单词并没有被包含在字典中怎么办呢？此时我们可以添加一个新的标记，也就是一个叫做 Unknown Word 的伪造单词，用 &lt;UNK&gt; 表示。具体的细节会在后面介绍。 3. Recurrent Neural Network Model 在介绍 RNN 之前，首先解释一下为什么之前的标准网络不再适用了。因为它有两个缺点： 输入和输出的长度不尽相同 无法共享从其他位置学来的特征 例如上一节中的 Harry 这个词是用 x&lt;1&gt;x^{&lt;1&gt;}x&lt;1&gt; 表示的，网络从该位置学习了它是一个人名。但我们希望无论 Harry 在哪个位置出现网络都能识别出这是一个人名的一部分，而标准网络无法做到这一点. 输入层，比如每个 x&lt;1&gt;x^{&lt;1&gt;}x&lt;1&gt; 都是一个 1000 维的向量，这样输入层很庞大, 那么第一层的权重矩阵就有着巨大的参数. 3.1 RNN 结构 还是以识别人名为例,第一个单词 x&lt;1&gt;x^{&lt;1&gt;}x&lt;1&gt; 输入神经网络得到输出 y&lt;1&gt;y^{&lt;1&gt;}y&lt;1&gt; 同理, 由 x&lt;2&gt;x^{&lt;2&gt;}x&lt;2&gt; 将得到 y&lt;2&gt;y^{&lt;2&gt;}y&lt;2&gt;,以此类推。但是这就是传统网络存在的问题，即单词之间没有联系 为了将单词之间关联起来，所以将前一层的结果也作为下一层的输入数据。如下图示 整体的 RNN 结构有两种表示形式，如下图示, 左边是完整的表达形式，注意第一层的 a&lt;0&gt;a^{&lt;0&gt;}a&lt;0&gt; 一般设置为 0向量. 要开始整个流程, 需要编造一个激活值, 这通常是 0向量, 有些研究人员会用其他方法随机初始化 a&lt;0&gt;=0⃗a^{&lt;0&gt;}=\\vec{0}a&lt;0&gt;=0. 不过使用 0向量，作为 0时刻 的伪激活值 是最常见的选择. 因此我们把它输入神经网络. (右边的示意图是 RNN 的简写示意图) 介绍完结构之后，我们还需要知道网络中参数的表达方式及其含义。如下图示，x&lt;i&gt;x^{&lt;{i}&gt;}x&lt;i&gt; 到网络的参数用 W_axW\\_{ax}W_ax 表示，a&lt;i&gt;a^{&lt;{i}&gt;}a&lt;i&gt; 到网络的参数用 W_aaW\\_{aa}W_aa 表示，y&lt;i&gt;y^{&lt;{i}&gt;}y&lt;i&gt; 到网络的参数用 W_yaW\\_{ya}W_ya 表示，具体含义将在下面进行说明. x&lt;1&gt;x^{&lt;1&gt;}x&lt;1&gt; 通过网络可以传递到 y&lt;3&gt;y^{&lt;3&gt;}y&lt;3&gt; 但是这存在一个问题，即每个输出只与前面的输入有关，而与后面的无关。这个问题会在后续内容中进行改进. 举个🌰: He said, “Teddy Roosevelt was a great President.” 对于这句话，只知道 He said 前面两个词，来判断 Teddy 是否是人名是不够的，还需后面的信息.（BRNN 可处理这问题） 3.2 RNN Forward Propagation 看 RNN Forward Propagation 之前，先看下基本的标准网络 RNN 在正向传播的过程中可以看到 a 的值随着时间的推移被传播了出去，也就一定程度上保存了单词之间的特性: a&lt;0&gt;=0⃗a^{&lt;0&gt;}=\\vec{0}a&lt;0&gt;=0 a&lt;1&gt;=g_1(W_aaa&lt;0&gt;+W_axx&lt;1&gt;+b_a)a^{&lt;1&gt;}=g\\_1(W\\_{aa}a^{&lt;0&gt;}+W\\_{ax}x^{&lt;1&gt;}+b\\_a)a&lt;1&gt;=g_1(W_aaa&lt;0&gt;+W_axx&lt;1&gt;+b_a) y&lt;1&gt;=g_2(W_yaa&lt;1&gt;+b_y)y^{&lt;1&gt;}=g\\_2(W\\_{ya}a^{&lt;1&gt;}+b\\_y)y&lt;1&gt;=g_2(W_yaa&lt;1&gt;+b_y) a&lt;t&gt;=g_1(W_aaa&lt;t−1&gt;+W_axx&lt;t&gt;+b_a)a^{&lt;{t}&gt;}=g\\_1(W\\_{aa}a^{&lt;{t-1}&gt;}+W\\_{ax}x^{&lt;{t}&gt;}+b\\_a)a&lt;t&gt;=g_1(W_aaa&lt;t−1&gt;+W_axx&lt;t&gt;+b_a) y&lt;t&gt;=g_2(W_yaa&lt;t&gt;+b_y)y^{&lt;{t}&gt;}=g\\_2(W\\_{ya}a^{&lt;{t}&gt;}+b\\_y)y&lt;t&gt;=g_2(W_yaa&lt;t&gt;+b_y) 激活函数：g_1g\\_1g_1 一般为 tanh函数 (或者是 Relu函数)，g_2g\\_2g_2 一般是 Sigmod函数. 注意: 参数的下标是有顺序含义的，如 W_axW\\_{ax}W_ax 下标的第一个参数表示要计算的量的类型，即要计算 aaa 矢量，第二个参数表示要进行乘法运算的数据类型，即需要与 xxx 矢量做运算。如 W_axxt→aW\\_{ax} x^{t}\\rightarrow{a}W_axxt→a Tx ， Ty 是时间单位, 这里统称为“时刻”，在这例子中对应不同时刻是输入的第几个单词， x 是“输入值”，例子中是当前时刻的单词（以独热编码的形式）， y 是“输出值” 0 或者 1， a 称为激活值用于将前一个单元的输出结果传递到下一个单元， Wax Way Waa 是不同的“权重矩阵”也就是我们神经网络 update 的值。每一个单元有两个输入，a&lt;T_x−1&gt;a^{&lt;{T\\_x-1}&gt;}a&lt;T_x−1&gt; 和 x ，有两个输出 a&lt;T_x&gt;a^{&lt;{T\\_x}&gt;}a&lt;T_x&gt; 和 y . 图中没有出现的 g 是“激活函数”。 符号 名字 xxx 输入值 aaa 激活值 T_xT\\_xT_x, T_yT\\_yT_y xxx,yyy 时刻 Wax, Way, Waa 权重矩阵 3.3 Simplified RNN notation 下面将对如下公式进行化简： 1. 简化 a&lt;t&gt;a^{&lt;{t}&gt;}a&lt;t&gt; \\begin{align} a^{&lt;{t}&gt;}&amp;= g(W\\_{aa}a^{&lt;{t-1}&gt;}+W\\_{ax}x^{&lt;{t}&gt;}+b\\_a) \\notag \\\\\\\\ &amp;= g(W\\_a [a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^{T}+b\\_a) \\notag \\end{align} 注意，公式中使用了两个矩阵进行化简，分别是 W_aW\\_aW_a 和 [a&lt;t−1&gt;,x&lt;t&gt;]T[a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^T[a&lt;t−1&gt;,x&lt;t&gt;]T (使用转置符号更易理解),下面分别进行说明： W_a=[W_aa,W_ax]W\\_a = [ W\\_{aa}, W\\_{ax} ]W_a=[W_aa,W_ax], 假设 W_aaW\\_{aa}W_aa 是 (100,100) 的矩阵，W_axW\\_{ax}W_ax 是 (100,10000) 的矩阵,那么 WWW 则是 (100,10100) 的矩阵. [a&lt;t−1&gt;,x&lt;t&gt;]T[a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^T[a&lt;t−1&gt;,x&lt;t&gt;]T 是下图示意: 故 W_a[a&lt;t−1&gt;,x&lt;t&gt;]TW\\_a [a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^{T}W_a[a&lt;t−1&gt;,x&lt;t&gt;]T 矩阵计算如下图示: 2. 简化 y&lt;t&gt;y^{&lt;{t}&gt;}y&lt;t&gt; 该节PPT内容： 再回顾下干净的前向传播概览图: 4. Backpropagation through time RNN 的反向传播通常都由类似 Tensorflow、Torch 之类的库或者框架帮你完成，不过感官上和普通神经网络类似，算梯度值然后更新权重矩阵. 但是下面这里依然会对反向传播进行详细的介绍，跟着下面一张一张的图片走起来 😄😄: 4.1 整体感受 首先再回顾一下 RNN 的整体结构: 要进行反向传播，首先需要前向传播，传播方向如蓝色箭头所示，其次再按照红色箭头进行反向传播 4.2 前向传播 首先给出所有输入数据，即从 x&lt;1&gt;x^{&lt;1&gt;}x&lt;1&gt; 到 x&lt;T_x&gt;x^{&lt;{T\\_x}&gt;}x&lt;T_x&gt;, T_xT\\_xT_x 表示输入数据的数量. 初始化参数 W_aW\\_aW_a, b_ab\\_ab_a，将输入数据输入网络得到对应的 a&lt;t&gt;a^{&lt;{t}&gt;}a&lt;t&gt; 再通过与初始化参数 W_yW\\_yW_y, b_yb\\_yb_y 得到 y&lt;t&gt;y^{&lt;{t}&gt;}y&lt;t&gt; 4.3 损失函数定义 要进行反向传播，必须得有损失函数嘛，所以我们将损失函数定义如下： 每个节点的损失函数: L&lt;t&gt;(y^&lt;t&gt;,y&lt;t&gt;)=y&lt;t&gt;log(y&lt;t&gt;)−(1−y&lt;t&gt;)log(1−y^&lt;t&gt;)L^{&lt;{t}&gt;}(\\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;})=y^{&lt;{t}&gt;}log(y^{&lt;{t}&gt;})-(1-y^{&lt;{t}&gt;})log(1-\\hat{y}^{&lt;{t}&gt;}) L&lt;t&gt;(y^​&lt;t&gt;,y&lt;t&gt;)=y&lt;t&gt;log(y&lt;t&gt;)−(1−y&lt;t&gt;)log(1−y^​&lt;t&gt;) 整个网络的损失函数: L(y^&lt;t&gt;,y&lt;t&gt;))=∑_t=1T_yL&lt;t&gt;(y^&lt;t&gt;,y&lt;t&gt;)L(\\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;)}) = \\sum\\_{t=1}^{T\\_y}L^{&lt;{t}&gt;}(\\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;}) L(y^​&lt;t&gt;,y&lt;t&gt;))=∑_t=1T_yL&lt;t&gt;(y^​&lt;t&gt;,y&lt;t&gt;) 4.4 反向传播 4.5 整个流程图 5. Different types of RNNs RNN 的不同应用领域: 序列模型对输入与输出的长度没有要求，在常见的例子中，机器翻译就是多个输入与多个输出，简称“多对多”， 语音识别可视为“单对多”， 它的反例是音乐生成-“多对单”。课程中介绍了多种可能的 RNN 模式，我们用下面一张图概括： RNN 不同的结构给了我们更多的可能性. 6. Language model and sequence generation 语言模型和序列生成 6.1 什么是语言模型 凡事开头举个🌰，一切都好说： 假设一个语音识别系统听一句话得到了如下两种选择，作为正常人肯定会选择第二种。但是机器才如何做判断呢？ 此时就需要通过语言模型来预测每句话的概率： 6.2 如何使用 RNN 构建语言模型 首先我们需要一个很大的语料库 (Corpus) 将每个单词字符化 (Tokenize，即使用One-shot编码) 得到词典,，假设有 10000 个单词 还需要添加两个特殊的单词 end of sentence. 终止符，表示句子结束. UNknown, 之前的笔记已介绍过 6.3 构建语言模型示例 假设要对这句话进行建模：Cats average 15 hours of sleep a day. 1. 初始化 这一步比较特殊，即 x&lt;1&gt;x^{&lt;1&gt;}x&lt;1&gt; 和 a&lt;0&gt;a^{&lt;0&gt;}a&lt;0&gt; 都需要初始化为 0⃗\\vec{0}0 . 此时 y^&lt;1&gt;\\hat{y}^{&lt;1&gt;}y^​&lt;1&gt; 将会对第一个字可能出现的每一个可能进行概率的判断,即 y^&lt;1&gt;=[p(a),…,p(cats),…]\\hat{y}^{&lt;1&gt;}=[p(a),…,p(cats),…]y^​&lt;1&gt;=[p(a),…,p(cats),…]. 当然在最开始的时候没有任何的依据，可能得到的是完全不相干的字，因为只是根据初始的值和激活函数做出的取样。 2. 将真实值作为输入值: 之所以将真实值作为输入值很好理解，如果我们一直传错误的值，将永远也无法得到字与字之间的关系 如下图示，将 y&lt;1&gt;y^{&lt;1&gt;}y&lt;1&gt; 所表示的真实值 Cats 作为输入，即 x&lt;2&gt;=y&lt;1&gt;x^{&lt;2&gt;}=y^{&lt;1&gt;}x&lt;2&gt;=y&lt;1&gt; 得到 y^&lt;2&gt;\\hat{y}^{&lt;2&gt;}y^​&lt;2&gt; 此时的 y^&lt;2&gt;=[p(a∣cats),…,p(average∣cats),…]\\hat{y}^{&lt;2&gt;}=[p(a|cats),…,p(average|cats),…]y^​&lt;2&gt;=[p(a∣cats),…,p(average∣cats),…] 同理有 y^&lt;3&gt;=[p(a∣cats average),…,p(average∣cats average),…]\\hat{y}^{&lt;3&gt;}=[p(a|cats\\, average),…,p(average|cats\\,average),…]y^​&lt;3&gt;=[p(a∣catsaverage),…,p(average∣catsaverage),…] 另外输入值满足： x&lt;t&gt;=y&lt;t−1&gt;x^{&lt;{t}&gt;}=y^{&lt;{t-1}&gt;}x&lt;t&gt;=y&lt;t−1&gt; 3. 计算出损失值: 下图给出了构建模型的过程以及损失值计算公式: 随着训练的次数的增多，或者常用词出现的频率的增多，语言模型便慢慢的会开始掌握简单的词语比如“平均”，“每天”，“小时”。一个完善的语言模型看到类似“ 10 个小”的时候，应该就能准确的判定下一个字是“时”。 （当然也许实际情况是“ 10 个小朋友”，所以通常会有更多的判断因素，这里只是一个例子） 7. Sampling novel sequences 当训练得到了一个模型之后，如想知道这个模型学到了些什么，一个非正式的方法就是对新序列进行采样。具体方法如下： 在每一步输出 y^\\hat{y}y^​ 时，通常使用 softmax 作为激活函数，然后根据输出的分布，随机选择一个值，也就是对应的一个字 或 英文单词。 然后将这个值作为下一个单元的 xxx 输入进去 (即 x&lt;t&gt;=y^&lt;t−1&gt;x^{&lt;{t}&gt;}=\\hat{y}^{&lt;{t-1}&gt;}x&lt;t&gt;=y^​&lt;t−1&gt;, 直到我们输出了终结符，或者输出长度超过了提前的预设值 n 才停止采样. 上述步骤具体如图示： 下图给出了采样之后得到的效果： 左边是对训练得到新闻信息模型进行采样得到的内容； 右边是莎士比亚模型采样得到的内容. 8. Vanishing gradients with RNNs 现在你已经学会了 基本的 RNN 如何应用在 比如 语言模型 还有 如何用反向传播来训练你的 RNN 模型, 但是还有一个问题就是 梯度消失 与 梯度爆炸 问题. 目前这种基本的 RNN 也不擅长捕获这种长期依赖效应. 梯度爆炸可以用梯度消减解决、梯度消失就有点麻烦了，需要用 GRU 来解决. RNN 的梯度消失、爆炸问题: 梯度值在 RNN 中也可能因为反向传播的层次太多导致过小 或 过大 当梯度值过小的时候，神经网络将无法有效地调整自己的权重矩阵导致训练效果不佳，称之为**“梯度消失问题”(gradient vanishing problem)**； 过大时可能直接影响到程序的运作因为程序已经无法存储那么大的值，直接返回 NaN ，称之为**“梯度爆炸问题”(gradient exploding problem)** 当梯度值过大的时候有一个比较简便的解决方法，每次将返回的梯度值进行检查，如果超出了预定的范围，则手动设置为范围的边界值. 123if (gradient &gt; max) &#123; gradient = max&#125; 但梯度值过小的解决方案要稍微复杂一点，比如下面两句话： “The cat，which already ate apple，yogurt，banana，…, was full.” “The cats，which already ate apple，yogurt，banana，…, were full.” 重点标出的 cat(s) 和 be 动词（was, were） 是有很重要的关联的，但是中间隔了一个 which 引导的定语从句，对于前面所介绍的基础的 RNN网络 很难学习到这个信息，尤其是当出现梯度消失时，而且这种情况很容易发生. 我们知道一旦神经网络层次很多时，反向传播很难影响前面层次的参数。所以为了 解决梯度消失 问题，提出了 GRU单元，下面一节具体介绍. 将在接下来的两个章节介绍两种方法来解决 梯度过小 问题，目标是当一些重要的单词离得很远的时候，比如例子中的 “cat” 和 “was”，能让语言模型准确的输出单数人称过去时的 “was”，而不是 “is” 或者 “were”. 两个方法都将引入“记忆”的概念，也就是为 RNN 赋予一个记忆的功能. 9. GRU - Gated Recurrent Unit GRU（Gated Recurrent Unit）是一种用来解决梯度值过小的方法，首先来看下在一个时刻下的 RNN单元，激活函数为 tanh 9.1 回顾普通 RNN单元 的结构 如图示，输入数据为 a&lt;t−1&gt;a^{&lt;{t-1}&gt;}a&lt;t−1&gt; 和 x&lt;t&gt;x^{&lt;{t}&gt;}x&lt;t&gt;, 与参数 W_aW\\_aW_a 进行线性运算后再使用 tanhtanhtanh 函数 转化得到 a&lt;t&gt;a^{&lt;{t}&gt;}a&lt;t&gt;. 当然再使用 softmax 函数处理可以得到预测值. 9.2 GRU结构 记忆细胞: 在 GRU中 会用到 “记忆细胞(Memory cell)” 这个概念, 我们用变量C表示。这个记忆细胞提供了记忆功能，例如它能够帮助记住 cat 对应 was, cats 对应 were. 而在 ttt 时刻，记忆细胞所包含的值其实就是 Activation function 值，即 c&lt;t&gt;=a&lt;t&gt;c^{&lt;{t}&gt;}=a^{&lt;{t}&gt;}c&lt;t&gt;=a&lt;t&gt; 注意：在这里两个变量的值虽然一样，但是含义不同。 另外在下节将介绍的 LSTM 中，二者值的大小有可能是不一样的，所以有必要使用这两种变量进行区分 为了更新记忆细胞的值，我们引入 c~\\tilde{c}c~ 来作为候选值从而来更新 c&lt;t&gt;c^{&lt;{t}&gt;}c&lt;t&gt;，其公式为： c~=tanh(W_c[c&lt;t−1&gt;,x&lt;t&gt;]+b_c)\\tilde{c}=tanh(W\\_c [c^{&lt;{t-1}&gt;}, x^{&lt;{t}&gt;}]+b\\_c) c~=tanh(W_c[c&lt;t−1&gt;,x&lt;t&gt;]+b_c) 更新门 (update gate): 更新门是 GRU 的核心概念，它的作用是用于判断是否需要进行更新. 更新门用 Γ_u\\Gamma\\_uΓ_u 表示，其公式为： Γ_u=σ(W_u[c&lt;t−1&gt;,x&lt;t&gt;]+b_u)\\Gamma\\_u=σ(W\\_u [c^{&lt;{t-1}&gt;}, x^{&lt;{t}&gt;}]+b\\_u) Γ_u=σ(W_u[c&lt;t−1&gt;,x&lt;t&gt;]+b_u) 如上图示，Γ_u\\Gamma\\_uΓ_u 值的大小大多分布在 0 或者 1，所以可以将其值的大小粗略的视为 0 或者 1。这就是为什么我们就可以将其理解为一扇门，如果 Γ_u=1\\Gamma\\_u=1Γ_u=1 , 就表示此时需要更新值，反之不用. ttt 时刻记忆细胞: 有了更新门公式后，我们则可以给出 ttt 时刻 Memory cell 的值的计算公式了: c^{&lt;{t}&gt;} = \\Gamma\\_u \\* \\tilde{c} + (1-\\Gamma\\_u) \\* c^{&lt;{t-1}&gt;} 注意：上面公式中的 * 表示元素之间进行乘法运算，而其他公式是 矩阵运算 公式很好理解，如果 Γ_u=1\\Gamma\\_u=1Γ_u=1，那么 ttt 时刻 记忆细胞的值就等于候选值 c~\\tilde{c}c~, 反之等于前一时刻记忆细胞的值. 下图给出了该公式很直观的解释： 在读到 “cat” 的时候 ，其他时候一直为 0，知道要 输出 “was” 的时刻我们仍然知道 “cat” 的存在，也就知道它为单数了 GRU 结构示意图 9.3 完整版 GRU 上简化了的 GRU，在完整版中还存在另一个符号 ，这符号的意义是控制 c~\\tilde{c}c~ 和 c&lt;t−1&gt;c^{&lt;{t-1}&gt;}c&lt;t−1&gt; 之间的联系强弱，完整版公式如下： 注意，完整公式中多出了一个 Γ_r\\Gamma\\_rΓ_r, 这个符号的作用是控制 c~&lt;t&gt;\\tilde{c}^{&lt;{t}&gt;}c~&lt;t&gt; 和 c&lt;t&gt;c^{&lt;{t}&gt;}c&lt;t&gt; 之间联系的强弱. 10. LSTM（long short term memory）unit 介绍完 GRU 后，再介绍 LSTM 会更加容易理解。下图是二者公式对比： GRU 只有两个门，而 LSTM 有三个门，分别是更新门 Γ_u\\Gamma\\_uΓ_u (是否需要更新为 c~&lt;t&gt;\\tilde{c}^{&lt;{t}&gt;}c~&lt;t&gt;，遗忘门 Γ_f\\Gamma\\_fΓ_f (是否需要丢弃上一个时刻的值)，输出门 Γ_o\\Gamma\\_oΓ_o (是否需要输出本时刻的值) 虽然 LSTM 比 GRU 更复杂，但是它比 GRU 更早提出哇😄。另外一般而言 LSTM 的表现要更好，但是计算量更大，毕竟多了一个门嘛。而 GRU 实际上是对 LSTM 的简化，它的表现也不错，能够更好地扩展到深层网络。所以二者各有优势。 下图是 LSTM 的结构示意图： 11. Bidirectional RNN 前面介绍的都是单向的 RNN 结构，在处理某些问题上得到的效果不尽人意 如下面两句话，我们要从中标出人名： He said, “Teddy Roosevelt was a great President”. He said, “Teddy bears are on sale”. 第一句中的 Teddy Roosevelt 是人名 第二句中的 Teddy bears 是泰迪熊，同样都是单词 Teddy 对应的输出在第一句中应该是 1，第二句中应该是 0 像这样的例子如果想让我们的序列模型明白就需要借助不同的结构比如 - 双向递归神经网络(Bidirectional RNN). 该神经网络首先从正面理解一遍这句话，再从反方向理解一遍. 双向递归神经网络结构如下： 下图摘自大数据文摘整理 12. Deep RNNs 深层，顾名思义就是层次增加。如下图是深层循环神经网络的示意图 横向表示时间展开，纵向则是层次展开。 注意激活值的表达形式有所改变，以 a^{\\[1\\]&lt;0&gt;} 为例进行解释： 1 表示第一层 &lt;0&gt; 表示第一个激活值 另外各个激活值的计算公式也略有不同，以 a^{\\[2\\]&lt;3&gt;} 为例，其计算公式如下： 13. Reference 网易云课堂 - deeplearning DeepLearning.ai 学习笔记汇总 大数据文摘 DeepLearning.ai 学习笔记 Sequence Models 英文版笔记","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Structured Machine Learning Projects (week2) - ML Strategy 2","slug":"deeplearning/Structured-Machine-Learning-Projects-week2","date":"2018-07-25T11:00:21.000Z","updated":"2021-06-20T04:12:28.259Z","comments":true,"path":"2018/07/25/deeplearning/Structured-Machine-Learning-Projects-week2/","link":"","permalink":"http://www.iequa.com/2018/07/25/deeplearning/Structured-Machine-Learning-Projects-week2/","excerpt":"如何进行 误差分析、标注错误数据、定位数据不匹配偏差与方差 知道如何应用端到端学习、迁移学习以及多任务学习","text":"如何进行 误差分析、标注错误数据、定位数据不匹配偏差与方差 知道如何应用端到端学习、迁移学习以及多任务学习 1. Carrying out error analysis 很多时候我们发现训练出来的模型有误差后，就会一股脑的想着法子去减少误差。想法固然好，但是有点 headlong 。。 这节视频中 Andrew Ng 介绍了一个比较科学的方法，具体的看下面的例子 还是以猫分类器为例，假设我们的模型表现的还不错，但是依旧存在误差，预测后错误标记的数据中有一部分狗图片被错误的标记成了猫。这个时候按照一般的思路可能是想通过训练出狗分类器模型来提高猫分类器，或者其他的办法，反正就是要让分类器更好地区分狗和猫。 但是现在的问题是，假如错误分类的100个样本中，只有5个狗样本被错误的标记成了猫，那么你费尽千辛万苦也最多只能提高一丢丢的准确度。所以对误差进行分析就显得比较重要，而且可以帮助我们在未来的工作中指明优化方向，节省时间。具体的方法按吴大大的说法是可以人工的对错误标记的样本进行再处理、分析。 下面以一个例子来介绍一下操作步骤 1. 人工标记 将错误标记样本以表格的形式列举出来，然后人工的标记处样本的分类，统计各分类(或者说错误标记的原因)所占比例. Image Dog Great cats(大型猫科动物，如狮子) Blurry(图片模糊) Comments 1 √ 2 √ 眯着眼 3 √ √ 在动物园，且下着雨 … % of total 8% 43% 61% - 注意:上面的分类并不是互相独立的，只是举个例子。。。抄下 Andrew Ng 的PPT 2. 分析误差 上面的结果可以知道，误差样本中只有8%是狗狗的图片，而43%是大型猫科动物，61%是因为图片模糊。很显然此时你即使用毕生所学去优化区别狗和猫的算法，整个模型的准确率提升的空间也远不如后两个特征高。所以如果人手够的话，也是可以选择几个特征进行优化的。 2. Cleaning up Incorrectly labeled data 机器预测可能会出错，那么人当然也有可能会出错。所以如果训练集和验证集中认为添加的标签Y出现误差该怎么处理呢？ 这里分两种情况： 1.随机误差 这种情况比较好，因为如果人为误差比较接近随机误差，那么可以睁一只眼闭一只眼，因为深度学习算法对于随机误差还是有一定的健壮性的 2.非随机误差 PS:不知道有没有非随机误差这个词。。我只是为了行文方便取的一个名字。 对于随机误差正常人可能都会问“what？我怎么知道是不是接近随机误差”，所以视频里 Andrew Ng 也给咱们提供了一个方法，这个方法和上一节中的表格法一样一样的： Image Dog Great cats(大型猫科动物，如狮子) Blurry(图片模糊) Incorrectly labeled Comments 1 √ 2 √ √ 只是一只手画的的猫，不是真的猫 3 √ 背景的角落里有一只猫 … % of total 8% 43% 61% 6% 有了上面这个表格，那么问题来了，此时我还需要修正这6%标记错误的样本吗？还是举个例子： 假设我们有如下数据： 总体验证集误差：10% 由人工错误标记引起的错误样本比例： 0.6% 由其他原因引起的错误样本比例：10%-0.6%=9.4% 所以这种情况下我们应该集中精力找出引起9.4%误差的原因，并进行修正，当然如果有余力也还是可以休整一下人工错误标记的数据的。 假如你通过优化算法，减少了因其他原因引起的误差，并且使得总体验证集误差降到了2%，此时我们再分析一下： 很显然，因为并没有对人工误差进行优化，所以由人工错误标记引起的错误样本比例依旧是0.6%(这个数据可能有点不能理解，要注意这个0.6%是相对于整体验证集而言的，所以不会变), 那么人工误差在总误差中所占的比例则达到了0.6%/2%=30%,相比于之前的6%影响力打了不小哦，所以此时则应该考虑对人工误差动手了. 3. Build your first system quickly, then iterate 还是有一个步骤流程的： 建立训练集，验证集，测试集 迅速搭建初始化系统 使用前面提到的Bias/Variance分析和误差分析来确定接下来的优化方向 4. Training and testing on different distributions 5. Bias and Variance with mismatched data distributions 对上面的PPT截图进行解释： 左边: 首先还是以喵咪分类器作为例子，假设人类的误差接近贝叶斯误差0%。而训练集误差和开发集误差分别为1%和10%，二者相差9%，而且如果两个数据集来自同一个分布，那么我们就可以说模型训练结果方差较大。 但是当两个数据集来自不同的分布时，我们就不能得出上面的结论了。另外，这9%的方差可能有两个原因导致的. 是我们自己实现的代码有问题 是数据分布不同，所以你很难确定哪个是更主要的原因. 因此为了找出是哪个原因我们做如下的事情： 创建Training-dev set(训练-开发集)，其实就是从原来的训练集中抽取一部分数据出来，但是不喂给模型。（如上图所示） 右边: 那怎么操作呢？很简单，下面以几个例子来说明： 1.因为Training-dev set(训练-开发集)和Training set同分布，所以假设训练出来的结果如下： training error: 1% training-dev error: 9% dev error: 10% 此时可以看到来自同分布数据的训练误差和训练-开发误差存在较大的方差，所以我们就可以确定肯定是我们滴代码还需要完善. 2.假设训练出来的结果如下： training error: 1% training-dev error: 1.5% dev error: 10% 此时就可以说不是我程序员的问题了，而是发生了data mismatch(数据不匹配问题) 上图右下角： 1.假设人类的误差接近贝叶斯误差0，且训练误差如下： training error: 10% training-dev error: 11% dev error: 12% 此时我们会认为模型与人类误差相比存在较大的偏差。所以就朝着 减小偏差的方向努力 吧少年. 2.同样假设人类的误差接近贝叶斯误差0，且训练误差如下： training error: 10% training-dev error: 11% dev error: 20% 此时我们会认为存在两个问题： 高偏差 数据不匹配问题 祝福你，继续修改代码吧… 6. Addressing data mismatch 虽然我们使用数据合成已经在语音识别方面取得了不错的效果提升 可以使用数据合成，但是要注意你的神经网络可能过拟合，过分学习了你这小部分数据集了。 7. Transfer learning 简单的解释就是假如我们之前训练好了一个喵咪分类器，后来我们有了新任务 — 做一个海豚分类器，那么就可以将之前创建的喵咪分类器模型运用到新任务中去 举个栗子，假设我们对信号灯的红、绿灯进行了大量数据的学习，现在有了新任务，即需要识别黄灯,此时我们就不需要从头搭建模型，我们可以继续使用红绿灯网络框架，只需修改神经网络最后一层，即输出层，然后用已经训练好的权重参数初始化这个模型，对黄灯数据进行训练学习。 为什么可以这么做呢？因为尽管最后的标签不一致，但是之前学习的红绿灯模型已经捕捉和学习了很多有用的特征和细节，这对于黄灯的学习十分有帮助，而且这么做也可以大大的加快模型的构建速度。 想将A模型运用到B模型, 一般来说是有条件限制的，如下： A 和 B 需要有相类似的输入数据集，例如要么都是图像识别，要么是语音识别 A 的数据集要足够多，即远多于B A 中学到一些 low level features 要对 B 有所帮助 8. Multi-task learning 在 Transfer learning 中，整个过程是串行的，即咱们首先得实现A模型，然后在运用到B模型。 在 Multi-task learning 中，可以是同时开始学习. 举个🌰： 现在很火的无人驾驶汽车，在行驶路上需要识别很多类型的物体，如行人、红绿灯、指路标志等等，所以此时可以使用 Multi-task learning 来实现。神经网络示意图如下： 如图示，最后的 y^\\hat{y}y^​ 是一个有4元素的向量，假设分别是行人、汽车、停车标志、信号灯。如果识别出图片中有哪一个元素，对应位置则输出1。 注意：这要与softmax进行区分，softmax 只是一次识别一种物体，比如说识别出是行人，则输出[1,0,0,0],而不会说同时识别出行人和信号灯. 适用情况： 最后，Andrew Ng 说在实际中迁移学习使用频率要远高于多任务学习，有个例外就是视觉检测项目中多任务学习比较多. 9. End-to-end deep learning 首先以现在广泛使用的人脸识别技术解释一下什么是端到端的深度学习. What is end-to-end learning? 假如咱们走进一个摄像头，最开始离得较远的时候摄像头捕捉到的是我们的全身，此时系统不会将这种照片喂给模型，而是通过算法找到人脸的位置，然后切割放大，最后喂给模型进行识别. 总结起来就是： 找人脸位置 将人脸图像切割放大，并喂给模型 Notes: 端到端的深度学习其实就不是像将问题细分化，流水线化，每个步骤各司其职，下一层依赖上一层 And you need a large data set before the end-to-end approach really shines. （你需要大量的数据，端到端的深度学习，才能发挥耀眼的光芒.） 10. Reference 网易云课堂 - deeplearning DeepLearning.ai学习笔记汇总","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Structured Machine Learning Projects (week1) - ML Strategy 1","slug":"deeplearning/Structured-Machine-Learning-Projects-week1","date":"2018-07-24T11:00:21.000Z","updated":"2021-06-20T04:12:28.255Z","comments":true,"path":"2018/07/24/deeplearning/Structured-Machine-Learning-Projects-week1/","link":"","permalink":"http://www.iequa.com/2018/07/24/deeplearning/Structured-Machine-Learning-Projects-week1/","excerpt":"这次我们要学习专项课程中第三门课 Structured Machine Learning Projects 学完这门课之后，你将会: 理解如何诊断机器学习系统中的错误 能够优先减小误差最有效的方向 理解复杂ML设定，例如训练/测试集不匹配，比较并/或超过人的表现 知道如何应用端到端学习、迁移学习以及多任务学习 很多团队浪费数月甚至数年来理解这门课所教授的准则，也就是说，这门两周的课可以为你节约数月的时间","text":"这次我们要学习专项课程中第三门课 Structured Machine Learning Projects 学完这门课之后，你将会: 理解如何诊断机器学习系统中的错误 能够优先减小误差最有效的方向 理解复杂ML设定，例如训练/测试集不匹配，比较并/或超过人的表现 知道如何应用端到端学习、迁移学习以及多任务学习 很多团队浪费数月甚至数年来理解这门课所教授的准则，也就是说，这门两周的课可以为你节约数月的时间 1. Why ML Strategy? 如上图示，假如我们在构建一个喵咪分类器，数据集就是上面几个图，训练之后准确率达到90%。虽然看起来挺高的，但是这显然并不具一般性，因为数据集太少了。那么此时可以想到的ML策略有哪些呢？总结如上图中 Ideas. 2. Orthogonalization Orthogonalization [ɔ:θɒɡənəlaɪ’zeɪʃn] 正交化 And when I train a neural network，I tend not to use early shopping. 因为 Early Stropping，这个按钮能同时影响两件事情. 就像一个按钮同时影响电视机的宽度和高度. 如果你有更多的正交化(Orthogonalization)的手段，用这些手段调网络会简单不少. When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal. No. strategy solutions 1. Fit training set well in cost function If it doesn’t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help. 2. Fit development set well on cost function If it doesn’t fit well, regularization or using bigger training set might help. 3. Fit test set well on cost function If it doesn’t fit well, the use of a bigger development set might help 4. Performs well in real world If it doesn’t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing 3. Single number evaluation metric 大致的思想就是首先按照单一数字评估指标对模型进行评价和优化。以精确率和召回率为例，这二者一般来说是一个不可兼得的指标，所以为了更好的衡量模型的好坏，引入F1算法来综合精确率和召回率对模型进行评估. 因此在实际操作过程中，我们可以以人类准确率为指标来评判我们训练的模型好坏程度 9. Avoidable bias Humans error 与 Training Error 之间的差距我们成为 Avoidable bias Training Error 与 Dev Error 之间的差距我们成为 Variance 10. Understanding human-level performance 解释说明 Example 1: 假如一个医院需要对一个医学影像进行分类识别，普通人，普通医生，有经验的医生和一群有经验的医生识别错误率分别为3%，1%，0.7%，0.5%。上一节中提到过Human Error，那此时的该如何确定Human Error呢？你可能会说取平均值，只能说Too Naive！当然是取最好的结果啦，也就是由一群经验丰富的医生组成的团体得到的结果作为Human Error。另外贝叶斯误差一定小于0.5%。 解释说明 Example 2: 还是以医学影像分类识别为例，假如现在分成了三种情况： Scenario A 让三类人群来划分后得到的误差分别为1%，0.7%，0.5%，而训练集和测试集误差分别为5%，6%。很显然此时的Avoidable Bias=4%~4.5%，Variance=1%，bias明显大于variance，所以此时应该将重心放到减小bias上去。 Scenario Bayse 同理此情况下的Avoidable Bias=0%~0.5%，Variance=4%，所以需要减小variance。 Scenario C Avoidable Bias=0.2%，Variance=0.1%，二者相差无几，但是此时训练的模型准确率还是不及人类，所以没办法咱们还得继续优化，都说枪打出头鸟，所以继续优化bias~ 11. Surpassing human-level performance Scenario A Avoidable Bias=0.1%，Variance=0.2%，所以此时应该将重心放到减小Variance上去 Scenario B Avoidable Bias=-0.2%，Variance=0.1%.乍一看可能会有点不知所措，而且训练集准确度也超过了人的最好成绩，不知道应该选择优化哪一项了，或者说这是不是就说明可以不用再优化了呢？ （还是可以继续优化的。不可否认在图像识别方面人类的确其优于机器的方面，但是在其他方面，如在线广告推送，贷款申请评测等方面机器人要远远比人类优秀，所以如果是在上面课件中提到的一些领域，即使机器准确度超过了人类，也还有很大的优化空间。具体怎么优化。。。以后再探索。。。） 12. Improving your model performance 13. Reference 网易云课堂 - deeplearning DeepLearning.ai学习笔记汇总 DeepLearning.ai学习笔记（三）结构化机器学习项目–week1 机器学习策略","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Improving DNN (week3) - Hyperparameter、Batch Regularization","slug":"deeplearning/Improving-Deep-Neural-Networks-week3","date":"2018-07-23T12:00:21.000Z","updated":"2021-06-20T04:12:28.258Z","comments":true,"path":"2018/07/23/deeplearning/Improving-Deep-Neural-Networks-week3/","link":"","permalink":"http://www.iequa.com/2018/07/23/deeplearning/Improving-Deep-Neural-Networks-week3/","excerpt":"Hyperparameter Tuning process、Normalizing Activations in a network Fitting Batch Norm into a neural network、Why does Batch Norm work?、Batch Norm at test time Softmax regression、TensorFlow","text":"Hyperparameter Tuning process、Normalizing Activations in a network Fitting Batch Norm into a neural network、Why does Batch Norm work?、Batch Norm at test time Softmax regression、TensorFlow 1. Hyperparameter Tuning process 正常情况有如下超参数: Hyperparameter Desc Importance level α 最重要 1 hidden units 2 mini-batch size 2 β 2 layers 3 learning rate decay 3 β_1,β_2,εβ\\_1,β\\_2,εβ_1,β_2,ε 最不重要 4 颜色表示重要性，以及调试过程中可能会需要修改的程度. 那么如何选择超参数的值呢？: 首先是粗略地随机地寻找最优参数 建议使用图右的方式，原因如下： 对于图左的超参数分布而言，可能会使得参考性降低，我们假设超参1是学习率α，超参2是ε，根据week2中Adam算法的介绍，我们知道ε的作用几乎可以忽略，所以对于图左25中参数分布来说，其本质只有5种参数分布。而右边则是25种随机分布，更能帮助我们选择合适的超参数. 其次在上面找到的最优参数分布周围再随机地寻找最有参数 2. Using an appropriate scale to pick hyperparameters 上一节提到的的随机采样虽然能帮助我们寻找最优参数分布，但是这有点像大海捞针，如果能够指出参数取值的范围，然后再去寻找最优的参数分布岂不是更加的美滋滋？那如何为超参数选择合适的范围呢？ n[l]=50,……,100n^{[l]}=50,……,100n[l]=50,……,100 layers=2 4layers=2~4layers=2 4 α=0.0001，……,1α=0.0001，……,1α=0.0001，……,1 此时注意: 如按照线性划分的话(如下图)，那么随机采样的值 90% 的数据来自 [0.1,1] 这个区间, 这显然与不太符合随机性. 所以为了改进这一问题，我们需要将区间对数化来采样. 举个🌰： 我们将 [0.0001,1] 转化成四个区间 [0.0001,0.001], [0.001,0.01], [0.01,0.1], [0.1,1], 再转化成对数就是 [-4,-3], [-3,-2], [-2,-1], [-1,0]. (10−4=0.000110^{−4}=0.000110−4=0.0001，其他同理取指数). 然后我们可以用 Python 中提供的方法来实现随机采样： 12r = -4*np.random.rand() # rand()表示在[0,1]上均匀采样, 最后的采样区间是[-4, 0]a = pow(10, r) β=0.9,……,0.999β=0.9,……,0.999β=0.9,……,0.999 同理这里也不能使用线性轴来采样数据，我们可以通过对 1-β=0.1,……,0.001 来间接采样。转化成 [0.1, 0.01], [0.01,0.001], 转化成对数指数 [-1,-2],[-2,-3]。 即: r∈[−3,−1],1−β=10r,β=1−10rr∈[-3,-1], 1-β=10^r, β=1-10^rr∈[−3,−1],1−β=10r,β=1−10r 当 β 接近 1 时, β 就会对细微的变化变得很敏感. for example : 0.999, 0.9995 =&gt; 1000 -&gt; 2000 所以你需要更加密集的取值，在 β 接近 1 的时候. 3. Hyperparameters tuning in practice: Pandas vs Caviar Babysitting one model: 这种方法适用于有足够的数据集，但是 GPU，CPU 资源有限的情况，所以可能只能训练一个模型，然后每天对模型做某一项超参数的修改，查看效果是否变得更好. 例如第一天令所有超参数随机初始化。到了第二天发现效果还不错，此时可以去增加学习率(也可以修改其他参数)。……，到了某一天加入修改了mini-batch size，结果效果明显减弱，这时则需要重新恢复到前一天的状态。 总的来说这一过程就像熊猫一样，只照顾一个宝宝，多的照顾不过来. Train many models in parallel: 这种方法适用于财大气粗的情况，即并行训练多个模型，最后选出效果最好的一个即可。这就像鱼子酱一样，一下生多大一亿的孩子. 4. Normalizing Activations in a network 不仅要归一化输入数据 XXX,隐藏层的数据也是要归一化的. 一般来说隐藏层数据有 ZZZ 和 aaa 两种，Andrew Ng 推荐归一化 zzz. Batch 归一化 由 Sergey loffe 和 Christian Szegedy 两位研究者创造. Batch 归一化，会使你的参数搜索变得容易, 使神经网络对超参数的搜索更加稳定. 这样也会使得你容易训练深层神经网络。 输入数据 XXX 归一化方法: μ=1m∑_ix(i)μ=\\frac{1}{m}\\sum\\_{i}{x^{(i)}} μ=m1​∑_ix(i) σ2=1m∑_ix(i)2σ^2=\\frac{1}{m}\\sum\\_{i}x^{(i)^2} σ2=m1​∑_ix(i)2 x=x−μσ2x=\\frac{x-μ}{σ^2} x=σ2x−μ​ m 为 mini-batch 中的 m， 而不是整个训练集 隐藏层数据归一化方法: μ=1m∑_iz(i)−μμ=\\frac{1}{m}\\sum\\_{i}{z^{(i)}-μ} μ=m1​∑_iz(i)−μ σ2=1m∑_i(z(i)2−μ)2σ^2=\\frac{1}{m}\\sum\\_{i}(z^{(i)^2}-μ)^2 σ2=m1​∑_i(z(i)2−μ)2 z(i)_norm=z(i)−μσ2+εz^{(i)}\\_{norm}=\\frac{z^{(i)}-μ}{\\sqrt{σ^2+ε}} z(i)_norm=σ2+ε​z(i)−μ​ 上面的归一化后的数据 zzz 都是服从均值为 0，方差为 1 的，显然这样不能满足咱们的需求，所以还需要做进一步处理，如下： z~(i)=γz(i)+β\\tilde{z}^{(i)}=γz^{(i)} + β z~(i)=γz(i)+β 上式中的 γγγ 可以设置方差，βββ 可以设置均值. 你也许不想隐层单元值必须是平均值0 和 方差1, 比如你有一个 sigmoid 函数，你不想让它的值完全集中在这里, 你不想使他们平均值和方差一直是0和1， 这样可以更好的利用非线性的 Sigmoid 函数， 而不是所有值都集中在线性的区域, γγγ 和 βββ 可以确保所有的 Z(i)Z^{(i)}Z(i) 值，可以是你想赋予的任意值. 或者 它的作用是保证隐藏的单元已使均值和方差标准化. 那里均值和方差由两参数控制. γγγ 和 βββ 学习算法可以设置为任何值. 所以它的真正作用是使均值和方差标准化. Z(i)Z^{(i)}Z(i) 有固定的均值和方差，均值和方差可以是0和1，也可以是其他值，由 γγγ 和 βββ 确定. In practice， normlizing Z^{\\[2\\]} is done much more often. 5. Fitting Batch Norm into a neural network adding batch Norm to a network working with mini-batches 一般的方法中 z[l]=w[l]a[l−1]+b[l]z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]} z[l]=w[l]a[l−1]+b[l] 在上面归一化数据过程中需要减去均值，所以 b[l]b^{[l]}b[l] 这一项可以省略掉,所以归一化后是 z_norm[l]=w[l]a[l−1]z\\_{norm}^{[l]}=w^{[l]}a^{[l-1]} z_norm[l]=w[l]a[l−1] 为了能够使数据分布更加满足我们的要求，可以用如下公式 z~[l]=γ[l]z_norm[l]+β[l]\\tilde{z}^{[l]}=γ^{[l]}z\\_{norm}^{[l]}+β^{[l]} z~[l]=γ[l]z_norm[l]+β[l] Implementing gradient descent for t= 1,……,numMinBatches 计算基于第 ttt 批数据的前向传播 在计算反向传播时使用 z~[l]\\tilde{z}^{[l]}z~[l], 得到 dw[l],dβ[l],dγ[l]dw^{[l]},dβ^{[l]},dγ^{[l]}dw[l],dβ[l],dγ[l] 更新参数 w[l]=w[l]−αdw[l]β[l]=β[l]−αdβ[l]γ[l]=γ[l]−αdγ[l]w^{[l]}=w^{[l]}-αdw^{[l]} \\\\\\\\ β^{[l]}=β^{[l]}-αdβ^{[l]} \\\\\\\\ γ^{[l]}=γ^{[l]}-αdγ^{[l]} w[l]=w[l]−αdw[l]β[l]=β[l]−αdβ[l]γ[l]=γ[l]−αdγ[l] 6. Why does Batch Norm work? 原因一: batch norm 可以使得权重比你的网络更滞后或更深层，为了更好地理解可以看下面的例子: 如上图所示，假设我们现在要计算第三层隐藏层的值，很显然该层的计算结果依赖第二层的数据，但是第二层的数据如果未归一化之前是不可知的，分布是随机的。而如果进行归一化后，即 \\tilde{z}^{\\[2\\]}=γ^{\\[2\\]}z\\_{norm}^{\\[2\\]}+β^{\\[2\\]} 可以将第二层数据限制为均值为 β^{\\[2\\]}, 方差为 γ^{\\[2\\]} 的分布,注意这两个参数并不需要人为设置，它会自动学习的。所以即使输入数据千变万化，但是经过归一化后分布都是可以满足我们的需求的，更简单地说就是归一化数据可以减弱前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习。 原因二: batch norm 奏效的另一个原因则是它具有正则化的效果。其与dropout有异曲同工之妙，我们知道dropout会随机的丢掉一些节点，即数据，这样使得模型训练不会过分依赖某一个节点或某一层数据。batch norm也是如此，通过归一化使得各层之间的依赖性降低，并且会给每层都加入一些噪声，从而达到正则化的目的 Batch 它限制了在前层的参数更新，会影响数值分布的程度，第三层看到的这种情况，因此得学习. batch 归一化减少了输入值改变的问题, 它的确是这些值变得更稳定. 神经网络的之后层就会有更坚实的基础. 即使输入分布改变了一些，它会改变得更少，它做的是 当前层保持学习，当层改变时，迫使后层, 适应的程度减少了，你可以这样想，它减弱了前层参数的作用，与后层参数的作用之间的联系，它使得网络每层都可以自己学习. 稍稍独立于其它层，这有助于加速整个网络的学习. batch norm 中有一个作用，可以起到轻微 正则化 的作用. (因为添加的噪音很微小，所以并不是巨大的正则化)， 你可以将 batch norm 和 dropout 一起使用. dropout, 你应用较大的 mini-batch 比如 512，那么可以减少噪音也, 因此减少了正则化的效果. 这是 dropout 的一个奇怪的性质. batch norm 是一个正则化的规则，而不要把它当做目的. 但是有时候，它会对你的算法有额外的期望和非期望效果. batch norm 一次只能处理 一个 mini-batch 的数据. 它在 mini-batch 上计算期望与方差. 7. Batch Norm at test time 前面提到的 batch norm 都是基于训练集的，但是在测试集上，有时候可能我们的测试数据很少，例如只有1个，在这个时候进行归一化则显得没多大意义了。那么该怎么办呢？均值μμμ 和 方差σ2σ^2σ2该如何确定呢？ 方法还是有的，而且已经在上面提到过了, 就是第三节所介绍的指数加权平均啦，原理是类似的 假设一共有如下 x{1},x{2},……,x{5000}x^{\\{1\\}},x^{\\{2\\}},……,x^{\\{5000\\}}x{1},x{2},……,x{5000} 的批量数据，每组mini-batch 都得到了对应的均值μμμ, (方差同理，不详细说明了)，即 μ{1},μ{2},……,μ{5000}μ^{\\{1\\}},μ^{\\{2\\}},……,μ^{\\{5000\\}}μ{1},μ{2},……,μ{5000}, 如果测试集数据很少，那么就可以使用指数加权平均的方法来得到测试集的均值和方差。 之后就根据指数加权平均计算得到的值来计算归一化后的输入值即可. Andrew Ng 语录: 如果将你的神经网络用于测试，你需要单独估算 μμμ 和 σ2σ^2σ2, 在典型的 Batch 归一化运用中，你需要用一个指数加权平均来估算，整个平均数覆盖了所有的 mini-batch . z(i)_norm=z(i)−μσ2+εz^{(i)}\\_{norm}=\\frac{z^{(i)}-μ}{\\sqrt{σ^2+ε}} z(i)_norm=σ2+ε​z(i)−μ​ 上个式子 z(i)_normz^{(i)}\\_{norm}z(i)_norm 中的，μμμ, σ2σ^2σ2 是类似加权平均出来的值. 注意：测试集的均值和方差生成的方式不一定非得是上面提到的指数加权平均，也可以是简单粗暴的计算所有训练集的均值和方差，视频中 Andrew Ng 说这也是可行的. 8. Softmax regression 假设第 lll 层有 z^{\\[l\\]}=w^{\\[l\\]}a^{\\[l-1\\]}+b^{\\[l\\]}, 激活函数为 a^{\\[l\\]}=\\frac{e^{z^{\\[l\\]}}}{\\sum\\_{j=1}^{n\\_l}e^{z^{\\[l\\]}\\_j}} 该节视频中 Andrew Ng 并没有很详细的介绍 softmax 的原理和公式推导，感兴趣的可以戳如下链接进行进一步了解： ufldl: Softmax 回归 softmax 公式推导&amp;算法实现 9. Trying a softmax classifier 转载: 具体实践项目可参见softmax分类算法原理(用python实现) 上面的转载实现 softmax 需要再仔细研究. 10. Deep learning frameworks 11. TensorFlow Example Andrew Ng 演示了 TensorFlow 使用方法. 我推荐一个比较好的 TensorFlow 的练手项目：TensorFlow Example 12. Reference 网易云课堂 - deeplearning deeplearning.ai 专项课程二第一周 Coursera - Deep Learning Specialization DeepLearning.ai学习笔记汇总 TensorFlow-Examples 吴恩达老师的深度学习课程笔记及资源","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Taiwan Travel plan","slug":"world/Taiwan","date":"2018-07-22T14:46:48.000Z","updated":"2021-06-22T06:08:56.308Z","comments":true,"path":"2018/07/22/world/Taiwan/","link":"","permalink":"http://www.iequa.com/2018/07/22/world/Taiwan/","excerpt":"台灣旅行","text":"台灣旅行 去东台湾看海：台北、宜兰、花莲、台东、垦丁… 台湾特色 台灣垃圾分类超认真 台灣腔MM超有人情味 台灣电影评级 - (普通級、保護級、輔導級、限制級) 台灣奶茶世界知名 - (Coco、 一點點、 貢茶 等都是) 台灣服務業做的超好 - (公車司機、警察、問路等等) 台灣流行音樂產業 - (周杰倫、五月天、林俊傑…) 台灣的大海 (台东 &amp; 垦丁 一带蓝到爆炸) 台灣的機車 (需要考执照) 台灣的 Youtuber 蠻多 旅游APP 現在玩台北 旅行台灣 台北等公車 食在方便 Imoney 雙鐵時刻表 旅行攻略 痞客邦 &amp; FB 可查攻略 攜程、馬蜂窩、天貓… 证件准备 Passport China Identity Card 台湾通行证 + G签注 + 入台证 台湾悠游卡黑熊限定 (711可买、711是萬能的) 台湾电话卡4G无限高速流量卡 台币 (现金+中行信用卡+招商信用卡) 台北故宫 + 101 联票 台铁票需要提前买 导游机可租 交通介绍 台湾 的交通还是很方便的，但是跟大陆的叫法不一样 地铁叫捷运 火车叫台铁 高铁叫高铁 在 台北 ，捷运、高铁、地铁、机场捷运都是可以在 台北 车站搭乘的，如果要从 台北 去别的地方， 比如 花莲 、 高雄 等地方，需要坐火车或者高铁的，住宿选择在 台北 车站附近会比较方便，因为早上不用早起赶火车. 如何到高铁台北站（运营时间：6：00-24：00） 台北车站，就是高铁台北站，从捷运台北站M4号出口，出来就是台北高铁站。台北车站是，台铁，高铁，捷运共构的. 台湾图览 东台湾看海 跟西部不同，台湾的东部海岸线更为壮阔 沿海公路也是给力得很~ 地震、台风时常光顾，造就了东部海岸和山脉的种种壮美景色 台湾的东部海岸从北至南，主要值得去的地方有宜兰、花莲、瑞穗、池上、台东。其中宜兰、花莲、台东算是比较大一点的城市，也是风景较多的地方。 行程总概 (台北、宜兰、花莲、台北) 6952301022230 Day Date Dest Type Desc Day1 2018-10-04 杭州 － 台北 总统府/自由广场/中正紀念堂/国父纪念馆 西门町夜市 (阿宗麵線、大拇哥、芝士手抓饼…) 宿台北 Day2 2018-10-05 台北 一 宜蘭 烏石港→兰阳博物馆→头城老街→阿宗芋冰城几米公园→丟丟噹森林→幸福轉運站 宿宜蘭 Day3 2018-10-06 宜蘭 一 花莲 罗东→蘇澳→南方澳→花莲罗东火车站搭火車去蘇澳车站, 再搭计程/公車南方澳 宿花莲 Day4 2018-10-07 花莲 一 日游 清水断崖/太鲁阁/七星潭/夜市(出发点)/花莲市区 宿花莲 Day5 2018-10-08 花莲 一 台北 出海赏鲸/…/饒河夜市/101夜景/101東區小酌 根据情况考虑，套装回来 宿台北 Day6 2018-10-09 台北 一 日游 台大/淡水/漁人碼頭/士林夜市 宿台北 Day7 2018-10-10 台北 一 日游 台北市立美术馆/士林官邸/故宫/阳明山 宿台北 Day8 2018-10-11 台北 一 杭州 猫空/诚品书店/逛买/下午去机场 宿杭州 酒店安排 No. Date Title Del 台北 西门町 1. 2018-10-04 台北禾顺商旅(Your Hotel) 2018-10-01 12:00前可免费取消修改 宜兰 礁溪 2. 2018-10-05 宜兰礁溪若轻温泉饭店(Ruocing Inn) 2018-09-28 12:00 前可免费取消修改 花莲 火车站 市区 接机 3. 2018-10-06 花莲世外桃源童话城堡民宿(Hualien Paradise Castle) 2018-09-29 12:00 前可免费取消修改 4. 2018-10-07 花莲世外桃源童话城堡民宿(Hualien Paradise Castle) 2018-09-30 12:00 前可免费取消修改 台北 西门町 5. 2018-10-08 台北禾顺商旅(Your Hotel) 2018-10-05 12:00前可免费取消修改 台北 士林观光夜市 双早 6. 2018-10-09 台北佳佳士林珮柏公寓(Papersun Apartment) 2018-10-03 17:00前可免费取消修改 7. 2018-10-10 台北佳佳士林珮柏公寓(Papersun Apartment) 2018-10-03 17:00前可免费取消修改 美食推荐 饶河夜市 (唯 木瓜牛奶 好喝，又便宜又新鲜) 自强夜市 (芋圆 和 牛乳葱油饼干、台湾钓鱼) 不错 花莲夜市 (烤台湾鲷，淡水鱼、欣叶和饕食天堂) 很赞 宁夏夜市 (人多价高味差浪费时间) 不太 OK 日月潭的云品酒店，价格不高，品种十分丰富 又美味 垦丁 兴海食堂、一品卤味 不错 芋圆红豆豆花冰好吃 九份不太好玩 花莲七星潭和七星潭的公路很美，清水断崖上面看下就行了。 苏花公路很美，群主拼車最好 支付寶很強大也很便宜活動 垦丁拍图很美 Day3 交通说明: 礁溪→烏石港: 從礁溪到131, 1766或紅1號公車前往烏石港, 或是你要搭火車到比較近的頭城火車站轉搭公車前往也可以 烏石港→蘭陽博物館: 步行即可，非常近 到礁溪坐台铁10分钟 / 礁溪到头城坐台铁10分钟 Day4 交通说明: 社會福利館, 西堤烏橋跟津海棧道是在宜蘭市區內 礁溪→宜蘭: 因為你要去社會福利處, 西堤屋橋跟津海木棧道，所以你可以直接從生活學習館那邊搭1740(班次很少), 1766或紅1號公車(假日行駛)前往, 然後在蘭陽別院下車 宜蘭→蘇澳→南方澳: 你可以從宜蘭車站搭台铁蘇澳(30分鐘), 再轉搭计程车(或公車)去南方澳(5到10分鐘), 或是你直接從宜蘭搭1766或紅2號公車(假日行駛)直接到南方澳也可以， 宜兰到南方澳的话，时间略长，因为要从苏澳坐车才能到南方澳，所以差不多要1小时左右 16点多开始游览南方澳，差不多1-2小时结束游览可以在这里吃饭，也可以坐计程车回苏澳站，坐台铁回宜兰吃晚饭 龟山岛、樱桃陵园 是比较远的. 可以选择舍弃，暂时不排进来. 开往 礁溪 的列车，沿着太平洋的行驶，远处是龟山岛 礁溪 离 宜兰 坐台铁仅十分钟左右路程 头城镇是台湾省宜兰县下辖的一个镇，位于宜兰县最北端，北邻台北县贡寮乡、西接台北县双溪乡、南为礁溪乡和壮围乡、东滨太平洋。是宜兰县最早开发的地方 头城镇 ： 乌石港、兰阳博物馆、乌石港冲浪、头城老街、鹰石尖 兰阳博物馆是宜兰的必打卡景点第一名！ 社會福利館, 西堤烏橋跟津海棧道 罗东林场、罗东夜市 （葱油饼应该是特色） 南方澳: 非常美的一个小渔村，来这里最应该干的事情就是——吃海鲜！没有更赞！ 龟山岛是在宜兰县头城镇海岸以东约10公里处！也是宜兰县县属岛屿之中最大的岛屿，还是台湾地区目前尚存的活火山。来这里玩的主要就是为了出海看海豚或者鲸鱼！不过不是每次都能看到，得碰运气的！ 社會福利館, 西堤烏橋跟津海棧道是在宜蘭市區內, 礁溪生活學習館是在礁溪火車站附近. 一些攻略: 1，台北猫空缆车。360°无死角的台北全景都在你的脚下。到猫空山要吃茶餐。 2，去北投泡温泉。 3，去阳明山看日落。 4，故宫也很值得一去。 5，西门超多明星开的潮牌店和咖啡店 6，台湾101大楼。如果想要跟未来的自己说点什么,可去101的89楼寄明信片。 7，宁夏夜市。（去的时候下雨了，但还是吃了很多好吃的。） 8，喜欢宫崎骏，你一定不能错过九份山城。还有周边的平溪铁路，可以平溪十分猫村三个站点玩过去。这个的话我会推荐你早晨去玩平溪一条线，傍晚到九份山城感受小资的傍晚，晚上就住在九份。 9，渔人码头，很美。 (留着下次吧) 10，诚品书店，可以泡上一天。 行程细则: Day1 台北 一 日游 平溪线+九份=十分，幸福 第二天的行程是游 平溪 线和 九份 山城。 一早，搭乘台铁区间车到达 瑞芳 车站，在车站就可以直接购买 平溪 线的车票了，80台币。 平溪 线的车是1小时一班的，但不是每个站都值得玩，我们就只选择了菁桐， 平溪 和十分。 下车游玩前记得先看看车站的时刻表，好安排坐下一站的班车。 Reference 台湾北部8日玩法 台北、垦丁、花莲7日游（干货） 台北到垦丁的高铁怎么走方便从哪转车 環島-台灣七日環島 与你相遇，好幸运（台湾~台北自由行攻略） 花莲台东速览 台东和花莲逛了5天的深度游，来了不想走，走了还想来！ 首次台湾游～台东、花莲线 去东台湾看海：宜兰、花莲、台东 暖暖的台湾-2014.5第二次台湾之旅（8天7夜-正篇2 宜蘭 花蓮）（更新完） 从台北到台东怎么走方便？高铁与火车比较 发现不一样的北台湾—7天台北、宜兰、花… 花东海岸线一日游【天空步道+石门班哨角+北回归线+长虹桥】 台湾8天自由行（台北、宜兰、花莲）–6人 花莲旅游攻略 / 带我走——2018 台北、宜兰、花莲吃喝玩… 宜兰头城罗东一日游 宜兰2日游 『台湾秘境』宜蘭~四季皆動人的後山小城… next ⋯⋯","categories":[{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"}],"tags":[{"name":"Taiwan","slug":"Taiwan","permalink":"http://www.iequa.com/tags/Taiwan/"}]},{"title":"Improving Deep Neural Networks (week2) - Optimization Algorithm","slug":"deeplearning/Improving-Deep-Neural-Networks-week2","date":"2018-07-21T02:00:21.000Z","updated":"2021-06-20T04:12:28.259Z","comments":true,"path":"2018/07/21/deeplearning/Improving-Deep-Neural-Networks-week2/","link":"","permalink":"http://www.iequa.com/2018/07/21/deeplearning/Improving-Deep-Neural-Networks-week2/","excerpt":"Mini-batch、指数加权平均-偏差修正、Momentum、RMSprop、Adam、学习率衰减、局部最优 这节课每一节的知识点都很重要，所以本次笔记几乎涵盖了全部小视频课程的记录","text":"Mini-batch、指数加权平均-偏差修正、Momentum、RMSprop、Adam、学习率衰减、局部最优 这节课每一节的知识点都很重要，所以本次笔记几乎涵盖了全部小视频课程的记录 1. Mini-batch 随机梯度下降法的一大缺点是, 你会失去所有向量化带给你的加速，因为一次性只处理了一个样本，这样效率过于低下, 所以实践中最好 选择不大不小 的 Mini-batch 尺寸. 实际上学习率达到最快，你会发现2个好处，你得到了大量向量化，另一方面 你不需要等待整个训练集被处理完，你就可以开始进行后续工作. 它不会总朝着最小值靠近，但它比随机梯度下降要更持续地靠近最小值的方向. 它也不一定在很小的范围内收敛，如出现这个问题，你可以减小 学习率. 样本集比较小，就没必要使用 mini-batch. 经验值 ： 如果 m &lt;= 2000, 可以使用 batch， 不然样本数目 m 较大，一般 mini-batch 大小设置为 64 or 128 or… or 512… 算法初步 对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有 500万 或 5000万 的训练数据，处理速度就会比较慢。 但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 Mini-batch。 如图，以 1000 为单位，将数据划分，令 x1=x(1),x(2)……x(1000)x^{\\\\{1\\\\}}=\\\\{x^{(1)},x^{(2)}……x^{(1000)}\\\\}x1=x(1),x(2)……x(1000), 一般用 xtx^{ \\\\{ t \\\\} }xt, yty^{ \\\\{t\\\\} }yt 表示划分后的 mini-batch. 注意区分该系列教学视频的符号标记： 小括号() 表示具体的某一个元素，指一个具体的值，例如 x(i)x^{(i)}x(i) 中括号[] 表示神经网络中的某一层, 例如 Z[l]Z^{[l]}Z[l] 大括号{} 表示将数据细分后的一个集合, 例如 x1=x(1),x(2)……x(1000)x^{\\\\{1\\\\}} = \\\\{x^{(1)},x^{(2)}……x^{(1000)}\\\\}x1=x(1),x(2)……x(1000) 算法核心 假设我们有 5,000,000 个数据，每 1000 作为一个集合，计入上面所提到的 x1=x(1),x(2)……x(5000),……x^{\\\\{1\\\\}}=\\\\{x^{(1)},x^{(2)}……x^{(5000)}\\\\},……x1=x(1),x(2)……x(5000),…… 需要迭代运行 5000次 神经网络运算. 每一次迭代其实与之前笔记中所提到的计算过程一样，首先是前向传播，但是每次计算的数量是 1000. 计算损失函数，如果有 Regularization ，则记得加上 Regularization Item Backward propagation 注意，mini-batch 相比于之前一次性计算所有数据不仅速度快，而且反向传播需要计算 5000次，所以效果也更好. epoch 对于普通的梯度下降法，一个 epoch 只能进行一次梯度下降； 对于 Mini-batch 梯度下降法，一个 epoch 可以进行 Mini-batch 的个数次梯度下降; epoch : 当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个 epoch。 比如对于一个有 2000 个训练样本的数据集。将 2000 个样本分成大小为 500 的 batch，那么完成一个 epoch 需要 4 个 iteration。 不同 size 大小的比较 普通的 batch 梯度下降法 和 Mini-batch梯度下降法 代价函数的变化趋势，如下图所示： Batch梯度下降 （如下图中蓝色）: 对所有 m 个训练样本执行一次梯度下降，每一次迭代时间较长； Cost function 总是向减小的方向下降。 说明: mini-batch size = m，此时即为 Batch gradient descent (x{t},y{t})=(X,Y)(x^{\\{t\\}},y^{\\{t\\}})=(X,Y)(x{t},y{t})=(X,Y) 随机梯度下降 （如下图中紫色）: -对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速； Cost function 总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式. 说明: mini-batch size = 1，此时即为 Stochastic gradient descent (xt,yt)=(x(i),y(i))(x^{\\\\{t\\\\}},y^{\\\\{t\\\\}})=(x^{(i)},y^{(i)})(xt,yt)=(x(i),y(i)) Mini-batch梯度下降 （如下图中绿色）: 选一个 1&lt;size&lt;m1&lt;size&lt;m1&lt;size&lt;m 的合适的 size 进行 Mini-batch 梯度下降，可实现快速学习，也应用了向量化带来的好处 Cost function 的下降处于前两者之间 Mini-batch 大小的选择 如果训练样本的大小比较小时，如 m⩽2000m⩽2000m⩽2000 时 — 选择 batch 梯度下降法； 如果训练样本的大小比较大时，典型的大小为：26、27、⋯、2102^{6}、2^{7}、\\cdots、2^{10}26、27、⋯、210 Mini-batch 的大小要符合 CPU/GPU 内存， 运算起来会更快一些. 2. Exponentially weighted averages 为了理解后面会提到的各种优化算法，我们需要用到指数加权平均，在统计学中也叫做指数加权移动平均. 指数加权平均的关键函数： v_t=βv_t−1+(1−β)θ_tv\\_{t} = \\beta v\\_{t-1}+(1-\\beta)\\theta\\_{t} v_t=βv_t−1+(1−β)θ_t 首先我们假设有一年的温度数据，如下图所示 我们现在需要计算出一个温度趋势曲线，计算方法(指数加权平均实现)如下： v_0=0v_1=βv_0+(1−β)θ_1v_2=βv_1+(1−β)θ_2v_3=βv_2+(1−β)θ_3…v\\_{0} =0 \\\\\\\\ v\\_{1}= \\beta v\\_{0}+(1-\\beta)\\theta\\_{1} \\\\\\\\ v\\_{2}= \\beta v\\_{1}+(1-\\beta)\\theta\\_{2} \\\\\\\\ v\\_{3}= \\beta v\\_{2}+(1-\\beta)\\theta\\_{3} \\\\\\\\ \\ldots v_0=0v_1=βv_0+(1−β)θ_1v_2=βv_1+(1−β)θ_2v_3=βv_2+(1−β)θ_3… 上面的 θ_tθ\\_tθ_t 表示第 ttt 天的温度，β 是可调节的参数，V_tV\\_tV_t 表示 11−β\\frac{1}{1-β}1−β1​ 天的每日温度","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Improving Deep Neural Networks (week1) - 深度学习的实用层面","slug":"deeplearning/Improving-Deep-Neural-Networks-week1","date":"2018-07-19T12:00:21.000Z","updated":"2021-06-20T04:12:28.253Z","comments":true,"path":"2018/07/19/deeplearning/Improving-Deep-Neural-Networks-week1/","link":"","permalink":"http://www.iequa.com/2018/07/19/deeplearning/Improving-Deep-Neural-Networks-week1/","excerpt":"这次我们要学习专项课程中第二门课 Improving Deep Neural Networks 学完这门课之后，你将会: 能够高效地使用神经网络通用的技巧，包括 初始化、L2和dropout正则化、Batch归一化、梯度检验。 能够实现并应用各种优化算法，例如 Mini-batch、Momentum、RMSprop、Adam，并检查它们的收敛程度。 理解深度学习时代关于如何 构建训练/开发/测试集 以及 偏差/方差分析 最新最有效的方法. 能够用TensorFlow实现一个神经网络 这门课将会详尽地介绍深度学习的基本原理，而不仅仅只进行理论概述.","text":"这次我们要学习专项课程中第二门课 Improving Deep Neural Networks 学完这门课之后，你将会: 能够高效地使用神经网络通用的技巧，包括 初始化、L2和dropout正则化、Batch归一化、梯度检验。 能够实现并应用各种优化算法，例如 Mini-batch、Momentum、RMSprop、Adam，并检查它们的收敛程度。 理解深度学习时代关于如何 构建训练/开发/测试集 以及 偏差/方差分析 最新最有效的方法. 能够用TensorFlow实现一个神经网络 这门课将会详尽地介绍深度学习的基本原理，而不仅仅只进行理论概述. 本周主要内容包括: Data set partition Bias / Variance Regularization Normalization Gradient Checking 1. Train/dev/test 在上一周的内容中, 介绍了神经网络中的常用符号以及各种变量的维度. 不清楚的可以回顾上周的笔记内容. 1.1 Data set partition 在训练完一个模型时, 我们需要知道这个模型预测的效果. 此时就需要一个额外的数据集, 我们称为 dev/hold out/validation set, 这里我们就统一称之为验证集. 如果我们需要知道模型最终效果的无偏估计, 那么我们还需要一个测试集. 在以往传统的机器学习中, 我们通常按照 70/30 来数据集分为 Train set/Validation set, 或者按照 60/20/20 的比例分为 Train/Validation/Test. 但在今天机器学习问题中, 我们可用的数据集的量级非常大 (例如有 100W 个样本). 这时我们就不需要给验证集和测试集太大的比例, 例如 98/1/1. 1.2 Data src distribution 在划分数据集中, 有一个比较常见的错误就是不小心使得在训练集中的数据和验证或测试集中的数据来自于不同的分布. 例如我们想要做一个猫的分类器, 在划分数据的时候发现训练集中的图片全都是来自于网页, 而验证集和测试集中的数据全都来自于用户. 这是一种完全错误的做法, 在实际中一定要杜绝. 2. Bias / Variance 关于 Bias / Variance 相比大家都很熟悉了, 在机器学习的课程中也已经学习到. 下面祭出 Andrew Ng 经典的图例解释: 我们该如何定位模型所处的问题? 如下图所示, 这里举了四中情况下的训练集和验证集误差. 当 训练误差很小, 验证误差很大时 为 High Variance 当 训练误差 和 验证误差 接近 且 都很大 时为 High Bias 当 训练误差很大, 验证误差更大时为 High Variance &amp;&amp; High Bias 当 训练误差 和 验证误差接近且都很小时为 Low Variance &amp;&amp; Low Bias 关于高方差高偏差可能是第一次听过, 如下图所示, 整体上模型处于高偏差, 但是对于一些噪声又拟合地很好. 此时就处于高偏差高方差的状态. 当我们学会定位模型的问题后, 那么该怎样解决对应的问题呢? 见下图: 若 High bias, 我们可以增加模型的复杂度例如使用一个”更大”的网络结构或者训练更久一点. 如 High variance, 我们可以想办法 get more data, 或者使用接下来我们要讲的 Regularization. 3. Regularization 为什么正则化没有加 λ2mb2\\frac{\\lambda}{2m} b^22mλ​b2: 因为 www 通常是一个高维参数矢量, 已经可以表达 High bias 的问题, www 可能含有很多参数，我们不可能拟合所有参数, 而 bbb 只是单个数字, 所以 www 几乎覆盖了所有参数，而不是 bbb, 如果加了 bbb 也没有影响，因为 bbb 只是众多参数中的一个. 关于 L1 regularization : 如果用的是 L1 regularization, then www will end up being sprase 稀疏的, 也就是说 www 向量中有很多 0. 有人说这样有利于压缩模型，但是我觉得不是很合适. 越来越多的人使用 L2. Notes: 不称为:矩阵 L2 范数， 按照惯例我们称为: Frobenius norm of a matrix, 其实就是 : 矩阵 L2 范数。 3.1 L2 regularization L2 regularization 下的 Cost Function 如下所示, 只需要添加正则项 λ2m∑_l=1L∣∣w[l]∣∣2_F\\frac{\\lambda}{2m}\\sum\\_{l=1}^L||w^{[l]}||^2\\_F2mλ​∑_l=1L∣∣w[l]∣∣2_F, 其中 F 代表 Frobenius Norm. 在添加了正则项之后, 相应的梯度也要变化, 所以在更新参数的时候需要加上对应的项. 这里注意一点, 我们只对参数 www 正则, 而不对 bbb. 因为对于每一层来说, www 有很高的维度, 而 bbb 只是一个标量. www 对整个模型的影响远大于 bbb. 下面给出添加 regularization 为什么能防止过拟合给出直观的解释. 如下图所示: 当我们的 λ 比较大的时候, 模型就会加大对 w 的惩罚, 这样有些 w 就会变得很小 (L2 Regularization 也叫权重衰减, weights decay). 从下图左边的神经网络来看, 效果就是整个神经网络变得简单了(一些隐藏层甚至 www 趋向于 0), 从而降低了过拟合的风险. 那些 隐藏层 并没有被消除，只是影响变得更小了，神经网络变得简单了. 从另一个角度来看. 以 tanh激活函数 为例, 当 λλλ 增加时, www 会偏小, 这样 z=wa+bz = wa +bz=wa+b 也会偏小, 此时的激活函数大致是线性的. 这样模型的复杂度也就降低了, 即降低了过拟合的风险. 如果神经网络每层都是线性的，其实整个还是一个线性的, 即使是一个很深的网络，因为线性激活函数的特征，最终我们只能计算线性函数. 3.2 Dropout dropout 也是一种正则化的手段, 在训练时以 1-keep_prob 随机地”丢弃”一些节点. 如下图所示. 具体可参考如下实现方式, 在前向传播时将 aaa 中的某些值置为0, 为了保证大概的大小不受添加 dropout 影响, 再将处理后的 aaa 除以 keep_prob. dropout 将产生收缩权重的平方范数的效果, 和 L2 类似，实施 dropout 的结果是它会压缩权重，并完成一些预防过拟合的外层正则化，事实证明 dropout 被正式地作为一种正则化的替代形式 L2 对不同权重的衰减是不同的，它取决于倍增的激活函数的大小. dropout 的功能类似于 L2 正则化. 甚至 dropout 更适用于不同的输入范围. Notes: 每一层的 keep_prob 可能是不同的, keep_prob 取 1， 则是该层保留所有单元. 输出层的 keep_prob 经常设置为 1，有时候也可以设置为 1.9 (&gt;1). &lt; 1 通常在输出层是不太可能的. 输入层的 keep_prob 经常设置为 1，有时候也可以设置为 0.9， 如果是 0.5 消减一半，通常是不可能的. 其他 : 计算机视觉的人员非常钟情 dropout 函数. Notes: dropout 的一大缺点就是 J 不会被明确定义. 每次迭代都会被随机删除一些节点. 如果再三检查梯度下降的性能，实际上是很难复查的. 定义明确的代价函数，每次迭代都会下降. 因为 dropout 使得 J 没有被明确定义，或者在某种程度上很难计算. 所以我们失去了调试工具，我通常会关闭 dropout. keep_prob 设置为 1， 运行代码，确保 J 函数单调递减, 然后在打开 dropout, 在 dropout 的过程中，代码并未引入bug. 实现代码(未完成) 3.3 Other Regularization Data augmentation Early stopping W 开始是变小的，之后会随着迭代越来越大. early stopping 就是在中间点停止迭代过程. Notes: early stopping 缺点是 提早停止，w 是防止了过拟合，但是 J 没有被继续下降. L2 正则化 的缺点是，要用大量精力搜索合适的 λ . 我个人也是更倾向于使用 L2，如果你可以负担大量的计算代价. 4. Normalization 0 均值化 归一化 方差 上图2， 特征 x1 的方差 比 特征 x2 的方差 大很多 上图3， 特征 x1 和 特征 x2 的 方差 都是 1 注意: 不论 训练集 和 测试集，都是通过相同的 μ\\muμ 和 σ2{\\sigma}^2σ2 定义的相同数据转换, 其中 μ\\muμ 和 σ2{\\sigma}^2σ2 是由训练数据计算而来. 5. Vanishing/Exploding gradients Vanishing/Exploding gradients 指的是随着前向传播不断地进行, 激活单元的值会逐层指数级地增加或减小, 从而导致梯度无限增大或者趋近于零, 这样会严重影响神经网络的训练. 如下图. 为了直观理解梯度消失和梯度爆炸，我们假设所有激活函数为线性激活函数，即 g(z)=zg(z)=zg(z)=z。 并假设前 L−1 个权重矩阵都相等, 即为 W_linearW\\_{linear}W_linear，所以可以得到 y_hat=W_linearL−1W_LXy\\_{hat}=W\\_{linear}^{L-1}W\\_{L}Xy_hat=W_linearL−1W_LX 假设 W_linearW\\_{linear}W_linear 都等于这个: 那么则有 y_hat=1.5L−1W_LXy\\_{hat}=1.5^{L-1}W\\_LXy_hat=1.5L−1W_LX，很显然当 L 很大时则会出现梯度爆炸。 同理若将权重的值设置为小于1，那么则会出现梯度消失。 一个可以减小这种情况发生的方法, 就是用有效的参数初始化 (该方法并不能完全解决这个问题). 但是也是有意义的 设置合理的权重，希望你设置的权重矩阵，既不会增长过快，也不会下降过快到 0. 想更加了解如何初始化权重可以看下这篇文章 神经网络权重初始化问题，其中很详细的介绍了权重初始化问题。 6. Gradient checking implementation 很难用梯度检验来双重检验 dropout 的计算， 所以我不同时使用梯度检验和 dropout，除非 dropout keep.prob 设置为 1. 我建议关闭 dropout 用梯度检验进行双重检查. 在没有 dropout 的情况下，确保你的算法是正确的，然后再打开 dropout. 现实中 几乎不会出现, 当 w 和 b 接近 0 时，梯度下降的实施是正确的. 8. Reference 网易云课堂 - deeplearning deeplearning.ai 专项课程二第一周 Coursera - Deep Learning Specialization DeepLearning.ai学习笔记汇总","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Neural Networks and Deep Learning (week4) - Deep Neural Networks","slug":"deeplearning/Neural-Networks-and-Deep-Learning-week4","date":"2018-07-15T12:00:21.000Z","updated":"2021-06-20T04:12:28.250Z","comments":true,"path":"2018/07/15/deeplearning/Neural-Networks-and-Deep-Learning-week4/","link":"","permalink":"http://www.iequa.com/2018/07/15/deeplearning/Neural-Networks-and-Deep-Learning-week4/","excerpt":"本周重点任务是使用Python要实现一个任意层的神经网络, 并在cat数据上测试.","text":"本周重点任务是使用Python要实现一个任意层的神经网络, 并在cat数据上测试. 1. 深度神经网络中的常用符号回顾 在上一周的内容中, 介绍了神经网络中的常用符号以及各种变量的维度. 不清楚的可以回顾上周的笔记内容. 2. Intuition about deep representation 关于深度神经网络直观地解释这部分笔记暂略, 请直接观看课程视频内容: Why deep representation?. 3. 深度神经网络中的前向/反向传播 在第三周的笔记中详细介绍了神经网络的前向/反向传播, 这里完全套用, 只是多了层数而已. 需要再详细了解手推的同学可以仔细研究上周的笔记内容 4. 参数与超参数 在神经网络中参数指的是 WWW, bbb, 这两个参数是通过梯度下降算法不断优化的. 而超参数指的是学习率, 迭代次数, 决定神经网络结构的参数以及激活函数的选择等等, 在后面我们还会提到 momentum, minibatch size, regularization等等. 这些都属于超参数, 需要我们手动设定. 这些超参数也决定了最终的参数 WWW, bbb. 不同的超参数的选择会导致模型很大的差别. 所以超参数的选择也非常重要 (后面的课程会讲解如何选择超参数). 5. 使用Python实现深度神经网络 DeepNeuralNetwork.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169def sigmoid(z): return 1. / (1.+np.exp(-z))def relu(Z): A = np.maximum(0,Z) return Adef leaky_relu(Z): A = np.maximum(0,Z) A[Z &lt; 0] = 0.01 * Z return Aclass DeepNeuralNetwork(): def __init__(self, layers_dim, activations): # assert (layers_dim[-1] == 1) # assert (activations[-1] == &#x27;sigmoid&#x27;) # assert (len(activations) == len(layers_dims)-1) np.random.seed(1) self.layers_dim = layers_dim self.__num_layers = len(layers_dim) self.activations = activations self.input_size = layers_dim[0] self.parameters = self.__parameters_initializer(layers_dim) self.output_size = layers_dim[-1] def __parameters_initializer(self, layers_dim): # special initialzer with np.sqrt(layers_dims[l-1]) L = len(layers_dim) parameters = &#123;&#125; for l in range(1, L): parameters[&#x27;W&#x27;+str(l)] = np.random.randn(layers_dim[l], layers_dim[l-1]) / np.sqrt(layers_dims[l-1]) parameters[&#x27;b&#x27;+str(l)] = np.zeros((layers_dim[l], 1)) return parameters def __one_layer_forward(self, A_prev, W, b, activation): Z = np.dot(W, A_prev) + b if activation == &#x27;sigmoid&#x27;: A = sigmoid(Z) if activation == &#x27;relu&#x27;: A = relu(Z) if activation == &#x27;leaky_relu&#x27;: A = leaky_relu(Z) if activation == &#x27;tanh&#x27;: A = np.tanh(Z) cache = &#123;&#x27;Z&#x27;: Z, &#x27;A&#x27;: A&#125; return A, cache def __forward_propagation(self, X): caches = [] A_prev = X caches.append(&#123;&#x27;A&#x27;: A_prev&#125;) # forward propagation by laryer for l in range(1, len(self.layers_dim)): W, b = self.parameters[&#x27;W&#x27;+str(l)], self.parameters[&#x27;b&#x27;+str(l)] A_prev, cache = self.__one_layer_forward(A_prev, W, b, self.activations[l-1]) caches.append(cache) AL = caches[-1][&#x27;A&#x27;] return AL, caches def __compute_cost(self, AL, Y): m = Y.shape[1] cost = -np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL)) / m return cost def cost_function(self, X, Y): # use the result from forward propagation and the label Y to compute cost assert (self.input_size == X.shape[0]) AL, _ = self.__forward_propagation(X) return self.__compute_cost(AL, Y) def sigmoid_backward(self, dA, Z): s = sigmoid(Z) dZ = dA * s*(1-s) return dZ def relu_backward(self, dA, Z): dZ = np.array(dA, copy=True) dZ[Z &lt;= 0] = 0 return dZ def leaky_relu_backward(self, dA, Z): dZ = np.array(dA, copy=True) dZ[Z &lt;= 0] = 0.01 return dZ def tanh_backward(self, dA, Z): s = np.tanh(Z) dZ = 1 - s*s return dZ def __linear_backward(self, dZ, A_prev, W): # assert(dZ.shape[0] == W.shape[0]) # assert(W.shape[1] == A_prev.shape[0]) m = A_prev.shape[1] dW = np.dot(dZ, A_prev.T) / m db = np.sum(dZ, axis=1, keepdims=True) / m dA_prev = np.dot(W.T, dZ) return dA_prev, dW, db def __activation_backward(self, dA, Z, activation): assert (dA.shape == Z.shape) if activation == &#x27;sigmoid&#x27;: dZ = self.sigmoid_backward(dA, Z) if activation == &#x27;relu&#x27;: dZ = self.relu_backward(dA, Z) if activation == &#x27;leaky_relu&#x27;: dZ = self.leaky_relu_backward(dA, Z) if activation == &#x27;tanh&#x27;: dZ = self.tanh_backward(dA, Z) return dZ def __backward_propagation(self, caches, Y): m = Y.shape[1] L = self.__num_layers grads = &#123;&#125; # backward propagate last layer AL, A_prev = caches[L-1][&#x27;A&#x27;], caches[L-2][&#x27;A&#x27;] dAL = - (Y/AL - (1-Y)/(1-AL)) grads[&#x27;dZ&#x27;+str(L-1)] = self.__activation_backward(dAL, caches[L-1][&#x27;Z&#x27;], self.activations[-1]) grads[&#x27;dA&#x27;+str(L-2)], \\ grads[&#x27;dW&#x27;+str(L-1)], \\ grads[&#x27;db&#x27;+str(L-1)] = self.__linear_backward(grads[&#x27;dZ&#x27;+str(L-1)], A_prev, self.parameters[&#x27;W&#x27;+str(L-1)]) # backward propagate by layer for l in reversed(range(1, L-1)): grads[&#x27;dZ&#x27;+str(l)] = self.__activation_backward(grads[&#x27;dA&#x27;+str(l)], caches[l][&#x27;Z&#x27;], self.activations[l-1]) A_prev = caches[l-1][&#x27;A&#x27;] grads[&#x27;dA&#x27;+str(l-1)], \\ grads[&#x27;dW&#x27;+str(l)], \\ grads[&#x27;db&#x27;+str(l)] = self.__linear_backward(grads[&#x27;dZ&#x27;+str(l)], A_prev, self.parameters[&#x27;W&#x27;+str(l)]) return grads def __update_parameters(self, grads, learning_rate): for l in range(1, self.__num_layers): # assert (self.parameters[&#x27;W&#x27;+str(l)].shape == grads[&#x27;dW&#x27;+str(l)].shape) # assert (self.parameters[&#x27;b&#x27;+str(l)].shape == grads[&#x27;db&#x27;+str(l)].shape) self.parameters[&#x27;W&#x27;+str(l)] -= learning_rate * grads[&#x27;dW&#x27;+str(l)] self.parameters[&#x27;b&#x27;+str(l)] -= learning_rate * grads[&#x27;db&#x27;+str(l)] def fit(self, X, Y, num_iterations, learning_rate, print_cost=False, print_num=100): for i in range(num_iterations): # forward propagation AL, caches = self.__forward_propagation(X) # compute cost cost = self.__compute_cost(AL, Y) # backward propagation grads = self.__backward_propagation(caches, Y) # update parameters self.__update_parameters(grads, learning_rate) # print cost if i % print_num == 0 and print_cost: print (&quot;Cost after iteration %i: %f&quot; %(i, cost)) return self def predict_prob(self, X): A, _ = self.__forward_propagation(X) return A def predict(self, X, threshold=0.5): pred_prob = self.predict_prob(X) threshold_func = np.vectorize(lambda x: 1 if x &gt; threshold else 0) Y_prediction = threshold_func(pred_prob) return Y_prediction def accuracy_score(self, X, Y): pred = self.predict(X) return len(Y[pred == Y]) / Y.shape[1] main.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import timeimport numpy as npimport h5pyimport matplotlib.pyplot as pltimport scipyfrom PIL import Imagefrom scipy import ndimagefrom dnn_app_utils_v2 import *%matplotlib inlineplt.rcParams[&#x27;figure.figsize&#x27;] = (5.0, 4.0) # set default size of plotsplt.rcParams[&#x27;image.interpolation&#x27;] = &#x27;nearest&#x27;plt.rcParams[&#x27;image.cmap&#x27;] = &#x27;gray&#x27;%load_ext autoreload%autoreload 2np.random.seed(1)train_x_orig, train_y, test_x_orig, test_y, classes = load_data()# Explore your datasetm_train = train_x_orig.shape[0]num_px = train_x_orig.shape[1]m_test = test_x_orig.shape[0]print (&quot;Number of training examples: &quot; + str(m_train))print (&quot;Number of testing examples: &quot; + str(m_test))print (&quot;Each image is of size: (&quot; + str(num_px) + &quot;, &quot; + str(num_px) + &quot;, 3)&quot;)print (&quot;train_x_orig shape: &quot; + str(train_x_orig.shape))print (&quot;train_y shape: &quot; + str(train_y.shape))print (&quot;test_x_orig shape: &quot; + str(test_x_orig.shape))print (&quot;test_y shape: &quot; + str(test_y.shape))# Reshape the training and test examplestrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T # The &quot;-1&quot; makes reshape flatten the remaining dimensionstest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T# Standardize data to have feature values between 0 and 1.train_x = train_x_flatten/255.test_x = test_x_flatten/255.print (&quot;train_x&#x27;s shape: &quot; + str(train_x.shape))print (&quot;test_x&#x27;s shape: &quot; + str(test_x.shape))# Please note that the above code is from the programming assignmentimport DeepNeuralNetworklayers_dims = (12288, 20, 7, 5, 1)# layers_dims = (12288, 10, 1)# layers_dims = [12288, 20, 7, 5, 1] # 5-layer modelactivations = [&#x27;relu&#x27;, &#x27;relu&#x27;, &#x27;relu&#x27;,&#x27;sigmoid&#x27;]num_iter = 2500learning_rate = 0.0075clf = DeepNeuralNetwork(layers_dims, activations)\\ .fit(train_x, train_y, num_iter, learning_rate, True, 100)print(&#x27;train accuracy: &#123;:.2f&#125;%&#x27;.format(clf.accuracy_score(train_x, train_y)*100))print(&#x27;test accuracy: &#123;:.2f&#125;%&#x27;.format(clf.accuracy_score(test_x, test_y)*100))# output# Cost after iteration 0: 0.771749# Cost after iteration 100: 0.672053# Cost after iteration 200: 0.648263# Cost after iteration 300: 0.611507# Cost after iteration 400: 0.567047# Cost after iteration 500: 0.540138# Cost after iteration 600: 0.527930# Cost after iteration 700: 0.465477# Cost after iteration 800: 0.369126# Cost after iteration 900: 0.391747# Cost after iteration 1000: 0.315187# Cost after iteration 1100: 0.272700# Cost after iteration 1200: 0.237419# Cost after iteration 1300: 0.199601# Cost after iteration 1400: 0.189263# Cost after iteration 1500: 0.161189# Cost after iteration 1600: 0.148214# Cost after iteration 1700: 0.137775# Cost after iteration 1800: 0.129740# Cost after iteration 1900: 0.121225# Cost after iteration 2000: 0.113821# Cost after iteration 2100: 0.107839# Cost after iteration 2200: 0.102855# Cost after iteration 2300: 0.100897# Cost after iteration 2400: 0.092878# train accuracy: 98.56%# test accuracy: 80.00% 6. 本周内容回顾 深度神经网络中的前向/反向传播 参数与超参数 使用Python实现深度神经网络 7. Reference 网易云课堂 - deeplearning deeplearning.ai 专项课程一第四周 Coursera - Deep Learning Specialization","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Neural Networks and Deep Learning (week3) - Shallow Neural Networks","slug":"deeplearning/Neural-Networks-and-Deep-Learning-week3","date":"2018-07-14T06:55:21.000Z","updated":"2021-06-20T04:12:28.265Z","comments":true,"path":"2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/","link":"","permalink":"http://www.iequa.com/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/","excerpt":"正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。 在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。","text":"正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。 在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。 1. 常用符号与基本概念 该神经网络完全可以使用上一周所讲的计算图来表示, 和 LRLRLR 计算图的区别仅仅在于多了一个 zzz 和 aaa 的计算而已. 如果你已经完全掌握了上一周的内容, 那么其实你已经知道了神经网络的前向传播, 反向传播(梯度计算)等等. 要注意的是各种参数, 中间变量 (a,z)(a, z)(a,z) 的维度问题. 关于神经网络的基本概念, 这里就不赘述了. 见下图回顾一下: 2. 神经网络中的前向传播 我们先以一个训练样本来看神经网络中的前向传播. 我们只看这个神经网络中的输入层和隐藏层的第一个激活单元(如下图右边所示). 其实这就是一个Logistic Regression. 神经网络中输入层和隐藏层 (不看输出层), 这就不就是四个LR放在一起吗? 在 LR 中 zzz 和 aaa 的计算我们已经掌握了, 那么在神经网络中 zzz 和 aaa 又是什么呢? 我们记隐藏层第一个 zzz 为 z_1z\\_1z_1, 第二个 zzz 记为 z_2z\\_2z_2 以此类推. 只要将这四个 zzz 纵向叠加在一起称为一个**列向量 即可得到神经网络中这一层的 zzz** (aaa同理). 那么这一层的 w,bw, bw,b 又是如何得到的? 别忘了, 对于参数 www 来说, 它本身就是一个列项量, 那么它是如何做纵向叠加的呢? 我们只需要将其转置变成一个横向量, 再纵向叠加即可. 得到隐藏层的 aaa 之后, 我们可以将其视为输入, 现只看神经网络的隐藏层和输出层, 我们发现这不就是个 LRLRLR 嘛. 这里总结一下各种变量的维度 (注意: 这里是针对一个训练样本来说的, n_Ln\\_Ln_L 代表的 LLL 层的节点个数): w.shape:(n_L,n_(L−1))w.shape : (n\\_L, n\\_{(L-1)})w.shape:(n_L,n_(L−1)) b.shape:(n_L,1)b.shape : (n\\_L, 1)b.shape:(n_L,1) z.shape:(n_L,1)z.shape : (n\\_L, 1)z.shape:(n_L,1) a.shape:(n_L,1)a.shape : (n\\_L, 1)a.shape:(n_L,1) 那么如果有 mmm 个训练样本这些变量的维度又是怎样的呢. 我们思考哪些变量的维度会随着样本数的变化而变化. www 是参数显然它的维度是不会变的. 而输入每一个样本都会有一个 zzz 和 aaa, 还记得 XXX 的形式吗? 同样地, ZZZ 就是将每个样本算出来的 zzz 横向叠加(A同理). 具体计算过程如下图: 3. 神经网络中的激活函数 四种常用的激活函数: Sigmoid, Tanh, ReLU, Leaky ReLU. 其中 sigmoid 我们已经见过了, 它的输出可以看成一个概率值, 往往用在输出层. 对于中间层来说, 往往是ReLU的效果最好. Tanh 数据平均值为 0，具有数据中心化的效果，几乎在任何场合都优于 Sigmoid 以上激活函数的导数请自行在草稿纸上推导. derivative of sigmoid activation function derivative of tanh activation function derivative of ReLU and Leaky ReLU activation function 为什么需要激活函数? 如果没有激活函数, 那么不论多少层的神经网络都只相当于一个LR. 证明如下: it turns out that if you use a linear activation function or alternatively if you don’t have an activation function, then no matter how many layers your neural network has, always doing just computing a linear activation function, so you might as well not have any hidden layers. so unless you throw a non-linearity in there, then you’re not computing more interesting functions. 你可以在隐藏层用 tanh，输出层用 sigmoid，说明不同层的激活函数可以不一样。 现实情况是 : the tanh is pretty much stricly superior. never use sigmoid ReLU (rectified linear unit 矫正线性单元) tanh 和 sigmoid 都有一个缺点，就是 z 非常大或者非常小，函数的斜率(导数梯度)就会非常小, 梯度下降很慢. the slope of the function you know ends up being close to zero, and so this can slow down gradient descent ReLU (rectified linear unit) is well, z = 0 的时候，你可以给导数赋值为 0 or 1，虽然这个点是不可微的. 但实现没有影响. 虽然 z &lt; 0, 的时候，斜率为0， 但在实践中，有足够多的隐藏单元 令 z &gt; 0, 对大多数训练样本来说是很快的. Notes: so the one place you might use as linear activation function, others usually in the output layer. 4. 神经网络中的反向传播 back propagation 反向传播最主要的就是计算梯度, 在上一周的内容中, 我们已经知道了LR梯度的计算. 同样的方式, 我们使用计算图来计算神经网络中的各种梯度. dz^{\\[2\\]} = \\frac{dL}{dz}= \\frac{dL}{da^{\\[2\\]}}\\frac{da^{\\[2\\]}}{dz^{\\[2\\]}}=a^{\\[2\\]}-y dW^{\\[2\\]}=\\frac{dL}{dW^{\\[2\\]}}=\\frac{dL}{dz^{\\[2\\]}}\\frac{dz^{\\[2\\]}}{dW^{\\[2\\]}}=dz^{\\[2\\]}a^{\\[1\\]} db^{\\[2\\]}=\\frac{dL}{db^{\\[2\\]}}=\\frac{dL}{dz^{\\[2\\]}}\\frac{dz^{\\[2\\]}}{db^{\\[2\\]}}=dz^{\\[2\\]} backward propagation : dz^{\\[1\\]} = \\frac{dL}{dz^{\\[2\\]}}\\frac{dz^{\\[2\\]}}{da^{\\[1\\]}}\\frac{da^{\\[1\\]}}{dz^{\\[1\\]}}=W^{\\[2\\]T}dz^{\\[2\\]}*g^{\\[1\\]’}(z^{\\[1\\]}) dW^{\\[1\\]}=\\frac{dL}{dW^{\\[1\\]}}=\\frac{dL}{dz^{\\[1\\]}}\\frac{dz^{\\[1\\]}}{dW^{\\[1\\]}}=dz^{\\[1\\]}x^T db^{\\[1\\]}=\\frac{dL}{db^{\\[1\\]}}=\\frac{dL}{dz^{\\[1\\]}}\\frac{dz^{\\[1\\]}}{db^{\\[1\\]}}=dz^{\\[1\\]} Notes: \\frac{dL}{dz^{\\[2\\]}} = dz^{\\[2\\]} ， \\frac{dz^{\\[2\\]}}{da^{\\[1\\]}} = W^{\\[2\\]} ， \\frac{da^{\\[1\\]}}{dz^{\\[1\\]}}=g^{\\[1\\]’}(z^{\\[1\\]}) 下图右边为在mmm个训练样本上的向量化表达: Notes: n^\\[0\\] = input features n^\\[1\\] = hidden units n^\\[2\\] = output units 5. 神经网络中的参数初始化 在 LR 中我们的参数 www 初始化为 0, 如果在神经网络中也是用相同的初始化, 那么一个隐藏层的每个节点都是相同的, 不论迭代多少次. 这显然是不合理的, 所以我们应该 随机地初始化 www 从而解决这个 sysmmetry breaking problem. 破坏对称问题 具体初始化代码可参见下图, 其中 乘以 0.01 是为了让参数 www 较小, 加速梯度下降 如激活函数为 tanh 时, 若参数较大则 zzz 也较大, 此时的梯度接近于 0, 更新缓慢. 如不是 tanh or sigmoid 则问题不大. this is a relatively shallow neural network without too many hidden layers, so 0.01 maybe work ok. finally it turns out that sometimes there can be better constants than 0.01. bbb 并没有这个 sysmmetry breaking problem, 所以可以 np.zeros((2,1))np.zeros((2, 1))np.zeros((2,1)) 6. 用Python搭建简单神经网络 使用Python+Numpy实现一个简单的神经网络. 以下为参考代码 SimpleNeuralNetwork.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889def sigmoid(z): return 1. / (1.+np.exp(-z))class SimpleNeuralNetwork(): # simple neural network with one hidden layer def __init__(self, input_size, hidden_layer_size): self.paramters = self.__parameter_initailizer(input_size, hidden_layer_size) def __parameter_initailizer(self, n_x, n_h): # W cannot be initialized with zeros W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(1, n_h) * 0.01 b2 = np.zeros((1, 1)) return &#123;&#x27;W1&#x27;: W1,&#x27;b1&#x27;: b1,&#x27;W2&#x27;: W2,&#x27;b2&#x27;: b2&#125; def __forward_propagation(self, X): W1 = self.paramters[&#x27;W1&#x27;] b1 = self.paramters[&#x27;b1&#x27;] W2 = self.paramters[&#x27;W2&#x27;] b2 = self.paramters[&#x27;b2&#x27;] # forward propagation Z1 = np.dot(W1, X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) cache = &#123;&#x27;Z1&#x27;: Z1,&#x27;A1&#x27;: A1,&#x27;Z2&#x27;: Z2,&#x27;A2&#x27;: A2&#125; return A2, cache def __compute_cost(self, A2, Y): m = A2.shape[1] cost = -np.sum(Y*np.log(A2) + (1-Y)*np.log(1-A2)) / m return cost def cost_function(self, X, Y): # use the result from forward propagation and the label Y to compute cost A2, cache = self.__forward_propagation(X) cost = self.__compute_cost(A2, Y) return cost def __backward_propagation(self, cache, Y): A1, A2 = cache[&#x27;A1&#x27;], cache[&#x27;A2&#x27;] W2 = self.paramters[&#x27;W2&#x27;] m = X.shape[1] # backward propagation computes gradients dZ2 = A2 - Y dW2 = np.dot(dZ2, A1.T) / m db2 = np.sum(dZ2, axis=1, keepdims=True) / m dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = np.dot(dZ1, X.T) / m db1 = np.sum(dZ1, axis=1, keepdims=True) / m grads = &#123;&#x27;dW1&#x27;: dW1,&#x27;db1&#x27;: db1,&#x27;dW2&#x27;: dW2,&#x27;db2&#x27;: db2&#125; return grads def __update_parameters(self, grads, learning_rate): self.paramters[&#x27;W1&#x27;] -= learning_rate * grads[&#x27;dW1&#x27;] self.paramters[&#x27;b1&#x27;] -= learning_rate * grads[&#x27;db1&#x27;] self.paramters[&#x27;W2&#x27;] -= learning_rate * grads[&#x27;dW2&#x27;] self.paramters[&#x27;b2&#x27;] -= learning_rate * grads[&#x27;db2&#x27;] def fit(self, X, Y, num_iterations, learning_rate, print_cost=False, print_num=100): for i in range(num_iterations): # forward propagation A2, cache = self.__forward_propagation(X) # compute cost cost = self.cost_function(X, Y) # backward propagation grads = self.__backward_propagation(cache, Y) # update parameters self.__update_parameters(grads, learning_rate) # print cost if i % print_num == 0 and print_cost: print (&quot;Cost after iteration %i: %f&quot; %(i, cost)) return self def predict_prob(self, X): # result of forward_propagation is the probability A2, _ = self.__forward_propagation(X) return A2 def predict(self, X, threshold=0.5): pred_prob = self.predict_prob(X) threshold_func = np.vectorize(lambda x: 1 if x &gt; threshold else 0) Y_prediction = threshold_func(pred_prob) return Y_prediction def accuracy_score(self, X, Y): pred = self.predict(X) return len(Y[pred == Y]) / Y.shape[1] main.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Package importsimport numpy as npimport matplotlib.pyplot as pltfrom testCases import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets%matplotlib inlinenp.random.seed(1) # set a seed so that the results are consistentX, Y = load_planar_dataset()# Please note that the above code is from the programming assignmentimport SimpleNeuralNetworknp.random.seed(3)num_iter = 10001learning_rate = 1.2input_size = X.shape[0]hidden_layer_size = 4clf = SimpleNeuralNetwork(input_size=input_size, hidden_layer_size=hidden_layer_size)\\ .fit(X, Y, num_iter, learning_rate, True, 1000)train_acc = clf.accuracy_score(X, Y)print(&#x27;training accuracy: &#123;&#125;%&#x27;.format(train_acc*100))# output# Cost after iteration 0: 0.693162# Cost after iteration 1000: 0.258625# Cost after iteration 2000: 0.239334# Cost after iteration 3000: 0.230802# Cost after iteration 4000: 0.225528# Cost after iteration 5000: 0.221845# Cost after iteration 6000: 0.219094# Cost after iteration 7000: 0.220628# Cost after iteration 8000: 0.219400# Cost after iteration 9000: 0.218482# Cost after iteration 10000: 0.217738# training accuracy: 90.5%for hidden_layer_size in [1, 2, 3, 4, 5, 20, 50]: clf = SimpleNeuralNetwork(input_size=input_size, hidden_layer_size=hidden_layer_size)\\ .fit(X, Y, num_iter, learning_rate, False) print(&#x27;&#123;&#125; hidden units, cost: &#123;&#125;, accuracy: &#123;&#125;%&#x27; .format(hidden_layer_size, clf.cost_function(X, Y), clf.accuracy_score(X, Y)))# output# 1 hidden units, cost: 0.6315593779798304, accuracy: 67.5%# 2 hidden units, cost: 0.5727606525435293, accuracy: 67.25%# 3 hidden units, cost: 0.2521014374551156, accuracy: 91.0%# 4 hidden units, cost: 0.24703039056643344, accuracy: 91.25%# 5 hidden units, cost: 0.17206481441467936, accuracy: 91.5%# 20 hidden units, cost: 0.16003869681611513, accuracy: 92.25%# 50 hidden units, cost: 0.16000569403994763, accuracy: 92.5% 7. 本周内容回顾 学习了神经网络的基本概念 掌握了神经网络中各种变量的维度 掌握了神经网络中的前向传播与反向传播 了解了神经网络中的激活函数 学习了神经网络中参数初始化的重要性 掌握了使用Python实现简单的神经网络 Reference 网易云课堂 - deeplearning deeplearning.ai 专项课程一第三周 Coursera - Deep Learning Specialization","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Neural Networks and Deep Learning (week2) - Neural Networks Basics","slug":"deeplearning/Neural-Networks-and-Deep-Learning-week2","date":"2018-07-07T01:55:21.000Z","updated":"2021-06-20T04:12:28.262Z","comments":true,"path":"2018/07/07/deeplearning/Neural-Networks-and-Deep-Learning-week2/","link":"","permalink":"http://www.iequa.com/2018/07/07/deeplearning/Neural-Networks-and-Deep-Learning-week2/","excerpt":"本周我们将要学习 Logistic Regression, 它是神经网络的基础. Logistic Regression 可以看成是一种只有输入层和输出层(没有隐藏层)的神经网络.","text":"本周我们将要学习 Logistic Regression, 它是神经网络的基础. Logistic Regression 可以看成是一种只有输入层和输出层(没有隐藏层)的神经网络.","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"Ensumble 集成学习小记","slug":"ml/ensumble-part1","date":"2018-07-03T09:43:21.000Z","updated":"2021-06-20T04:12:28.307Z","comments":true,"path":"2018/07/03/ml/ensumble-part1/","link":"","permalink":"http://www.iequa.com/2018/07/03/ml/ensumble-part1/","excerpt":"本文主要基于周志华《机器学习》一书第八章 集成学习内容做的整理笔记，此外查阅了网上的一些博客和问答网站","text":"本文主要基于周志华《机器学习》一书第八章 集成学习内容做的整理笔记，此外查阅了网上的一些博客和问答网站 1. Ensumble 概念 Ensumble 是通过构建并结合多个学习器来完成学习任务。 曾经听过一句话，”Feature为主，Ensemble为后”。 Feature 决定了模型效果的上限，而 Ensemble 就是让你更接近这个上限。 Ensemble 讲究“好而不同”，不同是指模型的学习到的侧重面不一样。 举个直观的例子，比如数学考试，A的函数题做的比B好，B的几何题做的比A好，那么他们合作完成的分数通常比他们各自单独完成的要高。 常见的Ensemble方法有Bagging、Boosting、Stacking、Blending. Notes: Stacking 与 Blending 类似，区别可参见 ensumble 中的 好而不同 如何使得集成学习性能比最好的单一学习器更好？ 准确性 多样性 好而不同 如何产生并结合 “好而不同” 的个体学习器 ? 集成学习研究的核心, 当前按照个体学习器的生成方式划分: bagging（及其变体随机森林）— 个体学习器间不存在强依赖关系，可同时生成的并行化方法 boosting - 个体学习器间存在强依赖关系，必须串行生成的序列化方法 从偏差-方差分解的角度: bagging 关注降低方差 boosting 关注降低偏差 也可以按照机器学习技法的两张图来理解 第一幅图(对应boosting)可看作进行了一个特征转换来防止欠拟合，第二幅图(对应bagging)则进行了一个正则化来防止过拟合 2. Boosting 首先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T,最终将这T个基学习器进行加权组合. 2.1 adaboost 推导方式，基于“加性模型”，即基于基学习器的线性组合 如何在每一轮修改样本分布 重赋权法：在每一轮训练中，根据样本分布为每一个训练样本重新赋予一个权重（比如《机器学习实战》一书中就是利用这种方法构建提升树算法，通过修改权重来计算每一轮的损失） 重采样法：利用重采样的训练集来对基学习器进行训练–&gt;重启动：避免训练过程过早停止 Notes: 西瓜书上的算法还提到训练的每一轮开始都要检查当前学习器是否比随机猜测好，若条件不满足则抛弃当前学习器，这种情形可能会导致学习过程未达到T轮即停止，所以有重采样的方法来进行重启动避免出现此种情况；但是另一方面《统计学习方法》以及《机器学习实战》中的算法并未有这条判断语句； 《机器学习》一书中提到，从统计学的出发认为AdaBoost实质上是基于加性模型（后续指出这一视角阐释的是一个与AdaBoost很相似的过程而非其本身），以类似牛顿迭代法来优化指数损失函数，通过将迭代优化过程替换为其他优化方法产生了GradientBoosting、LPBoost等；而这里也提到每一种变体针对不同的问题（噪声、数据不平衡等）而拥有不同的权重更新规则. 2.2 特点 从bias-variance分解的角度来看，Boosting主要关注降低 bias，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成（与bagging不同） Notes: boosting 对于噪音数据较为敏感 3. Bagging (Bootstrap aggregating ) 出发点依然是“好而不同” “不同” — 不同基学习器基于不同的样本子集 “好” — 使用相互有交叠的采样子集 3.1 工作机制 基于自助采样法（bootstrap sampling）— “有放回地全抽” 从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果,至于为什么叫bootstrap aggregation，因为它抽取训练样本的时候采用的就是bootstrap的方法！ 3.2 优点（相对于boosting） 高效 - 训练一个bagging集成与直接使用基学习器算法训练一个学习器的复杂度同阶 baggign能不经修改地用于多分类、回归任务（标准AdaBoosting只能适用于二分类任务） 包外估计——自助采样过程中剩余的样本可以作为验证集来对泛化性能进行“包外估计” 3.3 特点 从偏差-方差角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用明显. 4. Random Forest 随机森林 Bagging 的一个扩展变体 4.1 概述 以决策树为基学习器构建Bagging集成 在决策树的训练过程中引入了随机属性选择 传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在随机森林中，对基决策树的每个节点，先从该节点属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，其中k控制了随机性的引入程度 4.2 特点 简单、易于实现、计算开销小 样本扰动+属性扰动 4.3 bagging vs. 随机森林 两者收敛性相似，随机森林的起始性能往往相对较差，但会收敛到更低的泛化误差 随机森林的训练效率常优于Bagging，主要是决策树划分属性时，原始baggin需要对属性全集进行考虑，而 RF 是针对一个子集 5. 结合策略 5.1 学习器结合的好处 统计的角度：假设空间很大时，可能存在多个假设在训练集上达到同等性能，但学习其可能误选导致泛化性能不佳，结合多个学习器可以减小该风险 计算的角度：降低陷入糟糕局部极小点的风险 表示的角度：结合多个学习器可扩大假设空间，对于真实假设在假设空间之外的情形可能学得更好的近似 5.2 策略 平均法(Averaging) 简单平均法 加权平均法 投票法(Voting) 多数投票法 加权投票法 若按个体学习器输出值类型划分 (硬投票：预测为0/1 | 软投票：相当于对后验概率的一个估计) 学习法 Stacking:先从初始数据集训练出初级学习器，然后生成一个新的数据集用于训练元学习器，在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记；一般使用交叉验证法或留一法来用训练初级学习器未使用的样本来产生元学习器的训练样本 Notes: 关于Stacking -《机器学习》作者也指出Stacking本身是一种著名的集成方法，且有不少变体和特例，但他这里是作为一种特殊的结合策略看待 关于Stacking的细节详述，如何在 Kaggle 首战中进入前 10% 以一幅图来说说明5折Stacking的过程 推荐一个Python的实现了Stacking集成的库mlxtend 原作者举了一个5折stacking的例子，基本方法是， 每一折取训练集80%的数据训练一个基模型并对剩下的20%的数据进行预测，同时将该模型对测试集做出预测，保留训练子集的预测结果和测试集的预测结果 将5折的训练子集预测结果结合起来构成第二层元模型的输入特征进行训练得到元分类器 将前面每一折在测试集预测得到的结果取均值作为最终元分类器的预测输入(最终的测试数据)，并使用训练好的元分类器在该数据上作出最终预测 此外知乎上的一篇文章还提到 可以将K个模型对Test Data的预测结果求平均，也可以用所有的Train Data重新训练一个新模型来预测Test Data 6. 多样性 6.1 误差-分歧分解 集成学习“好而不同”的理论分析，见《机器学习》P185~186 寻找 集成泛化误差、个体学习器泛化误差、个体学习器间 的分歧三者之间的关系 6.2 多样性度量 多样性度量是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度，典型做法是考虑个体分类器的两两相似/不相似性 度量方法 {不合度量、相关系数、Q-统计量、k-统计量} Notes: 目前我还没有做过这种类似测试 😢 6.3 多样性增强 如何增强多样性？— 在学习过程中引入随机性 数据样本扰动 输入属性扰动 输出表示扰动 算法参数扰动 数据样本扰动 给定初始数据集，可从中产生不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，通常基于采样法 输入属性扰动 从初始属性集中抽取若干个属性子集、基于每个属性子集训练一个基学习器（如随机子空间算法），最后结合 算法参数扰动 通常可以通过随机设置不同的参数，从而产生差别较大的个体学习器。 Notes: 数据样本扰动中相对的稳定基学习器包括：线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等 输入属性扰动 : 感觉典型的是 Random Forest 就做了这件事. 对于算法参数扰动，与交叉验证做比较，交叉验证常常是在不同参数组合模型里选择最优参数组合模型，而集成则是将这些不同参数组合的模型结合起来，所以集成学习技术的实际计算开销并不比使用单一学习器大很多 Reference article 周志华《机器学习》 kaggle-ensembling-guide Bagging, Boosting &amp; Stacking stackexchange及评论区 如何在 Kaggle 首战中进入前 10% 分分钟带你杀入Kaggle Top 1% 懒死骆驼 机器学习技法","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"Ensumble","slug":"Ensumble","permalink":"http://www.iequa.com/tags/Ensumble/"}]},{"title":"Decision Tree part2","slug":"ml/decisionTree-part2","date":"2018-06-27T08:43:21.000Z","updated":"2021-06-22T06:40:40.541Z","comments":true,"path":"2018/06/27/ml/decisionTree-part2/","link":"","permalink":"http://www.iequa.com/2018/06/27/ml/decisionTree-part2/","excerpt":"","text":"1. 决策树模型 决策树（decision tree）是一个树结构（可以是二叉树或非二叉树） 关于 ID3 与 C4.5 的具体计算流程示例，请参见 desicion tree (part1) 1.1. 定义 分类决策树模型是一种描述对实例进行分类的树形结构，其中包含两种类型的节点 内部节点: 表示一个 feature（属性） 叶节点: 表示一个 class 一般决策树可以根据以下四个方面划分归类 : 分支数 划分策略 终止策略 基分类器 1.2. if-then规则集合 一条由根节点到叶节点的路径 –&gt; 一条规则 路径上内部节点的特征 –&gt; 规则的条件 叶节点的类 –&gt; 规则的结论 class 性质：互斥且完备 1.3. 条件概率分布 给定 feature 条件下 class 的条件概率分布 2. 决策树的学习 决策树学习本质是从 train datasets 中归纳出一组分类规则，另一个角度，学习是由 train datasets 估计条件概率模型 2.1 目的 得到一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力 2.2 策略 以损失函数（通常为正则化的极大似然函数）为目标函数的最小化，并在损失函数确定后，选择最优决策树 2.3 学习算法 理论上：从所有可能的决策树中选取最优决策树，NP完全问题 实际中：采用启发式方法，近似求解（得到次最优决策树）–&gt; 递归的选择最优特征，并根据该最优特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。 主要步骤： (1). 特征选择 (2). 决策树的生成 (3). 决策树的剪枝 Notes: 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。 3. 特征选择 实质 ： 选取对于训练数据具有分类能力的特征（决定用哪个 feature 来划分 feature space） 常用准则 Algorithm Feature 选择方法 Author ID3 Information gain Quinlan. 1986 C4.5 Gain ratio Quinlan. 1993. CART | 回归树： 最小二乘分类树： 基尼指数 Gini index | Breiman. 1984(Classification and Regression Tree 分类与回归树) 3.1 信息增益 Information gain g(D,A)=H(D)−H(D∣A)g(D, A)=H(D)-H(D|A) g(D,A)=H(D)−H(D∣A) g(D,A)g(D, A)g(D,A)即信息增益，表示得知特征AAA的信息而使得类DDD的信息的不确定性减少的程度 H(D)H(D)H(D) 为集合 DDD 的信息熵 其中假设 DDD 是一个取有限个值的离散随机变量，概率分布为 P(X=xi)=pi,i=1,2,…,nP(X=x_i)=p_i, i=1, 2,…,nP(X=xi​)=pi​,i=1,2,…,n 熵是表示随机变量不确定性的度量，定义 H(D)=−∑i=1npilogpiH(D)=- \\sum_{i=1}^n p_ilogp_iH(D)=−∑i=1n​pi​logpi​，熵越大，随机变量的不确定性就越大，0≤H(D)≤logn0 \\leq H(D) \\leq logn0≤H(D)≤logn H(D∣A)H(D|A)H(D∣A) 经验条件熵在已知随机变量AAA（特征）的条件下随机变量 DDD 的不确定性H(D∣A)=∑i=1npiH(D∣A=ai)H(D|A)= \\sum_{i=1}^{n}p_iH(D|A=a_i)H(D∣A)=∑i=1n​pi​H(D∣A=ai​) 一般将熵 H(D)H(D)H(D) 与条件熵 H(D∣A)H(D|A)H(D∣A) 之差称为互信息，决策树学习中的信息增益等价于类与特征的互信息 小结: 给定训练数据集 DDD 和特征 AAA，经验熵 H(D)H(D)H(D) 表示对数据集 DDD 进行分类的不确定性，而经验条件熵 H(D∣A)H(D|A)H(D∣A) 表示在特征 AAA 给定的条件下对数据集进行分类的不确定性，因此两者之差即信息增益表示由于特征 AAA 而使得数据集 DDD 的分类的不确定性减少的程度. 对于数据集 DDD 而言，信息增益依赖于特征，不同特征具有不同的信息增益，信息增益大的特征具有更强的分类能力（也就是我们需要挑选的目标） 3.2 信息增益比 Gain ratio 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以对这问题进行校正. 特征AAA对训练数据集DDD的信息增益比gR(D,A)g_R(D,A)gR​(D,A)定义为其信息增益g(D,A)g(D,A)g(D,A)与训练数据集DDD关于特征AAA的值的熵HA(D)H_A(D)HA​(D)之比： gR(D,A)=g(D,A)HA(D)g_R(D,A)= \\frac{g(D,A)}{H_A(D)} gR​(D,A)=HA​(D)g(D,A)​ 其中 H_A(D)=−∑_i=1n∣D_i∣∣D∣log_2∣D_i∣∣D∣H\\_A(D)=- \\sum\\_{i=1}^{n} \\frac{|D\\_{i}|}{|D|}log\\_2 \\frac{|D\\_{i}|}{|D|} H_A(D)=−∑_i=1n∣D∣∣D_i∣​log_2∣D∣∣D_i∣​ nnn 为特征 AAA 的取值个数，DiD_iDi​ 表示据特征 AAA 的取值将 DDD 分成的子集 4. 决策树的生成 4.1 ID3 核心思想: 在决策树的各个结点上应用信息增益准则选择特征，递归地构建决策树。 递归终止条件：所有特征的信息增益（设置信息增益的阈值来判断是否进一步划分）均很小或没特征可选为止. （每选一个特征则后期划分子树不再使用前面用过的特征，因为子树已经是在该特征下属于同一取值的实例集合） 决策树的生成: 输入：训练数据集 DDD 和特征集 AAA，阈值 εεε； 输出：决策树 TTT (1) （叶子结点）若 DDD 中所有实例属于同一类 C_kC\\_kC_k, 则 TTT 为单节点树，并将类C_kC\\_kC_k 作为该结点的类标记，返回 TTT (2) （终止条件之没有特征可供选择） 若 A=∅A=∅A=∅, 则 TTT为单节点树，并将 DDD 中实例数最大的类 C_kC\\_kC_k 作为该结点的 class 标记（多数表决规则），返回 TTT (3) （计算 AAA 中各特征对 DDD 的信息增益，选择信息增益最大的特征 A_gA\\_gA_g (4) （终止条件之阈值) 若 A_gA\\_gA_g 的信息增益小于阈值 εεε，则置 TTT 为单节点树，并将 DDD 中实例数最大的类 C_kC\\_kC_k 作为该结点的类标记，返回 TTT (5) 否则，对 A_gA\\_gA_g 的每一可能值 a_ia\\_ia_i，依 A_g=a_iA\\_g=a\\_iA_g=a_i 将 DDD 划分为若干非空子集 D_iD\\_iD_i，并将 D_iD\\_iD_i 中实例数最大的类作为标记构建子节点，返回 T_iT\\_iT_i (6) 对第 iii 个子节点，以 D_iD\\_iD_i 为训练集，A−A_gA − {A\\_g}A−A_g 为特征集，递归地调用(1)~(5)得到子树 T_iT\\_iT_i 并返回. 4.2 C4.5 C4.5算法与ID3算法类似，不同之处在于，C4.5在生成的过程中，用Gain ratio来选择特征。 Notes: 上述决策树的生成算法只有树的生成，而且是针对训练集构造的树，容易产生过拟合。 5. CART(classification and regression tree) CART 假设决策树是二叉树，而 ID3,C4.5 生成的过程中并无此假设，这也导致了两者的根本不同，ID3,C4.5 每次选择出最佳特征之后，是按照该特征的每一个取值划分子树； CART 则是对每一个特征、每一个特征的每一个取值计算基尼指数（分类树）然后从所有特征对应的取值计算所得的基尼指数中最小的特征及特征值作为切分点来划分子树，并在这些子空间上确定预测的概率分布，也就是在输入给定的条件下输出对应的条件概率分布. 5.1 CART 纯度度量 CART 中用于选择变量的不纯性度量是 Gini index；如果目标变量是标称的，并且是具有两个以上的类别，则 CART 可能考虑将目标类别合并成两个超类别（双化）；如果目标变量是连续的，则 CART 找出一组基于树的回归方程来预测目标变量。 Algorithm Feature Selection Author CART 回归树： 最小二乘分类树： 基尼指数 Gini index Breiman. 1984(Classification and Regression Tree 分类与回归树) 5.2 CART 步骤 build decision tree时通常采用自上而下的方法，在每一步选择一个最好的属性来分裂。 “最好” 的定义是使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义&quot;最好&quot;。 CART 是在给定输入随机变量 XXX 条件下求得输出随机变量 YYY 的条件概率分布的学习方法。 可以看出CART算法在叶节点表示上不同于ID3、C4.5方法，后二者叶节点对应数据子集通过“多数表决”的方式来确定一个类别（固定一个值）；而CART算法的叶节点对应类别的概率分布。如此看来，我们可以很容易地用 CART 来学习一个 multi-label 的分类任务。 CART 算法也主要由两步组成: 决策树的生成：基于训练数据集生成一棵二分决策树； 决策树的剪枝：用验证集对已生成的二叉决策树进行剪枝，剪枝的标准为损失函数最小化。 由于分类树与回归树在递归地构建二叉决策树的过程中，选择特征划分的准则不同。 二叉分类树构建过程中采用基尼指数（Gini Index）为特征选择标准； 二叉回归树采用平方误差最小化作为特征选择标准。 5.3 树的构建 5.3.1 二叉回归树 设 XXX, YYY 分别为输入和输出变量，其中 YYY 为连续变量，给定训练数据集 D={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}D= \\lbrace (x\\_1, y\\_1), (x\\_2, y\\_2),…,(x\\_N, y\\_N) \\rbraceD={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)} 决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树 一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值，所以我们的主要目的是要构建回归树，也就是如何划分输入空间，因为一旦划分好输入空间，如将输入空间划分为 MMM 个单元, R_1,R_2,…,R_MR\\_1, R\\_2, … , R\\_MR_1,R_2,…,R_M , 并且在每个单元 R_mR\\_mR_m 上有一个固定的输出值 c_mc\\_mc_m，那么回归树的模型就可以表示为 f(x)=∑_m=1Mc_mI(x∈R_m)f(x)=\\sum\\_{m=1}^Mc\\_mI(x \\in R\\_m) f(x)=∑_m=1Mc_mI(x∈R_m) 概念强调 首先要强调的是 CART假设决策树是二叉树，内部结点特征的取值只有“是”和“否”，左分支是取值为“是”的分支，有分支则相反。这样的决策树等价于递归地二分每个特征. 最小二叉回归树生成算法 输入：训练数据集DDD； 输出：回归树 f(x)f(x)f(x) 算法：在训练集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上输出值，构建二叉决策树 (1). 择最优切分变量jjj与切分点sss，求解 min⁡_j,s[min⁡_c_1∑_x_i∈R_1(j,s)(y_i−c_1)2+min⁡_c_2∑_x_i∈R_2(j,s)(y_i−c_2)2]\\min\\_{j,s}[\\min\\_{c\\_1} \\sum\\_{x\\_i\\in R\\_1(j,s)}(y\\_i-c\\_1)^2 + \\min\\_{c\\_2} \\sum\\_{x\\_i\\in R\\_2(j,s)}(y\\_i-c\\_2)^2] min_j,s[min_c_1∑_x_i∈R_1(j,s)(y_i−c_1)2+min_c_2∑_x_i∈R_2(j,s)(y_i−c_2)2] 遍历变量jjj，对固定的切分变量jjj扫描切分点sss，选择使上式最小值的对(j,s)(j,s)(j,s)。其中R_mR\\_mR_m是被划分的输入空间，c_mc\\_mc_m是空间R_mR\\_mR_m对应的固定输出值。 (2). 用选定的对(j,s)(j,s)(j,s)划分区域并决定相应的输出值： R1(j,s)={x∣x(j)≤s},R2(j,s)={x∣x(j)&gt;s}R_1(j,s)=\\lbrace x\\mid x^{(j)} \\le s \\rbrace , \\quad R_2(j,s)=\\lbrace x \\mid x^{(j)} \\gt s \\rbrace R1​(j,s)={x∣x(j)≤s},R2​(j,s)={x∣x(j)&gt;s} c^_m=1N_m∑_x_i∈R_m(j,s)y_i,x∈R_m,m=1,2 \\hat c\\_m = {1\\over N\\_m}\\sum\\_{x\\_i\\in R\\_m(j,s)}y\\_i , \\quad x\\in R\\_m , m=1,2 c^_m=N_m1​∑_x_i∈R_m(j,s)y_i,x∈R_m,m=1,2 (3). 继续对两个子区域调用步骤（1），（2），直至满足停止条件。 (4). 将输入空间划分为MMM个区域R_1R\\_1R_1,R_2R\\_2R_2 , … , R_MR\\_MR_M，生成决策树： f(x)=∑_m=1Mc^_mI(x∈R)f(x) = \\sum\\_{m=1}^M\\hat c\\_m I(x\\in R) f(x)=∑_m=1Mc^_mI(x∈R) 举个🌰🌰🌰: 训练数据见下表，xxx 的取值范围为区间 [0.5,10.5][0.5,10.5][0.5,10.5], yyy 的取值范围为区间[5.0,10.0][5.0,10.0][5.0,10.0] ,学习这个回归问题的最小二叉回归树。 x_ix\\_ix_i | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 ---- | — | — | — | — | — | — | — | — | — y_iy\\_iy_i | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 首先来看这个优化问题 min⁡_j,s[min⁡_c_1∑_x_i∈R_1(j,s)(y_i−c_1)2+min⁡_c_2∑_x_i∈R_2(j,s)(y_i−c_2)2]\\min\\_{j,s}[\\min\\_{c\\_1} \\sum\\_{x\\_i\\in R\\_1(j,s)}(y\\_i-c\\_1)^2 + \\min\\_{c\\_2} \\sum\\_{x\\_i\\in R\\_2(j,s)}(y\\_i-c\\_2)^2] min_j,s[min_c_1∑_x_i∈R_1(j,s)(y_i−c_1)2+min_c_2∑_x_i∈R_2(j,s)(y_i−c_2)2] 求解训练数据的切分点s: R1(j,s)={x∣x(j)≤s},R2(j,s)={x∣x(j)&gt;s}R_1(j,s)=\\lbrace x\\mid x^{(j)} \\le s \\rbrace , \\quad R_2(j,s)=\\lbrace x \\mid x^{(j)} \\gt s \\rbrace R1​(j,s)={x∣x(j)≤s},R2​(j,s)={x∣x(j)&gt;s} 容易求得在 R_1R\\_1R_1,R_2R\\_2R_2 内部使得平方损失误差达到最小值的 c_1c\\_1c_1,c_2c\\_2c_2 为： c_1=1N_1∑_x_i∈R_1y_i,c_2=1N_2∑_x_i∈R_2y_ic\\_1={1\\over N\\_1}\\sum\\_{x\\_i \\in R\\_1}y\\_i , \\quad c\\_2={1\\over N\\_2}\\sum\\_{x\\_i \\in R\\_2}y\\_i c_1=N_11​∑_x_i∈R_1y_i,c_2=N_21​∑_x_i∈R_2y_i 这里 N_1N\\_1N_1,N_2N\\_2N_2 是 R_1R\\_1R_1,R_2R\\_2R_2的样本点数。 求训练数据的切分点，根据所给数据，考虑如下切分点： 1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.51.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5 1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5 对各切分点，不难求出相应的 R_1R\\_1R_1 , R_2R\\_2R_2 , c_1c\\_1c_1 , c_2c\\_2c_2 及 m(s)=min⁡_j,s[min⁡_c_1∑_x_i∈R_1(j,s)(y_i−c_1)2+min⁡_c_2∑_x_i∈R_2(j,s)(y_i−c_2)2]m(s)=\\min\\_{j,s}[\\min\\_{c\\_1} \\sum\\_{x\\_i\\in R\\_1(j,s)}(y\\_i-c\\_1)^2 + \\min\\_{c\\_2} \\sum\\_{x\\_i\\in R\\_2(j,s)}(y\\_i-c\\_2)^2] m(s)=min_j,s[min_c_1∑_x_i∈R_1(j,s)(y_i−c_1)2+min_c_2∑_x_i∈R_2(j,s)(y_i−c_2)2] 例如，当 s=1.5s=1.5s=1.5 时，R_1={1}R\\_1 = \\lbrace 1\\rbraceR_1={1}, R_2={2,3,…,10}R\\_2 = \\lbrace 2, 3 , \\ldots , 10\\rbraceR_2={2,3,…,10}, c_1=5.56c\\_1 = 5.56c_1=5.56, c_2=7.50c\\_2 = 7.50c_2=7.50, m(s)=min⁡_j,s[min⁡_c_1∑_x_i∈R_1(j,s)(y_i−c_1)2+min⁡_c_2∑_x_i∈R_2(j,s)(y_i−c_2)2]=0+15.72=15.72m(s)=\\min\\_{j,s}[\\min\\_{c\\_1} \\sum\\_{x\\_i\\in R\\_1(j,s)}(y\\_i-c\\_1)^2 + \\min\\_{c\\_2} \\sum\\_{x\\_i\\in R\\_2(j,s)}(y\\_i-c\\_2)^2] = 0+15.72 = 15.72 m(s)=min_j,s[min_c_1∑_x_i∈R_1(j,s)(y_i−c_1)2+min_c_2∑_x_i∈R_2(j,s)(y_i−c_2)2]=0+15.72=15.72 现将sss及m(s)m(s)m(s)的计算结果列表如下： s 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 m(s) 15.72 12.07 8.36 5.78 3.91 1.93 8.01 11.73 15.74 由上表可知，当x=6.5x=6.5x=6.5的时候达到最小值，此时 R_1={1,2,…,6}R\\_1 = \\lbrace 1 ,2 , \\ldots , 6\\rbraceR_1={1,2,…,6}, R_2=7,8,9,10R\\_2 ={7 ,8 ,9 , 10}R_2=7,8,9,10, c_1=6.24c\\_1 = 6.24c_1=6.24, c_2=8.9c\\_2 = 8.9c_2=8.9 所以回归树 T_1(x)T\\_1(x)T_1(x) 为： T_1(x)={6.24,x&lt;6.58.91,x≥6.5T\\_1(x) = \\begin{cases} 6.24, &amp; x\\lt 6.5 \\\\\\\\ 8.91, &amp; x \\ge 6.5 \\\\\\\\ \\end{cases} T_1(x)=⎩⎪⎪⎪⎨⎪⎪⎪⎧​6.24,8.91,​x&lt;6.5x≥6.5​ f_1(x)=T_1(x)f\\_1(x) = T\\_1(x) f_1(x)=T_1(x) 用 f_1(x)f\\_1(x)f_1(x) 拟合训练数据的残差见下表，表中 r_2i=y_i−f_1(x_i),i=1,2,…,10r\\_{2i} = y\\_i - f\\_1(x\\_i),i=1,2,\\ldots , 10r_2i=y_i−f_1(x_i),i=1,2,…,10 x_ix\\_ix_i | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 ---- | — | — | — | — | — | — | — | — | — y_iy\\_iy_i | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 用f_1(x)f\\_1(x)f_1(x)拟合训练数据的平方误差： L(y,f_1(x))=∑_i=110(y_i−f_1(x_i))2=1.93L(y,f\\_1(x)) = \\sum\\_{i=1}^{10}(y\\_i-f\\_1(x\\_i))^2 = 1.93 L(y,f_1(x))=∑_i=110(y_i−f_1(x_i))2=1.93 第2步求 T_2(x)T\\_2(x)T_2(x).方法与求 T_1(x)T\\_1(x)T_1(x) 一样，只是拟合的数据是上表的残差，可以得到： T_2(x)={−0.52,x&lt;3.50.22,x≥3.5T\\_2(x) = \\begin{cases} -0.52, &amp; x\\lt 3.5 \\\\\\\\ 0.22, &amp; x \\ge 3.5 \\\\\\\\ \\end{cases} T_2(x)=⎩⎪⎪⎪⎨⎪⎪⎪⎧​−0.52,0.22,​x&lt;3.5x≥3.5​ f_2(x)=f_1(x)+T_2(x)={5.72,x&lt;3.56.46,3.5≤x&lt;6.59.13,x≥6.5f\\_2(x) = f\\_1(x) + T\\_2(x)= \\begin{cases} 5.72, &amp; x\\lt 3.5 \\\\\\\\ 6.46, &amp; 3.5\\le x \\lt 6.5 \\\\\\\\ 9.13, &amp; x\\ge 6.5 \\\\\\\\ \\end{cases} f_2(x)=f_1(x)+T_2(x)=⎩⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎧​5.72,6.46,9.13,​x&lt;3.53.5≤x&lt;6.5x≥6.5​ 用f_2(x)f\\_2(x)f_2(x)拟合训练数据的平方误差是： L(y,f_2(x))=∑_i=110(y_i−f_2(x_i))2=0.79L(y,f\\_2(x)) = \\sum\\_{i=1}^{10}(y\\_i-f\\_2(x\\_i))^2 = 0.79 L(y,f_2(x))=∑_i=110(y_i−f_2(x_i))2=0.79 继续求得 T_3(x)={0.15,x&lt;6.5−0.22,x≥6.5L(y,f_3(x))=0.47,T\\_3(x) = \\begin{cases} 0.15, &amp; x\\lt 6.5 \\\\\\\\ -0.22, &amp; x \\ge 6.5 \\\\\\\\ \\end{cases} \\quad L(y,f\\_3(x)) = 0.47 , T_3(x)=⎩⎪⎪⎪⎨⎪⎪⎪⎧​0.15,−0.22,​x&lt;6.5x≥6.5​L(y,f_3(x))=0.47, T_4(x)={−0.16,x&lt;4.50.11,x≥4.5L(y,f_3(x))=0.30,T\\_4(x) = \\begin{cases} -0.16, &amp; x\\lt 4.5 \\\\\\\\ 0.11, &amp; x \\ge 4.5 \\\\\\\\ \\end{cases} \\quad L(y,f\\_3(x)) = 0.30 , T_4(x)=⎩⎪⎪⎪⎨⎪⎪⎪⎧​−0.16,0.11,​x&lt;4.5x≥4.5​L(y,f_3(x))=0.30, T_5(x)={0.07,x&lt;6.5−0.11,x≥6.5L(y,f_3(x))=0.23,T\\_5(x) = \\begin{cases} 0.07, &amp; x\\lt 6.5 \\\\\\\\ -0.11, &amp; x \\ge 6.5 \\\\\\\\ \\end{cases} \\quad L(y,f\\_3(x)) = 0.23 , T_5(x)=⎩⎪⎪⎪⎨⎪⎪⎪⎧​0.07,−0.11,​x&lt;6.5x≥6.5​L(y,f_3(x))=0.23, T_6(x)={−0.15,x&lt;2.50.04,x≥2.5T\\_6(x) = \\begin{cases} -0.15, &amp; x\\lt 2.5 \\\\ 0.04, &amp; x \\ge 2.5 \\\\ \\end{cases} T_6(x)={−0.15,0.04,​x&lt;2.5x≥2.5​ f_6(x)=f_5(x)+T_6(x)=T_1(x)+…+T_5(x)+T_6(x)={5.63,x&lt;2.55.82,2.5≤x&lt;3.56.56,3.5≤x&lt;4.56.83,4.5≤x&lt;6.58.95,x≥6.5f\\_6(x) = f\\_5(x)+T\\_6(x) =T\\_1(x)+ \\ldots + T\\_5(x) + T\\_6(x)= \\begin{cases} 5.63, &amp; x\\lt 2.5 \\\\\\\\ 5.82, &amp; 2.5 \\le x\\lt 3.5 \\\\\\\\ 6.56, &amp; 3.5 \\le x\\lt 4.5 \\\\\\\\ 6.83, &amp; 4.5 \\le x\\lt 6.5 \\\\\\\\ 8.95, &amp; x\\ge 6.5 \\\\\\\\ \\end{cases} f_6(x)=f_5(x)+T_6(x)=T_1(x)+…+T_5(x)+T_6(x)=⎩⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎧​5.63,5.82,6.56,6.83,8.95,​x&lt;2.52.5≤x&lt;3.53.5≤x&lt;4.54.5≤x&lt;6.5x≥6.5​ 用f_6(x)f\\_6(x)f_6(x)拟合训练数据的平方损失误差是 L(y,f_6(x))=∑_i=110(y_i−f_6(x_i))2=0.71L(y,f\\_6(x)) = \\sum\\_{i=1}^{10}(y\\_i-f\\_6(x\\_i))^2 = 0.71 L(y,f_6(x))=∑_i=110(y_i−f_6(x_i))2=0.71 假设此时已经满足误差要求，那么 f(x)=f_6(x)f(x) = f\\_6(x)f(x)=f_6(x) 即为所求的回归树。 5.3.2 二叉分类树 二叉分类树中用基尼指数（Gini Index）作为最优特征选择的度量标准。基尼指数定义如下： Gini Index : 是一种不等性度量； 通常用来度量收入不平衡，可以用来度量任何不均匀分布； 是介于0~1之间的数，0-完全相等，1-完全不相等； 总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似） Gini Index : 同样以分类系统为例，数据集 DDD 中类别 CCC 可能的取值为c_1,c_2,⋯ ,c_kc\\_1, c\\_2, \\cdots, c\\_kc_1,c_2,⋯,c_k （kkk是类别数），一个样本属于类别 c_ic\\_ic_i 的概率为p(i)p(i)p(i)。那么概率分布的 Gini index 公式表示为： Gini(D)=1−∑i=1kp_i2(fmt.2.1.1)Gini(D) = 1 - \\sum_{i=1}^{k} {p\\_i}^2 \\qquad(fmt.2.1.1) Gini(D)=1−i=1∑k​p_i2(fmt.2.1.1) 其中p_i=类别属于c_i的样本数总样本数p\\_i = \\frac{类别属于c\\_i的样本数}{总样本数}p_i=总样本数类别属于c_i的样本数​。如果所有的样本 Category 相同，则 p1=1,p2=p3=⋯=pk=0p_1 = 1, p_2 = p_3 = \\cdots = p_k = 0p1​=1,p2​=p3​=⋯=pk​=0，则有p1=1,p2=p3=⋯=pk=0p_1 = 1, p_2 = p_3 = \\cdots = p_k = 0p1​=1,p2​=p3​=⋯=pk​=0，此时数据不纯度最低。Gini(D)Gini(D)Gini(D) 的物理含义是表示数据集 DDD 的不确定性。数值越大，表明其不确定性越大（这一点与 Info Entropy 相似）。 如果 k=2k=2k=2（二分类问题，类别命名为正类和负类），若样本属于正类的概率是 ppp，那么对应基尼指数为： \\begin{align} Gini(D) &amp; = 1 - [p^2 + {(1-p)}^2] \\\\\\ &amp; = \\underline {2p (1-p)} \\qquad\\qquad (fmt.2.1.2) \\end{align} 如果数据集 DDD 根据特征 fff 是否取某一可能值 f_∗f\\_∗f_∗，将 DDD 划分为 D_1={(x,y)∈D∣f(x)=f_∗},D_2=D−D_1D\\_1=\\{(x, y) \\in D | f(x) = f\\_{\\ast}\\}, D\\_2=D-D\\_1D_1={(x,y)∈D∣f(x)=f_∗},D_2=D−D_1。那么特征 fff 在数据集 DDD 上的 Gini index 定义为： Gini(D,f=f_∗)=∣D1∣∣D∣Gini(D_1)+∣D_2∣∣D∣Gini(D2)(fmt.2.1.3)Gini(D, f=f\\_{\\ast}) = \\frac{\\vert D_1 \\vert}{\\vert D \\vert} Gini(D\\_1) + \\frac{\\vert D\\_2 \\vert}{\\vert D \\vert} Gini(D_2) \\qquad\\qquad (fmt.2.1.3) Gini(D,f=f_∗)=∣D∣∣D1​∣​Gini(D_1)+∣D∣∣D_2∣​Gini(D2​)(fmt.2.1.3) 代表性的例子说明 : ID 阴晴(F) 温度(F) 湿度(F) 刮风(F) 是否玩（C） 1 sunny hot high false 否 2 sunny hot high true 否 3 overcast hot high false 是 4 rainy mild high false 是 5 rainy cool normal false 是 6 rainy cool normal true 否 7 overcast cool normal true 是 8 sunny mild high false 否 9 sunny cool normal false 是 10 rainy mild normal false 是 11 sunny mild normal true 是 12 overcast mild high true 是 13 overcast hot normal false 是 14 rainy mild high true 否 下图是 IG 的决策树，并不是 二分分类树 Gini{Gini}Gini index{index}index 决策树, 放这里仅仅是为了感知一下 &lt;img src=&quot;/images/ml/decision-tree/decision-tree-2.png&quot; width=“530” /img&gt; 在实际操作中，通过遍历所有特征（如果是连续值，需做离散化）及其取值，选择 Mingini−indexMin_{gini-index}Mingini−index​ 所对应的特征和特征值。 这里仍然以天气数据为例，给出特征阴晴的 Gini index 计算过程。 (1). 当特征“阴晴”取值为”sunny”时，D_1={1,2,8,9,11},∣D_1∣=5D\\_1 = \\{1,2,8,9,11\\}, |D\\_1|=5D_1={1,2,8,9,11},∣D_1∣=5；D_2={3,4,5,6,7,10,12,13,14},∣D_2∣=9D\\_2=\\{3,4,5,6,7,10,12,13,14\\}, |D\\_2|=9D_2={3,4,5,6,7,10,12,13,14},∣D_2∣=9. 数据自己对应的类别数分别为 (+2,−3)、(+7,−2)(+2,-3)、(+7,-2)(+2,−3)、(+7,−2)。因此 Gini(D_1)=2⋅35⋅25=1225Gini(D\\_1) = 2 \\cdot \\frac{3}{5} \\cdot \\frac{2}{5} = \\frac{12}{25}Gini(D_1)=2⋅53​⋅52​=2512​；Gini(D2)=2⋅79⋅29=2881Gini(D_2) = 2 \\cdot \\frac{7}{9} \\cdot \\frac{2}{9} = \\frac{28}{81}Gini(D2​)=2⋅97​⋅92​=8128​. 对应的基尼指数为： Gini(C,“阴晴”=”sunny”)=514Gini(D1)+914Gini(D2)=5141225+9142881=0.394(exp.2.2.1)Gini(C, “阴晴”=”sunny”) = \\frac{5}{14} Gini(D_1) + \\frac{9}{14} Gini(D_2) = \\frac{5}{14} \\frac{12}{25} + \\frac{9}{14} \\frac{28}{81} = 0.394 \\quad(exp.2.2.1) Gini(C,“阴晴”=”sunny”)=145​Gini(D1​)+149​Gini(D2​)=145​2512​+149​8128​=0.394(exp.2.2.1) (2). 当特征“阴晴”取值为”overcast”时，D_1={2,7,12,13},∣D_1∣=4D\\_1 = \\{2,7,12,13\\}, |D\\_1|=4D_1={2,7,12,13},∣D_1∣=4；D_2={1,2,4,5,6,8,9,10,11,14},∣D_2∣=10D\\_2=\\{1,2,4,5,6,8,9,10,11,14\\}, |D\\_2|=10D_2={1,2,4,5,6,8,9,10,11,14},∣D_2∣=10。D_1D\\_1D_1、D_2D\\_2D_2 数据自己对应的类别数分别为 (+4,−0)、(+5,−5)(+4,-0)、(+5,-5)(+4,−0)、(+5,−5)。因此 Gini(D_1)=2⋅1⋅0=0；Gini(D_2)=2⋅510⋅510=12Gini(D\\_1) = 2 \\cdot 1 \\cdot 0 = 0；Gini(D\\_2) = 2 \\cdot \\frac{5}{10} \\cdot \\frac{5}{10} = \\frac{1}{2}Gini(D_1)=2⋅1⋅0=0；Gini(D_2)=2⋅105​⋅105​=21​ 对应的基尼指数为： Gini(C,“阴晴”=”overcast”)=414Gini(D1)+1014Gini(D2)=0+1014⋅12=514=0.357(exp.2.2.2)Gini(C, “阴晴”=”overcast”) = \\frac{4}{14} Gini(D_1) + \\frac{10}{14} Gini(D_2) = 0 + \\frac{10}{14} \\cdot \\frac{1}{2} = \\frac{5}{14} = 0.357 \\quad(exp.2.2.2) Gini(C,“阴晴”=”overcast”)=144​Gini(D1​)+1410​Gini(D2​)=0+1410​⋅21​=145​=0.357(exp.2.2.2) (3). 当特征“阴晴”取值为”rainy”时，D_1={4,5,6,10,14},∣D_1∣=5D\\_1 = \\{4,5,6,10,14\\}, |D\\_1|=5D_1={4,5,6,10,14},∣D_1∣=5; D2={1,2,3,7,8,9,11,12,13},∣D2∣=9D_2=\\{1,2,3,7,8,9,11,12,13\\}, |D_2|=9D2​={1,2,3,7,8,9,11,12,13},∣D2​∣=9。 D_1D\\_1D_1、D_2D\\_2D_2 数据自己对应的类别数分别为 (+3,−2)、(+6,−3)(+3,−2)、(+6,−3)(+3,−2)、(+6,−3)。因此 Gini(D_1)=2⋅35⋅25=1225Gini(D\\_1) = 2 \\cdot \\frac{3}{5} \\cdot \\frac{2}{5} = \\frac{12}{25}Gini(D_1)=2⋅53​⋅52​=2512​；Gini(D_2)=2⋅69⋅39=49Gini(D\\_2) = 2 \\cdot \\frac{6}{9} \\cdot \\frac{3}{9} = \\frac{4}{9}Gini(D_2)=2⋅96​⋅93​=94​。 对应的基尼指数为： Gini(C,“阴晴”=”rainy”)=514Gini(D1)+914Gini(D2)=5141225+91449=47=0.457(exp.2.2.3)Gini(C, “阴晴”=”rainy”) = \\frac{5}{14} Gini(D_1) + \\frac{9}{14} Gini(D_2) = \\frac{5}{14} \\frac{12}{25} + \\frac{9}{14} \\frac{4}{9} = \\frac{4}{7} = 0.457 \\quad(exp.2.2.3) Gini(C,“阴晴”=”rainy”)=145​Gini(D1​)+149​Gini(D2​)=145​2512​+149​94​=74​=0.457(exp.2.2.3) 如果特征”阴晴”是最优特征的话，那么特征取值为”overcast”应作为划分节点。 6. 决策树模型的优缺点 6.1 优点 可解释性–模拟人类决策过程 训练、预测效率较高–关于其切分方式，每次是在一个条件下的局部空间划分样本，而类似Adaboost则是每次在整个空间划分样本，所以就决策树而言相对高效 适用于类别类型数据–decision set(穷举类别特征值然后按照特征值的子集集合来划分样本) 能够很方便的由二分类模型转换为多分类模型–主要修改不纯度计算以及返回值的设置 能够处理缺失特征值–用其他的特征值来替代进行划分(一般要求替代特征划分结果接近缺失特征值) 易于实现 6.2 缺点 经验多于理论，大多数决策树模型是根据经验来判断的，效果好坏尚无较好的理论支撑 Reference article 逗比算法工程师、算法杂货铺 52caml、懒死骆驼 决策树ID3、C4.5、CART算法：信息熵，区别，剪枝理论总结 Markdown 学习好教材、CART之回归树构建 《机器学习导论》《统计学习方法》","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"decision-tree","slug":"decision-tree","permalink":"http://www.iequa.com/tags/decision-tree/"}]},{"title":"Support Vecor Machine (六部曲)","slug":"ml/svm-hanxiaoyang","date":"2018-06-20T08:08:21.000Z","updated":"2021-06-20T04:12:28.313Z","comments":true,"path":"2018/06/20/ml/svm-hanxiaoyang/","link":"","permalink":"http://www.iequa.com/2018/06/20/ml/svm-hanxiaoyang/","excerpt":"Support Vecor Machine, 自一诞生便由于它良好的分类性能席卷了机器学习领域，并牢牢压制了神经网络领域好多年。 如果不考虑集成学习的算法，不考虑特定的训练数据集，在分类算法中的表现SVM说是排第一估计是没有什么异议的.","text":"Support Vecor Machine, 自一诞生便由于它良好的分类性能席卷了机器学习领域，并牢牢压制了神经网络领域好多年。 如果不考虑集成学习的算法，不考虑特定的训练数据集，在分类算法中的表现SVM说是排第一估计是没有什么异议的. 1. SVM 间隔 Margin 支持向量机（SVM）的目标是什么？ 什么是分离超平面， Margin 详情可参 : 机器学习系列(13)_SVM碎碎念part1：间隔 认识一下SVM中很重要的一个概念：Margin，也就是间隔。 2. SVM 向量与空间距离 从向量到距离计算 (向量定义、计算方向向量、向量的和与差、向量内积、向量正交投影) SVM的超平面 (1 计算点到超平面距离、2 计算超平面的间隔) 详情可参 : 机器学习系列(14)_SVM碎碎念part2：SVM中的向量与空间距离 回顾了一下向量中的一些概念，依用向量的知识，怎么帮助我们去计算超平面间隔，有兴趣的同学请接着看part3 $ w^{T}X = 0 $， w 3. SVM 如何找到最优分离超平面 如何找到最优超平面 如何计算两超平面间的距离 SVM的最优化问题是什么 找到两个平行超平面，可以划分数据并且两平面之间没有数据点 两个超平面之间的距离最大化 详情可参 : 机器学习系列(15)_SVM碎碎念part3：如何找到最优分离超平面 4. SVM 无约束最小化问题 详情可参 : 机器学习系列(21)_SVM碎碎念part4：无约束最小化问题 5. SVM 凸函数与优化 详情可参 : 机器学习系列(22)_SVM碎碎念part5：凸函数与优化 6. SVM 对偶和拉格朗日乘子 详情可参 : 机器学习系列(23)_SVM碎碎念part6：对偶和拉格朗日乘子 Reference article","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"互联网金融风控中的数据科学 (part3) ： Lending Club 的数据试验","slug":"datascience/internet-finance-3","date":"2018-04-23T05:28:21.000Z","updated":"2021-06-20T04:12:28.341Z","comments":true,"path":"2018/04/23/datascience/internet-finance-3/","link":"","permalink":"http://www.iequa.com/2018/04/23/datascience/internet-finance-3/","excerpt":"用户全流程欺诈⻛险评分体系","text":"用户全流程欺诈⻛险评分体系 反欺诈是一种机器学习过程 对于做互联网金融一般情况是 正负样本 是极度不平衡的(最高可以达到 100 : 1), 这样的情况对于 SVM 这种分类器是不合适的，所以在做金融的评分卡模型 或 欺诈模型 也好，这样对特征的处理 和 样本的非平衡处理是比较高的. 好坏用户的定义，一般是根据用户的贷后表现，来定义好坏用户的. 举个栗子🌰 : 用户借款 5W 元，可能是分期还款 12个月，这样每个月都会还一笔固定的额度. 信用风险 : 在挺长的时间可以按时还款. 欺诈风险 : 用户可能 第 1、2 期 是还的，之后是不还的. (因为中介也越来越聪明，给他自己留出时间，躲避催收的手段，也躲避追踪等等) 贷前审核 （触碰到拒贷规则） 造假行为 （信息资料造假） 调查员 调查出来是 中介 或者 有欺诈风向的，进入黑名单的. … 所以我们在定义模型负样本的话，我们可能定义为 m1+ 信用风险、m3+ (90天以上不还款的话)，我们可以定义为欺诈风险 坏用户 ： 欺诈风险用户 好用户 ： 一天都不逾期还款 灰用户 ： m1+ 未还款，但是90天之内可以还款的 (不放在训练中，否则会给模型带来很多额外的信息，影响效果) 灰用户不放在模型中，这样训练出的模型对好坏用户的区分程度也越高. 金融模型 和 CTR预估 相比是 有一个周期性质的 广告点击的话，用户点击，立马有一个样本出现 做长期现金贷，选择样本是选择半年之前的用户，作为样本 正负样本 真实场景正负样本比例 (10~30) ：1 （成熟平台的风险是越来越小的，所以我们拿到的 正负样本比例是逐步增加的）. 数据的不平衡处理 ： 降采样、过采样、SMOTE 降采样 ： 正负样本成 3:1， 5:1 来做一个模型, 坏样本是全部取的, (一般这种情况 做评分卡的时候是需要做的) 过采样 ： 实际用的不多，如果负样本实在是过少 都 &lt;100 个， 那么可以考虑 减低我们的观察周期， 或者 欺诈定义的并不是一个很严格 来放进来多一些的 负样本过来来做训练，或者在拒贷里面找一些人过来. SMOTE ： 在分布上模拟一些数据，模拟完的数据可做训练，比较经典拿真实的数据做训练是更贴近真实的情况. 做模型 如 GBRT、RF 等，他们对不平衡的数据是有容忍的，这种直接用真实数据进行训练，也能得到很好的效果. 模型选型 对于做评分卡的模型 或者 LR 的话，样本的平衡要在 10:1 范围内， LR 对变量相关性的筛选 和 数据平衡 有要求 做模型，至少要用 RF 来做模型， 或者 GBDT、GBRT，这种 Boosting 的模型，对于样本的不平衡容忍度更高一些，他们对于学习 更小而细微 的特征和变量 可以学习的更深一些. Lending Club Lending Club 创立于 2016年， 主要做一个提供 P2P 贷款的平台中介服务，2016年底在 纽交所上市，后来爆出来很多丑闻，创始人离职，股价下跌. 但是不管怎么样，它的数据在我们做反欺诈等是非常有重要的. Lending Club 2016 的借贷数据，Q3，Q4 可以一起做一下，半年的数据做训练是更好的. 1. Data Lending Club 2016年Q3数据：https://www.lendingclub.com/info/download-data.action 参考：http://kldavenport.com/lending-club-data-analysis-revisted-with-python/ 看下数据，其实我也不能完全了解这些所有字段的含义 int_rate 利率 term 待多少期 grade 等级 C、B、D，7个等级吧 sub_grade 会分为更细的等级. 后面这些是从 FICO 获取的数据吧… 我们的目的是判断，来了一个用户，之后输入该用户的这些特征，我们判断他是不是一个欺诈用户 如果用户填写假资料 用户贷款之后的表现，如果填的真假我们不了解，填写的是假资料，但是之后还款表现好那么它还是一个好用户. 数据上 我只取了2016年Q3数据，9W+ 的数据，列数 122 列。数据有 99124行， 去掉表头，有 99123 行 2. Keep what we need 我们初步做特征筛选…, 我们在看的时候，可以分片分片的看这 122 个列… 2.1 特征分析 part1 id 和 member_id 不作为特征，可以直接去掉, int_rate 带 % 的可以直接去掉 %， 变为 float 的 Loan Amount Requested Verus the Funded Amount 2.2 特征分析 part2 123456df.ix[:5,8:15]print(df.emp_title.value_counts().head())print(df.emp_title.value_counts().tail())df.emp_title.unique().shape # 37421 emp_title， 太多了，可信度不高，我们也无法做 emp_title 非数值型变量的 one-hot enconding 1df.drop([&#x27;emp_title&#x27;],1, inplace=True) employment title LC code 3. 总结 删不删除变量，需要看模型，LR 需要删除，GBRT 不用删除也可以. LR 做评分卡模型，变量 一定是强变量，20个左右，不会几百个，有个变量可以训练处系数 出来 0 ~ 1 概率，拉一下橡皮筋， 分数映射，拉到 300 ~ 900 分，可以做一个评分卡，600分 可能是好用户:坏用户，可能是 50:1 评分卡的阶梯可能是增加。 模型不同 0 ~ 1 的概率可能是不同的，那么我增加50分，风险水平会降低一半 550 25:1 评分卡分的映射和模型是没有关系的，是样本里人群的好坏是有关系的，所以模型的参数做映射，是不需要重新训练的。 lending club 要求 FICO 是个特定的评分方法，是一个固定的评分方法。 比如 芝麻信用都是用自己的模型，自己算出来的 如果人群变了 模型的稳定性非常重要，当前要评估的人群已经和去年下半年的用户已经不一样的，所以训练的时候要尽可能提升模型的稳定性，如果训练时模型稳定性非常差，那么一上线就崩溃了。 如何提高模型稳定性，2种方法 特征筛选的时候，我会去把特征从样本的时间开始，2016.06 开始每个月我一直在看它的均值和方差的变化是否在容忍的范围内，比如我去年这个月这个特征是30，当前 2017.06 这个特征变为了 100，那么这个特征变化太大，是不能用的，超过50%， 太不稳定了，其实这个变量，或者做评分卡，反欺诈等是不合适的，直接扔掉。 尽量做模型融合，单模型的模型稳定性是不好的，随着月份的变化，你的预测是有变化的，波动的范围是有点低，ensemble 集成学习， 三种方法 (bagging、boosting，Stacking） 2.1 Bagging 比如 RF，每个模型取一样的权重，进行评估 2.2 Boosting 根据模型训练出不同权重，给予不同的权重 2.3 Stacking 我在用一个分类器，去处理我要集成的这3，4种模型，训练出一个参数 这三种方法，都能提高模型的稳定性 让你在线上运行 3~6 个月，信贷产品比较长的话，2个月更新一次比较好，贷款周期短的话，周更新都可以 有做 KS 比较高的话，会送大家小礼品， 我们线上有用 spark streaming 也有处理实时特征，但是目前体量，一般单机和离线处理就够了。 9W 个用户，100多个变量，那根本不需要用分布式来计算了。 半年的样本数据，把数据取出来之后，你要定义你的好坏样本,会把一些灰色地段的用户给他摘除掉，只留下最好或者最坏的用户，这些用户提特征之后，在做训练，样本内的验证和跨时间的验证 ，就是说我的时间段是完全不一样的，那么能够验证模型的稳定性，那么最好就要拿 2017年，1和2月的数据，在做一个跨时间的验证，跨时间的验证才是你真正上线之后的效果，因为你在时间窗口内训练或者test的话，它的 ks 可能 30 多，如果跨时间验证的话，你的人群可能会偏移，那么ks可能会下降，ks就变为20，如果差别控制在 15%，差别大稳定性就很不好，是不能上线的。 模型的话，你现在开始做模型，你一定取的是 去年 下半年的是数据，做验证的话是拿去年1月份的数据，一个月的数据还有5，6，7个还款表现，基本上等你做完模型，你做跨时间验证的话，刚刚好，你花2个礼拜做一个模型，上线的时候，你就不需要重新训练了。除非你到9月份上线，那么时间久了，就需要重新训练，一般是不需要重新训练的。 欺诈模型的稳定性评价指标： 1. 对比训练集 与 跨时间验证集 的 KS 偏差，一般偏差大不大的话，觉得这个模型是可以在时间维度上hold住的，那么可以模型上线。另外指标金融上比较常用的指标是 psi，这个是验证不同人群的偏移程度，以后可以自己查查资料。 Reference 金融反欺诈场景下的Spark实践","categories":[{"name":"data-science","slug":"data-science","permalink":"http://www.iequa.com/categories/data-science/"}],"tags":[{"name":"ITFIN","slug":"ITFIN","permalink":"http://www.iequa.com/tags/ITFIN/"}]},{"title":"互联网金融风控中的数据科学 (part2)  ： 模型策略","slug":"datascience/internet-finance-2","date":"2018-04-21T10:28:21.000Z","updated":"2021-06-20T04:12:28.340Z","comments":true,"path":"2018/04/21/datascience/internet-finance-2/","link":"","permalink":"http://www.iequa.com/2018/04/21/datascience/internet-finance-2/","excerpt":"反欺诈也是一种机器学习过程， 反欺诈建模中的数据科学","text":"反欺诈也是一种机器学习过程， 反欺诈建模中的数据科学 反欺诈也是一种机器学习过程 对于做互联网金融一般情况是 正负样本 是极度不平衡的(最高可以达到 100 : 1), 这样的情况对于 SVM 这种分类器是不合适的，所以在做金融的评分卡模型 或 欺诈模型 也好，这样对特征的处理 和 样本的非平衡处理是比较高的. 好坏用户的定义，一般是根据用户的贷后表现，来定义好坏用户的. 举个栗子🌰 : 用户借款 5W 元，可能是分期还款 12个月，这样每个月都会还一笔固定的额度. 信用风险 : 在挺长的时间可以按时还款. 欺诈风险 : 用户可能 第 1、2 期 是还的，之后是不还的. (因为中介也越来越聪明，给他自己留出时间，躲避催收的手段，也躲避追踪等等) 贷前审核 （触碰到拒贷规则） 造假行为 （信息资料造假） 调查员 调查出来是 中介 或者 有欺诈风向的，进入黑名单的. … 所以我们在定义模型负样本的话，我们可能定义为 m1+ 信用风险、m3+ (90天以上不还款的话)，我们可以定义为欺诈风险 坏用户 ： 欺诈风险用户 好用户 ： 一天都不逾期还款 灰用户 ： m1+ 未还款，但是90天之内可以还款的 (不放在训练中，否则会给模型带来很多额外的信息，影响效果) 金融模型 和 CTR 预估的相比是 有一个周期性质的 广告点击的话，用户点击，立马有一个样本出现 做长期现金贷，选择样本是选择半年之前的用户，作为样本 模型策略 1. Linear Regression 2 Logistic Regression 3. Decision Tree 4. Random Forest 5. Gradient Boosting RT 结果评估-混淆矩阵 Precision: 评估认定坏用户的精确度 Recall: 评估坏用户的召回率 F-Measure: 组合判断 Reference 金融反欺诈场景下的Spark实践 大咖说 王婷","categories":[{"name":"data-science","slug":"data-science","permalink":"http://www.iequa.com/categories/data-science/"}],"tags":[{"name":"ITFIN","slug":"ITFIN","permalink":"http://www.iequa.com/tags/ITFIN/"}]},{"title":"互联网金融风控中的数据科学 (part1) ： 金融科技企业面临的欺诈⻛险","slug":"datascience/internet-finance-1","date":"2018-04-20T05:28:21.000Z","updated":"2021-06-20T04:12:28.342Z","comments":true,"path":"2018/04/20/datascience/internet-finance-1/","link":"","permalink":"http://www.iequa.com/2018/04/20/datascience/internet-finance-1/","excerpt":"金融科技企业面临的欺诈⻛险介绍 , 互联网金融 主要是通过互联网平台，连接 出借方 和 借款方","text":"金融科技企业面临的欺诈⻛险介绍 , 互联网金融 主要是通过互联网平台，连接 出借方 和 借款方 1. 金融与科技的结晶 金融的本质 : 资源的最合理化应用 互联网技术 : 交易的边界成本趋向“零” 金融科技 : 用大数据、云计算等技术实现的资金融通、支付、投资和信息中介服务 我们国家没有覆盖度很全的所有人的征信，虽然有央行，但是还远远不够… 授权抓取的用户的数据，(百融、同盾、芝麻信用 等，工作中可能都会对接到这些平台)，这些平台会通过很多渠道收集用户，然后给用户打一些风险标签和欺诈的评分. 2. 中国信用贷款行业 网贷之家 收集P2P网贷平台的运营数据，并为行业排行, 以下为 2018年3月数据做的行业排行 : 网贷之家 : http://www.wdzj.com/pingji.html 现在消亡的 P2P 公司逐渐消亡的也很多，之前 3000 多家，行业大洗牌之后，现在 2000 多家… 在整个行业的体量上，陆金所 一定是体量非常大的，整个网贷的市场大概有万亿级别. 3. 中国信用贷款行业分层 APR (Annual Percentage Rate) 年利率, 对银行信用卡来说，一般 APR 在 16% ~ 18%, 那么日利率为 0.04%~0.05% 之间 举个栗子🌰 : 银行是按日收利息的。简单介绍一下利息是如何计算的 : 假设用信用卡提现1000刀，20天后还清，这张卡的Cash Advance APR是25.49%. 一年有365天（部分银行按360天算（不要问我闰年怎么算Orz））日利率应该是25.49%/365=0.07%. 20天后产生的利息为1000 * 20 * 0.07% = 14刀 APR 可以划分人群，APR 不同级别，贷前贷后的审核，催收的制度 也是不同的 APR越低，人群是越好的. 在 APR 低的人群，是基本不需要催收的，在 APR 高的人群，是要催收的. 高 APR 人群: 对于 APR 在 40% ~ 80% 的，比如 拍拍贷，这种小额的现金贷，5000 以下，7天~1个月，贷款的时间也短，多还的利息用户是不感知的，但是如果变成年化，APR 就会非常高 在 APR &gt; 80%, 是 现金巴士，用钱宝，这些存在也是有人们的需求存在的 APR 低的用户，就是信用好，APR高的话，就是信用没有那么好，或者还款能力没有那么好 做金融最大的本质就是在控制风险，在风险可控的情况下获得最大的利润. 4. 个人对个人的信用贷款 5. 急速信任-自动化信用评估 国内外 P2P 网贷的比较 : 国家金融环境存在较大区别,在信用体系建设等方面也都存在很大的差异，国外拥有较为完善的信用评估体制,中国在这方面却非常缺失，所以国内的借贷平台在用户信用评估方面都做出自己的努力，构建了不同形式的评价方法. 国外有完善的信用评估体质，有 FICO官方的评分. 国外80%都是信用风险，20%是欺诈风险. 中国更多的是欺诈风险. FICO 成立于1956年，为纽交所上市公司，市值52亿美金，提供跨多个行业的分析软件和工具. 国内黑产业链 : 国内不还钱的话惩罚的措施跟不上，国内有些中介我不还钱的话，你找不到我的话，这个钱就是我空手套白狼的利润，这样催生了越来越多贷款的欺诈的情况，他们有一个黑产业链，从账号的获取到恶意的注册，再到互联网金融公司的平台申请贷款，有的中介会有一些现象. 举个栗子🌰: 他们会到燕郊找一批老人妇女，然后说我给你3000元钱，你跟着我走一趟。还有一些客户对自己的资质没有信心，然后找中介包装一些材料。有好中介，有坏中介，如果坏中介带你贷了5W元钱，然后给你2.5W告诉你爱还不还，然后还可以带你的信息再去其他家平台再贷款，这样用户在不知情的情况下会背负很多债务信息. 中介做的事，就是不停的去试各个P2P平台产品，发现其中漏洞，这些中介比产品经理还要了解这个产品，然后他帮助他的客户去做包装，这样比如一下子可能进来 100 个欺诈用户，每个用户5W，这样一下子就是500W，对企业来说损失很大，然后在这种高额收入的诱惑下，这些中介会升级不断自己的伪造技术. 6. 金融科技企业面临的欺诈风险 对于线上反欺诈来说，你看不见用户，只面对数据，要发现数据之间的异常、用户与用户之间有没有异常相似度联系等. 7. Reference 宜人贷数据科学家王婷: 金融反欺诈场景下的Spark实践 大咖说 王婷","categories":[{"name":"data-science","slug":"data-science","permalink":"http://www.iequa.com/categories/data-science/"}],"tags":[{"name":"ITFIN","slug":"ITFIN","permalink":"http://www.iequa.com/tags/ITFIN/"}]},{"title":"Ensemble Learning (part2)","slug":"ml/ensumble-boosting-2","date":"2018-04-11T08:08:21.000Z","updated":"2021-06-20T04:12:28.310Z","comments":true,"path":"2018/04/11/ml/ensumble-boosting-2/","link":"","permalink":"http://www.iequa.com/2018/04/11/ml/ensumble-boosting-2/","excerpt":"提升（boosting）方法是一类应用广泛且非常有效的统计学习方法。 Boosting 概念 代表性 Boosting 算法 AbaBoost 介绍","text":"提升（boosting）方法是一类应用广泛且非常有效的统计学习方法。 Boosting 概念 代表性 Boosting 算法 AbaBoost 介绍 《An Empirical Comparison of Supervised Learning Algorithms》ICML2006. Adaboost 在处理二类分类问题时，随着弱分类器的个数增加，训练误差与测试误差的曲线图。 从图中可以看出，Adaboost算法随着模型复杂度的增加，测试误差（红色点线）基本保持稳定，并没有出现过拟合的现象。 其实不仅是Adaboost算法有这种表现，Boosting方法的学习思想和模型结构上可以保证其不容易产生过拟合（除非Weak Learner本身出现过拟合）。 下面我们主要是从损失函数的差异，来介绍Boosting的家族成员；然后我们针对每个具体的家族成员，详细介绍其学习过程和核心公式；最后从算法应用场景和工具方法给出简单的介绍。 Boosting Boosting方法基于这样一种思想： 对于一个复杂任务来说，将多个专家的判定进行适当的综合得出的判断，要比其中任何一个专家单独的判断好。 就是 “三个臭皮匠顶个诸葛亮” …😄😄😄 1. 概率可学习性 (PAC) PAC理论是由2010年图灵奖的得主Valiant和Kearns提出的一套理论体系，主要讨论什么时候，一个问题是可以被学习的。 PAC体系定义了学习算法的强弱： (1) 弱学习算法 : 如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好， (2) 强学习算法 : 存在一个多项式的学习算法能够学习它，并且正确率很高 在概率近似正确（probably approximately correct，PAC）学习框架中： ①. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的； ②. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。 Valiant和 Kearns提出PAC学习模型中弱学习算法和强学习算法的等价性猜想： 该猜想最重要的含义是，如果二者等价 ,那么只需找到一个比随机猜测略好的弱学习算法就可以将其提升为强学习算法，而不必寻找很难获得的强学习算法。 该问题的重要性随即引起方法论大师的追捧，大家都在试图设计算法来验证PAC理论的正确性。 1996，Schapire提出一种新的名叫AdaBoost的算法证明了上述猜想。AdaBoost把多个不同的决策树用一种非随机的方式组合起来，表现出惊人的性能。同时，Schapire证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。 Summary : 强可学习⇔弱可学习 Boosting 对于一个学习问题来说（以分类问题为例），给定训练数据集，求一个弱学习算法要比求一个强学习算法要容易的多。Boosting方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合弱分类器，得到一个强分类器。Boosting方法在学习过程中通过改变训练数据的权值分布，针对不同的数据分布调用弱学习算法得到一系列弱分类器。 还有就是，Boosting算法更加关注错分的样本，这点和Active Learning的寻找最有价值的训练样本有点遥相呼应的感觉 很抽象对不对，但是过一会儿我们通过Adaboost来理解这个核心思想 回答两个问题 ： 在每一轮学习之前，如何改变训练数据的权值分布？ 如何将一组弱分类器组合成一个强分类器？ 具体不同的boosting实现，主要区别在弱学习算法本身和上面两个问题的回答上。 问题1，Adaboost算法的做法是 ： 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。 如此，那些没有得到正确分类的样本，由于其权值加大而受到后一轮的弱分类器的更大关注。 问题2，AdaBoost采取加权多数表决的方法 ： (1). 加大 分类误差率小 的弱分类器的权值，使其在表决中起较大的作用； (2). 减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。 AdaBoost算法的巧妙之处就在于它将这些学习思路自然并且有效地在一个算法里面实现。 Boosting算法代表 ：Adaboost(Adaptive Boosting) 核心思想：一种迭代算法，针对同一个训练集训练不同的分类器(弱分类器)，然后进行分类，对于分类正确的样本权值低，分类错误的样本权值高（通常是边界附近的样本），最后的分类器是很多弱分类器的线性叠加（加权组合），分类器相当简单。实际上就是一个简单的弱分类算法提升(boost)的过程。 看图形来过一遍Adaboost算法 算法开始前，需要将每个样本的权重初始化为 1/m, 这样一开始每个样本都是等概率的分布，每个分类器都会公正对待. Round1，因为样本权重都一样，所以分类器开始划分，根据自己分类器的情况，只和分类器有关。划分之后发现分错了三个&quot;+&quot;号，那么这些分错的样本，在给下一个分类器的时候权重就得到提高,也就是会影响到下次取训练样本的分布，就是提醒下一个分类器，“诶！你注意点这几个小子，我上次栽在他们手里了！” Round2,第二代分类器信誓旦旦的对上一代分类器说&quot;我知道了，大哥！我一定睁大眼睛好好分着三个玩意！“ok，这次三个上次分错的都被分出来了，但是并不是全部正确，这次又栽倒在左下角三个”-“上了，然后临死前，第二代分类器对下一代分类器说&quot;这次我和上一代分类器已经把他们摸得差不多了，你再稍微注意下左下角那三个小子，也别忘了上面那三个(一代错分的那三个”+&quot;)！&quot; Round3:有了上面两位大哥的提醒，第三代分类器表示，我差不多都知道上次大哥们都错哪了，我只要小心这几个，应该没什么问题！只要把他们弄错的我给整对了，然后把我们收集的信息一对，这不就行了么！ok，第三代分类器不负众望，成功分对上面两代分类器重点关注的对象，至于分错的那几个小的，以前大哥们都分对了，我们坐下来核对一下就行了！ 最后，三个分类器坐下来，各自谈了谈心得，分配了下权重，然后一个诸葛亮就诞生啦！这也就是 “三个臭皮匠顶个诸葛亮的故事” …😄😄😄, 是不是道理很简单！至于权重如何计算，暂不在本文讨论. Adaboost 优点 可以使用各种方法构造子分类器，Adaboost算法提供的是框架 简单，不用做特征筛选 相比较于RF，更不用担心过拟合问题 Adaboost 缺点 Adaboost对于噪声是十分敏感的。Boosting方法本身对噪声点异常点很敏感，因此在每次迭代时候会给噪声点较大的权重，这不是我们系统所期望的。 运行速度慢，凡是涉及迭代的基本上都无法采用并行计算，Adaboost是一种&quot;串行&quot;算法.所以GBDT(Gradient Boosting Decision Tree)也非常慢。 Pay Attention Bagging 树&quot;并行&quot;生成,如 Random Forest ; Boosting：树&quot;串行&quot;生成,如Adaboost Boosting 中基模型为弱模型，而 Random Forest 中的基树是强模型(大多数情况) Boosting 重采样的不是样本，而是样本的分布，每次迭代之后，样本的分布会发生变化，也就是被分错的样本会更多的出现在下一次训练集中 明确一点，我们迭代也好(Adaboost), 并行(RF)也好，只和训练集有关，和测试集真的一毛钱关系都没有好么！我们先把原始数据分类测试集和训练集，然后测试集放一边，训练集里面再挑子集作为迭代算法用的训练集！这个和K-Fold Cross-Validation思想类似. Reference article 总结：Bootstrap(自助法)，Bagging，Boosting(提升) Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost 机器学习选讲：AdaBoost方法详解 52caml 统计学习方法 Scikit-Learn 中文文档 概率校准 - 监督学习","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Ensemble Learning (part1)","slug":"ml/ensumble-boosting-1","date":"2018-04-07T08:08:21.000Z","updated":"2021-06-20T04:12:28.314Z","comments":true,"path":"2018/04/07/ml/ensumble-boosting-1/","link":"","permalink":"http://www.iequa.com/2018/04/07/ml/ensumble-boosting-1/","excerpt":"Ensemble learning（集成学习）：是目前机器学习的一大热门方向，所谓集成学习简单理解就是指采用多个分类器对数据集进行预测，从而提高整体分类器的泛化能力。","text":"Ensemble learning（集成学习）：是目前机器学习的一大热门方向，所谓集成学习简单理解就是指采用多个分类器对数据集进行预测，从而提高整体分类器的泛化能力。 Bootstraping Bagging、Boosting、Stacking Bootstraping Bootstraping的名称来自成语 “pull up by your own bootstraps”，意思是依靠你自己的资源，它是一种有放回的抽样方法. 注:Bootstrap本义是指高靴子口后面的悬挂物、小环、带子，是穿靴子时用手向上拉的工具。“pull up by your own bootstraps”即“通过拉靴子让自己上 升”，意思是“不可能发生的事情”。后来意思发 生了转变，隐喻“不需要外界帮助，仅依靠自身力 量让自己变得更好” bootstraping 的思想和步骤如下： 举个🌰：我要统计鱼塘里面的鱼的条数，怎么统计呢？假设鱼塘总共有鱼1000条，我是开了上帝视角的，但是你是不知道里面有多少。 步骤： 承包鱼塘，不让别人捞鱼(规定总体分布不变)。 自己捞鱼，捞100条，都打上标签(构造样本) 把鱼放回鱼塘，休息一晚(使之混入整个鱼群，确保之后抽样随机) 开始捞鱼，每次捞100条，数一下，自己昨天标记的鱼有多少条，占比多少(一次重采样取分布)。 重复3，4步骤n次。建立分布。 假设一下，第一次重新捕鱼100条，发现里面有标记的鱼12条，记下为12%，放回去，再捕鱼100条，发现标记的为9条，记下9%，重复重复好多次之后，假设取置信区间95%，你会发现，每次捕鱼平均在10条左右有标记，所以，我们可以大致推测出鱼塘有1000条左右。其实是一个很简单的类似于一个比例问题。这也是因为提出者Efron给统计学顶级期刊投稿的时候被拒绝的理由–“太简单”。这也就解释了，为什么在小样本的时候，bootstrap效果较好，你这样想，如果我想统计大海里有多少鱼，你标记100000条也没用啊，因为实际数量太过庞大，你取的样本相比于太过渺小，最实际的就是，你下次再捕100000的时候，发现一条都没有标记，，，这TM就尴尬了。。 Bootstrap 经典语录 Bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。 就是一个在自身样本重采样的方法来估计真实分布的问题 当我们不知道样本分布的时候，bootstrap方法最有用。 Ensemble learning 了解 boosting 和 bagging 之前，先了解一下什么是 ensemble，一句话，三个臭皮匠顶个诸葛亮，一箭易折十箭难折，千里之堤溃于蚁穴 …😄😄😄 ，在分类的表现上就是，多个弱分类器组合变成强分类器。 假设各弱分类器间具有一定差异性（如不同的算法，或相同算法不同参数配置），这会导致生成的分类决策边界不同，也就是说它们在决策时会犯不同的错误。将它们结合后能得到更合理的边界，减少整体错误，实现更好的分类效果。 Bagging (bootstrap aggregation) bagging：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果,至于为什么叫bootstrap aggregation，因为它抽取训练样本的时候采用的就是bootstrap的方法！ Bagging 策略过程 😄 : 从样本集中重采样(有重复的)选出n个样本 在所有属性上，对这n个样本建立分类器 (ID3、C4.5、CART、SVM、Logistic回归等) 重复以上两步m次，即获得了m个分类器 将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类. 投票机制 (多数服从少数, 民主政治) 看到底分到哪一类(分类问题) Bagging 代表算法 - Random Forest 随机森林，它不仅可以用来做分类，也可用来做回归即预测，一般随机森林机由多个决策树构成，相比于单个决策树算法，它分类、预测效果更好，不容易出现过度拟合的情况。 1.训练样本选择方面的Random： Bootstrap方法随机选择子样本 2.特征选择方面的Random： 属性集中随机选择k个属性，每个树节点分裂时，从这随机的k个属性，选择最优的(如何选择最优又有各种最大增益的方法，不在本文讨论范围内)。 决策树 决策树对训练样本有良好的分类能力，只要我们的层数不加限制，我们甚至可以把它分的没有任何误差，这样可能导致你的泛化能力很弱。 缓解的方法就是 1. 剪枝 2. 随机森林 剪枝 我还没用过，所以我们看常用的 随机森林 Romdom Forest 决策树 ： 特征选择 ID3 仅具有教学价值 gini 系数，作为指标比较多，在实践当中 Random Forest 构造流程 用 Random(训练样本用Bootstrap方法，选择分离叶子节点用上面的2)的方式构造一棵决策树(CART) 用1的方法构造很多决策树, 不剪枝, 许多决策树构成一片森林，决策树之间没有联系 测试数据进入每一棵决策树，每棵树做出自己的判断，然后投票选出最终所属类别(默认每棵树权重一致) Random Forest 优点 不容易出现过拟合，因为选择训练样本的时候就不是全部样本。 既可处理属性为离散值的量，比如ID3算法来构造树，也可以处理属性为连续值的量，比如C4.5算法来构造树 对于高维数据集的处理能力令人兴奋，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。该模型能够输出变量的重要性程度，这是一个非常便利的功能。 分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法 Random Forest 缺点 随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。 对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。 优缺点，需要与杰神商讨再? Reference article Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost Bootstrap(自助法)，Bagging，Boosting(提升)","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"Ensemble","slug":"Ensemble","permalink":"http://www.iequa.com/tags/Ensemble/"}]},{"title":"一般英文聽力練習方法","slug":"English/practice-listening-by-cindy","date":"2018-01-28T14:47:16.000Z","updated":"2021-06-22T06:33:48.668Z","comments":true,"path":"2018/01/28/English/practice-listening-by-cindy/","link":"","permalink":"http://www.iequa.com/2018/01/28/English/practice-listening-by-cindy/","excerpt":"一般英文聽力加強","text":"一般英文聽力加強 多益 以及 一般英文聽力加強 | 和Cindy學英文 笔记 練習聽力 Useful Websites News In Levels Umano Lyrics Training Listen to Music Lyrics Training Taylor Swift、Miley Cyrus、Blake Shelton… Sitcom and Movies Friends How I met your Mother Titanic Hobbit Reference 多益 英文聽力加強 和Cindy學英文","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"}],"tags":[{"name":"Cindy-English","slug":"Cindy-English","permalink":"http://www.iequa.com/tags/Cindy-English/"}]},{"title":"Python copy & deepcopy","slug":"python/language/py-language-13-copy","date":"2018-01-24T07:00:21.000Z","updated":"2021-06-20T04:12:28.232Z","comments":true,"path":"2018/01/24/python/language/py-language-13-copy/","link":"","permalink":"http://www.iequa.com/2018/01/24/python/language/py-language-13-copy/","excerpt":"对象的赋值，拷贝（深/浅拷贝）之间是有差异的，如果使用不当，可能产生意外的结果.","text":"对象的赋值，拷贝（深/浅拷贝）之间是有差异的，如果使用不当，可能产生意外的结果. id 什么是id？一个对象的id值在CPython解释器里就代表它在内存中的`地址 123456789101112import copya=[1,2,3]b=aprint(id(a))print(id(b))print(id(a)==id(b)) #附值后，两者的id相同，为true。b[0]=222222 # 此时，改变b的第一个值，也会导致a值改变。print(a,b) 4449594888 4449594888 True [222222, 2, 3] [222222, 2, 3] 浅拷贝 当使用浅拷贝时，python 只是拷贝了最外围的对象本身，内部的元素都只是拷贝了一个引用而已 1234567891011import copya=[1,2,3]c=copy.copy(a) #拷贝了a的外围对象本身,print(id(c))print(id(a)==id(c)) #id 改变 为falsec[1]=22222 #此时，我去改变c的第二个值时，a不会被改变。print(a,c)# [1, 2, 3] [1, 22222, 3] #a值不变,c的第二个值变了，这就是copy和‘==’的不同 4449594440 False [1, 2, 3] [1, 22222, 3] 深拷贝 deepcopy 对外围和内部元素都进行了拷贝对象本身，而不是对象的引用 1234567891011121314151617181920#copy.copy()a=[1,2,[3,4]] #第三个值为列表[3,4],即内部元素d=copy.copy(a) #浅拷贝a中的[3，4]内部元素的引用，非内部元素对象的本身&gt;&gt;&gt; id(a)==id(d)False&gt;&gt;&gt; id(a[2])==id(d[2])True&gt;&gt;&gt; a[2][0]=3333 #改变a中内部原属列表中的第一个值&gt;&gt;&gt; d #这时d中的列表元素也会被改变[1, 2, [3333, 4]]#copy.deepcopy()&gt;&gt;&gt; e=copy.deepcopy(a) #e为深拷贝了a&gt;&gt;&gt; a[2][0]=333 #改变a中内部元素列表第一个的值&gt;&gt;&gt; e[1, 2, [3333, 4]] #因为时深拷贝，这时e中内部元素[]列表的值不会因为a中的值改变而改变&gt;&gt;&gt; Reference docs.python.org python morvanzhou python liaoxuefeng","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python try … except … as …","slug":"python/language/py-language-12-try-exception","date":"2018-01-24T06:02:21.000Z","updated":"2021-06-20T04:12:28.234Z","comments":true,"path":"2018/01/24/python/language/py-language-12-try-exception/","link":"","permalink":"http://www.iequa.com/2018/01/24/python/language/py-language-12-try-exception/","excerpt":"try:, except … as …:","text":"try:, except … as …: 错误处理 输出错误：try:, except ... as ...: 看如下代码 1234try: file=open(&#x27;eeee.txt&#x27;,&#x27;r&#x27;) #会报错的代码except Exception as e: # 将报错存储在 e 中 print(e) [Errno 2] No such file or directory: 'eeee.txt' 123456789101112try: file=open(&#x27;eeee.txt&#x27;,&#x27;r+&#x27;)except Exception as e: print(e) response = input(&#x27;do you want to create a new file:&#x27;) if response==&#x27;y&#x27;: file=open(&#x27;eeee.txt&#x27;,&#x27;w&#x27;) else: passelse: file.write(&#x27;ssss&#x27;) file.close() [Errno 2] No such file or directory: 'eeee.txt' do you want to create a new file:y Reference docs.python.org python morvanzhou python liaoxuefeng","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python 函数式编程 zip、lambda、map...","slug":"python/language/py-language-11-zip-lambda-map","date":"2018-01-24T06:00:21.000Z","updated":"2021-06-20T04:12:28.224Z","comments":true,"path":"2018/01/24/python/language/py-language-11-zip-lambda-map/","link":"","permalink":"http://www.iequa.com/2018/01/24/python/language/py-language-11-zip-lambda-map/","excerpt":"zip、lambda、map…","text":"zip、lambda、map… zip zip 函数接受任意多个（包括0个和1个）序列作为参数，合并后返回一个tuple列表 1234a=[1,2,3]b=[4,5,6]ab=zip(a,b)print(list(ab)) #需要加list来可视化这个功能 [(1, 4), (2, 5), (3, 6)] 12345678a=[1,2,3]b=[4,5,6]ab=zip(a,b)print(list(ab))for i,j in zip(a,b): print(i/2,j*2) [(1, 4), (2, 5), (3, 6)] 0.5 8 1.0 10 1.5 12 lambda lambda 定义一个简单的函数，实现简化代码的功能，看代码会更好理解。 fun = lambda x,y : x+y, 冒号前的x,y为自变量，冒号后x+y为具体运算 1234fun= lambda x,y:x+yx=int(input(&#x27;x=&#x27;)) #这里要定义int整数，否则会默认为字符串y=int(input(&#x27;y=&#x27;))print(fun(x,y)) x=4 y=6 10 map map 是把 函数 和 参数 绑定在一起. 123456def fun(x,y): return (x+y)print(list(map(fun,[1],[2])))list(map(fun,[1,2],[3,4])) [3] [4, 6] Reference docs.python.org python morvanzhou python liaoxuefeng","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Matplotlib Bar","slug":"python/matplotlib/py-matplotlib-9-bar","date":"2018-01-24T05:20:21.000Z","updated":"2021-06-20T04:12:28.240Z","comments":true,"path":"2018/01/24/python/matplotlib/py-matplotlib-9-bar/","link":"","permalink":"http://www.iequa.com/2018/01/24/python/matplotlib/py-matplotlib-9-bar/","excerpt":"上篇学习了如何 plot Scatter，今天我们讲述如何 plot Bar","text":"上篇学习了如何 plot Scatter，今天我们讲述如何 plot Bar 今日目标 : 柱状图分成上下两部分，每一个柱体上都有相应的数值标注，并且取消坐标轴的显示. 生成基本图形 向上向下生成12个数据，X 为 [0,11] 的整数 ，Y是均匀分布的随机数据。 使用的函数是plt.bar，参数为X和Y： 1234567891011121314151617181920import matplotlib.pyplot as pltimport numpy as npn = 12X = np.arange(n)Y1 = (1 - X / float(n)) * np.random.uniform(0.5, 1.0, n)Y2 = (1 - X / float(n)) * np.random.uniform(0.5, 1.0, n)plt.bar(X, +Y1)plt.bar(X, -Y2)plt.xlim(-.5, n)plt.xticks(())plt.ylim(-1.25, 1.25)plt.yticks(())plt.show() 加颜色和数据 用facecolor设置主体颜色，edgecolor设置边框颜色为白色， 1234plt.bar(X, +Y1, facecolor=&#x27;#9999ff&#x27;, edgecolor=&#x27;white&#x27;)plt.bar(X, -Y2, facecolor=&#x27;#ff9999&#x27;, edgecolor=&#x27;white&#x27;)plt.show() 接下来我们用函数plt.text分别在柱体上方（下方）加上数值，用%.2f保留两位小数，横向居中对齐ha='center'，纵向底部（顶部）对齐va='bottom'： 1234567891011121314plt.bar(X, +Y1, facecolor=&#x27;#9999ff&#x27;, edgecolor=&#x27;white&#x27;)plt.bar(X, -Y2, facecolor=&#x27;#ff9999&#x27;, edgecolor=&#x27;white&#x27;)for x, y in zip(X, Y1): # ha: horizontal alignment # va: vertical alignment plt.text(x + 0.4, y + 0.05, &#x27;%.2f&#x27; % y, ha=&#x27;center&#x27;, va=&#x27;bottom&#x27;)for x, y in zip(X, Y2): # ha: horizontal alignment # va: vertical alignment plt.text(x + 0.4, -y - 0.05, &#x27;%.2f&#x27; % y, ha=&#x27;center&#x27;, va=&#x27;top&#x27;) plt.show() Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Matplotlib Scatter","slug":"python/matplotlib/py-matplotlib-8-scatter","date":"2018-01-24T03:08:21.000Z","updated":"2021-06-20T04:12:28.244Z","comments":true,"path":"2018/01/24/python/matplotlib/py-matplotlib-8-scatter/","link":"","permalink":"http://www.iequa.com/2018/01/24/python/matplotlib/py-matplotlib-8-scatter/","excerpt":"上篇学习了如何 plot 线，今天学习如何 plot Scatter 散点图","text":"上篇学习了如何 plot 线，今天学习如何 plot Scatter 散点图 引入模块numpy用来产生一些随机数据。生成1024个呈标准正态分布的二维数据组 (平均数是0，方差为1) 作为一个数据集，并图像化这个数据集。每一个点的颜色值用T来表示： 123456789import matplotlib.pyplot as pltimport numpy as npn = 1024 # data sizeX = np.random.normal(0, 1, n) # 每一个点的X值Y = np.random.normal(0, 1, n) # 每一个点的Y值T = np.arctan2(Y,X) # for color value 数据集生成完毕，现在来用 scatter plot 这个点集 输入X和Y作为location，size=75，颜色为T，color map 用默认值，透明度alpha 为 50%。 x轴显示范围定位(-1.5，1.5)，并用xtick() 函数来隐藏x坐标轴，y轴同理： 12345678plt.scatter(X, Y, s=75, c=T, alpha=.5)plt.xlim(-1.5, 1.5)plt.xticks(()) # ignore xticksplt.ylim(-1.5, 1.5)plt.yticks(()) # ignore yticksplt.show() Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Matplotlib Tick bbox","slug":"python/matplotlib/py-matplotlib-7-tick","date":"2018-01-23T09:08:21.000Z","updated":"2021-06-20T04:12:28.239Z","comments":true,"path":"2018/01/23/python/matplotlib/py-matplotlib-7-tick/","link":"","permalink":"http://www.iequa.com/2018/01/23/python/matplotlib/py-matplotlib-7-tick/","excerpt":"图中的内容较多，可通过设置相关内容的透明度来使图片更易于观察，也即是本节中的bbox参数设置来调节图像信息.","text":"图中的内容较多，可通过设置相关内容的透明度来使图片更易于观察，也即是本节中的bbox参数设置来调节图像信息. 生成图形 1234567891011121314151617181920212223import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 0.1*xplt.figure()# 在 plt 2.0.2 或更高的版本中, 设置 zorder 给 plot 在 z 轴方向排序plt.plot(x, y, linewidth=10, zorder=1)plt.ylim(-2, 2)ax = plt.gca()ax.spines[&#x27;right&#x27;].set_color(&#x27;none&#x27;)ax.spines[&#x27;top&#x27;].set_color(&#x27;none&#x27;)ax.xaxis.set_ticks_position(&#x27;bottom&#x27;)ax.spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;, 0))ax.yaxis.set_ticks_position(&#x27;left&#x27;)ax.spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;, 0))plt.show() 调整坐标 bbox 然后对被遮挡的图像调节相关透明度，本例中设置 x轴 和 y轴 的刻度数字进行透明度设置 其中label.set_fontsize(12)重新调节字体大小，bbox设置目的内容的透明度相关参数，facecolor调节 box 前景色，edgecolor 设置边框， 本处设置边框为无，alpha设置透明度. 12345for label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontsize(12) # 在 plt 2.0.2 或更高的版本中, 设置 zorder 给 plot 在 z 轴方向排序 label.set_bbox(dict(facecolor=&#x27;white&#x27;, edgecolor=&#x27;None&#x27;, alpha=0.7, zorder=2))plt.show() Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Matplotlib Annotation","slug":"python/matplotlib/py-matplotlib-6-Annotation","date":"2018-01-23T08:38:21.000Z","updated":"2021-06-20T04:12:28.242Z","comments":true,"path":"2018/01/23/python/matplotlib/py-matplotlib-6-Annotation/","link":"","permalink":"http://www.iequa.com/2018/01/23/python/matplotlib/py-matplotlib-6-Annotation/","excerpt":"当图线中某些特殊地方需要标注时，我们可以使用 annotation. matplotlib 中的 annotation 有两种方法， 一种是用 plt 里面的 annotate，一种是直接用 plt 里面的 text 来写标注.","text":"当图线中某些特殊地方需要标注时，我们可以使用 annotation. matplotlib 中的 annotation 有两种方法， 一种是用 plt 里面的 annotate，一种是直接用 plt 里面的 text 来写标注. 画出基本图 首先，我们在坐标轴中绘制一条直线. 123456789import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 2*x + 1plt.figure(num=1, figsize=(8, 5),)plt.plot(x, y,)plt.show() 移动坐标 123456789101112131415161718192021import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 2*x + 1plt.figure(num=1, figsize=(8, 5),)plt.plot(x, y,)# 移动坐标ax = plt.gca()ax.spines[&#x27;right&#x27;].set_color(&#x27;none&#x27;)ax.spines[&#x27;top&#x27;].set_color(&#x27;none&#x27;)ax.xaxis.set_ticks_position(&#x27;bottom&#x27;)ax.spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;, 0))ax.yaxis.set_ticks_position(&#x27;left&#x27;)ax.spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;, 0))plt.show() 然后标注出点(x0, y0)的位置信息. 用 plt.plot([x0, x0,], [0, y0,], 'k--', linewidth=2.5) 画出一条垂直于x轴的虚线. 1234567891011121314151617181920212223242526272829import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 2*x + 1plt.figure(num=1, figsize=(8, 5),)plt.plot(x, y,)# 移动坐标ax = plt.gca()ax.spines[&#x27;right&#x27;].set_color(&#x27;none&#x27;)ax.spines[&#x27;top&#x27;].set_color(&#x27;none&#x27;)ax.xaxis.set_ticks_position(&#x27;bottom&#x27;)ax.spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;, 0))ax.yaxis.set_ticks_position(&#x27;left&#x27;)ax.spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;, 0))x0 = 1y0 = 2*x0 + 1### plt.plot([x0, x0,], [0, y0,], &#x27;k--&#x27;, linewidth=2.5) 画出一条垂直于x轴的虚线. ###plt.plot([x0, x0,], [0, y0,], &#x27;k--&#x27;, linewidth=2.5)# set dot stylesplt.scatter([x0, ], [y0, ], s=50, color=&#x27;b&#x27;)plt.show() 加注释 annotate 接下来我们就对(x0, y0)这个点进行标注. 123456789101112131415161718192021222324252627282930313233import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-4, 4, 50)y = 2*x + 1plt.figure(num=1, figsize=(10, 6),)plt.plot(x, y,)# 移动坐标ax = plt.gca()ax.spines[&#x27;right&#x27;].set_color(&#x27;none&#x27;)ax.spines[&#x27;top&#x27;].set_color(&#x27;none&#x27;)ax.xaxis.set_ticks_position(&#x27;bottom&#x27;)ax.spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;, 0))ax.yaxis.set_ticks_position(&#x27;left&#x27;)ax.spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;, 0))x0 = 1y0 = 2*x0 + 1plt.plot([x0, x0,], [0, y0,], &#x27;k--&#x27;, linewidth=2.5)# set dot stylesplt.scatter([x0, ], [y0, ], s=50, color=&#x27;b&#x27;)############## 添加注释 annotate ###############plt.annotate(r&#x27;$2x+1=%s$&#x27; % y0, xy=(x0, y0), xycoords=&#x27;data&#x27;, xytext=(+30, -30), textcoords=&#x27;offset points&#x27;, fontsize=16, arrowprops=dict(arrowstyle=&#x27;-&gt;&#x27;, connectionstyle=&quot;arc3,rad=.2&quot;))plt.show() 其中参数 xycoords='data' 是说基于数据的值来选位置, xytext=(+30, -30) 和 textcoords='offset points' 对于标注位置的描述 和 xy 偏差值, arrowprops是对图中箭头类型的一些设置. 加注释 text 12plt.text(-3.7, 3, r&#x27;$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$&#x27;, fontdict=&#123;&#x27;size&#x27;: 16, &#x27;color&#x27;: &#x27;r&#x27;&#125;) Text(-3.7,3,'$This\\\\ is\\\\ the\\\\ some\\\\ text. \\\\mu\\\\ \\\\sigma_i\\\\ \\\\alpha_t$') 其中-3.7, 3,是选取text的位置, 空格需要用到转字符\\ ,fontdict设置文本字体. 123456789101112131415161718192021222324252627282930313233343536373839import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-4, 4, 50)y = 2*x + 1plt.figure(num=1, figsize=(10, 6),)plt.plot(x, y,)# 移动坐标ax = plt.gca()ax.spines[&#x27;right&#x27;].set_color(&#x27;none&#x27;)ax.spines[&#x27;top&#x27;].set_color(&#x27;none&#x27;)ax.xaxis.set_ticks_position(&#x27;bottom&#x27;)ax.spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;, 0))ax.yaxis.set_ticks_position(&#x27;left&#x27;)ax.spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;, 0))x0 = 1y0 = 2*x0 + 1plt.plot([x0, x0,], [0, y0,], &#x27;k--&#x27;, linewidth=2.5)# set dot stylesplt.scatter([x0, ], [y0, ], s=50, color=&#x27;b&#x27;)############## 添加注释 annotate ###############plt.annotate(r&#x27;$2x+1=%s$&#x27; % y0, xy=(x0, y0), xycoords=&#x27;data&#x27;, xytext=(+30, -30), textcoords=&#x27;offset points&#x27;, fontsize=16, arrowprops=dict(arrowstyle=&#x27;-&gt;&#x27;, connectionstyle=&quot;arc3,rad=.2&quot;))############# 添加注释 text #################plt.text(-3.7, 3, r&#x27;$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$&#x27;, fontdict=&#123;&#x27;size&#x27;: 16, &#x27;color&#x27;: &#x27;r&#x27;&#125;)plt.show() Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Matplotlib Legend","slug":"python/matplotlib/py-matplotlib-5-legend","date":"2018-01-23T07:30:21.000Z","updated":"2021-06-20T04:12:28.239Z","comments":true,"path":"2018/01/23/python/matplotlib/py-matplotlib-5-legend/","link":"","permalink":"http://www.iequa.com/2018/01/23/python/matplotlib/py-matplotlib-5-legend/","excerpt":"matplotlib 中的 legend 图例就是为了展示出每个数据对应的图像名称,可读性更好.","text":"matplotlib 中的 legend 图例就是为了展示出每个数据对应的图像名称,可读性更好. 添加图例 legend 123456789101112131415161718import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure()#set x limitsplt.xlim((-1, 2))plt.ylim((-2, 3))# set new sticksnew_sticks = np.linspace(-1, 2, 5)plt.xticks(new_sticks)# set tick labelsplt.yticks([-2, -1.8, -1, 1.22, 3], [r&#x27;$really\\ bad$&#x27;, r&#x27;$bad$&#x27;, r&#x27;$normal$&#x27;, r&#x27;$good$&#x27;, r&#x27;$really\\ good$&#x27;]) ([&lt;matplotlib.axis.YTick at 0x1195c3358&gt;, &lt;matplotlib.axis.YTick at 0x112681080&gt;, &lt;matplotlib.axis.YTick at 0x1195ce710&gt;, &lt;matplotlib.axis.YTick at 0x1195f5240&gt;, &lt;matplotlib.axis.YTick at 0x1195fc550&gt;], &lt;a list of 5 Text yticklabel objects&gt;) 对图中的两条线绘制图例，首先我们设置两条线的类型等信息（蓝色实线与红色虚线). 123# set line sylesl1, = plt.plot(x, y1, label=&#x27;linear line&#x27;)l2, = plt.plot(x, y2, color=&#x27;red&#x27;, linewidth=1.0, linestyle=&#x27;--&#x27;, label=&#x27;square line&#x27;) 需要注意的是 l1, l2, 要以逗号结尾, 因为 plt.plot() 返回的是一个list. legend 将要显示的信息来自于上面代码中的 label. 所以我们只需要简单写一下代码, plt 就能自动的为我们添加图例. 12plt.legend(loc=&#x27;upper right&#x27;)plt.show() 参数 loc='upper right' 表示图例将添加在图中的右上角. 调整位置和名称 如果我们想单独修改之前的 label 信息, 给不同类型的线条设置图例信息. 我们可以在 plt.legend 输入更多参数. 如果以下面这种形式添加 legend, 我们需要确保, 在上面的代码 plt.plot(x, y2, label='linear line') 和 plt.plot(x, y1, label='square line') 中有用变量 l1 和 l2 分别存储起来. 12plt.legend(handles=[l1, l2], labels=[&#x27;up&#x27;, &#x27;down&#x27;], loc=&#x27;best&#x27;)plt.show() 这样我们就能分别重新设置线条对应的 label 了. 其中’loc’参数有多种，’best’表示自动分配最佳位置，其余的如下： 1234567891011&#x27;best&#x27; : 0, &#x27;upper right&#x27; : 1,&#x27;upper left&#x27; : 2,&#x27;lower left&#x27; : 3,&#x27;lower right&#x27; : 4,&#x27;right&#x27; : 5,&#x27;center left&#x27; : 6,&#x27;center right&#x27; : 7,&#x27;lower center&#x27; : 8,&#x27;upper center&#x27; : 9,&#x27;center&#x27; : 10, Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Matplotlib Coordinate axis","slug":"python/matplotlib/py-matplotlib-4-coordinate_axis","date":"2018-01-23T06:08:21.000Z","updated":"2021-06-20T04:12:28.241Z","comments":true,"path":"2018/01/23/python/matplotlib/py-matplotlib-4-coordinate_axis/","link":"","permalink":"http://www.iequa.com/2018/01/23/python/matplotlib/py-matplotlib-4-coordinate_axis/","excerpt":"如何移动 matplotlib 中 axis 坐标轴的位置.","text":"如何移动 matplotlib 中 axis 坐标轴的位置. 设置名字和位置 12import matplotlib.pyplot as pltimport numpy as np 使用 np.linspace 定义 x ：范围是(-3,3);个数是50. 仿真一维数据组(x ,y1)表示曲线1. 仿真一维数据组(x ,y2)表示曲线2. 123x = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2 使用plt.figure定义一个图像窗口. 使用plt.plot画(x ,y2)曲线. 使用plt.plot画(x ,y1)曲线，曲线的颜色属性(color)为红色; 曲线的宽度(linewidth) 为 1.0; 曲线的类型(linestyle)为虚线. 使用plt.xlim设置x坐标轴范围: (-1, 2); 使用plt.ylim设置y坐标轴范围: (-2, 3); 12345plt.figure()plt.plot(x, y2)plt.plot(x, y1, color=&#x27;red&#x27;, linewidth=1.0, linestyle=&#x27;--&#x27;)plt.xlim((-1, 2))plt.ylim((-2, 3)) (-2, 3) 使用np.linspace定义范围以及个数：范围是(-1,2);个数是5. 使用plt.xticks设置x轴刻度：范围是(-1,2);个数是5. 使用plt.yticks设置y轴刻度以及名称: 刻度为[-2, -1.8, -1, 1.22, 3]; 对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 123new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)plt.yticks([-2, -1.8, -1, 1.22, 3],[&#x27;$really\\ bad$&#x27;, &#x27;$bad$&#x27;, &#x27;$normal$&#x27;, &#x27;$good$&#x27;, &#x27;$really\\ good$&#x27;]) function desc 设置效果 plt.gca 获取当前坐标轴信息 - .spines 设置边框 右侧边框 &amp; 上边框 .set_color 设置边框颜色 默认白色 1234ax = plt.gca()ax.spines[&#x27;right&#x27;].set_color(&#x27;none&#x27;)ax.spines[&#x27;top&#x27;].set_color(&#x27;none&#x27;)plt.show() 调整坐标轴 使用 .xaxis.set_ticks_position设置x坐标刻度数字或名称的位置：bottom.（所有位置：top，bottom，both，default，none） 使用 .spines 设置边框：x轴； 使用 .set_position 设置边框位置：y=0 的位置；（位置所有属性：outward，axes，data） 12345ax.xaxis.set_ticks_position(&#x27;bottom&#x27;)ax.spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;, 0))plt.show() 使用.yaxis.set_ticks_position设置y坐标刻度数字或名称的位置：left.（所有位置：left，right，both，default，none） 1ax.yaxis.set_ticks_position(&#x27;left&#x27;) 使用.spines设置边框：y轴; 使用.set_position设置边框位置：x=0的位置；（位置所有属性：outward，axes，data） 使用plt.show显示图像. 12ax.spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;,0))plt.show() Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"pickle","slug":"python/language/py-language-14-pickle","date":"2018-01-19T05:00:21.000Z","updated":"2021-06-20T04:12:28.226Z","comments":true,"path":"2018/01/19/python/language/py-language-14-pickle/","link":"","permalink":"http://www.iequa.com/2018/01/19/python/language/py-language-14-pickle/","excerpt":"Python 语言特定的序列化模块是pickle，但如果要把序列化搞得更通用、更符合Web标准，可以使用json模块","text":"Python 语言特定的序列化模块是pickle，但如果要把序列化搞得更通用、更符合Web标准，可以使用json模块 pickle 是一个 python 中, 压缩/保存/提取 文件的模块. 最一般的使用方式非常简单. pickle 保存 字典和列表都是能被保存的. 12345678import picklea_dict = &#123;&#x27;da&#x27;: 111, 2: [23,1,4], &#x27;23&#x27;: &#123;1:2,&#x27;d&#x27;:&#x27;sad&#x27;&#125;&#125;# pickle a variable to a filefile = open(&#x27;pickle_example.pickle&#x27;, &#x27;wb&#x27;)pickle.dump(a_dict, file)file.close() pickle.dump 你要保存的东西去这个打开的 file. 最后关闭 file 你就会发现你的文件目录里多了一个 pickle_example.pickle 文件, 这就是那个字典了. pickle 提取 12345# reload a file to a variablewith open(&#x27;pickle_example.pickle&#x27;, &#x27;rb&#x27;) as file: a_dict1 =pickle.load(file)print(a_dict1) &#123;'da': 111, 2: [23, 1, 4], '23': &#123;1: 2, 'd': 'sad'&#125;&#125; Reference docs.python.org python morvanzhou python liaoxuefeng","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Sklearn Save Model","slug":"python/sklearn/py-sklearn-7-save-model","date":"2018-01-10T05:17:21.000Z","updated":"2021-06-20T04:12:28.237Z","comments":true,"path":"2018/01/10/python/sklearn/py-sklearn-7-save-model/","link":"","permalink":"http://www.iequa.com/2018/01/10/python/sklearn/py-sklearn-7-save-model/","excerpt":"我们训练好了一个 Model 以后总需要保存和再次预测, 所以保存和读取我们的sklearn model也是同样重要的一步。 这次主要介绍两种保存Model的模块 pickle 与 joblib","text":"我们训练好了一个 Model 以后总需要保存和再次预测, 所以保存和读取我们的sklearn model也是同样重要的一步。 这次主要介绍两种保存Model的模块 pickle 与 joblib pickle 保存 首先简单建立与训练一个 SVC Model 1234567from sklearn import svmfrom sklearn import datasetsclf = svm.SVC()iris = datasets.load_iris()X, y = iris.data, iris.targetclf.fit(X,y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 使用 pickle 来保存与读取训练好的 Model 12345678910111213import pickle #pickle模块#保存Model(注:save文件夹要预先建立，否则会报错)with open(&#x27;save/clf.pickle&#x27;, &#x27;wb&#x27;) as f: pickle.dump(clf, f)#读取Modelwith open(&#x27;save/clf.pickle&#x27;, &#x27;rb&#x27;) as f: clf2 = pickle.load(f) #测试读取后的Model print(clf2.predict(X[0:1]))# [0] [0] joblib 保存 joblib 是 sklearn的外部模块 123456789101112from sklearn.externals import joblib #jbolib模块#保存Model(注:save文件夹要预先建立，否则会报错)joblib.dump(clf, &#x27;save/clf.pkl&#x27;)#读取Modelclf3 = joblib.load(&#x27;save/clf.pkl&#x27;)#测试读取后的Modelprint(clf3.predict(X[0:1]))# [0] [0] joblib 在使用上比较容易，读取速度也相对pickle快 Reference scikit-learn.org scikit-learn docs scikit-learn morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn Cross-validation 3","slug":"python/sklearn/py-sklearn-6-cross-validation-3","date":"2018-01-09T07:17:21.000Z","updated":"2021-06-20T04:12:28.235Z","comments":true,"path":"2018/01/09/python/sklearn/py-sklearn-6-cross-validation-3/","link":"","permalink":"http://www.iequa.com/2018/01/09/python/sklearn/py-sklearn-6-cross-validation-3/","excerpt":"交叉验证(cross validation)让我们知道在机器学习中验证是有多么的重要, 这一次的 sklearn 中我们用到了sklearn.learning_curve当中的另外一种, 叫做validation_curve,用这一种曲线我们就能更加直观看出改变模型中的参数的时候有没有过拟合(overfitting)的问题了. 这也是可以让我们更好的选择参数的方法.","text":"交叉验证(cross validation)让我们知道在机器学习中验证是有多么的重要, 这一次的 sklearn 中我们用到了sklearn.learning_curve当中的另外一种, 叫做validation_curve,用这一种曲线我们就能更加直观看出改变模型中的参数的时候有没有过拟合(overfitting)的问题了. 这也是可以让我们更好的选择参数的方法. validation_curve 检视过拟合 验证SVC中的一个参数 gamma 在什么范围内能使 model 产生好的结果. 以及过拟合和 gamma 取值的关系. 1234567891011121314151617181920212223242526272829303132from sklearn.learning_curve import validation_curve #validation_curve模块from sklearn.datasets import load_digits from sklearn.svm import SVC import matplotlib.pyplot as plt import numpy as np#digits数据集digits = load_digits()X = digits.datay = digits.target#建立参数测试集param_range = np.logspace(-6, -2.3, 5)#使用validation_curve快速找出参数对模型的影响train_loss, test_loss = validation_curve( SVC(), X, y, param_name=&#x27;gamma&#x27;, param_range=param_range, cv=10, scoring=&#x27;mean_squared_error&#x27;)#平均每一轮的平均方差train_loss_mean = -np.mean(train_loss, axis=1)test_loss_mean = -np.mean(test_loss, axis=1)#可视化图形plt.plot(param_range, train_loss_mean, &#x27;o-&#x27;, color=&quot;r&quot;, label=&quot;Training&quot;)plt.plot(param_range, test_loss_mean, &#x27;o-&#x27;, color=&quot;g&quot;, label=&quot;Cross-validation&quot;)plt.xlabel(&quot;gamma&quot;)plt.ylabel(&quot;Loss&quot;)plt.legend(loc=&quot;best&quot;)plt.show() 由图中可以明显看到 gamma 值大于 0.001，模型就会有过拟合(Overfitting)的问题。 Reference scikit-learn.org scikit-learn docs scikit-learn morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn Cross-validation 2","slug":"python/sklearn/py-sklearn-6-cross-validation-2","date":"2018-01-09T02:17:21.000Z","updated":"2021-06-20T04:12:28.234Z","comments":true,"path":"2018/01/09/python/sklearn/py-sklearn-6-cross-validation-2/","link":"","permalink":"http://www.iequa.com/2018/01/09/python/sklearn/py-sklearn-6-cross-validation-2/","excerpt":"Sklearn 中的 learning curve 可以很直观的看出我们的 model 学习的进度, 对比发现有没有 overfitting 的问题. 然后我们可以对我们的 model 进行调整, 克服 overfitting 的问题.","text":"Sklearn 中的 learning curve 可以很直观的看出我们的 model 学习的进度, 对比发现有没有 overfitting 的问题. 然后我们可以对我们的 model 进行调整, 克服 overfitting 的问题. Learning curve 检视过拟合 加载对应模块: 12345from sklearn.learning_curve import learning_curve #学习曲线模块from sklearn.datasets import load_digits #digits数据集from sklearn.svm import SVC #Support Vector Classifierimport matplotlib.pyplot as plt #可视化模块import numpy as np 加载digits数据集，其包含的是手写体的数字，从0到9。 数据集总共有1797个样本，每个样本由64个特征组成， 分别为其手写体对应的8×8像素表示，每个特征取值0~16。 12345digits = load_digits()X = digits.datay = digits.target#print(len(X[0])) 观察样本由小到大的学习曲线变化, 采用K折交叉验证 cv=10, 选择平均方差检视模型效能 scoring='mean_squared_error', 样本由小到大分成5轮检视学习曲线(10%, 25%, 50%, 75%, 100%): 1234567train_sizes, train_loss, test_loss = learning_curve( SVC(gamma=0.001), X, y, cv=10, scoring=&#x27;mean_squared_error&#x27;, train_sizes=[0.1, 0.25, 0.5, 0.75, 1])#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%)train_loss_mean = -np.mean(train_loss, axis=1)test_loss_mean = -np.mean(test_loss, axis=1) 可视化图形: 123456789plt.plot(train_sizes, train_loss_mean, &#x27;o-&#x27;, color=&quot;r&quot;, label=&quot;Training&quot;)plt.plot(train_sizes, test_loss_mean, &#x27;o-&#x27;, color=&quot;g&quot;, label=&quot;Cross-validation&quot;)plt.xlabel(&quot;Training examples&quot;)plt.ylabel(&quot;Loss&quot;)plt.legend(loc=&quot;best&quot;)plt.show() Reference scikit-learn.org scikit-learn docs scikit-learn morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn Cross-validation 1","slug":"python/sklearn/py-sklearn-6-cross-validation-1","date":"2018-01-08T10:17:21.000Z","updated":"2021-06-20T04:12:28.237Z","comments":true,"path":"2018/01/08/python/sklearn/py-sklearn-6-cross-validation-1/","link":"","permalink":"http://www.iequa.com/2018/01/08/python/sklearn/py-sklearn-6-cross-validation-1/","excerpt":"Sklearn 中的 Cross-validation 对于我们选择正确的 Model 和 Model 的参数是非常有用， 有了它我们能直观的看出不同 Model 或者参数对结构准确度的影响。","text":"Sklearn 中的 Cross-validation 对于我们选择正确的 Model 和 Model 的参数是非常有用， 有了它我们能直观的看出不同 Model 或者参数对结构准确度的影响。 Model 基础验证法 1234567891011121314151617181920from sklearn.datasets import load_iris # iris数据集from sklearn.model_selection import train_test_split # 分割数据模块from sklearn.neighbors import KNeighborsClassifier # K最近邻(kNN，k-NearestNeighbor)分类算法#加载iris数据集iris = load_iris()X = iris.datay = iris.target#分割数据并X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)#建立模型knn = KNeighborsClassifier()#训练模型knn.fit(X_train, y_train)#将准确率打印出print(knn.score(X_test, y_test)) 0.973684210526 可以看到基础验证的准确率为 0.973684210526 Model Cross Validation 12345678910from sklearn.cross_validation import cross_val_score # K折交叉验证模块#使用K折交叉验证模块scores = cross_val_score(knn, X, y, cv=5, scoring=&#x27;accuracy&#x27;)#将5次的预测准确率打印出print(scores)#将5次的预测准确平均率打印出print(scores.mean()) [ 0.96666667 1. 0.93333333 0.96666667 1. ] 0.973333333333 可以看到交叉验证的准确平均率为 0.973333333333 Aaccuracy 准确率判断 一般来说 准确率(accuracy) 会用于判断分类(Classification)模型的好坏 123456789101112131415161718import matplotlib.pyplot as plt #可视化模块#建立测试参数集k_range = range(1, 31)k_scores = []#藉由迭代的方式来计算不同参数对模型的影响，并返回交叉验证后的平均准确率for k in k_range: knn = KNeighborsClassifier(n_neighbors=k) scores = cross_val_score(knn, X, y, cv=10, scoring=&#x27;accuracy&#x27;) k_scores.append(scores.mean())#可视化数据plt.plot(k_range, k_scores)plt.xlabel(&#x27;Value of K for KNN&#x27;)plt.ylabel(&#x27;Cross-Validated Accuracy&#x27;)plt.show() 从图中得知，选择 12~18 的 k 值最好。高过 18 之后，准确率开始下降则是因为过拟合(Over fitting)的问题。 Mean squared error 一般来说平均方差(Mean squared error)会用于判断回归(Regression)模型的好坏 123456789101112import matplotlib.pyplot as pltk_range = range(1, 31)k_scores = []for k in k_range: knn = KNeighborsClassifier(n_neighbors=k) loss = -cross_val_score(knn, X, y, cv=10, scoring=&#x27;mean_squared_error&#x27;) k_scores.append(loss.mean())plt.plot(k_range, k_scores)plt.xlabel(&#x27;Value of K for KNN&#x27;)plt.ylabel(&#x27;Cross-Validated MSE&#x27;)plt.show() 由图可以得知，平均方差越低越好，因此选择13~18左右的K值会最好 Reference scikit-learn.org scikit-learn docs scikit-learn morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn Normalization","slug":"python/sklearn/py-sklearn-5-normalization","date":"2018-01-06T11:52:21.000Z","updated":"2021-06-20T04:12:28.236Z","comments":true,"path":"2018/01/06/python/sklearn/py-sklearn-5-normalization/","link":"","permalink":"http://www.iequa.com/2018/01/06/python/sklearn/py-sklearn-5-normalization/","excerpt":"Data Normalization 可以提升机器学习的成效","text":"Data Normalization 可以提升机器学习的成效 Normalization 12345678910from sklearn import preprocessing #标准化数据模块import numpy as np# 建立Arraya = np.array([[10, 2.7, 3.6], [-100, 5, -2], [120, 20, 40]], dtype=np.float64)# 将normalized后的a打印出print(preprocessing.scale(a)) [[ 0. -0.85170713 -0.55138018] [-1.22474487 -0.55187146 -0.852133 ] [ 1.22474487 1.40357859 1.40351318]] Normalization 对结果的影响 123456789101112131415# 标准化数据模块from sklearn import preprocessing import numpy as np# 将资料分割成train与test的模块from sklearn.model_selection import train_test_split# 生成适合做classification资料的模块from sklearn.datasets.samples_generator import make_classification # Support Vector Machine中的Support Vector Classifierfrom sklearn.svm import SVC # 可视化数据的模块import matplotlib.pyplot as plt 生成适合做 Classification 数据 12345678910111213141516# 生成具有2种属性的300笔数据X, y = make_classification( n_samples=300, n_features=2, n_redundant=0, n_informative=2, random_state=22, n_clusters_per_class=1, scale=100)# n_features 特征个数 = n_informative（） + n_redundant + n_repeated# n_informative 多信息特征的个数# n_redundant 冗余信息，informative 特征的随机线性组合# n_classes 分类类别# n_clusters_per_class 某一个类别是由几个 cluster 构成的#可视化数据plt.scatter(X[:, 0], X[:, 1], c=y)plt.show() data normalization before 1234X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)clf = SVC()clf.fit(X_train, y_train)print(clf.score(X_test, y_test)) 0.477777777778 data normalization after 数据的单位发生了变化, X 数据也被压缩到差不多大小范围. 123456X = preprocessing.scale(X)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)clf = SVC()clf.fit(X_train, y_train)print(clf.score(X_test, y_test))# 0.9 0.933333333333 Reference scikit-learn.org scikit-learn docs scikit-learn morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn General Learning Model","slug":"python/sklearn/py-sklearn-2-general-learning-model","date":"2018-01-05T07:05:21.000Z","updated":"2021-06-20T04:12:28.235Z","comments":true,"path":"2018/01/05/python/sklearn/py-sklearn-2-general-learning-model/","link":"","permalink":"http://www.iequa.com/2018/01/05/python/sklearn/py-sklearn-2-general-learning-model/","excerpt":"Sklearn 把所有机器学习的模式整合统一起来了，学会了一个模式就可以通吃其他不同类型的学习模式","text":"Sklearn 把所有机器学习的模式整合统一起来了，学会了一个模式就可以通吃其他不同类型的学习模式 使用分类器 Sklearn 本身就有很多数据库，可以用来练习。 以 Iris 的数据为例，这种花有四个属性，花瓣的长宽，茎的长宽，根据这些属性把花分为三类。 我们要用 分类器 去把四种类型的花分开。 今天用 KNN classifier，就是选择几个临近点，综合它们做个平均来作为预测值 导入模块 123from sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier 创建数据 加载 iris 的数据，把属性存在 X，类别标签存在 y： 1234iris = datasets.load_iris()iris_X = iris.datairis_y = iris.target 观察一下数据集，X 有四个属性，y 有 0，1，2 三类： 12print(iris_X[:2, :])print(iris_y) [[ 5.1 3.5 1.4 0.2] [ 4.9 3. 1.4 0.2]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] 把数据集分为训练集和测试集，其中 test_size=0.3，即测试集占总数据的 30%： 1X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3) 可以看到分开后的数据集，顺序也被打乱，这样更有利于学习模型： 1print(y_train) [0 0 2 2 1 0 2 0 1 1 0 2 1 2 2 0 0 1 0 1 0 2 1 1 1 2 2 1 0 0 2 2 2 2 2 1 0 0 0 0 1 2 1 2 1 0 2 1 2 2 2 1 0 1 2 1 0 0 2 1 1 0 2 2 0 2 1 0 0 2 0 0 0 1 2 0 1 1 2 2 0 1 0 2 2 0 1 0 0 1 2 1 1 2 2 1 1 0 0 2 0 0 1 1 0] 建立模型－训练－预测 定义模块方式 KNeighborsClassifier()， 用 fit 来训练 training data，这一步就完成了训练的所有步骤， 后面的 knn 就已经是训练好的模型，可以直接用来 predict 测试集的数据， 对比用模型预测的值与真实的值，可以看到大概模拟出了数据，但是有误差，是不会完完全全预测正确的 12345knn = KNeighborsClassifier()knn.fit(X_train, y_train)print(knn.predict(X_test))print(y_test) [1 0 1 0 1 2 0 1 0 1 2 1 1 1 2 2 1 2 0 1 2 0 0 2 1 2 1 1 0 1 1 0 2 2 2 0 1 0 2 0 2 0 1 2 1] [1 0 1 0 1 2 0 1 0 1 2 1 1 1 2 2 1 2 0 1 2 0 0 2 1 2 1 1 0 1 1 0 2 2 2 0 1 0 2 0 2 0 2 2 1] Reference scikit-learn.org scikit-learn docs scikit-learn morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn Common Attributes and Functions","slug":"python/sklearn/py-sklearn-4-common-attributes","date":"2018-01-05T06:15:21.000Z","updated":"2021-06-20T04:12:28.235Z","comments":true,"path":"2018/01/05/python/sklearn/py-sklearn-4-common-attributes/","link":"","permalink":"http://www.iequa.com/2018/01/05/python/sklearn/py-sklearn-4-common-attributes/","excerpt":"今天来看 Model 的属性和功能, 这里以 LinearRegressor 为例，所以先导入包，数据，还有模型","text":"今天来看 Model 的属性和功能, 这里以 LinearRegressor 为例，所以先导入包，数据，还有模型 123456789from sklearn import datasetsfrom sklearn.linear_model import LinearRegressionloaded_data = datasets.load_boston()data_X = loaded_data.datadata_y = loaded_data.targetmodel = LinearRegression() 训练和预测 接下来 model.fit 和 model.predict 就属于 Model 的功能，用来训练模型，用训练好的模型预测 123model.fit(data_X, data_y)print(model.predict(data_X[:4, :])) [ 30.00821269 25.0298606 30.5702317 28.60814055] 参数和分数 model.coef_ 和 model.intercept_ 属于 Model 的属性， 例如对于 LinearRegressor 这个模型，这两个属性分别输出模型的斜率和截距（与y轴的交点） 12print(model.coef_)print(model.intercept_) [ -1.07170557e-01 4.63952195e-02 2.08602395e-02 2.68856140e+00 -1.77957587e+01 3.80475246e+00 7.51061703e-04 -1.47575880e+00 3.05655038e-01 -1.23293463e-02 -9.53463555e-01 9.39251272e-03 -5.25466633e-01] 36.4911032804 model.get_params() 也是功能，它可以取出之前定义的参数 1print(model.get_params()) &#123;'copy_X': True, 'fit_intercept': True, 'n_jobs': 1, 'normalize': False&#125; model.score(data_X, data_y) 它可以对 Model 用 R^2 的方式进行打分，输出精确度。 关于 R^2 coefficient of determination 可以查看 Coefficient_of_determination 1print(model.score(data_X, data_y)) # R^2 coefficient of determination 0.740607742865 按标准的来说, 是要将数据分成训练数据和测试数据, 这里不是一个完整的测试, 只是展示 model 里面的一些属性. 正确率很少能真正100%, 取决于拟合度怎么样. 拟合度好, 正确率高 Reference scikit-learn.org scikit-learn docs scikit-learn morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn DataSets","slug":"python/sklearn/py-sklearn-3-database","date":"2018-01-03T10:05:21.000Z","updated":"2021-06-20T04:12:28.236Z","comments":true,"path":"2018/01/03/python/sklearn/py-sklearn-3-database/","link":"","permalink":"http://www.iequa.com/2018/01/03/python/sklearn/py-sklearn-3-database/","excerpt":"Sklearn 中的 data sets，很多而且有用，可以用来学习算法模型","text":"Sklearn 中的 data sets，很多而且有用，可以用来学习算法模型 要点 eg: boston 房价, 糖尿病, 数字, Iris 花。 也可以生成虚拟的数据，例如用来训练线性回归模型的数据，可以用函数来生成 例如，点击进入 boston 房价的数据，可以看到 sample 的总数，属性，以及 label 等信息 如果是自己生成数据，按照函数的形式，输入 sample，feature，target 的个数等等。 1sklearn.datasets.make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)[source] 接下来用代码练习… 导入模块 导入 datasets 包，以 Linear Regression 为例 123456from __future__ import print_functionfrom sklearn import datasetsfrom sklearn.linear_model import LinearRegressionimport matplotlib.pyplot as plt 导入数据－训练模型 用 datasets.load_boston() 的形式加载数据，并给 X 和 y 赋值，这种形式在 Sklearn 中都是高度统一的 1234567loaded_data = datasets.load_boston()data_X = loaded_data.datadata_y = loaded_data.targetprint(data_X[:4, 0]) # == print(data_X[:4][0])print(data_y[:4]) [ 0.00632 0.02731 0.02729 0.03237] [ 24. 21.6 34.7 33.4] 定义模型 可以直接用默认值去建立 model，默认值也不错，也可以自己改变参数使模型更好。 然后用 training data 训练模型 12model = LinearRegression()model.fit(data_X, data_y) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 再打印出预测值，这里用 X 的前 4 个来预测，同时打印真实值，作为对比，可以看到是有些误差的 12print(model.predict(data_X[:4, :]))print(data_y[:4]) [ 30.00821269 25.0298606 30.5702317 28.60814055] [ 24. 21.6 34.7 33.4] 为了提高准确度，可以通过尝试不同的 model，不同的参数，不同的预处理等方法，入门的话可以直接用默认值 创建虚拟数据－可视化 下面是创造数据的例子。 用函数来建立 100 个 sample，有一个 feature，和一个 target，这样比较方便可视化。 1X, y = datasets.make_regression(n_samples=100, n_features=1, n_targets=1, noise=3) 用 scatter 的形式来输出结果 12plt.scatter(X, y)plt.show() 可以看到用函数生成的 Linear Regression 用的数据。 noise 越大的话，点就会越来越离散，例如 noise 由 10 变为 50. 123X, y = datasets.make_regression(n_samples=100, n_features=1, n_targets=1, noise=50)plt.scatter(X, y)plt.show() Reference scikit-learn.org scikit-learn docs scikit-learn machine_learning_map","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn Choosing The Right Estimator","slug":"python/sklearn/py-sklearn-1-choosing-estimator","date":"2018-01-03T05:22:21.000Z","updated":"2021-06-20T04:12:28.236Z","comments":true,"path":"2018/01/03/python/sklearn/py-sklearn-1-choosing-estimator/","link":"","permalink":"http://www.iequa.com/2018/01/03/python/sklearn/py-sklearn-1-choosing-estimator/","excerpt":"Sciki-learn 选择学习方法，选择模型 流程图","text":"Sciki-learn 选择学习方法，选择模型 流程图 Sklearn 官网提供了一个流程图，蓝色圆圈内是判断条件，绿色方框内是可以选择的算法：详情 ![http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html]]img-1 从 START 开始，首先看数据的样本是否 &gt;50，小于则需要收集更多的数据。 由图中，可以看到算法有四类，分类，回归，聚类，降维。 algorithm desc 分类 监督式学习，即每个数据对应一个 label 回归 监督式学习，即每个数据对应一个 label 聚类 非监督式学习，即没有 label。 降维 当数据集有很多很多属性的时候，可以通过 降维 算法把属性归纳起来。 例如 20 个属性只变成 2 个，注意，这不是挑出 2 个，而是压缩成为 2 个，它们集合了 20 个属性的所有特征，相当于把重要的信息提取的更好，不重要的信息就不要了 然后看问题属于哪一类问题，是分类还是回归，还是聚类，就选择相应的算法。 当然还要考虑数据的大小，例如 100K 是一个阈值。 可以发现有些方法是既可以作为分类，也可以作为回归，例如 SGD。 Reference scikit-learn.org scikit-learn docs scikit-learn machine_learning_map","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"}]},{"title":"Sklearn Why ?","slug":"python/sklearn/py-sklearn-0-why","date":"2018-01-03T05:16:21.000Z","updated":"2021-06-20T04:12:28.238Z","comments":true,"path":"2018/01/03/python/sklearn/py-sklearn-0-why/","link":"","permalink":"http://www.iequa.com/2018/01/03/python/sklearn/py-sklearn-0-why/","excerpt":"Scikit learn 也简称 sklearn, 是机器学习领域当中最知名的 python 模块之一.","text":"Scikit learn 也简称 sklearn, 是机器学习领域当中最知名的 python 模块之一. Sklearn 包含了很多种机器学习的方式: Classification 分类 Regression 回归 Clustering 非监督分类 Dimensionality reduction 数据降维 Model Selection 模型选择 Preprocessing 数据预处理 我们总能够从这些方法中挑选出一个适合于自己问题的, 然后解决自己的问题. Reference scikit-learn.org scikit-learn docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Lucy地道美语 英语国际音标","slug":"English/IELTS/english-ielts-lucy-speaking","date":"2018-01-03T02:59:21.000Z","updated":"2021-06-20T04:12:28.299Z","comments":true,"path":"2018/01/03/English/IELTS/english-ielts-lucy-speaking/","link":"","permalink":"http://www.iequa.com/2018/01/03/English/IELTS/english-ielts-lucy-speaking/","excerpt":"一定要不断重复，不要相信自己的耳朵，要相信音标 最开始练习，一定要超级夸张，一个音一个音读","text":"一定要不断重复，不要相信自己的耳朵，要相信音标 最开始练习，一定要超级夸张，一个音一个音读 1. 分组元音 元音 1 desc sample 元音 2 desc sample i: 微笑音，嘴巴拉长 please、sleep i 军训音 big、pig e 一个手指音 egg、bed æ 大嘴巴音 /ɛ/+/a:/ bad、dad a: 尾部儿话~ car, star ɒ 嘴张开、无变化 not、lost、job ɔ: 有点像 ‘沃’,多儿话 horse、short ʊ 发音靠后，脖子下 good、should、would、could u: 悟空的‘悟’拉长 school、room ʌ 短元音 love、come ɜ: - bird、first ə - about、asleep、China eɪ - baby、face aɪ - hi aʊ - cow、how ɪə® - ear、dear eə - air、hair əʊ - boat、coat ɔɪ - boy、joy ʊə - sure /[ʃʊə®]/ China ə 词尾变音 2. 清浊辅音 清辅音 desc sample 浊辅音 desc sample p pen、map b big、job k book、week g egg、dog f wife、knife v love、give s 舌头放嘴里 sink、sick、month、nice、rice z his、size ʃ wish、dish ʒ usually、vision θ 用力吐舌头 think、thick、mouth、month、bath ð with、smooth r sorry、story h hand、home tʃ teach、lunch dʒ page、large tr tree、trip dr dress、dry ts cats、hats dz beds、cards m mom、some n pen、son、sun ŋ drink l let、ball w why、water j year、yellow clothes [kləʊðz] 3. Part of Speech","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"Matplotlib Figure","slug":"python/matplotlib/py-matplotlib-3-figure-image","date":"2018-01-01T05:08:21.000Z","updated":"2021-06-20T04:12:28.246Z","comments":true,"path":"2018/01/01/python/matplotlib/py-matplotlib-3-figure-image/","link":"","permalink":"http://www.iequa.com/2018/01/01/python/matplotlib/py-matplotlib-3-figure-image/","excerpt":"matplotlib 的 figure 就是一个 单独的 figure 小窗口, 小窗口里面还可以有更多的小图片","text":"matplotlib 的 figure 就是一个 单独的 figure 小窗口, 小窗口里面还可以有更多的小图片 12import matplotlib.pyplot as pltimport numpy as np 使用np.linspace定义x：范围是(-3,3);个数是50. 仿真一维数据组(x ,y1)表示曲线1. 仿真一维数据组(x ,y2)表示曲线2. 123x = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2 使用plt.figure定义一个图像窗口. 使用plt.plot画(x ,y1)曲线. 123plt.figure()plt.plot(x, y1)plt.show() 使用plt.figure定义一个图像窗口：编号为3；大小为(8, 5). 使用plt.plot画(x ,y2)曲线. 使用plt.plot画(x ,y1)曲线，曲线的颜色属性(color)为红色;曲线的宽度(linewidth)为1.0；曲线的类型(linestyle)为虚线. 使用plt.show显示图像. 1234plt.figure(num=3, figsize=(8, 5),)plt.plot(x, y2)plt.plot(x, y1, color=&#x27;red&#x27;, linewidth=1.0, linestyle=&#x27;--&#x27;)plt.show() Set Coordinate axis 使用plt.xlim设置x坐标轴范围：(-1, 2)； 使用plt.ylim设置y坐标轴范围：(-2, 3)； 使用plt.xlabel设置x坐标轴名称：’I am x’； 使用plt.ylabel设置y坐标轴名称：’I am y’； 123456789plt.figure(num=3, figsize=(8, 5),)plt.plot(x, y2)plt.plot(x, y1, color=&#x27;red&#x27;, linewidth=1.0, linestyle=&#x27;--&#x27;)plt.xlim((-1, 2))plt.ylim((-2, 3))plt.xlabel(&#x27;I am x&#x27;)plt.ylabel(&#x27;I am y&#x27;)plt.show() 使用 np.linspace 定义范围以及个数：范围是(-1,2);个数是5. 使用 print 打印出新定义的范围. 使用 plt.xticks 设置x轴刻度：范围是(-1,2);个数是5. 123456789101112plt.figure(num=3, figsize=(8, 5),)plt.plot(x, y2)plt.plot(x, y1, color=&#x27;red&#x27;, linewidth=1.0, linestyle=&#x27;--&#x27;)plt.xlim((-1, 2))plt.ylim((-2, 3))plt.xlabel(&#x27;I am x&#x27;)plt.ylabel(&#x27;I am y&#x27;)new_ticks = np.linspace(-1, 2, 5)print(new_ticks)plt.xticks(new_ticks) [-1. -0.25 0.5 1.25 2. ] ([&lt;matplotlib.axis.XTick at 0x11dd68ba8&gt;, &lt;matplotlib.axis.XTick at 0x11dd68a58&gt;, &lt;matplotlib.axis.XTick at 0x11dd7d5c0&gt;, &lt;matplotlib.axis.XTick at 0x11e04af28&gt;, &lt;matplotlib.axis.XTick at 0x11e0515c0&gt;], &lt;a list of 5 Text xticklabel objects&gt;) 使用plt.yticks设置y轴刻度以及名称：刻度为[-2, -1.8, -1, 1.22, 3]；对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 使用plt.show显示图像. 12plt.yticks([-2, -1.8, -1, 1.22, 3],[r&#x27;$really\\ bad$&#x27;, r&#x27;$bad$&#x27;, r&#x27;$normal$&#x27;, r&#x27;$good$&#x27;, r&#x27;$really\\ good$&#x27;])plt.show() Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Matplotlib Basic Use","slug":"python/matplotlib/py-matplotlib-2-basic-use","date":"2018-01-01T04:46:21.000Z","updated":"2021-06-20T04:12:28.238Z","comments":true,"path":"2018/01/01/python/matplotlib/py-matplotlib-2-basic-use/","link":"","permalink":"http://www.iequa.com/2018/01/01/python/matplotlib/py-matplotlib-2-basic-use/","excerpt":"Matplotlib 最基本的使用介绍","text":"Matplotlib 最基本的使用介绍 基础应用 使用import导入模块matplotlib.pyplot，并简写成plt 使用import导入模块numpy，并简写成np 12import matplotlib.pyplot as pltimport numpy as np 1np.linspace(-1, 1, 50) array([-1. , -0.95918367, -0.91836735, -0.87755102, -0.83673469, -0.79591837, -0.75510204, -0.71428571, -0.67346939, -0.63265306, -0.59183673, -0.55102041, -0.51020408, -0.46938776, -0.42857143, -0.3877551 , -0.34693878, -0.30612245, -0.26530612, -0.2244898 , -0.18367347, -0.14285714, -0.10204082, -0.06122449, -0.02040816, 0.02040816, 0.06122449, 0.10204082, 0.14285714, 0.18367347, 0.2244898 , 0.26530612, 0.30612245, 0.34693878, 0.3877551 , 0.42857143, 0.46938776, 0.51020408, 0.55102041, 0.59183673, 0.63265306, 0.67346939, 0.71428571, 0.75510204, 0.79591837, 0.83673469, 0.87755102, 0.91836735, 0.95918367, 1. ]) 使用 np.linspace 定义x：范围是(-1,1); 个数是50. 仿真一维数据组(x ,y)表示曲线1. 12x = np.linspace(-1, 1, 50)y = 2*x + 1 使用plt.figure定义一个图像窗口. 使用plt.plot画(x ,y)曲线. 使用plt.show显示图像. 123plt.figure()plt.plot(x, y)plt.show() Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Matplotlib Why ?","slug":"python/matplotlib/py-matplotlib-1-why","date":"2018-01-01T04:12:21.000Z","updated":"2021-06-20T04:12:28.241Z","comments":true,"path":"2018/01/01/python/matplotlib/py-matplotlib-1-why/","link":"","permalink":"http://www.iequa.com/2018/01/01/python/matplotlib/py-matplotlib-1-why/","excerpt":"Matplotlib 是一个非常强大的 Python 画图工具","text":"Matplotlib 是一个非常强大的 Python 画图工具 Matplotlib 它能帮你画出美丽的: 线图; 散点图; 等高线图; 条形图; 柱状图; 3D 图形, 甚至是图形动画等等. 下面是一些例图: Reference matplotlib.org matplotlib docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"}]},{"title":"Pandas Matplotlib Intro","slug":"python/numpy-pandas/py-pandas-8-matplotlib","date":"2017-12-31T03:12:21.000Z","updated":"2021-06-20T04:12:28.220Z","comments":true,"path":"2017/12/31/python/numpy-pandas/py-pandas-8-matplotlib/","link":"","permalink":"http://www.iequa.com/2017/12/31/python/numpy-pandas/py-pandas-8-matplotlib/","excerpt":"matplotlib 将数据可视化. 仅仅是用来 show 图片的, 即 plt.show()","text":"matplotlib 将数据可视化. 仅仅是用来 show 图片的, 即 plt.show() 123import pandas as pdimport numpy as npimport matplotlib.pyplot as plt 创建一个Series 这是一个线性的数据，我们随机生成1000个数据，Series 默认的 index 就是从0开始的整数 1234567# 随机生成1000个数据data = pd.Series(np.random.randn(1000),index=np.arange(1000)) # pandas 数据可以直接观看其可视化形式data.plot()plt.show() 可以使用 plt.plot(x=, y=)，把x,y的数据作为参数存进去，但是data本来就是一个数据，所以我们可以直接plot Dataframe 可视化 我们生成一个 1000*4 的 DataFrame，并对他们累加 123456789data = pd.DataFrame( np.random.randn(1000,4), index=np.arange(1000), columns=list(&quot;ABCD&quot;) )#data.cumsum()print(data)data.plot()plt.show() A B C D 0 1.163604 -0.689103 1.958018 0.241444 1 0.595765 0.816026 1.573164 -0.443003 2 -0.101446 0.768321 -0.203069 -0.638841 3 -0.439233 -0.161273 0.398774 1.309622 4 -0.524647 -0.180073 -1.499978 0.628436 5 -0.305683 0.668840 0.243668 -1.386839 .. ... ... ... ... 998 -0.243955 -0.190122 -0.299633 3.350200 999 -0.055184 0.936187 0.146156 0.604271 [1000 rows x 4 columns] 这个就是我们刚刚生成的4个column的数据，因为有4组数据，所以4组数据会分别plot出来。 plot 可以指定很多参数，具体参见官方文档 除了plot，我经常会用到还有scatter，这个会显示散点图，首先说一下在 pandas 中有多少种方法 bar hist box kde area scatter hexbin 主要说一下 plot 和 scatter. 因为 scatter 只有 x，y 两个属性，可以分别给 x, y 指定数据 1ax = data.plot.scatter(x=&#x27;A&#x27;,y=&#x27;B&#x27;,color=&#x27;DarkBlue&#x27;,label=&#x27;Class1&#x27;) 然后我们在可以再画一个在同一个ax上面，选择不一样的数据列，不同的 color 和 label 123# 将之下这个 data 画在上一个 ax 上面data.plot.scatter(x=&#x27;A&#x27;,y=&#x27;C&#x27;,color=&#x27;LightGreen&#x27;,label=&#x27;Class2&#x27;,ax=ax)plt.show() 两种呈现方式，一种是线性的方式，一种是散点图 Reference pandas.pydata.org pandas docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"}]},{"title":"Pandas Merge","slug":"python/numpy-pandas/py-pandas-7-merge","date":"2017-12-31T02:12:21.000Z","updated":"2021-06-20T04:12:28.209Z","comments":true,"path":"2017/12/31/python/numpy-pandas/py-pandas-7-merge/","link":"","permalink":"http://www.iequa.com/2017/12/31/python/numpy-pandas/py-pandas-7-merge/","excerpt":"pandas中的merge和concat类似,但主要是用于两组有key column的数据,统一索引的数据.","text":"pandas中的merge和concat类似,但主要是用于两组有key column的数据,统一索引的数据. 依据一组key合并 12345678910111213141516171819202122232425262728import pandas as pd#定义资料集并打印出left = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;, &#x27;K3&#x27;], &#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;], &#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;, &#x27;B3&#x27;]&#125;)right = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;, &#x27;K3&#x27;], &#x27;C&#x27;: [&#x27;C0&#x27;, &#x27;C1&#x27;, &#x27;C2&#x27;, &#x27;C3&#x27;], &#x27;D&#x27;: [&#x27;D0&#x27;, &#x27;D1&#x27;, &#x27;D2&#x27;, &#x27;D3&#x27;]&#125;)# print(left)# A B key# 0 A0 B0 K0# 1 A1 B1 K1# 2 A2 B2 K2# 3 A3 B3 K3# print(right)# C D key# 0 C0 D0 K0# 1 C1 D1 K1# 2 C2 D2 K2# 3 C3 D3 K3#依据key column合并，并打印出res = pd.merge(left, right, on=&#x27;key&#x27;)print(res) A B key C D 0 A0 B0 K0 C0 D0 1 A1 B1 K1 C1 D1 2 A2 B2 K2 C2 D2 3 A3 B3 K3 C3 D3 依据两组key合并 合并时有4种方法how = ['left', 'right', 'outer', 'inner']，预设值how='inner' 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import pandas as pd#定义资料集并打印出left = pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;], &#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;], &#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;], &#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;, &#x27;B3&#x27;]&#125;)right = pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;], &#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;], &#x27;C&#x27;: [&#x27;C0&#x27;, &#x27;C1&#x27;, &#x27;C2&#x27;, &#x27;C3&#x27;], &#x27;D&#x27;: [&#x27;D0&#x27;, &#x27;D1&#x27;, &#x27;D2&#x27;, &#x27;D3&#x27;]&#125;)print(left)# A B key1 key2# 0 A0 B0 K0 K0# 1 A1 B1 K0 K1# 2 A2 B2 K1 K0# 3 A3 B3 K2 K1print(right)# C D key1 key2# 0 C0 D0 K0 K0# 1 C1 D1 K1 K0# 2 C2 D2 K1 K0# 3 C3 D3 K2 K0#依据key1与key2 columns进行合并，并打印出四种结果[&#x27;left&#x27;, &#x27;right&#x27;, &#x27;outer&#x27;, &#x27;inner&#x27;]res = pd.merge(left, right, on=[&#x27;key1&#x27;, &#x27;key2&#x27;], how=&#x27;inner&#x27;)print(res)# A B key1 key2 C D# 0 A0 B0 K0 K0 C0 D0# 1 A2 B2 K1 K0 C1 D1# 2 A2 B2 K1 K0 C2 D2res = pd.merge(left, right, on=[&#x27;key1&#x27;, &#x27;key2&#x27;], how=&#x27;outer&#x27;)print(res)# A B key1 key2 C D# 0 A0 B0 K0 K0 C0 D0# 1 A1 B1 K0 K1 NaN NaN# 2 A2 B2 K1 K0 C1 D1# 3 A2 B2 K1 K0 C2 D2# 4 A3 B3 K2 K1 NaN NaN# 5 NaN NaN K2 K0 C3 D3res = pd.merge(left, right, on=[&#x27;key1&#x27;, &#x27;key2&#x27;], how=&#x27;left&#x27;)print(res)# A B key1 key2 C D# 0 A0 B0 K0 K0 C0 D0# 1 A1 B1 K0 K1 NaN NaN# 2 A2 B2 K1 K0 C1 D1# 3 A2 B2 K1 K0 C2 D2# 4 A3 B3 K2 K1 NaN NaNres = pd.merge(left, right, on=[&#x27;key1&#x27;, &#x27;key2&#x27;], how=&#x27;right&#x27;)print(res)# A B key1 key2 C D# 0 A0 B0 K0 K0 C0 D0# 1 A2 B2 K1 K0 C1 D1# 2 A2 B2 K1 K0 C2 D2# 3 NaN NaN K2 K0 C3 D3 Indicator indicator=True会将合并的记录放在新的一列 12345678910111213141516171819202122232425262728293031323334import pandas as pd#定义资料集并打印出df1 = pd.DataFrame(&#123;&#x27;col1&#x27;:[0,1], &#x27;col_left&#x27;:[&#x27;a&#x27;,&#x27;b&#x27;]&#125;)df2 = pd.DataFrame(&#123;&#x27;col1&#x27;:[1,2,2],&#x27;col_right&#x27;:[2,2,2]&#125;)print(df1)# col1 col_left# 0 0 a# 1 1 bprint(df2)# col1 col_right# 0 1 2# 1 2 2# 2 2 2# 依据col1进行合并，并启用indicator=True，最后打印出res = pd.merge(df1, df2, on=&#x27;col1&#x27;, how=&#x27;outer&#x27;, indicator=True)print(res)# col1 col_left col_right _merge# 0 0.0 a NaN left_only# 1 1.0 b 2.0 both# 2 2.0 NaN 2.0 right_only# 3 2.0 NaN 2.0 right_only# 自定indicator column的名称，并打印出res = pd.merge(df1, df2, on=&#x27;col1&#x27;, how=&#x27;outer&#x27;, indicator=&#x27;indicator_column&#x27;)print(res)# col1 col_left col_right indicator_column# 0 0.0 a NaN left_only# 1 1.0 b 2.0 both# 2 2.0 NaN 2.0 right_only# 3 2.0 NaN 2.0 right_only col1 col_left 0 0 a 1 1 b col1 col_right 0 1 2 1 2 2 2 2 2 col1 col_left col_right _merge 0 0 a NaN left_only 1 1 b 2.0 both 2 2 NaN 2.0 right_only 3 2 NaN 2.0 right_only col1 col_left col_right indicator_column 0 0 a NaN left_only 1 1 b 2.0 both 2 2 NaN 2.0 right_only 3 2 NaN 2.0 right_only 依据index合并 12345678910111213141516171819202122232425262728293031323334353637import pandas as pd#定义资料集并打印出left = pd.DataFrame(&#123;&#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;], &#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;]&#125;, index=[&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;])right = pd.DataFrame(&#123;&#x27;C&#x27;: [&#x27;C0&#x27;, &#x27;C2&#x27;, &#x27;C3&#x27;], &#x27;D&#x27;: [&#x27;D0&#x27;, &#x27;D2&#x27;, &#x27;D3&#x27;]&#125;, index=[&#x27;K0&#x27;, &#x27;K2&#x27;, &#x27;K3&#x27;])print(left)# A B# K0 A0 B0# K1 A1 B1# K2 A2 B2print(right)# C D# K0 C0 D0# K2 C2 D2# K3 C3 D3#依据左右资料集的index进行合并，how=&#x27;outer&#x27;,并打印出res = pd.merge(left, right, left_index=True, right_index=True, how=&#x27;outer&#x27;)print(res)# A B C D# K0 A0 B0 C0 D0# K1 A1 B1 NaN NaN# K2 A2 B2 C2 D2# K3 NaN NaN C3 D3#依据左右资料集的index进行合并，how=&#x27;inner&#x27;,并打印出res = pd.merge(left, right, left_index=True, right_index=True, how=&#x27;inner&#x27;)print(res)# A B C D# K0 A0 B0 C0 D0# K2 A2 B2 C2 D2 A B K0 A0 B0 K1 A1 B1 K2 A2 B2 C D K0 C0 D0 K2 C2 D2 K3 C3 D3 A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 K3 NaN NaN C3 D3 A B C D K0 A0 B0 C0 D0 K2 A2 B2 C2 D2 解决overlapping的问题 12345678910111213import pandas as pd#定义资料集boys = pd.DataFrame(&#123;&#x27;k&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;], &#x27;age&#x27;: [1, 2, 3]&#125;)girls = pd.DataFrame(&#123;&#x27;k&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K3&#x27;], &#x27;age&#x27;: [4, 5, 6]&#125;)print(boys)print(girls)#使用suffixes解决overlapping的问题res = pd.merge(boys, girls, on=&#x27;k&#x27;, suffixes=[&#x27;_boy&#x27;, &#x27;_girl&#x27;], how=&#x27;inner&#x27;)print(res) age k 0 1 K0 1 2 K1 2 3 K2 age k 0 4 K0 1 5 K0 2 6 K3 age_boy k age_girl 0 1 K0 4 1 1 K0 5 pandas 也有 join 和 merge 是类似的，如需要使用，请参考官方文档 Reference pandas.pydata.org pandas docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"}]},{"title":"Pandas Concat、Join、join_axes、append","slug":"python/numpy-pandas/py-pandas-6-concat-join-append","date":"2017-12-31T01:22:21.000Z","updated":"2021-06-20T04:12:28.214Z","comments":true,"path":"2017/12/31/python/numpy-pandas/py-pandas-6-concat-join-append/","link":"","permalink":"http://www.iequa.com/2017/12/31/python/numpy-pandas/py-pandas-6-concat-join-append/","excerpt":"pandas 处理多组数据的时候往往会要用到数据的合并处理,使用 concat 是一种基本的合并方式.而且 concat 中有很多参数可以调整,合并成你想要的数据形式.","text":"pandas 处理多组数据的时候往往会要用到数据的合并处理,使用 concat 是一种基本的合并方式.而且 concat 中有很多参数可以调整,合并成你想要的数据形式. axis (合并方向) axis=0 是预设值，因此未设定任何参数时，默认axis=0 12345678910111213import pandas as pdimport numpy as np#定义资料集df1 = pd.DataFrame(np.ones((3,4))*0, columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])df2 = pd.DataFrame(np.ones((3,4))*1, columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])df3 = pd.DataFrame(np.ones((3,4))*2, columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])#concat纵向合并res = pd.concat([df1, df2, df3], axis=0)#打印结果print(res) a b c d 0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0 1.0 1.0 1.0 1.0 1 1.0 1.0 1.0 1.0 2 1.0 1.0 1.0 1.0 0 2.0 2.0 2.0 2.0 1 2.0 2.0 2.0 2.0 2 2.0 2.0 2.0 2.0 仔细观察会发现结果的index是0, 1, 2, 0, 1, 2, 0, 1, 2，若要将index重置，请看例子二。 ignore_index (重置 index) 12345#承上一个例子，并将index_ignore设定为Trueres = pd.concat([df1, df2, df3], axis=0, ignore_index=True)#打印结果print(res) a b c d 0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 3 1.0 1.0 1.0 1.0 4 1.0 1.0 1.0 1.0 5 1.0 1.0 1.0 1.0 6 2.0 2.0 2.0 2.0 7 2.0 2.0 2.0 2.0 8 2.0 2.0 2.0 2.0 结果的index变 0, 1, 2, 3, 4, 5, 6, 7, 8 join (合并方式) join='outer' 为预设值，未设定任何参数时，函数默认 join='outer'。此方式是依照column来做纵向合并，有相同的column上下合并在一起，其他独自的column个自成列，原本没有值的位置皆以NaN填充。 1234567891011import pandas as pdimport numpy as np#定义资料集df1 = pd.DataFrame(np.ones((3,4))*0, columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;], index=[1,2,3])df2 = pd.DataFrame(np.ones((3,4))*1, columns=[&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;,&#x27;e&#x27;], index=[2,3,4])#纵向&quot;外&quot;合并df1与df2res = pd.concat([df1, df2], axis=0, join=&#x27;outer&#x27;)print(res) a b c d e 1 0.0 0.0 0.0 0.0 NaN 2 0.0 0.0 0.0 0.0 NaN 3 0.0 0.0 0.0 0.0 NaN 2 NaN 1.0 1.0 1.0 1.0 3 NaN 1.0 1.0 1.0 1.0 4 NaN 1.0 1.0 1.0 1.0 只有相同的column合并在一起，其他的会被抛弃 12345678#承上一个例子#纵向&quot;内&quot;合并df1与df2res = pd.concat([df1, df2], axis=0, join=&#x27;inner&#x27;)#重置index并打印结果res = pd.concat([df1, df2], axis=0, join=&#x27;inner&#x27;, ignore_index=True)print(res) b c d 0 0.0 0.0 0.0 1 0.0 0.0 0.0 2 0.0 0.0 0.0 3 1.0 1.0 1.0 4 1.0 1.0 1.0 5 1.0 1.0 1.0 join_axes (依照 axes 合并) 1234567891011121314151617181920import pandas as pdimport numpy as np#定义资料集df1 = pd.DataFrame(np.ones((3,4))*0, columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;], index=[1,2,3])df2 = pd.DataFrame(np.ones((3,4))*1, columns=[&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;,&#x27;e&#x27;], index=[2,3,4])#依照`df1.index`进行横向合并res = pd.concat([df1, df2], axis=1, join_axes=[df1.index])#打印结果print(res)# a b c d b c d e# 1 0.0 0.0 0.0 0.0 NaN NaN NaN NaN# 2 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0# 3 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0#移除join_axes，并打印结果res = pd.concat([df1, df2], axis=1)print(res) a b c d b c d e 1 0.0 0.0 0.0 0.0 NaN NaN NaN NaN 2 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 3 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 a b c d b c d e 1 0.0 0.0 0.0 0.0 NaN NaN NaN NaN 2 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 3 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 4 NaN NaN NaN NaN 1.0 1.0 1.0 1.0 append (添加数据) append只有纵向合并，没有横向合并 123456789101112131415161718192021222324252627282930313233343536373839404142import pandas as pdimport numpy as np#定义资料集df1 = pd.DataFrame(np.ones((3,4))*0, columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])df2 = pd.DataFrame(np.ones((3,4))*1, columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])df3 = pd.DataFrame(np.ones((3,4))*1, columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])s1 = pd.Series([1,2,3,4], index=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])#将df2合并到df1的下面，以及重置index，并打印出结果res = df1.append(df2, ignore_index=True)print(res)# a b c d# 0 0.0 0.0 0.0 0.0# 1 0.0 0.0 0.0 0.0# 2 0.0 0.0 0.0 0.0# 3 1.0 1.0 1.0 1.0# 4 1.0 1.0 1.0 1.0# 5 1.0 1.0 1.0 1.0#合并多个df，将df2与df3合并至df1的下面，以及重置index，并打印出结果res = df1.append([df2, df3], ignore_index=True)print(res)# a b c d# 0 0.0 0.0 0.0 0.0# 1 0.0 0.0 0.0 0.0# 2 0.0 0.0 0.0 0.0# 3 1.0 1.0 1.0 1.0# 4 1.0 1.0 1.0 1.0# 5 1.0 1.0 1.0 1.0# 6 1.0 1.0 1.0 1.0# 7 1.0 1.0 1.0 1.0# 8 1.0 1.0 1.0 1.0#合并series，将s1合并至df1，以及重置index，并打印出结果res = df1.append(s1, ignore_index=True)print(res)# a b c d# 0 0.0 0.0 0.0 0.0# 1 0.0 0.0 0.0 0.0# 2 0.0 0.0 0.0 0.0# 3 1.0 2.0 3.0 4.0 a b c d 0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 3 1.0 1.0 1.0 1.0 4 1.0 1.0 1.0 1.0 5 1.0 1.0 1.0 1.0 a b c d 0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 3 1.0 1.0 1.0 1.0 4 1.0 1.0 1.0 1.0 5 1.0 1.0 1.0 1.0 6 1.0 1.0 1.0 1.0 7 1.0 1.0 1.0 1.0 8 1.0 1.0 1.0 1.0 a b c d 0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 3 1.0 2.0 3.0 4.0 Reference pandas.pydata.org pandas docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"}]},{"title":"Pandas IO","slug":"python/numpy-pandas/py-pandas-5-import-output","date":"2017-12-30T13:15:21.000Z","updated":"2021-06-20T04:12:28.208Z","comments":true,"path":"2017/12/30/python/numpy-pandas/py-pandas-5-import-output/","link":"","permalink":"http://www.iequa.com/2017/12/30/python/numpy-pandas/py-pandas-5-import-output/","excerpt":"pandas 可以读取与存取的资料格式有很多种，像 csv、excel、json、html 与 pickle 等… 详细请看官方文档","text":"pandas 可以读取与存取的资料格式有很多种，像 csv、excel、json、html 与 pickle 等… 详细请看官方文档 读取csv 示范档案下载 - student.csv 1234567import pandas as pd #加载模块#读取csvdata = pd.read_csv(&#x27;students.csv&#x27;)#打印出dataprint(data) Student ID name age gender 0 1100 Kelly 22 Female 1 1101 Clo 21 Female 2 1102 Tilly 22 Female 3 1103 Tony 24 Male 4 1104 David 20 Male 5 1105 Catty 22 Female 6 1106 M 3 Female 7 1107 N 43 Male 8 1108 A 13 Male 9 1109 S 12 Male 10 1110 David 33 Male 11 1111 Dw 3 Female 12 1112 Q 23 Male 13 1113 W 21 Female 将资料存取成pickle 1data.to_pickle(&#x27;student.pickle&#x27;) Reference pandas.pydata.org pandas docs morvanzhou pandas IO Tools","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"}]},{"title":"Pandas Deal NaN Value","slug":"python/numpy-pandas/py-pandas-4-deal-NaN-value","date":"2017-12-30T12:56:21.000Z","updated":"2021-06-20T04:12:28.210Z","comments":true,"path":"2017/12/30/python/numpy-pandas/py-pandas-4-deal-NaN-value/","link":"","permalink":"http://www.iequa.com/2017/12/30/python/numpy-pandas/py-pandas-4-deal-NaN-value/","excerpt":"处理 NaN 数据, 一些 空 或者 NaN 数据, 如何删除或者填补这些 NaN 数据.","text":"处理 NaN 数据, 一些 空 或者 NaN 数据, 如何删除或者填补这些 NaN 数据. 创建含 NaN 的矩阵 建立了一个6X4的矩阵数据并且把两个位置置为空. 12345678import pandas as pdimport numpy as npdates = pd.date_range(&#x27;20130101&#x27;, periods=6)df = pd.DataFrame(np.arange(24).reshape((6,4)),index=dates, columns=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;])df.iloc[0,1] = np.nandf.iloc[1,2] = np.nanprint(df) A B C D 2013-01-01 0 NaN 2.0 3 2013-01-02 4 5.0 NaN 7 2013-01-03 8 9.0 10.0 11 2013-01-04 12 13.0 14.0 15 2013-01-05 16 17.0 18.0 19 2013-01-06 20 21.0 22.0 23 pd.dropna() 如果想直接去掉有 NaN 的行或列, 可以使用 dropna 12345df1 = df.dropna( axis=0, # 0: 对行进行操作; 1: 对列进行操作 how=&#x27;any&#x27; # &#x27;any&#x27;: 只要存在 NaN 就 drop 掉; &#x27;all&#x27;: 必须全部是 NaN 才 drop ) print(df1) A B C D 2013-01-03 8 9.0 10.0 11 2013-01-04 12 13.0 14.0 15 2013-01-05 16 17.0 18.0 19 2013-01-06 20 21.0 22.0 23 pd.fillna() 如果是将 NaN 的值用其他值代替, 比如代替成 0: 12df2 = df.fillna(value=0)print(df2) A B C D 2013-01-01 0 0.0 2.0 3 2013-01-02 4 5.0 0.0 7 2013-01-03 8 9.0 10.0 11 2013-01-04 12 13.0 14.0 15 2013-01-05 16 17.0 18.0 19 2013-01-06 20 21.0 22.0 23 pd.isnull() 判断是否有缺失数据 NaN, 为 True 表示缺失数据: 1print(df.isnull()) A B C D 2013-01-01 False True False False 2013-01-02 False False True False 2013-01-03 False False False False 2013-01-04 False False False False 2013-01-05 False False False False 2013-01-06 False False False False 检测在数据中是否存在 NaN, 如果存在就返回 True: 12np.any(df.isnull()) == True # True True Reference pandas.pydata.org pandas docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"}]},{"title":"Pandas Set Value","slug":"python/numpy-pandas/py-pandas-3-set-value","date":"2017-12-30T12:11:21.000Z","updated":"2021-06-20T04:12:28.215Z","comments":true,"path":"2017/12/30/python/numpy-pandas/py-pandas-3-set-value/","link":"","permalink":"http://www.iequa.com/2017/12/30/python/numpy-pandas/py-pandas-3-set-value/","excerpt":"我们可以根据自己的需求, 用 pandas 进行更改数据里面的值, 或者加上一些空的,或者有数值的列.","text":"我们可以根据自己的需求, 用 pandas 进行更改数据里面的值, 或者加上一些空的,或者有数值的列. 创建数据 首先建立了一个 6X4 的矩阵数据 123456import pandas as pdimport numpy as npdates = pd.date_range(&#x27;20130101&#x27;, periods=6) # 2013-01-01 结果一样df = pd.DataFrame(np.arange(24).reshape((6,4)),index=dates, columns=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;])print(df) A B C D 2013-01-01 0 1 2 3 2013-01-02 4 5 6 7 2013-01-03 8 9 10 11 2013-01-04 12 13 14 15 2013-01-05 16 17 18 19 2013-01-06 20 21 22 23 设置 loc 和 iloc 我们可以利用索引或者标签确定需要修改值的位置 123df.iloc[2,2] = 1111df.loc[&#x27;2013-01-01&#x27;,&#x27;B&#x27;] = 2222print(df) A B C D 2013-01-01 0 2222 2 3 2013-01-02 4 5 6 7 2013-01-03 8 0 1111 11 2013-01-04 12 0 14 15 2013-01-05 16 0 18 19 2013-01-06 20 0 22 23 根据条件设置 如果现在的判断条件是这样, 我们想要更改B中的数, 而更改的位置是取决于 A 的. 对于A大于4的位置. 更改B在相应位置上的数为0 12df.B[df.A&gt;4] = 0print(df) A B C D 2013-01-01 0 2222 2 3 2013-01-02 4 5 6 7 2013-01-03 8 0 1111 11 2013-01-04 12 0 14 15 2013-01-05 16 0 18 19 2013-01-06 20 0 22 23 按行或列设置 如果对整列做批处理, 加上一列 ‘F’, 并将 F 列全改为 NaN, 如下: 12df[&#x27;F&#x27;] = np.nanprint(df) A B C D F 2013-01-01 0 2222 2 3 NaN 2013-01-02 4 5 6 7 NaN 2013-01-03 8 0 1111 11 NaN 2013-01-04 12 0 14 15 NaN 2013-01-05 16 0 18 19 NaN 2013-01-06 20 0 22 23 NaN 添加数据 用上面的方法也可以加上 Series 序列（但是长度必须对齐） 12df[&#x27;E&#x27;] = pd.Series([1,2,3,4,5,6], index=pd.date_range(&#x27;20130101&#x27;,periods=6)) print(df) A B C D F E 2013-01-01 0 2222 2 3 NaN 1 2013-01-02 4 5 6 7 NaN 2 2013-01-03 8 0 1111 11 NaN 3 2013-01-04 12 0 14 15 NaN 4 2013-01-05 16 0 18 19 NaN 5 2013-01-06 20 0 22 23 NaN 6 这样我们大概学会了如何对 DataFrame 中在自己想要的地方赋值或者增加数据。 Reference pandas.pydata.org pandas docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"}]},{"title":"Pandas Select Data","slug":"python/numpy-pandas/py-pandas-2-select-data","date":"2017-12-28T15:28:21.000Z","updated":"2021-06-20T04:12:28.208Z","comments":true,"path":"2017/12/28/python/numpy-pandas/py-pandas-2-select-data/","link":"","permalink":"http://www.iequa.com/2017/12/28/python/numpy-pandas/py-pandas-2-select-data/","excerpt":"pandas 选择数据, 首先我们建立了一个 6X4 的矩阵数据","text":"pandas 选择数据, 首先我们建立了一个 6X4 的矩阵数据 123456import pandas as pdimport numpy as npdates = pd.date_range(&#x27;20130101&#x27;, periods=6)df = pd.DataFrame(np.arange(24).reshape((6,4)),index=dates, columns=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;])print(df) A B C D 2013-01-01 0 1 2 3 2013-01-02 4 5 6 7 2013-01-03 8 9 10 11 2013-01-04 12 13 14 15 2013-01-05 16 17 18 19 2013-01-06 20 21 22 23 简单的筛选 如果我们想选取 DataFrame 中的数据，下面描述了两种途径, 他们都能达到同一个目的： 12print(df[&#x27;A&#x27;])print(df.A) 2013-01-01 0 2013-01-02 4 2013-01-03 8 2013-01-04 12 2013-01-05 16 2013-01-06 20 Freq: D, Name: A, dtype: int64 2013-01-01 0 2013-01-02 4 2013-01-03 8 2013-01-04 12 2013-01-05 16 2013-01-06 20 Freq: D, Name: A, dtype: int64 让选择跨越多行或多列: 1print(df[0:3]) A B C D 2013-01-01 0 1 2 3 2013-01-02 4 5 6 7 2013-01-03 8 9 10 11 1print(df[3:3]) Empty DataFrame Columns: [A, B, C, D] Index: [] 1print(df[&#x27;20130102&#x27;:&#x27;20130104&#x27;]) A B C D 2013-01-02 4 5 6 7 2013-01-03 8 9 10 11 2013-01-04 12 13 14 15 如果 df[3:3] 将会是一个空对象。后者选择 20130102 到 20130104 标签之间的数据，并且包括这两个标签 根据标签 loc 可以使用标签来选择数据 loc, 本例子主要通过标签名字选择某一行数据， 或者通过选择某行或者所有行（:代表所有行）然后选其中某一列或几列数据 : 1print(df.loc[&#x27;20130102&#x27;]) A 4 B 5 C 6 D 7 Name: 2013-01-02 00:00:00, dtype: int64 1print(df.loc[:,[&#x27;A&#x27;,&#x27;B&#x27;]]) A B 2013-01-01 0 1 2013-01-02 4 5 2013-01-03 8 9 2013-01-04 12 13 2013-01-05 16 17 2013-01-06 20 21 1print(df.loc[&#x27;20130102&#x27;,[&#x27;A&#x27;,&#x27;B&#x27;]]) A 4 B 5 Name: 2013-01-02 00:00:00, dtype: int64 根据序列 iloc 可以采用位置进行选择 iloc, 在这里我们可以通过位置选择在不同情况下所需要的数据例如选某一个，连续选或者跨行选等操作。 1print(df.iloc[3,1]) 13 1print(df.iloc[3:5,1:3]) B C 2013-01-04 13 14 2013-01-05 17 18 1print(df.iloc[[1,3,5],1:3]) B C 2013-01-02 5 6 2013-01-04 13 14 2013-01-06 21 22 在这里我们可以通过位置选择在不同情况下所需要的数据, 例如选某一个，连续选或者跨行选等操作。 根据混合两种 ix 当然我们可以采用混合选择 ix, 其中选择’A’和’C’的两列，并选择前三行的数据。 1print(df.ix[:3,[&#x27;A&#x27;,&#x27;C&#x27;]]) A C 2013-01-01 0 2 2013-01-02 4 6 2013-01-03 8 10 /Users/blair/.pyenv/versions/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: .ix is deprecated. Please use .loc for label based indexing or .iloc for positional indexing See the documentation here: http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated &quot;&quot;&quot;Entry point for launching an IPython kernel. 通过判断的筛选 最后我们可以采用判断指令 (Boolean indexing) 进行选择. 我们可以约束某项条件然后选择出当前所有数据. 12print(df[df.A&gt;8])df.A&gt;8 A B C D 2013-01-04 12 13 14 15 2013-01-05 16 17 18 19 2013-01-06 20 21 22 23 2013-01-01 False 2013-01-02 False 2013-01-03 False 2013-01-04 True 2013-01-05 True 2013-01-06 True Freq: D, Name: A, dtype: bool 下节我们将会讲到Pandas中如何设置值。 Reference pandas.pydata.org pandas docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"}]},{"title":"Pandas Basic Intro","slug":"python/numpy-pandas/py-pandas-1-intro","date":"2017-12-27T08:28:21.000Z","updated":"2021-06-20T04:12:28.207Z","comments":true,"path":"2017/12/27/python/numpy-pandas/py-pandas-1-intro/","link":"","permalink":"http://www.iequa.com/2017/12/27/python/numpy-pandas/py-pandas-1-intro/","excerpt":"如果用列表和字典来作比较, 那么可以说 Numpy 是列表形式的，没有数值标签，而 Pandas 就是字典形式","text":"如果用列表和字典来作比较, 那么可以说 Numpy 是列表形式的，没有数值标签，而 Pandas 就是字典形式 Pandas是基于Numpy构建的，让Numpy为中心的应用变得更加简单。 要使用pandas，首先需要了解他主要两个数据结构：Series 和 DataFrame。 Series 123456import pandas as pdimport numpy as nps = pd.Series([1,3,6,np.nan,44,1])print(s) 0 1.0 1 3.0 2 6.0 3 NaN 4 44.0 5 1.0 dtype: float64 Series 的字符串表现形式为：索引在左边，值在右边。 由于我们没有为数据指定索引。于是会自动创建一个0到N-1（N为长度）的整数型索引。 DataFrame 矩阵创建 1234dates = pd.date_range(&#x27;20160101&#x27;,periods=6)df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])print(df) a b c d 2016-01-01 -0.186992 0.228857 0.572464 -0.842974 2016-01-02 -0.689623 -1.491299 0.647805 0.819846 2016-01-03 -1.294425 0.138935 1.729793 -1.270880 2016-01-04 0.088744 0.745256 0.380425 0.048070 2016-01-05 0.003135 -2.240388 0.188038 -0.069044 2016-01-06 -1.358217 -0.820133 1.606467 -1.622589 DataFrame 是一个表格型的数据结构，它包含有一组有序的列，每列可以是不同的值类型（数值，字符串，布尔值等）。 DataFrame 既有行索引也有列索引， 它可以被看做由 Series 组成的大字典。 我们可以根据每一个不同的索引来挑选数据, 比如挑选 b 的元素: 1print(df[&#x27;b&#x27;]) 2016-01-01 0.228857 2016-01-02 -1.491299 2016-01-03 0.138935 2016-01-04 0.745256 2016-01-05 -2.240388 2016-01-06 -0.820133 Freq: D, Name: b, dtype: float64 我们在创建一组没有给定行标签和列标签的数据 df1: 12df1 = pd.DataFrame(np.arange(12).reshape((3,4)))print(df1) 0 1 2 3 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 这样,他就会采取默认的从0开始 index. DataFrame 字典创建 还有一种生成 df 的方法, 如下 df2: 12345678df2 = pd.DataFrame(&#123;&#x27;A&#x27; : [1, 3, 7, 5], &#x27;B&#x27; : pd.Timestamp(&#x27;20130102&#x27;), &#x27;C&#x27; : pd.Series(1,index=list(range(4)),dtype=&#x27;float32&#x27;), &#x27;D&#x27; : np.array([3] * 4,dtype=&#x27;int32&#x27;), &#x27;E&#x27; : pd.Categorical([&quot;test&quot;,&quot;train&quot;,&quot;test&quot;,&quot;train&quot;]), &#x27;F&#x27; : &#x27;foo&#x27;&#125;) print(df2) A B C D E F 0 1 2013-01-02 1.0 3 test foo 1 3 2013-01-02 1.0 3 train foo 2 7 2013-01-02 1.0 3 test foo 3 5 2013-01-02 1.0 3 train foo 这种方法能对每一列的数据进行特殊对待. 如果想要查看数据中的类型, 我们可以用 dtype 这个属性: 1print(df2.dtypes) A int64 B datetime64[ns] C float32 D int32 E category F object dtype: object 如果想看对列的序号: 1print(df2.index) Int64Index([0, 1, 2, 3], dtype='int64') 同样, 每种数据的名称也能看到: 1print(df2.columns) Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object') 如果只想看所有 df2 的值: 1print(df2.values) [[1 Timestamp('2013-01-02 00:00:00') 1.0 3 'test' 'foo'] [3 Timestamp('2013-01-02 00:00:00') 1.0 3 'train' 'foo'] [7 Timestamp('2013-01-02 00:00:00') 1.0 3 'test' 'foo'] [5 Timestamp('2013-01-02 00:00:00') 1.0 3 'train' 'foo']] 想知道数据的总结, 可以用 describe(): 1df2.describe() A C D count 4.000000 4.0 4.0 mean 4.000000 1.0 3.0 std 2.581989 0.0 0.0 min 1.000000 1.0 3.0 25% 2.500000 1.0 3.0 50% 4.000000 1.0 3.0 75% 5.500000 1.0 3.0 max 7.000000 1.0 3.0 如果想翻转数据, transpose: 1print(df2.T) 0 1 2 \\ A 1 3 7 B 2013-01-02 00:00:00 2013-01-02 00:00:00 2013-01-02 00:00:00 C 1 1 1 D 3 3 3 E test train test F foo foo foo 3 A 5 B 2013-01-02 00:00:00 C 1 D 3 E train F foo 如果想对数据的 index 进行排序并输出: 123print(df2.sort_index(axis=1, ascending=False)) # 对列名称进行排序，索引名，倒排序print(df2.sort_index(axis=0, ascending=False)) # 对列名称进行排序，索引名，倒排序 F E D C B A 0 foo test 3 1.0 2013-01-02 1 1 foo train 3 1.0 2013-01-02 3 2 foo test 3 1.0 2013-01-02 7 3 foo train 3 1.0 2013-01-02 5 A B C D E F 3 5 2013-01-02 1.0 3 train foo 2 7 2013-01-02 1.0 3 test foo 1 3 2013-01-02 1.0 3 train foo 0 1 2013-01-02 1.0 3 test foo 如果是对数据 值 排序输出: 1print(df2.sort_values(by=&#x27;E&#x27;)) A B C D E F 0 1 2013-01-02 1.0 3 test foo 2 7 2013-01-02 1.0 3 test foo 1 3 2013-01-02 1.0 3 train foo 3 5 2013-01-02 1.0 3 train foo Reference pandas.pydata.org pandas docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"}]},{"title":"Numpy copy & deep copy","slug":"python/numpy-pandas/py-numpy-8-copy-deep-copy","date":"2017-12-27T06:28:21.000Z","updated":"2021-06-20T04:12:28.218Z","comments":true,"path":"2017/12/27/python/numpy-pandas/py-numpy-8-copy-deep-copy/","link":"","permalink":"http://www.iequa.com/2017/12/27/python/numpy-pandas/py-numpy-8-copy-deep-copy/","excerpt":"numpy copy &amp; deep copy","text":"numpy copy &amp; deep copy = 的赋值方式会带有关联性 12345678import numpy as npa = np.arange(4)# array([0, 1, 2, 3])b = ac = ad = b 改变a的第一个值，b、c、d的第一个值也会同时改变。 123a[0] = 11print(a)# array([11, 1, 2, 3]) [11 1 2 3] 确认b、c、d是否与a相同 123b is a # Truec is a # Trued is a # True copy() 的赋值方式没有关联性 12345b = a.copy() # deep copyprint(b) # array([11, 22, 33, 3])a[3] = 44print(a) # array([11, 22, 33, 44])print(b) # array([11, 22, 33, 3]) [11 1 2 3] [11 1 2 44] [11 1 2 3] Reference numpy.org numpy docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"}]},{"title":"Numpy Array Split","slug":"python/numpy-pandas/py-numpy-7-Split","date":"2017-12-27T05:28:21.000Z","updated":"2021-06-20T04:12:28.207Z","comments":true,"path":"2017/12/27/python/numpy-pandas/py-numpy-7-Split/","link":"","permalink":"http://www.iequa.com/2017/12/27/python/numpy-pandas/py-numpy-7-Split/","excerpt":"Numpy array 横向分割、纵向分割、等量分割、非等量分割","text":"Numpy array 横向分割、纵向分割、等量分割、非等量分割 创建数据 1234567891011import numpy as npA = np.arange(12).reshape((3, 4))&quot;&quot;&quot;array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&quot;&quot;&quot;print(A) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 纵向分割 12345678print(np.split(A, 2, axis=1))&quot;&quot;&quot;[array([[0, 1], [4, 5], [8, 9]]), array([[ 2, 3], [ 6, 7], [10, 11]])]&quot;&quot;&quot; 横向分割 123print(np.split(A, 3, axis=0))# [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] 不等量的分割 在机器学习时经常会需要将数据做不等量的分割，因此解决办法为np.array_split() 12345678910print(np.array_split(A, 3, axis=1))&quot;&quot;&quot;[array([[0, 1], [4, 5], [8, 9]]), array([[ 2], [ 6], [10]]), array([[ 3], [ 7], [11]])]&quot;&quot;&quot; 成功将Array不等量分割! 其他的分割方式 在Numpy里还有np.vsplit()与横np.hsplit()方式可用。 12345678910111213print(np.vsplit(A, 3)) #等于 print(np.split(A, 3, axis=0))# [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])]print(np.hsplit(A, 2)) #等于 print(np.split(A, 2, axis=1))&quot;&quot;&quot;[array([[0, 1], [4, 5], [8, 9]]), array([[ 2, 3], [ 6, 7], [10, 11]])]&quot;&quot;&quot; Reference numpy.org numpy docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"}]},{"title":"Numpy Array Merge","slug":"python/numpy-pandas/py-numpy-6-Array-Merge","date":"2017-12-26T09:28:21.000Z","updated":"2021-06-20T04:12:28.211Z","comments":true,"path":"2017/12/26/python/numpy-pandas/py-numpy-6-Array-Merge/","link":"","permalink":"http://www.iequa.com/2017/12/26/python/numpy-pandas/py-numpy-6-Array-Merge/","excerpt":"对于一个array的合并，我们可以想到按行、按列等多种方式进行合并","text":"对于一个array的合并，我们可以想到按行、按列等多种方式进行合并 np.vstack() 123456789101112import numpy as npA = np.array([1,1,1])B = np.array([2,2,2]) print(np.vstack((A,B))) # vertical stack&quot;&quot;&quot;[[1,1,1] [2,2,2]]&quot;&quot;&quot;np.vstack((A,B)) [[1 1 1] [2 2 2]] array([[1, 1, 1], [2, 2, 2]]) vertical stack 本身属于一种上下合并，即对括号中的两个整体进行对应操作。此时我们对组合而成的矩阵进行属性探究： 1234C = np.vstack((A,B)) print(A.shape,C.shape)# (3,) (2,3) # A 是序列, 序列合并后 C 为矩阵 (3,) (2, 3) 利用shape函数可以让我们很容易地知道A和C的属性，从打印出的结果来看，A仅仅是一个拥有3项元素的数组（数列），而合并后得到的C是一个2行3列的矩阵。 np.hstack() 左右合并： 1234567D = np.hstack((A,B)) # horizontal stackprint(D)# [1,1,1,2,2,2]print(A.shape,D.shape)# (3,) (6,) [1 1 1 2 2 2] (3,) (6,) 不能用 A.T 这样的方法，将一个序列变为矩阵 12print(A.T)print(A.T.shape) [1 1 1] (3,) 通过打印出的结果可以看出：D本身来源于A，B两个数列的左右合并，而且新生成的D本身也是一个含有6项元素的序列。 np.newaxis() 如果面对如同前文所述的A序列， 转置操作便很有可能无法对其进行转置（因为A并不是矩阵的属性），此时就需要我们借助其他的函数操作进行转置： 123456789101112131415print(A[np.newaxis,:])# [[1 1 1]]print(A[np.newaxis,:].shape)# (1,3)print(A[:,np.newaxis])&quot;&quot;&quot;[[1][1][1]]&quot;&quot;&quot;print(A[:,np.newaxis].shape)# (3,1) [[1 1 1]] (1, 3) [[1] [1] [1]] (3, 1) 此时我们便将具有3个元素的array转换为了1行3列以及3行1列的矩阵了。 综合总结 12345678910111213141516import numpy as npA = np.array([1,1,1])[:,np.newaxis]B = np.array([2,2,2])[:,np.newaxis] C = np.vstack((A,B)) # vertical stackD = np.hstack((A,B)) # horizontal stackprint(D)&quot;&quot;&quot;[[1 2][1 2][1 2]]&quot;&quot;&quot;print(A.shape,D.shape)# (3,1) (3,2) [[1 2] [1 2] [1 2]] (3, 1) (3, 2) np.concatenate() 当你的合并操作需要针对多个矩阵或序列时，借助concatenate函数可能会让你使用起来比前述的函数更加方便： 1234567891011121314151617181920212223242526C = np.concatenate((A,B,B,A),axis=0)print(C)&quot;&quot;&quot;array([[1], [1], [1], [2], [2], [2], [2], [2], [2], [1], [1], [1]])&quot;&quot;&quot;D = np.concatenate((A,B,B,A),axis=1)print(D)&quot;&quot;&quot;array([[1, 2, 2, 1], [1, 2, 2, 1], [1, 2, 2, 1]])&quot;&quot;&quot; [[1 2 2 1] [1 2 2 1] [1 2 2 1]] axis 参数很好的控制了矩阵的纵向或是横向打印，相比较 vstack 和 hstack 函数显得更加方便。 Reference numpy.org numpy docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"}]},{"title":"Numpy Index","slug":"python/numpy-pandas/py-numpy-5-Index","date":"2017-12-26T06:28:21.000Z","updated":"2021-06-20T04:12:28.207Z","comments":true,"path":"2017/12/26/python/numpy-pandas/py-numpy-5-Index/","link":"","permalink":"http://www.iequa.com/2017/12/26/python/numpy-pandas/py-numpy-5-Index/","excerpt":"在元素列表或者数组中，我们可以用如同 a[2] 一样的表示方法，同样的，Numpy中也有相应的表示方法","text":"在元素列表或者数组中，我们可以用如同 a[2] 一样的表示方法，同样的，Numpy中也有相应的表示方法 一维索引 123456import numpy as npA = np.arange(3,15)# array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) print(A[3]) # 6 6 让我们将矩阵转换为二维的，此时进行同样的操作： 123456789A = np.arange(3,15).reshape((3,4))&quot;&quot;&quot;array([[ 3, 4, 5, 6] [ 7, 8, 9, 10] [11, 12, 13, 14]])&quot;&quot;&quot; print(A[2]) # [11 12 13 14] [11 12 13 14] 实际上这时的 A[2] 对应的就是 矩阵A 中第三行(从0开始算第一行)的所有元素。 二维索引 12print(A[1][1]) # 8print(A[1, 1]) # 8 8 8 在Python的 list 中，我们可以利用:对一定范围内的元素进行切片操作，在Numpy中我们依然可以给出相应的方法： 1print(A[1, 1:3]) # [8 9] [8 9] 这一表示形式即针对第二行中第2到第4列元素进行切片输出（不包含第4列）。 此时我们适当的利用for函数进行打印： 1234567for row in A: print(row)&quot;&quot;&quot; [ 3, 4, 5, 6][ 7, 8, 9, 10][11, 12, 13, 14]&quot;&quot;&quot; 此时它会逐行进行打印操作。如果想进行逐列打印，就需要稍稍变化一下： 12345678for column in A.T: print(column)&quot;&quot;&quot; [ 3, 7, 11][ 4, 8, 12][ 5, 9, 13][ 6, 10, 14]&quot;&quot;&quot; 说一些关于迭代输出的问题： 12345678910111213import numpy as npA = np.arange(3,15).reshape((3,4)) print(A.flatten()) # array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])for item in A.flat: print(item) # 3# 4# ……# 14 [ 3 4 5 6 7 8 9 10 11 12 13 14] 3 4 5 6 7 8 9 10 11 12 13 14 这一脚本中的flatten是一个展开性质的函数，将多维的矩阵进行展开成1行的数列。而flat是一个迭代器，本身是一个object属性。 Reference numpy.org numpy docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"}]},{"title":"Numpy Basic Operation 2","slug":"python/numpy-pandas/py-numpy-4-basic-operation-2","date":"2017-12-25T06:43:21.000Z","updated":"2021-06-20T04:12:28.209Z","comments":true,"path":"2017/12/25/python/numpy-pandas/py-numpy-4-basic-operation-2/","link":"","permalink":"http://www.iequa.com/2017/12/25/python/numpy-pandas/py-numpy-4-basic-operation-2/","excerpt":"numpy 矩阵的基本操作，argmin/argmax、mean/average、cumsum、sort、transpose/A.T、clip","text":"numpy 矩阵的基本操作，argmin/argmax、mean/average、cumsum、sort、transpose/A.T、clip argmin &amp; argmax 矩阵对应元素的索引也是非常重要的 其中的 argmin() 和 argmax() 两个函数分别对应着求矩阵中最小元素和最大元素的索引。相应的，在矩阵的12个元素中，最小值即2，对应索引0，最大值为13，对应索引为11。 123456789import numpy as npA = np.arange(2,14).reshape((3,4)) # array([[ 2, 3, 4, 5]# [ 6, 7, 8, 9]# [10,11,12,13]]) print(np.argmin(A)) # 0print(np.argmax(A)) # 11 0 11 mean &amp; average 统计中的均值，可以利用下面的方式，将整个矩阵的均值求出来： 12print(np.mean(A)) # 7.5print(np.average(A)) # 7.5 7.5 7.5 仿照着前一节中dot() 的使用法则，mean()函数还有另外一种写法： 1print(A.mean()) # 7.5 7.5 cumsum 和matlab中的cumsum()累加函数类似，Numpy中也具有cumsum()函数，其用法如下： 123print(np.cumsum(A)) # [2 5 9 14 20 27 35 44 54 65 77 90] [ 2 5 9 14 20 27 35 44 54 65 77 90] 在cumsum()函数中：生成的每一项矩阵元素均是从原矩阵首项累加到对应项的元素之和。比如元素9，在cumsum()生成的矩阵中序号为3，即原矩阵中2，3，4三个元素的和。 diff 相应的有累差运算函数： 123456print(np.diff(A)) # [[1 1 1]# [1 1 1]# [1 1 1]] [[1 1 1] [1 1 1] [1 1 1]] 该函数计算的便是每一行中后一项与前一项之差。故一个3行4列矩阵通过函数计算得到的矩阵便是3行3列的矩阵。 nonzero() 函数, 觉得用处不大未学。 sort 123456789101112import numpy as npA = np.arange(14,2, -1).reshape((3,4)) # array([[14, 13, 12, 11],# [10, 9, 8, 7],# [ 6, 5, 4, 3]])print(np.sort(A)) # array([[11,12,13,14]# [ 7, 8, 9,10]# [ 3, 4, 5, 6]]) [[11 12 13 14] [ 7 8 9 10] [ 3 4 5 6]] transpose &amp; A.T 矩阵的转置有两种表示方法： 123456789101112print(np.transpose(A)) print(A.T)# array([[14,10, 6]# [13, 9, 5]# [12, 8, 4]# [11, 7, 3]])# array([[14,10, 6]# [13, 9, 5]# [12, 8, 4]# [11, 7, 3]]) [[14 10 6] [13 9 5] [12 8 4] [11 7 3]] [[14 10 6] [13 9 5] [12 8 4] [11 7 3]] clip 特别的，在Numpy中具有clip()函数，例子如下： 12345678910print(A)# array([[14,13,12,11]# [10, 9, 8, 7]# [ 6, 5, 4, 3]])print(np.clip(A,5,9)) # array([[ 9, 9, 9, 9]# [ 9, 9, 8, 7]# [ 6, 5, 5, 5]]) [[14 13 12 11] [10 9 8 7] [ 6 5 4 3]] [[9 9 9 9] [9 9 8 7] [6 5 5 5]] 这个函数的格式是clip(Array,Array_min,Array_max)，顾名思义，Array指的是将要被执行用的矩阵，而后面的最小值最大值则用于让函数判断矩阵中元素是否有比最小值小的或者比最大值大的元素，并将这些指定的元素转换为最小值或者最大值。 实际上每一个Numpy中大多数函数均具有很多变量可以操作，你可以指定行、列甚至某一范围中的元素。更多具体的使用细节请记得查阅Numpy官方文档。 Reference numpy.org numpy docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"}]},{"title":"Numpy Basic Operation 1","slug":"python/numpy-pandas/py-numpy-3-basic-operation-1","date":"2017-12-25T03:43:21.000Z","updated":"2021-06-20T04:12:28.216Z","comments":true,"path":"2017/12/25/python/numpy-pandas/py-numpy-3-basic-operation-1/","link":"","permalink":"http://www.iequa.com/2017/12/25/python/numpy-pandas/py-numpy-3-basic-operation-1/","excerpt":"numpy 矩阵的基本运算，加减乘除、数学函数、最大最小值、axis 查找 等","text":"numpy 矩阵的基本运算，加减乘除、数学函数、最大最小值、axis 查找 等 numpy 生成矩阵 1234import numpy as npa=np.array([10,20,30,40]) # array([10, 20, 30, 40])b=np.arange(4) # array([0, 1, 2, 3])b array([0, 1, 2, 3]) numpy 加减乘除 上述代码中的 a 和 b 是两个属性为 array 也就是矩阵的变量，而且二者都是1行4列的矩阵， 其中b矩阵中的元素分别是从0到3。 123456789c=a-b # array([10, 19, 28, 37])print(c)c=a+b # array([10, 21, 32, 43])print(c)c=a*b # array([ 0, 20, 60, 120])print(c)c=b**2 # array([0, 1, 4, 9])print(c) [10 19 28 37] [10 21 32 43] [ 0 20 60 120] [0 1 4 9] numpy 数学函数 numpy 三角函数等，当我们需要对矩阵中每一项元素进行函数运算时，可以很简便的调用它们（以sin函数为例）： 123c=10*np.sin(a) # array([-5.44021111, 9.12945251, -9.88031624, 7.4511316 ])print(c) [-5.44021111 9.12945251 -9.88031624 7.4511316 ] print 函数进行逻辑判断 12print(b&lt;3) # array([ True, True, True, False], dtype=bool) [ True True True False] 上述运算均是建立在一维矩阵，即只有一行的矩阵上面的计算，如果我们想要对多行多维度的矩阵进行操作，需要对开始的脚本进行一些修改： 123456789101112a=np.array([[1,1],[0,1]])b=np.arange(4).reshape((2,2))print(a)# array([[1, 1],# [0, 1]])print(b)# array([[0, 1],# [2, 3]])print(b &gt; 2) [[1 1] [0 1]] [[0 1] [2 3]] [[False False] [False True]] numpy 两种矩阵乘法 此时构造出来的矩阵a和b便是2行2列的，其中 reshape 操作是对矩阵的形状进行重构， 其重构的形状便是括号中给出的数字。 稍显不同的是，Numpy 中的矩阵乘法分为两种， 其一是前文中的对应元素相乘，其二是标准的矩阵乘法运算，即对应行乘对应列得到相应元素： 12345678print(a)print(b)c_dot = np.dot(a,b)# array([[2, 4],# [2, 3]])print(&quot;========&quot;)print(c_dot) [[1 1] [0 1]] [[0 1] [2 3]] ======== [[2 4] [2 3]] 除此之外还有另外的一种关于dot的表示方法，即： 123c_dot_2 = a.dot(b)# array([[2, 4],# [2, 3]]) sum(), min(), max() 下面我们将重新定义一个脚本, 来看看关于 sum(), min(), max() 的使用： 12345import numpy as npa=np.random.random((2,4))print(a)# array([[ 0.94692159, 0.20821798, 0.35339414, 0.2805278 ],# [ 0.04836775, 0.04023552, 0.44091941, 0.21665268]]) [[ 0.38281924 0.40654978 0.69744113 0.90707595] [ 0.40572074 0.652105 0.24226191 0.95931459]] 因为是随机生成数字, 所以你的结果可能会不一样. 在第二行中对a的操作是令a中生成一个2行4列的矩阵，且每一元素均是来自从0到1的随机数。 在这个随机生成的矩阵中，我们可以对元素进行求和以及寻找极值的操作，具体如下： 123np.sum(a) # 4.6532883360785817np.min(a) # 0.24226191007863129np.max(a) # 0.95931458707579575 0.95931458707579575 axis 进行赋值 对应的便是对矩阵中所有元素进行求和，寻找最小值，寻找最大值的操作。 可以通过print()函数对相应值进行打印检验。 如果你需要对行或者列进行查找运算，就需要在上述代码中为 axis 进行赋值。 当axis的值为0的时候，将会以列作为查找单元， 当axis的值为1的时候，将会以行作为查找单元。 为了更加清晰，在刚才的例子中我们继续进行查找： 123456789101112print(&quot;a =&quot;,a)# a = [[ 0.23651224 0.41900661 0.84869417 0.46456022]# [ 0.60771087 0.9043845 0.36603285 0.55746074]]print(&quot;sum =&quot;,np.sum(a,axis=1))# sum = [ 1.96877324 2.43558896]print(&quot;min =&quot;,np.min(a,axis=0))# min = [ 0.23651224 0.41900661 0.36603285 0.46456022]print(&quot;max =&quot;,np.max(a,axis=1))# max = [ 0.84869417 0.9043845 ] a = [[ 0.38281924 0.40654978 0.69744113 0.90707595] [ 0.40572074 0.652105 0.24226191 0.95931459]] sum = [ 2.3938861 2.25940224] min = [ 0.38281924 0.40654978 0.24226191 0.90707595] max = [ 0.90707595 0.95931459] Reference numpy.org numpy docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"}]},{"title":"Numpy Array","slug":"python/numpy-pandas/py-numpy-2-array","date":"2017-12-22T08:43:21.000Z","updated":"2021-06-20T04:12:28.209Z","comments":true,"path":"2017/12/22/python/numpy-pandas/py-numpy-2-array/","link":"","permalink":"http://www.iequa.com/2017/12/22/python/numpy-pandas/py-numpy-2-array/","excerpt":"创建 array 有很多 形式","text":"创建 array 有很多 形式 关键字 array：创建数组 dtype：指定数据类型 zeros：数据全为0 ones：数据全为1 arrange：按指定范围创建数据 linspace：创建线段 与 List 区别之一 : 没有逗号分隔 123a = np.array([2,23,4])print(a)# [ 2 23 4] [ 2 23 4] int64 指定数据 12345678910111213141516a = np.array([2,23,4], dtype=np.int) # 默认 int 为 int64print(a.dtype) #int64a = np.array([2,23,4],dtype=np.int32)print(a.dtype)# int32a = np.array([2,23.1,4.0], dtype=np.float) # 默认 float 为 float64print(a)print(a.dtype)# float64a = np.array([2,23,4],dtype=np.float32)print(a.dtype)# float32 int64 int32 [ 2. 23.1 4. ] float64 float32 创建特定数据 123456789101112a = np.array( [ [2,23,4], [2,23,4] ]) # 2d 矩阵 2行3列print(a)&quot;&quot;&quot;[[ 2 23 4] [ 2 32 4]]&quot;&quot;&quot; 全零数组 12345678# 创建全零数组a = np.zeros((3,4)) # 数据全为0，3行4列&quot;&quot;&quot;array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]])&quot;&quot;&quot;print(a) 全一数组 同时也能指定这些特定数据的 dtype: 1234567a = np.ones((3,4),dtype = np.int) # 数据为1，3行4列&quot;&quot;&quot;array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]])&quot;&quot;&quot;print(a) 全空数组 1np.empty( (2,3) ) # 这个方法最大的好处就是速度快，因为少了初始化空间的操作 array([[ 9.88131292e-324, 1.13635099e-322, 1.97626258e-323], [ 9.88131292e-324, 1.13635099e-322, 1.97626258e-323]]) 连续数组arange 12345678a = np.arange(10,20,2) # 10-19 的数据，2步长&quot;&quot;&quot;array([10, 12, 14, 16, 18])&quot;&quot;&quot;print(a)b = np.arange(12)b [10 12 14 16 18] array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) 改变数据的形状reshape 1234567a = np.arange(12).reshape((3,4)) # 3行4列，0到11&quot;&quot;&quot;array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&quot;&quot;&quot;a 线段型数据linspace 123456789a = np.linspace(1,10,20) # 开始端1，结束端10，且分割成20个数据，生成线段&quot;&quot;&quot;array([ 1. , 1.47368421, 1.94736842, 2.42105263, 2.89473684, 3.36842105, 3.84210526, 4.31578947, 4.78947368, 5.26315789, 5.73684211, 6.21052632, 6.68421053, 7.15789474, 7.63157895, 8.10526316, 8.57894737, 9.05263158, 9.52631579, 10. ])&quot;&quot;&quot;a 也能进行 reshape 工作: 12345678a = np.linspace(1,10,20).reshape((5,4)) # 更改shape&quot;&quot;&quot;array([[ 1. , 1.47368421, 1.94736842, 2.42105263], [ 2.89473684, 3.36842105, 3.84210526, 4.31578947], [ 4.78947368, 5.26315789, 5.73684211, 6.21052632], [ 6.68421053, 7.15789474, 7.63157895, 8.10526316], [ 8.57894737, 9.05263158, 9.52631579, 10. ]])&quot;&quot;&quot; Reference numpy.org numpy docs morvanzhou","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"}]},{"title":"Numpy Attribute","slug":"python/numpy-pandas/py-numpy-1-attribute","date":"2017-12-21T08:43:21.000Z","updated":"2021-06-20T04:12:28.210Z","comments":true,"path":"2017/12/21/python/numpy-pandas/py-numpy-1-attribute/","link":"","permalink":"http://www.iequa.com/2017/12/21/python/numpy-pandas/py-numpy-1-attribute/","excerpt":"numpy 的几种属性 维度、行列个数、元素个数","text":"numpy 的几种属性 维度、行列个数、元素个数 ndim：维度 shape：行数和列数 size：元素个数 列表转化为矩阵： 1234567891011121314151617181920import numpy as nparray1 = np.array( [ [1,2,3], [2,3,4] ])print(array1)print(&#x27;number of dim:&#x27;, array1.ndim)print(&#x27;shape:&#x27;, array1.shape)print(&#x27;size:&#x27;, array1.size)&quot;&quot;&quot; [[1 2 3] [2 3 4]] number of dim: 2 shape: (2, 3) size: 6&quot;&quot;&quot; Reference numpy.org numpy docs","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"}]},{"title":"杨帅完美发音班(2)","slug":"English/IELTS/english-ielts-speaking-yangshuai-2","date":"2017-12-10T00:18:21.000Z","updated":"2021-06-20T04:12:28.301Z","comments":true,"path":"2017/12/10/English/IELTS/english-ielts-speaking-yangshuai-2/","link":"","permalink":"http://www.iequa.com/2017/12/10/English/IELTS/english-ielts-speaking-yangshuai-2/","excerpt":"一定要不断重复，不要相信自己的耳朵，要相信音标 最开始练习，一定要超级夸张，一个音一个音读","text":"一定要不断重复，不要相信自己的耳朵，要相信音标 最开始练习，一定要超级夸张，一个音一个音读 音标 读法 常见组合 /i:/ 长元紧张音 ee, ea, e, ie, ei /i/ 短元放松音 i, y, e /ɛ/ /i/ 嘴型稍微大些 e、ea、a、ai /æ/ /ɛ/+/a:/ a /ɒ/ 嘴张开、无变化 o、a /ɔ:/ 有点像 ‘沃’ or、our、ar、al、au、ou /aʊ/ /æ/+/u/ ou、ow i:、i、u:、u、ɛ、æ、ɒ、ɔ:、aʊ i: seat、deed、least、feet、colleague [ˈkɒli:g]、eat、beat、leave、steel、steal i sit、did、list、fit、college、it、bit、live、still u: food、pool、fool、mood、moon、room、school、lose、true、blue、glue ʊ look、good、foot、book、cook、full、pull、push、put、should、would、could ɛ bed、better、get、internet、lesson、desk、sell、slept、then、help、very、never、head、bread、pleasure、treasure、health、breath、said、there、their、where、everywhere、any、many、guess æ flag、cat、back、that、have、activity、natural、travel、relax、attractive、casual、magazine、fancy ɒ not、lost、loss、box、mop、collar、shop、job、top、soccer、biography、psychology、novel、want ɔ: horse、more、short、store、court、course、mourn、warm、quarter、small、wall、ball、talk、walk、autumn、caught、taught、bought、thought、brought aʊ house、out、about、loud、down、town、cow、however、now、how、mouse、mouth、around、ground、found、accounting、sound","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"Pyenv Install Anaconda3","slug":"devops/ops-pyenv-Anaconda3-note","date":"2017-12-07T02:16:21.000Z","updated":"2021-06-20T04:12:28.203Z","comments":true,"path":"2017/12/07/devops/ops-pyenv-Anaconda3-note/","link":"","permalink":"http://www.iequa.com/2017/12/07/devops/ops-pyenv-Anaconda3-note/","excerpt":"pyenv install anaconda3-5.0.0 无法下载包的情况下使用的，因为网速实在太慢了T T","text":"pyenv install anaconda3-5.0.0 无法下载包的情况下使用的，因为网速实在太慢了T T Anaconda3 官网 下载了.sh文件，在该.sh文件目录使用下面的命令安装该文件 Anaconda3-5.0.1-MacOSX-x86_64.sh 12➜ mv Anaconda3-5.0.1-MacOSX-x86_64.sh ~/.pyenv/cache/➜ cache git:(master) sh Anaconda3-5.0.1-MacOSX-x86_64.sh 安装过程需要操作的地方： 12345Please, press ENTER to continue&gt;&gt;&gt;...Do you accept the license terms? [yes|no][no] &gt;&gt;&gt; yes 12345Anaconda3 will now be installed into this location:/Users/blair/anaconda3 - Press ENTER to confirm the location- Press CTRL-C to abort the installation- Or specify a different location below [/Users/blair/anaconda3] 改为 /Users/blair/.pyenv/versions/anaconda3 输入需要安装的目录，因为要添加到pyenv管理器中，所以与其他以安装的Python版本放在同一目录下。 开始输出安装信息，会安装许多包： 12345678[/Users/blair/anaconda3] &gt;&gt;&gt; /Users/blair/.pyenv/versions/anaconda3PREFIX=/Users/blair/.pyenv/versions/anaconda3installing: python-3.6.3-h6804ab2_0 ...Python 3.6.3 :: Anaconda, Inc.installing: bzip2-1.0.6-h92991f9_1 ...installing: ca-certificates-2017.08.26-ha1e5d58_0 ...installing: conda-env-2.6.0-h36134e3_0 ...…… 然后出现提示，输入yes即可 1234installation finished.Do you wish the installer to prepend the Anaconda3 install locationto PATH in your /Users/blair/.bash_profile ? [yes|no][yes] &gt;&gt;&gt; yes 安装成功！ 12345678910111213➜ pyenv versions system 2.7.14 2.7.14/envs/vpy2 3.5.4 3.5.4/envs/vpy3.5* anaconda3 (set by /Users/blair/.pyenv/version) anaconda3/envs/vconda3 vconda3 vpy2 vpy3.5(anaconda3)# ~/ghome [10:34:54] 为anaconda3创建虚拟环境 1➜ cache git:(master) pyenv virtualenv vconda3 可能会装一些更新包，会有提示： 123456## To activate this environment, use:# &gt; source activate vconda3## To deactivate an active environment, use:# &gt; source deactivate 根据提示激活： 1➜ cache git:(master) source activate vconda3","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Anaconda3","slug":"Anaconda3","permalink":"http://www.iequa.com/tags/Anaconda3/"}]},{"title":"杨帅完美发音班(1)","slug":"English/IELTS/english-ielts-speaking-yangshuai-1","date":"2017-12-03T13:55:21.000Z","updated":"2021-06-20T04:12:28.303Z","comments":true,"path":"2017/12/03/English/IELTS/english-ielts-speaking-yangshuai-1/","link":"","permalink":"http://www.iequa.com/2017/12/03/English/IELTS/english-ielts-speaking-yangshuai-1/","excerpt":"把发音练好的方法就是重复, 语言学习的本质就是模仿 but you’re old… 0 ~ 6 years 可以通过耳朵识别 140 个不同的音, 也可以马上模仿 6 岁之后，这种能力逐渐下降…， 所以你天天听剑桥英语，或者你去外国10几年，还是中式发音","text":"把发音练好的方法就是重复, 语言学习的本质就是模仿 but you’re old… 0 ~ 6 years 可以通过耳朵识别 140 个不同的音, 也可以马上模仿 6 岁之后，这种能力逐渐下降…， 所以你天天听剑桥英语，或者你去外国10几年，还是中式发音 1. 理论 + 模仿 Less is more. Practice makes progress 少而精，融入血液，融入骨髓 理论 - 不用天天 (让模仿更加有效果) 模仿 - 是天天需要的 2. 发音学习的几大注意: 懂音标 (自学) 一定要不断重复 不要相信自己的耳朵，要相信音标 最开始练习，一定要超级夸张 一个音一个音读 1234experience [ɪkˈspɪriəns]serious [ˈsɪriəs]series [ˈsɪri:z]down [daʊn] 推荐字典 Oxford (英音) Merriam Webster (美音) 最易错元音 – 保6 辅音 + 语调 (重读、连读) – 争7 略读 + 英美音主要区别 – 带你装带你飞 从英到美 1234567flat [flæt]、cat [kæt]kite [kaɪt]、 dog [dɒg] [dɔ:g] bed [bed]、[bɛd]bought [bɔ:t]、[bɔt]usually [ˈju:ʒuəli]、 [ˈjuːʒuəli] garage [ˈgærɑ:ʒ]、[gəˈrɑ:ʒ] ...... 中国人问题最大的三个元音 1/ai/ /æ/ /ɛ/ /ai/ = /a:/-/i/ 字母组合: 主要 i，ie，y，igh，uy time，white，die，tied dry，why，July hight，night、right buy，guy kind，mind，wind all kinds of，spring to mind，wind down，unwind word phonetic symbol sentence die [dai] I don’t want to die. died [daid] My dog died yesterday. dying [ˈdaɪɪŋ] The man is dying. dead [ded]，[dɛd] Her cat is dead. 长短音的区分 长音: tense(紧张音) – 长元紧张音 - beach 短音: lax (放松音) – 短元放松音 - bitch /i:/ 不放松音 , 字母组合: 主要 ee，ea，e，ie，ei week，weak，tree，meet，three，see，seek，meat，leave，eat，beat lead，each，colleague，team，read，speak，please，piece，receive extremely，media /ei/ 一定要饱满 , 字母组合: 主要 ay，a tree. tray [trei] see. say [sei] week. wake [weik] piece. pace [peis] /i/ 是 /i:/ 的放松不用力音 , 字母组合: 主要 i，y sit，it，is，live，did，this，kid，list，six fix，fit，click，if，big，give，gift，trip still，miss，finish，habit，mystery [ˈmɪstəri]，college [ˈkɑ:lɪdʒ]","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"}],"tags":[{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"}]},{"title":"mac 安装 mysql 与 常用命令","slug":"devops/ops-mac-mysql","date":"2017-12-03T01:16:21.000Z","updated":"2021-06-20T04:12:28.200Z","comments":true,"path":"2017/12/03/devops/ops-mac-mysql/","link":"","permalink":"http://www.iequa.com/2017/12/03/devops/ops-mac-mysql/","excerpt":"介绍 Mac 安装 Mysql 与 mysql 在 mac 在的命令","text":"介绍 Mac 安装 Mysql 与 mysql 在 mac 在的命令 1. brew install 1brew install mysql 在mac下使用 brew 安装 mysql，之前没有使用过，今天启动的时候发现启动不了 123# /usr/local/bin [9:31:54]➜ mysqlERROR 2002 (HY000): Can&#x27;t connect to local MySQL server through socket &#x27;/tmp/mysql.sock&#x27; (2) 12➜ brew info mysqlmysql: stable 8.0.12 (bottled) 2. 启动mysql 1➜ mysql.server start 3. 设置密码 1mysql_secure_installation 4. 进入mysql 1mysql -u root -p 123456789101112mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec)mysql&gt; 5. 常用命令 12345$ mysql.server start$ mysql.server restart$ mysql.server stop$ mysql.server status$ mysql -u root -p Reference mac用brew安装mysql,设置初始密码","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://www.iequa.com/tags/mysql/"}]},{"title":"Neural Networks and Deep Learning (week1) - Introduction to Deep Learning","slug":"deeplearning/Neural-Networks-and-Deep-Learning-week1","date":"2017-12-01T11:55:21.000Z","updated":"2021-06-20T04:12:28.252Z","comments":true,"path":"2017/12/01/deeplearning/Neural-Networks-and-Deep-Learning-week1/","link":"","permalink":"http://www.iequa.com/2017/12/01/deeplearning/Neural-Networks-and-Deep-Learning-week1/","excerpt":"Introduction to Deep Learning What is a Neural Network ? Why is Deep Learning taking off ?","text":"Introduction to Deep Learning What is a Neural Network ? Why is Deep Learning taking off ? 1. Introduction to Deep Learning What’s you’ll learn 2. What is a Neural Network ? Housing Price Prediction 3. Why is Deep Learning taking off ? Data 、 Computation 、 Algorithms Reference Andrew Ng - deeplearning.ai 网易云课堂 - 第一周深度学习概论","categories":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"}]},{"title":"理论 seq2seq+Attention 机制模型详解","slug":"nlp/seq2seq+Attention","date":"2017-11-17T12:00:21.000Z","updated":"2021-06-20T04:12:28.335Z","comments":true,"path":"2017/11/17/nlp/seq2seq+Attention/","link":"","permalink":"http://www.iequa.com/2017/11/17/nlp/seq2seq+Attention/","excerpt":"从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。","text":"从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。 Seq-to-Seq 框架1 Seq-to-Seq 框架2（teacher forcing） Seq-to-Seq with Attention（NMT） Seq-to-Seq with Attention 各种变形 Seq-to-Seq with Beam-Search 当输入输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder）1 或者 seq2seq 模型 2。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。 1. Seq2Seq 框架1 Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation 1.1 encode 编码器 编码器的作用是把一个不定长的输入序列变换成一个定长的背景变量 ccc，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。 让我们考虑批量大小为 1 的时序数据样本。假设输入序列是 x1,…,xTx_1,\\ldots,x_Tx1​,…,xT​, 例如 xix_ixi​ 是输入句子中的第 iii 个词。在时间步 ttt，循环神经网络将输入 xtx_txt​ 的特征向量 xtx_txt​ 和上个时间步的隐藏状态 ht−1\\boldsymbol{h}_{t-1}ht−1​ 变换为当前时间步的隐藏状态 hth_tht​。我们可以用函数 fff 表达循环神经网络隐藏层的变换： ht=f(xt,ht−1).\\boldsymbol{h}_t = f(\\boldsymbol{x}_t, \\boldsymbol{h}_{t-1}). ht​=f(xt​,ht−1​). 接下来编码器通过自定义函数 qqq 将各个时间步的隐藏状态变换为背景变量 c=q(h1,…,hT).\\boldsymbol{c} = q(\\boldsymbol{h}_1, \\ldots, \\boldsymbol{h}_T). c=q(h1​,…,hT​). 例如，当选择 q(h1,…,hT)=hTq(\\boldsymbol{h}_1, \\ldots, \\boldsymbol{h}_T) = \\boldsymbol{h}_Tq(h1​,…,hT​)=hT​ 时，背景变量是输入序列最终时间步的隐藏状态 hT\\boldsymbol{h}_ThT​。 以上描述的编码器是一个单向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用 Bi-RNN 构造编码器。这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。 1.2 decode 解码器 Encode 编码器输出的背景变量 ccc 编码了整个输入序列 x1,…,xTx_1, \\ldots, x_Tx1​,…,xT​ 的信息。给定训练样本中的输出序列 y1,y2,…,yT′y_1, y_2, \\ldots, y_{T&#x27;}y1​,y2​,…,yT′​，对每个时间步 t′t&#x27;t′（符号与输入序列或编码器的时间步 ttt 有区别）， 解码器输出 yt′y_{t&#x27;}yt′​ 的条件概率将基于之前的输出序列 y1,…,yt′−1y_1,\\ldots,y_{t&#x27;-1}y1​,…,yt′−1​ 和背景变量 ccc，即 P(yt′∣y1,…,yt′−1,c)\\mathbb{P}(y_{t&#x27;} \\mid y_1, \\ldots, y_{t&#x27;-1}, \\boldsymbol{c})P(yt′​∣y1​,…,yt′−1​,c)。 为此，我们可以使用另一个RNN作为解码器。 在输出序列的时间步 t′t^\\primet′，解码器将上一时间步的输出 yt′−1y_{t^\\prime-1}yt′−1​ 以及背景变量 ccc 作为输入，并将它们与上一时间步的隐藏状态 ht′−1\\boldsymbol{h}_{t^\\prime-1}ht′−1​ 变换为当前时间步的隐藏状态 ht′\\boldsymbol{h}_{t^\\prime}ht′​。因此，我们可以用函数 ggg 表达解码器隐藏层的变换： ht′=g(yt′−1,c,ht′−1).\\boldsymbol{h}_{t^\\prime} = g(y_{t^\\prime-1}, \\boldsymbol{c}, \\boldsymbol{h}_{t^\\prime-1}). ht′​=g(yt′−1​,c,ht′−1​). 有了decode的隐藏状态后，我们可以使用自定义的输出层和 softmax 运算来计算 P(yt′∣y1,…,yt′−1,c)\\mathbb{P}(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\boldsymbol{c})P(yt′​∣y1​,…,yt′−1​,c)，例如基于当前时间步的解码器隐藏状态 ht′\\boldsymbol{h}_{t^\\prime}ht′​、上一时间步的输出 yt′−1y_{t^\\prime-1}yt′−1​ 以及背景变量 ccc 来计算当前时间步输出 yt′y_{t^\\prime}yt′​ 的概率分布。 1.3 train 模型训练 根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率 \\begin{split}\\begin{aligned} \\mathbb{P}(y_1, \\ldots, y_{T&#039;} \\mid x_1, \\ldots, x_T) &amp;= \\prod_{t&#039;=1}^{T&#039;} \\mathbb{P}(y_{t&#039;} \\mid y_1, \\ldots, y_{t&#039;-1}, x_1, \\ldots, x_T)\\\\\\\\ &amp;= \\prod_{t&#039;=1}^{T&#039;} \\mathbb{P}(y_{t&#039;} \\mid y_1, \\ldots, y_{t&#039;-1}, \\boldsymbol{c}), \\end{aligned}\\end{split} 并得到该输出序列的损失 −log⁡P(y1,…,yT′∣x1,…,xT)=−∑t′=1T′log⁡P(yt′∣y1,…,yt′−1,c), - \\log\\mathbb{P}(y_1, \\ldots, y_{T&#x27;} \\mid x_1, \\ldots, x_T) = -\\sum_{t&#x27;=1}^{T&#x27;} \\log \\mathbb{P}(y_{t&#x27;} \\mid y_1, \\ldots, y_{t&#x27;-1}, \\boldsymbol{c}), −logP(y1​,…,yT′​∣x1​,…,xT​)=−t′=1∑T′​logP(yt′​∣y1​,…,yt′−1​,c), 在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图中所描述的模型预测中，我们需要将decode在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列在上一个时间步的标签作为decode在当前时间步的输入。这叫做强制教学（teacher forcing）。 1.4 小结 编码器 - 解码器（seq2seq）可以输入并输出不定长的序列。 编码器—解码器使用了两个循环神经网络。 在编码器—解码器的训练中，我们可以采用强制教学。 （这也是 Seq2Seq 2 的内容） 2. Seq2Seq 框架2 第二个要讲的Seq-to-Seq模型来自于 “Sequence to Sequence Learning with Neural Networks”，其模型结构图如下所示： 与上面模型最大的区别在于其source编码后的 向量CCC 直接作为Decoder阶段RNN的初始化state，而不是在每次decode时都作为RNN cell的输入。此外，decode时RNN的输入是目标值，而不是前一时刻的输出。首先看一下编码阶段： 就是简单的RNN模型，每个词经过RNN之后都会编码为hidden state（e0,e1,e2），并且source序列的编码向量e就是最终的hidden state e2。接下来再看一下解码阶段： e向量仅作为RNN的初始化状态传入decode模型。接下来就是标准的循环神经网络，每一时刻输入都是前一时刻的正确label。直到最终输入符号截止滚动。 3. Seq2Seq Attention decode 在各个时间步依赖相同的 背景变量 ccc 来获取输入序列信息。当 encode 为 RNN 时，背景变量ccc 来自它最终时间步的隐藏状态。 英语输入：“They”、“are”、“watching”、“.” 法语输出：“Ils”、“regardent”、“.” 翻译例子：输入为英语序列“They”、“are”、“watching”、“.”，输出为法语序列“Ils”、“regardent”、“.”。，decode 在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步 1，解码器可以主要依赖“They”、“are”的信息来生成“Ils”，在时间步 2 则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步 3 则直接映射句号“.”。这看上去就像是在 decode 的每一时间步对输入序列中不同时间步的编码信息分配不同的注意力一样。这也是注意力机制的由来 1。 仍以 RNN 为例，Attention 通过对 Encode 所有时间步的隐藏状态做加权平均来得到背景变量ccc。Decode 在每一时间步调整这些权重，即 Attention weight，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量ccc。本节我们将讨论 Attention机制 是怎么工作的。 在“编码器—解码器（seq2seq）”, 解码器在时间步 t′t&#x27;t′ 的隐藏状态 st′=g(yt′−1,c,st′−1)\\boldsymbol{s}_{t&#x27;} = g(\\boldsymbol{y}_{t&#x27;-1}, \\boldsymbol{c}, \\boldsymbol{s}_{t&#x27;-1}) st′​=g(yt′−1​,c,st′−1​) 在 Attention机制 中, 解码器的每一时间步将使用可变的背景变量ccc st′=g(yt′−1,ct′,st′−1).\\boldsymbol{s}_{t&#x27;} = g(\\boldsymbol{y}_{t&#x27;-1}, \\boldsymbol{c}_{t&#x27;}, \\boldsymbol{s}_{t&#x27;-1}). st′​=g(yt′−1​,ct′​,st′−1​). 关键是如何计算背景变量 ct′\\boldsymbol{c}_{t&#x27;}ct′​ 和如何利用它来更新隐藏状态 st′\\boldsymbol{s}_{t&#x27;}st′​。以下将分别描述这两个关键点。 3.1 计算背景变量 c ct′=∑t=1Tαt′tht,\\boldsymbol{c}_{t&#x27;} = \\sum_{t=1}^T \\alpha_{t&#x27; t} \\boldsymbol{h}_t, ct′​=t=1∑T​αt′t​ht​, 其中给定 t′t&#x27;t′ 时，权重 αt′t\\alpha_{t&#x27; t}αt′t​ 在 t=1,…,Tt=1,\\ldots,Tt=1,…,T 的值是一个概率分布。为了得到概率分布，可以使用 softmax 运算: αt′t=exp⁡(et′t)∑k=1Texp⁡(et′k),t=1,…,T.\\alpha_{t&#x27; t} = \\frac{\\exp(e_{t&#x27; t})}{ \\sum_{k=1}^T \\exp(e_{t&#x27; k}) },\\quad t=1,\\ldots,T. αt′t​=∑k=1T​exp(et′k​)exp(et′t​)​,t=1,…,T. 现在，我们需要定义如何计算上式中 softmax 运算的输入 et′te_{t&#x27; t}et′t​。由于 et′te_{t&#x27; t}et′t​ 同时取决于decode的时间步 t′t&#x27;t′ 和encode的时间步 ttt，我们不妨以解码器在时间步 t′−1t&#x27;−1t′−1 的隐藏状态 st′−1\\boldsymbol{s}_{t&#x27; - 1}st′−1​ 与编码器在时间步 ttt 的隐藏状态 hth_tht​ 为输入，并通过函数 aaa 计算 et′te_{t&#x27; t}et′t​： et′t=a(st′−1,ht).e_{t&#x27; t} = a(\\boldsymbol{s}_{t&#x27; - 1}, \\boldsymbol{h}_t). et′t​=a(st′−1​,ht​). 这里函数 a 有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积 a(s,h)=s⊤ha(\\boldsymbol{s}, \\boldsymbol{h})=\\boldsymbol{s}^\\top \\boldsymbol{h}a(s,h)=s⊤h。而最早提出Attention机制的论文则将输入连结后通过含单隐藏层的多层感知机MLP 变换 a(s,h)=v⊤tanh⁡(Wss+Whh),a(\\boldsymbol{s}, \\boldsymbol{h}) = \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}_s \\boldsymbol{s} + \\boldsymbol{W}_h \\boldsymbol{h}), a(s,h)=v⊤tanh(Ws​s+Wh​h), 其中 v、Ws、Whv、W_s、W_hv、Ws​、Wh​ 都是可以学习的模型参数。 3.2 更新隐藏状态 以门控循环单元为例，在解码器中我们可以对门控循环单元的设计稍作修改。解码器在时间步 t′t&#x27;t′ 的隐藏状态为 st′=zt′⊙st′−1+(1−zt′)⊙s~t′,\\boldsymbol{s}_{t&#x27;} = \\boldsymbol{z}_{t&#x27;} \\odot \\boldsymbol{s}_{t&#x27;-1} + (1 - \\boldsymbol{z}_{t&#x27;}) \\odot \\tilde{\\boldsymbol{s}}_{t&#x27;}, st′​=zt′​⊙st′−1​+(1−zt′​)⊙s~t′​, 其中的重置门、更新门和候选隐含状态分别为 : \\begin{split}\\begin{aligned} \\boldsymbol{r}_{t&#039;} &amp;= \\sigma(\\boldsymbol{W}_{yr} \\boldsymbol{y}_{t&#039;-1} + \\boldsymbol{W}_{sr} \\boldsymbol{s}_{t&#039; - 1} + \\boldsymbol{W}_{cr} \\boldsymbol{c}_{t&#039;} + \\boldsymbol{b}_r),\\\\\\\\ \\boldsymbol{z}_{t&#039;} &amp;= \\sigma(\\boldsymbol{W}_{yz} \\boldsymbol{y}_{t&#039;-1} + \\boldsymbol{W}_{sz} \\boldsymbol{s}_{t&#039; - 1} + \\boldsymbol{W}_{cz} \\boldsymbol{c}_{t&#039;} + \\boldsymbol{b}_z),\\\\\\\\ \\tilde{\\boldsymbol{s}}_{t&#039;} &amp;= \\text{tanh}(\\boldsymbol{W}_{ys} \\boldsymbol{y}_{t&#039;-1} + \\boldsymbol{W}_{ss} (\\boldsymbol{s}_{t&#039; - 1} \\odot \\boldsymbol{r}_{t&#039;}) + \\boldsymbol{W}_{cs} \\boldsymbol{c}_{t&#039;} + \\boldsymbol{b}_s), \\end{aligned}\\end{split} 其中含下标的 W 和 b 分别为门控循环单元的权重参数和偏差参数。 3.3 小结 可以在decode的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。 Attention机制可以采用更为高效的矢量化计算。 4. Seq2Seq Attention各种变形 第四个Seq-to-Seq模型，来自于论文 Effective Approaches to Attention-based Neural Machine Translation 这篇论文提出了两种 Seq2Seq模型 分别是global Attention 和 local Attention。 5. Seq2Seq with Beam-Search 上面讲的几种Seq2Seq模型都是从模型结构上进行的改进，也就说为了从训练的层面上改善模型的效果，但这里要介绍的beam-search是在测试的时候才用到的技术。 Reference 动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制 门控循环单元（GRU） seq2seq+Attention机制模型详解 三分钟带你对 Softmax 划重点 Softmax 回归","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"seq2seq+Attention","slug":"seq2seq-Attention","permalink":"http://www.iequa.com/tags/seq2seq-Attention/"}]},{"title":"搬瓦工VPS 配置 SSH 与 OpenVpn","slug":"devops/ops-bandwagonhost-ssh-openvpn","date":"2017-11-12T05:16:21.000Z","updated":"2021-06-20T04:12:28.198Z","comments":true,"path":"2017/11/12/devops/ops-bandwagonhost-ssh-openvpn/","link":"","permalink":"http://www.iequa.com/2017/11/12/devops/ops-bandwagonhost-ssh-openvpn/","excerpt":"搬瓦工VPS 配置 ssh 登录 与 iphone 配置使用 openvpn","text":"搬瓦工VPS 配置 ssh 登录 与 iphone 配置使用 openvpn SSH 配置 ssh 登录搬瓦工机器 stop server @Main controls Root password modification start Server Root shell - interactive vi /etc/ssh/sshd_config, add 12PermitRootLogin yesPort 22 /etc/init.d/sshd restart ssh root@ip OpenVpn 配置 在iOS设备上打开app store，搜索openvpn，install 打开 bandwagon my service pandel My Service Pandel -&gt; KiwiVM Control Pandel -&gt; OpenVPN Server -&gt; install OpenVPN -&gt; Download Key Files 把 ca.crt、client1.crt、client1.key 证书放入 .ovpn 配置文件 在 .ovpn 文件尾部中新增 标签 &lt;ca&gt;、&lt;cert&gt;、&lt;key&gt; 标签 123456789ca.crt 文件内容复制到 &lt;ca&gt;和&lt;/ca&gt; 的中间，client1.crt 文件内容复制到 &lt;cert&gt;和&lt;/cert&gt; 的中间，client1.key 文件内容复制到 &lt;key&gt;和&lt;/key&gt; 的中间，修改完成后删除 .ovpn 配置文件中类似ca ca.crtcert client1.crtkey client1.key .ovpn 通过 Airdrop 传到 iphone 手机里 打开OpenVPN，点那些绿色加号，将配置文件导入 向右滑动最下面那个白色滑块至蓝色，连接服务器。 连接成功。如果显示 Connected，表示连接成功了。 Reference www.yuntionly.com www.wisevpn.net www.banwago.com www.godaddy.com 搬瓦工中文网 搬瓦工购买页面 搬瓦工VPS续费的那些事 OpenVPN支持iOS啦 绿色便携汉化可保存密码的OpenVPN客户端","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"banwagon","slug":"banwagon","permalink":"http://www.iequa.com/tags/banwagon/"}]},{"title":"文字和语言 vs 数字和信息","slug":"nlp/word-language-number-info-history","date":"2017-11-08T02:08:21.000Z","updated":"2021-06-22T06:48:39.553Z","comments":true,"path":"2017/11/08/nlp/word-language-number-info-history/","link":"","permalink":"http://www.iequa.com/2017/11/08/nlp/word-language-number-info-history/","excerpt":"数字、文字、自然语言 一样，都是信息的载体。","text":"数字、文字、自然语言 一样，都是信息的载体。 语言和数字的产生为的是同一个目的 ：记录和传播信息。 1948年，香农提出信息论，人们才把 数学和语言 联系起来。 1. 信息 &lt;img src=&quot;/images/nlp/history-info-1.jpg&quot; width=“480” height=“400” align=“middle” /img&gt; 人类最早利用 voice 进行 通信 &lt;img src=&quot;/images/nlp/history-info-2.png&quot; width=“650” height=“100” align=“middle” /img&gt; 人类文明的进步，需要表达的信息量越来越多，人类发明的自然语言 语言的出现是为了人类之间的通信。字母、文字、数字 是信息编码的不同单位。任何一种语言都是一种编码方式，语言的语法规则是编解码的算法。 2. 文字和数字 当语言和词汇多到一定程度，人类大脑记不住所有词汇，高效记录信息的形式出现，人类便发明了 文字 文字 古埃及，读音相同的词用同一个符号来记录。 文字按照聚类会带来歧义性，弄不清多义字在特定环境的含义，就要依靠上下文。 不同的文明，由于地域原因，历史上互相隔绝，便会有不同的文字。文明的融合与冲突，不同文明下的人们进行交流(通信)，那么翻译的需求便产生了。不同的文字系统在记录信息上的能力是等价的。 罗塞塔 Rosetta Stone 石碑 [古埃及象形文字、埃及拼音文字、古希腊文] 的破译对于 NLP 学者的两点指导意义 : 信息的冗余是信息安全的保障 语言的数据，我们称之为 语料 Rosetta Stone, Google推出的翻译软件 数字 祖先需要记录物件的个数越来越多，所以开始发明了计数系统，也就有了 数字。因为 10 个手指头，所以发明了 10 进制。 描述数字最有效的是 古印度人，他们发明了 10个 阿拉伯数字。数字的革命性在于它的简单有效，而且 标志着数字和文字的分离。这在客观上让 自然语言的研究 和 数学的研究 在几千年里没有重合的轨迹。 3. 文字和语言背后的数学 从 象形文字 到 拼音文字 是一个飞跃，描述物体方式上，从外表进化到了抽象的概念，同时不自觉的采用了对 信息的编码。同时祖先对文字的编码还非常合理，常用字短，生僻字长。这完全符合信息论中的 最短编码理论。 这种文字设计(其实是一种编码方法)带来的好处是写起来省时间、省材料。 公元前26世纪,约4700年前，出现了楔形文字(一种拼音文字) 在古代，在造纸术发明之前，人们说话还是类似白话文，文字书写要刻在 龟壳、石碑、竹简 等上，很费时间和材料，所以惜墨如金，使得古文非常简洁。 类比信息科学 ： 通信时: 如果信道较宽，信息不必压缩，可直接传递; 通信时: 如果信道较窄，信息在传递前需要尽可能压缩，然后在接收端进行解压缩。 词语 ： 是有限和封闭的集合 (其实可设置完备的编码规则) 语言 ： 是无限和开放的集合 (不可以设置完备的编码规则) 任何语言的都有语法规则覆盖不到的地方，这些不精确性，也造就了语言的丰富多彩。 4. 小结 了解 文字、数字、语言 的历史 通信的原理和信息传播的模型 (信源) 编码 和 最短编码 解码的规则 : 语法 聚类的概念 双语对照文本，语料库 和 机器翻译 多义性和利用上下文消除歧义性 Reference 《数学之美》 读书笔记","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"word2vec","slug":"word2vec","permalink":"http://www.iequa.com/tags/word2vec/"}]},{"title":"python 结构化您的工程","slug":"python/language/py-language-structure-project","date":"2017-11-04T02:05:21.000Z","updated":"2021-06-20T04:12:28.226Z","comments":true,"path":"2017/11/04/python/language/py-language-structure-project/","link":"","permalink":"http://www.iequa.com/2017/11/04/python/language/py-language-structure-project/","excerpt":"如何利用Python的特性来创造简洁、高效的代码。 “结构化” 意味着通过编写简洁的代码，使逻辑和依赖清晰.","text":"如何利用Python的特性来创造简洁、高效的代码。 “结构化” 意味着通过编写简洁的代码，使逻辑和依赖清晰. 仓库的结构 在一个健康的开发周期中，代码风格，API设计和自动化是非常关键的。同样的，对于工程的 架构 ,仓库的结构也是关键的一部分。 当一个潜在的用户和贡献者登录到您的仓库页面时，他们会看到这些: 工程的名字 工程的描述 一系列的文件 拥有良好的布局，事半功倍。 仓库样例 1234567891011README.rstLICENSEsetup.pyrequirements.txtsample/__init__.pysample/core.pysample/helpers.pydocs/conf.pydocs/index.rsttests/test_basic.pytests/test_advanced.py License 除了源代码本身以外，这个毫无疑问是您仓库最重要的一部分。在这个文件中要有完整的许可说明和授权。 如果您不太清楚您应该使用哪种许可方式，请查看 choosealicense.com. Setup.py … Reference 结构化你的工程","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Guitar 基本乐理","slug":"tools/music-guitar-3","date":"2017-10-28T13:00:16.000Z","updated":"2021-06-22T06:52:37.111Z","comments":true,"path":"2017/10/28/tools/music-guitar-3/","link":"","permalink":"http://www.iequa.com/2017/10/28/tools/music-guitar-3/","excerpt":"音乐分为 噪音 与 乐音","text":"音乐分为 噪音 与 乐音 1. 入门 1 2 3 4 5 6 7 i 音名 c d e f g a b 唱名 Do Re Mi Fa Sol La Si 2. 和弦 和弦: 几个相互和谐融洽的音放在一起组成的一组音 我们人所听到的声音，究其物理实质，是因为空气振动而形成的，振动频率越高，音就越高。有时候，某一振动频率的音和另外一振动频率的音在一起的时候，我们人的耳朵听起来就感觉非常舒服，这是因为在物理原理上，在声学原理上，两种频率产生了谐振，因此彼此相互支持、相互加强、相互映衬了；而有时候，当两个不同振动频率的音在一起的时候，人们听起来就觉得非常刺耳、非常不舒服，这是因为两种频率相互背道而弛、相互对抗相互拆台了。 音名组合 唱名 和弦 C、E、G 1、3、5 C和弦 D、F、A 2、4、6 D和弦 E、G、B 3、5、7 E和弦 F、A、C1 中音的4、6 和 高音的1组合起来 F和弦 G、B、D1 中音的5、7和高音的2组合起来。根音5 G和弦 A、C1、E1 中音的6和高音的1、3组合。根音6 A和弦 B、D1、F1 中音7和高音2、4组合。根音7 B和弦 3. 音及音高 Reference Douban Guitar","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"Guitar","slug":"Guitar","permalink":"http://www.iequa.com/tags/Guitar/"}]},{"title":"Linux Search Cmd","slug":"devops/linux-cmd-file-search","date":"2017-10-24T14:04:21.000Z","updated":"2021-06-20T04:12:28.196Z","comments":true,"path":"2017/10/24/devops/linux-cmd-file-search/","link":"","permalink":"http://www.iequa.com/2017/10/24/devops/linux-cmd-file-search/","excerpt":"which，whereis，locate，find，find exec，find xargs，find…","text":"which，whereis，locate，find，find exec，find xargs，find… which 查看可执行文件的位置。 whereis 查看文件的位置。 locate 配合数据库查看文件位置。 find 实际搜寻硬盘查询文件名称。 1. which which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。 12345[root@localhost ~]# which pwd/bin/pwd[root@localhost ~]# which adduser/usr/sbin/adduser[root@localhost ~]# cd 是bash 内建的命令！ 12➜ which cdcd: shell built-in command which 默认是找 PATH 内所规范的目录，所以当然一定找不到的！ 2. whereis whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 和find相比，whereis查找的速度非常快，这是因为linux系统会将 系统内的所有文件都记录在一个数据库文件中，当使用whereis和下面即将介绍的locate时，会从数据库中查找数据，而不是像find命令那样，通 过遍历硬盘来查找，效率自然会很高。 但是该数据库文件并不是实时更新，默认情况下时一星期更新一次，因此，我们在用whereis和locate 查找文件时，有时会找到已经被删除的数据，或者刚刚建立文件，却无法查找到，原因就是因为数据库文件没有被更新。 3. locate locate命令可以在搜寻数据库时快速找到档案，数据库由updatedb程序来更新，updatedb是由cron daemon周期性建立的，locate命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是locate所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb每天会跑一次，可以由修改crontab来更新设定值。(etc/crontab) locate指定用在搜寻符合条件的档案，它会去储存档案与目录名称的数据库内，寻找合乎范本样式条件的档案或目录录，可以使用特殊字元（如”” 或”?”等）来指定范本样式，如指定范本为kcpaner, locate会找出所有起始字串为kcpa且结尾为ner的档案或目录，如名称为kcpartner若目录录名称为kcpa_ner则会列出该目录下包括 子目录在内的所有档案。 locate指令和find找寻档案的功能类似，但locate是透过update程序将硬盘中的所有档案和目录资料先建立一个索引数据库，在 执行loacte时直接找该索引，查询速度会较快，索引数据库一般是由操作系统管理，但也可以直接下达update强迫系统立即修改索引数据库。 mac 默认没启动 locate 4. find 4.1 查找指定时间内修改过的文件 123456[root@peidachang ~]# find -atime -2../logs/monitor./.bashrc./.bash_profile./.bash_history 说明：超找48小时内修改过的文件 4.2 根据关键字查找 1find . -name &quot;*.log&quot; 4.3 查找当前所有目录并排序 1find . -type d | sort 4.4 按类型查找 1find . -type f -name &quot;*.log&quot; 4.5 查找当前所有目录并排序 1find . -type d | sort 4.6 按大小查找文件 1find . -size +1000c -print 5. find命令之exec find是我们很常用的一个Linux命令，但是我们一般查找出来的并不仅仅是看看而已，还会有进一步的操作，这个时候exec的作用就显现出来了。 exec解释： -exec 参数后面跟的是command命令，它的终止是以;为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。 {} 花括号代表前面find查找出来的文件名。 使用find时，只要把想要的操作写在一个文件里，就可以用exec来配合find查找，很方便的。在有些操作系统中只允许-exec选项执行诸如l s或ls -l这样的命令。大多数用户使用这一选项是为了查找旧文件并删除它们。建议在真正执行rm命令删除文件之前，最好先用ls命令看一下，确认它们是所要删除的文件。 exec选项后面跟随着所要执行的命令或脚本，然后是一对儿{ }，一个空格和一个\\，最后是一个分号。为了使用exec选项，必须要同时使用print选项。如果验证一下find命令，会发现该命令只输出从当前路径起的相对路径及文件名。 5.1 ls -l命令放find命令的-exec 1234567891011121314151617# ~/ghome/github/test [15:45:03]➜ lltotal 0-rw-r--r-- 1 blair staff 0B Oct 28 15:44 log1-rw-r--r-- 1 blair staff 0B Oct 28 15:45 log2-rw-r--r-- 1 blair staff 0B Oct 28 15:45 log3-rw-r--r-- 1 blair staff 0B Oct 28 15:45 log4(vpy2)# ~/ghome/github/test [15:45:03]➜ find . -type f -exec ls -l &#123;&#125; \\;-rw-r--r-- 1 blair staff 0 Oct 28 15:45 ./log4-rw-r--r-- 1 blair staff 0 Oct 28 15:45 ./log3-rw-r--r-- 1 blair staff 0 Oct 28 15:45 ./log2-rw-r--r-- 1 blair staff 0 Oct 28 15:44 ./log1(vpy2)# ~/ghome/github/test [15:45:04]➜ 5.2 更改时间在n日以前的文件并删除 1find . -type f -mtime +14 -exec rm &#123;&#125; \\; 5.3 更改时间在n日前文件并提示删除 1234567891011# ~/ghome/github/test [15:47:35]➜ lltotal 0-rw-r--r-- 1 blair staff 0B Oct 28 15:44 log1-rw-r--r-- 1 blair staff 0B Oct 28 15:45 log2-rw-r--r-- 1 blair staff 0B Oct 28 15:45 log3-rw-r--r-- 1 blair staff 0B Oct 28 15:45 log4(vpy2)# ~/ghome/github/test [15:47:38]➜ find . -name &quot;*log*&quot; -mtime -1 -ok rm &#123;&#125; \\;&quot;rm ./log4&quot;? 在目录中查找更改时间在n日以前的文件并删除它们，在删除之前先给出提示 5.4 -exec中使用grep命令 1find /etc -name &quot;passwd*&quot; -exec grep &quot;root&quot; &#123;&#125; \\; 5.5 查找文件移动到指定目录 1find . -name &quot;*.log&quot; -exec mv &#123;&#125; .. \\; 5.6 exec选项执行cp命令 1find . -name &quot;*.log&quot; -exec cp &#123;&#125; test3 \\; 6. find命令之xargs 在使用 find命令的-exec选项处理匹配到的文件时， find命令将所有匹配到的文件一起传递给exec执行。但有些系统对能够传递给exec的命令长度有限制，这样在find命令运行几分钟之后，就会出现溢出错误。错误信息通常是“参数列太长”或“参数列溢出”。这就是xargs命令的用处所在，特别是与find命令一起使用。 find命令把匹配到的文件传递给xargs命令，而xargs命令每次只获取一部分文件而不是全部，不像-exec选项那样。这样它可以先处理最先获取的一部分文件，然后是下一批，并如此继续下去。 在有些系统中，使用-exec选项会为处理每一个匹配到的文件而发起一个相应的进程，并非将匹配到的文件全部作为参数一次执行；这样在有些情况下就会出现进程过多，系统性能下降的问题，因而效率不高； 而使用xargs命令则只有一个进程。另外，在使用xargs命令时，究竟是一次获取所有的参数，还是分批取得参数，以及每一次获取参数的数目都会根据该命令的选项及系统内核中相应的可调参数来确定。 6.1 find . -type f -print | xargs file 1234567# ~/ghome/github/test [7:41:40]➜ find . -type f -print | xargs file./log4: empty./log3: empty./log2: empty./log1: ASCII text(vpy2) 6.2 用户具有读、写和执行权限的文件，并收回相应的写权限 find . -perm -7 -print | xargs chmod o-w 12345678910111213141516# ~/ghome/github/test [7:44:26]➜ find . -perm -7 -print./log3(vpy2)# ~/ghome/github/test [7:44:27]➜ find . -perm -7 -print | xargs chmod o-w(vpy2)# ~/ghome/github/test [7:44:49]➜ lltotal 8-rw-r--r-- 1 blair staff 32B Oct 28 15:54 log1-rw-r--r-- 1 blair staff 0B Oct 28 15:45 log2-rwxrwxr-x 1 blair staff 0B Oct 28 15:45 log3-rw-r--r-- 1 blair staff 0B Oct 28 15:45 log4(vpy2)# ~/ghome/github/test [7:44:50] 6.3 grep命令在所有的普通文件中搜索hostname这个词 find . -type f -print | xargs grep “hostname” 1234[root@localhost test]# find . -type f -print | xargs grep &quot;hostname&quot;./log2013.log:hostnamebaidu=baidu.com./log2013.log:hostnamesina=sina.com./log2013.log:hostnames=true[root@localhost test]# 6.4 使用xargs执行mv find . -name “*.log” | xargs -i mv {} test4 使用-i参数默认的前面输出用{}代替，-I参数可以指定其他代替字符 6.5 xargs的-p参数的使用 find . -name “*.log” | xargs -p -i mv {} … -p参数会提示让你确认是否执行后面的命令,y执行，n不执行。 7. find 命令的参数详解 Reference 每天一个linux命令目录","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.iequa.com/tags/linux/"}]},{"title":"Linux File Basic Cmd","slug":"devops/linux-cmd-1-file-directory-operation","date":"2017-10-23T14:04:21.000Z","updated":"2021-06-20T04:12:28.201Z","comments":true,"path":"2017/10/23/devops/linux-cmd-1-file-directory-operation/","link":"","permalink":"http://www.iequa.com/2017/10/23/devops/linux-cmd-1-file-directory-operation/","excerpt":"ls, cd, pwd, mkdir, rm, rmdir, mv, cp, touch, cat, nl, more, less, head, tail","text":"ls, cd, pwd, mkdir, rm, rmdir, mv, cp, touch, cat, nl, more, less, head, tail 1. ls 12ls -l -R /home/blair/ghomels -ctrl s* 列出 /opt/soft 文件下面的子目录 1ls -F /opt/soft |grep /$ 目录于名称后加&quot;/&quot;, 可执行档于名称后加&quot;*&quot; 1ls -AFl 列出当前目录下的所有文件（包括隐藏文件）的绝对路径， 对目录不做递归 1find $PWD -maxdepth 1 | xargs ls -ld 递归列出当前目录下的所有文件（包括隐藏文件）的绝对路径 1find $PWD | xargs ls -ld 2. cd cd - or cd or cd ~ 3. pwd pwd -P 显示出实际路径，而非使用连接（link）路径 12345678910# /usr/local/xsoft/software/scala [22:17:59]➜ pwd/usr/local/xsoft/software/scala(vpy3)# /usr/local/xsoft/software/scala [22:17:59]➜ pwd -P/usr/local/xsoft/deploy/scala-2.11.7(vpy3)# /usr/local/xsoft/software/scala [22:18:02]➜ 4. mkdir 1234mkdir test1mkdir -p test2/test22mkdir -m 777 test3mkdir -v test4 一个命令创建项目的目录结构 123# ~/ghome/seek/test2 [22:23:06]➜ mkdir -vp scf/&#123;lib/,bin/,doc/&#123;info,product&#125;,logs/&#123;info,product&#125;,service/deploy/&#123;info,product&#125;&#125;(vpy3) 1234567891011121314151617181920# ~/ghome/seek/test2 [22:23:08]➜ tree.└── scf ├── bin ├── doc │ ├── info │ └── product ├── lib ├── logs │ ├── info │ └── product └── service └── deploy ├── info └── product13 directories, 0 files(vpy3)# ~/ghome/seek/test2 [22:23:10] 5. rm -f, --force 忽略不存在的文件，从不给出提示。 -i, --interactive 进行交互式删除 -r, -R, --recursive 指示rm将参数中列出的全部目录和子目录均递归地删除。 -v, --verbose 详细显示进行的步骤 123# ~/ghome/seek/test2 [22:29:28]➜ rm -i f1remove f1? 自定义回收站功能 1myrm()&#123; D=/tmp/$(date +%Y%m%d%H%M%S); mkdir -p $D; mv &quot;$@&quot; $D &amp;&amp; echo &quot;moved to $D ok&quot;; &#125; for example: 12345678910111213141516171819202122[root@localhost test]# myrm()&#123; D=/tmp/$(date +%Y%m%d%H%M%S); mkdir -p $D; mv &quot;$@&quot; $D &amp;&amp; echo &quot;moved to $D ok&quot;; &#125;[root@localhost test]# alias rm=&#x27;myrm&#x27;[root@localhost test]# touch 1.log 2.log 3.log[root@localhost test]# ll总计 16-rw-r--r-- 1 root root 0 10-26 15:08 1.log-rw-r--r-- 1 root root 0 10-26 15:08 2.log-rw-r--r-- 1 root root 0 10-26 15:08 3.logdrwxr-xr-x 7 root root 4096 10-25 18:07 scfdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5[root@localhost test]# rm [123].logmoved to /tmp/20121026150901 ok[root@localhost test]# ll总计 16drwxr-xr-x 7 root root 4096 10-25 18:07 scfdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5[root@localhost test]# ls /tmp/20121026150901/1.log 2.log 3.log[root@localhost test]# **Reference: ** 每天一个linux命令（5）：rm 命令 6. rmdir 删除空目录 rmdir -p 当子目录被删除后使它也成为空目录的话，则顺便一并删除 12345678910111213141516[root@localhost scf]# rmdir -p logsrmdir: logs: 目录非空[root@localhost scf]# tree.|-- bin|-- doc|-- lib|-- logs| `-- product`-- service `-- deploy |-- info `-- product 9 directories, 0 files [root@localhost scf]# rmdir -p logs/product 123456789[root@localhost scf]# tree.|-- bin|-- doc|-- lib`-- service`-- deploy |-- info `-- product 7. mv -i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！ -t ： --target-directory=DIRECTORY move all SOURCE arguments into DIRECTORY，即指定mv的目标目录，该选项适用于移动多个源文件到一个目录的情况，此时目标目录在前，源文件在后。 8. cp 12cp -a test3 test4cp -s log.log log_link.log 9. touch 设定文件的时间戳 1touch -t 201211142234.50 log.log 更新 log2012.log 的时间和 log.log 时间戳相同 1touch -r log.log log2012.log 10. cat 一次显示整个文件:cat filename 从键盘创建一个文件:cat &gt; filename 只能创建新文件,不能编辑已有文件. 将几个文件合并为一个文件:cat file1 file2 &gt; file 1234567891011➜ cat -n log.log log2012.log 1 asd 2 3 asd 1 k1 2 k2 3 4 k3(vpy3)# ~/ghome/seek/test2 [23:02:19]➜ tac (反向列示). tac log.txt 11. nl nl命令在linux系统中用来计算文件中行号。nl 可以将输出的文件内容自动的加上行号！ 命令参数： 123456789-b ：指定行号指定的方式，主要有两种：-b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)；-b t ：如果有空行，空的那一行不要列出行号(默认值)；-n ：列出行号表示的方法，主要有三种：-n ln ：行号在萤幕的最左方显示；-n rn ：行号在自己栏位的最右方显示，且不加 0 ；-n rz ：行号在自己栏位的最右方显示，且加 0 ；-w ：行号栏位的占用的位数。-p 在逻辑定界符处不重新开始计算。 123456[root@localhost test]# nl log2012.log 1 2012-01 2 2012-02 3 ======[root@localhost test]# 123456[root@localhost test]# nl -b a log2012.log 1 2012-01 2 2012-02 3 4 5 ======[root@localhost test]# 12345678910111213141516171819202122232425262728[root@localhost test]# nl -b a -n rz log2014.log 000001 2014-01000002 2014-02000003 2014-03000004 2014-04000005 2014-05000006 2014-06000007 2014-07000008 2014-08000009 2014-09000010 2014-10000011 2014-11000012 2014-12000013 =======[root@localhost test]# nl -b a -n rz -w 3 log2014.log 001 2014-01002 2014-02003 2014-03004 2014-04005 2014-05006 2014-06007 2014-07008 2014-08009 2014-09010 2014-10011 2014-11012 2014-12013 ======= 说明： nl -b a -n rz 命令行号默认为六位，要调整位数可以加上参数 -w 3 调整为3位。 12. more 命令参数： 12+n 从笫n行开始显示-n 定义屏幕大小为n行 12345678910111213[root@localhost test]# cat log2012.log 2012-012012-022012-032012-04-day12012-04-day22012-04-day3======[root@localhost test]# more +3 log2012.log 2012-032012-04-day12012-04-day22012-04-day3======[root@localhost test]# 设定每屏显示行数 more -5 log2012.log. 123456[root@localhost test]# more -5 log2012.log 2012-012012-022012-032012-04-day12012-04-day2 列一个目录下的文件，由于内容太多，我们应该学会用more来分页显示。这得和管道 | 结合起来 1ls -l | more -5 13. less less 工具也是对文件或其它输出进行分页显示的工具，应该说是linux正统查看文件内容的工具，功能极其强大。 less 的用法比起 more 更加的有弹性。在 more 的时候，我们并没有办法向前面翻， 只能往后面看，但若使用了 less 时，就可以使用 [pageup] [pagedown] 等按键的功能来往前往后翻看文件，更容易用来查看一个文件的内容！ 在 less 里头可以拥有更多的搜索功能，不止可以向下搜，也可以向上搜。 12345less log2013.logps -ef |lesshistory | less 输出文件除了最后n行的全部内容 1head -n -6 log2014.log 14. head 123456[root@localhost test]# head -n 5 log2014.log 2014-012014-022014-032014-042014-05[root@localhost test]# 显示文件前n个字节 head -c 20 log2014.log 12345[root@localhost test]# head -c 20 log2014.log2014-012014-022014[root@localhost test]# 输出文件除了最后n行的全部内容 1head -n -6 log2014.log 15. tail 显示文件末尾内容. tail -n 5 log2014.log 123456[root@localhost test]# tail -n 5 log2014.log 2014-092014-102014-112014-12==============================[root@localhost test]# 循环查看文件内容 tail -f test.log Reference 每日一linux命令 每天一个linux命令目录","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.iequa.com/tags/linux/"}]},{"title":"My Zsh Config Files","slug":"devops/oh-my-zsh-config","date":"2017-10-21T10:16:21.000Z","updated":"2021-06-20T04:12:28.202Z","comments":true,"path":"2017/10/21/devops/oh-my-zsh-config/","link":"","permalink":"http://www.iequa.com/2017/10/21/devops/oh-my-zsh-config/","excerpt":"我的配置文件 ~/.zprofile 和 ~/.zshrc","text":"我的配置文件 ~/.zprofile 和 ~/.zshrc zprofile 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152###################################################### blair custom config @2020.07.07 ######################################################alias ipython=&#x27;python -m IPython&#x27;MS=/usr/local/xsoft### JAVA ###JAVA_HOME=$MS/jdk/Contents/HomeJAVA_BIN=$JAVA_HOME/binPATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jarexport JAVA_HOME JAVA_BIN PATH CLASSPATHexport HADOOP_HOME=/usr/local/Cellar/hadoop/3.2.1_1export HADOOP_COMMON_HOME=$HADOOP_HOMEexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/binalias start-hp=&#x27;/usr/local/Cellar/hadoop/3.2.1_1/sbin/start-all.sh&#x27;alias stop-hp=&#x27;/usr/local/Cellar/hadoop/3.2.1_1/sbin/stop-all.sh&#x27;export SCALA_HOME=/usr/local/Cellar/scala/2.13.3export PATH=$PATH:$SCALA_HOME/binexport SPARK_HOME=$MS/sparkexport PATH=$PATH:$SPARK_HOME/binalias start-sk=&#x27;/usr/local/xsoft/spark/sbin/start-all.sh&#x27;alias stop-sk=&#x27;/usr/local/xsoft/spark/sbin/stop-all.sh&#x27;### Maven ###M2_HOME=/usr/local/xsoft/software/apache-mavenMAVEN_HOME=$M2_HOMEM3_HOME=$M2_HOMEPATH=$M3_HOME/bin:$PATH#MAVEN_OPTS=&quot;-Xms128m -Xmx512m&quot;export MAVEN_HOME M2_HOME PATH### Tomcat ###CATALINA_HOME=/usr/local/xsoft/software/apache-tomcatPATH=$CATALINA_HOME/bin:$PATHexport CATALINA_HOME PATH### Node.js ###export NVM_DIR=&quot;$HOME/.nvm&quot;#[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; \\. &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm#[ -s &quot;$NVM_DIR/bash_completion&quot; ] &amp;&amp; \\. &quot;$NVM_DIR/bash_completion&quot; # This loads nvm bash_completion### LANG ###export LC_ALL=en_US.UTF-8export LANG=en_US.UTF-8### Fast Function #### show ipalias ip=&#x27;ipconfig getifaddr en0&#x27;alias ip0=&#x27;ipconfig getifaddr en0&#x27;alias ip1=&#x27;ipconfig getifaddr en1&#x27;# shutdownalias shuth=&#x27;sudo shutdown -h now&#x27;alias shutr=&#x27;sudo shutdown -r now&#x27;# grepalias fgstr=&#x27;find . | xargs grep -ri&#x27;#find .|xargs grep -ri &quot;IBM&quot; # pip listalias pipl=&#x27;pip list --format=columns&#x27;# lsof -i:portalias lsofi=&#x27;lsof -i:&#x27;# hexoalias hs=&#x27;cd ~/ghome/blog &amp;&amp; hexo clean &amp;&amp; hexo s -p 5000 &amp;&#x27;alias hsy=&#x27;cd ~/ghome/blogyih &amp;&amp; hexo clean &amp;&amp; hexo s -p 5001 &amp;&#x27;alias cpost=&#x27;cd ~/ghome/blog/source/_posts&#x27;# deploy blogalias db=&#x27;~/ghome/blog &amp;&amp; sh dp.sh&#x27;alias dby=&#x27;cd ~/ghome/blogyih &amp;&amp; sh dp.sh&#x27;alias gp=&#x27;cd ~/ghome/blog/source &amp;&amp; git pull &amp;&amp; cd ~/ghome/blog/52binge.github.io &amp;&amp; git pull &amp;&amp; cd .. &amp;&amp; clear&#x27;alias gpy=&#x27;cd ~/ghome/blogyih/source &amp;&amp; git pull &amp;&amp; cd ~/ghome/blogyih/52binge.github.io &amp;&amp; git pull &amp;&amp; cd .. &amp;&amp; clear&#x27;# Dockeralias dis=&#x27;docker images&#x27;alias dri=&#x27;docker rmi&#x27;alias dpa=&#x27;docker ps -a&#x27;alias dks=&#x27;docker stop&#x27;alias dck=&#x27;docker container kill&#x27;alias dcr=&#x27;docker container rm&#x27;alias dlog=&#x27;docker logs&#x27;# k8salias kb=&#x27;kubectl&#x27;alias kbg=&#x27;kubectl get&#x27;alias kbd=&#x27;kubectl delete&#x27;alias kbgdps=&#x27;kbg deploy,po,svc&#x27;alias kbdev=&#x27;export KUBECONFIG=~/kube/dev-kubeconfig&#x27;alias kbtest=&#x27;export KUBECONFIG=~/kube/test-kubeconfig&#x27;# work## downloadrqh=&#x27;rsync -v --progress qh:&#x27;# rsync -v --progress qh:/home/clb/Cyberduck-7.1.0.31395.zip .## upload# rsync -z -P demo.zip qh:/data/Libin/work_project/alias cpysk=&quot;/Users/blair/ghome/github/spark3.0/pyspark&quot;alias cwk=&#x27;cd ~/ghome/6E/work_project&#x27;alias csf=&#x27;cd ~/ghome/6E/self_project&#x27;alias cmlsearch=&#x27;cd ~/ghome/6E/work_project/mlsearch&#x27;alias cmlar=&#x27;cd ~/ghome/6E/work_project/mlar&#x27;alias sqh=&#x27;ssh -L 8889:localhost:8888 -CN qh&#x27;alias sqh-mlsearch=&#x27;ssh -L 8001:localhost:8001 -CN qh&#x27;alias sqh-mlar=&#x27;ssh -L 8003:localhost:8003 -CN qh&#x27;alias sqh-mlar-cs=&#x27;ssh -L 8300:localhost:8300 -CN qh&#x27;alias local_to_es=&#x27;ssh -L 9200:192.168.x.xxx:9200 -CN onenorth&#x27;alias kibana=&quot;ssh -D 1080 -CN onenorth&quot;alias catma=&#x27;conda activate mlar&#x27;alias catms=&#x27;conda activate mlsearch&#x27;alias catnn=&#x27;conda activate nn_framework&#x27;alias catsk=&#x27;conda activate spark&#x27;alias cata=&#x27;conda activate anaconda3&#x27;alias doc=&#x27;~/ghome/6E/chenlibin/data-doc/source/weekly&#x27;### Pyenv ####export PATH=&quot;/Users/blair/.pyenv/bin:$PATH&quot;#alias pyenv_init=&#x27;eval &quot;$(pyenv init -)&quot; &amp;&amp; eval &quot;$(pyenv virtualenv-init -)&quot;&#x27;#pyenv_init#export PYENV_VIRTUALENV_DISABLE_PROMPT=1# phalias gitph=&#x27;vim ~/.oh-my-zsh/plugins/git/git.plugin.zsh&#x27;# zshrcalias vzp=&#x27;vim ~/.zprofile&#x27;alias szp=&#x27;source ~/.zprofile&#x27;#docker run --publish=7474:7474 --publish=7687:7687 --volume=$HOME/neo4j/data:/data neo4j#pyenv-virtualenv: prompt changing will be removed from future release. configure `export PYENV_VIRTUALENV_DISABLE_PROMPT=1&#x27; to simulate the behavior. 以下内容是上一个版本的内容，请忽略… 1. python 12345678910111213141516171819### IPython ###alias ipython=&#x27;python -m IPython&#x27;### Pyenv ###export PATH=&quot;/Users/blair/.pyenv/bin:$PATH&quot;alias pyenv_init=&#x27;eval &quot;$(pyenv init -)&quot; &amp;&amp; eval &quot;$(pyenv virtualenv-init -)&quot;&#x27;pyenv_init#export PYENV_VIRTUALENV_DISABLE_PROMPT=1# pip listalias pipl=&#x27;pip list --format=columns&#x27;# pyenvalias pyenv_install_python_pre=&#x27;source $MS/custom-machine/pyenv-install.sh&#x27;alias py2=&#x27;pyenv_init &amp;&amp; pyenv activate vpy2 &amp;&amp; clear &amp;&amp; python -V&#x27;alias py3=&#x27;pyenv_init &amp;&amp; pyenv activate vpy3.5 &amp;&amp; clear &amp;&amp; python -V&#x27;alias vca3=&#x27;pyenv activate vconda3 &amp;&amp; clear &amp;&amp; python -V&#x27;alias pyde=&#x27;pyenv deactivate&#x27;alias pys=&#x27;pyenv activate vpy3 &amp;&amp; pyenv global system &amp;&amp; pyde &amp;&amp; python -V&#x27; 2. hexo blog 12345678910# hexoalias hs=&#x27;cd ~/ghome/blog &amp;&amp; hexo clean &amp;&amp; hexo s -p 5000 &amp;&#x27;alias hsy=&#x27;cd ~/ghome/blogyih &amp;&amp; hexo clean &amp;&amp; hexo s -p 5001 &amp;&#x27;# deploy blogalias db=&#x27;~/ghome/blog &amp;&amp; sh dp.sh&#x27;alias dby=&#x27;~/ghome/blogyih &amp;&amp; sh dp.sh&#x27;alias gp=&#x27;cd ~/ghome/blog/source &amp;&amp; git pull &amp;&amp; cd ~/ghome/blog/52binge.github.io &amp;&amp; git pull &amp;&amp; cd .. &amp;&amp; clear&#x27;alias gpy=&#x27;cd ~/ghome/blogyih/source &amp;&amp; git pull &amp;&amp; cd ~/ghome/blogyih/52binge.github.io &amp;&amp; git pull &amp;&amp; cd .. &amp;&amp; clear&#x27; 3. shortcut approach 1234567891011121314151617181920212223### Fast Function #### show ipalias ip=&#x27;ipconfig getifaddr en0&#x27;alias ip0=&#x27;ipconfig getifaddr en0&#x27;alias ip1=&#x27;ipconfig getifaddr en1&#x27;# shutdownalias shuth=&#x27;sudo shutdown -h now&#x27;alias shutr=&#x27;sudo shutdown -r now&#x27;# grepalias fgstr=&#x27;find . | xargs grep -ri&#x27;#find .|xargs grep -ri &quot;IBM&quot; # lsof -i:portalias lsofi=&#x27;lsof -i:&#x27;# zshrcalias vzp=&#x27;vim ~/.zprofile&#x27;alias szp=&#x27;source ~/.zprofile&#x27;#pyenv-virtualenv: prompt changing will be removed from future release. configure `export PYENV_VIRTUALENV_DISABLE_PROMPT=1&#x27; to simulate the behavior. 4. docker, k8s 12345678910111213141516# Dockeralias dis=&#x27;docker images&#x27;alias dri=&#x27;docker rmi&#x27;alias dpa=&#x27;docker ps -a&#x27;alias dks=&#x27;docker stop&#x27;alias dck=&#x27;docker container kill&#x27;alias dcr=&#x27;docker container rm&#x27;alias dlog=&#x27;docker logs&#x27;# k8salias kb=&#x27;kubectl&#x27;alias kbg=&#x27;kubectl get&#x27;alias kbd=&#x27;kubectl delete&#x27;alias kbgdps=&#x27;kbg deploy,po,svc&#x27;alias kbdev=&#x27;export KUBECONFIG=~/kube/dev-kubeconfig&#x27;alias kbtest=&#x27;export KUBECONFIG=~/kube/test-kubeconfig&#x27; 5. work 12345678910111213141516# workalias rqh=&#x27;rsync -v --progress qh:&#x27;alias cw=&#x27;cd ~/ghome/6E/work_project&#x27;alias cs=&#x27;cd ~/ghome/6E/self_project&#x27;alias cmlar=&#x27;cd ~/ghome/6E/work_project/mlar&#x27;alias sqh=&#x27;ssh -L 8889:localhost:8888 -CN qh&#x27;alias catm=&#x27;conda activate mlar&#x27;alias catn=&#x27;conda activate nn_framework&#x27;alias cata=&#x27;conda activate anaconda3&#x27;alias doc=&#x27;~/ghome/6E/chenlibin/data-doc&#x27; 6. node.js 1234### Node.js ###export NVM_DIR=&quot;$HOME/.nvm&quot;#[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; \\. &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm#[ -s &quot;$NVM_DIR/bash_completion&quot; ] &amp;&amp; \\. &quot;$NVM_DIR/bash_completion&quot; # This loads nvm bash_completion 7. mac env 123### LANG ###export LC_ALL=en_US.UTF-8export LANG=en_US.UTF-8 8. Java 12345678910111213141516171819### JAVA ###JAVA_HOME=/Library/Java/JavaVirtualMachines/Contents/HomeJAVA_BIN=$JAVA_HOME/binPATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jarexport JAVA_HOME JAVA_BIN PATH CLASSPATH### Maven ###M2_HOME=/usr/local/xsoft/software/apache-mavenMAVEN_HOME=$M2_HOMEM3_HOME=$M2_HOMEPATH=$M3_HOME/bin:$PATHMAVEN_OPTS=&quot;-Xms128m -Xmx512m&quot;export MAVEN_HOME M2_HOME PATH### Tomcat ###CATALINA_HOME=/usr/local/xsoft/software/apache-tomcatPATH=$CATALINA_HOME/bin:$PATHexport CATALINA_HOME PATH 9. Scala 12345678910111213### Scala ###export SCALA_HOME=/usr/local/xsoft/software/scalaexport PATH=$&#123;SCALA_HOME&#125;/bin:$PATH#export SCALA_HOME=/usr/local/Cellar/scala/2.11.5### KafkaKAFKA_HOME=/usr/local/Cellar/kafka/2.1.0PATH=$KAFKA_HOME/bin:$PATHexport KAFKA_HOME PATH### Spark ###export SPARK_HOME=/usr/local/xsoft/software/sparkexport PATH=$SPARK_HOME/bin:$PATH 10. zshrc 123456789101112131415161718# If you come from bash you might have to change your $PATH.# export PATH=$HOME/bin:/usr/local/bin:$PATH# Path to your oh-my-zsh installation.export ZSH=/Users/blair/.oh-my-zsh# Set name of the theme to load. Optionally, if you set this to &quot;random&quot;# it&#x27;ll load a random theme each time that oh-my-zsh is loaded.# See https://github.com/robbyrussell/oh-my-zsh/wiki/ThemesZSH_THEME=&quot;robbyrussell&quot;ZSH_THEME=&quot;astro&quot; plugins=(git)plugins=(git autojump) # by blair add @2017-10-10#[[ -s $(brew --prefix)/etc/profile.d/autojump.sh ]] &amp;&amp; . $(brew --prefix)/etc/profile.d/autojump.shsource $ZSH/oh-my-zsh.sh","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"zshrc","slug":"zshrc","permalink":"http://www.iequa.com/tags/zshrc/"}]},{"title":"Macos Terminal Set Shadowsocks","slug":"devops/ops-mac-for-shadowsocks","date":"2017-10-19T14:24:21.000Z","updated":"2021-06-20T04:12:28.199Z","comments":true,"path":"2017/10/19/devops/ops-mac-for-shadowsocks/","link":"","permalink":"http://www.iequa.com/2017/10/19/devops/ops-mac-for-shadowsocks/","excerpt":"first, you need to have vps second, you need to have shadowsocks app","text":"first, you need to have vps second, you need to have shadowsocks app &lt;img src=&quot;/images/ops/ops-vpn-shadowsocks.png&quot; width=“520” height=“300” align=“middle” /img&gt; install 1brew install polipo config 设置每次登陆启动polipo 1ln -sfv /usr/local/opt/polipo/*.plist ~/Library/LaunchAgents 修改文件 /usr/local/opt/polipo/homebrew.mxcl.polipo.plist 设置parentProxy 1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;&lt;plist version=&quot;1.0&quot;&gt; &lt;dict&gt; &lt;key&gt;Label&lt;/key&gt; &lt;string&gt;homebrew.mxcl.polipo&lt;/string&gt; &lt;key&gt;RunAtLoad&lt;/key&gt; &lt;true/&gt; &lt;key&gt;KeepAlive&lt;/key&gt; &lt;true/&gt; &lt;key&gt;ProgramArguments&lt;/key&gt; &lt;array&gt; &lt;string&gt;/usr/local/opt/polipo/bin/polipo&lt;/string&gt; &lt;string&gt;socksParentProxy=localhost:1080&lt;/string&gt; &lt;/array&gt; &lt;!-- Set `ulimit -n 20480`. The default OS X limit is 256, that&#x27;s not enough for Polipo (displays &#x27;too many files open&#x27; errors). It seems like you have no reason to lower this limit (and unlikely will want to raise it). --&gt; &lt;key&gt;SoftResourceLimits&lt;/key&gt; &lt;dict&gt; &lt;key&gt;NumberOfFiles&lt;/key&gt; &lt;integer&gt;20480&lt;/integer&gt; &lt;/dict&gt; &lt;/dict&gt;&lt;/plist&gt; 修改的地方是增加了 &lt;string&gt;socksParentProxy=localhost:1080&lt;/string&gt; start / stop 12launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.polipo.plistlaunchctl load ~/Library/LaunchAgents/homebrew.mxcl.polipo.plist setting profile vim ~/.zshrc 12#export http_proxy=http://localhost:8123alias hp=&quot;http_proxy=http://localhost:8123&quot; source ~/.zshrc test 12➜ hp curl ip.cn当前 IP：97.64.**.** 来自：美国 reference article 技术小黑屋 Mac+shadowsocks+polipo快捷实现终端科学上网 VPS-install-shadowsocks-proxy","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Shadowsocks","slug":"Shadowsocks","permalink":"http://www.iequa.com/tags/Shadowsocks/"}]},{"title":"Macos Sierra Uninstall App & Solve Allow apps Anywhere","slug":"devops/ops-mac-sierra-some-tips","date":"2017-10-19T13:57:21.000Z","updated":"2021-06-20T04:12:28.201Z","comments":true,"path":"2017/10/19/devops/ops-mac-sierra-some-tips/","link":"","permalink":"http://www.iequa.com/2017/10/19/devops/ops-mac-sierra-some-tips/","excerpt":"Here are some tips to help you fun macos sierra","text":"Here are some tips to help you fun macos sierra MacOS Sierra &amp; High Sierra Complete Uninstall App About This Mac -&gt; Storage -&gt; Manage -&gt; Applications -&gt; Delete How to Allow Apps from Anywhere in Gatekeeper for macOS High Sierra 1). Open the Terminal app from the /Applications/Utilities/ folder and then enter the following command syntax: 1sudo spctl --master-disable Hit return and authenticate with an admin password 2). Relaunch System Preferences and go to “Security &amp; Privacy” and the “General” tab 3). You will now see the “Anywhere” option under ‘Allow apps downloaded from:’ Gatekeeper options","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"mac","slug":"mac","permalink":"http://www.iequa.com/tags/mac/"}]},{"title":"Pyenv Install For Virtual Multi Python Version Switch","slug":"devops/ops-pyenv-install","date":"2017-10-18T12:16:21.000Z","updated":"2021-06-20T04:12:28.203Z","comments":true,"path":"2017/10/18/devops/ops-pyenv-install/","link":"","permalink":"http://www.iequa.com/2017/10/18/devops/ops-pyenv-install/","excerpt":"It needs to be used in both python2 and python3 environments, or different packages need to be installed in different projects.","text":"It needs to be used in both python2 and python3 environments, or different packages need to be installed in different projects. we hope that the packages installed between different projects do not interfere with each other, and then you can configure the virtual environment of Python using pyenv. installation pyenv Official Pyenv Install 1234export PATH=&quot;~/.pyenv/bin:$PATH&quot;eval &quot;$(pyenv init -)&quot;eval &quot;$(pyenv virtualenv-init -)&quot;#export PYENV_VIRTUALENV_DISABLE_PROMPT=1 按照官方文档配置即可，mac zsh 用户，将 以上三句放入到 .zshrc 即可。 see available versions 1pyenv install -l install python in virtual env install python 2.7.14 1pyenv install 2.7.14 install python 3.6.3 1pyenv install 3.6.3 solve macOS High Sierra: ERROR: The Python ssl extension was not compiled. Missing the OpenSSL lib? 1234567891011# about zlibexport CFLAGS=&quot;-I$(xcrun --show-sdk-path)/usr/include&quot;# about readlineexport CFLAGS=&quot;-I$(brew --prefix readline)/include $CFLAGS&quot;export LDFLAGS=&quot;-L$(brew --prefix readline)/lib $LDFLAGS&quot;# about opensslexport CFLAGS=&quot;-I$(brew --prefix openssl)/include $CFLAGS&quot;export LDFLAGS=&quot;-L$(brew --prefix openssl)/lib $LDFLAGS&quot;# about SQLite (maybe not necessary)export CFLAGS=&quot;-I$(brew --prefix sqlite)/include $CFLAGS&quot;export LDFLAGS=&quot;-L$(brew --prefix sqlite)/lib $LDFLAGS&quot; Set or show the global Python version 1$ pyenv global 3.6.3 or system system stands for this mac show list all Python versions available to pyenv 1$ pyenv versions create virtual env create current 3.6.3 version python virtual env 1$ pyenv virtualenv vpy3 vpy3 is this virtual env alias pyenv activate &amp; deactivate 1$ pyenv activate vpy3 这是时候就可以开始pip安装依赖包了 123456789101112➜ pyenv activate vpy3pyenv-virtualenv: prompt changing will be removed from future release. configure `export PYENV_VIRTUALENV_DISABLE_PROMPT=1&#x27; to simulate the behavior.(vpy3)# ~ [12:25:44]➜ python -VPython 3.6.3(vpy3)# ~ [12:25:49]➜ pyenv deactivate vpy3# ~ [12:25:57]➜ python -VPython 2.7.10","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"help","slug":"help","permalink":"http://www.iequa.com/tags/help/"}]},{"title":"Common Useful Links","slug":"devops/ops-common-links","date":"2017-10-14T12:16:21.000Z","updated":"2021-06-20T04:12:28.200Z","comments":true,"path":"2017/10/14/devops/ops-common-links/","link":"","permalink":"http://www.iequa.com/2017/10/14/devops/ops-common-links/","excerpt":"Here are some useful links","text":"Here are some useful links Friends ali wuchong 阮一峰的网络日志 ICML 学霸 David Abel 酷壳 vps www.yuntionly.com www.wisevpn.net www.banwago.com www.godaddy.com 搬瓦工中文网 搬瓦工购买页面 搬瓦工VPS续费的那些事 搬瓦工取消一键SS功能后，教您三种方法轻松搭建SS！ 登陆到搬瓦工后台, 一键安装SS，类似方法一（小白适用 Centos6.8搭建SS ssh 登录搬瓦工机器 stop server @Main controls Root password modification start Server Root shell - interactive vi /etc/ssh/sshd_config, add 12PermitRootLogin yesPort 22 /etc/init.d/sshd restart ssh root@ip python 廖雪峰 Python 3 elasticsearch-dsl-py pypi.python.org/pypi install pyenv flask microframework pip install elasticsearch_dsl==0.0.11 shell runoob linux Mac OSX 沒有的 rename，用 brew 抓回來～ spark spark.apache.org python spark 1.6.3 github spark 在 pycharm 上配置 pyspark pycharm 上配置 pyspark Pycharm开发spark程序 devops 免登陆设置 ssh-copy-id -i id_rsa.pub hdfs@192.192.0.27 mac macOS (OS X) 有哪些常用的快捷键？ blog 鼓励工程师写blog 技术人员的发展之路","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"help","slug":"help","permalink":"http://www.iequa.com/tags/help/"}]},{"title":"Coursera 7 - Support Vector Machines","slug":"ml/coursera-ng-w7-svm","date":"2017-10-13T08:08:21.000Z","updated":"2021-06-20T04:12:28.312Z","comments":true,"path":"2017/10/13/ml/coursera-ng-w7-svm/","link":"","permalink":"http://www.iequa.com/2017/10/13/ml/coursera-ng-w7-svm/","excerpt":"From Logistic Regression to Support Vector Machines","text":"From Logistic Regression to Support Vector Machines 1. Large Margin Classification Alternation view of logistic regression $ \\begin{align} h_\\theta (x) = g({\\theta^T x}) = \\dfrac{1}{1 + e{-\\thetaT x}} \\end{align} ; , ; h_\\theta (x) \\in [0, 1] $ &lt;img src=&quot;/images/ml/coursera/ml-ng-w3-02.png&quot; width=“820” height=“500” align=“middle” /img&gt; $ y = 1 ; when ; h_\\theta(x) = g(\\theta^T x) \\geq 0.5 ; when ; \\theta^T x \\geq 0 $. $ y = 0 ; when ; h_\\theta(x) = g(\\theta^T x) \\le 0.5 ; when ; \\theta^T x \\le 0 $ We can compress our cost function’s two conditional cases into one case: $ \\mathrm{Cost}(h_\\theta(x),y) = - y \\cdot \\log(h_\\theta(x)) - (1 - y) \\cdot \\log(1 - h_\\theta(x))$ We can fully write out our entire cost function as follows: J(θ)=−1m∑_i=1m[y(i)log⁡(h_θ(x(i)))+(1−y(i))log⁡(1−h_θ(x(i)))] J(\\theta) = - \\frac{1}{m} \\displaystyle \\sum\\_{i=1}^m [y^{(i)}\\log (h\\_\\theta (x^{(i)})) + (1 - y^{(i)})\\log (1 - h\\_\\theta(x^{(i)}))] J(θ)=−m1​∑_i=1m[y(i)log(h_θ(x(i)))+(1−y(i))log(1−h_θ(x(i)))] J(θ)=min__θ1m[∑_i=1my(i) (−log⁡h_θ(x(i)))+(1−y(i))(−log⁡(1−h_θ(x(i))))]+λ2m∑_j=1nθ_j2 J(\\theta) = \\mathop{min}\\limits\\_{\\_\\theta} \\frac{1}{m} \\left[ \\displaystyle \\sum\\_{i=1}^m y^{(i)}\\ \\left(-\\log h\\_\\theta (x^{(i)}) \\right) + (1 - y^{(i)}) \\left( - \\log (1 - h\\_\\theta(x^{(i)})) \\right) \\right]+ \\frac{\\lambda}{2m} \\displaystyle \\sum\\_{j=1}^n \\theta\\_j^2 J(θ)=min__θm1​[∑_i=1my(i) (−logh_θ(x(i)))+(1−y(i))(−log(1−h_θ(x(i))))]+2mλ​∑_j=1nθ_j2 cost_1(θTxi)=−log⁡h_θ(x(i))cost\\_1(\\theta^T x^{i}) = -\\log h\\_\\theta (x^{(i)})cost_1(θTxi)=−logh_θ(x(i)) cost_0(θTxi)=−log⁡(1−h_θ(x(i)))cost\\_0(\\theta^T x^{i}) = - \\log (1 - h\\_\\theta(x^{(i)}))cost_0(θTxi)=−log(1−h_θ(x(i))) J(θ)=min__θ1m[∑_i=1my(i) (cost_1(θTxi))+(1−y(i))(cost_0(θTxi))]+λ2m∑_j=1nθ_j2 J(\\theta) = \\mathop{min}\\limits\\_{\\_\\theta} \\frac{1}{m} \\left[ \\displaystyle \\sum\\_{i=1}^m y^{(i)}\\ \\left(cost\\_1(\\theta^T x^{i}) \\right) + (1 - y^{(i)}) \\left( cost\\_0(\\theta^T x^{i}) \\right) \\right]+ \\frac{\\lambda}{2m} \\displaystyle \\sum\\_{j=1}^n \\theta\\_j^2 J(θ)=min__θm1​[∑_i=1my(i) (cost_1(θTxi))+(1−y(i))(cost_0(θTxi))]+2mλ​∑_j=1nθ_j2 1.1 Optimization Objective J(θ)=min__θ1m[∑_i=1my(i) (cost_1(θTxi))+(1−y(i))(cost_0(θTxi))]+λ2m∑_j=1nθ_j2 J(\\theta) = \\mathop{min}\\limits\\_{\\_\\theta} \\frac{1}{m} \\left[ \\displaystyle \\sum\\_{i=1}^m y^{(i)}\\ \\left(cost\\_1(\\theta^T x^{i}) \\right) + (1 - y^{(i)}) \\left( cost\\_0(\\theta^T x^{i}) \\right) \\right]+ \\frac{\\lambda}{2m} \\displaystyle \\sum\\_{j=1}^n \\theta\\_j^2 J(θ)=min__θm1​[∑_i=1my(i) (cost_1(θTxi))+(1−y(i))(cost_0(θTxi))]+2mλ​∑_j=1nθ_j2 令 C=1θC = \\frac{1}{\\theta}C=θ1​ J(θ)=min__θC∑_i=1m[y(i) cost_1(θTxi)+(1−y(i))cost_0(θTxi)]+12m∑_j=1nθ_j2 J(\\theta) = \\mathop{min}\\limits\\_{\\_\\theta} C \\displaystyle \\sum\\_{i=1}^m \\left[ y^{(i)}\\ cost\\_1(\\theta^T x^{i}) + (1 - y^{(i)}) cost\\_0(\\theta^T x^{i}) \\right]+ \\frac{1}{2m} \\displaystyle \\sum\\_{j=1}^n \\theta\\_j^2 J(θ)=min__θC∑_i=1m[y(i) cost_1(θTxi)+(1−y(i))cost_0(θTxi)]+2m1​∑_j=1nθ_j2 1.2 Large Margin Intuition &lt;img src=&quot;/images/ml/coursera/ml-ng-w7-svm-1.png&quot; width=“620” height=“400” align=“middle” /img&gt; &lt;img src=&quot;/images/ml/coursera/ml-ng-w7-svm-2.png&quot; width=“620” height=“400” align=“middle” /img&gt; &lt;img src=&quot;/images/ml/coursera/ml-ng-w7-svm-3.png&quot; width=“620” height=“400” align=“middle” /img&gt; 1.3 Mathematics Behind Large Margin Classification &lt;img src=&quot;/images/ml/coursera/ml-ng-w7-svm-4.png&quot; width=“620” height=“400” align=“middle” /img&gt; &lt;img src=&quot;/images/ml/coursera/ml-ng-w7-svm-5.png&quot; width=“620” height=“400” align=“middle” /img&gt; &lt;img src=&quot;/images/ml/coursera/ml-ng-w7-svm-6.png&quot; width=“620” height=“400” align=“middle” /img&gt; 2. Kernels 2.1 Kernels I 2.2 Kernels II 3. SVMs in Practice 3.1 Using An SVM Reference 零基础学SVM—Support Vector Machine(一)","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"http://www.iequa.com/tags/SVM/"}]},{"title":"My Blog Config.yml","slug":"devops/ops-hexo-blog_config.yml","date":"2017-10-08T12:16:21.000Z","updated":"2021-06-22T02:35:27.066Z","comments":true,"path":"2017/10/08/devops/ops-hexo-blog_config.yml/","link":"","permalink":"http://www.iequa.com/2017/10/08/devops/ops-hexo-blog_config.yml/","excerpt":"hexo blog config.yml file","text":"hexo blog config.yml file 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Blair&#x27;s Blogsubtitle: 春有百花秋有月，夏有涼風冬有雪 .# New Add @2021-06-14import: meta: - &lt;meta name=&quot;msapplication-TileColor&quot; content=&quot;#ffffff&quot;&gt; - &lt;meta name=&quot;msapplication-config&quot; content=&quot;https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/browserconfig.xml&quot;&gt; - &lt;meta name=&quot;theme-color&quot; content=&quot;#ffffff&quot;&gt; link: - &lt;link rel=&quot;apple-touch-icon&quot; sizes=&quot;180x180&quot; href=&quot;https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/apple-touch-icon.png&quot;&gt;# - &lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; sizes=&quot;32x32&quot; href=&quot;https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/favicon-32x32.png&quot;&gt;# - &lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; sizes=&quot;16x16&quot; href=&quot;https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/favicon-16x16.png&quot;&gt;# - &lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; sizes=&quot;32x32&quot; href=&quot;/images/logos/qq.png&quot;&gt; - &lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; sizes=&quot;32x32&quot; href=&quot;/images/logos/title-icon-1.png&quot;&gt; - &lt;link rel=&quot;manifest&quot; href=&quot;https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/site.webmanifest&quot;&gt; - &lt;link rel=&quot;mask-icon&quot; href=&quot;https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/safari-pinned-tab.svg&quot; color=&quot;#5bbad5&quot;&gt;# - &lt;link rel=&quot;shortcut icon&quot; href=&quot;https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/favicon.ico&quot;&gt; - &lt;link rel=&quot;shortcut icon&quot; href=&quot;/images/logos/title-icon-1.png&quot;&gt; script: - &lt;script&gt;&lt;/script&gt;language: - en - zh-Hans - zh-tw# - zh-CN# - zh-HK# - zh-TW# - zh-twdescription: Everyone should not forget his dreamauthor: Blair Chan#avatar: /images/avatar.jpegavatar: https://sfault-avatar.b0.upaiyun.com/336/414/3364146348-567f9c35c9ee6_huge256timezone:#leancloud_visitors:# enable: true# app_id: #&lt;AppID&gt;# app_key: #&lt;AppKEY&gt;#comments#disqus_shortname: blairos-sn# URL## If your site is put in a subdirectory, set url as &#x27;http://yoursite.com/child&#x27; and root as &#x27;/child/&#x27;#url: http://iequa.com/url: http://www.iequa.com/root: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 15pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/#theme: minos#theme: blairos# theme: landscape-plus#theme: next#theme: yilia# theme: matery#theme: even#theme: hexo-theme-fluid#theme: hexo-theme-ayer#theme: hexo-theme-volantistheme: my_volantis#theme: hexo-theme-ymd45921# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:type: gitrepository: https://github.com/52binge/52binge.github.io.gitbranch: mastermessage: Site updated at &#123;&#123; now(&quot;YYYY-MM-DD HH:mm:ss&quot;) &#125;&#125; # 设置我们提交的信息","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"macos","slug":"macos","permalink":"http://www.iequa.com/tags/macos/"}]},{"title":"My Vim Config File","slug":"devops/ops-vimrc","date":"2017-10-04T12:16:21.000Z","updated":"2021-06-20T04:12:28.200Z","comments":true,"path":"2017/10/04/devops/ops-vimrc/","link":"","permalink":"http://www.iequa.com/2017/10/04/devops/ops-vimrc/","excerpt":"my vim custom config .vimrc file","text":"my vim custom config .vimrc file 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352set nocompatible &quot; 关闭 vi 兼容模式 syntax on &quot; 自动语法高亮 set number &quot; 显示行号 &quot; set cursorline 突出显示当前行 set ruler &quot; 打开状态栏标尺 (不错) set shiftwidth=4 &quot; 设定 &lt;&lt; 和 &gt;&gt; 命令移动时的宽度为 4 set smartindent set tabstop=4 &quot; tabstop=4 设定 tab 长度为4 set softtabstop=4 set expandtab set encoding=utf-8 fileencodings=ucs-bom,utf-8,cp936set nobackup &quot; 覆盖文件时不备份 set ignorecase smartcase &quot; 搜索时忽略大小写，但在有一个或以上大写字母时仍大小写敏感 &quot;set nowrapscan &quot; 禁止在搜索到文件两端时重新搜索 set nowrapscan &quot; 禁止在搜索到文件两端时重新搜索 set incsearch &quot; 输入搜索内容时就显示搜索结果 set hlsearch &quot; 搜索时高亮显示被找到的文本 set noerrorbells &quot; 关闭错误信息响铃 set novisualbell &quot; 关闭使用可视响铃代替呼叫 set t_vb= &quot; 置空错误铃声的终端代码 set showmatch &quot; 插入括号时，短暂地跳转到匹配的对应括号 &quot; set matchtime=2 &quot; 短暂跳转到匹配括号的时间 set magic &quot; 设置魔术 set hidden &quot; 允许在有未保存的修改时切换缓冲区，此时的修改由 vim 负责保存 &quot;set guioptions-=T &quot; 隐藏工具栏 &quot;set guioptions-=m &quot; 隐藏菜单栏 set smartindent &quot; 开启新行时使用智能自动缩进 set backspace=indent,eol,start &quot; 不设定在插入状态无法用退格键和 Delete 键删除回车符 set cmdheight=1 &quot; 设定命令行的行数为 1 set laststatus=2 &quot; 显示状态栏 (默认值为 1, 无法显示状态栏) set statusline=\\ %&lt;%F[%1*%M%*%n%R%H]%=\\ %y\\ %0(%&#123;&amp;fileformat&#125;\\ %&#123;&amp;encoding&#125;\\ %c:%l/%L%)\\ &quot; 设置在状态行显示的信息 &quot;set foldenable &quot; 开始折叠 &quot;set foldmethod=syntax &quot; 设置语法折叠 &quot;set foldcolumn=0 &quot; 设置折叠区域的宽度 &quot;setlocal foldlevel=1 &quot; 设置折叠层数为 &quot; set foldclose=all &quot; 设置为自动关闭折叠 &quot; nnoremap &lt;space&gt; @=((foldclosed(line(&#x27;.&#x27;)) &lt; 0) ? &#x27;zc&#x27; : &#x27;zo&#x27;)&lt;CR&gt; &quot; 用空格键来开关折叠 set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936set termencoding=utf-8set encoding=utf-8 &quot; return OS type, eg: windows, or linux, mac, et.st.. function! MySys() if has(&quot;win16&quot;) || has(&quot;win32&quot;) || has(&quot;win64&quot;) || has(&quot;win95&quot;) return &quot;windows&quot; elseif has(&quot;unix&quot;) return &quot;linux&quot; endif endfunction &quot; 用户目录变量$VIMFILES if MySys() == &quot;windows&quot; let $VIMFILES = $VIM.&#x27;/vimfiles&#x27; elseif MySys() == &quot;linux&quot; let $VIMFILES = $HOME.&#x27;/.vim&#x27; endif &quot; 设定doc文档目录 let helptags=$VIMFILES.&#x27;/doc&#x27; &quot; 设置字体 以及中文支持 if has(&quot;win32&quot;) set guifont=Inconsolata:h12:cANSI endif &quot; 配置多语言环境 if has(&quot;multi_byte&quot;) &quot; UTF-8 编码 set encoding=utf-8 set termencoding=utf-8 set formatoptions+=mM set fencs=utf-8,gbk if v:lang =~? &#x27;^\\(zh\\)\\|\\(ja\\)\\|\\(ko\\)&#x27; set ambiwidth=double endif if has(&quot;win32&quot;) source $VIMRUNTIME/delmenu.vim source $VIMRUNTIME/menu.vim language messages zh_CN.utf-8 endif else echoerr &quot;Sorry, this version of (g)vim was not compiled with +multi_byte&quot; endif &quot; Buffers操作快捷方式! nnoremap &lt;C-RETURN&gt; :bnext&lt;CR&gt; nnoremap &lt;C-S-RETURN&gt; :bprevious&lt;CR&gt; &quot; Tab操作快捷方式! nnoremap &lt;C-TAB&gt; :tabnext&lt;CR&gt; nnoremap &lt;C-S-TAB&gt; :tabprev&lt;CR&gt; &quot;关于tab的快捷键 &quot; map tn :tabnext&lt;cr&gt; &quot; map tp :tabprevious&lt;cr&gt; &quot; map td :tabnew .&lt;cr&gt; &quot; map te :tabedit &quot; map tc :tabclose&lt;cr&gt; &quot;窗口分割时,进行切换的按键热键需要连接两次,比如从下方窗口移动 &quot;光标到上方窗口,需要&lt;c-w&gt;&lt;c-w&gt;k,非常麻烦,现在重映射为&lt;c-k&gt;,切换的 &quot;时候会变得非常方便. nnoremap &lt;C-h&gt; &lt;C-w&gt;h nnoremap &lt;C-j&gt; &lt;C-w&gt;j nnoremap &lt;C-k&gt; &lt;C-w&gt;k nnoremap &lt;C-l&gt; &lt;C-w&gt;l &quot;一些不错的映射转换语法（如果在一个文件中混合了不同语言时有用） nnoremap &lt;leader&gt;1 :set filetype=xhtml&lt;CR&gt; nnoremap &lt;leader&gt;2 :set filetype=css&lt;CR&gt; nnoremap &lt;leader&gt;3 :set filetype=javascript&lt;CR&gt; nnoremap &lt;leader&gt;4 :set filetype=php&lt;CR&gt; &quot; set fileformats=unix,dos,mac &quot; nmap &lt;leader&gt;fd :se fileformat=dos&lt;CR&gt; &quot; nmap &lt;leader&gt;fu :se fileformat=unix&lt;CR&gt; &quot; use Ctrl+[l|n|p|cc] to list|next|previous|jump to count the result &quot; map &lt;C-x&gt;l &lt;ESC&gt;:cl&lt;CR&gt; &quot; map &lt;C-x&gt;n &lt;ESC&gt;:cn&lt;CR&gt; &quot; map &lt;C-x&gt;p &lt;ESC&gt;:cp&lt;CR&gt; &quot; map &lt;C-x&gt;c &lt;ESC&gt;:cc&lt;CR&gt; &quot; 让 Tohtml 产生有 CSS 语法的 html &quot; syntax/2html.vim，可以用:runtime! syntax/2html.vim let html_use_css=1 &quot; Python 文件的一般设置，比如不要 tab 等 autocmd FileType python set tabstop=4 shiftwidth=4 expandtab autocmd FileType python map &lt;F12&gt; :!python %&lt;CR&gt; &quot; 选中状态下 Ctrl+c 复制 vmap &lt;C-c&gt; &quot;+y &quot; 打开javascript折叠 let b:javascript_fold=1 &quot; 打开javascript对dom、html和css的支持 let javascript_enable_domhtmlcss=1 &quot; 设置字典 ~/.vim/dict/文件的路径 autocmd filetype javascript set dictionary=$VIMFILES/dict/javascript.dict autocmd filetype css set dictionary=$VIMFILES/dict/css.dict autocmd filetype php set dictionary=$VIMFILES/dict/php.dict &quot;----------------------------------------------------------------- &quot; plugin - bufexplorer.vim Buffers切换 &quot; \\be 全屏方式查看全部打开的文件列表 &quot; \\bv 左右方式查看 \\bs 上下方式查看 &quot;----------------------------------------------------------------- &quot;----------------------------------------------------------------- &quot; plugin - taglist.vim 查看函数列表，需要ctags程序 &quot; F4 打开隐藏taglist窗口 &quot;----------------------------------------------------------------- if MySys() == &quot;windows&quot; &quot; 设定windows系统中ctags程序的位置 let Tlist_Ctags_Cmd = &#x27;&quot;&#x27;.$VIMRUNTIME.&#x27;/ctags.exe&quot;&#x27; elseif MySys() == &quot;linux&quot; &quot; 设定windows系统中ctags程序的位置 let Tlist_Ctags_Cmd = &#x27;/usr/bin/ctags&#x27; endif nnoremap &lt;silent&gt;&lt;F4&gt; :TlistToggle&lt;CR&gt; let Tlist_Show_One_File = 1 &quot; 不同时显示多个文件的tag，只显示当前文件的 let Tlist_Exit_OnlyWindow = 1 &quot; 如果taglist窗口是最后一个窗口，则退出vim let Tlist_Use_Right_Window = 1 &quot; 在右侧窗口中显示taglist窗口 let Tlist_File_Fold_Auto_Close=1 &quot; 自动折叠当前非编辑文件的方法列表 let Tlist_Auto_Open = 0 let Tlist_Auto_Update = 1 let Tlist_Hightlight_Tag_On_BufEnter = 1 let Tlist_Enable_Fold_Column = 0 let Tlist_Process_File_Always = 1 let Tlist_Display_Prototype = 0 let Tlist_Compact_Format = 1 &quot;----------------------------------------------------------------- &quot; plugin - mark.vim 给各种tags标记不同的颜色，便于观看调式的插件。 &quot; \\m mark or unmark the word under (or before) the cursor &quot; \\r manually input a regular expression. 用于搜索. &quot; \\n clear this mark (i.e. the mark under the cursor), or clear all highlighted marks . &quot; \\* 当前MarkWord的下一个 \\# 当前MarkWord的上一个 &quot; \\/ 所有MarkWords的下一个 \\? 所有MarkWords的上一个 &quot;----------------------------------------------------------------- &quot;----------------------------------------------------------------- &quot; plugin - NERD_tree.vim 以树状方式浏览系统中的文件和目录 &quot; :ERDtree 打开NERD_tree :NERDtreeClose 关闭NERD_tree &quot; o 打开关闭文件或者目录 t 在标签页中打开 &quot; T 在后台标签页中打开 ! 执行此文件 &quot; p 到上层目录 P 到根目录 &quot; K 到第一个节点 J 到最后一个节点 &quot; u 打开上层目录 m 显示文件系统菜单（添加、删除、移动操作） &quot; r 递归刷新当前目录 R 递归刷新当前根目录 &quot;----------------------------------------------------------------- &quot; F3 NERDTree 切换 map &lt;F3&gt; :NERDTreeToggle&lt;CR&gt; imap &lt;F3&gt; &lt;ESC&gt;:NERDTreeToggle&lt;CR&gt; &quot;----------------------------------------------------------------- &quot; plugin - NERD_commenter.vim 注释代码用的， &quot; [count],cc 光标以下count行逐行添加注释(7,cc) &quot; [count],cu 光标以下count行逐行取消注释(7,cu) &quot; [count],cm 光标以下count行尝试添加块注释(7,cm) &quot; ,cA 在行尾插入 /* */,并且进入插入模式。 这个命令方便写注释。 &quot; 注：count参数可选，无则默认为选中行或当前行 &quot;----------------------------------------------------------------- &quot;let NERDSpaceDelims=1 &quot; 让注释符与语句之间留一个空格 &quot;let NERDCompact***yComs=1 &quot; 多行注释时样子更好看 &quot;----------------------------------------------------------------- &quot; plugin - DoxygenToolkit.vim 由注释生成文档，并且能够快速生成函数标准注释 &quot;----------------------------------------------------------------- let g:DoxygenToolkit_authorName=&quot;Asins - asinsimple AT gmail DOT com&quot; let g:DoxygenToolkit_briefTag_funcName=&quot;yes&quot; map &lt;leader&gt;da :DoxAuthor&lt;CR&gt; map &lt;leader&gt;df :Dox&lt;CR&gt; map &lt;leader&gt;db :DoxBlock&lt;CR&gt; map &lt;leader&gt;dc a /* */&lt;LEFT&gt;&lt;LEFT&gt;&lt;LEFT&gt; &quot;----------------------------------------------------------------- &quot; plugin – ZenCoding.vim 很酷的插件，HTML代码生成 &quot; 插件最新版：http://github.com/mattn/zencoding-vim &quot; 常用命令可看：http://nootn.com/blog/Tool/23/ &quot;----------------------------------------------------------------- &quot;----------------------------------------------------------------- &quot; plugin – checksyntax.vim JavaScript常见语法错误检查 &quot; 默认快捷方式为 F5 &quot;----------------------------------------------------------------- let g:checksyntax_auto = 0 &quot; 不自动检查 &quot;----------------------------------------------------------------- &quot; plugin - NeoComplCache.vim 自动补全插件 &quot;----------------------------------------------------------------- let g:AutoComplPop_NotEnableAtStartup = 1 let g:NeoComplCache_EnableAtStartup = 1 let g:NeoComplCache_SmartCase = 1 let g:NeoComplCache_TagsAutoUpdate = 1 let g:NeoComplCache_EnableInfo = 1 let g:NeoComplCache_EnableCamelCaseCompletion = 1 let g:NeoComplCache_MinSyntaxLength = 3 let g:NeoComplCache_EnableSkipCompletion = 1 let g:NeoComplCache_SkipInputTime = &#x27;0.5&#x27; let g:NeoComplCache_SnippetsDir = $VIMFILES.&#x27;/snippets&#x27; &quot; &lt;TAB&gt; completion. inoremap &lt;expr&gt;&lt;TAB&gt; pumvisible() ? &quot;\\&lt;C-n&gt;&quot; : &quot;\\&lt;TAB&gt;&quot; &quot; snippets expand key imap &lt;silent&gt; &lt;C-e&gt; &lt;Plug&gt;(neocomplcache_snippets_expand) smap &lt;silent&gt; &lt;C-e&gt; &lt;Plug&gt;(neocomplcache_snippets_expand) &quot;----------------------------------------------------------------- &quot; plugin - matchit.vim 对%命令进行扩展使得能在嵌套标签和语句之间跳转 &quot; % 正向匹配 g% 反向匹配 &quot; [% 定位块首 ]% 定位块尾 &quot;----------------------------------------------------------------- &quot;----------------------------------------------------------------- &quot; plugin - vcscommand.vim 对%命令进行扩展使得能在嵌套标签和语句之间跳转 &quot; SVN/git管理工具 &quot;----------------------------------------------------------------- &quot;----------------------------------------------------------------- &quot; plugin – a.vim &quot;----------------------------------------------------------------- &quot;--------------------------------------------- &quot;定义函数SetTitle，自动插入文件头 func SetTitle() if &amp;filetype == &#x27;c&#x27; call setline(1, &quot;#include &lt;stdio.h&gt;&quot;) call setline(2, &quot;#include &lt;stdlib.h&gt;&quot;) call setline(3, &quot;&quot;) call setline(4, &quot;int main() &#123;&quot;) elseif &amp;filetype == &#x27;cpp&#x27; call setline(1, &quot;#include &lt;iostream&gt;&quot;) call setline(2, &quot;&quot;) call setline(3, &quot;int main() &#123;&quot;) elseif &amp;filetype == &#x27;sh&#x27; call setline(1, &quot;#!/bin/bash&quot;) call setline(2, &quot;&quot;) elseif &amp;filetype == &#x27;py&#x27; call setline(1, &quot;#!coding:utf-8&quot;) call setline(2, &quot;&quot;) endif endfunc set completeopt=longest,menu &quot;新建.c,.h,.sh,.java文件，自动插入文件头 autocmd BufNewFile *.[ch],*.sh,*.cpp,*.java exec &quot;:call SetTitle()&quot; &quot;--------------------------------------------- &quot;定义CompileRun函数，用来调用进行编译和运行 func! CompileRun() exec &quot;w&quot; &quot;C程序 if &amp;filetype == &#x27;c&#x27; exec &quot;!gcc % -g -o %&lt;.exe&quot; exec &quot;! ./%&lt;.exe&quot; elseif &amp;filetype == &#x27;cpp&#x27; exec &quot;!g++ % -g -o %&lt;&quot; exec &quot;! ./%&lt;&quot; &quot;Java程序 elseif &amp;filetype == &#x27;java&#x27; exec &quot;!javac %&quot; exec &quot;!java %&lt;&quot; endif endfunc &quot;结束定义CompileRun &quot;------------------------- &quot; ======= 编译 &amp;&amp; 运行 ======= &quot; &quot; 编译源文件 func! CompileCode() exec &quot;w&quot; if &amp;filetype == &quot;c&quot; exec &quot;!gcc -Wall -std=c99 %&lt;.c -o %&lt;&quot; elseif &amp;filetype == &quot;cpp&quot; exec &quot;!g++ -Wall -std=c++98 %&lt;.cpp -o %&lt;&quot; elseif &amp;filetype == &quot;java&quot; exec &quot;!javac %&lt;.java&quot; endif endfunc &quot; 运行可执行文件 func! RunCode() exec &quot;w&quot; if &amp;filetype == &quot;c&quot; || &amp;filetype == &quot;cpp&quot; || &amp;filetype == &quot;haskell&quot; exec &quot;! ./%&lt;&quot; elseif &amp;filetype == &quot;java&quot; exec &quot;!java %&lt;&quot; endif endfunc &quot;--------------------------------------------- &quot; Ctrl + F9 一键保存, 编译 &quot; Ctrl + F10 一键保存，运行 &quot; F9 编译 + 运行 &quot; F10 Debug map&lt;C-F9&gt;:call CompileCode()&lt;CR&gt; imap&lt;C-F9&gt; &lt;ESC&gt;:call CompileCode()&lt;CR&gt; vmap&lt;C-F9&gt; &lt;ESC&gt;:call CompileCode()&lt;CR&gt; map&lt;C-F10&gt;:call RunCode()&lt;CR&gt; imap&lt;C-F10&gt; &lt;ESC&gt;:call RunCode()&lt;CR&gt; vmap&lt;C-F10&gt; &lt;ESC&gt;:call RunCode()&lt;CR&gt; map&lt;F9&gt;:call CompileRun()&lt;CR&gt; imap&lt;F9&gt; &lt;ESC&gt;:call CompileRun()&lt;CR&gt; vmap &lt;F9&gt; &lt;ESC&gt;:call CompileRun()&lt;CR&gt; &quot; &quot;set mouse=v &quot; 鼠标支持","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"vimrc","slug":"vimrc","permalink":"http://www.iequa.com/tags/vimrc/"}]},{"title":"Macos High Sierra 10.13 Work Environment Install","slug":"devops/ops-mac10.13-install-env","date":"2017-10-02T23:08:21.000Z","updated":"2021-06-20T04:12:28.199Z","comments":true,"path":"2017/10/03/devops/ops-mac10.13-install-env/","link":"","permalink":"http://www.iequa.com/2017/10/03/devops/ops-mac10.13-install-env/","excerpt":"Enable Dragging With Three Finger : System Preferences -&gt; Accessibility -&gt; Mouse &amp; Trackpad -&gt; Trackpad Options.","text":"Enable Dragging With Three Finger : System Preferences -&gt; Accessibility -&gt; Mouse &amp; Trackpad -&gt; Trackpad Options. 1. Common Soft Chrome NeteaseMusic Baiduyun &amp; Aira2GUI Microsoft_Office_2016_15.38.17090200_Installer.pkg Google Chrome is up to date Version 61.0.3163.100 (Official Build) (64-bit) 百度云破解限速 (Aria2GUI + chrome plugin) Evernote 国际版与国内版分开管理的. 2. Dev Tools 2.1 general dev tool Macdown Alfred Atom SubLime Text Homebrew Iterm2 Oh-my-zsh PyCharm &amp; IDEA GNU_Octave_3.8.0-6.dmg brew (install 过程会自动需要 Xcode 被安装) brew install wget tree wget https://bootstrap.pypa.io/get-pip.py sudo python get-pip.py 2.2 iterm &amp; zsh Iterm2 Change Font Iterm2 -&gt; Preference -&gt; Profiles -&gt; Text -&gt; Change Font -&gt; 17pt Courier New Bold Iterm2 Hide scrollbars And title bar Preference -&gt; Appearance 取消 show per-pane title bar with split panes. 勾选 Hide scrollbars Iterm2 Color Presets Iterm2 -&gt; Preference -&gt; Profiles -&gt; Color -&gt; Color Presets -&gt; your_theme maybe Atom, Brogrammer, Darkside Zsh astro theme oh-my-zsh 自带 git 插件，里面的针对git 的别名设置见: ➜ &gt;vim .oh-my-zsh/plugins/git/git.plugin.zsh oh-my-zsh autojump plugin install brew install autojump vim .zshrc 12plugins=(git autojump)[[ -s $(brew --prefix)/etc/profile.d/autojump.sh ]] &amp;&amp; . $(brew --prefix)/etc/profile.d/autojump.sh then, source ~/.zshrc Reference Article Iterm2-color-schemes Iterm-colors Zsh astro theme 使用 Zsh 的十大优点. oh-my-zsh配置你的zsh提高shell逼格终极选择. 打造高效个性Terminal（一）之 iTerm. 打造高效个性Terminal（二）之 zsh. Mac下的效率工具autojump 2.3 ssh config ssh-keygen -t rsa -C “your-company-email-full-address” ~/.ssh/id_rsa.pub 粘贴到运维平台 mac iterm2 ssh 跳转失败, 解决办法 : (1) 新建并编辑 .ssh/config, 并复制以下内容到 config文件中 123456Host * ForwardAgent yes PasswordAuthentication yes StrictHostKeyChecking no HashKnownHosts yes Compression yes (2) cd ～/.ssh, 并在 terminal 中执行 ssh-add 2.4 navicat for MySQL Encoding 设置为 utf-8 则，查询数据库，汉字乱码，改为 Auto 解决。 3. Java JDK Maven Tomcat Scala Spark 123456789➜ software pwd/usr/local/xsoft/software➜ software lltotal 0lrwxr-xr-x apache-maven -&gt; /usr/local/xsoft/deploy/apache-maven-3.3.9lrwxr-xr-x apache-tomcat -&gt; /usr/local/xsoft/deploy/apache-tomcat-7.0.59lrwxr-xr-x scala -&gt; /usr/local/xsoft/deploy/scala-2.11.7lrwxr-xr-x spark -&gt; /usr/local/xsoft/deploy/spark-1.6.3-bin-hadoop2.6➜ software 4. Blog hexo Install Node.js 1$ wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash Once nvm is installed, restart the terminal and run the following command to install Node.js: 12$ nvm install v4.1.0$ npm install -g hexo-cli v4.1.0 更合适 hexo 5. Python 5.1 this mac install pip Python pip , sudo python get-pip.py then, terminal input pip list. If exist warning: DEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning. solve this warning: ~.vim ~/.pip/pip.conf [list] format=columns 5.2 pyenv install package First, you need to install python pyenv environment 12345678910111213141516Python 3.6.3(vpy3)➜pip install numpypip install scipypip install matplotlibpip install pandaspip install xlrdpip install xlwtpip install StatsModelspip install scikit-learnpip install jiebapip install --upgrade gensim pip install elasticsearch==1.9 ipython 1pip install ipython 然后, 如 terminall input ipython 不存在, 则 pip show ipython, python -m IPython 试试. notebook 123#pip install --ignore-installed six#pip install target-gsheet tap-fixeriopip install notebook pip install notebook, 如 macos High Sierra 10.13 报错，则 pip install --ignore-installed six pip install target-gsheet tap-fixerio then, pip install notebook 8. Reference Homebrew Get-pip IntelliJ、Pycharm激活 Mac OSX 安裝JDK Hexo Doc Mac 上完整卸载Node.js Mac OSX 完整卸载Node.js node版本管理工具nvm-Mac下安装及使用 XClient.info Mac App Stackoverflow python on MacOS 10.10 - command not found Blair python install data mining env Macos NSNavRecentPlaces 内部自动生成的配置，别乱改。 defaults write -g NSNavRecentPlaces ‘(&quot;~/Desktop&quot;, “/usr/local/xsoft/software”)’;","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"macos","slug":"macos","permalink":"http://www.iequa.com/tags/macos/"}]},{"title":"TensorFlow Why?","slug":"tensorflow/tf-1.1-why","date":"2017-08-22T05:17:21.000Z","updated":"2021-06-20T04:12:28.331Z","comments":true,"path":"2017/08/22/tensorflow/tf-1.1-why/","link":"","permalink":"http://www.iequa.com/2017/08/22/tensorflow/tf-1.1-why/","excerpt":"2015.10 开源的 TensorFlow 是由 Google brain 的工程师开发出来，用于机器学习和神经网络方面的研究","text":"2015.10 开源的 TensorFlow 是由 Google brain 的工程师开发出来，用于机器学习和神经网络方面的研究 TensorFlow 是一款神经网络的 Python 外部的结构包, 也是一个采用数据流图来进行数值计算的开源软件库. 1. TensorFlow Why TensorFlow 擅长的任务就是训练深度神经网络. 使用它我们就可以大大降低深度学习的开发成本和开发难度; TensorFlow 架构灵活，支持各种异构的平台，支持多CPU/GPU， 能够支持各种网络模型，具有良好的通用性; TensorFlow 用户可以方便地用它设计 Neural Network 结构，而不必亲自写 C++或 CUDA 代码。支持自动求导，用户不需要再通过反向传播求解梯度。 2. TF.Learn 和 TF.Slim 上层组件 TensorFlow 内置的 TF.Learn 和 TF.Slim 等上层组件可以帮助快速地设计新网络，并且兼容 Scikit-learn estimator 接口，可以方便地实现 evaluate、grid search、cross validation 等功能。 同时 TensorFlow 不只局限于神经网络，其数据流式图支持非常自由的算法表达，当然也可以轻松实现深度学习以外的机器学习算法。 3. DeepLearning Framework Reference tensorflow.org tensorflow.org get_started TensorFlow 和 Caffe、MXNet、Keras等其他深度学习框架的对比","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"}]},{"title":"Python 字符串处理-正则表达式","slug":"python/string-operation-re","date":"2017-07-30T10:08:21.000Z","updated":"2021-06-20T04:12:28.207Z","comments":true,"path":"2017/07/30/python/string-operation-re/","link":"","permalink":"http://www.iequa.com/2017/07/30/python/string-operation-re/","excerpt":"Python 字符串处理 之 正则表达式 Github-ipynb 字符串操作 我们一起回归一下python字符串的相关操作，这是非常基础的知识，但却是使用频度非常高的一些功能。","text":"Python 字符串处理 之 正则表达式 Github-ipynb 字符串操作 我们一起回归一下python字符串的相关操作，这是非常基础的知识，但却是使用频度非常高的一些功能。 1.1 去空格及特殊符号 1234s = &#x27; hello, world!&#x27;print s.strip()print s.lstrip(&#x27; hello, &#x27;)print s.rstrip(&#x27;!&#x27;) hello, world! world! hello, world 1.2 连接字符串 1234sStr1 = &#x27;strcat&#x27;sStr2 = &#x27;append&#x27;sStr1 += sStr2print sStr1 strcatappend 1.3 查找字符 12345# &lt; 0 为未找到sStr1 = &#x27;strchr&#x27;sStr2 = &#x27;r&#x27;nPos = sStr1.index(sStr2)print nPos 2 1.4 较字符串 12345sStr1 = &#x27;strchr&#x27;sStr2 = &#x27;strch&#x27;print cmp(sStr2,sStr1)print cmp(sStr1,sStr2)print cmp(sStr1,sStr1) -1 1 0 1.5 大小写转换 1234sStr1 = &#x27;JCstrlwr&#x27;sStr1 = sStr1.upper()#sStr1 = sStr1.lower()print sStr1 JCSTRLWR 1.6 翻转字符串 123sStr1 = &#x27;abcdefg&#x27;sStr1 = sStr1[::-1]print sStr1 gfedcba 1.7 查找字符串 123sStr1 = &#x27;abcdefg&#x27;sStr2 = &#x27;cde&#x27;print sStr1.find(sStr2) 2 1.8 分割字符串 1234567sStr1 = &#x27;ab,cde,fgh,ijk&#x27;sStr2 = &#x27;,&#x27;sStr1 = sStr1[sStr1.find(sStr2) + 1:]print sStr1#或者s = &#x27;ab,cde,fgh,ijk&#x27;print(s.split(&#x27;,&#x27;)) cde,fgh,ijk ['ab', 'cde', 'fgh', 'ijk'] 1.9 频次最高的字母 12345678910111213141516#version 1import refrom collections import Counterdef get_max_value_v1(text): text = text.lower() result = re.findall(&#x27;[a-zA-Z]&#x27;, text) # 去掉列表中的符号符 count = Counter(result) # Counter(&#123;&#x27;l&#x27;: 3, &#x27;o&#x27;: 2, &#x27;d&#x27;: 1, &#x27;h&#x27;: 1, &#x27;r&#x27;: 1, &#x27;e&#x27;: 1, &#x27;w&#x27;: 1&#125;) count_list = list(count.values()) max_value = max(count_list) max_list = [] for k, v in count.items(): if v == max_value: max_list.append(k) max_list = sorted(max_list) return max_list[0] 1234567#version 2from collections import Counterdef get_max_value(text): count = Counter([x for x in text.lower() if x.isalpha()]) m = max(count.values()) return sorted([x for (x, y) in count.items() if y == m])[0] 123456#version 3import stringdef get_max_value(text): text = text.lower() return max(string.ascii_lowercase, key=text.count) 123456789101112131415161718192021max(range(6), key = lambda x : x&gt;2)# &gt;&gt;&gt; 3# 带入key函数中，各个元素返回布尔值，相当于[False, False, False, True, True, True]# key函数要求返回值为True，有多个符合的值，则挑选第一个。max([3,5,2,1,4,3,0], key = lambda x : x)# &gt;&gt;&gt; 5# 带入key函数中，各个元素返回自身的值，最大的值为5，返回5.max(&#x27;ah&#x27;, &#x27;bf&#x27;, key=lambda x: x[1])# &gt;&gt;&gt; &#x27;ah&#x27;# 带入key函数，各个字符串返回最后一个字符，其中&#x27;ah&#x27;的h要大于&#x27;bf&#x27;中的f，因此返回&#x27;ah&#x27;max(&#x27;ah&#x27;, &#x27;bf&#x27;, key=lambda x: x[0])# &gt;&gt;&gt; &#x27;bf&#x27;# 带入key函数，各个字符串返回第一个字符，其中&#x27;bf&#x27;的b要大于&#x27;ah&#x27;中的a，因此返回&#x27;bf&#x27;text = &#x27;Hello World&#x27;max(&#x27;abcdefghijklmnopqrstuvwxyz&#x27;, key=text.count)# &gt;&gt;&gt; &#x27;l&#x27;# 带入key函数，返回各个字符在&#x27;Hello World&#x27;中出现的次数，出现次数最多的字符为&#x27;l&#x27;,因此输出&#x27;l&#x27; 'l' Count occurrence of a character in a Python string 12345678910#T h e M i s s i s s i p p i R i v e r#[1, 1, 2, 2, 1, 5, 4, 4, 5, 4, 4, 5, 2, 2, 5, 2, 1, 5, 1, 2, 1]sentence=&#x27;The Mississippi River&#x27;def count_chars(s): s=s.lower() count=list(map(s.count,s)) return (max(count))print count_chars(sentence) 5","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Jieba 中文处理","slug":"nlp/jieba","date":"2017-07-29T10:08:21.000Z","updated":"2021-06-20T04:12:28.334Z","comments":true,"path":"2017/07/29/nlp/jieba/","link":"","permalink":"http://www.iequa.com/2017/07/29/nlp/jieba/","excerpt":"NLP 基础技能 ： Jieba 中文分词与处理 Github-ipynb","text":"NLP 基础技能 ： Jieba 中文分词与处理 Github-ipynb 词汇是我们对句子和文章理解的基础，因此需要一个工具去把完整的文本中分解成粒度更细的词 jieba 就是这样一个非常好用的中文工具，是以分词起家的，但是功能比分词要强大很多。 1. 基本分词函数与用法 jieba.cut jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode) jieba.cut 方法接受三个输入参数: 需要分词的字符串 cut_all 参数用来控制是否采用全模式 HMM 参数用来控制是否使用 HMM 模型 jieba.cut_for_search 方法接受两个参数 需要分词的字符串 是否使用 HMM 模型。 该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细 123456789101112131415# encoding=utf-8import jiebaseg_list = jieba.cut(&quot;我在学习自然语言处理&quot;, cut_all=True)print seg_listprint(&quot;Full Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 全模式seg_list = jieba.cut(&quot;我在学习自然语言处理&quot;, cut_all=False)print(&quot;Default Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 精确模式seg_list = jieba.cut(&quot;他毕业于上海交通大学，在百度深度学习研究院进行研究&quot;) # 默认是精确模式print(&quot;, &quot;.join(seg_list))seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在哈佛大学深造&quot;) # 搜索引擎模式print(&quot;, &quot;.join(seg_list)) 1234567891011Building prefix dict from the default dictionary ...&lt;generator object cut at 0x110370460&gt;Dumping model to file cache /var/folders/mf/_jgd83rx0rgcmt42cp7fkkd00000gn/T/jieba.cacheLoading model cost 2.184 seconds.Prefix dict has been built succesfully.Full Mode: 我/ 在/ 学习/ 自然/ 自然语言/ 语言/ 处理Default Mode: 我/ 在/ 学习/ 自然语言/ 处理他, 毕业, 于, 上海交通大学, ，, 在, 百度, 深度, 学习, 研究院, 进行, 研究小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 哈佛, 大学, 哈佛大学, 深造 jieba.lcut 以及 jieba.lcut_for_search 直接返回 list 1234result_lcut = jieba.lcut(&quot;小明硕士毕业于中国科学院计算所，后在哈佛大学深造&quot;)print result_lcutprint &quot; &quot;.join(result_lcut)print &quot; &quot;.join(jieba.lcut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在哈佛大学深造&quot;)) 123[u&#x27;\\u5c0f\\u660e&#x27;, u&#x27;\\u7855\\u58eb&#x27;, u&#x27;\\u6bd5\\u4e1a&#x27;, u&#x27;\\u4e8e&#x27;, u&#x27;\\u4e2d\\u56fd\\u79d1\\u5b66\\u9662&#x27;, u&#x27;\\u8ba1\\u7b97\\u6240&#x27;, u&#x27;\\uff0c&#x27;, u&#x27;\\u540e&#x27;, u&#x27;\\u5728&#x27;, u&#x27;\\u54c8\\u4f5b\\u5927\\u5b66&#x27;, u&#x27;\\u6df1\\u9020&#x27;]小明 硕士 毕业 于 中国科学院 计算所 ， 后 在 哈佛大学 深造小明 硕士 毕业 于 中国 科学 学院 科学院 中国科学院 计算 计算所 ， 后 在 哈佛 大学 哈佛大学 深造 1.1 添加用户自定义词典 很多时候我们需要针对自己的场景进行分词，会有一些领域内的专有词汇。 可以用 jieba.load_userdict(file_name) 加载用户字典 少量的词汇可以自己用下面方法手动添加： 用 add_word(word, freq=None, tag=None) 和 del_word(word) 在程序中动态修改词典 用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。 123print(&#x27;/&#x27;.join(jieba.cut(&#x27;如果放到旧字典中将出错。&#x27;, HMM=False)))print jieba.suggest_freq((&#x27;中&#x27;, &#x27;将&#x27;), True)print(&#x27;/&#x27;.join(jieba.cut(&#x27;如果放到旧字典中将出错。&#x27;, HMM=False))) 123如果/放到/旧/字典/中将/出错/。494如果/放到/旧/字典/中/将/出错/。 2. 关键词提取 2.1 TF-IDF 算法的关键词抽取 import jieba.analyse jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) sentence 为待提取的文本 topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20 withWeight 为是否一并返回关键词权重值，默认值为 False allowPOS 仅包括指定词性的词，默认值为空，即不筛选 123import jieba.analyse as analyselines = open(&#x27;NBA.txt&#x27;).read()print &quot; &quot;.join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())) 12韦少 杜兰特 全明星 全明星赛 MVP 威少 正赛 科尔 投篮 勇士 球员 斯布鲁克 更衣柜 张卫平 三连庄 NBA 西部 指导 雷霆 明星队 12lines = open(u&#x27;西游记.txt&#x27;).read()print &quot; &quot;.join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())) 12行者 八戒 师父 三藏 唐僧 大圣 沙僧 妖精 菩萨 和尚 那怪 那里 长老 呆子 徒弟 怎么 不知 老孙 国王 一个 2.2 TF-IDF 关键词抽取补充 关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径 用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径 自定义语料库示例见这里 用法示例见这里 关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径 用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径 自定义语料库示例见这里 用法示例见这里 关键词一并返回关键词权重值示例 用法示例见这里 2.3 TextRank 的关键词抽取 jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’)) 直接使用，接口相同，注意默认过滤词性。 jieba.analyse.TextRank() 新建自定义 TextRank 实例 算法论文： TextRank: Bringing Order into Texts 基本思想: 将待抽取关键词的文本进行分词 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图 计算图中节点的PageRank，注意是无向带权图 12345import jieba.analyse as analyselines = open(&#x27;NBA.txt&#x27;).read()print &quot; &quot;.join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=(&#x27;ns&#x27;, &#x27;n&#x27;, &#x27;vn&#x27;, &#x27;v&#x27;)))print &quot;---------------------我是分割线----------------&quot;print &quot; &quot;.join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=(&#x27;ns&#x27;, &#x27;n&#x27;))) 12345678910Building prefix dict from the default dictionary ...Loading model from cache /var/folders/mf/_jgd83rx0rgcmt42cp7fkkd00000gn/T/jieba.cacheLoading model cost 0.530 seconds.Prefix dict has been built succesfully.全明星赛 勇士 正赛 指导 对方 投篮 球员 没有 出现 时间 威少 认为 看来 结果 相隔 助攻 现场 三连庄 介绍 嘉宾---------------------我是分割线----------------勇士 正赛 全明星赛 指导 投篮 玩命 时间 对方 现场 结果 球员 嘉宾 时候 全队 主持人 特点 大伙 肥皂剧 全程 快船队 12lines = open(u&#x27;西游记.txt&#x27;).read()print &quot; &quot;.join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=(&#x27;ns&#x27;, &#x27;n&#x27;, &#x27;vn&#x27;, &#x27;v&#x27;))) 12行者 师父 八戒 三藏 大圣 不知 菩萨 妖精 只见 长老 国王 却说 呆子 徒弟 小妖 出来 不得 不见 不能 师徒 3. 词性标注 jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。 jieba.posseg.dt 为默认词性标注分词器。 标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。 具体的词性对照表参见计算所汉语词性标记集 计算所汉语词性标记集 ictclas 1234import jieba.posseg as psegwords = pseg.cut(&quot;我爱自然语言处理&quot;)for word, flag in words: print(&#x27;%s %s&#x27; % (word, flag)) 1234我 r爱 v自然语言 l处理 v 4. 并行分词 原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows 用法： jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 jieba.disable_parallel() # 关闭并行分词模式 实验结果：在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。 注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。 12345678910111213141516171819import sysimport timeimport jiebajieba.enable_parallel()content = open(u&#x27;西游记.txt&#x27;,&quot;r&quot;).read()t1 = time.time()words = &quot;/ &quot;.join(jieba.cut(content))t2 = time.time()tm_cost = t2-t1print(&#x27;并行分词速度为 %s bytes/second&#x27; % (len(content)/tm_cost))jieba.disable_parallel()content = open(u&#x27;西游记.txt&#x27;,&quot;r&quot;).read()t1 = time.time()words = &quot;/ &quot;.join(jieba.cut(content))t2 = time.time()tm_cost = t2-t1print(&#x27;非并行分词速度为 %s bytes/second&#x27; % (len(content)/tm_cost)) 12并行分词速度为 415863.760491 bytes/second非并行分词速度为 242471.700496 bytes/second 命令行分词 1234567891011121314151617181920212223242526使用示例：python -m jieba news.txt &gt; cut_result.txt命令行选项（翻译）：使用: python -m jieba [options] filename结巴命令行界面。固定参数: filename 输入文件可选参数: -h, --help 显示此帮助信息并退出 -d [DELIM], --delimiter [DELIM] 使用 DELIM 分隔词语，而不是用默认的&#x27; / &#x27;。 若不指定 DELIM，则使用一个空格分隔。 -p [DELIM], --pos [DELIM] 启用词性标注；如果指定 DELIM，词语和词性之间 用它分隔，否则用 _ 分隔 -D DICT, --dict DICT 使用 DICT 代替默认词典 -u USER_DICT, --user-dict USER_DICT 使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用 -a, --cut-all 全模式分词（不支持词性标注） -n, --no-hmm 不使用隐含马尔可夫模型 -q, --quiet 不输出载入信息到 STDERR -V, --version 显示版本信息并退出如果没有指定文件名，则使用标准输入。 Tokenize：返回词语在原文的起止位置 注意，输入参数只接受 unicode 1234567891011print &quot;这是默认模式的tokenize&quot;result = jieba.tokenize(u&#x27;自然语言处理非常有用&#x27;)for tk in result: print(&quot;%s\\t\\t start: %d \\t\\t end:%d&quot; % (tk[0],tk[1],tk[2]))print &quot;\\n-----------我是神奇的分割线------------\\n&quot;print &quot;这是搜索模式的tokenize&quot;result = jieba.tokenize(u&#x27;自然语言处理非常有用&#x27;, mode=&#x27;search&#x27;)for tk in result: print(&quot;%s\\t\\t start: %d \\t\\t end:%d&quot; % (tk[0],tk[1],tk[2])) 123456789101112131415这是默认模式的tokenize自然语言 start: 0 end:4处理 start: 4 end:6非常 start: 6 end:8有用 start: 8 end:10 -----------我是神奇的分割线------------ 这是搜索模式的tokenize自然 start: 0 end:2语言 start: 2 end:4自然语言 start: 0 end:4处理 start: 4 end:6非常 start: 6 end:8有用 start: 8 end:10","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"jieba","slug":"jieba","permalink":"http://www.iequa.com/tags/jieba/"}]},{"title":"Word2vec 基础","slug":"nlp/word2vector-basic","date":"2017-07-12T13:08:21.000Z","updated":"2021-06-20T04:12:28.334Z","comments":true,"path":"2017/07/12/nlp/word2vector-basic/","link":"","permalink":"http://www.iequa.com/2017/07/12/nlp/word2vector-basic/","excerpt":"Natural Language Processing，计算机科学与语言学中关注于计算机与人类语言间转换的领域","text":"Natural Language Processing，计算机科学与语言学中关注于计算机与人类语言间转换的领域 1. NLP 常见任务 自动摘要 指代消解 机器翻译 词性标注 分词 (中文、日文等) 主题识别 文本分类. … 自动摘要 -&gt; 搜索要索引，关键词等 指代消解 -&gt; 小明放学了, 妈妈去接他 机器翻译 -&gt; 小心地滑、干货 =&gt; Slide carefully 词性标注 -&gt; heat(v.) water(n.) in(p.) a(det.) pot(n.) 分词 (中文、日文等) -&gt; 大水沟/很/难/过 2. NLP 处理方法 2.1 传统: 基于规则 Dict… 简单、粗暴、有用 2.2 现代: 基于机器学习 HMM, CRF, SVM, LDA, CNN… “规则”隐含在模型参数里 3. 词编码和词向量初步 你需要一些 model，不管你是基于规则统计、机器学习、深度学习 的一些方法，第一步要做的，一定是对你的文本或者数据，进行表达, 词编码。 『词编码需要保证词的相似性』 Glove results Nearest words to frog toad rana … 『向量空间分布的相似性』 『向量空间子结构』 编码要尽量保证，相似的词的空间距离是相近的 V_KingV\\_{King}V_King - V_QueenV\\_{Queen}V_Queen + V_WomenV\\_{Women}V_Women = V_ManV\\_{Man}V_Man V_ParisV\\_{Paris}V_Paris - V_FranceV\\_{France}V_France + V_GermanV\\_{German}V_German = V_BerlinV\\_{Berlin}V_Berlin 最终目标: 词向量表示作为机器学习、特别是深度学习的输入和表示空间 你的 数据 决定了你的 结果上限 你的 算法 只是以多大程度去 逼近 Linguists &lt;img src=&quot;/images/nlp/word2vector-3.png&quot; width=“520” height=“400” align=“middle” /img&gt; 3.1 离散表示 One-hot 语料库 12John likes to watch movies. Mary likes too.John also likes to watch football games. 词典 12&#123;&quot;John&quot;: 1, &quot;likes&quot;: 2, &quot;to&quot;: 3, &quot;watch&quot;: 4, &quot;movies&quot;: 5, &quot;also&quot;: 6, &quot;football&quot;: 7, &quot;games&quot;: 8, &quot;Mary&quot;: 9, &quot;too&quot;: 10&#125; One-hot表示 1234John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]likes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]...too : [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 3.2 离散表示 Bag of Words 文档的向量表示可以直接将各词的词向量表示加和 John likes to watch movies. Mary likes too. =&gt; [1, 2, 1, 1, 1, 0, 0, 0, 1, 1] John also likes to watch football games. =&gt; [1, 1, 1, 1, 0, 1, 1, 1, 0, 0] 词权重 - (词在文档中的顺序没有被考虑) tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度 TF-IDF (Term Frequency - Inverse Document Frequency) Term Frequency : F(term) = (该word词出现在当前文档中的次数) / (当前文档所有word的总数). 信息检索词 t 的 IDF log⁡(1+Nnt)\\log (1 + {\\frac{N}{n^t}}) log(1+ntN​) N: 文档总数， n: 含有词 t 的文档数 [0.693, 1.386, 0.693, 0.693, 1.099, 0, 0, 0, 0.693, 0.693] Binary weighting 不做计数的版本 短文本相似性, Bernoulli Naive Bayes [1, 1, 1, 1, 1, 0, 0, 0, 1, 1] if so, I love you = you love I 3.3 离散表示 􏰜􏰝􏰞􏰟􏰒Bi-gram / 􏰪N-gram John likes to watch movies. Mary likes too. John also likes to watch football games. &lt;img src=&quot;/images/nlp/word2vector-4.png&quot; width=“620” height=“400” align=“middle” /img&gt; 3.4 语言模型 词组合出现的概率 一句话(词组合)出现的概率 &lt;img src=&quot;/images/nlp/word2vector-5.png&quot; width=“620” height=“400” align=“middle” /img&gt; 简化计算 : P(too∣Mark,likes)≈P(too∣likes)P(too | Mark, likes) \\approx P(too | likes)P(too∣Mark,likes)≈P(too∣likes), 可参见 吴军 《数学之美》 解释 3.5 离散表示 的缺点 &lt;img src=&quot;/images/nlp/word2vector-6.png&quot; width=“620” height=“400” align=“middle” /img&gt; 3.6 分布式表示 提出 &lt;img src=&quot;/images/nlp/word2vector-7.png&quot; width=“620” height=“400” align=“middle” /img&gt; Distributed representation : 用一个词附近的其他词来表示该词 - (不知道你的经济情况，就调查下你的狐朋狗友们) 􏰐􏱏􏲎􏰀􏳎􏳏􏰢􏳐􏳑􏰀􏳒􏰞You shall know a word by the company it keeps — J.R. Firth 1957 􏰐􏱏􏲎􏰀􏳎􏳏􏰢􏳐􏳑􏰀􏳒􏰞􏰟􏳓􏰀􏰑􏰐􏱏􏲎􏰀􏳎􏳏􏰢􏳐􏳑􏰀􏳒􏰞􏰟􏳓􏰀􏰑􏰐􏱏􏲎􏰀􏳎􏳏􏰢􏳐􏳑 􏱕􏳔􏳕􏳖􏳗&gt; 现代统计自然语言处理中最有创见的想法之一 4. 共现矩阵 (Cocurrence matrix) &lt;img src=&quot;/images/nlp/word2vector-8.png&quot; width=“620” height=“400” align=“middle” /img&gt; 共现 : 共同出现 4.1 Word - Word &lt;img src=&quot;/images/nlp/word2vector-9.png&quot; width=“620” height=“400” align=“middle” /img&gt; 左右窗 length 为 1， 得到的矩阵如上 存在缺点 &lt;img src=&quot;/images/nlp/word2vector-10.png&quot; width=“620” height=“400” align=“middle” /img&gt; 模型欠稳定，可以考虑下 LR 的各个参数等，变化太大，对模型求解有影响 用SVD对共现矩阵向量做降维 1234567891011121314151617181920import numpy as npla = np.linalgwords = [&quot;I&quot;, &quot;like&quot;, &quot;enjoy&quot;, &quot;deep&quot;, &quot;learning&quot;, &quot;NLP&quot;, &quot;flying&quot;, &quot;.&quot;]X = np.array( [ [0, 2, 1, 0, 0, 0, 0, 0], [2, 0, 0, 1, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 1, 1, 0] ])U, s, Vh = la.svd(X, full_matrices=False) SVD 降维存在的问题 计算量随语料库和词典增长膨胀太快，对X(n,n)维 的矩阵，计算量O(n^3)。而对大型的语料库， n400k，语料库大小160B token 难以为词典中新加入的词分配词向量 与其他深度学习模型框架差异大 4.2 NNLM 4.3 Word2Vec: CBOW 4.4 Word2Vec: Skip-Gram 4.5 Word2Vec 缺点 5. 总结 5.1 离散表示 One-hot representation, Bag Of Words Unigram语言模型 N-gram词向量表示和语言模型 Co-currence矩阵的行(列)向量作为词向量 5.2 分布式连续表示 Co-currence矩阵的SVD降维的低维词向量表示 Word2Vec: Continuous Bag of Words Model Word2Vec: Skip-Gram Model gensim 用 python 训练 word2vec 最好用的库 它的功能不至于 word2vec","categories":[{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"}],"tags":[{"name":"Word2Vec","slug":"Word2Vec","permalink":"http://www.iequa.com/tags/Word2Vec/"}]},{"title":"Guitar 调音换弦","slug":"tools/music-guitar-1","date":"2017-06-25T12:24:16.000Z","updated":"2021-06-22T06:52:37.115Z","comments":true,"path":"2017/06/25/tools/music-guitar-1/","link":"","permalink":"http://www.iequa.com/2017/06/25/tools/music-guitar-1/","excerpt":"Guitar 调音、换弦操作方法","text":"Guitar 调音、换弦操作方法 Guitar 调音操作 将调音器夹琴头空白位置，短按开关键将模式调节到G字母再进行调试~ 从细到粗1-6弦的标准音分别是：1E 2B 3G 4D 5A 6E， 要将每根弦都对应到正确的字母到中间变绿才是标准音哟~ 调音操作 调到 C 字母的调音方法 : 从细到粗1-6弦的标准音分别是：E B G D A E， 要将每根弦都对应到正确的字母到中间变绿才是标准音哟~ 注意先从最粗的6弦开始，最后调1弦，调音过程需要耐心多一点，力度小一点. 换弦操作方法 注意视频里面提到的撬弦器是没有赠送的，您可以使用钥匙或者小勺子放在固弦锥下方轻轻往上撬一下即可取出固弦锥. 换弦操作 吉他正对自己，一二三弦往自己怀里方向是拧紧调高，往外的方向是放松调低，四五六弦是相反方向","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"Guitar","slug":"Guitar","permalink":"http://www.iequa.com/tags/Guitar/"}]},{"title":"Python Basic Learning III","slug":"python/language/py-language-basic-learning-III","date":"2017-06-05T03:00:21.000Z","updated":"2021-06-20T04:12:28.220Z","comments":true,"path":"2017/06/05/python/language/py-language-basic-learning-III/","link":"","permalink":"http://www.iequa.com/2017/06/05/python/language/py-language-basic-learning-III/","excerpt":"廖雪峰的 Python 教程 - Functional Programming","text":"廖雪峰的 Python 教程 - Functional Programming 1. 函数式编程 函数就是面向过程的程序设计的基本单元。 Functional Programming 其思想更接近数学计算。 在计算机的层次上，CPU执行的是加减乘除的指令代码，以及各种条件判断和跳转指令，所以，汇编语言是最贴近计算机的语言。 而计算则指数学意义上的计算，越是抽象的计算，离计算机硬件越远。 对应到编程语言，就是越低级的语言，越贴近计算机，抽象程度低，执行效率高，比如C语言；越高级的语言，越贴近计算，抽象程度高，执行效率低，比如Lisp语言。 函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。 函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！ Python对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。 1.1 高阶函数 Higher-order function 2. 模块 Python内置的模块 和 来自第三方的模块。 每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。 Reference 廖雪峰的官方网站 liaoxuefeng","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python Class","slug":"python/language/py-language-8-class","date":"2017-06-05T02:00:21.000Z","updated":"2021-06-20T04:12:28.225Z","comments":true,"path":"2017/06/05/python/language/py-language-8-class/","link":"","permalink":"http://www.iequa.com/2017/06/05/python/language/py-language-8-class/","excerpt":"OO 最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类","text":"OO 最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类 Class def class 定义一个类,首字母大写，比如 Calculator. class可以先定义自己的属性，比如 name=‘Good Calculator’. 1234567891011121314151617181920class Calculator: #首字母要大写，冒号不能缺 name=&#x27;Good Calculator&#x27; #该行为class的属性 price=18 def add(self,x,y): print(self.name) result = x + y print(result) def minus(self,x,y): result=x-y print(result) def times(self,x,y): print(x*y) def divide(self,x,y): print(x/y) 1234cal=Calculator()print(cal.name)print(cal.price) Good Calculator 18 1234cal.add(10,20)cal.minus(10,20)cal.times(10,20)cal.divide(10,20) Good Calculator 30 -10 200 0.5 Class init 运行 c=Calculator('bad calculator',18,17,16,15), 然后调出每个初始值的值 123456789class Calculator: name=&#x27;good calculator&#x27; price=18 def __init__(self,name,price,height,width,weight): # 注意，这里的下划线是双下划线 self.name=name self.price=price self.h=height self.wi=width self.we=weight 123456c=Calculator(&#x27;bad calculator&#x27;,18,17,16,15)print(c.name)print(c.price)print(c.h)print(c.wi)print(c.we) bad calculator 18 17 16 15 设置class属性默认值 如何设置属性的默认值, 直接在def里输入即可，如下: def __init__(self,name,price,height=10,width=14,weight=16): 查看运行结果， 三个有默认值的属性，可以直接输出默认值. 这些默认值可以在code中更改, 比如c.wi=17再输出c.wi就会把wi属性值更改为17. 123456789class Calculator: name=&#x27;good calculator&#x27; price=18 def __init__(self,name,price,hight=10,width=14,weight=16): #后面三个属性设置默认值,查看运行 self.name=name self.price=price self.h=hight self.wi=width self.we=weight 123456c=Calculator(&#x27;bad calculator&#x27;,18)print(c.h)print(&quot;wi : &quot; + str(c.wi))c.wi = 17print(&quot;wi : &quot; + str(c.wi))print(c.we) 10 wi : 14 wi : 17 16 Reference docs.python.org python morvanzhou python liaoxuefeng","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python Read File","slug":"python/language/py-language-6-read-file","date":"2017-06-04T02:00:21.000Z","updated":"2021-06-20T04:12:28.226Z","comments":true,"path":"2017/06/04/python/language/py-language-6-read-file/","link":"","permalink":"http://www.iequa.com/2017/06/04/python/language/py-language-6-read-file/","excerpt":"open()、append、file.read()、file.readline()、file.readlines()、file.close()、with open … as f","text":"open()、append、file.read()、file.readline()、file.readlines()、file.close()、with open … as f 读写文件前，必须了解，在磁盘上读写文件的功能都是由操作系统提供的，现代操作系统不允许普通的程序直接操作磁盘，所以，读写文件就是请求操作系统打开一个文件对象（通常称为文件描述符），然后，通过操作系统提供的接口从这个文件对象中读取数据（读文件），或者把数据写入这个文件对象（写文件） 1. open 使用 open 能够打开一个文件, open 的第一个参数为文件名和路径 ‘my file.txt’, 第二个参数为将要以什么方式打开它, 比如 w 为可写方式. 如果计算机没有找到 ‘my file.txt’ 这个文件, w 方式能够创建一个新的文件, 并命名为 my file.txt 12345text = &#x27;This is my first test.&#x27;my_file=open(&#x27;my file.txt&#x27;,&#x27;w&#x27;) #用法: open(&#x27;文件名&#x27;,&#x27;形式&#x27;), 其中形式有&#x27;w&#x27;:write;&#x27;r&#x27;:read.my_file.write(text) #该语句会写入先前定义好的 textmy_file.close() #关闭文件 2. append 我们先保存一个已经有3行文字的 “my file.txt” 文件, 文件的内容如下: 123This is my first test. This is the second line.This the third 使用添加文字的方式给这个文件添加一行 “This is appended file.”, 并将这行文字储存在 append_file 里，注意\\n的适用性: 1234append_text=&#x27;\\nThis is appended file.&#x27; # 为这行文字提前空行 &quot;\\n&quot;my_file=open(&#x27;my file.txt&#x27;,&#x27;a&#x27;) # &#x27;a&#x27;=append 以增加内容的形式打开my_file.write(append_text)my_file.close() This is my first test. This is the second line. This the third line. This is appended file. 3. file.read() 调用 read() 会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。 123file= open(&#x27;my file.txt&#x27;,&#x27;r&#x27;) content=file.read() print(content) This is my first test. This is the second line. This the third line. This is appended file. 4. file.readline() 如果想在文本中一行行的读取文本, 可以使用 file.readline(), file.readline() 读取的内容和你使用的次数有关, 使用第二次的时候, 读取到的是文本的第二行, 并可以以此类推: 123file= open(&#x27;my file.txt&#x27;,&#x27;r&#x27;) content=file.readline() # 读取第一行print(content) This is my first test. 12second_read_time=file.readline() # 读取第二行print(second_read_time) This is the second line. 5. file.readlines() 如果想要读取所有行, 并可以使用像 for 一样的迭代器迭代这些行结果, 我们可以使用 file.readlines(), 将每一行的结果存储在 list 中, 方便以后迭代. 123file= open(&#x27;my file.txt&#x27;,&#x27;r&#x27;) content=file.readlines() # python_list 形式print(content) ['This is my first test.\\n', 'This is the second line.\\n', 'This the third line.\\n', 'This is appended file.'] 12345678910111213141516171819202122232425# 之后如果使用 for 来迭代输出:for item in content: print(item)``` This is my first test. This is the second line. This the third line. This is appended file.## 6. file.close()由于文件读写时都有可能产生`IOError`，一旦出错，后面的`f.close()`就不会调用。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用`try ... finally`来实现：```pythontry: f = open(&#x27;/path/to/file&#x27;, &#x27;r&#x27;) print f.read()finally: if f: f.close() 每次都这么写实在太繁琐，所以，Python引入了with语句来自动帮我们调用close()方法： 7. with open … as f 12with open(&#x27;/path/to/file&#x27;, &#x27;r&#x27;) as f: print f.read() 这和前面的try … finally是一样的，但是代码更佳简洁，并且不必调用f.close()方法。 Reference docs.python.org python morvanzhou python liaoxuefeng","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python Slice、Iteration、List generation、Generator","slug":"python/language/py-language-10-advanced","date":"2017-06-03T01:00:21.000Z","updated":"2021-06-20T04:12:28.221Z","comments":true,"path":"2017/06/03/python/language/py-language-10-advanced/","link":"","permalink":"http://www.iequa.com/2017/06/03/python/language/py-language-10-advanced/","excerpt":"Slice、Iteration、List generation、Generator","text":"Slice、Iteration、List generation、Generator 12345L = []n = 1while n &lt;= 99: L.append(n) n = n + 2 1. Slice 12345678910111213&gt;&gt;&gt; L = [&#x27;Michael&#x27;, &#x27;Sarah&#x27;, &#x27;Tracy&#x27;, &#x27;Bob&#x27;, &#x27;Jack&#x27;]&gt;&gt;&gt; L[1:3][&#x27;Sarah&#x27;, &#x27;Tracy&#x27;]&gt;&gt;&gt; L[-2:][&#x27;Bob&#x27;, &#x27;Jack&#x27;]&gt;&gt;&gt; L = range(100)&gt;&gt;&gt; L[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]&gt;&gt;&gt; L[:10:2][0, 2, 4, 6, 8]&gt;&gt;&gt; L[::5][0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]&gt;&gt;&gt; tuple也是一种list，唯一区别是tuple不可变。因此，tuple也可以用切片操作，只是操作的结果仍是tuple 12&gt;&gt;&gt; (0, 1, 2, 3, 4, 5)[:3](0, 1, 2) 字符串'xxx'或Unicode字符串u'xxx'也可以看成是一种list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串： 1234&gt;&gt;&gt; &#x27;ABCDEFG&#x27;[:3]&#x27;ABC&#x27;&gt;&gt;&gt; &#x27;ABCDEFG&#x27;[::2]&#x27;ACEG&#x27; 2. Iteration 只要是可迭代对象，无论有无下标，都可以迭代，比如dict就可以迭代 123for (i=0; i&lt;list.length; i++) &#123; n = list[i];&#125; 123d = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125;for key in d: print key 123456&gt;&gt;&gt; for ch in &#x27;ABC&#x27;:... print ch...ABC 如何判断一个对象是可迭代对象呢？方法是通过collections模块的Iterable类型判断： 1234567&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance(&#x27;abc&#x27;, Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False 如果要对list实现类似Java那样的下标循环怎么办？Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身： 123456&gt;&gt;&gt; for i, value in enumerate([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;]):... print i, value...0 A1 B2 C for循环里，同时引用了两个变量，在Python里是很常见 123456&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:... print x, y...1 12 43 9 3. List Generation 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in &#x27;ABC&#x27; for n in &#x27;XYZ&#x27;][&#x27;AX&#x27;, &#x27;AY&#x27;, &#x27;AZ&#x27;, &#x27;BX&#x27;, &#x27;BY&#x27;, &#x27;BZ&#x27;, &#x27;CX&#x27;, &#x27;CY&#x27;, &#x27;CZ&#x27;] 运用列表生成式，可以写出非常简洁的代码。 例如，列出当前目录下的所有文件和目录名 123&gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到&gt;&gt;&gt; [d for d in os.listdir(&#x27;.&#x27;)] # os.listdir可以列出文件和目录[&#x27;.emacs.d&#x27;, &#x27;.ssh&#x27;, &#x27;.Trash&#x27;, &#x27;Adlm&#x27;, &#x27;Applications&#x27;, &#x27;Desktop&#x27;, &#x27;Documents&#x27;, &#x27;Downloads&#x27;, &#x27;Library&#x27;, &#x27;Movies&#x27;, &#x27;Music&#x27;, &#x27;Pictures&#x27;, &#x27;Public&#x27;, &#x27;VirtualBox VMs&#x27;, &#x27;Workspace&#x27;, &#x27;XCode&#x27;] for循环其实可以同时使用两个甚至多个变量，比如dict的iteritems()可以同时迭代key和value： 1234567&gt;&gt;&gt; d = &#123;&#x27;x&#x27;: &#x27;A&#x27;, &#x27;y&#x27;: &#x27;B&#x27;, &#x27;z&#x27;: &#x27;C&#x27; &#125;&gt;&gt;&gt; for k, v in d.iteritems():... print k, &#x27;=&#x27;, v... y = Bx = Az = C 列表生成式也可以使用两个变量来生成list： 123&gt;&gt;&gt; d = &#123;&#x27;x&#x27;: &#x27;A&#x27;, &#x27;y&#x27;: &#x27;B&#x27;, &#x27;z&#x27;: &#x27;C&#x27; &#125;&gt;&gt;&gt; [k + &#x27;=&#x27; + v for k, v in d.iteritems()][&#x27;y=B&#x27;, &#x27;x=A&#x27;, &#x27;z=C&#x27;] 最后把一个list中所有的字符串变成小写： 123&gt;&gt;&gt; L = [&#x27;Hello&#x27;, &#x27;World&#x27;, &#x27;IBM&#x27;, &#x27;Apple&#x27;]&gt;&gt;&gt; [s.lower() for s in L][&#x27;hello&#x27;, &#x27;world&#x27;, &#x27;ibm&#x27;, &#x27;apple&#x27;] 小结 运用列表生成式，可以快速生成list，可以通过一个list推导出另一个list，而代码却十分简洁。 4. Generator 如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器（Generator） 要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： 123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x104feab40&gt; generator 是一个可迭代的对象 1234567891011121314&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; for n in g:... print n...0149162536496481 斐波拉契数列 123456def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print b a, b = b, a + b n = n + 1 fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。 把fib函数变成generator，只需要把 print b 改为 yield b 就可以了 123456def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 如果一个函数定义中包含 yield关键字，那么这个函数就不再是一个普通函数，而是一个generator 12&gt;&gt;&gt; fib(6)&lt;generator object fib at 0x104feaaa0&gt; 函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield 语句处继续执行。 12345678910111213141516171819202122&gt;&gt;&gt; def odd():... print &#x27;step 1&#x27;... yield 1... print &#x27;step 2&#x27;... yield 3... print &#x27;step 3&#x27;... yield 5...&gt;&gt;&gt; o = odd()&gt;&gt;&gt; o.next()step 11&gt;&gt;&gt; o.next()step 23&gt;&gt;&gt; o.next()step 35&gt;&gt;&gt; o.next()Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 小结 generator是非常强大的工具，在Python中，可以简单地把列表生成式改成generator，也可以通过函数实现复杂逻辑的generator。 要理解generator的工作原理，它是在for循环的过程中不断计算出下一个元素，并在适当的条件结束for循环。对于函数改成的generator来说，遇到return语句或者执行到函数体最后一行语句，就是结束generator的指令，for循环随之结束。 Reference 廖雪峰的官方网站","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python def函数、函数参数、函数默认参数","slug":"python/language/py-language-5-function","date":"2017-06-02T23:00:21.000Z","updated":"2021-06-20T04:12:28.227Z","comments":true,"path":"2017/06/03/python/language/py-language-5-function/","link":"","permalink":"http://www.iequa.com/2017/06/03/python/language/py-language-5-function/","excerpt":"定义函数、内置函数、函数参数、函数默认参数","text":"定义函数、内置函数、函数参数、函数默认参数 1. 内置函数 内置函数 12&gt;&gt;&gt; abs(-12.34)12.34 abs api 1234&gt;&gt;&gt; cmp(1, 2)-1&gt;&gt;&gt; cmp(10, 2)1 数据类型转换 1234567891011121314&gt;&gt;&gt; int(&#x27;123&#x27;)123&gt;&gt;&gt; int(12.34)12&gt;&gt;&gt; float(&#x27;12.34&#x27;)12.34&gt;&gt;&gt; str(1.23)&#x27;1.23&#x27;&gt;&gt;&gt; unicode(100)u&#x27;100&#x27;&gt;&gt;&gt; bool(1)True&gt;&gt;&gt; bool(&#x27;&#x27;)False 函数名其实就是指向一个函数对象的引用，完全可以把函数名赋给一个变量，相当于给这个函数起了一个“别名”： 123&gt;&gt;&gt; a = abs # 变量a指向abs函数&gt;&gt;&gt; a(-1) # 所以也可以通过a调用abs函数1 2. def函数 12345def my_abs(x): if x &gt;= 0: return x else: return -x return None 可以简写为 return。 2.1 空函数 12def nop(): pass 2.2 函数参数类型检查 1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError(&#x27;bad operand type&#x27;) if x &gt;= 0: return x else: return -x 2.3 返回多个值 (是一个tuple) 12345678910import mathdef move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny&gt;&gt;&gt; x, y = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print x, y151.961524227 70.0 3. 函数参数 123456def power(x, n=2): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 一是必选参数在前，默认参数在后，否则Python的解释器会报错 二是如何设置默认参数。 当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。 使用默认参数有什么好处？最大的好处是能降低调用函数的难度。 定义默认参数要牢记一点：默认参数必须指向不变对象！ 12345def add_end(L=None): if L is None: L = [] L.append(&#x27;END&#x27;) return L 为什么要设计str、None这样的不变对象呢？因为不变对象一旦创建，对象内部的数据就不能修改，这样就减少了由于修改数据导致的错误。此外，由于对象不变，多任务环境下同时读取对象不需要加锁，同时读一点问题都没有。我们在编写程序时，如果可以设计一个不变对象，那就尽量设计成不变对象。 可变参数 12345678910def calc(numbers): sum = 0 for n in numbers: sum = sum + n * n return sum&gt;&gt;&gt; calc([1, 2, 3])14&gt;&gt;&gt; calc((1, 3, 5, 7))84 如果利用可变参数，调用函数的方式可以简化成这样： 12345def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 定义可变参数和定义list或tuple参数相比，仅仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此，函数代码完全不变。但是，调用该函数时，可以传入任意个参数，包括0个参数： 1234&gt;&gt;&gt; calc(1, 2)5&gt;&gt;&gt; calc()0 这种写法相当有用，而且很常见，见如下 : 123&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums)14 关键字参数 可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。 关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。 请看示例： 12def person(name, age, **kw): print &#x27;name:&#x27;, name, &#x27;age:&#x27;, age, &#x27;other:&#x27;, kw 函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数; 也可以传入任意个数的关键字参数; 1234&gt;&gt;&gt; person(&#x27;Bob&#x27;, 35, city=&#x27;Beijing&#x27;)name: Bob age: 35 other: &#123;&#x27;city&#x27;: &#x27;Beijing&#x27;&#125;&gt;&gt;&gt; person(&#x27;Adam&#x27;, 45, gender=&#x27;M&#x27;, job=&#x27;Engineer&#x27;)name: Adam age: 45 other: &#123;&#x27;gender&#x27;: &#x27;M&#x27;, &#x27;job&#x27;: &#x27;Engineer&#x27;&#125; 123&gt;&gt;&gt; kw = &#123;&#x27;city&#x27;: &#x27;Beijing&#x27;, &#x27;job&#x27;: &#x27;Engineer&#x27;&#125;&gt;&gt;&gt; person(&#x27;Jack&#x27;, 24, **kw)name: Jack age: 24 other: &#123;&#x27;city&#x27;: &#x27;Beijing&#x27;, &#x27;job&#x27;: &#x27;Engineer&#x27;&#125; 参数组合 在Python中定义函数，可以用必选参数、默认参数、可变参数和关键字参数，这4种参数都可以一起使用，或者只用其中某些，但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数和关键字参数。 12def func(a, b, c=0, *args, **kw): print &#x27;a =&#x27;, a, &#x27;b =&#x27;, b, &#x27;c =&#x27;, c, &#x27;args =&#x27;, args, &#x27;kw =&#x27;, kw 在函数调用的时候，Python解释器自动按照参数位置和参数名把对应的参数传进去。 12345678&gt;&gt;&gt; func(1, 2)a = 1 b = 2 c = 0 args = () kw = &#123;&#125;&gt;&gt;&gt; func(1, 2, c=3)a = 1 b = 2 c = 3 args = () kw = &#123;&#125;&gt;&gt;&gt; func(1, 2, 3, &#x27;a&#x27;, &#x27;b&#x27;)a = 1 b = 2 c = 3 args = (&#x27;a&#x27;, &#x27;b&#x27;) kw = &#123;&#125;&gt;&gt;&gt; func(1, 2, 3, &#x27;a&#x27;, &#x27;b&#x27;, x=99)a = 1 b = 2 c = 3 args = (&#x27;a&#x27;, &#x27;b&#x27;) kw = &#123;&#x27;x&#x27;: 99&#125; 神奇的是通过一个tuple和dict，你也可以调用该函数： 1234&gt;&gt;&gt; args = (1, 2, 3, 4)&gt;&gt;&gt; kw = &#123;&#x27;x&#x27;: 99&#125;&gt;&gt;&gt; func(*args, **kw)a = 1 b = 2 c = 3 args = (4,) kw = &#123;&#x27;x&#x27;: 99&#125; 所以，对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。 小结 Python的函数具有非常灵活的参数形态，既可以实现简单的调用，又可以传入非常复杂的参数。 默认参数一定要用不可变对象，如果是可变对象，运行会有逻辑错误！ 要注意定义可变参数和关键字参数的语法： *args是可变参数，args接收的是一个tuple； **kw是关键字参数，kw接收的是一个dict。 以及调用函数时如何传入可变参数和关键字参数的语法： 可变参数既可直接传入：func(1, 2, 3)，又可先组装list或tuple，再通过*args传入：func(*(1, 2, 3))； 关键字参数既可直接传入：func(a=1, b=2)，又可先组装dict，再通过**kw传入：func(**&#123;'a': 1, 'b': 2&#125;)。 使用*args和**kw是Python的习惯写法，当然也可以用其他参数名，但最好使用习惯用法。 递归函数 1234def fact(n): if n==1: return 1 return n * fact(n - 1) 使用递归函数需要注意防止栈溢出。在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出。 使用递归函数的优点是逻辑简单清晰，缺点是过深的调用会导致栈溢出。 Reference article 廖雪峰的官方网站","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python 集合 List、Tuple、Dict、 Set","slug":"python/language/py-language-4-collection-type","date":"2017-05-31T08:00:21.000Z","updated":"2021-06-20T04:12:28.223Z","comments":true,"path":"2017/05/31/python/language/py-language-4-collection-type/","link":"","permalink":"http://www.iequa.com/2017/05/31/python/language/py-language-4-collection-type/","excerpt":"Python 集合 List、Tuple、Dict、 Set","text":"Python 集合 List、Tuple、Dict、 Set List 123&gt;&gt;&gt; classmates = [&#x27;Michael&#x27;, &#x27;Bob&#x27;, &#x27;Tracy&#x27;]&gt;&gt;&gt; classmates[&#x27;Michael&#x27;, &#x27;Bob&#x27;, &#x27;Tracy&#x27;] append 12345&gt;&gt;&gt; classmates[-1]&#x27;Tracy&#x27;&gt;&gt;&gt; classmates.append(&#x27;Adam&#x27;)&gt;&gt;&gt; classmates[&#x27;Michael&#x27;, &#x27;Bob&#x27;, &#x27;Tracy&#x27;, &#x27;Adam&#x27;] insert 123&gt;&gt;&gt; classmates.insert(1, &#x27;Jack&#x27;)&gt;&gt;&gt; classmates[&#x27;Michael&#x27;, &#x27;Jack&#x27;, &#x27;Bob&#x27;, &#x27;Tracy&#x27;, &#x27;Adam&#x27;] pop 1234&gt;&gt;&gt; classmates.pop()&#x27;Adam&#x27;&gt;&gt;&gt; classmates[&#x27;Michael&#x27;, &#x27;Jack&#x27;, &#x27;Bob&#x27;, &#x27;Tracy&#x27;] 要删除list末尾的元素，用pop()方法 pop(i) 1234&gt;&gt;&gt; classmates.pop(1)&#x27;Jack&#x27;&gt;&gt;&gt; classmates[&#x27;Michael&#x27;, &#x27;Bob&#x27;, &#x27;Tracy&#x27;] 要删除指定位置的元素，用pop(i)方法，其中i是索引位置： list 元素类型不同 1&gt;&gt;&gt; L = [&#x27;Apple&#x27;, 123, True] list 里面的元素的数据类型也可以不同 123&gt;&gt;&gt; s = [&#x27;python&#x27;, &#x27;java&#x27;, [&#x27;asp&#x27;, &#x27;php&#x27;], &#x27;scheme&#x27;]&gt;&gt;&gt; len(s)4 12&gt;&gt;&gt; p = [&#x27;asp&#x27;, &#x27;php&#x27;]&gt;&gt;&gt; s = [&#x27;python&#x27;, &#x27;java&#x27;, p, &#x27;scheme&#x27;] list元素也可以是另一个list Tuple tuple一旦初始化就不能修改,元素指向不改变 1&gt;&gt;&gt; classmates = (&#x27;Michael&#x27;, &#x27;Bob&#x27;, &#x27;Tracy&#x27;) 123&gt;&gt;&gt; t = (1, 2)&gt;&gt;&gt; t(1, 2) 如果要定义一个空的tuple，可以写成() tuple 的陷阱 123&gt;&gt;&gt; t = (1)&gt;&gt;&gt; t1 正确的方式如下 : 123&gt;&gt;&gt; t = (1,)&gt;&gt;&gt; t(1,) 最后来看一个“可变的”tuple： 12345&gt;&gt;&gt; t = (&#x27;a&#x27;, &#x27;b&#x27;, [&#x27;A&#x27;, &#x27;B&#x27;])&gt;&gt;&gt; t[2][0] = &#x27;X&#x27;&gt;&gt;&gt; t[2][1] = &#x27;Y&#x27;&gt;&gt;&gt; t(&#x27;a&#x27;, &#x27;b&#x27;, [&#x27;X&#x27;, &#x27;Y&#x27;]) 其实变的不是tuple的元素，而是list的元素。tuple一开始指向的list并没有改成别的list Dict 123456&gt;&gt;&gt; d = &#123;&#x27;Michael&#x27;: 95, &#x27;Bob&#x27;: 75, &#x27;Tracy&#x27;: 85&#125;&gt;&gt;&gt; d[&#x27;Michael&#x27;]95&gt;&gt;&gt; d[&#x27;Adam&#x27;] = 67&gt;&gt;&gt; d[&#x27;Adam&#x27;]67 判断 key 是否存在 12&gt;&gt;&gt; &#x27;Thomas&#x27; in dFalse 二是通过dict提供的get方法，如果key不存在，可以返回None，或者自己指定的value： 123&gt;&gt;&gt; d.get(&#x27;Thomas&#x27;)&gt;&gt;&gt; d.get(&#x27;Thomas&#x27;, -1)-1 注意：返回None的时候Python的交互式命令行不显示结果。 删除 key，pop(key) 要删除一个key，用pop(key)方法，对应的value也会从dict中删除： 1234&gt;&gt;&gt; d.pop(&#x27;Bob&#x27;)75&gt;&gt;&gt; d&#123;&#x27;Michael&#x27;: 95, &#x27;Tracy&#x27;: 85&#125; dict list 查找和插入的速度极快，不会随着key的增加而增加 查找和插入的时间随着元素的增加而增加 需要占用大量的内存，内存浪费多 占用空间小，浪费内存很少 所以，dict是用空间来换取时间的一种方法。 dict可以用在需要高速查找的很多地方，在Python代码中几乎无处不在，正确使用dict非常重要，需要牢记的第一条就是dict的key必须是不可变对象。 是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash）。 保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key： Set set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 set init 要创建一个set，需要提供一个list作为输入集合： 123&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; sset([1, 2, 3]) 注意，传入的参数[1, 2, 3]是一个list，而显示的set([1, 2, 3])只是告诉你这个set内部有1，2，3这3个元素，显示的[]不表示这是一个list。 add、remove 123456789101112&gt;&gt;&gt; s = set([1, 1, 2, 2, 3, 3])&gt;&gt;&gt; sset([1, 2, 3])&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; sset([1, 2, 3, 4])&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; sset([1, 2, 3, 4])&gt;&gt;&gt; s.remove(4)&gt;&gt;&gt; sset([1, 2, 3]) set &amp; and | set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： 123456&gt;&gt;&gt; s1 = set([1, 2, 3])&gt;&gt;&gt; s2 = set([2, 3, 4])&gt;&gt;&gt; s1 &amp; s2set([2, 3])&gt;&gt;&gt; s1 | s2set([1, 2, 3, 4]) set和dict的唯一区别仅在于没有存储对应的value，set的原理和dict一样. difference、intersection 我们还能进行一些筛选操作, 比如对比另一个东西, 看看原来的 set 里有没有和他不同的 (difference). 或者对比另一个东西, 看看 set 里有没有相同的 (intersection). 123456789print(unique_char)# &#123;&#x27;b&#x27;, &#x27;c&#x27;, &#x27;a&#x27;&#125;unique_char = set(char_list)print(unique_char.difference(&#123;&#x27;a&#x27;, &#x27;e&#x27;, &#x27;i&#x27;&#125;))# &#123;&#x27;b&#x27;, &#x27;d&#x27;, &#x27;c&#x27;&#125;print(unique_char.intersection(&#123;&#x27;a&#x27;, &#x27;e&#x27;, &#x27;i&#x27;&#125;))# &#123;&#x27;a&#x27;&#125; 议不可变对象 12345678910&gt;&gt;&gt; a = [&#x27;c&#x27;, &#x27;b&#x27;, &#x27;a&#x27;]&gt;&gt;&gt; a.sort()&gt;&gt;&gt; a[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]&gt;&gt;&gt; a = &#x27;abc&#x27;&gt;&gt;&gt; a.replace(&#x27;a&#x27;, &#x27;A&#x27;)&#x27;Abc&#x27;&gt;&gt;&gt; a&#x27;abc&#x27; 小结 : 使用key-value存储结构的dict在Python中非常有用，选择不可变对象作为key很重要，最常用的key是字符串。 tuple 虽然是不变对象，但试试把 (1, 2, 3) 和 (1, [2, 3]) 放入dict或set中，并解释结果。 Reference article 廖雪峰的官方网站","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python 字符编码 & 字符串","slug":"python/language/py-language-3-str-&-character-encoding","date":"2017-05-31T05:05:21.000Z","updated":"2021-06-20T04:12:28.227Z","comments":true,"path":"2017/05/31/python/language/py-language-3-str-&-character-encoding/","link":"","permalink":"http://www.iequa.com/2017/05/31/python/language/py-language-3-str-&-character-encoding/","excerpt":"Python 字符编码 大概原理 与 字符串简单操作","text":"Python 字符编码 大概原理 与 字符串简单操作 字符编码 因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。最早的计算机在设计时采用8个比特（bit）作为一个字节（byte），所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255），如果要表示更大的整数，就必须用更多的字节。比如两个字节可以表示的最大整数是65535，4个字节可以表示的最大整数是4294967295。 由于计算机是美国人发明的，因此，最早只有127个字母被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。 但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。 因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。 Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。 字母A用ASCII编码是十进制的65，二进制的01000001； 字符0用ASCII编码是十进制的48，二进制的00110000，注意字符’0’和整数0是不同的； 你可以猜测，如果把ASCII编码的A用Unicode编码，只需要在前面补0就可以，因此，A的Unicode编码是00000000 01000001。 新的问题又出现了：如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。 所以，本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间： 在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。 字符串 因为Python的诞生比Unicode标准发布的时间还要早，所以最早的Python只支持ASCII编码，普通的字符串'ABC'在Python内部都是ASCII编码的。 1234&gt;&gt;&gt; ord(&#x27;A&#x27;)65&gt;&gt;&gt; chr(65)&#x27;A&#x27; Python在后来添加了对Unicode的支持，以Unicode表示的字符串用u'...'表示，比如： 1234&gt;&gt;&gt; print u&#x27;中文&#x27;中文&gt;&gt;&gt; u&#x27;中&#x27;u&#x27;\\u4e2d&#x27; 写u'中'和u'\\u4e2d'是一样的，\\u后面是十六进制的Unicode码。因此，u'A'和u'\\u0041'也一样的。 两种字符串如何相互转换？字符串'xxx'虽然是ASCII编码，但也可以看成是UTF-8编码，而u'xxx'则只能是Unicode编码。 把u'xxx'转换为UTF-8编码的'xxx'用encode('utf-8')方法： 1234&gt;&gt;&gt; u&#x27;ABC&#x27;.encode(&#x27;utf-8&#x27;)&#x27;ABC&#x27;&gt;&gt;&gt; u&#x27;中文&#x27;.encode(&#x27;utf-8&#x27;)&#x27;\\xe4\\xb8\\xad\\xe6\\x96\\x87&#x27; 英文字符转换后表示的UTF-8的值和Unicode值相等（但占用的存储空间不同），而中文字符转换后1个Unicode字符将变为3个UTF-8字符，你看到的\\xe4就是其中一个字节，因为它的值是228，没有对应的字母可以显示，所以以十六进制显示字节的数值。len()函数可以返回字符串的长度： 12345678&gt;&gt;&gt; len(u&#x27;ABC&#x27;)3&gt;&gt;&gt; len(&#x27;ABC&#x27;)3&gt;&gt;&gt; len(u&#x27;中文&#x27;)2&gt;&gt;&gt; len(&#x27;\\xe4\\xb8\\xad\\xe6\\x96\\x87&#x27;)6 反过来，把UTF-8编码表示的字符串'xxx'转换为Unicode字符串u'xxx'用decode('utf-8')方法： 123456&gt;&gt;&gt; &#x27;abc&#x27;.decode(&#x27;utf-8&#x27;)u&#x27;abc&#x27;&gt;&gt;&gt; &#x27;\\xe4\\xb8\\xad\\xe6\\x96\\x87&#x27;.decode(&#x27;utf-8&#x27;)u&#x27;\\u4e2d\\u6587&#x27;&gt;&gt;&gt; print &#x27;\\xe4\\xb8\\xad\\xe6\\x96\\x87&#x27;.decode(&#x27;utf-8&#x27;)中文 格式化 1234&gt;&gt;&gt; &#x27;Hello, %s&#x27; % &#x27;world&#x27;&#x27;Hello, world&#x27;&gt;&gt;&gt; &#x27;Hi, %s, you have $%d.&#x27; % (&#x27;Michael&#x27;, 1000000)&#x27;Hi, Michael, you have $1000000.&#x27; 1234&gt;&gt;&gt; &#x27;%2d-%02d&#x27; % (3, 1)&#x27; 3-01&#x27;&gt;&gt;&gt; &#x27;%.2f&#x27; % 3.1415926&#x27;3.14&#x27; Reference 廖雪峰的官方网站","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Python Output、Variable、dataType、If、While/For、Py Head","slug":"python/language/py-language-2-Output-Variable-dataType-If-While:For-PyHead","date":"2017-05-31T03:00:21.000Z","updated":"2021-06-20T04:12:28.221Z","comments":true,"path":"2017/05/31/python/language/py-language-2-Output-Variable-dataType-If-While:For-PyHead/","link":"","permalink":"http://www.iequa.com/2017/05/31/python/language/py-language-2-Output-Variable-dataType-If-While:For-PyHead/","excerpt":"Python 的 print 语句、Variable 变量定义、 数据类型、条件与循环","text":"Python 的 print 语句、Variable 变量定义、 数据类型、条件与循环 Output print语句也可以跟上多个字符串，用逗号“,”隔开，就可以连成一串输出： 1print(&#x27;The quick brown fox&#x27;, &#x27;jumps over&#x27;, &#x27;the lazy dog&#x27;) The quick brown fox jumps over the lazy dog print会依次打印每个字符串，遇到逗号“,”会输出一个空格 123print(int(&#x27;2&#x27;)+3) # int 字符串会转为整数print(int(1.9)) # int会保留整数部分print(float(&#x27;1.2&#x27;)+3) #float()是浮点型，可以把字符串转换成小数 5 1 4.2 Input 1234&gt;&gt;&gt; name = input()Blair&gt;&gt;&gt; print(&#x27;Hello,&#x27;, name)Hello, Blair Variable 12a,b,c=11,12,13print(a,b,c) 11 12 13 global var 那如何在外部也能调用一个在局部里修改了的全局变量呢. 首先我们在外部定义一个全局变量 a=None, 然后再 fun() 中声明 这个 a 是来自外部的 a. 声明方式就是 global a. 然后对这个外部的 a 修改后, 修改的效果会被施加到外部的 a 上. 所以我们将能看到运行完 fun(), a 的值从 None 变成了 20. 1234567891011APPLY = 100 # 全局变量a = Nonedef fun(): global a # 使用之前在全局里定义的 a a = 20 # 现在的 a 是全局变量了 return a+100print(APPLE) # 100print(&#x27;a past:&#x27;, a) # Nonefun()print(&#x27;a now:&#x27;, a) # 20 dataType 序号 data type example value 1 int 3 2 float 1.2 3 str ‘hello’ or “hello” 4 boolean True/False 5 None None 6 常量 if/else 1234567age = 20if age &gt;= 6: print(&#x27;teenager&#x27;)elif age &gt;= 18: print(&#x27;adult&#x27;)else: print(&#x27;kid&#x27;) teenager if/while 遇 None 123v1 = Noneif v1: print(&#x27;v1&#x27;) 如果 if / while 后面接着的语句数据类型 None, 将与 False 处理方式相同 if/while 遇 空集合 1234567A = []if A: print(&quot;A is empty !&quot;)A = [1, 2, 3]if A: print(&quot;A is not empty !&quot;) A is not empty ! 在 Python 中集合类型有 list、 tuple 、dict 和 set 等，如果该集合对象作为 if 或 while 判断语句, 则与 False 处理方式相同 While/For 1234a = range(5)while a: print(a[-1]) a = a[:len(a)-1] 4 3 2 1 0 123names = [&#x27;Michael&#x27;, &#x27;Bob&#x27;, &#x27;Tracy&#x27;]for name in names: print(name) Michael Bob Tracy py 程序头部 12#!/usr/bin/env python# -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释； 第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，在源代码中写的中文输出可能会乱码。 Reference morvanzhou python 廖雪峰的官方网站","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Shuping Yang's University of Maryland speech","slug":"English/english-Shuping-Yang-at-Maryyland","date":"2017-05-27T23:08:21.000Z","updated":"2021-06-20T04:12:28.299Z","comments":true,"path":"2017/05/28/English/english-Shuping-Yang-at-Maryyland/","link":"","permalink":"http://www.iequa.com/2017/05/28/English/english-Shuping-Yang-at-Maryyland/","excerpt":"Shuping Yang Maryland Graduation Speech About Air and Free","text":"Shuping Yang Maryland Graduation Speech About Air and Free Speech Full Text Good afternoon faculty, students, parents and friends. I am truly honored and grateful to speak at the commencement for the University of Maryland, Class of 2017. People often ask me: Why did you come to the University of Maryland? I always answer: Fresh air. Five years ago, as I step off the plane from China, and left the terminal at Dallas Airport. I was ready to put on one of my five face masks, but when I took my first breath of American air. I put my mask away. The air was so sweet and fresh, and utterly luxurious. I was surprised by this. I grew up in a city in China, where I had to wear a face mask every time I went outside, otherwise, I might get sick. However, the moment Iinhaled and exhaled outside the airport，I felt free. No more fog on my glasses, no more difficult breathing, no more suppression. Every breath was a delight. As I stand here today, I cannot help, but recall that feeling of freedom. At the University of Maryland, I will soon feel another kind of fresh air for which I will beforever grateful — the fresh air of free speech. Before I came to the UnitedStates, I learned in history class about the Declaration of Independence, butthese words had no meaning to me— Life, Liberty and the Pursuit of happiness. I was merely memorizing the words to get good grades. These words sounded so strange, so abstract and foreign to me, until I came to University of Maryland. I have learned the right to freely express oneself is sacred in American. Each day in Maryland, I was encouraged to express my opinions on controversial issues. I could challenge astatement made by my instructor. I could even rate my professors online. But nothing prepared me forthe culture shock I experienced when I watched a university production of the play— Twilight: Los Angeles. Twilight is a play by AnnaDeavere Smith about the 1992 Los Angeles Riots. The riots followed acquittal of four Los Angeles police officers in the videotaped arrest and beating of Rodney King. For six days, the city was in chaos as citizens took to the streets. In Twilight, the student actors were openly talking about racism, sexism and politics. I was shocked, I never thought such topic could be discussed openly. The play was my first taste of political story telling, one that makes the audience think critically. I have always had a burning desire to tell these kinds of stories, but I was convinced that only authorities on the narrative, only authorities could define the truth. However, the opportunity toimmerse myself in the perse community at the University of Maryland exposed me to various, many different perspectives on truth. I soon realized that here I have the opportunity to speak freely. My voice matters. Your voice matters. Our voices matter. Civil engagement is not at ask just for politicians. I have witnessed this when I saw my fellow student smarching in Washington DC, voting in the presidential election and raising money to support various causes. I have seen that everyone has a right to participate and advocate for change. I used to believe that one inpidual participation could not make a difference, but here we are, United Terps. Together, we can push our society to be more just, open and peaceful. Class of 2017, we are graduating from a university that embraces a liberal arts education that nurtures us to think critically, and also to care and feel for humanity. We are equipped with the knowledge of various disciplines and we are ready to face the challenges of our society. Some of us may go to graduate school, some us may step into professions and some of us may begin ajourney of exploration. But no matter what we do,remember, democracy and free speech should not be taken for granted. Democracy and freedom are the fresh air that is worth fighting for. Freedom is oxygen. Freedomis passion. Freedom is love. And as a French philosopher Jean Paul Sartre once said: freedom is a choice, our future is dependent on the choices we make today and tomorrow. We are all playwrights of the next chapters of our lives. Together, we write the human history. My friends, enjoy the fresh air and never ever let it go. Thank you.","categories":[{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"}],"tags":[{"name":"Shuping Yang","slug":"Shuping-Yang","permalink":"http://www.iequa.com/tags/Shuping-Yang/"}]},{"title":"Coursera 6 - Advice for Applying Machine Learning *","slug":"ml/coursera-ng-w6-Advice-for-Applying-Machine-Learning","date":"2017-05-24T14:08:21.000Z","updated":"2021-06-20T04:12:28.316Z","comments":true,"path":"2017/05/24/ml/coursera-ng-w6-Advice-for-Applying-Machine-Learning/","link":"","permalink":"http://www.iequa.com/2017/05/24/ml/coursera-ng-w6-Advice-for-Applying-Machine-Learning/","excerpt":"Evaluating a Hypothesis -&gt; Model Selection、Diagnosing Bias vs Variance -&gt; Regularization、Learning Curves","text":"Evaluating a Hypothesis -&gt; Model Selection、Diagnosing Bias vs Variance -&gt; Regularization、Learning Curves 1. Evaluating a Hypothesis Once we have done some trouble shooting for errors in our predictions by: Getting more training examples Trying smaller sets of features Trying additional features Trying polynomial features Increasing or decreasing λλλ We can move on to evaluate our new hypothesis. A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70% of your data and the test set is the remaining 30%. The new procedure using these two sets is then: Learn Θ\\ThetaΘ and minimize Jtrain(Θ)J_{train}(\\Theta)Jtrain​(Θ) using the training set Compute the test set error J_test(Θ)J\\_{test}(\\Theta)J_test(Θ) The test set error For linear regression: J_test(Θ)=12m_test∑_i=1m_test(h_Θ(x(i)_test)−y(i)_test)2J\\_{test}(\\Theta) = \\dfrac{1}{2m\\_{test}} \\sum\\_{i=1}^{m\\_{test}}(h\\_\\Theta(x^{(i)}\\_{test}) - y^{(i)}\\_{test})^2J_test(Θ)=2m_test1​∑_i=1m_test(h_Θ(x(i)_test)−y(i)_test)2 For classification ~ Misclassification error (aka 0/1 misclassification error): err(h\\_\\Theta(x),y) = \\begin{matrix} 1 &amp; \\mbox{if } h\\_\\Theta(x) \\geq 0.5\\ and\\ y = 0\\ or\\ h\\_\\Theta(x) &lt; 0.5\\ and\\ y = 1\\newline 0 &amp; \\mbox otherwise \\end{matrix} This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is: Test Error=1m_test∑m_test_i=1err(h_Θ(x(i)_test),y(i)_test)\\text{Test Error} = \\dfrac{1}{m\\_{test}} \\sum^{m\\_{test}}\\_{i=1} err(h\\_\\Theta(x^{(i)}\\_{test}), y^{(i)}\\_{test}) Test Error=m_test1​∑m_test​_i=1err(h_Θ(x(i)_test),y(i)_test) This gives us the proportion of the test data that was misclassified. inaccurate[ɪn’ækjərət]、procedure [prə’sidʒɚ] 比例 remaining [ri’men…]、 Misclassification ['mis,klæsifi’keiʃən] validation [,vælə’deʃən] 确认，批准 2. Model Selection Train/Validtion/Test Sets Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set. Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result. One way to break down our dataset into the three sets is: Training set: 60% Cross validation set: 20% Test set: 20% We can now calculate three separate error values for the three different sets using the following method: Optimize the parameters in Θ using the training set for each polynomial degree. Find the polynomial degree d with the least error using the cross validation set. Estimate the generalization error using the test set with J_test(Θ(d))J\\_{test}(\\Theta^{(d)})J_test(Θ(d)), (d = theta from polynomial with lower error); This way, the degree of the polynomial d has not been trained using the test set. 3. Diagnosing Bias vs. Variance In this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis. We need to distinguish whether bias or variance is the problem contributing to bad predictions. High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two. The training error will tend to decrease as we increase the degree d of the polynomial. At the same time, the cross validation error will tend to decrease as we increase d up to a point, and then it will increase as d is increased, forming a convex curve. High bias (underfitting): both J_train(Θ)J\\_{train}(\\Theta)J_train(Θ) and J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ) will be high. Also, J_CV(Θ)≈J_train(Θ)J\\_{CV}(\\Theta) \\approx J\\_{train}(\\Theta)J_CV(Θ)≈J_train(Θ). High variance (overfitting): J_train(Θ)J\\_{train}(\\Theta)J_train(Θ) will be low and J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ) will be much greater than J_train(Θ)J\\_{train}(\\Theta)J_train(Θ). The is summarized in the figure below: 4. Regularization Bias/Variance Note: [The regularization term below and through out the video should be λ2m∑_j=1nθ_j2\\frac \\lambda {2m} \\sum\\_{j=1}^n \\theta\\_j ^22mλ​∑_j=1nθ_j2 and NOT λ2m∑_j=1mθ_j2\\frac \\lambda {2m} \\sum\\_{j=1}^m \\theta\\_j ^22mλ​∑_j=1mθ_j2] In the figure above, we see that as λ increases, our fit becomes more rigid. On the other hand, as λ approaches 0, we tend to over overfit the data. So how do we choose our parameter λ to get it ‘just right’ ? In order to choose the model and the regularization term λ, we need to: Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24}); Create a set of models with different degrees or any other variants. Iterate through the λs and for each λ go through all the models to learn some Θ. Compute the cross validation error using the learned Θ (computed with λ) on the J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ) without regularization or λ = 0. ？？ Select the best combo that produces the lowest error on the cross validation set. Using the best combo Θ and λ, apply it on J_test(Θ)J\\_{test}(\\Theta)J_test(Θ) to see if it has a good generalization of the problem. 5. Learning Curves Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence: As the training set gets larger, the error for a quadratic function increases. The error value will plateau out after a certain m, or training set size. Experiencing high bias: Low training set size: causes J_train(Θ)J\\_{train}(\\Theta)J_train(Θ) to be low and J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ) to be high. Large training set size: causes both J_train(Θ)J\\_{train}(\\Theta)J_train(Θ) and J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ) to be high with J_train(Θ)J\\_{train}(\\Theta)J_train(Θ)≈J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ). If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much. me : 随着train sets的增加，高偏差会降低，因为更容易找到合适的 Hypothesis 去拟合数据。所以CV error下降。 Experiencing high variance: Low training set size: J_train(Θ)J\\_{train}(\\Theta)J_train(Θ) will be low and J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ) will be high. Large training set size: J_train(Θ)J\\_{train}(\\Theta)J_train(Θ) increases with training set size and J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ) continues to decrease without leveling off. Also, J_train(Θ)J\\_{train}(\\Theta)J_train(Θ) &lt; J_CV(Θ)J\\_{CV}(\\Theta)J_CV(Θ) but the difference between them remains significant. If a learning algorithm is suffering from high variance, getting more training data is likely to help. me : 随着 train sets 的增加，overfiting 越来越不容易。造成 CV error 下降。 plateau out / leveling off 达到平稳状态 test error will be CV error on picture. 6. What to Do Next Revisited *Our decision process can be broken down as follows: Function Result Getting more training examples Fixes high variance Trying smaller sets of features Fixes high variance Increasing λ Fixes high variance Adding features Fixes high bias Adding polynomial features Fixes high bias Decreasing λ Fixes high bias 6.1 Diagnosing NN A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper. A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case you can use regularization (increase λ) to address the overfitting. 6.2 Model Complexity Effects Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently. Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance. In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well. Reference article coursera 6 @ng","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Spark ALS","slug":"spark/spark-ALS","date":"2017-05-07T02:28:21.000Z","updated":"2021-06-20T04:12:28.353Z","comments":true,"path":"2017/05/07/spark/spark-ALS/","link":"","permalink":"http://www.iequa.com/2017/05/07/spark/spark-ALS/","excerpt":"Spark.apache.org Java Chen Spark","text":"Spark.apache.org Java Chen Spark 以下为手动计算流程 : 4.2 启动 spark-shell 1SPARK_CLASSPATH=/opt/cloudera/parcels/CDH/lib/sqoop/mysql-connector-java-5.1.40.jar spark-shell 4.3 输入输出:相关变量 123val inputTable = &quot;mds_user_coupon_bhv&quot;val inputUrl = &quot;jdbc:mysql://192.168.xxx.xx:3306/com_profile?user=your_name&amp;password=your_password&quot;val outputTable = &quot;mds_rs_shop_coupon_tmp&quot; 4.4 核心程序代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.x.rs.serviceimport java.text.SimpleDateFormatimport java.util.Dateimport java.util.Propertiesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types._import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.recommendation.&#123;ALS, Rating&#125;/** * Date : 2017-04-20 * Author : Blair Chan */object RsCouponCalc &#123; def main(args: Array[String]) &#123; println(&quot;start...&quot;) if (args.length &lt; 3) &#123; System.err.println(&quot;Usage: &lt;file&gt;&quot;) System.exit(1) &#125; val inputTable = args(0) // Should be some file on your system // conf = new SparkConf().setAppName(appName).setMaster(&quot;local&quot;); val inputUrl = args(1) val outputTable = args(2) val conf = new SparkConf().setAppName(&quot;SparkRsOne&quot;); val sc = new SparkContext(conf) val sqlContext = new org.apache.spark.sql.SQLContext(sc) // val rawData = sc.textFile(inputFile) // val rawRatings = rawData.map(_.split(&quot;\\t&quot;).take(3)) val url = inputUrl val prop = new Properties() val dfForRawData = sqlContext.read.jdbc(url, inputTable, prop) val ratings_tmp = dfForRawData.map &#123; row =&gt; (row(1).toString().toInt, row(4).toString().toInt, row(6).toString().toDouble) &#125; val ratings = ratings_tmp.map &#123; case (uid, couponId, rating) =&gt; Rating(uid.toInt, couponId.toInt, rating.toDouble) &#125; val model = ALS.train(ratings, 50, 10, 0.01) model.userFeatures.count val K = 10 model.recommendProductsForUsers(K) val originResultRdd1 = model.recommendProductsForUsers(K) val curDate = new Date() val createDateString = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;).format(curDate) val originResultRdd2 = originResultRdd1.map(tuple =&gt; &#123; val uid = tuple._1 val product = tuple._2.map &#123; case Rating(user, product, score) =&gt; (product.toString, score.toString) &#125; (uid, product) &#125;).flatMap &#123; case (uid, product) =&gt; &#123; product.map &#123; case (itemId, score) =&gt; Row.apply(uid.toLong, itemId.toString, score.toDouble, createDateString.toString) &#125; &#125; &#125; // println(originResultRdd2.first()) val schema = StructType( StructField(&quot;uid&quot;, LongType) :: StructField(&quot;coupon_id&quot;, LongType) :: StructField(&quot;score&quot;, DoubleType) :: StructField(&quot;calc_date&quot;, StringType) :: Nil) val df = sqlContext.createDataFrame(originResultRdd2, schema) df.insertIntoJDBC(url, outputTable, false) // 设置为 true，则为 删除表，然后自动创建，再插入 println(&quot;end !&quot;) &#125;&#125; DF 通过插入 RMDB. schema 可以通过反射来使得程序扩展性提高。 spark sql internet","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"ALS","slug":"ALS","permalink":"http://www.iequa.com/tags/ALS/"}]},{"title":"Spark SQL编程","slug":"spark/spark-SQL","date":"2017-04-28T07:28:21.000Z","updated":"2021-06-20T04:12:28.356Z","comments":true,"path":"2017/04/28/spark/spark-SQL/","link":"","permalink":"http://www.iequa.com/2017/04/28/spark/spark-SQL/","excerpt":"Spark SQL，可对不同格式的数据执行ETL操作（如JSON，Parquet，数据库）然后完成特定的查询操作。","text":"Spark SQL，可对不同格式的数据执行ETL操作（如JSON，Parquet，数据库）然后完成特定的查询操作。 DataFrame Data Sources JDBC Server 使用Spark SQL时，最主要的两个组件就是 DataFrame 和 SQLContext。 1. DataFrame DataFrame 是一个分布式的，按照命名列的形式组织的数据集合。DataFrame基于R语言中的dataframe概念，与关系型数据库中的数据库表类似。 之前版本的Spark SQL API中的SchemaRDD已经更名为DataFrame 调用将DataFrame的内容作为行RDD（RDD of Rows）返回的rdd方法，可以将DataFrame转换成RDD。 创建 DataFrame 可以通过如下 数据源创建 DataFrame : 已有的RDD 结构化数据文件 JSON数据集 Hive表 外部数据库 DataFrame API Spark SQL和DataFrame API已经在下述几种程序设计语言中实现： Scala DataFrame API Java DataFrame API Python DataFrame API 2. SQLContext SQLContext封装Spark中的所有关系型功能。可以用之前的示例中的现有SparkContext创建SQLContext。 1val sqlContext = new org.apache.spark.sql.SQLContext(sc) SQLContext HiveContext 3. JDBC数据源 JDBC 数据源 可用于通过JDBC API读取关系型数据库中的数据。相比于使用JdbcRDD，应该将JDBC数据源的方式作为首选，因为JDBC数据源能够将结果作为DataFrame对象返回，直接用Spark SQL处理或与其他数据源连接。 为确保Spark Shell程序有足够的内存，可以在运行spark-shell命令时，加入driver-memory命令行参数，如下所示： 1spark-shell.cmd --driver-memory 1G Ref 用Apache Spark进行大数据处理——第二部分：Spark SQL","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Read Mysql 的四种方式","slug":"spark/spark-read-mysql-four-functions","date":"2017-04-20T07:28:21.000Z","updated":"2021-06-20T04:12:28.344Z","comments":true,"path":"2017/04/20/spark/spark-read-mysql-four-functions/","link":"","permalink":"http://www.iequa.com/2017/04/20/spark/spark-read-mysql-four-functions/","excerpt":"目前 Spark 支持四种方式从数据库中读取数据，这里以 MySQL 为例进行介绍。","text":"目前 Spark 支持四种方式从数据库中读取数据，这里以 MySQL 为例进行介绍。 Startup spark-shell 1SPARK_CLASSPATH=/opt/cloudera/parcels/CDH/lib/sqoop/mysql-connector-java-5.1.40.jar spark-shell 1. 不指定查询条件 1.1 function define 1def jdbc(url: String, table: String, properties: Properties): DataFrame 1.2 detail example 123456val url = &quot;jdbc:mysql://192.168.***.**:3306/your_lib_name?user= your_username&amp;password=your_password&quot;import java.util.Propertiesval prop = new Properties()val df = sqlContext.read.jdbc(url, &quot;mds_user_coupon_bhv&quot;, prop )println(df.count())println(df.rdd.partitions.size) 我们运行上面的程序，可以看到df.rdd.partitions.size输出结果是1，这个结果的含义是iteblog表的所有数据都是由RDD的一个分区处理的，所以说，如果你这个表很大，很可能会出现OOM Note : 这种方式在数据量大的时候不建议使用。 2. 指定数据库字段的范围 这种方式就是通过指定数据库中某个字段的范围，但是这个字段必须是数字，来看看这个函数的函数原型： 12345678def jdbc( url: String, table: String, columnName: String, lowerBound: Long, upperBound: Long, numPartitions: Int, connectionProperties: Properties): DataFrame … not finish 3. 根据任意字段进行分区 … 4. 通过 load 获取 123val df = sqlContext.load(&quot;jdbc&quot;, Map(&quot;url&quot; -&gt; &quot;jdbc:mysql://...&quot;, &quot;dbtable&quot; -&gt; &quot;mds_user_coupon_bhv&quot;) ) 换一种更正式的写法如下 : 1234val df = sqlContext.read.format(&quot;jdbc&quot;).options(Map( &quot;url&quot; -&gt; &quot;jdbc:mysql://192.168.***.**:3306/your_lib_name?user= your_username&amp;password=your_password&quot;, &quot;dbtable&quot; -&gt; &quot;mds_user_coupon_bhv&quot;) ).load() options函数支持url、driver、dbtable、partitionColumn、lowerBound、upperBound以及numPartitions选项，这个和方法二的参数一致。其内部实现原理部分和方法二大体一致。同时load方法还支持json、orc等数据源的读取。 Reading notes 5. Reference 尊重原创，转载请注明： 转载自过往记忆（http://www.iteblog.com/） Spark Read Mysql-csdn","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"标签 ID 定义示例","slug":"datascience/id-label-index","date":"2017-04-20T05:28:21.000Z","updated":"2021-06-20T04:12:28.340Z","comments":true,"path":"2017/04/20/datascience/id-label-index/","link":"","permalink":"http://www.iequa.com/2017/04/20/datascience/id-label-index/","excerpt":"标签 ID 定义示例","text":"标签 ID 定义示例 年龄 含义 性别 含义 兴趣 含义 人群 含义 地域 含义 00601 0-20岁 00501 男 00801 科技 00901 都市白领 331001 北京 00602 21-25岁 00502 女 00802 丽人 00902 运动健身 331002 上海 00603 26-30岁 … … 00803 房产 00903 时尚达人 332001 浙江省-杭州 00604 31-35岁 00804 教育 00904 青春校园 … … … … 00806 育儿 … … 已经接入DMP的客户 客户名称 客户ID 其他 老苏宁 1001 废弃 秒针 1002 秒针 百分点 1003 百分点 晶赞 1004 晶赞 华扬 1005 华扬 派择 1006 派择 苏宁 6809 和业务端统一","categories":[{"name":"data-science","slug":"data-science","permalink":"http://www.iequa.com/categories/data-science/"}],"tags":[{"name":"sina","slug":"sina","permalink":"http://www.iequa.com/tags/sina/"}]},{"title":"New Mac Install Brew Iterm Zsh","slug":"devops/ops-new-macosx-install-soft","date":"2017-03-22T08:43:21.000Z","updated":"2021-06-20T04:12:28.201Z","comments":true,"path":"2017/03/22/devops/ops-new-macosx-install-soft/","link":"","permalink":"http://www.iequa.com/2017/03/22/devops/ops-new-macosx-install-soft/","excerpt":"新机 mac 安装一些常用软件","text":"新机 mac 安装一些常用软件 1. install homebrew Mac 下面的包管理工具，通过 Github 托管适合 Mac 的编译配置以及 Patch，可以方便的安装开发工具。 Mac 自带ruby 所以安装起来很方便，同时它也会自动把git也给你装上。 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装过程可能会有点慢，完成之后，建议执行一下自检，brew doctor 看到 Your system is ready to brew. 那么恭喜你的 brew 已经可以开始使用了。 1brew install wget tree brew 常用命令： （所有软件以PHP5.5为例子） 123456789101112131415brew update #更新brew可安装包，建议每次执行一下brew search php55 #搜索php5.5brew tap josegonzalez/php #安装扩展&lt;gihhub_user/repo&gt; brew tap #查看安装的扩展列表brew install php55 #安装php5.5brew remove php55 #卸载php5.5brew upgrade php55 #升级php5.5brew options php55 #查看php5.5安装选项brew info php55 #查看php5.5相关信息brew home php55 #访问php5.5官方网站brew services list #查看系统通过 brew 安装的服务brew services cleanup #清除已卸载无用的启动配置文件brew services restart php55 #重启php-fpm 注意：brew services 相关命令最好别经常用了，提示会被移除 2. install zsh ohmyzsh &amp; iTerm2 两个神器，在Mac os x下是一定要装的. 2.1 install onmyzsh 1curl -L http://install.ohmyz.sh | sh 2.2 install zsh 在 Terminal 下，直接敲 zsh. 下面请暂时忽略 设置默认shell 1234cat /etc/shells # List of acceptable shells for chpass(1). # Ftpd will not allow users to connect who are not using # one of these shells. /bin/bash /bin/csh /bin/ksh /bin/sh /bin/tcsh /bin/zsh zsh --version zsh 5.0.2 (x86_64-apple-darwin13.0) chsh -s /bin/zsh 虽然Mac自带了zsh，如果你想要最新版的zsh，那么你用 brew install zsh安装一个最新的吧。 1/usr/local/bin/zsh --version zsh 5.0.5 (x86_64-apple-darwin13.3.0) 区别也不会很大， 默认的版本已经很新了。 安装后最好备份 : cp ~/.zshrc ~/.zshrc.orig 3. homebrew-cask install brew cask 1brew tap phinze/homebrew-cask cask常用命令： 12345brew cask search #列出所有可以被安装的软件brew cask search php #查找所有和php相关的应用brew cask list #列出所有通过cask安装的软件brew cask info phpstorm #查看 phpstorm 的信息brew cask uninstall qq #卸载 QQ brew cask install sublime-text 这里谈谈cask对比Mac App Store的优势： 对常用软件支持更全面（特别是开发者），cask里面会给你一些惊喜； 软件更新速度快，一般都是最新版本 Store上很久很久才会更新版本； 命令安装感觉比打开Store方便，另外Store在国内的速度也是XXOO。 homebrew-cask 你可以先不安装 4. iterm2 https://www.iterm2.com/ 5. SimpleHTTPServer A computer 1python -m http.server B computer 1wget http://192.168.xx.xx:8000/your-filename nc 瑞士军刀，也可以两台电脑传输文件","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"mac","slug":"mac","permalink":"http://www.iequa.com/tags/mac/"}]},{"title":"Recommendation System - CF","slug":"ml/recommendation-six-mouth-CF","date":"2017-02-28T02:28:21.000Z","updated":"2021-06-22T06:46:36.102Z","comments":true,"path":"2017/02/28/ml/recommendation-six-mouth-CF/","link":"","permalink":"http://www.iequa.com/2017/02/28/ml/recommendation-six-mouth-CF/","excerpt":"recommendation system and application","text":"recommendation system and application 推荐算法 基于内容的推荐 协同过滤 CF 矩阵分解 与 隐语义模型 1. 推荐系统 What? 1.1 数学定义 设 C 为全体用户集合 设 S 为全部商品/推荐内容集合 设 u 是评判把 s_is\\_is_i 推荐 c_ic\\_ic_i 的好坏评判函数 推荐是对于 c∈Cc∈Cc∈C ,找到 s∈Ss∈Ss∈S ,使得 uuu 最大 1.2 人话版本 根据用户的 : 历史行为 社交关系 兴趣点 上下文环境 … 去判断用户的当前需求 / 感兴趣的 Item 2. 推荐系统 Why? Infomation Overload 用户需求不明确 2.1 对用户 找到好玩的东西 发现新鲜事物 Surprise 2.2 对商家 个性化服务、提高粘性 增加营收 3. 评定标准 准确度 覆盖度 多样性 评估标准 3.1 准确度 Top N 设 R(u)R(u)R(u) 为根据训练建立的模型在测试集上的推荐, T(u)T(u)T(u) 为测试集上用户的选择。 Precision=∑_u∈U∣R(u)⋂T(u)∣∑_u∈UR(u)Precision = \\frac{\\sum\\_{u∈U} | R(u) \\bigcap T(u) | }{\\sum\\_{u∈U} R(u)} Precision=∑_u∈UR(u)∑_u∈U∣R(u)⋂T(u)∣​ Recall=∑_u∈U∣R(u)⋂T(u)∣∑_u∈UT(u)Recall = \\frac{\\sum\\_{u∈U} | R(u) \\bigcap T(u) | }{\\sum\\_{u∈U} T(u)} Recall=∑_u∈UT(u)∑_u∈U∣R(u)⋂T(u)∣​ Recall 说明 : 用户看的 80 篇新闻，你到底给我 推 出来多少篇 3.2 覆盖率 表示对物品长尾的发掘能力 (推荐系统希望消除马太效应) Coverage=∣Uu∈UR(u)∣∣I∣Coverage = \\frac{|U_{u∈U} R(u)|}{|I|} Coverage=∣I∣∣Uu∈U​R(u)∣​ 覆盖率是对平台所有物品所言，淘宝推荐等会关切这个指标。 非常独特的商品和新闻, 被看到的量, 是一条下滑曲线 希望个性化推荐，把小众的商品 也推荐展示出来 3.3 多样性 表示推荐列表中物品两两之间的不相似性. Diversity(R(u))=1−∑_i,j∈R(u),i≠j,s(i,j)12∣R(u)∣(∣R(u)−1∣)Diversity(R(u)) = 1 - \\frac {\\sum\\_{i, j \\in R(u), {i \\neq j}, {s(i, j)} }} {\\frac {1}{2} |R(u)|(|R(u) - 1|)} Diversity(R(u))=1−21​∣R(u)∣(∣R(u)−1∣)∑_i,j∈R(u),i​=j,s(i,j)​ Diversity=1∣U∣∑_u∈UDiversity(R(u))Diversity = \\frac {1} {|U|} {\\sum\\_{u \\in U} {Diversity(R(u))}} Diversity=∣U∣1​∑_u∈UDiversity(R(u)) 全是牛仔裤的话，用户审美疲劳, tag(纯棉 0，圆领 1) 连衣裙, 计算 vector 距离。品类不同，相似度设为 0. 3.4 评估标准 新颖度 : 给用户 Surprise 惊喜度 : 推荐和用户历史兴趣不相似，却满意的 信任度 : 提供可靠的推荐理由 实时性 : 实时更新程度 (context, session …) … 4. 相似度/距离定义 欧氏距离 Jaccard 相似度 余弦相似度 Pearson 相似度 4.1 欧氏距离 dist(X,Y)=(∑_i=1n∣x_i−y_i∣p)1pdist(X, Y) = \\left( \\sum\\_{i=1}^n {| x\\_i - y\\_i|}^p \\right)^{\\frac {1}{p}} dist(X,Y)=(∑_i=1n∣x_i−y_i∣p)p1​ 4.2 Jaccard 相似度 J(A,B)=∣A⋂B∣∣A⋃B∣J(A, B) = \\frac {|A \\bigcap B|}{|A \\bigcup B|} J(A,B)=∣A⋃B∣∣A⋂B∣​ 适用于 top N 推荐，要么 看，要么 没看 4.3 余弦相似度 cos⁡(θ)=aTb∣a∣⋅∣b∣\\cos(\\theta) = \\frac {a^Tb}{|a| \\cdot |b|} cos(θ)=∣a∣⋅∣b∣aTb​ 4.4 Pearson 相似度 ∑_i=1n(X_i−μ_x)(Y_i−μ_y)∑_i=1n(X_i−μ_x)2∑_i=1n(Y_i−μ_y)2\\frac { \\sum\\_{i=1}^n (X\\_i - {\\mu}\\_x) (Y\\_i - {\\mu}\\_y)} {\\sqrt{ \\sum\\_{i=1}^n (X\\_i - {\\mu}\\_x)^2} \\sqrt{ \\sum\\_{i=1}^n (Y\\_i - {\\mu}\\_y)^2}} ∑_i=1n(X_i−μ_x)2​∑_i=1n(Y_i−μ_y)2​∑_i=1n(X_i−μ_x)(Y_i−μ_y)​ (5, 6, 9) (1, 2, 6) 5. 推荐算法 基于内容的推荐 协同过滤CF 隐语义模型 5.1 基于内容推荐 基于用户喜欢的 Item 的属性 / 内容进行挖掘 基于分析内容，无需考虑当前user与其他user的行为的交互关联等 通常使用在 文本相关 的产品上进行推荐 Item 通过内容 (比如 关键词) 关联 在推荐电影中，也可以使用，但是效果不见得好. 你需要手动对 Item 进行离线挖掘，拿出 tag 电影题材 : 爱情 / 探险 / 动作 / 喜剧 标志特征 : 黄晓明 / 王宝强 年代 : 1995, 2016 … 关键词 两个Item根据 tag 的分值，进行求 距离. 举例文本挖掘 : One : 对于每个要推荐的内容,我们需要建立一份资料 : 比如词 k_ik\\_ik_i 在文档 d_jd\\_jd_j 中的权重 w_ijw\\_{ij}w_ij (常用的方法比如 TF-IDF) 有一个词表 Item [w_1w\\_1w_1, w_2w\\_2w_2, … ,w_4000w\\_{4000}w_4000], 对每个 document 建立一个词表 vector。 Two : 需要对用户也建立一份资料 : 比如说定义一个权重向量 (w_c1,...,w_ckw\\_{c1},...,w\\_{ck}w_c1,...,w_ck) , 其中 w_ciw\\_{ci}w_ci 表示第 k_ik\\_ik_i 个词对用户 ccc 的重要度 user 之前有看过的 小说 或 文档。(看过的文档放在一起搞一个doc_vector，或者 将 doc_vector 加权平均) Three : 计算匹配度 余弦距离公式 u(c,s)=cos⁡(w_c⃗,w_s⃗)=wc⃗⋅w_s⃗∣∣w_c∣∣⃗×∣∣w_s∣∣⃗u(c, s) = \\cos(\\vec{w\\_c}, \\vec{w\\_s}) = \\frac{\\vec{w_c} \\cdot \\vec{w\\_s}}{\\vec{||w\\_c||} \\times \\vec{||w\\_s||}} u(c,s)=cos(w_c​,w_s​)=∣∣w_c∣∣​×∣∣w_s∣∣​wc​​⋅w_s​​ 总结三步 : 对 每个 document 建立 vector 对 每个 user 建立 vector 比对 user 向量，与 该user 没有看过的 document 向量 之间的相似度 TF-IDF : 评估一个 word 对当前 document 的重要性。 当前升高而升高，所有doc中，升高而下降。 Sample Example : 基于书名进行书推荐 一个用户对《Building data mining applications for CRM》感兴趣，从以下书中进行推荐 : Building data mining applications for CRM Accelerating Customer Relationships: Using CRM and Relationship Technologies Mastering Data Mining: The Art and Science of Customer Relationship Management Data Mining Your Website Introduction to marketing Consumer behavior marketing research, a handbook Customer knowledge manag TF-IDF Normed Vectors 值 (0.187、0.316) 代表 Data 对于当前 标题 的重要程度. 根据 TF-IDF，当前书名 与 图书馆所有书名 出现的次数, (去掉停用词, 如 and) 计算出来的权重。 所以 《Building data mining applications for CRM》 已经有一个列 vector， 与 其他所有 书 计算 相似度，相似度 高的，为推荐最好的。 This is only a sample show example， not the real method of thing. 计算这本书和其余7本书的相似度，推荐最近的，这里结果为 : rank 1 : Data Mining Your Website 􏰅rank 2 : Accelerating Custom Relationships: Using CRM … rank 3 : Mastering Data Mining: The Art and Science… 􏰬􏰭􏰮􏰀􏰁… 结论 : 基于内容的推荐，无需用户行为的交互关联。 5.2 协同过滤 (User-based) 找到和 User 最近的 其他 User􏰼􏰽􏰟􏰲􏰳􏰧􏰨􏰒􏰠􏰾􏰲􏰳 􏱅􏰲􏰳􏱆􏱀􏱁􏱂􏰕, 找到他们 看/买过但当前 User 没看/买过的 item，根据距离加权打分 找得分最高的推荐 不需要提前挖掘 Item， 找品味接近的 5 个 friends。 近邻怎么找 ? U a b c d e A 5 1 5 3 2 B 4 3 1 1 5.3 协同过滤 (Item-based) 据现有 User 对 Item 的评级情况，来计算 Item 之间的某种相似度。已有Item 相似的 Item 被用来生成一个综合得分，而该得分用于评估未知物品的相似度。 根据 User 对 Item 的行为，计算 item 和 item 相似度，找到和当前 item 最近的进行推荐。 Collaborative filtering 基于 近邻 的思想 CF (Item-based) Summary : 一个 User List (u_iu\\_iu_i, i = 1, …, n), 一个 Item List (p_jp\\_jp_j, j = 1, …, m) 一个 n∗mn * mn∗m 矩阵 W, 每个元素 W_ijW\\_{ij}W_ij, 表示 User_i{User}\\_iUser_i 对 Itemj{Item}_jItemj​ 的打分 计算 item 和 item 之间的相似度 S(i,j)=cos⁡(i⃗,j⃗)=i⃗⋅j⃗∣∣i∣∣⃗×∣∣j∣∣⃗S(i, j) = \\cos(\\vec{i}, \\vec{j}) = \\frac{\\vec{i} \\cdot \\vec{j}}{\\vec{||i||} \\times \\vec{||j||}} S(i,j)=cos(i,j​)=∣∣i∣∣​×∣∣j∣∣​i⋅j​​ 选取 Top K 推荐 或 加权预测得分 r_xi=∑_j∈N(i;x)S_ij×r_xj∑_j∈N(i;x)S_ijr\\_{xi} = \\frac { {\\sum\\_{j \\in N (i;x)} S\\_{ij} \\times r\\_{xj}}} { {\\sum\\_{j \\in N (i;x)} S\\_{ij}}} r_xi=∑_j∈N(i;x)S_ij∑_j∈N(i;x)S_ij×r_xj​ Item-based Collaborative Filtering 1 与 4 之间的相似度-0.10不对， 应该是 -0.83 等，数据有误。(按照 Pearson overlap 计算). r_1.5r\\_{1.5}r_1.5 = (0.41*2 + 0.59*3) / (0.41 + 0.59) = 2.6 基于 Top N 来计算，这里是 Top 2. User CF vs Item CF 属性 UserCF ItemCF 性能 User多，计算User相似度矩阵代价很大 Item多，计算Item相似度矩阵代价很大 领域 User 个性化兴趣不太明显的领域 长尾物品丰富，用户个性化需求强烈的领域 实时性 用户有新行为，不一定推荐结果马上变化 用户有新行为，会导致推荐结果实时变化 冷启动 新用户行为，不能立即进行个性化推荐(用户相似度是每隔一段时间离线计算的) 新用户对物品有行为，就可以给他推荐和该物品相关物品 推荐理由 很难提供令用户信服的推荐解释 利用用户的历史行为给用户做推荐解释 CF 优缺点 优点 缺点 基于用户行为，因此对推荐内容无需先验知识 需要大量的显性 / 隐性用户行为 (冷启动) 只需要用户和商品关联矩阵即可，结构简单 需要完全相同的商品关联，相似的不同 在用户行为丰富的情况下，效果好 (a, c) (b, c) 通过传递关系，可以近似的计算出来 (a, b) 相似度，矩阵稀疏的二度关联。 6. 冷启动问题 6.1 New User 推荐 非常 热门 的商品，收集一些信息 用户注册的时候，收集信息，或者互动游戏，确定你喜欢否 6.2 New Item 根据本身属性，求与原来 Item 相似度 Item-based CF 可推荐出去","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"RecommendationSystem","slug":"RecommendationSystem","permalink":"http://www.iequa.com/tags/RecommendationSystem/"}]},{"title":"Spark Machine Learning p4 - Build Recommendation System","slug":"spark/spark-machine-learning-p4-rs","date":"2016-11-23T07:28:21.000Z","updated":"2021-06-20T04:12:28.358Z","comments":true,"path":"2016/11/23/spark/spark-machine-learning-p4-rs/","link":"","permalink":"http://www.iequa.com/2016/11/23/spark/spark-machine-learning-p4-rs/","excerpt":"Spark build Recommendation System, 推荐引擎试图对用户与某类物品之间的联系建模","text":"Spark build Recommendation System, 推荐引擎试图对用户与某类物品之间的联系建模 推荐引擎的类型； 用用户偏好数据来建立一个推荐模型； 为用户进行推荐和求指定物品的类似物品； 评估该模型的预测能力。 1. 推荐模型分类 内容过滤 协同过滤 矩阵分解 1.1 内容过滤 - (类似物品) 利用物品相似度定义，来求出与该物品类似的物品。 对用户的推荐可以根据用户的属性或描述得出，之后再通过相同的相似度定义来与物品属性做匹配。 1.2 协同过滤 - (估计未触) 协同过滤是一种利用大量已有的用户偏好来估计用户对其未接触过的物品的喜好程度。其内在思想是相似度的定义。 基于用户 如果两个用户表现出相似的偏好，认为他们的兴趣类似。要对他们中的一个用户推荐一个未知物品，便可选取若干与其类似的用户并根据他们的喜好计算出对各个物品的综合得分。 基于物品 据现有用户对物品的偏好或是评级情况，来计算物品之间的某种相似度。已有物品相似的物品被用来生成一个综合得分，而该得分用于评估未知物品的相似度。 基于用户或物品的方法得分取决于若干用户或是物品之间依据相似度所构成的集合（即邻居），故它们也常被称为KNN。 对“用户-物品”偏好建模 1.3 矩阵分解 Spark推荐模型库 包含基于矩阵分解（matrix factorization）的实现，该模型在协同过滤中的表现十分出色。 1.3.1 显式矩阵分解 显式自身偏好数据 这类数据包括如物品评级、赞、喜欢等用户对物品的评价。转换为以用户为行、物品为列的二维矩阵。 大部分情况下单个用户只会和少部分物品接触，所以该矩阵很稀疏。 12345Tom, Star Wars, 5Jane, Titanic, 4Bill, Batman, 3Jane, Star Wars, 2Bill, Titanic, 3 用户-物品 矩阵的维度为 U × I 为了降维 表示用户的 U × k 维矩阵 表征物品的 I × k 维矩阵 这两个矩阵也称作因子矩阵, 乘积是原始评级矩阵的一个近似. 原始评级矩阵通常很稀疏，但因子矩阵却是稠密的 因子可能表示了某些含义，比如对电影的某个导演、种类、风格或某些演员的偏好。 要计算给定用户对某个物品的预计评级 = 行（用户因子向量） 与 列（物品因子向量），两者点积 物品之间相似度的计算，转换为对两物品因子向量之间相似度的计算 优点 缺点 因子分解类模型建立，求解容易 物品或是用户的因子向量可能达到数以百万计。在存储和计算能力有挑战。 1.3.2. 隐式矩阵分解 隐含在用户与物品的交互之中。二元数据（比如用户是否观看了某个电影或是否购买了某个商品）和计数数据（比如用户观看某电影的次数）便是这类数据。 MLlib 处理隐式数据： 一个二元偏好矩阵 P 一个信心权重矩阵 C 隐式模型仍然会创建一个用户因子矩阵和一个物品因子矩阵。但是，模型所求解的是偏好矩阵而非评级矩阵的近似。 3. 最小二乘法 最小二乘法（Alternating Least Squares，ALS）是一种求解矩阵分解问题的最优化方法。且相对容易并行化。 ALS的实现原理是迭代式求解一系列最小二乘回归问题。在每一次迭代时，固定用户因子矩阵或是物品因子矩阵中的一个，然后用固定的这个矩阵以及评级数据来更新另一个矩阵。之后，被更新的矩阵被固定住，再更新另外一个矩阵。如此迭代，直到模型收敛（或是迭代了预设好的次数）。 2. 提取有效特征 123456789&gt;./bin/spark-shell –-driver-memory 2gval rawData = sc.textFile(&quot;/Users/hp/ghome/github/Recommendation/spark-ml/ml-100k/u.data&quot;)rawData.first()val rawRatings = rawData.map(_.split(&quot;\\t&quot;).take(3))import org.apache.spark.mllib.recommendation.ALSALS.asInstanceOf isInstanceOf main toString train trainImplicitALS.train ALS模型需要一个由Rating记录构成的RDD，而Rating类则是对用户ID、影片ID（这里是通称product）和实际星级这些参数的封装。 我们可以调用map方法将原来的各ID和星级的数组转换为对应的Rating对象，从而创建所需的评级数据集。 123456789101112scala&gt; import org.apache.spark.mllib.recommendation.Ratingimport org.apache.spark.mllib.recommendation.Ratingscala&gt; val ratings = rawRatings.map &#123; case Array(user, movie, rating) =&gt; | Rating(user.toInt, movie.toInt, rating.toDouble) &#125;ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[3] at map at &lt;console&gt;:27scala&gt; ratings.first()res3: org.apache.spark.mllib.recommendation.Rating = Rating(196,242,3.0)scala&gt; ratings.take(10)res4: Array[org.apache.spark.mllib.recommendation.Rating] = Array(Rating(196,242,3.0), Rating(186,302,3.0), Rating(22,377,1.0), Rating(244,51,2.0), Rating(166,346,1.0), Rating(298,474,4.0), Rating(115,265,2.0), Rating(253,465,5.0), Rating(305,451,3.0), Rating(6,86,3.0)) 3. 训练推荐模型 从原始数据提取出这些简单特征后，便可训练模型。MLlib已实现模型训练的细节，这不需要我们担心。我们只需提供上述指定类型的新RDD以及其他所需参数来作为训练的输入即可。 3.1 Movie-100k train model 现在开始训练模型了，所需的其他参数有以下几个 rank：对应ALS模型中的因子个数，也就是在低阶近似矩阵中的隐含特征个数。因子个数一般越多越好。但它也会直接影响模型训练和保存时所需的内存开销，尤其是在用户和物品很多的时候。因此实践中该参数常作为训练效果与系统开销之间的调节参数。通常，其合理取值为10到200。 iterations：对应运行时的迭代次数。ALS能确保每次迭代都能降低评级矩阵的重建误差，但一般经少数次迭代后ALS模型便已能收敛为一个比较合理的好模型。这样，大部分情况下都没必要迭代太多次（10次左右一般就挺好）。 lambda：该参数控制模型的正则化过程，从而控制模型的过拟合情况。其值越高，正则化越严厉。该参数的赋值与实际数据的大小、特征和稀疏程度有关。和其他的机器学习模型一样，正则参数应该通过用非样本的测试数据进行交叉验证来调整。 12345678scala&gt; val model = ALS.train(ratings, 50, 10, 0.01)model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@2e835760scala&gt; model.userFeaturesres5: org.apache.spark.rdd.RDD[(Int, Array[Double])] = users MapPartitionsRDD[209] at mapValues at ALS.scala:255scala&gt; model.userFeatures.countres6: Long = 943 MatrixFactorizationModel 对象将 用户因子和物品因子分别保存在一个 (id,factor) 对类型的RDD中。 它们分别称作userFeatures 和 productFeatures。 3.2 隐式反馈数据训练模型 MLlib中标准的矩阵分解模型用于显式评级数据的处理。若要处理隐式数据，则可使用trainImplicit函数。其调用方式和标准的train模式类似，但多了一个可设置的alpha参数（也是一个正则化参数，lambda应通过测试和交叉验证法来设置）。 alpha参数指定了信心权重所应达到的基准线。该值越高则所训练出的模型越认为用户与他所没评级过的电影之间没有相关性。 4. 使用推荐模型 预测通常有两种：为某个用户推荐物品，或找出与某个物品相关或相似的其他物品。 4.1 用户推荐 通过模型求出用户可能喜好程度最高的前K个商品。 基于用户的模型，则会利用相似用户的评级来计算对某个用户的推荐。 基于物品的模型，则会依靠用户接触过的物品与候选物品之间的相似度来获得推荐。 利用矩阵分解方法时，是直接对评级数据进行建模，所以预计得分可视作相应用户因子向量和物品因子向量的点积。 1. 从MovieLens 100k数据集生成电影推荐 MLlib的推荐模型基于矩阵分解，因此可用模型所求得的因子矩阵来计算用户对物品的预计评级。下面只针对利用MovieLens中显式数据做推荐的情形，使用隐式模型时的方法与之相同。 MatrixFactorizationModel类 提供了一个predict函数，以方便地计算给定用户对给定物品的预期得分： 12345678910111213141516171819202122232425262728scala&gt; val predictedRating = model.predict(789, 123)16/05/04 16:13:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS16/05/04 16:13:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLASpredictedRating: Double = 1.8390368814083764scala&gt; val predictedRating = model.predict(789, 123)predictedRating: Double = 1.8390368814083764scala&gt; val userId = 789userId: Int = 789scala&gt; val K = 10K: Int = 10scala&gt; val topKRecs = model.recommendProducts(userId, K)topKRecs: Array[org.apache.spark.mllib.recommendation.Rating] = Array(Rating(789,180,5.352418839062572), Rating(789,887,5.289455638310055), Rating(789,484,5.0301818688410025), Rating(789,475,5.011219778604191), Rating(789,150,5.003965038415291), Rating(789,663,4.991126084946501), Rating(789,56,4.974685008959871), Rating(789,48,4.965402351329832), Rating(789,9,4.963265626841469), Rating(789,127,4.963069165947614))scala&gt; println(topKRecs.mkString(&quot;\\n&quot;))Rating(789,180,5.352418839062572)Rating(789,887,5.289455638310055)Rating(789,484,5.0301818688410025)Rating(789,475,5.011219778604191)Rating(789,150,5.003965038415291)Rating(789,663,4.991126084946501)Rating(789,56,4.974685008959871)Rating(789,48,4.965402351329832)Rating(789,9,4.963265626841469)Rating(789,127,4.963069165947614)","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"},{"name":"recommendation-system","slug":"recommendation-system","permalink":"http://www.iequa.com/tags/recommendation-system/"}]},{"title":"Coursera Week 3 - Logistic Regression","slug":"ml/coursera-ng-w3-LR","date":"2016-10-24T02:28:21.000Z","updated":"2021-06-20T04:12:28.315Z","comments":true,"path":"2016/10/24/ml/coursera-ng-w3-LR/","link":"","permalink":"http://www.iequa.com/2016/10/24/ml/coursera-ng-w3-LR/","excerpt":"Logistic Regression","text":"Logistic Regression 1. Classification Email: Spam / Not Spam Online Transactions: Fraudulent (Yes/No)? Tumor: Malignant / Benign ? ['tju:mə®] [mə’lɪgnənt] / [bɪ’naɪn] $ y \\in {0, 1}$ 2. Binary Classification Instead of our output vector y being a continuous range of values, it will only be 0 or 1. y∈{0,1} Where 0 is usually taken as the “negative class” and 1 as the “positive class”, but you are free to assign any representation to it. Hypothesis Representation 0≤h_θ(x)≤10 \\leq h\\_\\theta (x) \\leq 1 0≤h_θ(x)≤1 \\begin{align} &amp; h\\_\\theta (x) = g ( \\theta^T x ) \\newline \\newline&amp; z = \\theta^T x \\newline&amp; g(z) = \\dfrac{1}{1 + e^{-z}} \\end{align} “Sigmoid Function,” also called the “Logistic Function”: h_θh\\_θh_θ will give us the probability that our output is 1. For example, h_θ=0.7h\\_θ=0.7h_θ=0.7 gives us the probability of 70% that our output is 1. \\begin{align} &amp; h\\_\\theta(x) = P(y=1 | x ; \\theta) = 1 - P(y=0 | x ; \\theta) \\newline&amp; P(y = 0 | x;\\theta) + P(y = 1 | x ; \\theta) = 1 \\end{align} Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%). 3. Decision Boundary \\begin{align} &amp; h\\_\\theta(x) \\geq 0.5 \\rightarrow y = 1 \\newline&amp; h\\_\\theta(x) &lt; 0.5 \\rightarrow y = 0 \\newline\\end{align} Logistic Function $ \\begin{align} &amp; g(z) \\geq 0.5 \\newline&amp; when ; z \\geq 0 \\end{align} $ $ \\begin{align} z=0, e^{0}=1 \\Rightarrow g(z)=1/2\\newline z \\to \\infty, e^{-\\infty} \\to 0 \\Rightarrow g(z)=1 \\newline z \\to -\\infty, e^{\\infty}\\to \\infty \\Rightarrow g(z)=0 \\end{align} $ g is θTX\\theta^T XθTX, then that means: $ h_\\theta(x) = g(\\theta^T x) \\geq 0.5 $ $ when ; \\theta^T x \\geq 0 $ $ \\begin{align} &amp; \\theta^T x \\geq 0 \\Rightarrow y = 1 \\newline&amp; \\theta^T x &lt; 0 \\Rightarrow y = 0 \\newline \\end{align} $ The decision boundary is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function. Example: \\begin{align} &amp; \\theta = \\begin{bmatrix}5 \\newline -1 \\newline 0\\end{bmatrix} \\newline &amp; y = 1 \\; if \\; 5 + (-1) x_1 + 0 x_2 \\geq 0 \\newline &amp; 5 - x_1 \\geq 0 \\newline &amp; - x_1 \\geq -5 \\newline&amp; x_1 \\leq 5 \\newline \\end{align} In this case, our decision boundary is a straight vertical line placed on the graph where x_1=5x\\_1=5x_1=5, and everything to the left of that denotes y=1y = 1y=1, while everything to the right denotes y=0y = 0y=0. linear decision boundaries Non-linear decision boundaries 4. Cost Function We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. Instead, our cost function for logistic regression looks like: \\begin{align} &amp; J(\\theta) = \\dfrac{1}{m} \\sum\\_{i=1}^m \\mathrm{Cost}(h\\_\\theta(x^{(i)}),y^{(i)}) \\newline &amp; \\mathrm{Cost}(h\\_\\theta(x),y) = -\\log(h\\_\\theta(x)) \\; &amp; \\text{if y = 1} \\newline &amp; \\mathrm{Cost}(h\\_\\theta(x),y) = -\\log(1-h\\_\\theta(x)) \\; &amp; \\text{if y = 0} \\end{align} \\begin{align} &amp; \\mathrm{Cost}(h\\_\\theta(x),y) = 0 \\text{ if } h\\_\\theta(x) = y \\newline &amp; \\mathrm{Cost}(h\\_\\theta(x),y) \\rightarrow \\infty \\text{ if } y = 0 \\; \\mathrm{and} \\; h\\_\\theta(x) \\rightarrow 1 \\newline &amp; \\mathrm{Cost}(h\\_\\theta(x),y) \\rightarrow \\infty \\text{ if } y = 1 \\; \\mathrm{and} \\; h\\_\\theta(x) \\rightarrow 0 \\newline \\end{align} If our correct answer ‘y’ is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity. If our correct answer ‘y’ is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity. 5. Cost Function &amp; Gradient Desc We can compress our cost function's two conditional cases into one case: $ \\mathrm{Cost}(h_\\theta(x),y) = - y \\cdot \\log(h_\\theta(x)) - (1 - y) \\cdot \\log(1 - h_\\theta(x))$ We can fully write out our entire cost function as follows: J(θ)=−1m∑_i=1m[y(i)log⁡(h_θ(x(i)))+(1−y(i))log⁡(1−h_θ(x(i)))] J(\\theta) = - \\frac{1}{m} \\displaystyle \\sum\\_{i=1}^m [y^{(i)}\\log (h\\_\\theta (x^{(i)})) + (1 - y^{(i)})\\log (1 - h\\_\\theta(x^{(i)}))] J(θ)=−m1​∑_i=1m[y(i)log(h_θ(x(i)))+(1−y(i))log(1−h_θ(x(i)))] A vectorized implementation is: \\begin{align} &amp; h = g(X\\theta)\\newline &amp; J(\\theta) = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\end{align} 5.1 Gradient Descent Remember that the general form of gradient descent is: \\begin{align} &amp; Repeat \\; \\lbrace \\newline &amp; \\; \\theta\\_j := \\theta\\_j - \\alpha \\dfrac{\\partial}{\\partial \\theta\\_j}J(\\theta) \\newline &amp; \\rbrace \\end{align} We can work out the derivative part using calculus to get: \\begin{align} &amp; Repeat \\; \\lbrace \\newline &amp; \\; \\theta\\_j := \\theta\\_j - \\frac{\\alpha}{m} \\sum\\_{i=1}^m (h\\_\\theta(x^{(i)}) - y^{(i)}) x\\_j^{(i)} \\newline &amp; \\rbrace \\end{align} seen 5.2 detailed A vectorized implementation is: θ:=θ−αmXT(g(Xθ)−y⃗) \\theta := \\theta - \\frac{\\alpha}{m} X^{T} (g(X \\theta ) - \\vec{y}) θ:=θ−mα​XT(g(Xθ)−y​) 5.2 Partial derivative of J(θ) partial ['pɑːʃ(ə)l] derivative [dɪ’rɪvətɪv] \\begin{align} \\sigma(x)&#039;&amp;=\\left(\\frac{1}{1+e^{-x}}\\right)&#039;=\\frac{-(1+e^{-x})&#039;}{(1+e^{-x})^2}=\\frac{-1&#039;-(e^{-x})&#039;}{(1+e^{-x})^2}=\\frac{0-(-x)&#039;(e^{-x})}{(1+e^{-x})^2}=\\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\\frac{e^{-x}}{(1+e^{-x})^2} \\newline &amp;=\\left(\\frac{1}{1+e^{-x}}\\right)\\left(\\frac{e^{-x}}{1+e^{-x}}\\right)=\\sigma(x)\\left(\\frac{+1-1 + e^{-x}}{1+e^{-x}}\\right)=\\sigma(x)\\left(\\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\right)=\\sigma(x)(1 - \\sigma(x)) \\end{align} Now we are ready to find out resulting partial derivative: \\begin{align} \\frac{\\partial}{\\partial \\theta\\_j} J(\\theta) &amp;= \\frac{\\partial}{\\partial \\theta\\_j} \\frac{-1}{m}\\sum\\_{i=1}^m \\left [ y^{(i)} log (h\\_\\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h\\_\\theta(x^{(i)})) \\right ] \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ y^{(i)} \\frac{\\partial}{\\partial \\theta\\_j} log (h\\_\\theta(x^{(i)})) + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta\\_j} log (1 - h\\_\\theta(x^{(i)}))\\right ] \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta\\_j} h\\_\\theta(x^{(i)})}{h\\_\\theta(x^{(i)})} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta\\_j} (1 - h\\_\\theta(x^{(i)}))}{1 - h\\_\\theta(x^{(i)})}\\right ] \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta\\_j} \\sigma(\\theta^T x^{(i)})}{h\\_\\theta(x^{(i)})} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta\\_j} (1 - \\sigma(\\theta^T x^{(i)}))}{1 - h\\_\\theta(x^{(i)})}\\right ] \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ \\frac{y^{(i)} \\sigma(\\theta^T x^{(i)}) (1 - \\sigma(\\theta^T x^{(i)})) \\frac{\\partial}{\\partial \\theta\\_j} \\theta^T x^{(i)}}{h\\_\\theta(x^{(i)})} + \\frac{- (1-y^{(i)}) \\sigma(\\theta^T x^{(i)}) (1 - \\sigma(\\theta^T x^{(i)})) \\frac{\\partial}{\\partial \\theta\\_j} \\theta^T x^{(i)}}{1 - h\\_\\theta(x^{(i)})}\\right ] \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ \\frac{y^{(i)} h\\_\\theta(x^{(i)}) (1 - h\\_\\theta(x^{(i)})) \\frac{\\partial}{\\partial \\theta\\_j} \\theta^T x^{(i)}}{h\\_\\theta(x^{(i)})} - \\frac{(1-y^{(i)}) h\\_\\theta(x^{(i)}) (1 - h\\_\\theta(x^{(i)})) \\frac{\\partial}{\\partial \\theta\\_j} \\theta^T x^{(i)}}{1 - h\\_\\theta(x^{(i)})}\\right ] \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ y^{(i)} (1 - h\\_\\theta(x^{(i)})) x^{(i)}\\_j - (1-y^{(i)}) h\\_\\theta(x^{(i)}) x^{(i)}\\_j\\right ] \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ y^{(i)} (1 - h\\_\\theta(x^{(i)})) - (1-y^{(i)}) h\\_\\theta(x^{(i)}) \\right ] x^{(i)}\\_j \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ y^{(i)} - y^{(i)} h\\_\\theta(x^{(i)}) - h\\_\\theta(x^{(i)}) + y^{(i)} h\\_\\theta(x^{(i)}) \\right ] x^{(i)}\\_j \\newline&amp;= - \\frac{1}{m}\\sum\\_{i=1}^m \\left [ y^{(i)} - h\\_\\theta(x^{(i)}) \\right ] x^{(i)}\\_j \\newline&amp;= \\frac{1}{m}\\sum\\_{i=1}^m \\left [ h\\_\\theta(x^{(i)}) - y^{(i)} \\right ] x^{(i)}\\_j \\end{align} The vectorized version; ∇J(θ)=1m⋅XT⋅(g(X⋅θ)−y⃗) \\nabla J(\\theta) = \\frac{1}{m} \\cdot X^T \\cdot \\left(g\\left(X\\cdot\\theta\\right) - \\vec{y}\\right) ∇J(θ)=m1​⋅XT⋅(g(X⋅θ)−y​) 6. Advanced Optimization We can apply regularization to both linear regression and logistic regression. We will approach linear regression first. We first need to provide a function that evaluates the following two functions for a given input value θ: \\begin{align} &amp; J(\\theta) \\newline &amp; \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\end{align} We can write a single function that returns both of these… 7. Multiclass Classification \\begin{align} &amp; y \\in \\lbrace0, 1 ... n\\rbrace \\newline&amp; h\\_\\theta^{(0)}(x) = P(y = 0 | x ; \\theta) \\newline&amp; h\\_\\theta^{(1)}(x) = P(y = 1 | x ; \\theta) \\newline&amp; \\cdots \\newline&amp; h\\_\\theta^{(n)}(x) = P(y = n | x ; \\theta) \\newline&amp; \\mathrm{prediction} = \\max\\_i( h\\_\\theta ^{(i)}(x) )\\newline \\end{align} Train a logistic regression classifier h_θ(i)(x)h\\_\\theta ^{(i)}(x)h_θ(i)(x) for each class iii to predict the probability that y=iy = iy=i . On a new input xxx, to make a prediction, pick the iii class that maximizes max⁡_i(h_θ(i)(x)\\max\\_i( h\\_\\theta ^{(i)}(x)max_i(h_θ(i)(x) @2017-02-10 review done. 8. Regularization The Problem of Overfitting Regularization is designed to address the problem of overfitting. High bias or underfitting is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. eg. if we take h_θ(x)=θ_0+θ_1⋅x_1+θ_2⋅x_2h\\_θ(x)=θ\\_0+θ\\_1 \\cdot x\\_1+θ\\_2 \\cdot x\\_2h_θ(x)=θ_0+θ_1⋅x_1+θ_2⋅x_2 then we are making an initial assumption that a linear model will fit the training data well and will be able to generalize but that may not be the case. At the other extreme, overfitting or high variance is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data. This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting: Reduce the number of features: a) Manually select which features to keep. b) Use a model selection algorithm . Regularization Keep all the features, but reduce the parameters θ_jθ\\_jθ_j. Regularization works well when we have a lot of slightly useful features. to address 提出，去解决 9. Regularization Cost Function If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost. Say we wanted to make the following function more quadratic [kwɒ’drætɪk]: θ_0+θ_1x+θ_2x2+θ_3x3+θ_4x4 \\theta\\_0 + \\theta\\_1x + \\theta\\_2x^2 + \\theta\\_3x^3 + \\theta\\_4x^4 θ_0+θ_1x+θ_2x2+θ_3x3+θ_4x4 We’ll want to eliminate the influence of θ_3x3\\theta\\_3x^3θ_3x3 and θ_4x4\\theta\\_4x^4θ_4x4. Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our cost function: min_θ 12m∑_i=1m(h_θ(x(i))−y(i))2+1000⋅θ_32+1000⋅θ_42 min\\_\\theta\\ \\dfrac{1}{2m}\\sum\\_{i=1}^m (h\\_\\theta(x^{(i)}) - y^{(i)})^2 + 1000 \\cdot \\theta\\_3^2 + 1000\\cdot\\theta\\_4^2 min_θ 2m1​∑_i=1m(h_θ(x(i))−y(i))2+1000⋅θ_32+1000⋅θ_42 We’ve added two extra terms at the end to inflate the cost of θ_3\\theta\\_3θ_3 and θ_4\\theta\\_4θ_4. Now, in order for the cost function to get close to zero, we will have to reduce the values of θ_3\\theta\\_3θ_3 and θ_4\\theta\\_4θ_4 to near zero. This will in turn greatly reduce the values of θ_3x3\\theta\\_3x^3θ_3x3 and θ_4x4\\theta\\_4x^4θ_4x4 in our hypothesis function. We could also regularize all of our theta parameters in a single summation: min_θ 12m [∑_i=1m(h_θ(x(i))−y(i))2+λ ∑j=1nθ_j2] min\\_\\theta\\ \\dfrac{1}{2m}\\ \\left[ \\sum\\_{i=1}^m (h\\_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\ \\sum_{j=1}^n \\theta\\_j^2 \\right] min_θ 2m1​ [∑_i=1m(h_θ(x(i))−y(i))2+λ ∑j=1n​θ_j2] The λ, or lambda, is the regularization parameter. It determines how much the costs of our theta parameters are inflated. You can visualize the effect of regularization in this interactive plot : https://www.desmos.com/calculator/1hexc8ntqp Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting. 我们可以平滑我们的假设函数的输出，以减少过度拟合 10. Regularized Linear 10.1 Gradient Descent \\begin{align} &amp; \\text{Repeat}\\ \\lbrace \\newline &amp; \\ \\ \\ \\ \\theta\\_0 := \\theta\\_0 - \\alpha\\ \\frac{1}{m}\\ \\sum\\_{i=1}^m (h\\_\\theta(x^{(i)}) - y^{(i)})x\\_0^{(i)} \\newline &amp; \\ \\ \\ \\ \\theta\\_j := \\theta\\_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum\\_{i=1}^m (h\\_\\theta(x^{(i)}) - y^{(i)})x\\_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta\\_j \\right] &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2...n\\rbrace\\newline &amp; \\rbrace \\end{align} The term λmθ_j\\frac{\\lambda}{m}\\theta\\_jmλ​θ_j performs our regularization. θ_j:=θ_j(1−αλm)−α1m∑_i=1m(h_θ(x(i))−y(i))x_j(i) \\theta\\_j := \\theta\\_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum\\_{i=1}^m(h\\_\\theta(x^{(i)}) - y^{(i)})x\\_j^{(i)} θ_j:=θ_j(1−αmλ​)−αm1​∑_i=1m(h_θ(x(i))−y(i))x_j(i) The first term in the above equation, 1−αλm1 - \\alpha\\frac{\\lambda}{m}1−αmλ​ will always be less than 1. Intuitively you can see it as reducing the value of θ_j\\theta\\_jθ_j by some amount on every update. 10.2 Normal Equation Use less, temporarily ignored 11. Regularized Logistic Cost Function J(θ)=−1m∑_i=1m[y(i) log⁡(h_θ(x(i)))+(1−y(i)) log⁡(1−h_θ(x(i)))] J(\\theta) = - \\frac{1}{m} \\sum\\_{i=1}^m \\large[ y^{(i)}\\ \\log (h\\_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h\\_\\theta(x^{(i)})) \\large] J(θ)=−m1​∑_i=1m[y(i) log(h_θ(x(i)))+(1−y(i)) log(1−h_θ(x(i)))] We can regularize this equation by adding a term to the end: J(θ)=−1m∑_i=1m[y(i) log⁡(h_θ(x(i)))+(1−y(i)) log⁡(1−h_θ(x(i)))]+λ2m∑_j=1nθ_j2 J(\\theta) = - \\frac{1}{m} \\sum\\_{i=1}^m \\large[ y^{(i)}\\ \\log (h\\_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h\\_\\theta(x^{(i)}))\\large] + \\frac{\\lambda}{2m}\\sum\\_{j=1}^n \\theta\\_j^2 J(θ)=−m1​∑_i=1m[y(i) log(h_θ(x(i)))+(1−y(i)) log(1−h_θ(x(i)))]+2mλ​∑_j=1nθ_j2 Gradient Descent Just like with linear regression, we will want to separately update θ_0\\theta\\_0θ_0 and the rest of the parameters because we do not want to regularize θ_0\\theta\\_0θ_0. \\begin{align} &amp; \\text{Repeat}\\ \\lbrace \\newline&amp; \\ \\ \\ \\ \\theta\\_0 := \\theta\\_0 - \\alpha\\ \\frac{1}{m}\\ \\sum\\_{i=1}^m (h\\_\\theta(x^{(i)}) - y^{(i)})x\\_0^{(i)} \\newline&amp; \\ \\ \\ \\ \\theta\\_j := \\theta\\_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum\\_{i=1}^m (h\\_\\theta(x^{(i)}) - y^{(i)})x\\_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta\\_j \\right] &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2...n\\rbrace\\newline&amp; \\rbrace\\end{align} Reference article Kaggle_Titanic","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Coursera Week 2 - Octave Tutorial By NG","slug":"ml/coursera-ng-w2-03","date":"2016-10-21T02:28:21.000Z","updated":"2021-06-20T04:12:28.312Z","comments":true,"path":"2016/10/21/ml/coursera-ng-w2-03/","link":"","permalink":"http://www.iequa.com/2016/10/21/ml/coursera-ng-w2-03/","excerpt":"ML:Octave Tutorial","text":"ML:Octave Tutorial 1. Basic Operations 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859%% Change Octave prompt PS1(&#x27;&gt;&gt; &#x27;);%% Change working directory in windows example:cd &#x27;c:/path/to/desired/directory name&#x27;%% Note that it uses normal slashes and does not use escape characters for the empty spaces.%% elementary operations5+63-25*81/22^61 == 2 % false1 ~= 2 % true. note, not &quot;!=&quot;1 &amp;&amp; 01 || 0xor(1,0)%% variable assignmenta = 3; % semicolon suppresses outputb = &#x27;hi&#x27;;c = 3&gt;=1;% Displaying them:a = pidisp(a)disp(sprintf(&#x27;2 decimals: %0.2f&#x27;, a))disp(sprintf(&#x27;6 decimals: %0.6f&#x27;, a))format longaformat shorta%% vectors and matricesA = [1 2; 3 4; 5 6]v = [1 2 3]v = [1; 2; 3]v = 1:0.1:2 % from 1 to 2, with stepsize of 0.1. Useful for plot axesv = 1:6 % from 1 to 6, assumes stepsize of 1 (row vector)C = 2*ones(2,3) % same as C = [2 2 2; 2 2 2]w = ones(1,3) % 1x3 vector of onesw = zeros(1,3)w = rand(1,3) % drawn from a uniform distribution w = randn(1,3)% drawn from a normal distribution (mean=0, var=1)w = -6 + sqrt(10)*(randn(1,10000)); % (mean = -6, var = 10) - note: add the semicolonhist(w) % plot histogram using 10 bins (default)hist(w,50) % plot histogram using 50 bins% note: if hist() crashes, try &quot;graphics_toolkit(&#x27;gnu_plot&#x27;)&quot; I = eye(4) % 4x4 identity matrix% help functionhelp eyehelp randhelp help 2. Moving Data Around 1234567891011121314151617181920212223242526272829303132333435363738%% dimensionssz = size(A) % 1x2 matrix: [(number of rows) (number of columns)]size(A,1) % number of rowssize(A,2) % number of colslength(v) % size of longest dimension%% loading datapwd % show current directory (current path)cd &#x27;C:\\Users\\ang\\Octave files&#x27; % change directory ls % list files in current directory load q1y.dat % alternatively, load(&#x27;q1y.dat&#x27;)load q1x.datwho % list variables in workspacewhos % list variables in workspace (detailed view) clear q1y % clear command without any args clears all varsv = q1x(1:10); % first 10 elements of q1x (counts down the columns)save hello.mat v; % save variable v into file hello.matsave hello.txt v -ascii; % save as ascii% fopen, fread, fprintf, fscanf also work [[not needed in class]]%% indexingA(3,2) % indexing is (row,col)A(2,:) % get the 2nd row. % &quot;:&quot; means every element along that dimensionA(:,2) % get the 2nd colA([1 3],:) % print all the elements of rows 1 and 3A(:,2) = [10; 11; 12] % change second columnA = [A, [100; 101; 102]]; % append column vecA(:) % Select all elements as a column vector.% Putting data together A = [1 2; 3 4; 5 6]B = [11 12; 13 14; 15 16] % same dims as AC = [A B] % concatenating A and B matrices side by sideC = [A, B] % concatenating A and B matrices side by sideC = [A; B] % Concatenating A and B top and bottom 3. Computing on Data 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253%% initialize variablesA = [1 2;3 4;5 6]B = [11 12;13 14;15 16]C = [1 1;2 2]v = [1;2;3]%% matrix operationsA * C % matrix multiplicationA .* B % element-wise multiplication% A .* C or A * B gives error - wrong dimensionsA .^ 2 % element-wise square of each element in A1./v % element-wise reciprocallog(v) % functions like this operate element-wise on vecs or matrices exp(v)abs(v)-v % -1*vv + ones(length(v), 1) % v + 1 % sameA&#x27; % matrix transpose%% misc useful functions% max (or min)a = [1 15 2 0.5]val = max(a)[val,ind] = max(a) % val - maximum element of the vector a and index - index value where maximum occurval = max(A) % if A is matrix, returns max from each column% compare values in a matrix &amp; finda &lt; 3 % checks which values in a are less than 3find(a &lt; 3) % gives location of elements less than 3A = magic(3) % generates a magic matrix - not much used in ML algorithms[r,c] = find(A&gt;=7) % row, column indices for values matching comparison% sum, prodsum(a)prod(a)floor(a) % or ceil(a)max(rand(3),rand(3))max(A,[],1) - maximum along columns(defaults to columns - max(A,[]))max(A,[],2) - maximum along rowsA = magic(9)sum(A,1)sum(A,2)sum(sum( A .* eye(9) ))sum(sum( A .* flipud(eye(9)) ))% Matrix inverse (pseudo-inverse)pinv(A) % inv(A&#x27;*A)*A&#x27; 4. Plotting Data 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt; t=[0:0.01:0.98];&gt;&gt; tt = Columns 1 through 10: 0.00000 0.01000 0.02000 0.03000 0.04000 0.05000 0.06000 0.07000 0.08000 0.09000 Columns 11 through 20: 0.10000 0.11000 0.12000 0.13000 0.14000 0.15000 0.16000 0.17000 0.18000 0.19000 Columns 21 through 30: 0.20000 0.21000 0.22000 0.23000 0.24000 0.25000 0.26000 0.27000 0.28000 0.29000 Columns 31 through 40: 0.30000 0.31000 0.32000 0.33000 0.34000 0.35000 0.36000 0.37000 0.38000 0.39000 Columns 41 through 50: 0.40000 0.41000 0.42000 0.43000 0.44000 0.45000 0.46000 0.47000 0.48000 0.49000 Columns 51 through 60: 0.50000 0.51000 0.52000 0.53000 0.54000 0.55000 0.56000 0.57000 0.58000 0.59000 Columns 61 through 70: 0.60000 0.61000 0.62000 0.63000 0.64000 0.65000 0.66000 0.67000 0.68000 0.69000 Columns 71 through 80: 0.70000 0.71000 0.72000 0.73000 0.74000 0.75000 0.76000 0.77000 0.78000 0.79000 Columns 81 through 90: 0.80000 0.81000 0.82000 0.83000 0.84000 0.85000 0.86000 0.87000 0.88000 0.89000 Columns 91 through 99: 0.90000 0.91000 0.92000 0.93000 0.94000 0.95000 0.96000 0.97000 0.98000&gt;&gt; y1=sin(2*pi*4*t);&gt;&gt; plot(t,y1)&gt;&gt; y2=cos(2*pi*4*t);&gt;&gt; plot(t,y2)&gt;&gt; hold on&gt;&gt; plot(t,y1)&gt;&gt; plot(t,y2,&#x27;r&#x27;)&gt;&gt; xlabel(&#x27;time&#x27;)&gt;&gt; ylabel(&#x27;value&#x27;)&gt;&gt; legend(&#x27;sin&#x27;,&#x27;cos&#x27;)&gt;&gt; title(&#x27;my plot&#x27;)&gt;&gt; print -dpng &#x27;myPlot.png&#x27;warning: print.m: fig2dev binary is not available.Some output formats are not available. 123456&gt;&gt; figure(2); plot(t, y2)&gt;&gt; subplot(1,2,1);&gt;&gt; plot(t,y1)&gt;&gt; subplot(1,2,2)&gt;&gt; plot(t,y2)&gt;&gt; axis([0.5 1 -1 1]) 123456789101112&gt;&gt; clf;&gt;&gt; A = magic(5)A = 17 24 1 8 15 23 5 7 14 16 4 6 13 20 22 10 12 19 21 3 11 18 25 2 9&gt;&gt; imagesc(A)&gt;&gt; imagesc(A), colorbar, colormap gray; 12345&gt;&gt; imagesc(magic(15)), colorbar, colormap gray;&gt;&gt; a=1,b=2,c=3a = 1b = 2c = 3 5. Control statements:for,while,if 12345678910111213141516171819202122232425262728v = zeros(10,1);for i=1:10, v(i) = 2^i;end;% Can also use &quot;break&quot; and &quot;continue&quot; inside for and while loops to control execution.i = 1;while i &lt;= 5, v(i) = 100; i = i+1;endi = 1;while true, v(i) = 999; i = i+1; if i == 6, break; end;endif v(1)==1, disp(&#x27;The value is one!&#x27;);elseif v(1)==2, disp(&#x27;The value is two!&#x27;);else disp(&#x27;The value is not one or two!&#x27;);end run example : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&gt;&gt; i = 1;&gt;&gt; while i &lt;= 5, v(i) = 100; i = i+1; end;&gt;&gt; vv = 100 100 100 100 100 64 128 256 512 1024&gt;&gt; i = 1;&gt;&gt; while true, v(i) = 999; i = i + 1; if i == 6, break; end; end;&gt;&gt; vv = 999 999 999 999 999 64 128 256 512 1024&gt;&gt; v(1)ans = 999&gt;&gt; v(1) = 2;&gt;&gt; if v(1) == 1, disp(&#x27;The value is one&#x27;); elseif v(1) == 2, disp(&#x27;The value is two&#x27;); else disp(&#x27;The value is not 1 or 2&#x27;); end;The value is two 6. Functions To call the function in Octave, do either: Navigate to the directory of the functionName.m file and call the function: 12345% Navigate to directory: cd /path/to/function % Call the function: functionName(args) simple compute line regression 123456789101112131415161718192021222324&gt;&gt; x = [1 1; 1 2; 1 3;]x = 1 1 1 2 1 3&gt;&gt; y = [1; 2; 3]y = 1 2 3&gt;&gt; theta = [0;1];&gt;&gt; j = costFunctionJ(x,y,theta)j = 0&gt;&gt; theta = [0:0];&gt;&gt; j = costFunctionJ(x, y, theta)warning: operator -: automatic broadcasting operation appliedj = 2.3333 2.3333 2.3333 2.3333 应该为一个 2.3333， 而不是矩阵 2.3333 1234567&gt;&gt; (1^2 + 2^2 + 3^2) / (2*3)ans = 2.3333&gt;&gt; [-1,-2,-3]*[-1;-2;-3]ans = 14&gt;&gt; 14 / 6ans = 2.3333&gt;&gt; 7. Vectorization Vectorization is the process of taking code that relies on loops and converting it into matrix operations. It is more efficient, more elegant, and more concise. As an example, let’s compute our prediction from a hypothesis. Theta is the vector of fields for the hypothesis and x is a vector of variables. With loops: 1234prediction = 0.0;for j = 1:n+1, prediction += theta(j) * x(j);end; With vectorization: 1prediction = theta&#x27; * x; Reference article coursera week 2 learning notes 学习一点","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Coursera Week 2 - Octave learning","slug":"ml/coursera-ng-w2-02","date":"2016-10-12T02:28:21.000Z","updated":"2021-06-20T04:12:28.311Z","comments":true,"path":"2016/10/12/ml/coursera-ng-w2-02/","link":"","permalink":"http://www.iequa.com/2016/10/12/ml/coursera-ng-w2-02/","excerpt":"Octave Tutorial, Octave Learning","text":"Octave Tutorial, Octave Learning 1. var 不像matlab有图形界面，octave只提供了命令行接口。 要启动octave，只需要在命令行输入octave即可。 1234567891011121314&gt;&gt; 2 * (3 + 5)ans = 16&gt;&gt; 2 ^ (3 + 5)ans = 256&gt;&gt; x = 2 * 3x = 6&gt;&gt; whoVariables in the current scope:ans x&gt;&gt; disp(x) 6&gt;&gt; 2. constant 1234567891011&gt; pians = 3.1416&gt;&gt; eans = 2.7183&gt;&gt; format long&gt;&gt; pians = 3.14159265358979&gt;&gt; format short&gt;&gt; pians = 3.1416&gt;&gt; octave系统定义了圆周率pi和自然指数e这两个常量, octave 可以定义显示结果 1234567&gt;&gt; 3/0warning: division by zeroans = Inf&gt;&gt; 0/0warning: division by zeroans = NaN&gt;&gt; 系统定义了Inf和NaN（注意要区分大小写）。Inf(Infinity)表示被零除的结果，NaN(Not a Number)表示零除零的结果。 3. workspace 使用save命令保存当前工作区到文件 work1 1234&gt;&gt; save work1&gt;&gt; load work1&gt;&gt; pians = 3.1416 4. semicolon 12345octave:32&gt; x = 2 * 3x = 6octave:33&gt; x = 2 * 3;octave:34&gt; disp(x) 6 5. matrix 矩阵使用方括号([])括起来，维度使用分号(;)分割。 同一维度之间的分隔符可以是空格或逗号(,) 1234567891011121314151617181920212223octave:35&gt; x = [ 2 3 5 ]x = 2 3 5octave:36&gt; y = [ 2, 3, 5 ]y = 2 3 5octave:37&gt; z = [ 2; 3; 5 ]z = 2 3 5octave:39&gt; a = [ 1 2; 1, 3; 1 5 ]a = 1 2 1 3 1 5 使用冒号表达式快速构造连续的向量 123456789octave:43&gt; v = 2:5v = 2 3 4 5 octave:44&gt; v = 2:0.3:3v = 2.0000 2.3000 2.6000 2.9000 构造矩阵的函数 linspace(start, end, N) 产生N个均匀分布于start和end之间的向量。 在绘图时用于产生x坐标特别有用。 logspace(start, end, N) 产生N个指数分布于10start和10end之间的向量。 在绘图时用于产生x坐标特别有用。 zeros(M, N) zeros(N) = zeros(N, N)。 ones(M, N) ones(N) = ones(N, N)。 rand(M, N) 值位于0~1的随机数的矩阵。 rand(N) = rand(N, N)。 123456789101112131415161718192021octave:66&gt; x = linspace (3, 4, 5)x = Columns 1 through 4: 3.00000000000000 3.25000000000000 3.50000000000000 3.75000000000000 Column 5: 4.00000000000000octave:67&gt; logspace (1, 2, 6)ans = Columns 1 through 4: 10.0000000000000 15.8489319246111 25.1188643150958 39.8107170553497 Columns 5 and 6: 63.0957344480193 100.0000000000000 6. matrix operation 1234A + BA - BA * BA \\ B 说明：A\\B为矩阵左除，用于求解线性方程Wx=b，其中W为一个nxn的矩阵，b为一个n维的列向量。 求解线性方式示例： 123456789octave:15&gt; W = [1 1 1 1; 1 2 3 4; 3 4 6 2; 2 7 10 5];octave:16&gt; b = [3; 5; 5; 8];octave:17&gt; x = W\\bx = 1.0000 3.0000 -2.0000 1.0000 6.1 matrix transpose 12345678910111213octave:9&gt; x = rand(3)x = 0.0052581 0.4446771 0.3970036 0.7844458 0.3317067 0.9633000 0.0577080 0.9015905 0.0344771octave:10&gt; x&#x27;ans = 0.0052581 0.7844458 0.0577080 0.4446771 0.3317067 0.9015905 0.3970036 0.9633000 0.034477 7. plotting 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt; t=[0:0.01:0.98];&gt;&gt; tt = Columns 1 through 10: 0.00000 0.01000 0.02000 0.03000 0.04000 0.05000 0.06000 0.07000 0.08000 0.09000 Columns 11 through 20: 0.10000 0.11000 0.12000 0.13000 0.14000 0.15000 0.16000 0.17000 0.18000 0.19000 Columns 21 through 30: 0.20000 0.21000 0.22000 0.23000 0.24000 0.25000 0.26000 0.27000 0.28000 0.29000 Columns 31 through 40: 0.30000 0.31000 0.32000 0.33000 0.34000 0.35000 0.36000 0.37000 0.38000 0.39000 Columns 41 through 50: 0.40000 0.41000 0.42000 0.43000 0.44000 0.45000 0.46000 0.47000 0.48000 0.49000 Columns 51 through 60: 0.50000 0.51000 0.52000 0.53000 0.54000 0.55000 0.56000 0.57000 0.58000 0.59000 Columns 61 through 70: 0.60000 0.61000 0.62000 0.63000 0.64000 0.65000 0.66000 0.67000 0.68000 0.69000 Columns 71 through 80: 0.70000 0.71000 0.72000 0.73000 0.74000 0.75000 0.76000 0.77000 0.78000 0.79000 Columns 81 through 90: 0.80000 0.81000 0.82000 0.83000 0.84000 0.85000 0.86000 0.87000 0.88000 0.89000 Columns 91 through 99: 0.90000 0.91000 0.92000 0.93000 0.94000 0.95000 0.96000 0.97000 0.98000&gt;&gt; y1=sin(2*pi*4*t);&gt;&gt; plot(t,y1)&gt;&gt; y2=cos(2*pi*4*t);&gt;&gt; plot(t,y2)&gt;&gt; hold on&gt;&gt; plot(t,y1)&gt;&gt; plot(t,y2,&#x27;r&#x27;)&gt;&gt; xlabel(&#x27;time&#x27;)&gt;&gt; ylabel(&#x27;value&#x27;)&gt;&gt; legend(&#x27;sin&#x27;,&#x27;cos&#x27;)&gt;&gt; title(&#x27;my plot&#x27;)&gt;&gt; print -dpng &#x27;myPlot.png&#x27;warning: print.m: fig2dev binary is not available.Some output formats are not available. 123456&gt;&gt; figure(2); plot(t, y2)&gt;&gt; subplot(1,2,1);&gt;&gt; plot(t,y1)&gt;&gt; subplot(1,2,2)&gt;&gt; plot(t,y2)&gt;&gt; axis([0.5 1 -1 1]) 123456789101112&gt;&gt; clf;&gt;&gt; A = magic(5)A = 17 24 1 8 15 23 5 7 14 16 4 6 13 20 22 10 12 19 21 3 11 18 25 2 9&gt;&gt; imagesc(A)&gt;&gt; imagesc(A), colorbar, colormap gray; 12345&gt;&gt; imagesc(magic(15)), colorbar, colormap gray;&gt;&gt; a=1,b=2,c=3a = 1b = 2c = 3 8. ng 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542&gt;&gt; A = [1 2; 3 4; 5 6;]A = 1 2 3 4 5 6&gt;&gt; save hello.mat v; (压缩比例很大)save hello.txt v -ascii % save as text(ASCII)&gt;&gt; whoVariables in the current scope:A&gt;&gt; whosVariables in the current scope: Attr Name Size Bytes Class ==== ==== ==== ===== ===== A 3x2 48 doubleTotal is 6 elements using 48 bytes&gt;&gt; clear&gt;&gt; A(3,2)ans = 6&gt;&gt; A(:,2)ans = 2 4 6&gt;&gt; A(2,:)ans = 3 4&gt;&gt; AA = 1 2 3 4 5 6&gt;&gt; A([1 3], :)ans = 1 2 5 6&gt;&gt; A(:,2)ans = 2 4 6&gt;&gt; A(:,2) = [10; 11; 12]A = 1 10 3 11 5 12&gt;&gt; A = [A, [100; 101; 102]];&gt;&gt; AA = 1 10 100 3 11 101 5 12 102&gt;&gt; [100;101;102]ans = 100 101 102&gt;&gt; size(A)ans = 3 3&gt;&gt; A(:)ans = 1 3 5 10 11 12 100 101 102&gt;&gt; A = [1 2; 3 4; 5 6;]A = 1 2 3 4 5 6&gt;&gt; B = [11 12; 13 14; 15 16]B = 11 12 13 14 15 16&gt;&gt; C = [A B]C = 1 2 11 12 3 4 13 14 5 6 15 16&gt;&gt; D = [A;B]D = 1 2 3 4 5 6 11 12 13 14 15 16&gt;&gt; size(D)ans = 6 2&gt;&gt; [A, B]ans = 1 2 11 12 3 4 13 14 5 6 15 16&gt;&gt; [A B]ans = 1 2 11 12 3 4 13 14 5 6 15 16&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; A .* Bans = 11 24 39 56 75 96&gt;&gt; A .^ 2ans = 1 4 9 16 25 36&gt;&gt; v = [1; 2; 3]v = 1 2 3&gt;&gt; 1 ./ vans = 1.00000 0.50000 0.33333&gt;&gt; 1 ./ Aans = 1.00000 0.50000 0.33333 0.25000 0.20000 0.16667&gt;&gt; log(v)ans = 0.00000 0.69315 1.09861&gt;&gt; exp(v)ans = 2.7183 7.3891 20.0855&gt;&gt; abs(v)ans = 1 2 3&gt;&gt; abs([-1; -2; -3])ans = 1 2 3&gt;&gt; V = vV = 1 2 3&gt;&gt; VV = 1 2 3&gt;&gt; VV = 1 2 3&gt;&gt; -Vans = -1 -2 -3&gt;&gt; V + ones(length(V))warning: operator +: automatic broadcasting operation appliedans = 2 2 2 3 3 3 4 4 4&gt;&gt; length(V)ans = 3&gt;&gt; ones(3,1)ans = 1 1 1&gt;&gt; V + ones(3, 1)ans = 2 3 4&gt;&gt; V + 2ans = 3 4 5&gt;&gt; VV = 1 2 3&gt;&gt; AA = 1 2 3 4 5 6&gt;&gt; A&#x27;ans = 1 3 5 2 4 6&gt;&gt; a = [1 15 2 0.5]a = 1.00000 15.00000 2.00000 0.50000&gt;&gt; val = max(a)val = 15&gt;&gt; [val, ind] = max(a)val = 15ind = 2&gt;&gt; max(A)ans = 5 6&gt;&gt; AA = 1 2 3 4 5 6&gt;&gt; aa = 1.00000 15.00000 2.00000 0.50000&gt;&gt; a &lt; 3ans = 1 0 1 1&gt;&gt; find(a &lt; 3)ans = 1 3 4&gt;&gt; A = magix(3)error: &#x27;magix&#x27; undefined near line 1 column 5&gt;&gt; A = magic(3)A = 8 1 6 3 5 7 4 9 2&gt;&gt; [r, c] = find(A &gt;= 7)r = 1 3 2c = 1 2 3&gt;&gt; A(2,3)ans = 7&gt;&gt; sum(a)ans = 18.500&gt;&gt; prod(a)ans = 15&gt;&gt; floor(a)ans = 1 15 2 0&gt;&gt; ceil(a)ans = 1 15 2 1&gt;&gt; rand(3)ans = 0.708800 0.905101 0.837562 0.264139 0.265985 0.671546 0.411435 0.058028 0.454436&gt;&gt; max(rand(3), rand(3))ans = 0.87641 0.74541 0.92027 0.61292 0.57756 0.95694 0.26555 0.76822 0.63566&gt;&gt; AA = 8 1 6 3 5 7 4 9 2&gt;&gt; max(A, [], 1)ans = 8 9 7&gt;&gt; max(A, [], 2)ans = 8 7 9&gt;&gt; max(A)ans = 8 9 7&gt;&gt; max(max(A))ans = 9&gt;&gt; A(:)ans = 8 3 4 1 5 9 6 7 2&gt;&gt; max(A(:))ans = 9&gt;&gt;&gt;&gt;&gt;&gt; A = magic(9)A = 47 58 69 80 1 12 23 34 45 57 68 79 9 11 22 33 44 46 67 78 8 10 21 32 43 54 56 77 7 18 20 31 42 53 55 66 6 17 19 30 41 52 63 65 76 16 27 29 40 51 62 64 75 5 26 28 39 50 61 72 74 4 15 36 38 49 60 71 73 3 14 25 37 48 59 70 81 2 13 24 35&gt;&gt; sum(A,1)ans = 369 369 369 369 369 369 369 369 369&gt;&gt; sum(A,2)ans = 369 369 369 369 369 369 369 369 369&gt;&gt; eye(9)ans =Diagonal Matrix 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1&gt;&gt; AA = 47 58 69 80 1 12 23 34 45 57 68 79 9 11 22 33 44 46 67 78 8 10 21 32 43 54 56 77 7 18 20 31 42 53 55 66 6 17 19 30 41 52 63 65 76 16 27 29 40 51 62 64 75 5 26 28 39 50 61 72 74 4 15 36 38 49 60 71 73 3 14 25 37 48 59 70 81 2 13 24 35&gt;&gt; A .* eye(9)ans = 47 0 0 0 0 0 0 0 0 0 68 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 20 0 0 0 0 0 0 0 0 0 41 0 0 0 0 0 0 0 0 0 62 0 0 0 0 0 0 0 0 0 74 0 0 0 0 0 0 0 0 0 14 0 0 0 0 0 0 0 0 0 35&gt;&gt; sum(sum(A .* eye(9)))ans = 369&gt;&gt; flipud(eye(9))ans =Permutation Matrix 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0&gt;&gt; sum(sum(A.*flipud(eye(9))))ans = 369&gt;&gt; AA = 47 58 69 80 1 12 23 34 45 57 68 79 9 11 22 33 44 46 67 78 8 10 21 32 43 54 56 77 7 18 20 31 42 53 55 66 6 17 19 30 41 52 63 65 76 16 27 29 40 51 62 64 75 5 26 28 39 50 61 72 74 4 15 36 38 49 60 71 73 3 14 25 37 48 59 70 81 2 13 24 35&gt;&gt; A = magic(3)A = 8 1 6 3 5 7 4 9 2&gt;&gt; temp = pinv(A)temp = 0.147222 -0.144444 0.063889 -0.061111 0.022222 0.105556 -0.019444 0.188889 -0.102778 Reference article coursera week 2 learning notes 学习一点","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Coursera Week 2 - Linear Regression with Multiple Variables","slug":"ml/coursera-ng-w2-01-Linear-Regression","date":"2016-10-08T04:28:21.000Z","updated":"2021-06-20T04:12:28.317Z","comments":true,"path":"2016/10/08/ml/coursera-ng-w2-01-Linear-Regression/","link":"","permalink":"http://www.iequa.com/2016/10/08/ml/coursera-ng-w2-01-Linear-Regression/","excerpt":"coursera week 2 - linear regression with multiple variables 1","text":"coursera week 2 - linear regression with multiple variables 1 1. Multiple Features \\begin{align}x_j^{(i)} &amp;= \\text{value of feature } j \\text{ in the }i^{th}\\text{ training example} \\newline x^{(i)}&amp; = \\text{the column vector of all the feature inputs of the }i^{th}\\text{ training example} \\newline m &amp;= \\text{the number of training examples} \\newline n &amp;= \\left| x^{(i)} \\right| ; \\text{(the number of features)} \\end{align} Macdown Version 0.6.4 (786) MathJax the same this web 1.1 hypothesis function Now define the multivariable form of the hypothesis function as follows, accommodating these multiple features: hθ(x)=θ0+θ1x1+θ2x2+θ3x3+⋯+θnxn h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n hθ​(x)=θ0​+θ1​x1​+θ2​x2​+θ3​x3​+⋯+θn​xn​ multivariable hypothesis function Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as: \\begin{align} h_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} ... \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x \\end{align} The training examples are stored in X row-wise, like such: \\begin{align} X = \\begin{bmatrix}x^{(1)}_0 &amp; x^{(1)}\\_1 \\newline x^{(2)}_0 &amp; x^{(2)}\\_1 \\newline x^{(3)}\\_0 &amp; x^{(3)}\\_1 \\end{bmatrix}&amp;,\\theta = \\begin{bmatrix}\\theta\\_0 \\newline \\theta\\_1 \\newline \\end{bmatrix} \\end{align} You can calculate the hypothesis as a column vector of size (m x 1) with: h_θ(X)=Xθ h\\_\\theta(X) = X \\theta h_θ(X)=Xθ For the rest of these notes, X will represent a matrix of training examples x_(i)x\\_{(i)}x_(i) 2. Cost function For the parameter vector θ (of type Rn+1\\mathbb{R}^{n+1}Rn+1 or in R(n+1)×1\\mathbb{R}^{(n+1) \\times 1}R(n+1)×1, the cost function is: J(θ)=12m∑_i=1m(hθ(x(i))−y(i))2 J(\\theta) = \\dfrac {1}{2m} \\displaystyle \\sum\\_{i=1}^m \\left (h_\\theta (x^{(i)}) - y^{(i)} \\right)^2 J(θ)=2m1​∑_i=1m(hθ​(x(i))−y(i))2 The vectorized version is: J(θ)=12m(Xθ−y⃗)T(Xθ−y⃗) J(\\theta) = \\dfrac {1}{2m} (X\\theta - \\vec{y})^{T} (X\\theta - \\vec{y}) J(θ)=2m1​(Xθ−y​)T(Xθ−y​) vectorized version is very good! 3. Gradient Desc Multivariable Matrix Notation The Gradient Descent rule can be expressed as: θ:=θ−α∇J(θ) \\theta := \\theta - \\alpha \\nabla J(\\theta) θ:=θ−α∇J(θ) Where ∇J(θ)\\nabla J(\\theta)∇J(θ) is a column vector of the form: ∇J(θ)=[∂J(θ)∂θ0∂J(θ)∂θ1⋮∂J(θ)∂θn] \\nabla J(\\theta) = \\begin{bmatrix}\\frac{\\partial J(\\theta)}{\\partial \\theta_0} \\newline \\frac{\\partial J(\\theta)}{\\partial \\theta_1} \\newline \\vdots \\newline \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\end{bmatrix} ∇J(θ)=[∂θ0​∂J(θ)​∂θ1​∂J(θ)​⋮∂θn​∂J(θ)​​] The j-th component of the gradient is the summation of the product of two terms: \\begin{align} \\; &amp;\\frac{\\partial J(\\theta)}{\\partial \\theta\\_j} &amp;=&amp; \\frac{1}{m} \\sum\\limits\\_{i=1}^{m} \\left(h\\_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot x\\_j^{(i)} \\newline \\; &amp; &amp;=&amp; \\frac{1}{m} \\sum\\limits\\_{i=1}^{m} x\\_j^{(i)} \\cdot \\left(h\\_\\theta(x^{(i)}) - y^{(i)} \\right) \\end{align} 在数学中，一个多变量的函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定。 Sometimes, the summation of the product of two terms can be expressed as the product of two vectors. \\begin{align}\\; &amp;\\frac{\\partial J(\\theta)}{\\partial \\theta\\_j} = \\frac1m \\vec{x\\_j}^{T} (X\\theta - \\vec{y}) \\newline &amp;\\nabla J(\\theta) = \\frac 1m X^{T} (X\\theta - \\vec{y}) \\newline \\end{align} Finally, the matrix notation (vectorized) of the Gradient Descent rule is: θ:=θ−αmXT(Xθ−y⃗) \\theta := \\theta - \\frac{\\alpha}{m} X^{T} (X\\theta - \\vec{y}) θ:=θ−mα​XT(Xθ−y​) The gradient descent equation itself is generally the same form; we just have to repeat it for our ‘n’ features: \\begin{align} &amp; \\text{repeat until convergence:} \\; \\lbrace \\newline \\; &amp; \\theta\\_0 := \\theta\\_0 - \\alpha \\frac{1}{m} \\sum\\limits\\_{i=1}^{m} (h\\_\\theta(x^{(i)}) - y^{(i)}) \\cdot x\\_0^{(i)}\\newline \\; &amp; \\theta\\_1 := \\theta\\_1 - \\alpha \\frac{1}{m} \\sum\\limits\\_{i=1}^{m} (h\\_\\theta(x^{(i)}) - y^{(i)}) \\cdot x\\_1^{(i)} \\newline \\; &amp; \\theta\\_2 := \\theta\\_2 - \\alpha \\frac{1}{m} \\sum\\limits\\_{i=1}^{m} (h\\_\\theta(x^{(i)}) - y^{(i)}) \\cdot x\\_2^{(i)} \\newline &amp; \\cdots \\newline \\rbrace \\end{align} In other words: \\begin{align} &amp; \\text{repeat until convergence:} \\; \\lbrace \\newline \\; &amp; \\theta\\_j := \\theta\\_j - \\alpha \\frac{1}{m} \\sum\\limits\\_{i=1}^{m} (h\\_\\theta(x^{(i)}) - y^{(i)}) \\cdot x\\_j^{(i)} \\; &amp; \\text{for j := 0..n} \\newline \\rbrace \\end{align} 3.1 Feature Scaling Idea : Make sure features are on a similar scale 特征缩放 Get every feature into approximately a −1≤xi≤1-1 \\leq x_i \\leq 1−1≤xi​≤1 range. Replace x_ix\\_ix_i with x_i−u_ix\\_i - u\\_ix_i−u_i to make features have approximately zero mean (Do not apply to x_0x\\_0x_0 = 1). 如果多个特征值，大多处在一个相近的范围，梯度下降就能更快的收敛。 因为 2000/5 比较大，所以轮廓图，使得椭圆更加的瘦长，好比 J(θ)J(\\theta)J(θ) 收敛的更慢。 3.2 learning rate \\begin{align} \\theta\\_j := \\theta\\_j - \\alpha \\frac{\\partial}{\\partial \\theta\\_j} J(\\theta) \\end{align} Debugging : How to make sure gradient descent is working correctly How to choose learning rate α\\alphaα Summary if α\\alphaα is too small: slow convergence [kən’vɜːdʒəns] 收敛 if α\\alphaα is too large: J(θ)J(\\theta)J(θ) may not decrease on every iteration; may not converge. To choose α\\alphaα, try …, 0.001, 0.01, 0.1, 1, … 4. Polynomial Regression 4.1 Polynomial Regression Feature normalization is very important 4.2 Choice of features @2017-02-10 review done 5. Normal Equation θ=(XTX)−1XTy\\theta = (X^T X)^{-1}X^T yθ=(XTX)−1XTy 5.1 num and vector J(θ)=12m(Xθ−y⃗)T(Xθ−y⃗) J(\\theta) = \\dfrac {1}{2m} (X\\theta - \\vec{y})^{T} (X\\theta - \\vec{y}) J(θ)=2m1​(Xθ−y​)T(Xθ−y​) \\begin{align}\\; &amp;\\frac{\\partial J(\\theta)}{\\partial \\theta\\_j} = \\frac1m \\vec{x\\_j}^{T} (X\\theta - \\vec{y}) \\newline &amp;\\nabla J(\\theta) = \\frac 1m X^{T} (X\\theta - \\vec{y}) \\newline \\end{align} 5.2 house price example \\begin{align} \\nabla J(\\theta) = \\frac 1m X^{T} (X\\theta - \\vec{y}) \\newline \\end{align} 令 $\\nabla J(\\theta) = 0 $ So, $\\theta = (X^T X){-1}XT y $ 5.3 mmm training, nnn features Gradient Descent Normal Equation Need to choose α\\alphaα No need to choose α\\alphaα Needs many iterations Don’t need to iterate Works well even when nnn is large Slow if nnn is very large it is usually around ten thousand that I might start to consider switching over to gradient descents or maybe, some other algorithms that we’ll talk about later in this class 5.4 XTXX^T XXTX is non-invertible $\\theta = (X^T X){-1}XT y $ What XTXX^T XXTX is non-invertible? （singular / degenerate） When XTXX^T XXTX is non-invertible, this is very few. What XTXX^T XXTX is non-invertible?","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Coursera Week 1 - Linear Algebra Matrices And Vectors","slug":"ml/coursera-ng-w1-03-Linear-Algebra","date":"2016-09-30T06:22:21.000Z","updated":"2021-06-20T04:12:28.313Z","comments":true,"path":"2016/09/30/ml/coursera-ng-w1-03-Linear-Algebra/","link":"","permalink":"http://www.iequa.com/2016/09/30/ml/coursera-ng-w1-03-Linear-Algebra/","excerpt":"coursera week 1 - matrices and vectors","text":"coursera week 1 - matrices and vectors 1. Matrix Elements A = \\begin{bmatrix} 1 &amp; 2 \\\\\\ 4 &amp; 5 \\\\\\ 7 &amp; 8 \\end{bmatrix} \\tag{fmt.1 R^{32}} $ A_{ij} = $ &quot;i,ji, ji,j entry&quot; in the ithi^{th}ith row, jthj^{th}jth column $ A_{32} = 8 $ 2. Vector A_nA\\_nA_n n*1 matrix y = \\begin{bmatrix} 460 \\\\\\ 232 \\\\\\ 315 \\\\\\ 178 \\end{bmatrix} \\tag{fmt.2} R4R^4R4 4 dimensional vector y_i=ithelementy\\_i = i^{th} elementy_i=ithelement 2.1 math 1-indexed y = \\begin{bmatrix} y1 \\\\\\ y2 \\\\\\ y3 \\\\\\ y4 \\end{bmatrix} \\tag{fmt.3} 2.2 machine-learning 0-indexed y = \\begin{bmatrix} y0 \\\\\\ y1 \\\\\\ y2 \\\\\\ y3 \\end{bmatrix} \\tag{fmt.4} 3. Matrix Addition \\begin{bmatrix} 1 &amp; 0 \\\\\\ 2 &amp; 5 \\\\\\ 3 &amp; 1 \\end{bmatrix} + \\begin{bmatrix} 4 &amp; 0.5 \\\\\\ 2 &amp; 5 \\\\\\ 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 5 &amp; 0.5 \\\\\\ 4 &amp; 10 \\\\\\ 3 &amp; 2 \\end{bmatrix} 4. Scalar Multiplication 3 \\times \\begin{bmatrix} 1 &amp; 0 \\\\\\ 2 &amp; 5 \\\\\\ 3 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 0 \\\\\\ 6 &amp; 15 \\\\\\ 9 &amp; 3 \\end{bmatrix} 5. Combination of Operands 3 \\times \\begin{bmatrix} 1 \\\\\\ 4 \\\\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 5 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\\\ 0 \\\\\\ 2 \\end{bmatrix} / 3 = \\begin{bmatrix} 2 \\\\\\ 12 \\\\\\ 31/3 \\end{bmatrix} 6. Matrix Vector Multiplication [13 40 21]×[1 5]=[16 4 7]\\begin{bmatrix} 1 &amp; 3 \\\\\\ 4 &amp; 0 \\\\\\ 2 &amp; 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\\\ 5 \\end{bmatrix} = \\begin{bmatrix} 16 \\\\\\ 4 \\\\\\ 7 \\end{bmatrix} ⎣⎡​1 4 2​301​⎦⎤​×[1 5​]=⎣⎡​16 4 7​⎦⎤​ Matrix Vector Multiplication Fmt : 6.1 House sizes example h_θ(x)=−40+0.25xh\\_{\\theta} (x) = -40 + 0.25 x h_θ(x)=−40+0.25x House sizes Price 2104 ? 1416 ? 1534 ? 852 ? \\begin{bmatrix} 1 &amp; 2104 \\\\\\ 1 &amp; 1416 \\\\\\ 1 &amp; 1534 \\\\\\ 1 &amp; 852 \\end{bmatrix} \\times \\begin{bmatrix} -40 \\\\\\ 0.25 \\end{bmatrix} = \\begin{bmatrix} -40 \\times 1 + 0.25 \\times 2104 \\\\\\ ... \\\\\\ ... \\\\\\ ... \\end{bmatrix} 7. Practice Example \\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\\\ 4 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 3 \\\\\\ 0 &amp; 1 \\\\\\ 5 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 11 &amp; 10 \\\\\\ 9 &amp; 14 \\end{bmatrix} $ A_{2 \\times 3} \\times A_{3 \\times 2} = A_{2 \\times 2} $ 8. House Example 9. Matrix A×B≠B×AA \\times B \\neq B \\times AA×B​=B×A But, 结合律，可以的 $ A \\times B \\times C = (A \\times B) \\times C = A \\times (B \\times C) $ 10. Identity Matrix Denoted I (or I_{n*n}). 10.1 2×22 \\times 22×2 \\begin{bmatrix} 1 &amp; 0 \\\\\\ 0 &amp; 1 \\end{bmatrix} \\tag{fmt.1 R^{32}} 10.2 3×33 \\times 33×3 \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\\\ 0 &amp; 1 &amp; 0 \\\\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\tag{fmt.1 R^{32}} Z×I=I×Z=ZZ \\times I = I \\times Z = ZZ×I=I×Z=Z 11. Matrix Inverse 3×3−1=13 \\times 3^{-1} = 13×3−1=1 Not all numbers have an inverse. if AAA is an m×mm \\times mm×m matrix, and if it has an inverse A×A−1=A−1×A=IA \\times A^{-1} = A^{-1} \\times A = IA×A−1=A−1×A=I A = \\begin{bmatrix} 3 &amp; 4 \\\\\\ 2 &amp; 16 \\\\\\ \\end{bmatrix} A^{-1} = \\begin{bmatrix} 0.4 &amp; -0.1 \\\\\\ -0.05 &amp; 0.075 \\\\\\ \\end{bmatrix} $ A \\times A^{-1} = I_{2 \\times 2} $ I\\_{2 \\times 2} = \\begin{bmatrix} 1 &amp; 0 \\\\\\ 0 &amp; 1 \\\\\\ \\end{bmatrix} 12. Matrix Transpose A = \\begin{bmatrix} 1 &amp; 2 &amp; 0 \\\\\\ 3 &amp; 5 &amp; 9 \\\\\\ \\end{bmatrix} A^T = \\begin{bmatrix} 1 &amp; 3 \\\\\\ 2 &amp; 5 \\\\\\ 0 &amp; 9 \\end{bmatrix} Let AAA be an m×nm \\times nm×n matrix, and let B=ATB = A^TB=AT. Then BBB is an n×mn \\times mn×m matrix, and B_ij=A_jiB\\_{ij} = A\\_{ji}B_ij=A_ji","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Coursera Week 1 - Linear Regression Cost Function & Gradient descent","slug":"ml/coursera-ng-w1-02-cost-function-gradient-descent","date":"2016-09-28T09:22:21.000Z","updated":"2021-06-20T04:12:28.314Z","comments":true,"path":"2016/09/28/ml/coursera-ng-w1-02-cost-function-gradient-descent/","link":"","permalink":"http://www.iequa.com/2016/09/28/ml/coursera-ng-w1-02-cost-function-gradient-descent/","excerpt":"Linear Regression Cost Function &amp; Gradient descent","text":"Linear Regression Cost Function &amp; Gradient descent 1. Linear Regression 2. Cost Function Choose θ0，θ1\\theta_0，\\theta_1θ0​，θ1​ so that hθ(x)h_{\\theta} (x)hθ​(x) is close to yyy for our training examples (x,y){(x, y)}(x,y) Title fmt Hypothesis hθ(x)=θ0+θ1xh_{\\theta} (x) = \\theta_0 + \\theta_1 xhθ​(x)=θ0​+θ1​x Parameters θ0、θ1\\theta_0 、\\theta_1θ0​、θ1​ Cost Function J(θ0，θ1)=12m∑i=1m(hθ(xi)−(yi))2J(\\theta_0，\\theta_1) = {\\frac {1} {2m}} \\sum_{i=1}^m (h_{\\theta} (x^{i}) - (y^{i}))^2J(θ0​，θ1​)=2m1​∑i=1m​(hθ​(xi)−(yi))2 Goal minimizeJ(θ0，θ1)minimize J(\\theta_0，\\theta_1)minimizeJ(θ0​，θ1​) 3. Simplified Fmt θ0\\theta_0θ0​ = 0 hypothesis function hθ(x)h_{\\theta} (x)hθ​(x) cost function J(θ1)J(\\theta_1)J(θ1​) 4. Cost function visable 把 x, y 想象成向量，确定的向量，向量再想象为一个确定的数，总之它是一个二次函数，抽象的想一下，会不会理解 contour plots contour figures 5. Gradient descent target 6. Gradient descent visable Convex function 7. Gradient descent algorithm $ \\alpha $ : learning rate 8. Gradient descent only $ \\theta_{1} $ 9. Linear Regression Model 9.1 Batch Gradient Descent Batch : Each step of gradient descent uses all the training examples Coursera Learning Notes","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Coursera Week 1 - Machine Learning Introduction","slug":"ml/coursera-ng-w1-01-introduce","date":"2016-09-20T02:22:21.000Z","updated":"2021-06-20T04:12:28.308Z","comments":true,"path":"2016/09/20/ml/coursera-ng-w1-01-introduce/","link":"","permalink":"http://www.iequa.com/2016/09/20/ml/coursera-ng-w1-01-introduce/","excerpt":"Machine-learning, Grew out of work in Artificial Intelligence, New capability for computers","text":"Machine-learning, Grew out of work in Artificial Intelligence, New capability for computers Machine Learning Grew out of work in Artificial Intelligence New capability for computers search engine, recommendation system, image recognition web click data, medical records , biology, engineering Natural Language Processing (NLP), Computer Vision Machine Learning definition Field of study that gives computers the ability to learn without being explicitly programmed. by ArthurSamuel(1959) 1. Supervised learning 2. Regression &amp; Classification 3. Unsupervised learning Unsupervised Examples What Google News does is everyday it goes and looks at tens of thousands or hundreds of thousands of new stories on the web and it groups them into cohesive news stories. 4. Experience Xiaoyang 语录 : 『解决一个问题的方法和思路不止一种』 『没有所谓的机器学习算法优劣，也没有绝对高性能的机器学习算法，只有在特定的场景、数据和特征下更合适的机器学习算法。』 Andrew Ng 语录 应用机器学习，不要一上来就试图做到完美，先lu一个baseline的model出来，再进行后续的分析步骤，一步步提高，所谓后续步骤可能包括『分析model现在的状态(欠/过拟合)，分析我们使用的feature的作用大小，进行feature selection，以及我们模型下的bad case和产生的原因』等等。 Kaggle大神们 experience 总结 ： 『对数据的认识太重要了！』 『数据中的特殊点/离群点的分析和处理太重要了！』 『特征工程(feature engineering)太重要了！在很多Kaggle的场景下，甚至比model本身还要重要』 『要做模型融合(model ensemble)啊啊啊！』","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"}]},{"title":"Java程序员需要知道的计算机原理 (not finish)","slug":"java/java-special-arms-p2-computer-principle","date":"2016-09-13T08:54:16.000Z","updated":"2021-06-20T04:12:28.296Z","comments":true,"path":"2016/09/13/java/java-special-arms-p2-computer-principle/","link":"","permalink":"http://www.iequa.com/2016/09/13/java/java-special-arms-p2-computer-principle/","excerpt":"Java特种兵 - Java程序员需要知道的计算机原理，Reading Notes","text":"Java特种兵 - Java程序员需要知道的计算机原理，Reading Notes 1. 计算机原理 计算机总体体系结构的变化，一直不是特别大，基础原理将引导我们从整体上认识计算机本身。 2. CPU 每个进程 or 线程 发出请求, 最后会由 CPU 来分配时间片处理，处理时 操作数 传递给 CPU, CPU 计算将其回写到本地变量。这个本地变量通常会存在程序所谓的 栈 中，多次对其操作，它可能会被 Cache 到 CPU 的缓存之中。CPU 有 寄存器，一级缓存，二级缓存 … , 其实设计这些组件就是为了那四个字 就近原则。 2.1 Cpu 联系 Java 在 编译阶段，Java 就可以决定方法的 LocalVariable 的个数，在方法调用的时候，就可直接分配一个 LocalVariable 区域，这个空间是基于 slot 来分配的，每个 slot 占用 32 bit，boolean 占用 1 slot，long and double 占 2 个 slot。 LocalVariableTable : Start Length Slot Name Signature 0 9 0 args Ljava/lang/String; 2 7 1 a I … … … … … Start : 代表LocalVariable在虚指令作用域的起始位置 Length : 代表LocalVariable在虚指令作用域的长度(如第1个本地变量args是9条指令的作用域) 2.2 多核 为了多个计算中心同时做事情，增加效率。 考虑需要和解决的问题 一个指令来了，哪个Cpu来处理？ 同一份数据被多个 Cpu 处理，如何协调并让其他Cpu都知道。 当发起一个计算请求，例如一个中断，这么多 Cpu 会干什么？ 中断 : 指当出现需要时，CPU暂时停止当前程序的执行转而执行处理新情况的程序和执行过程。 Cpu 的计算速度非常快，OS不希望它等待或者停止，所以在出现 I/O 等待 (网络I/O 和 磁盘I/O), 它中途基本不参与，而以事件注册的方式来实现回调，对于某些执行时间长的task，Cpu会分配一些时间片执行其他的task. 当 Cpu 不断去切换 task 处理时，这就会涉及到 上下文切换. 2.3 Cache line 办事就近原则，可以一次办多件事情 或 一件事情的多个步骤可以一次办完。 Cache line 就是 将 连续的一段内存区域 进行 Cache，不是每次就 Cache 一个内存单元，而是一系列内存单元。计算机中，通常以连续 64bit 进行Cache。 感悟 : Cache Line 就是 一次拿一批信息去处理。 Cache line 的目的是为了 快速访问。 2.4 缓存一致性协议 当有 来自于 内存 中的同一份数据 Cache 在多个 CPU 中，且要求这些数据的读写一致时，多个 CPU 之间就需要遵循缓存共享的一致性原则。 相当于大家都来修改一份设计报告，大家都拷贝了一份，回家修改，每个人修改要及时让其他人都知道。有点像版本控制 内存单元的状态 Modified Exclusive Shared Invalid 多个 CPU 通过总线相互连接，每个 CPU 的cache处理器 要响应本身所对应的CPU读写操作外，还需要监听总线上其他CPU操作，通过监听对自己的Cache做处理，形成虚共享，这个协议叫做 MESI 协议。 一个数据修改了，它需要告诉其他 CPU 这份数据被修改了，现在Intel通过 QPI 来完成。不同CPU之间交互需要时间 20~40 ns 级别。 1234567class VolatileInteger &#123; volatile int number;&#125;VolatileInteger[] values = new VolatileInteger[10];for (int i = 0; i &lt; 10; i++) &#123; values[i] = new VolatileInteger();&#125; 这段代码的例子，很可能使的每个CPU可能只修改到某个元素，但会有大量 QPI 存在。 QPI : Quick Path Interconnect 2.5 Context switch 线程已经执行了一部分内容，需要记录下它的内容和状态，中途由于调度算法。 CPU 调度 的最基本单位是线程，Java也是基于多线程模式。由于多线程模型中多个线程共享进程的资源，所以Java程序，如某一个线程占用资源过大时，就可能导致整个JVM进程挂掉。(影响都是相对的) 在实际运行中会有代码段和数据段，内容切换时要保存这些运行中的上下文信息，再使用的时候，再加载回来。 日志写操作 (如 : log4j) 都采用 异步 模式 实现，而程序通常不直接参与这个过程。 实现方式 (日志写操作只是将日志写入一个消息队列中，由单独的线程来完成写操作) 2.6 并发与争用 只要是服务器端程序，迟早会遇到并发。当 并发 时，就会存在对各种资源的争用，包括对各个部件(如CPU)的争用。 Web 程序也会经常遇到并发问题的，编写者，没有遇到是因为 Web 容器帮助处理好了线程的本地变量分配，我们几乎不用关注并发。 2.6.1 临界区 当程序出现 “加锁” 时 (如 Java的 synchronized) 说明这一块是临界区，只允许一个线程访问，其他线程来回进行等待队列。 争用带来的是同步的开销，它会发出许多指令要求所有CPU处理中不允许其他线程进入临界区，且需要将等待线程放入队列阻塞。 争用 CPU 的访问也不仅仅体现在锁上面，CPU本身数量也有限。 单个CPU会对任务进行 基于 时间片、优先级、任务大小分别调度。 2.6.2 线程池数 在理想的 CPU 密集型系统，线程数是 CPU数+1 / CPU-1 系统CPU密集度 一般系统分为 计算密集型 和 I/O 密集型。 系统中关键程序访问总共花费 120ms, I/O操作 占用 100ms，100ms时间内 CPU是可以被其他线程访问的。此时，这个程序在单核系统中的线程数理论上可以设置为 6， 在多核系统就是乘以CPU个数 左右这个数字。 一般这个线程数的决定是通过测试的。 2.6.3 锁 锁 就是临界区的范围，有粒度。如果在 锁 内部发送 I/O, 大循环、递归等，那么就必须等待这些操作处理完成后才能有下一个线程进入处理。如果这段程序是 关键程序, 当系统真正并发的时候，很多线程都会阻塞在这里。这时要计算线程数，要看锁对象 是不是静态对象或Class, (如果是，则是一个JVM全局锁)，无论配置多少线程效果都一样。锁是全局的，无论多少个CPU也是无济于事的。 结论 : 锁尽量不要设置为 全局锁，能用粒度控制，尽量粒度控制 2.6.4 JVM自身调节 不论 CPU 跑多快，如 JVM hold不住节奏，不断做GC，那么如何配置线程池，系统性能还是上不来。 可以根据 JVM 运行日志中，平均做 Young GC 的时间间隔 (通过 Young GC 与 运行时长对比)，以及系统的QPS，来估算每个请求大致占用的内存大小，有时不准，但具有参考价值。 如何计算 Eden 空间的大小我们是知道的。通常一个请求分配空间都在 Eden 区域，Eden区域满发生Young GC。Young GC 时间间隔就是Eden满的时间间隔，例如 3s， 进一步通过 QPS*3 得到多少个请求可以填充满 Eden 区域。这可初步估算每个请求占用的内存空间。 3. 内存 基本所有的程序猿与程序媛都知道，它是跑程序的地方。 磁盘存储 与 CPU 之间的桥梁。拥有比 CPU缓存 大几百倍、上千倍的空间。CPU三级缓存也就 几十M。 磁盘 到达 CPU需要经过 (主板-南桥、北桥) 才能到达CPU。很慢。 内存的容量都是 GB 单位，大量程序运行都依赖内存，又 OS 来管理和调度。 地址位数、逻辑地址、虚拟地址、物理地址、线性地址、内核区域等，很多人看到这，直接疯了，但是你需要淡定。 3.1 虚拟地址 所有的程序中使用的地址都是虚拟地址 (在段式管理中也叫逻辑地址)，这些地址在不同的进程之间是可以重复的。 程序为什么要使用 虚拟地址? C语言来说，编译后的指令中，许多调用的地址在编译阶段就得确定下来，许多方法入口和变量位置在编译时确定了虚拟地址，真正运行时要由OS来分配实际的地址给程序。 使用虚拟地址后，地址是可以被复用的，程序不关心与其他进程是否会使用同一地址，OS会分配确保。 3.2 分段机制 也就是为进程分配的一段内存区 (连续的区域)，它的 起始位置 + 逻辑地址 = 线性地址 (就是物理地址) 本进程访问其他进程内存，内存不能read 错误。 3.3 分页机制 分页机制 可支撑较大的内存，物理上大多将其划分为 4KB/页 3.4 Java Heap Java语言，主要看 Heap 区域，系统参数设置为 -Xms, -Xmx 时，JVM 通常是申请一个连续的虚拟地址。OS预先分配的物理内存空间是 -Xms 的大小， -Xmx 许多空间真正使用的时候才会分配。 32 bit 系统中，1.5GB 的 Heap 区域是比较合适的。64bit 空间不会受到限制 (JVM也必须换成64bit模式) 4. Disk disk 一直在拖着计算机的后腿, SSD好一些. 5. Cache CPU 有 cache，系统架构有 cache，存储上有cache，分布式上有 cache，数据库上有cache，ORM框架上有cache … Cache 就是 靠近原则 6. 网络与数据库 6.1 Java 基本 IO 只要是内存程序的通信，都可以理解为 Input / Output","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"}]},{"title":"SpringMVC Demo","slug":"java/java-springMVC-mybatis-demo","date":"2016-09-10T23:54:16.000Z","updated":"2021-06-20T04:12:28.297Z","comments":true,"path":"2016/09/11/java/java-springMVC-mybatis-demo/","link":"","permalink":"http://www.iequa.com/2016/09/11/java/java-springMVC-mybatis-demo/","excerpt":"写一个入门整洁的编写 java 后端程序的代码架, 主要使用了 java + springMvc + mybatis + logback + spring task 等技术. blair’s github springMvc_demo","text":"写一个入门整洁的编写 java 后端程序的代码架, 主要使用了 java + springMvc + mybatis + logback + spring task 等技术. blair’s github springMvc_demo Starting Point 为一些新手和我个人备份，写一个入门整洁的编写 java 后端程序的代码架. 写这样的程序，最重要的是 整洁与约定规范 而不是多么高深的技术，让接手你代码的人不痛苦，这才是成功 Involved Tech Java Restful SpringMVC Mybatis logback Spring-task logback : 一个“可靠、通用、快速而又灵活的Java日志框架”。 Spring-task 编写非web程序，仅仅是后台需要定时跑的任务，经常被用到。 How to Run 1. 修改数据库连接信息 编辑 ~/resources/props/db.properties 将其中的 12345# main mysql lib dataSourcemain.jdbc.driverClassName=com.mysql.jdbc.Drivermain.jdbc.url=jdbc:mysql://192.168.***.**:3306/testdb01main.jdbc.username=your_usernamemain.jdbc.password=your_password 改为你自己的 dataSource 连接信息 2. 数据库中建立你用到的表 参见语句 ~/resources/sql/projects.sql 在你的 数据库 中执行其中语句，建立 table user. 3. 确认需要的环境已准备好 确认 ~/resources/logback.xml 中，日志的打印路径，是否适合你的环境 我这里是 /data0/www/logs/ ， 如有需要改变，请自行更改。（如果为 windows 环境，请注意路径是否正确） 4. 编译-打包-启动jetty 12345678910111213141516171819202122➜ github lltotal 0drwxr-xr-x 13 hp staff 442 Sep 10 14:03 language/➜ github cd language/java/springMVC_demo➜ springMVC_demo git:(master) ✗ lltotal 24-rw-r--r-- 1 hp staff 665 Sep 11 15:52 README.md-rw-r--r-- 1 hp staff 10712 Sep 11 15:10 pom.xmldrwxr-xr-x 4 hp staff 136 Sep 10 13:30 src/➜ springMVC_demo git:(master) ✗ mvn clean➜ springMVC_demo git:(master) ✗ mvn compile➜ springMVC_demo git:(master) ✗ mvn clean package➜ springMVC_demo git:(master) ✗ mvn jetty:run[INFO] Scanning for projects...[INFO][INFO] ------------------------------------------------------------------------[INFO] Building x_demo Maven Webapp 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] FrameworkServlet &#x27;mvc-dispatcher&#x27;: initialization completed in 572 ms[INFO] Started SelectChannelConnector@0.0.0.0:8080[INFO] Started Jetty Server[INFO] Starting scanner at interval of 5 seconds. 当然你也可以通过 IDEA -&gt; Maven Projects -&gt; Plugins -&gt; jetty:run 启动 (或者 Tomcat 启动) 启动成功后，这时你可以在你的浏览器分别访问以下接口，查看效果了 12345678910111213141516171819http://localhost:8080/http://localhost:8080/user/getusershttp://localhost:8080/user/addusershttp://localhost:8080/user/getusershttp://localhost:8080/user/getuser/2------http://localhost:8080/user/getuser/2&#123; &quot;status&quot;: 0, &quot;errmsg&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;id&quot;: 2, &quot;firstName&quot;: &quot;Andy&quot;, &quot;lastName&quot;: &quot;Wong&quot;, &quot;age&quot;: 31 &#125;&#125; 在你测试的时候，如果你想在浏览器中看到格式化后的json，请自行安装 chrome 相关的json插件等。 Desc 展示了前后端开发如何用json进行交互的主流方法，标志状态位与错误信息，返回结果呈现给前端，用一个 JsonResult 类来封装，同时在 web 层，用MappingJackson2HttpMessageConverter 配置，可自动将 Map&lt;String, Object&gt; 转换为 json 呈现给前端等。 代码编写比较规范，用了主流日志框架，将 info 与 error 日志分开打印，不吞异常。分层规范，还特意写了两个数据源如何配置的样例等。 DAO 层不写实现，只写接口，用 Mybatis 来承接，类对象与数据库表 直接自动转换识别，包含了 数据库表字段下划线与java类字段驼峰标识 如何匹配等。 Attentions 注意： 版本控制中，涉及的敏感 库地址，用户名，密码 等 不上传. Reference blair’s github springMVC_demo","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://www.iequa.com/tags/spring/"}]},{"title":"Spark Machine Learning p3 - 数据的获取、处理与准备","slug":"spark/spark-machine-learning-p3","date":"2016-09-09T08:07:21.000Z","updated":"2021-06-20T04:12:28.355Z","comments":true,"path":"2016/09/09/spark/spark-machine-learning-p3/","link":"","permalink":"http://www.iequa.com/2016/09/09/spark/spark-machine-learning-p3/","excerpt":"《Spark Machine Learing》 Reading Notes ： Spark上数据的获取、处理与准备","text":"《Spark Machine Learing》 Reading Notes ： Spark上数据的获取、处理与准备 MovieStream 包括网站提供的电影数据、用户的服务信息数据以及行为数据。 这些数据涉及电影和相关内容（比如标题、分类、图片、演员和导演）、用户信息（比如用户属性、位置和其他信息）以及用户活动数据（比如浏览数、预览的标题和次数、评级、评论，以及如赞、分享之类的社交数据，还有包括像Facebook和Twitter之类的社交网络属性）。 其外部数据来源则可能包括天气和地理定位信息，以及如IMDB和Rotten Tomators之类的第三方电影评级与评论信息等。 一个预测精准的好模型有着极高的商业价值（Netflix Prize 和 Kaggle 上机器学习比赛的成功就是很好的见证） focus on 数据的处理、清理、探索和可视化方法； 原始数据转换为可用于机器学习算法特征的各种技术； 学习如何使用外部库或Spark内置函数来正则化输入特征. 1. 获取公开数据集 UCL机器学习知识库 包括近300个不同大小和类型的数据集，可用于分类、回归、聚类和推荐系统任务。数据集列表位于：http://archive.ics.uci.edu/ml/。 Amazon AWS公开数据集 包含的通常是大型数据集，可通过Amazon S3访问。这些数据集包括人类基因组项目、Common Crawl网页语料库、维基百科数据和Google Books Ngrams。 相关信息可参见：http://aws.amazon.com/publicdatasets/。 Kaggle 这里集合了Kaggle举行的各种机器学习竞赛所用的数据集。 它们覆盖分类、回归、排名、推荐系统以及图像分析领域，可从Competitions区域下载：http://www.kaggle.com/competitions。 KDnuggets 这里包含一个详细的公开数据集列表，其中一些上面提到过的。 该列表位于：http://www.kdnuggets.com/datasets/index.html。 MovieLens 100k数据集 MovieLens 100k数据集包含表示多个用户对多部电影的10万次评级数据，也包含电影元数据和用户属性信息 http://files.grouplens.org/datasets/movielens/ml-100k.zip ml-100k/ u.user（用户属性文件）、u.item（电影元数据）和u.data（用户对电影的评级） 1234567&gt;unzip ml-100k.zip inflating: ml-100k/allbut.pl inflating: ml-100k/mku.sh inflating: ml-100k/README ... inflating: ml-100k/ub.base inflating: ml-100k/ub.test u.user user.id、age、gender、occupation、ZIP code 123456&gt;head -5 u.user 1|24|M|technician|85711 2|53|F|other|94043 3|23|M|writer|32067 4|24|M|technician|43537 5|33|F|other|15213 u.item movie id、title、release date以及若干与IMDB link和电影分类相关的属性 123456&gt;head -5 u.item 1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20 Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0 2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title- exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0 3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title- exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0 4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title- exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0 5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title- exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0 u.data user id、movie id、rating（从1到5）和timestamp属性，各属性间用制表符（\\t）分隔 123456&gt;head -5 u.data196 242 3 881250949186 302 3 89171774222 377 1 878887116244 51 2 880606923166 346 1 886397596 2. 探索与可视化数据 IPython的安装方法可参考如下指引：http://ipython.org/install.html。 如果这是你第一次使用IPython，这里有一个教程：http://ipython.org/ipython-doc/stable/interactive/tutorial.html。 1&gt;IPYTHON=1 IPYTHON_OPTS=&quot;--pylab&quot; ./bin/pyspark 终端里的IPython 2.3.1 – An enhanced Interactive Python和Using matplotlib backend: MacOSX输出行表示IPython和pylab均已被PySpark启用。 1234567891011Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &#x27;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 1.5.2 /_/Using Python version 2.7.10 (default, Jul 14 2015 19:46:27)SparkContext available as sc, HiveContext available as sqlContext.In [1]: 可以将样本代码输入到IPython终端，也可通过IPython提供的Notebook 应用来完成。Notebook支持HTML显示，且在IPython终端的基础上提供了一些增强功能，如即时绘图、HTML标记，以及独立运行代码片段的功能。 IPython Notebook 使用指南：http://ipython.org/ipython-doc/stable/interactive/notebook.html 2.1 探索用户数据 python123user_data = sc.textFile(&quot;/Users/hp/ghome/ml/ml-100k/u.user&quot;)user_data.first()user_data.take(5) python123456user_fields = user_data.map(lambda line: line.split(&quot;|&quot;))num_users = user_fields.map(lambda fields: fields[0]).count()num_genders = user_fields.map(lambda fields: fields[2]).distinct().count()num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count()print &quot;Users: %d, genders: %d, occupations: %d, ZIP codes: %d&quot; % (num_users, num_genders, num_occupations, num_zipcodes) Output 1Users: 943, genders: 2, occupations: 21, ZIP codes: 795 matplotlib的hist个直方图，以分析用户年龄的分布情况： age distribution 1234ages = user_fields.map(lambda x: int(x[1])).collect()hist(ages, bins=20, color=&#x27;lightblue&#x27;, normed=True)fig = matplotlib.pyplot.gcf()fig.set_size_inches(16, 10) occupation distribution 123456789101112131415count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()x_axis1 = np.array([c[0] for c in count_by_occupation])y_axis1 = np.array([c[1] for c in count_by_occupation])print x_axis1[u&#x27;administrator&#x27; u&#x27;retired&#x27; u&#x27;lawyer&#x27; u&#x27;none&#x27; u&#x27;student&#x27; u&#x27;technician&#x27; u&#x27;programmer&#x27; u&#x27;salesman&#x27; u&#x27;homemaker&#x27; u&#x27;writer&#x27; u&#x27;doctor&#x27; u&#x27;entertainment&#x27; u&#x27;marketing&#x27; u&#x27;executive&#x27; u&#x27;scientist&#x27; u&#x27;educator&#x27; u&#x27;healthcare&#x27; u&#x27;librarian&#x27; u&#x27;artist&#x27; u&#x27;other&#x27; u&#x27;engineer&#x27;]print y_axis1[ 79 14 12 9 196 27 66 12 7 45 7 18 26 32 31 95 16 51 28 105 67] plt.xticks(rotation=30)之类的代码 是 美化条形图 1234567891011pos = np.arange(len(x_axis))width = 1.0ax = plt.axes()ax.set_xticks(pos + (width / 2))ax.set_xticklabels(x_axis)plt.bar(pos, y_axis, width, color=&#x27;lightblue&#x27;)plt.xticks(rotation=30)fig = matplotlib.pyplot.gcf()fig.set_size_inches(16, 10) Spark对RDD提供了一个名为countByValue的便捷函数 python123456count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()print &quot;Map-reduce approach:&quot;print dict(count_by_occupation2)print &quot;&quot;print &quot;countByValue approach:&quot;print dict(count_by_occupation) 2.2 探索电影数据 1234movie_data = sc.textFile(&quot;/PATH/ml-100k/u.item&quot;)print movie_data.first()num_movies = movie_data.count()print &quot;Movies: %d&quot; % num_movies 1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0 Movies: 1682 12345def convert_year(x): try: return int(x[-4:]) except: return 1900 1234567891011movie_fields = movie_data.map(lambda lines: lines.split(&quot;|&quot;))years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))years_filtered = years.filter(lambda x: x != 1900)movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()values = movie_ages.values()bins = movie_ages.keys()hist(values, bins=bins, color=&#x27;lightblue&#x27;, normed=True)fig = matplotlib.pyplot.gcf()fig.set_size_inches(16,10) 2.3 探索评级数据 1234rating_data = sc.textFile(&quot;/Users/hp/ghome/ml/ml-100k/u.data&quot;)print rating_data.first()num_ratings = rating_data.count()print &quot;Ratings: %d&quot; % num_ratings 1234567891011121314rating_data = rating_data.map(lambda line: line.split(&quot;\\t&quot;))ratings = rating_data.map(lambda fields: int(fields[2]))max_rating = ratings.reduce(lambda x, y: max(x, y))min_rating = ratings.reduce(lambda x, y: min(x, y))mean_rating = ratings.reduce(lambda x, y: x + y) / num_ratingsmedian_rating = np.median(ratings.collect())ratings_per_user = num_ratings / num_usersratings_per_movie = num_ratings / num_moviesprint &quot;Min rating: %d&quot; % min_ratingprint &quot;Max rating: %d&quot; % max_ratingprint &quot;Average rating: %2.2f&quot; % mean_ratingprint &quot;Median rating: %d&quot; % median_ratingprint &quot;Average # of ratings per user: %2.2f&quot; % ratings_per_userprint &quot;Average # of ratings per movie: %2.2f&quot; % ratings_per_movie Max rating: 5 Average rating: 3.00 Median rating: 4 Average # of ratings per user: 106.00 Average # of ratings per movie: 59.00 Spark对RDD也提供一个名为states的函数。该函数包含一个数值变量用于做类似的统计： 1234ratings.stats()其输出为：(count: 100000, mean: 3.52986, stdev: 1.12566797076, max: 5.0, min: 1.0) 12345678910111213141516count_by_rating = ratings.countByValue()x_axis = np.array(count_by_rating.keys())y_axis = np.array([float(c) for c in count_by_rating.values()])# 这里对y轴正则化，使它表示百分比y_axis_normed = y_axis / y_axis.sum()pos = np.arange(len(x_axis))width = 1.0ax = plt.axes()ax.set_xticks(pos + (width / 2))ax.set_xticklabels(x_axis)plt.bar(pos, y_axis_normed, width, color=&#x27;lightblue&#x27;)plt.xticks(rotation=30)fig = matplotlib.pyplot.gcf()fig.set_size_inches(16, 10) 各个用户评级次数的分布情况 12345678910111213141516user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))).groupByKey()user_ratings_byuser = user_ratings_grouped.map(lambda (k, v): (k, len(v)))user_ratings_byuser.take(10)Out[91]:[(2, 62), (4, 24), (6, 211), (8, 59), (10, 184), (12, 51), (14, 98), (16, 140), (18, 277), (20, 48)] 1234user_ratings_byuser_local = user_ratings_byuser.map(lambda (k, v): v).collect()hist(user_ratings_byuser_local, bins=200, color=&#x27;lightblue&#x27;, normed=True)fig = matplotlib.pyplot.gcf()fig.set_size_inches(16,10) 3. 处理与转换数据 非规整数据和缺失数据的填充 4. 从数据中提取有用特征 在完成对数据的初步探索、处理和清理后，便可从中提取可供机器学习模型训练用的特征。 特征（feature）指那些用于***模型训练的变量***。每一行数据包含可供提取到训练样本中的各种信息。 几乎所有机器学习模型都是与用向量表示的数值特征打交道；需将原始数据转换为数值。 特征可以概括地分为如下几种。 数值特征（numerical feature）：这些特征通常为实数或整数，比如之前例子中提到的年龄。 类别特征（categorical feature）：我们数据集中的用户性别、职业或电影类别便是这类。 文本特征（text feature）：它们派生自数据中的文本内容，比如电影名、描述或是评论。 其他特征：… 地理位置则可由经纬度或地理散列（geohash）表示。 4.1 数值特征 原始的数值和一个数值特征之间的区别是什么？ 机器学习模型中所学习的是各个特征所对应的向量的权值。这些权值在特征值到输出或是目标变量（指在监督学习模型中）is very important。 当数值特征仍处于原始形式时，其可用性相对较低，但可以转化为更有用的表示形式。 如 (位置信息 : 原始位置信息（比如用经纬度表示的），信息可用性很低。 然若对位置进行聚合（比如聚焦为一个city or country），和特定输出 之间存在某种关联。 4.2 类别特征 将类别特征表示为数字形式，常可借助 k 之1（1-of-k）方法进行 比如，可取occupation 所有可能取值： 12all_occupations = user_fields.map(lambda fields: fields[3]). distinct().collect()all_occupations.sort() 然可依次对各可能的职业分配序号（注意 从0开始编号）： 12345678idx = 0all_occupations_dict = &#123;&#125;for o in all_occupations: all_occupations_dict[o] = idx idx +=1# 看一下“k之1”编码会对新的例子分配什么值print &quot;Encoding of &#x27;doctor&#x27;: %d&quot; % all_occupations_dict[&#x27;doctor&#x27;]print &quot;Encoding of &#x27;programmer&#x27;: %d&quot; % all_occupations_dict[&#x27;programmer&#x27;] 其输出如下： 12Encoding of &#x27;doctor&#x27;: 2Encoding of &#x27;programmer&#x27;: 14 4.3 派生特征 从原始数据派生特征的例子包括计算平均值、中位值、方差、和、差、最大值或最小值以及计数。从电影的发行年份和当前年份派生了新的movie age特征的。这类转换背后的想法常常是对数值数据进行某种概括，并期望它能让模型学习更容易。 数值特征到类别特征的转换也很常见，比如划分为区间特征。进行这类转换的变量常见的有年龄、地理位置和时间。 如 ： 将时间戳转为类别特 电影评级发生的时间 [‘afternoon’, ‘evening’, ‘morning’, ‘morning’, ‘morning’] 4.4 文本特征 文本特征也是一种类别特征或派生特征 NLP 便是专注于文本内容的处理、表示和建模的一个领域。 介绍一种简单且标准化的文本特征提取方法。该方法被称为词袋（bag-of-word）表示法。 词袋法将一段文本视为由其中的文本或数字组成的集合，其处理过程如下。 bag-of-word (1) 分词（tokenization） 首先会应用某些分词方法来将文本分隔为一个由词（一般如单词、数字等）组成的集合。 (2) 删除停用词（stop words removal) 删除常见的单词，比如the、and和but（这些词被称作停用词）。 (3) 提取词干（stemming）： 是指将各个词简化为其基本的形式或者干词。常见的例子如复数变为单数（比如dogs变为dog等）。提取的方法有很多种，文本处理算法库中常常会包括多种词干提取方法。 (4) 向量化（vectorization） ： 向量来表示处理好的词。二元向量可能是最为简单的表示方式。它用1和0来分别表示是否存在某个词。从根本上说，这与之前提到的 k 之1编码相同。与 k 之1相同，它需要一个词的字典来实现词到索引序号的映射。随着遇到的词增多，各种词可能达数百万。由此，使用稀疏矩阵来表示就很关键。这种表示只记录某个词是否出现过，从而节省内存和磁盘空间，以及计算时间。 提取简单的文本特征 参见 : http://www.ituring.com.cn/tupubarticle/5567 现在每一个电影标题都被转换为一个稀疏向量。 4.5 正则化特征 在将特征提取为向量形式后，一种常见的预处理方式是将数值数据正则化（normalization）。其背后的思想是将各个数值特征进行转换，以将它们的值域规范到一个标准区间内。正则化的方法有如下几种。 正则化特征：这实际上是对数据集中的单个特征进行转换。比如减去平均值（特征对齐）或是进行标准的正则转换（以使得该特征的平均值和标准差分别为0和1）。 正则化特征向量：这通常是对数据中的某一行的所有特征进行转换，以让转换后的特征向量的长度标准化。也就是缩放向量中的各个特征以使得向量的范数为1（常指一阶或二阶范数）。 向量正则化可通过numpy的norm函数来实现。具体来说，先计算一个随机向量的二阶范数，然后让向量中的每一个元素都除该范数，从而得到正则化后的向量： 12345678np.random.seed(42)x = np.random.randn(10)norm_x_2 = np.linalg.norm(x)normalized_x = x / norm_x_2print &quot;x:\\n%s&quot; % xprint &quot;2-Norm of x: %2.4f&quot; % norm_x_2print &quot;Normalized x:\\n%s&quot; % normalized_xprint &quot;2-Norm of normalized_x: %2.4f&quot; % np.linalg.norm(normalized_x) 其输出应该如下（上面将随机种子的值设为42，保证每次运行的结果相同）： 12345x: [ 0.49671415 -0.1382643 0.64768854 1.52302986 -0.23415337 -0.234136961.57921282 0.76743473 -0.46947439 0.54256004]2-Norm of x: 2.5908Normalized x: [ 0.19172213 -0.05336737 0.24999534 0.58786029 -0.09037871 -0.09037237 0.60954584 0.29621508 -0.1812081 0.20941776]2-Norm of normalized_x: 1.0000 用 MLlib 正则化特征 Spark在其MLlib机器学习库中内置了一些函数用于特征的缩放和标准化。它们包括供标准正态变换的StandardScaler，以及提供与上述相同的特征向量正则化的 Normalizer。 比较一下MLlib的Normalizer与我们自己函数的结果： 123from pyspark.mllib.feature import Normalizernormalizer = Normalizer()vector =sc.parallelize([x]) 在导入所需的类后，会要初始化Normalizer（其默认使用与之前相同的二阶范数）。注意用Spark时，大部分情况下Normalizer所需的输入为一个RDD（它包含numpy数值或MLlib向量）。作为举例，我们会从x向量创建一个单元素的RDD。 之后将会对我们的RDD调用Normalizer的transform函数。由于该RDD只含有一个向量，可通过first函数来返回向量到驱动程序。接着调用toArray函数来将该向量转换为numpy数组： 1234567normalized_x_mllib = normalizer.transform(vector).first().toArray()#最后来看一下之前打印过的那些值，并做个比较：print &quot;x:\\n%s&quot; % xprint &quot;2-Norm of x: %2.4f&quot; % norm_x_2print &quot;Normalized x MLlib:\\n%s&quot; % normalized_x_mllibprint &quot;2-Norm of normalized_x_mllib: %2.4f&quot; % np.linalg.norm(normalized_x_mllib) 相比自己编写的函数，使用 MLlib内置的函数 更方便 4.6 用软件包提取特征 特征提取可借助的软件包有scikit-learn、gensim、scikit-image、matplotlib、Python的NLTK、Java编写的OpenNLP以及用Scala编写的Breeze和Chalk。Breeze自Spark 1.0开始就成为Spark的一部分了。Breeze有线性代数功能。 5. 小结 了解 如何导入、处理和清理数据，如何将原始数据转为特征向量以供模型训练的常见方法","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Spark Machine Learning p2 - 设计机器学习系统","slug":"spark/spark-machine-learning-p2-design-ml-sys","date":"2016-09-08T02:07:21.000Z","updated":"2021-06-20T04:12:28.343Z","comments":true,"path":"2016/09/08/spark/spark-machine-learning-p2-design-ml-sys/","link":"","permalink":"http://www.iequa.com/2016/09/08/spark/spark-machine-learning-p2-design-ml-sys/","excerpt":"《Spark Machine Learing》 Reading Notes ： 如何设计机器学习系统 moiveStream","text":"《Spark Machine Learing》 Reading Notes ： 如何设计机器学习系统 moiveStream img { display: block !important; height: 400px; width: 500px; margin-left: 180px !important; } 1. 原始 MovieStream 介绍 1.1 个性化 个性化 是根据各因素来改变用户体验和呈现给用户内容。这些因素可能包括用户的行为数据和外部因素。 推荐（recommendation）, 常指向用户呈现一个他们可能感兴趣的物品列表。 个性化和推荐十分相似, 根据因素改变搜索的呈现不同用户不同内容，这是隐式个性化 1.2 客户细分 目标营销用与推荐类似的方法从用户群中找出要营销的对象。一般来说，推荐和个性化的应用场景都是一对一，根据用户的特征进行分组，并可能参考行为数据。也可能使用了某种机器学习模型，比如 聚类。 1.3 预测建模 预测性分析 从某种意义上说还覆盖推荐、个性化和目标营销。用预测建模（predictive modeling）来表示其他做预测的模型。借助活动记录、收入数据以及内容属性，MovieStream 可以创建一个回归模型（regression model）来预测新电影的市场表现。 另外，我们也可使用分类模型（classificaiton model）来对只有部分数据的新电影自动分配标签、关键字或分类。 2. 机器学习模型的种类 supervised learning：这种方法使用已标记数据来学习。推荐引擎、回归和分类便是例子。它们所使用的标记数据可以是用户对电影的评级（对推荐来说）、电影标签（对上述分类例子来说）或是收入数字（对回归预测来说）. unsupervised learning：一些模型的学习过程不需要标记数据，我们称其为无监督学习。这类模型试图学习或是提取数据背后的结构或从中抽取最为重要的特征。聚类、降维和文本处理的某些特征提取都是无监督学习. 3. 数据驱动ML系统的组成 3.1 数据获取与存储 MovieStream 的数据通常来自用户活动. 要存储的数据包括：原始数据、即时处理后的数据，以及可用于生产系统的最终建模结果。 数据存储 文件系统 : 如 HDFS、Amazon S3 等； SQL数据库 : 如 MySQL、PostgreSQL； NoSQL : -如 HBase、Cassandra、Mongodb； 搜索引擎 : 如 Solr 、Elasticsearch； 流数据 : – 如 Kafka、Flume、Amazon Kinesis 3.2 数据清理与转换 大部分机器学习模型所处理的都是 feature。特征 通常是输入变量所对应的可用于模型的数值表示。 原始数据 预处理 几种 情况 数据过滤 合并多个数据源 数据汇总 对许多模型类型来说，这种表示就是包含 数值数据的 向量 or 矩阵。 将类别数据（比如地理位置所在的国家或是电影的类别）编码为对应的数值表示。 文本数据提取有用信息。 处理图像或是音频数据。 数值数据常被转换为类别数据以减少某个变量的可能值的数目。例如将年龄分为 601, 602… 对特征进行正则化、标准化，以保证同一模型的不同输入变量的值域相同。 这些数据清理、探索、聚合和转换步骤，都能通过Spark核心API、SparkSQL引擎和其他外部Scala、Java或Python包做到。借助 Spark 的 Hadoop功能 还能实现上述多种存储系统上的读写。 3.3 模型训练与测试回路 当数据已转换为可用于模型的形式，便可开始模型的训练和测试。 在训练数据集上运行模型并在测试数据集（即为评估模型而预留的数据，在训练阶段模型没接触过该数据）上测试其效果，这个过程一般相对直接，被称作交叉验证（cross-validation）。 Spark MLlib 来实现对各种机器学习方法的模型训练、评估以及交叉验证。 3.4 模型部署与整合 通过训练测试循环找出最佳模型后，要让它能得出可付诸实践的预测，还需将其部署到生产系统中。 这个过程一般要将已训练的模型导入特定的数据存储中。 3.5 模型监控与反馈 监控机器学习系统在生产环境下的表现十分重要。 同样值得注意的是，模型准确度和预测效果只是现实中系统表现的一部分。 我们可以尽可能在生产系统中部署不同的模型，通过调整它们而优化业务指标。实践中，这通常通过在线分割测试（live split test）进行。 模型反馈（model feedback），指通过用户的行为来对模型的预测进行反馈的过程。在现实系统中，模型的应用将影响用户的决策和潜在行为，从而反过来将从根本上改变模型自己将来的训练数据。 3.6 批处理/实时方案选择 常见的批处理方法。模型用所有数据或一部分数据进行周期性的重新训练。由于上述流程会花费一定的时间，这就使得批处理方法难以在新数据到达时立即完成模型的更新。 存在一类名为在线学习（online learning）的机器学习方法。它们在新数据到达时便能立即更新模型，从而使实时系统成为可能。常见的例子有对线性模型的在线优化算法，如随机梯度下降法。 4. 机器学习系统架构 机器学习流程示意图的内容： 收集与用户、用户行为和电影标题有关的数据； 将这些数据转为特征； 模型训练，包括训练-测试和模型选择环节； 将已训练模型部署到在线服务系统，并用于离线处理； 通过推荐和目标页面将模型结果反馈到MovieStream站点； 将模型结果返回到MovieStream的个性化营销渠道； 使用离线模型来为MovieSteam的各个团队提供工具，以帮助其理解用户的行为、内容目录的特点和业务收入的驱动因素。","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"},{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"JVM 跨平台与字节码原理初步","slug":"java/java-special-arms-p3-jvm","date":"2016-08-16T08:54:16.000Z","updated":"2021-06-20T04:12:28.298Z","comments":true,"path":"2016/08/16/java/java-special-arms-p3-jvm/","link":"","permalink":"http://www.iequa.com/2016/08/16/java/java-special-arms-p3-jvm/","excerpt":"Java特种兵 - JVM 跨平台与字节码原理，Reading Notes","text":"Java特种兵 - JVM 跨平台与字节码原理，Reading Notes 用到 JVM 的场景 Out of memory 时，团队高手不在 系统服务器架构，老大问你 投入多少服务器成本，VM 分配多大， 如何分配? 1. javap 命令 通过这种方式认知比 Java 更低一个抽象层次的逻辑，虚指令有时候更好解释问题。 1234567public class StringTest &#123; public static void test1() &#123; String a = &quot;a&quot; + &quot;b&quot; + 1; String b = &quot;ab1&quot;; System.out.println(a == b); // true 编译时优化 &#125;&#125; 12345678910➜ p3jvm git:(master) ✗ pwd/Users/hp/ghome/github/language/java/jsarms/p3jvm➜ p3jvm git:(master) ✗ javacUsage: javac &lt;options&gt; &lt;source files&gt;where possible options include: -g Generate all debugging info -g:none Generate no debugging info -g:&#123;lines,vars,source&#125; Generate only some debugging info -nowarn Generate no warnings -verbose Output messages about what the compiler is doing 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107➜ p3jvm git:(master) ✗ javac -g:vars,lines StringTest.java➜ p3jvm git:(master) ✗ javap -verbose StringTestClassfile /Users/hp/ghome/github/language/java/jsarms/p3jvm/StringTest.class Last modified Aug 16, 2016; size 559 bytes MD5 checksum 772d18512cb982c953e7db8c72522918public class StringTest minor version: 0 major version: 51 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #6.#21 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = String #22 // ab1 #3 = Fieldref #23.#24 // java/lang/System.out:Ljava/io/PrintStream; #4 = Methodref #25.#26 // java/io/PrintStream.println:(Z)V #5 = Class #27 // StringTest #6 = Class #28 // java/lang/Object #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 LocalVariableTable #12 = Utf8 this #13 = Utf8 LStringTest; #14 = Utf8 test1 #15 = Utf8 a #16 = Utf8 Ljava/lang/String; #17 = Utf8 b #18 = Utf8 StackMapTable #19 = Class #29 // java/lang/String #20 = Class #30 // java/io/PrintStream #21 = NameAndType #7:#8 // &quot;&lt;init&gt;&quot;:()V #22 = Utf8 ab1 #23 = Class #31 // java/lang/System #24 = NameAndType #32:#33 // out:Ljava/io/PrintStream; #25 = Class #30 // java/io/PrintStream #26 = NameAndType #34:#35 // println:(Z)V #27 = Utf8 StringTest #28 = Utf8 java/lang/Object #29 = Utf8 java/lang/String #30 = Utf8 java/io/PrintStream #31 = Utf8 java/lang/System #32 = Utf8 out #33 = Utf8 Ljava/io/PrintStream; #34 = Utf8 println #35 = Utf8 (Z)V// 以上是 Constant pool， 仅仅是陈列操作，并没有开始执行任务，看下面开始&#123; public StringTest(); flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 // 所有方法都会有。 // stack 为栈顶的单位大小 (每个大小为 1 slot，4 byte) // locals=1，非静态方法，本地变量增加 this 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 1: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this LStringTest; public static void test1(); flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=2, args_size=0 // stack=3，本地栈slot个数为3，String需要load，String.out 占用一个再。当对比发生 boolean 时，两个String引用栈顶pop // locals=2， 因为只有两个 String // args_size=0，方法没有入口参数 0: ldc #2 // String ab1 // 引用常量池内容 2: astore_0 // 将栈顶引用值，写入第 1 个 slot 所在的本地变量 3: ldc #2 // String ab1 5: astore_1 6: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; // 获取静态域，放入栈顶，此时静态域是 System.out 对象 9: aload_0 10: aload_1 11: if_acmpne 18 14: iconst_1 15: goto 19 18: iconst_0 19: invokevirtual #4 // Method java/io/PrintStream.println:(Z)V 22: return LineNumberTable: line 4: 0 line 5: 3 line 6: 6 line 7: 22 LocalVariableTable: Start Length Slot Name Signature 3 20 0 a Ljava/lang/String; 6 17 1 b Ljava/lang/String; // 本地变量列表 LocalVariableTable. from javac -g:vars StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 18 locals = [ class java/lang/String, class java/lang/String ] stack = [ class java/io/PrintStream ] frame_type = 255 /* full_frame */ offset_delta = 0 locals = [ class java/lang/String, class java/lang/String ] stack = [ class java/io/PrintStream, int ]&#125;➜ p3jvm git:(master) ✗ 2. Java 字节码结构 javac 命令本身只是一个引导器，它引导编译器程序的运行。编译器本身是一个java程序 com.sun.tools.javac.main.JavaCompiler, 该类完成 java 源文件 的 Parser、Annotation process、检查、泛型处理、语法转换等，最终胜出 Class 文件。 Java 字节码文件主体结构: Class 文件头部 Constant pool 当前Clas的描述信息 属性列表 方法列表 …","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://www.iequa.com/tags/jvm/"}]},{"title":"Decision Tree part1","slug":"ml/decisionTree-part1","date":"2016-08-16T08:43:21.000Z","updated":"2021-06-20T04:12:28.316Z","comments":true,"path":"2016/08/16/ml/decisionTree-part1/","link":"","permalink":"http://www.iequa.com/2016/08/16/ml/decisionTree-part1/","excerpt":"决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值。","text":"决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值。 1. Classification Introduce 分类有着广泛的应用，如医学疾病判别、垃圾邮件过滤、垃圾短信拦截、客户分析等等。分类问题可以分为两类 1.1 归类 : 离散 归类 是指对离散数据的分类，比如对根据一个人的笔迹判别这个是男还是女，这里的 Category 只有两个，类别是离散的集合空间{男，女}的。 1.2 预测 : 连续 预测 是指对连续数据的分类，比如预测明天8点天气的湿度情况，天气的湿度在随时变化，8点时的天气是一个具体值，它不属于某个有限集合空间。预测也叫回归分析，在金融领域有着广泛应用。 虽然对离散数据和连续数据的处理方式有所不同，但其实他们之间相互转化，比如我们可以根据比较的某个特征值判断，如果值大于0.5就认定为男性，小于等于0.5就认为是女性，这样就转化为连续处理方式；将天气湿度值分段处理也就转化为离散数据。 数据分类 分两个步骤： 构造模型，利用训练数据集 训练 分类器； 利用建好的分类器模型对测试数据进行分类。 好的分类器具有很好的泛化能力，即它不仅在训练数据集上能达到很高的正确率，而且能在未见过得测试数据集也能达到较高的正确率。如果一个分类器只是在训练数据上表现优秀，但在测试数据上表现稀烂，这个分类器就已经过拟合了，它只是把训练数据记下来了，并没有抓到整个数据空间的特征。 2. Decision Tree’ Classification 代表性的例子说明 : ID 阴晴(F) 温度(F) 湿度(F) 刮风(F) 是否玩（C） 1 sunny hot high false 否 2 sunny hot high true 否 3 overcast hot high false 是 4 rainy mild high false 是 5 rainy cool normal false 是 6 rainy cool normal true 否 7 overcast cool normal true 是 8 sunny mild high false 否 9 sunny cool normal false 是 10 rainy mild normal false 是 11 sunny mild normal true 是 12 overcast mild high true 是 13 overcast hot normal false 是 14 rainy mild high true 否 利用ID3算法中的 Info Gain Feature Selection，递归的学习一棵决策树，得到树结构如下 &lt;img src=&quot;/images/ml/decision-tree/decision-tree-2.png&quot; width=“560” height=“400”/img&gt; 决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个Feature属性上的测试，每个分支代表这个Feature属性在某个值域上的输出，而每个叶节点存放一个 Category 。使用 DT 进行决策的过程就是从 root 开始，测试待分类项中相应的 Feature 属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的 Category 作为决策结果。 Feature Selection，如何量化最优Feature? -&gt; 导致 DT Algorithm 出现了 ID3、C4.5、C5.0、CART 等。 3. Decision Tree’ Build 构造 Decision Tree 的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。 构造决策树的过程本质上就是根据 data-feature 将 数据集(D) 分类的递归过程，我们需要解决的第一个问题就是，当前 数据集(D) 上哪个 Feature 在划分数据分类时起决定性作用 3.1 构造 DT 流程 训练数据集 D=(x(1)，y(1))，(x(2)，y(2))，⋯，(x(m)，y(m))D = \\\\{ (x^{(1)}，y^{(1)}) ， (x^{(2)}，y^{(2)})， ⋯ ， (x^{(m)}，y^{(m)}) \\\\}D=(x(1)，y(1))，(x(2)，y(2))，⋯，(x(m)，y(m)) (Feature用离散值表示) 候选特征集 F=f1，f2，⋯，fnF = \\\\{f^1，f^2， ⋯，f^n \\\\}F=f1，f2，⋯，fn 开始建立 Root节点，将所有训练数据都置于根节点（mmm条样本）。从 feature集合 FFF 中选择一个最优特征 f∗f^∗f∗，按照 f∗f^∗f∗ 取值将 训练数据集(D) 切分成若干子集，使得各个自己有一个在当前条件下最好的分类。 如果子集中样本类别基本相同，那么构建叶节点，并将数据子集划分给对应的叶节点；如果子集中样本类别差异较大，不能被基本正确分类，需要在剩下的特征集合 （F−f∗）（F−{f^∗}）（F−f∗） 中选择新的最优特征，继续对数据子集进行切分。如此递归地进行下去，直至所有数据自己都能被基本正确 Category，或者没有合适的最优特征为止。 这样最终结果是每个子集都被分到叶节点上，对应着一个明确的类别。那么，递归生成的层级结构即为一棵 DT。 3.2 伪代码构造 DT 输入 : 训练数据集 D=(x(1)，y(1))，(x(2)，y(2))，⋯，(x(m)，y(m))D = \\\\{ (x^{(1)}，y^{(1)}) ， (x^{(2)}，y^{(2)})， ⋯ ， (x^{(m)}，y^{(m)}) \\\\}D=(x(1)，y(1))，(x(2)，y(2))，⋯，(x(m)，y(m))(Feature用离散值表示) 候选特征集 F=f1，f2，⋯，fnF = \\\\{f^1，f^2， ⋯，f^n \\\\}F=f1，f2，⋯，fn 输出 : T(D, F) &lt;img src=&quot;/images/ml/decision-tree/decision-tree-3.png&quot; width=“760” height=“400”/img&gt; 决策树学习过程中递归的每一步，在选择最优特征后，根据特征取值切割当前节点的数据集，得到若干数据子集。 算法的时间复杂度是O(k*|D|*log(|D|))，k为属性个数，|D|为记录集D的记录数。 4. Feature Selection 递归地选择最优feature，根据feature取值切割数据集，使得对应的数据子集有一个较好的分类。从伪代码中也可以看出，在决策树学习过程中，最重要的是第07行，即如何选择最优feature？也就是我们常说的feature选择问题。 在这里，希望随着feature选择过程地不断进行，决策树的分支节点所包含的样本尽可能属于同一类别，即希望节点的”纯度（purity）”越来越高。 如子集中的样本都属于同一个类别，是最好的结果；如果说大多数的样本类型相同，只有少部分样本不同，也可以接受。 那么如何才能做到选择的特征对应的样本子集纯度最高呢？ Algorithm Feature 选择方法 Author ID3 Information gain Quinlan. 1986 C4.5 Gain ratio Quinlan. 1993. CART 回归树： 最小二乘分类树： 基尼指数 Gini index Breiman. 1984(Classification and Regression Tree 分类与回归树) ID3 (Iterative Dichotomiser) 4.1 Information Gain 信息增益（Information Gain）衡量 Feature 的重要性是根据当前 Feature 为划分带来多少信息量，带来的信息越多，该 Feature 就越重要，此时节点的”纯度”也就越高。 Infomation Entropy 对一个分类系统来说，假设类别 CCC 可能的取值为 c1，c2，⋯，ckc_1，c_2，⋯，c_kc1​，c2​，⋯，ck​（kkk是类别总数），每一个类别出现的概率分别是 p(c1)，p(c2)，⋯，p(ck)p(c_1)，p(c_2)，⋯，p(c_k)p(c1​)，p(c2​)，⋯，p(ck​)。此时，分类系统的 Entropy 可以表示为: info(C)=−∑i=1kp(c_i)⋅log_2p(c_i)(fml.4.1.1)info(C) = - \\sum_{i=1}^k p(c\\_i) \\cdot log\\_2 p(c\\_i) \\qquad (fml.4.1.1) info(C)=−i=1∑k​p(c_i)⋅log_2p(c_i)(fml.4.1.1) 分类系统的作用就是输出一个特征向量（文本特征、ID特征 等）属于哪个 Category 的值，而这个值可能是 c1，c2，⋯，ckc_1，c_2，⋯，c_kc1​，c2​，⋯，ck​ ，因此这个值所携带的信息量就是 (fml.4.1.1) 公式这么多 Condition Entropy 假设 离散特征 fff 的取值有 III 个，info(C∣f=f_i)info(C|f=f\\_i)info(C∣f=f_i) 表示特征 fff 被取值为 f_if\\_if_i 时的***Condition Entropy***； info(C∣f)info(C|f)info(C∣f) 是指特征 fff 被固定时的 Condition Entropy。二者之间的关系是： \\begin{align} info(C|f) &amp; = p\\_1 \\cdot info(C|f=f\\_1) + p\\_2 \\cdot info(C|f=f\\_2) + ... + p\\_k \\cdot info(C|f=f\\_{k}) \\\\\\ &amp; = \\sum_{i=1}^{I} p\\_i \\cdot info(C|f=f\\_i) \\end{align} \\quad (fml.4.1.2) 假设总样本数有 mmm 条，特征 f=f_if=f\\_if=f_i 时的样本数 m_i，p_i=m_imm\\_i，p\\_i=\\frac{m\\_i}{m}m_i，p_i=mm_i​. 如何求 P(C∣f=f_i)P(C|f=f\\_i)P(C∣f=f_i) ? 二分类情况 : 以二分类为例（正例为1，负例为0），总样本数为 mmm 条，特征 fff 的取值为 III 个，其中特征 f=f_if=f\\_if=f_i 对应的样本数为 m_im\\_im_i 条，其中正例 m_i1m\\_{i1}m_i1 条，负例 m_i0m\\_{i0}m_i0 条 m_i=m_i0+m_i1m\\_i = m\\_{i0} + m\\_{i1}m_i=m_i0+m_i1 。那么有： \\begin{align} info(C|f=f\\_i) &amp; = - \\frac{m\\_{i1}}{m\\_i} \\cdot log\\_{2} \\frac{m\\_{i1}}{m\\_i} - \\frac{m\\_{i0}}{m\\_i} \\cdot log\\_{2} \\frac{m\\_{i0}}{m\\_i} \\end{align} \\qquad (fml.4.1.3) 多分类情况: \\begin{align} info(C|f=f\\_i) = -\\sum\\_{j=0}^{k-1} \\frac{m\\_{ij}}{m\\_i} \\cdot log\\_{2} \\frac{m\\_{ij}}{m\\_i} \\end{align} \\qquad (fml.4.1.4) 公式m_ijm_i\\frac{m\\_{ij}}{m\\_i}m_im_ij​物理含义是当 f=f_if=f\\_if=f_i 且 C=c_jC=c\\_jC=c_j 的概率，即条件概率 p(c_j∣f_i)p(c\\_j|f\\_i)p(c_j∣f_i) 因此，Condition Entropy 计算公式为： \\begin{align} info(C|f) &amp; = \\sum\\_{i=1}^{I} p(f\\_i) \\cdot info(C|f=f\\_i) \\\\\\ &amp; = - \\sum\\_{i=1}^{I} p(f\\_i) \\cdot \\underline { \\sum\\_{j=0}^{k-1} p(c\\_j|f\\_i) \\cdot log\\_2 p(c\\_j|f\\_i) } \\qquad (fml.4.1.5) \\end{align} 特征 fff 给系统带来的 info gain 等于系统原有的 Entropy 与固定特征 fff 的 Condition Entropy 之差，公式表示如下: \\begin{align} IG(F) &amp; = E(C) - E(C|F) \\\\\\ &amp; = -\\sum\\_{i=1}^{k} p(c\\_i) \\cdot \\log\\_{2} p(c\\_i) + \\sum\\_{i=1}^{I} p(f\\_i) \\cdot \\underline { \\sum\\_{j=0}^{k-1} p(c\\_j|f\\_i) \\cdot log\\_2 p(c\\_j|f\\_i) } \\end{align} \\qquad (fml.4.1.6) nnn 表示特征 fff 取值个数，kkk 表示类别 CCC 个数，∑_j=0n−1m_ijm_i⋅log_2m_ijm_i\\sum\\_{j=0}^{n-1} \\frac{m\\_{ij}}{m\\_i} \\cdot log\\_{2} \\frac{m\\_{ij}}{m\\_i}∑_j=0n−1m_im_ij​⋅log_2m_im_ij​ 表示每一个类别对应的 Entropy 。 下面以天气数据为例,通过 Info gain 选择最优 feature 的过程 : 根据 阴晴、温度、湿度 和 刮风 来决定是否出去玩。样本中总共有 14 条记录，取值为 是(9个正样本)、否(5个负样本)，用 S(9+,5−)S(9+,5−)S(9+,5−) 表示. (1). 分类系统的 Entropy : Entropy(S)=info(9,5)=(−914llog_2(914))+(−514llog_2(514))=0.940位(exp.4.1.1)Entropy(S) = info(9,5) = (-\\frac{9}{14} _ llog\\_2 (\\frac{9}{14})) + (- \\frac{5}{14} _ llog\\_2 (\\frac{5}{14})) = 0.940位 \\quad (exp.4.1.1) Entropy(S)=info(9,5)=(−149​l​log_2(149​))+(−145​l​log_2(145​))=0.940位(exp.4.1.1) (2). 如果以特征”阴晴”作为根节点。“阴晴”取值为{sunny, overcast, rainy}, 分别对应的正负样本数分别为(2+,3-), (4+,0-), (3+,2-)，那么在这三个节点上的 info Entropy 分别为： \\begin{align} &amp; Entropy(S| “阴晴”=sunny) = info(2,3) = 0.971位 \\quad(exp.4.1.1) \\\\\\ &amp; Entropy(S| “阴晴”=overcast) = info(4,0) = 0位 \\;\\;\\quad(exp.4.1.2) \\\\\\ &amp; Entropy(S| “阴晴”=rainy) = info(3,2) = 0.971位 \\;\\quad(exp.4.1.3) \\end{align} 以 Feature “阴晴” 为根节点，平均信息值（即 Condition Entropy）为： Entropy(S|“阴晴”) = \\frac{5}{14} \\* 0.971 + \\frac{4}{14} \\* 0 + \\frac{5}{14} \\* 0.971 = 0.693位 \\quad (exp.4.1.4) 以 Feature “阴晴” 为条件，计算得到的 Condition Entropy 代表了期望的信息总量，即对于一个新样本判定其属于哪个类别所必需的信息量。 (3). 计算特征“阴晴”“阴晴”对应的信息增益: IG(“阴晴”)=Entropy(S)−Entropy(S∣“阴晴”)=0.247位(exp.4.1.5)IG( “阴晴”) = Entropy(S) - Entropy(S| “阴晴”) = 0.247位 \\quad\\quad(exp.4.1.5) IG(“阴晴”)=Entropy(S)−Entropy(S∣“阴晴”)=0.247位(exp.4.1.5) 同样的计算方法，可得每个特征对应的信息增益，即 IG(“刮风”)=Entropy(S)−Entropy(S∣“刮风”)=0.048位(exp.4.1.6) IG(“湿度”)=Entropy(S)−Entropy(S∣“湿度”)=0.152位(exp.4.1.7) IG(“温度”)=Entropy(S)−Entropy(S∣“温度”)=0.029位(exp.4.1.8)IG(“刮风”) = Entropy(S) - Entropy(S|“刮风”) = 0.048位 \\qquad\\qquad(exp.4.1.6) \\\\\\ IG(“湿度”) = Entropy(S) - Entropy(S|“湿度”) = 0.152位 \\qquad\\qquad(exp.4.1.7) \\\\\\ IG(“温度”) = Entropy(S) - Entropy(S|“温度”) = 0.029位 \\qquad\\qquad(exp.4.1.8) IG(“刮风”)=Entropy(S)−Entropy(S∣“刮风”)=0.048位(exp.4.1.6) IG(“湿度”)=Entropy(S)−Entropy(S∣“湿度”)=0.152位(exp.4.1.7) IG(“温度”)=Entropy(S)−Entropy(S∣“温度”)=0.029位(exp.4.1.8) 显然，Feature “阴晴” 的 info gain 最大，于是把它作为划分特征。基于“阴晴”对根节点进行划分的结果，如图4.5所示（决策树学习过程部分）。决策树学习算法对子节点进一步划分，重复上面的计算步骤。 &lt;img src=&quot;/images/ml/decision-tree/decision-tree-2.png&quot; width=“560” height=“400”/img&gt; 4.2 Gain ratio 与 Info Gain 不同，Gain ratio 的计算考虑了 F 分裂数据集后所产生的子节点的数量和规模，而忽略任何有关类别的信息。 以 info gain 示例为例，按照 特征“阴晴” 将数据集分裂成3个子集，规模分别为5、4和5，因此不考虑子集中所包含的类别，产生一个分裂信息为： SplitInfo(“阴晴”)=info(5,4,5)=1.577位(exp.4.2.1)SplitInfo(“阴晴”) = info(5,4,5) = 1.577位 \\qquad (exp.4.2.1) SplitInfo(“阴晴”)=info(5,4,5)=1.577位(exp.4.2.1) Split Information Entropy 可简单地理解为表示信息分支所需要的信息量。 那么 Info Gain ratio ： IG_ratio(F)=IG(F)SplitInfo(F)(exp.4.2.2)IG\\_{ratio}(F) = \\frac {IG(F)} {SplitInfo(F)} \\qquad (exp.4.2.2) IG_ratio(F)=SplitInfo(F)IG(F)​(exp.4.2.2) 在这里，特征 “阴晴”的 Gain ratio 为 IGratio(“阴晴”)=0.2471.577=0.157IG_{ratio}( “阴晴”)=\\frac{0.247}{1.577} = 0.157IGratio​(“阴晴”)=1.5770.247​=0.157。减少信息增益方法对取值数较多的特征的影响。(可以减少过拟合，这等于是对 某特征取值过多的一个惩罚) 123&gt;&gt;&gt; -(math.log((5.0/14.0), 2) * (5.0/14.0) * 2 + (4.0/14.0) * (math.log((4.0/14.0), 2)))1.577406282852345&gt;&gt;&gt; Reference 逗比算法工程师、算法杂货铺、52caml 决策树ID3、C4.5、CART算法：信息熵，区别，剪枝理论总结 《机器学习导论》《统计学习方法》","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"decision-tree","slug":"decision-tree","permalink":"http://www.iequa.com/tags/decision-tree/"}]},{"title":"中国人起名学问","slug":"tools/Chinese-named","date":"2016-08-12T23:54:16.000Z","updated":"2021-06-22T06:52:37.114Z","comments":true,"path":"2016/08/13/tools/Chinese-named/","link":"","permalink":"http://www.iequa.com/2016/08/13/tools/Chinese-named/","excerpt":"我对中国人起名学问的自我研究记录","text":"我对中国人起名学问的自我研究记录 1. 五行八字介绍 1.1 五行 五行就是周易中常说的木、火、土、金、水。易经中各类事物均可以五行来区分，如干支、地支、季节、方位、人体、颜色、味道等等。五行有相生与相克的特质，很多的事物可藉由五行的运算，了解元素之兴衰，来判别事物的起伏变化和你一生的好坏吉凶。 1.2 三才 三才配置即天、人、地。中国传统文化理解人立足于大地之上，在天之下，受到上天之眷顾，大地的滋养，而生生不息。亦有顺应天时、地利、人和的行事哲理。三才配置在姓名学中，占有很大的分量。 1.3 五格 五格配置是指天格、地格、人格、外格、总格共五格之间的关系。天格是由祖先流传而来，单独出现对人生没有多大影响;人格是姓名剖象数理的中心所在，对人生的影响最大;人格与地格结合的数理则为基础运。地格主要是36岁前的人生，也叫前运力，外格代表人的外围，吉凶无谓。总格是36岁以后的人生，也是后运力。五格配置在姓名学中占有主要位置。 2. 判断五行所缺 起名字，需要根据五行八字所缺少的五行来用相应的属性字(更准确的说法是用喜用神)来填补，所以需要判断五行所缺。 解析四柱八字的精髓喜用神 喜用神百科 如何知道自己八字的喜神，用神 八字就是从这个平衡理论，去分析人一生的起落 知道了五行的知识，我们也知道很多人的八字五行都不太齐全，有的不只缺一个，还会缺两个甚至三个。比如我们常听说的“五行缺金”之类的话，那么如何知道起名字五行缺什么呢? 2.1 判断五行八字及喜用神 查询五行八字是为了看八字中缺少五行中的哪一行(以喜用神为准)，然后用名中字尽量补齐 : 金、木、水、火、土 以 2015年10月11日13时30分 为例查询检验判断五行八字 以下网址查询结果一致 : 三藏八字查询 周易网 中国健康网 以下网址查询结果一致 : 美名腾 易安居算命网 浮图塔 生辰八字喜用神查询 中华忘忧网 这类网址很多，如 : 美名腾、太平洋亲子网、周易网、中国健康网、三藏八字查询 等等。请自行判断查询结果准确性。 周易网、中国健康网、三藏八字查询结果 与 老爷书 一致 喜用神查询你也可以参考如下网址查询，但准确性请自行裁断。 美名腾 易安居 浮图塔 babyqiming喜用神 优酷视频: 怎么样算八字 什么是八字喜用神 四柱 2.2 五行不缺的情况 五行不缺的情况，参见 喜用神。 五行缺少的情况，参见 喜用神，但在多数情况下，缺少的一个就为它的喜用神，具体参见喜用神查询。 喜用神查询 与 八字五行 查询，请仔细辨别查询的结果可靠性。 3. 姓名笔画吉凶 推荐 31 或者 23画 二十三画 旭日升天，名显四方，渐次进展，终成大业。 （吉） 三十一画 此数大吉，名利双收，渐进向上，大业成就。 （吉） 详见姓名笔画吉凶大全 起名中用字的笔画数参见 康子字典 (非简体字或繁体字) 4. 手机APP的运用 美名腾智能起名 APP … 运用相关手机app，可以更方便的辅助想出好名字 5. 姓名打分测试 打分测试这种网站比较多，根据我个人的经验判别我推荐 三藏网打分测试，根据三藏网查询的结果，可以看到 三才五格的解析以及康熙字典的笔画数目等。 举例如下 : 6. 小结 综上所述: 起名字只需要在名字中，使用喜用神的字 并且 三才五格打分 都比较不错的情况下，方为好名字。 个人意见 : 名字还是 顺耳，脱俗 的名字是最重要的。 起名选择字的方法一如如下图片所示，可参考百度搜索的姓名学 7. Reference article 周易网 中国健康网 美名腾 太平洋亲子网 解析四柱八字的精髓喜用神 不一定准确的喜用神查询 八字五行算命和人生起伏圖 详见姓名笔画吉凶大全 康子字典 三藏网 起名网 … 12345678910111213141516公历：****年**月**日(星期四)11点农历：丙申年四月十三日午时春节：2月8日节前：乙未年节后：丙申年八字：丙申 癸巳 辛丑 甲午五行：火金 水火 金土 木火方位：南西 北南 西中 东南生肖：猴92 三才配置 吉， 天地人外总格 大吉 31画陈俊妃陈俊帆陈泊羊陈泊亦陈音竹...","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"Chinese's name","slug":"Chinese-s-name","permalink":"http://www.iequa.com/tags/Chinese-s-name/"}]},{"title":"Python Data Mining and Analysis environment","slug":"python/language/py-language-1-data-analysis-environment","date":"2016-08-02T08:43:21.000Z","updated":"2021-06-20T04:12:28.221Z","comments":true,"path":"2016/08/02/python/language/py-language-1-data-analysis-environment/","link":"","permalink":"http://www.iequa.com/2016/08/02/python/language/py-language-1-data-analysis-environment/","excerpt":"这是用 Python 进行数据分析挖掘的一小部分，包括 高维数组、数值计算、机器学习、神经网络 和 语言模型等。","text":"这是用 Python 进行数据分析挖掘的一小部分，包括 高维数组、数值计算、机器学习、神经网络 和 语言模型等。 1. Python data analysis intro Python 优雅的语法和动态类型 拥有高级数据结构、OO Functional Programming 解释性、胶水语言，开发效率高 库丰富, NumPy, SciPy, Matplotlib, Pandas 适合于 Scientific Computing、Mathematical Modeling、Data mining … import future feature 12from __futrue__ import print_functionfrom __futrue__ import division install third package 三种方式 下载源代码自行安装 : 安装灵活， 但需要自行解决上级依赖问题。 用 pip 安装 : 比较方便，自动解决上级依赖问题 系统自带的安装方式 : apt-get or brew … 1.1 Install pip pip 是安装python包的工具，提供了安装包，列出已经安装的包，升级包以及卸载包的功能。 pip 是对easy_install的取代，提供了和easy_install相同的查找包的功能 123which pythonwget https://bootstrap.pypa.io/get-pip.pysudo python get-pip.py 12345678910修改pip源 （可选）由于天朝原因,使用pip安装一些模块会特别慢甚至无法下载,因此我们需要修改pip的源到国内的一些镜像地址.cd ~mkdir .pipvim pip.conf添加以下两行[global]index-url = http://pypi.v2ex.com/simple把index-url的值设置为自己实际源的地址.至此pip源修改成功,以后使用pip安装模块时都会从这个源去下载安装. or 1234567891011➜ tar.gz ll-rw-r--r-- 1 hp staff 1138794 Mar 11 16:09 pip-8.1.0.tar.gz-rw-r--r-- 1 hp staff 630700 Mar 11 13:38 setuptools-18.1.tar.gztar -xvf setuptools-18.1.tar.gztar -xvf pip-8.1.0.tar.gzcd setuptools-18.1python setup.py buildpython setup.py installcd pip-9.0.1/python setup.py buildpython setup.py install 9.0.1 见 https://pypi.python.org/pypi/pip ipython sudo pip install --upgrade ipython --ignore-installed six sudo pip install notebook startup ipython notebook 1ipython notebook 1PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook --ip=192.168.140.159&quot; $SPARK_HOME/bin/pyspark 2. Python Tools for data analysis Extension lib introduction Numpy 提供数组支持，以及相应的高效处理函数 Scipy 提供矩阵支持，以及矩阵相关的数值计算模块 Matplotlib 数据可视化工具，作图库 Pandas 数据分析和探索工具 StatsModels 统计建模和计量经济学，包括描述统计，统计模型估计和推断 Scikit-Learn 支持回归，分类，聚类 等强大的机器学习库 Keras 深度学习库，用于建立神经网络以及 deep learning model Gensim 用来做 text topic model 的库 Pillow 旧版的PIL, 图片处理相关 OpenCV video 处理相关 GMPY2 高精度计算相关 2.1 Numpy Numpy 提供了数据功能, 后续 Scipy、Matplotlib、Pandas 等都依赖于它。 12345678# -*- coding: utf-8 -*-# sudo pip install numpyimport numpy as npa = np.array([2, 0, 1, 5])print(a)print(&quot;hello world 3.0 !&quot;) 2.2 Scipy Numpy 提供了多维数据功能，但它只是数组，并不是矩阵。Scipy 提供了真正的矩阵，以及大量矩阵运算的对象和函数。 Scipy 依赖于 Numpy 2.3 Matplotlib 著名的绘图库，主要用于二维绘图，当然也可以进行三维绘图。 sudo pip install matplotlib 2.4 Pandas Pandas 是 Python 下最强大的数据分析 Tool，没有之一。Pandas 构建在 Numpy 之上。 Pandas Function 类SQL，CRUD 数据处理函数 时间序列分析功能 灵活处理缺失数据 sudo pip install pandas sudo pip install xlrd sudo pip install xlwt 《利用python进行数据分析》讲解详细，针对 Pandas。 Pandas 基本的数据结构是 : Series 和 DataFrame (它的每一列都是一个Series)。每个 Series 都会有一个对应的 Index，用来标记元。(Index类似于 SQL 主键) 123456789101112131415# -*- coding: utf-8 -*-import pandas as pds = pd.Series([1, 2, 3], index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]) # 创建一个序列 sd2 = pd.DataFrame(s)d = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]) # 创建一个 tabled.head() # 默认预览前 5 行d.describe() # 数据基本统计量# 读取文件pd.read_excel(&#x27;data.xls&#x27;) # 读取 Excel 文件, 创建 DataFrame.# pd.read_csv(&#x27;data.csv&#x27;, encoding=&#x27;utf-8&#x27;) # 读取文本, 一般指定 encoding 2.5 StatsModels StatsModels 主要是对，数据的读取、处理、探索，更加注重数据的统计建模分析，有 R 语言味道。 StatsModels 与 Pandas 结合, 成为 Python 下强大的数据挖掘组合。 sudo pip install StatsModels 2.6 Scikit-Learn Scikit-Learn 强大的 ML 工具包。包括 数据预处理、分类、回归、聚类、预测 和 模型分析等。 Scikit-Learn 依赖于 Numpy、Scipy、Matplotlib。 install pip install scikit-learn 用 pip 安装这个包之后，在使用的时候会出现 ValueError: numpy.dtype has the wrong 等错误。 solution fun sudo pip install cython git clone https://github.com/scikit-learn/scikit-learn sudo make sudo python setup.py install 不安装 cython ，安装 scikit-learn 会报错。 这种方式 安装 scikit-learn 过程中的一些错误或警告不需要管。安装完成测试使用正常 pip list scikit-learn (0.18.dev0) scipy (0.13.0b1) 2.7 Keras 神经网络model 2.8 Gensim topic modelling for humans！NLP","categories":[{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"},{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"}]},{"title":"Spark Introduce 1","slug":"spark/spark-review-1-introduce","date":"2016-06-29T02:07:21.000Z","updated":"2021-06-20T04:12:28.351Z","comments":true,"path":"2016/06/29/spark/spark-review-1-introduce/","link":"","permalink":"http://www.iequa.com/2016/06/29/spark/spark-review-1-introduce/","excerpt":"Spark","text":"Spark Spark 发源于University of California, Berkeley, AMPLap 大数据分析平台 Spark 立足于内存计算、从多迭代批量处理出发 Spark 兼顾数据仓库、流处理、图计算 等多种计算范式，大数据系统领域全栈计算平台 spark.apache.org localhost:8080 1. Spark 的历史与发展 2009 : Spark 诞生于 AMPLab 2014 : Spark 1.0 发布 2019 : Spark 3.0 发布 2. Spark 之于 Hadoop Spark 是 MapReduce 的替代方案, 且兼容 HDFS、Hive 等分布式存储层。 Spark 相比 Hadoop MapReduce 的优势如下 : 中间结果输出 数据格式和内存布局 执行策略 任务调度的开销 Spark用事件驱动类库AKKA来启动任务, 通过线程池复用线程避免进线程启动切换开销 3. Spark 能带来什么 ? 打造全栈多计算范式的高效数据流水线 轻量级快速处理, 并支持 Scala、Python、Java 与 HDFS 等 存储层 兼容 4. Spark 安装与部署 Spark 主要使用 HDFS 充当持久化层，所以完整的安装 Spark 需要先安装 Hadoop. Spark 是计算框架, 它主要使用 HDFS 充当持久化层。 Linux 集群安装 Spark 安装 JDK (green download install) 配置 SSH 免密码登陆 (可选) 安装 Hadoop (brew install hadoop) 安装 Scala (brew install scala) 安装 Spark (green download install) 启动 Spark 集群 1234567891011121314151617MS=/usr/local/xsoft### JAVA ###JAVA_HOME=$MS/jdk/Contents/HomeJAVA_BIN=$JAVA_HOME/binPATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jarexport JAVA_HOME JAVA_BIN PATH CLASSPATHexport HADOOP_HOME=/usr/local/Cellar/hadoop/3.2.1_1export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/binexport SCALA_HOME=/usr/local/Cellar/scala/2.13.3export PATH=$PATH:$SCALA_HOME/binexport SPARK_HOME=$MS/sparkexport PATH=$PATH:$SPARK_HOME/bin Spark官网下载 4.1 安装 Spark 123456789101112131415161718192021222324252627282930313233(1). tar -xzvf spark-3.0.0-bin-hadoop2.7.tgz(2). cd /usr/local/xsoft/spark-3.0.0-bin-hadoop3.2/(3). 配置 conf/spark-env.sh 1) 详细复杂参数配置参见 官网 Configuration 2) vim conf/spark-env.sh #!/usr/bin/env bash export SCALA_HOME=/usr/local/Cellar/scala/2.13.3 export SPARK_HOME=/usr/local/xsoft/spark export SPARK_MASTER_IP=localhost export MASTER=spark://localhost:7077 #export IPYTHON=1 export PYSPARK_PYTHON=/Users/blair/.pyenv/versions/anaconda3/envs/spark/bin/python3 export PYSPARK_DRIVER_PYTHON=&quot;jupyter&quot; export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot; #export SPARK_WORKER_MEMORY=2g #export SPARK_EXECUTOR_INSTANCES=2 #export SPARK_EXECUTOR_CORES=1 #export SPARK_WORKER_MEMORY=2000m #export SPARK_EXECUTOR_MEMORY=500m #export SPARK_LIBRARY_PATH=$&#123;SPARK_HOME&#125;/lib(4). 配置 conf/slaves (测试可选)(5). 一般需要 startup ssh server. 4.2 启动 Spark 集群 在 Spark 根目录启动 Spark 12./sbin/start-all.sh./sbin/stop-all.sh 启动后 jps 查看 会有 Master 进程存在 1234➜ spark jps11262 Jps11101 Master11221 Worker 4.3 Spark 集群初试 可以通过两种方式运行 Spark 样例 : 以 ./run-example 的方式执行 123➜ cd /usr/local/xsoft/spark➜ spark ./sbin/start-all.sh➜ spark ./bin/run-example org.apache.spark.examples.SparkPi 以 ./Spark Shell 方式执行 12345678910111213141516171819202122232425262728scala&gt; import org.apache.spark._import org.apache.spark._scala&gt; object SparkPi &#123; | | def main(args: Array[String]) &#123; | | val slices = 2 | val n = 100000 * slices | | val count = sc.parallelize(1 to n, slices).map &#123; i =&gt; | | val x = math.random * 2 - 1 | val y = math.random * 2 - 1 | | if (x * x + y * y &lt; 1) 1 else 0 | | &#125;.reduce(_ + _) | | println(&quot;Pi is rounghly &quot; + 4.0 * count / n) | | &#125; | &#125;defined module SparkPiscala&gt;// Spark Shell 已默认将 SparkContext 类初始化为对象 sc, 用户代码可直接使用。// Spark 自带的交互式的 Shell 程序，方便进行交互式编程。 通过 Web UI 查看集群状态 http：//masterIp:8080 http://localhost:8080/ 4.4 Spark quick start quick-start : https://spark.apache.org/docs/latest/quick-start.html ./bin/spark-shell 1234567891011scala&gt; val textFile = sc.textFile(&quot;README.md&quot;)# val textFile = sc.textFile(&quot;file:///usr/local/xsoft/spark/README.md&quot;)textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let’s start with a few actions:scala&gt; textFile.count() // Number of items in this RDDres0: Long = 126scala&gt; textFile.first() // First item in this RDDres1: String = # Apache Spark 5. Spark 生态 BDAS Spark 框架、架构、计算模型、数据管理策略 Spark BDAS 项目及其子项目进行了简要介绍 Spark 生态系统包含的多个子项目 : SparkSql、Spark Streaming、GraphX、MLlib Spark 是 BDAS 核心, 是一 大数据分布式编程框架 6. Spark 架构 Spark 的代码结构 Spark 的架构 Spark 运行逻辑 6.1 Spark 的代码结构 12345678scheduler：文件夹中含有负责整体的Spark应用、任务调度的代码。broadcast：含有Broadcast（广播变量）的实现代码，API中是Java和Python API的实现。deploy：含有Spark部署与启动运行的代码。common：不是一个文件夹，而是代表Spark通用的类和逻辑实现，有5000行代码。metrics：是运行时状态监控逻辑代码，Executor中含有Worker节点负责计算的逻辑代码。partial：含有近似评估代码。 6.2 Spark 的架构 Spark架构采用了分布式计算中的Master-Slave模型。 Role description Master 对应集群中的含有Master进程的节点, 集群的控制器 Slave 集群中含有Worker进程的节点 Client 作为用户的客户端负责提交应用 Driver 运行Application的main()函数并创建SparkContext。负责作业的调度，即Task任务的分发 Worker 管理计算节点和创建Executor，启动Executor 或 Driver. 接收主节点命令与进行状态汇报 Executor Worker node执行任务的组件,负责 Task 的执行,用于启动线程池运行任务 ClusterManager Standalone 模式中为 Master, 控制整个集群, 监控Worker SparkContext 整个应用的上下文, 控制App的生命周期 RDD Spark的基本计算单元，一组RDD可形成执行的 DAG Num Spark App 流程 1. Client 提交应用 2. Master 找到一个 Worker 启动 Driver 3. Driver 向 Master 或者 资源管理器申请资源，之后将应用转化为 RDD Graph 4. DAGScheduler 将 RDD Graph 转化为 Stage的有向无环图 提交给 TaskScheduler 5. TaskScheduler 提交 task 给Executor执行 6. 在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行 在执行阶段，Driver 会将 Task 和 Task所依赖的file 和 jar 序列化后传递给对应的 Worker机器，同时 Executor对相应数据分区的任务进行处理。 7. 小结 由于 Spark 主要使用 HDFS 充当持久化层，所以完整的使用 Spark 需要预先安装 Hadoop. Spark 将分布式的内存数据抽象为弹性分布式数据集 (RDD), 并在其上实现了丰富的算子，从而对 RDD 进行计算，最后将 算子序列 转化为 DAG 进行执行和调度。 Spark的Python API几乎覆盖了所有Scala API所能提供的功能. 但的确有些特性，比如Spark Streaming和个别的API方法，暂不支持。 具体参见《Spark编程指南》的Python部分 体会了 函数式 编程. 个人认为 scala、python 比较适合写 spark 程序. Reference Mac上安装Spark3.0.0以及Hadoop 大数据入门与实战-PySpark的使用教程","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Elasticsearch * 入门","slug":"elasticsearch/es-indoor","date":"2016-05-17T07:59:16.000Z","updated":"2021-06-20T04:12:28.342Z","comments":true,"path":"2016/05/17/elasticsearch/es-indoor/","link":"","permalink":"http://www.iequa.com/2016/05/17/elasticsearch/es-indoor/","excerpt":"Elasticsearch 是一个基于Apache Lucene™的开源搜索引擎 、 实时分布式搜索 和 分析引擎。","text":"Elasticsearch 是一个基于Apache Lucene™的开源搜索引擎 、 实时分布式搜索 和 分析引擎。 Lucene 是 成熟的全文索引与信息检索(IR)库，采用Java实现。信息检索式指文档搜索、文档内信息搜索或者文档相关的元数据搜索等操作。。 Solr是一个基于Lucene java库的企业级搜索服务器，包含XML/HTTP，JSON API，高亮查询结果，缓存，复制，还有一个WEB管理界面。Solr运行在Servlet容器中 2010 年 Elasticsearch 出现公开版本 Elasticsearch 涉及的技术 全文搜索 分析系统 分布式数据库 谁在使用 Elasticsearch? 维基百科 StackOverflow Github … 1. 概念 Elasticsearch 是 开源搜索引擎. Elasticsearch 不仅是全文搜索，还是： 分布式 实时文件存储，每个字段都被索引并可被搜索 分布式 实时分析搜索引擎 可扩展服务器，处理PB级结构化或非结构化数据 这些功能被集成到一个服务里面，应用可通过 RESTful API、各种语言的客户端、命令行 与之交互。 Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 more_info_install_elaticsearch 123456[deploy@node196 config]$ lltotal 20-rw-rw-r-- 1 deploy deploy 13915 May 10 10:02 elasticsearch.yml-rw-rw-r--. 1 deploy deploy 2054 Jul 16 2015 logging.yml[deploy@node196 config]$ pwd/home/deploy/elasticsearch-1.7.1/config 编辑 elasticsearch.yml 替代cluster.name的默认值，这样可以防止一个新启动的节点加入到相同网络中的另一个同名的集群中。 123456789cluster.name: elasticsearch_your-company-name#################################### Node ###################################### Node names are generated dynamically on startup, so you&#x27;re relieved# from configuring them manually. You can tie this node to a specific name:#node.name: &quot;node196&quot; 2. API 2.1 Java API 节点客户端(node client) 节点客户端以无数据节点(none data node)身份加入集群，换言之，它自己不存储任何数据，但是它知道数据在集群中的具体位置，并且能直接转发请求到对应的节点上。 传输客户端(Transport client) 更轻量的传输客户端 能发送请求到远程集群。它自己不加入集群，只是简单转发请求给集群中的节点。 两个Java客户端都通过 9300端口 与 集群交互，使用 Elasticsearch传输协议(Elasticsearch Transport Protocol)。集群中的节点之间也通过 9300 port 进行通信。 more_info_Java-API 2.2 RESTful API 基于 HTTP 协议，以 JSON 为数据交互格式的 RESTful API 向 Elasticsearch 发出的请求的组成部分与其它普通的HTTP请求是一样的： curl -X ‘😕/:/?&lt;QUERY_STRING&gt;’ -d ‘’ VERB HTTP方法：GET, POST, PUT, HEAD, DELETE … example : 查询集群中 文档数量 1234567891011121314[deploy@node196 config]$ curl -u username:passwd -XGET &#x27;localhost:9200/_count?pretty&#x27; -d &#x27;&gt; &#123;&gt; &quot;query&quot;: &#123;&gt; &quot;match_all&quot;: &#123;&#125;&gt; &#125;&gt; &#125;&#x27;&#123; &quot;count&quot; : 100001234, &quot;_shards&quot; : &#123; &quot;total&quot; : 376, &quot;successful&quot; : 376, &quot;failed&quot; : 0 &#125;&#125; 1curl -u name:pass -X DELETE http://ip:9200/your_index 123GET /coupon_seeker/coupon_seeker/_search?q=source:dianpingcurl -u name:pass -XGET &#x27;http://192.168.181.xxx:9200/coupon_seeker/coupon_seeker/_search?q=source:dianping 有条件的精确匹配删除命令 1curl -u name:pass -XDELETE &#x27;http://192.168.181.xxx:9200/coupon_seeker/coupon_seeker/_query?pretty=true&#x27; -d &#x27;&#123;&quot;query&quot;:&#123;&quot;match&quot;:&#123;source:&quot;dianping&quot;&#125;&#125;&#125;&#x27; 3. 文档 面向文档 Elasticsearch is document oriented，这意味着它可以存储整个 object 或 document。 Elasticsearch 还可以 索引(index) 每个文档的内容使之可以被 搜索。 可对 document （而非成行成列的数据）进行 index、搜索、排序、过滤。 这种理解数据的方式与以往完全不同，这也是 Elasticsearch 能够执行复杂的全文搜索的原因之一。 JSON (JavaScript Object Notation)，文档序列化格式 1234567891011&#123; &quot;email&quot;: &quot;john@smith.com&quot;, &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;info&quot;: &#123; &quot;bio&quot;: &quot;Eco-warrior and defender of the weak&quot;, &quot;age&quot;: 25, &quot;interests&quot;: [ &quot;dolphins&quot;, &quot;whales&quot; ] &#125;, &quot;join_date&quot;: &quot;2014/05/01&quot;&#125; 如下 user对象很复杂，但它的结构和对象的含义已经被完整的体现在JSON中了，在Elasticsearch中将对象转化为 JSON 并做 index索引 要比在表结构中做相同的事情简单的多。 5. 索引 123456789[hdfs@node196 data_analysis]$ curl -u username:passwd -XPUT http://node190:9200/megacorp/employee/1 -d &#x27;&gt; &#123;&gt; &quot;first_name&quot; : &quot;John&quot;,&gt; &quot;last_name&quot; : &quot;Smith&quot;,&gt; &quot;age&quot; : 25,&gt; &quot;about&quot; : &quot;I love to go rock climbing&quot;,&gt; &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&gt; &#125;&#x27;&#123;&quot;_index&quot;:&quot;megacorp&quot;,&quot;_type&quot;:&quot;employee&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:1,&quot;created&quot;:true&#125;[hdfs@node196 data_analysis]$ indexing search aggregations /ˌæɡrɪˈɡeɪʃən/ Elasticsearch能做的事 场景: 假设我们刚好在Megacorp工作，这时人力资源部门出于某种目的需要让我们创建一个员工目录，这个目录用于促进人文关怀和用于实时协同工作，所以它有以下不同的需求： 数据能够包含多个值的标签、数字和纯文本。 检索任何员工的所有信息。 支持结构化搜索，例如查找30岁以上的员工。 支持简单的全文搜索和更复杂的短语(phrase)搜索 高亮搜索结果中的关键字 能够利用图表管理分析这些数据 索引员工文档 12Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields Elasticsearch Relational DB Indices Databases Types Tables Documents Rows Fields Columns Elasticsearch 索引」含义的区分 index_num. : index (数据库)，它是相关文档存储的地方， index_verb. 「索引一个文档」表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL 中的 INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。 倒排索引 : 传统数据库为特定列增加一个索引，例如 B-Tree索引 来加速检索。Elasticsearch和Lucene使用倒排索引(inverted index)的数据结构来达到相同目的。 12345678PUT /megacorp/employee/1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; 4.1 检索文档 1GET /megacorp/employee/1 1234567891011121314&#123; &quot;_index&quot; : &quot;megacorp&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;&#125; 4.2 简单搜索 1GET /megacorp/employee/_search 查询 last_name 为 Smith 的记录 1GET /megacorp/employee/_search?q=last_name:Smith 1234567891011121314151617181920212223242526272829&#123; ... &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.30685282, &quot;hits&quot;: [ &#123; ... &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125; &#125;, &#123; ... &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 32, &quot;about&quot;: &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125; &#125; ] &#125;&#125; 4.3 使用DSL语句查询 DSL(Domain Specific Language特定领域语言) 查询字符串等价于 q=last_name:Smith **DSL查询 : ** 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;Smith&quot; &#125; &#125;&#125; 4.4 更复杂的搜索 filter range GET /megacorp/employee/_search { “query” : { “filtered” : { “filter” : { “range” : { “age” : { “gt” : 30 } &lt;1&gt; } }, “query” : { “match” : { “last_name” : “smith” &lt;2&gt; } } } } } 4.5 全文搜索 一种传统数据库难以实现的功能 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125; Result : 12345678910111213141516171819202122232425262728293031&#123; ... &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.16273327, &quot;hits&quot;: [ &#123; ... &quot;_score&quot;: 0.16273327, &lt;1&gt; &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125; &#125;, &#123; ... &quot;_score&quot;: 0.016878016, &lt;2&gt; &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 32, &quot;about&quot;: &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125; &#125; ] &#125;&#125; 4.6 短语搜索 – phrases 想要确切的匹配若干个单词或者短语(phrases), 例如 我们想要查询同时包含&quot;rock&quot;和&quot;climbing&quot;（并且是相邻的）的员工记录。 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125; 增加高亮 12345678910111213GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;about&quot; : &#123;&#125; &#125; &#125;&#125; 5. aggregations 聚合相当于 group by 123456789101112131415GET /megacorp/employee/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;last_name&quot;: &quot;smith&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125; &#125; &#125;&#125; 12345678910111213...&quot;all_interests&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1 &#125; ]&#125; 聚合也允许分级汇总。例如，让我们统计每种兴趣下职员的平均年龄 12345678910111213GET /megacorp/employee/_search&#123; &quot;aggs&quot; : &#123; &quot;all_interests&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;interests&quot; &#125;, &quot;aggs&quot; : &#123; &quot;avg_age&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 教程小结 为了保持简短，还有很多的特性未提及——像 推荐、定位、渗透、模糊 以及 部分匹配等。但这也突出了构建高级搜索功能是多么的容易。无需配置，只需要添加数据然后开始搜索！ 6. 分布式的特性 Elasticsearch 你不需要知道任何关于分布式系统、分片、集群发现或者其他大量的分布式概念。所有的教程你即可以运行在你的笔记本上，也可以运行在拥有100个节点的集群上，其工作方式是一样的。 Elasticsearch 致力于隐藏分布式系统的复杂性。以下这些操作都是在底层自动完成的： 将你的文档分区到不同的容器或者分片(shards)中，它们可存于一或多个节点中。 将分片均匀的分配到各个节点，对索引和搜索做负载均衡。 冗余每一个分片，防止硬件故障造成的数据丢失。 将集群中任意一个节点上的请求路由到相应数据所在的节点。 无论是增加节点，还是移除节点，分片都可以做到无缝的扩展和迁移。","categories":[{"name":"elastic","slug":"elastic","permalink":"http://www.iequa.com/categories/elastic/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://www.iequa.com/tags/elasticsearch/"}]},{"title":"Elasticsearch installation plugins","slug":"elasticsearch/es-install-plugins","date":"2016-05-17T07:59:16.000Z","updated":"2021-06-20T04:12:28.343Z","comments":true,"path":"2016/05/17/elasticsearch/es-install-plugins/","link":"","permalink":"http://www.iequa.com/2016/05/17/elasticsearch/es-install-plugins/","excerpt":"Elasticsearch 扩展性非常好，有很多官方和第三方开发的插件","text":"Elasticsearch 扩展性非常好，有很多官方和第三方开发的插件 1. Elasticsearch-Install 官网 : https://www.elastic.co/ Install es 123456789download elasticsearch-1.7.5.tar.gzcd usr/local/mySoft/deploytar -xvf elasticsearch-1.7.5.tar.gzln -s /usr/local/mySoft/deploy/elasticsearch-1.7.5/ elasticsearch------vim ~/.zshrcexport ES_HOME=/usr/local/xSoft/elasticsearch Config es $ES_HOME/config/elasticsearch.yml 12cluster.name: elasticsearch_xnode.name=test-node1 Startup 123./bin/elasticsearch./bin/elasticsearch -d -Xms512m -Xmx512m 运行之后，会产生 data 和 logs 目录 123456789101112➜ elasticsearch lltotal 28-rw-r--r-- 1 hp staff 11358 Feb 2 17:24 LICENSE.txt-rw-r--r-- 1 hp staff 150 Feb 2 17:24 NOTICE.txt-rw-r--r-- 1 hp staff 8700 Feb 2 17:24 README.textiledrwxr-xr-x 14 hp staff 476 May 26 15:42 bin/drwxr-xr-x 4 hp staff 136 May 27 11:03 config/drwxr-xr-x 3 hp staff 102 May 26 11:01 data/drwxr-xr-x 26 hp staff 884 May 26 09:58 lib/drwxr-xr-x 7 hp staff 238 May 27 09:58 logs/drwxr-xr-x 7 hp staff 238 May 27 10:48 plugins/➜ elasticsearch Verify open http://ip:9200/ 12345678910111213&#123; &quot;status&quot; : 200, &quot;name&quot; : &quot;node01&quot;, &quot;cluster_name&quot; : &quot;elasticsearch_x&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;1.7.5&quot;, &quot;build_hash&quot; : &quot;00f95f4ffca6de89d68b7ccaf80d148f1f70e4d4&quot;, &quot;build_timestamp&quot; : &quot;2016-02-02T09:55:30Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;4.10.4&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 2. Elasticsearch-Head ElasticSearch-Head 是一个与Elastic集群（Cluster）相交互的 Web 前台。 ES-Head的主要作用 它展现ES集群的拓扑结构，并且可以通过它来进行索引（Index）和节点（Node）级别的操作 它提供一组针对集群的查询API，并将结果以json和表格形式返回 它提供一些快捷菜单，用以展现集群的各种状态 Install-Verify 123elasticsearch/bin/plugin install mobz/elasticsearch-headopen ip:9200/_plugin/head/open ip:9200/_cluster/health?pretty 3. Elasticsearch-Kopf Kopf是一个ElasticSearch的管理工具，它也提供了对ES集群操作的API。 Install-Verify 12./elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf/&#123;branch|version&#125;open http://localhost:9200/_plugin/kopf 4. Elasticsearch-bigdesk Bigdesk为Elastic集群提供动态的图表与统计数据。 Install-Verify 1234bin/plugin -install lukas-vlcek/bigdesk删除bin/plugin --remove bigdeskopen ip:9200/_plugin/bigdeskopen ip:9200/_cluster/state?pretty 5. Elasticsearch-service elasticsearch 作为一个系统service应用 ，可以安装elasticsearch-servicewrapper插件 github-es-service 123git clone https://github.com/elasticsearch/elasticsearch-servicewrapper下载该插件后，解压缩。将service目录拷贝到elasticsearch安装目录的bin目录下。 12345678910➜ service lltotal 76-rwxr-xr-x 1 hp staff 55710 May 26 15:42 elasticsearch*-rw-r--r-- 1 hp staff 2610 May 26 15:42 elasticsearch.bat-rw-r--r-- 1 hp staff 4754 May 26 15:42 elasticsearch.conf-rwxr-xr-x 1 hp staff 64 May 26 15:42 elasticsearch32*-rwxr-xr-x 1 hp staff 64 May 26 15:42 elasticsearch64*drwxr-xr-x 16 hp staff 544 May 26 15:42 exec/drwxr-xr-x 17 hp staff 578 May 26 15:42 lib/➜ service 运行这个插件的好处是：elasticsearch 需要的jvm参数和其它配置都已经配置好了，非常方便。 123sh elasticsearch start;sh elasticsearch restart;sh elasticsearch stop; 在实际生产环境中，该插件基本把参数都配置好了。我们只需要修改一下jvm分配的内存空间就好了，如 : 123set.default.ES_HEAP_SIZE=16384set.default.ES_MIN_MEM=16384set.default.ES_MAX_MEM=19660 第一次运行 elaticsearch 会产生 data-dir 与 log-dir service log 在 logs/service.log 中。 more_info-service Mac OS X Mountain Lion missing 32-bit Java apple 6 maybe could 6. Http-basic-server-plugin 不要裸奔，穿一套比基尼吧。 做一个简单的HTTP认证，elasticsearch-http-basic 提供了针对 ES HTTP 连接 的 IP白名单、密码权限 和 信任代理功能。 github : Asquera_http_basic Install-Verify elasticsearch-http-basic还不支持ES标准的bin/plugin install [github-name]/[repo-name]的安装方式, 所以按照如下方式安装 12mkdir -p plugins/http-basic; mv elasticsearch-http-basic-1.5.1.jar plugins/http-basic Config http-basic param 1234567http.basic.enabled: truehttp.basic.user: &quot;admin&quot;http.basic.password: &quot;admin&quot;http.basic.ipwhitelist: [&quot;localhost&quot;, &quot;127.0.0.1&quot;]http.basic.trusted_proxy_chains: []http.basic.log: true... 7. Elasticsearch-sql install 1./plugin -u https://github.com/NLPchina/elasticsearch-sql/releases/download/1.4.5/elasticsearch-sql-1.4.5.zip --install sql Verify 1open http://node01:9200/_plugin/sql/ ./bin/plugin --list 12345678910➜ elasticsearch ./bin/plugin --listInstalled plugins: - bigdesk - head - http-basic - jdbc - kopf - license - shield - sql 8. Elasticsearch-jdbc 关系型数据库的同步插件 install 1./plugin --install jdbc --url http://xbib.org/repository/org/xbib/elasticsearch/plugin/elasticsearch-river-jdbc/1.5.0.5/elasticsearch-river-jdbc-1.5.0.5-plugin.zip download and add mysql-driver 12345curl -o mysql-connector-java-5.1.33.zip -L &#x27;http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.33.zip/from/http://cdn.mysql.com/&#x27;cp mysql-connector-java-5.1.33-bin.jar $ES_HOME/plugins/jdbc/chmod 644 $ES_HOME/plugins/jdbc/* 停止river 1curl -XDELETE &#x27;localhost:9200/_river/my_jdbc_river/&#x27; Verify 1open http://node01:9200/_nodes/node01/plugins?pretty=true 9. Basic operation 查看该节点安装的所有插件列表 http://node01:9200/_nodes/node01/plugins?pretty=true 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&#123; &quot;cluster_name&quot; : &quot;elasticsearch_x&quot;, &quot;nodes&quot; : &#123; &quot;nSitXzd8QvSxQRz3mni3BA&quot; : &#123; &quot;name&quot; : &quot;node01&quot;, &quot;transport_address&quot; : &quot;inet[/192.168.181.35:9300]&quot;, &quot;host&quot; : &quot;unix.local&quot;, &quot;ip&quot; : &quot;192.168.181.35&quot;, &quot;version&quot; : &quot;1.7.5&quot;, &quot;build&quot; : &quot;00f95f4&quot;, &quot;http_address&quot; : &quot;inet[/192.168.181.35:9200]&quot;, &quot;plugins&quot; : [ &#123; &quot;name&quot; : &quot;sql&quot;, &quot;version&quot; : &quot;1.4.5&quot;, &quot;description&quot; : &quot;Use sql to query elasticsearch.&quot;, &quot;url&quot; : &quot;/_plugin/sql/&quot;, &quot;jvm&quot; : true, &quot;site&quot; : true &#125;, &#123; &quot;name&quot; : &quot;http-basic-server-plugin&quot;, &quot;version&quot; : &quot;NA&quot;, &quot;description&quot; : &quot;HTTP Basic Server Plugin&quot;, &quot;jvm&quot; : true, &quot;site&quot; : false &#125;, &#123; &quot;name&quot; : &quot;bigdesk&quot;, &quot;version&quot; : &quot;NA&quot;, &quot;description&quot; : &quot;No description found.&quot;, &quot;url&quot; : &quot;/_plugin/bigdesk/&quot;, &quot;jvm&quot; : false, &quot;site&quot; : true &#125;, &#123; &quot;name&quot; : &quot;head&quot;, &quot;version&quot; : &quot;NA&quot;, &quot;description&quot; : &quot;No description found.&quot;, &quot;url&quot; : &quot;/_plugin/head/&quot;, &quot;jvm&quot; : false, &quot;site&quot; : true &#125;, &#123; &quot;name&quot; : &quot;kopf&quot;, &quot;version&quot; : &quot;1.5.7-SNAPSHOT&quot;, &quot;description&quot; : &quot;kopf - simple web administration tool for ElasticSearch&quot;, &quot;url&quot; : &quot;/_plugin/kopf/&quot;, &quot;jvm&quot; : false, &quot;site&quot; : true &#125; ] &#125; &#125;&#125; XPUT data 12345678curl -u admin:admin -XPUT http://node01:9200/megacorp/employee/1 -d &#x27; &#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;&#x27; XGET data 123456curl -XGET &#x27;localhost:9200/_count?pretty&#x27; -d &#x27; &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125; &#125;&#x27; output 12345678&#123; &quot;count&quot; : 1, &quot;_shards&quot; : &#123; &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 &#125;&#125; 10. Reference article csdn-004-Elasticsearch插件的介绍 插件安装Head、Kopf与Bigdesk chepoo.com/elasticsearch-service elastic.co/guide/ NLPchina/elasticsearch-sql elasticsearch-http-user-auth (这个我没有使用) 建造者说","categories":[{"name":"elastic","slug":"elastic","permalink":"http://www.iequa.com/categories/elastic/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://www.iequa.com/tags/elasticsearch/"}]},{"title":"Point to Plane","slug":"ml/math-distance-formula-of-point-to-plain","date":"2016-05-17T07:07:21.000Z","updated":"2021-06-20T04:12:28.315Z","comments":true,"path":"2016/05/17/ml/math-distance-formula-of-point-to-plain/","link":"","permalink":"http://www.iequa.com/2016/05/17/ml/math-distance-formula-of-point-to-plain/","excerpt":"平面的一般式方程, 向量的模（长度）, 向量的数量积, 点到平面的距离","text":"平面的一般式方程, 向量的模（长度）, 向量的数量积, 点到平面的距离 维基百科_Vector 1. 平面的一般式方程 Ax +By +Cz + D = 0 其中n = (A, B, C)是平面的法向量，D是将平面平移到坐标原点所需距离（所以D=0时，平面过原点） 2. 向量的模（长度） 给定一个向量V（x, y, z),则|V| = sqrt(x * x + y * y + z * z) 在数学中，矢量常采用更为抽象的矢量空间（也称为线性空间）来定义，而定义具有物理意义上的大小和方向的矢量概念则需要引进了范数和内积的欧几里得空间。 范数， 模长 3. 向量的数量积/点积/内积 给定两个向量V1(x1, y1, z1)和V2(x2, y2, z2)则他们的内积是 V1V2 = x1x2 + y1y2 + z1z2 数量积被广泛应用于物理中，如做功就是用力的矢量乘位移的矢量，即 4. 点到平面的距离(Yes) 求点到直线的距离不再是难事，有图有真相 如果法向量是单位向量的话，那么分母为1","categories":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"}],"tags":[{"name":"math","slug":"math","permalink":"http://www.iequa.com/tags/math/"}]},{"title":"Spark Machine Learning p1 - Spark编程入门","slug":"spark/spark-machine-learning-p1","date":"2016-04-25T02:07:21.000Z","updated":"2021-06-20T04:12:28.349Z","comments":true,"path":"2016/04/25/spark/spark-machine-learning-p1/","link":"","permalink":"http://www.iequa.com/2016/04/25/spark/spark-machine-learning-p1/","excerpt":"Spark 的环境搭建与运行, 接触了 RDD 与 SparkContext, 启动 Spark-Shell 以及如何使用 Scala、Python 编写 Spark 程序.","text":"Spark 的环境搭建与运行, 接触了 RDD 与 SparkContext, 启动 Spark-Shell 以及如何使用 Scala、Python 编写 Spark 程序. Apache Spark Spark 的设计目标 即: 迭代式+低延迟 适合 Machine Learning 算法的特性 Spark 分布式计算框架, 将中间数据和结果保存在内存中 Spark 提供函数式API, 并兼容 Hadoop 生态 Spark 框架对 资源调度、任务提交、执行、跟踪， 节点间通信以及数据并行处理的内在底层操作都进行了抽象 简化了海量数据的存储(HDFS) 和 计算(MR) 流程。MapReduce 缺点, 如: 启动任务时的高开销、对中间数据 和 计算结果 写入磁盘的依赖。这使得 Hadoop 不适合 迭代式 或 低延迟 的任务。 Spark 的四种运行模式 本地单机模式 – Spark 进程 all run in one JVM 集群单机模式 – 使用 Spark 自己内置的 任务调度框架 基于 Mesos 一个开源集群计算框架 基于 YARN 与 Hadoop2 关联形成集群计算和资源调度框架 1. Spark运行 运行示例程序来测试是否一切正常： ./bin/run-example org.apache.spark.examples.SparkPi 该命令将在本地单机模式下执行SparkPi这个示例。在该模式下，所有的Spark进程均运行于同一个JVM中，而并行处理则通过多线程来实现。默认情况下，该示例会启用与本地系统的CPU核心数目相同的线程。 要在本地模式下设置并行的级别，以local[N]的格式来指定一个master变量即可。比如只使用两个线程时，可输入如下命令： MASTER=local[2] ./bin/run-example org.apache.spark.examples.SparkPi 2. Spark集群 Spark集群由两类程序构成: 一个驱动程序 多个执行程序 本地模式下所有的处理都运行在同一个JVM内，而在集群模式时它们通常运行在不同的节点上。 一个采用单机模式的Spark集群包括： 一个运行Spark单机主进程和驱动程序的 Master； 各自运行一个执行程序进程的多个 Worker。 比如在一个Spark单机集群上运行，只需传入主节点的URL即可： MASTER=spark://IP:PORT ./bin/run-example org.apache.spark.examples.SparkPi 其中的IP和PORT分别是主节点IP地址和端口号。这是告诉Spark让示例程序运行在主节点所对应的集群上 3. Spark编程模型 3.1 SparkContext类 SparkContext类与SparkConf类 任何Spark程序的编写都是从SparkContext开始的。SparkContext的初始化需要一个SparkConf对象，后者包含了Spark集群配置的各种参数（比如主节点的URL）。 初始化后，我们便可用SparkContext对象所包含的各种方法来创建和操作RDD。Spark shell（在Scala和Python下可以，但不支持Java）能自动完成上述初始化。若要用Scala代码来实现的话，可参照下面的代码： 12val conf = new SparkConf().setAppName(&quot;Test Spark App&quot;).setMaster(&quot;local[4]&quot;)val sc = new SparkContext(conf) 这段代码会创建一个4线程的SparkContext对象，并将其相应的任务命名为Test Spark APP。我们也可通过如下方式调用SparkContext的简单构造函数 1val sc = new SparkContext(&quot;local[4]&quot;, &quot;Test Spark App&quot;) 3.2 Spark shell Spark支持 用 Scala or Python REPL（Read-Eval-Print-Loop，即交互式shell）来进行交互式的程序编写。 1./bin/spark-shell 会启动Scala shell 并初始化一个SparkContext对象。我们可以通过 sc这个Scala值来调用这个对象 3.3 RDD 一个 RDD 代表一系列的“记录”（严格来说，某种类型的对象）。 这些记录被分配或分区到一个集群的多个节点上（在本地模式下，可以类似地理解为单个进程里的多个线程上）。 Spark中的RDD具备容错性，即当某个节点或任务失败时（因非用户代码原因而引起，如硬件故障、网络不通等），RDD会在余下的节点上自动重建，以便任务能最终完成。 1. 创建RDD RDD可从现有的集合创建 ： 12val collection = List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;)val rddFromCollection = sc.parallelize(collection) RDD也可以基于Hadoop的输入源创建，比如本地文件系统、HDFS。基于Hadoop的RDD可以使用任何实现了Hadoop InputFormat接口的输入格式，包括文本文件、其他Hadoop标准格式、HBase等。以下举例说明如何用一个本地文件系统里的文件创建RDD： 1val rddFromTextFile = sc.textFile(&quot;LICENSE&quot;) 上述代码中的textFile函数（方法）会返回一个RDD对象。该对象的每一条记录都是一个表示文本文件中某一行文字的String（字符串）对象。 2. Spark操作 在Spark编程模式下，所有的操作被分为 transformation 和 action 两种。 transformation 操作是对一个数据集里的所有记录执行某种函数，从而使记录发生改变； action 通常是运行某些计算或聚合操作，并将结果返回运行 SparkContext 的那个驱动程序。 Spark 的操作通常采用函数式风格。 Spark程序中最常用的转换操作便是map操作。该操作对一个RDD里的每一条记录都执行某个函数，从而将输入映射成为新的输出。 比如，下面这段代码便对一个从本地文本文件创建的RDD进行操作。它对该RDD中的每一条记录都执行size函数。 创建一个这样的由若干String构成的RDD对象。通过map函数，我们将每一个字符串都转换为一个整数，从而返回一个由若干Int构成的RDD对象。 12345678910111213141516171819202122scala&gt; rddFromTextFile.countres2: Long = 294scala&gt; val intsFromStringsRDD = rddFromTextFile.map(line =&gt; line.size)intsFromStringsRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at map at &lt;console&gt;:23scala&gt; intsFromStringsRDD.countres3: Long = 294scala&gt; val sumOfRecords = intsFromStringsRDD.sumsumOfRecords: Double = 17062.0scala&gt; val numRecords = intsFromStringsRDD.countnumRecords: Long = 294scala&gt; val aveLengthOfRecord = sumOfRecords / numRecordsaveLengthOfRecord: Double = 58.034013605442176// 等价于val aveLengthOfRecordChained = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count 示例中 =&gt; 是Scala下表示匿名函数的语法。语法 line =&gt; line.size 表示以 =&gt; 操作符左边的部分作为输入，对其执行一个函数，并以 =&gt; 操作符右边代码的执行结果为输出。在这个例子中，输入为line，输出则是 line.size 函数的执行结果。在Scala语言中，这种将一个String对象映射为一个Int的函数被表示为String =&gt; Int。 Spark的大多数操作都会返回一个新RDD，但多数的Action操作则是返回计算的结果 注 : Spark 中的转换操作是延后的。也就是说，在RDD上调用一个转换操作并不会立即触发相应的计算。 只有必要时才计算结果并将其返回给驱动程序，从而提高了Spark的效率。 12345scala&gt; val transformedRDD = rddFromTextFile.map(line =&gt; line.size). | filter(size =&gt; size &gt; 10).map(size =&gt; size * 2)transformedRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at map at &lt;console&gt;:24scala&gt; 没有触发任何计算，也没有结果被返回。 如果我们现在在新的RDD上调用一个执行操作，比如sum，该计算将会被触发： 触发计算 12scala&gt; val computation = transformedRDD.sumcomputation: Double = 34106.0 3. RDD缓存策略 Spark最为强大的功能之一便是能够把数据缓存在集群的内存里。这通过调用RDD的cache函数来实现： 12345scala&gt; rddFromTextFile.cacheres4: rddFromTextFile.type = MapPartitionsRDD[2] at textFile at &lt;console&gt;:21scala&gt; val aveLengthOfRecordChainedFromCached = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.countaveLengthOfRecordChainedFromCached: Double = 58.034013605442176 在RDD首次调用一个执行操作时，这个操作对应的计算会立即执行，数据会从数据源里读出并保存到内存。因此，首次调用cache函数所需要的时间会部分取决于Spark从输入源读取数据所需要的时间。但是，当下一次访问该数据集的时候，数据可以直接从内存中读出从而减少低效的I/O操作，加快计算。多数情况下，这会取得数倍的速度提升。 Spark支持更为细化的缓存策略。通过persist函数可以指定Spark的数据缓存策略。关于RDD缓存的更多信息可参见：http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence。 3.4 广播变量和累加器 Spark的另一个核心功能是能创建两种特殊类型的变量：广播变量 和 累加器。 广播变量（broadcast variable）为只读变量，它由运行SparkContext的驱动程序创建后发送给会参与计算的节点。对那些需要让各工作节点高效地访问相同数据的应用场景，比如机器学习，这非常有用。 Spark下创建广播变量只需在SparkContext上调用一个方法即可： 1234scala&gt; val broadcastAList = sc.broadcast(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;))broadcastAList: org.apache.spark.broadcast.Broadcast[List[String]] = Broadcast(11)scala&gt; 广播变量 也可以被非驱动程序所在的节点（即工作节点）访问，访问的方法是调用该变量的value方法： 12345scala&gt; val broadcastAList = sc.broadcast(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;))broadcastAList: org.apache.spark.broadcast.Broadcast[List[String]] = Broadcast(11)scala&gt; sc.parallelize(List(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)).map(x =&gt; broadcastAList.value ++ x).collectres5: Array[List[Any]] = Array(List(a, b, c, d, e, 1), List(a, b, c, d, e, 2), List(a, b, c, d, e, 3)) 注意，collect 函数一般仅在的确需要将整个结果集返回驱动程序并进行后续处理时才有必要调用。如果在一个非常大的数据集上调用该函数，可能耗尽驱动程序的可用内存，进而导致程序崩溃。 高负荷的处理应尽可能地在整个集群上进行，从而避免驱动程序成为系统瓶颈。然而在不少情况下，将结果收集到驱动程序的确是有必要的。很多机器学习算法的迭代过程便属于这类情况。 累加器（accumulator）也是一种被广播到工作节点的变量。累加器与广播变量的关键不同，是后者只能读取而前者却可累加。 关于累加器的更多信息，可参见《Spark编程指南》：http://spark.apache.org/docs/latest/programming-guide.html#shared-variables。 4. Spark Scala 编程入门 scala-spark-app 123456789101112131415161718192021222324252627282930313233343536373839404142import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._/** * A simple Spark app in Scala */object ScalaApp &#123; def main(args: Array[String]) &#123; val sc = new SparkContext(&quot;local[2]&quot;, &quot;First Spark App&quot;) // we take the raw data in CSV format and convert it into a set of records of the form (user, product, price) val data = sc.textFile(&quot;data/UserPurchaseHistory.csv&quot;) .map(line =&gt; line.split(&quot;,&quot;)) .map(purchaseRecord =&gt; (purchaseRecord(0), purchaseRecord(1), purchaseRecord(2))) // let&#x27;s count the number of purchases val numPurchases = data.count() // let&#x27;s count how many unique users made purchases val uniqueUsers = data.map &#123; case (user, product, price) =&gt; user &#125;.distinct().count() // let&#x27;s sum up our total revenue val totalRevenue = data.map &#123; case (user, product, price) =&gt; price.toDouble &#125;.sum() // let&#x27;s find our most popular product val productsByPopularity = data .map &#123; case (user, product, price) =&gt; (product, 1) &#125; .reduceByKey(_ + _) .collect() .sortBy(-_._2) val mostPopular = productsByPopularity(0) // finally, print everything out println(&quot;Total purchases: &quot; + numPurchases) println(&quot;Unique users: &quot; + uniqueUsers) println(&quot;Total revenue: &quot; + totalRevenue) println(&quot;Most popular product: %s with %d purchases&quot;.format(mostPopular._1, mostPopular._2)) sc.stop() &#125;&#125; 5. Spark Java 编程入门 Java API与Scala API本质上很相似。Scala代码可以很方便地调用Java代码，但某些Scala代码却无法在Java里调用，特别是那些使用了隐式类型转换、默认参数和采用了某些Scala反射机制的代码。 SparkContext有了对应的Java版本JavaSparkContext，而RDD则对应JavaRDD。 Spark提供对Java 8匿名函数（lambda）语法的支持。 用Scala编写时，键/值对记录的RDD能支持一些特别的操作（比如reduceByKey和saveAsSequenceFile）。这些操作可以通过隐式类型转换而自动被调用。用Java编写时，则需要特别类型的JavaRDD来支持这些操作。它们包括用于键/值对的JavaPairRDD，以及用于数值记录的JavaDoubleRDD。 Java 8 RDD以及Java 8 lambda表达式更多信息可参见《Spark编程指南》：http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations。 6. Spark Python 编程入门 12345678910111213141516171819202122&quot;&quot;&quot;用Python编写的一个简单Spark应用&quot;&quot;&quot;from pyspark import SparkContextsc = SparkContext(&quot;local[2]&quot;, &quot;First Spark App&quot;)# 将CSV格式的原始数据转化为(user,product,price)格式的记录集data = sc.textFile(&quot;data/UserPurchaseHistory.csv&quot;).map(lambda line:line.split(&quot;,&quot;)).map(lambda record: (record[0], record[1], record[2]))# 求总购买次数numPurchases = data.count()# 求有多少不同客户购买过商品uniqueUsers = data.map(lambda record: record[0]).distinct().count()# 求和得出总收入totalRevenue = data.map(lambda record: float(record[2])).sum()# 求最畅销的产品是什么products = data.map(lambda record: (record[1], 1.0)).reduceByKey(lambda a, b: a + b).collect()mostPopular = sorted(products, key=lambda x: x[1], reverse=True)[0]print &quot;Total purchases: %d&quot; % numPurchasesprint &quot;Unique users: %d&quot; % uniqueUsersprint &quot;Total revenue: %2.2f&quot; % totalRevenueprint &quot;Most popular product: %s with %d purchases&quot; % (mostPopular[0], mostPopular[1]) 匿名函数在Python语言中亦称lambda函数，lambda也是语法表达上的关键字。 用Scala编写时，一个将输入x映射为输出y的匿名函数表示为x =&gt; y，而在Python中则是lambda x : y。 123456789101112➜ python-spark-app git:(master) ✗ pwd/Users/hp/ghome/hadoop-spark/spark/Spark-Machine-Learning_8519OSCode/Chapter01/python-spark-app➜ python-spark-app git:(master) ✗ $SPARK_HOME/bin/spark-submit pythonapp.pyUsing Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties16/08/26 15:56:02 INFO SparkContext: Running Spark version 1.5.2...Total purchases: 5Unique users: 4Total revenue: 39.91Most popular product: iPhone Cover with 2 purchases16/08/26 15:56:07 INFO SparkUI: Stopped Spark web UI at http://192.168.143.84:4040... Spark的Python API几乎覆盖了所有Scala API所能提供的功能. 但的确有些特性，比如Spark Streaming和个别的API方法，暂不支持。 具体参见《Spark编程指南》的Python部分 7. 小结 体会了 函数式 编程的威力， scala、python 都可以。java 不适合写 spark 程序","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"SBT Hello","slug":"spark/spark-scala-sbt-hello","date":"2016-03-15T23:54:16.000Z","updated":"2021-06-20T04:12:28.348Z","comments":true,"path":"2016/03/16/spark/spark-scala-sbt-hello/","link":"","permalink":"http://www.iequa.com/2016/03/16/spark/spark-scala-sbt-hello/","excerpt":"什么是 SBT ? SBT 项目工程目录 SBT 编译打包 Scala HelloWorld","text":"什么是 SBT ? SBT 项目工程目录 SBT 编译打包 Scala HelloWorld 1. SBT, What? SBT 是 Simple Build Tool 的简称. SBT 可以认为是 Scala 世界的 maven。 SBT的着迷特性，比如： DSL build构建, 并可混合构建 Java 和 Scala 项目； 通过触发执行 (trigger execution) 特性支持持续的编译与测试； 可以重用 Maven 或者 ivy的repository 进行依赖管理； 增量编译、并行任务等等… 2. Hello, SBT 一个极致简单的 Scala项目 （hello simple project） hello/HelloWorld.scala 12345object HelloWorld &#123; def main(args: Array[String]) &#123; println(&quot;Hello, SBT&quot;) &#125;&#125; sbt run 1234567891011➜ hello git:(master) ✗ sbt[info] Set current project to hello (in build file:/Users/hp/ghome/Spark-Scala/hello/)&gt; run[info] Updating &#123;file:/Users/hp/ghome/Spark-Scala/hello/&#125;hello...[info] Resolving org.fusesource.jansi#jansi;1.4 ...[info] Done updating.[info] Compiling 1 Scala source to /Users/hp/ghome/Spark-Scala/hello/target/scala-2.10/classes...[info] Running HelloWorldHello, SBT[success] Total time: 3 s, completed 2016-3-17 9:38:44&gt; 3. SBT 项目工程结构详解 一个典型的SBT项目工程结构如下图所示： build.sbt 详解 build.sbt 相当于 maven-pom.xml，它是build定义文件。 SBT 运行 使用 2 种形式 的 build 定义文件， one, put your project’s base directory，-- build.sbt， a simple build definition； other one, put project directory，can Use Scala language, more expressive。 一个简单的build.sbt文件内容如下： 1234567891011name := &quot;hello&quot; // 项目名称organization := &quot;xxx.xxx.xxx&quot; // 组织名称version := &quot;0.0.1-SNAPSHOT&quot; // 版本号scalaVersion := &quot;2.9.2&quot; // 使用的Scala版本号// 其它build定义 name 和 version的定义是必须的，因为如果想生成jar包的话，这两个属性的值将作为jar包名称的一部分, 各行之间以空行分隔。 除了定义以上项目相关信息，我们还可以在build.sbt中添加项目依赖： 123456789101112131415// 添加源代码编译或者运行期间使用的依赖libraryDependencies += &quot;ch.qos.logback&quot; % &quot;logback-core&quot; % &quot;1.0.0&quot;libraryDependencies += &quot;ch.qos.logback&quot; % &quot;logback-classic&quot; % &quot;1.0.0&quot;// 或者libraryDependencies ++= Seq( &quot;ch.qos.logback&quot; % &quot;logback-core&quot; % &quot;1.0.0&quot;, &quot;ch.qos.logback&quot; % &quot;logback-classic&quot; % &quot;1.0.0&quot;, ... )// 添加测试代码编译或者运行期间使用的依赖libraryDependencies ++= Seq(&quot;org.scalatest&quot; %% &quot;scalatest&quot; % &quot;1.8&quot; % &quot;test&quot;) 当然， build.sbt文件中还可以定义很多东西，比如添加插件，声明额外的repository，声明各种编译参数等等 project目录即相关文件介绍 project目录下的几个文件可以根据情况添加。 build.properties 文件声明使用的要使用哪个版本的SBT来编译当前项目， 最新的sbt boot launcher可以能够兼容编译所有0.10.x版本的SBT构建项目，比如如果我使用的是0.12版本的sbt，但却想用0.11.3版本的sbt来编译当前项目，则可以在build.properties文件中添加sbt.version=0.11.3来指定。 plugins.sbt 文件用来声明当前项目希望使用哪些插件来增强当前项目使用的sbt的功能，比如像assembly功能，清理ivy local cache功能，都有相应的sbt插件供使用， 要使用这些插件只需要在 plugins.sbt 中声明即可. 为了能够成功加载这些sbt插件，我们将他们的查找位置添加到resolovers当中. 其他 123456789101112$ touch build.sbt$ mkdir src$ mkdir src/main$ mkdir src/main/java$ mkdir src/main/resources$ mkdir src/main/scala$ mkdir src/test$ mkdir src/test/java$ mkdir src/test/resources$ mkdir src/test/scala$ mkdir project$ ... 可以使用giter8来自动化以上步骤. giter8的更多信息可参考https://github.com//giter8. 4. SBT Cmd actions – 显示对当前工程可用的命令 update – 下载依赖 compile – 编译代码 test – 运行测试代码 package – 创建一个可发布的jar包 publish-local – 把构建出来的jar包安装到本地的ivy缓存 publish – 把jar包发布到远程仓库（如果配置了的话) more cmd test-failed – 运行失败的spec test-quick – 运行所有失败的以及/或者是由依赖更新的spec clean-cache – 清除所有的sbt缓存。类似于sbt的clean命令 clean-lib – 删除lib_managed下的所有内容 5. Scala HelloWorld SBT Scala HelloWorld 具体请看 : Scala-Projects/HelloWorld ➜ HelloWorld&gt; sbt package 12345[info] Loading project definition from /Users/hp/spark/HelloWorld/project[info] Set current project to HelloWorld (in build file:/Users/hp/spark/HelloWorld/)[info] Packaging /Users/hp/spark/HelloWorld/target/scala-2.11/helloworld_2.11-0.0.1-SNAPSHOT.jar ...[info] Done packaging.[success] Total time: 1 s, completed 2016-3-17 9:05:44 ➜ HelloWorld&gt; sbt run 12345[info] Loading project definition from /Users/hp/spark/HelloWorld/project[info] Set current project to HelloWorld (in build file:/Users/hp/spark/HelloWorld/)[info] Running HiHi![success] Total time: 1 s, completed 2016-3-17 9:07:43 6. Spark HelloWorld Spark HelloWorld 具体请看 : Spark-Projects/HelloWorld ➜ HelloWorld&gt; sbt compile ➜ HelloWorld&gt; sbt package 123$SPARK_HOME/bin/spark-submit \\ --class &quot;HelloWorld&quot; \\ target/scala-2.11/helloworld_2.11-1.0.jar 7. Referenced# 参考 : scala-sbt.org/0.13/docs/zh-cn/Getting-Started.html 参考 : CSUG/real_world_scala/blob/master/02_sbt.markdown 参考 : scala-sbt.org/0.13.1/docs/Getting-Started 参考 : 译言网","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://www.iequa.com/tags/scala/"},{"name":"sbt","slug":"sbt","permalink":"http://www.iequa.com/tags/sbt/"}]},{"title":"大数据平台CDH集群在线安装","slug":"hadoop/hadoop-cdh-install-online","date":"2016-03-14T07:54:16.000Z","updated":"2021-06-20T04:12:28.321Z","comments":true,"path":"2016/03/14/hadoop/hadoop-cdh-install-online/","link":"","permalink":"http://www.iequa.com/2016/03/14/hadoop/hadoop-cdh-install-online/","excerpt":"介绍了 CDH 集群的搭建与安装，其中 Server 安装步骤非常准确, Agent 需要进一步验证.","text":"介绍了 CDH 集群的搭建与安装，其中 Server 安装步骤非常准确, Agent 需要进一步验证. 标签： Cloudera-Manager CDH Hadoop 部署 集群 摘要：管理、部署Hadoop集群需要工具，Cloudera Manager便是其一。本文详细记录了以在线方式部署CDH集群&gt;的步骤。 以Apache Hadoop为主导的大数据技术的出现，使得中小型公司对于大数据的存储与处理也拥有了武器。 目前Hadoop比较流行的主要有2个版本，Apache和Cloudera版本。 Apache Hadoop：维护人员比较多，更新频率比较快，但是稳定性比较差。 Cloudera Hadoop（CDH）：CDH：Cloudera公司的发行版本，基于Apache Hadoop的二次开发，优化了组件兼容和交互接口、简化安装配置、增加Cloudera兼容特性。 1大数据平台CDH集群 cdh-5.70-rpm_install 详细过程 Part 1 install cdh server 1.1 Ready install resources CentOS Linux release 7.1.1503 (Core) cm-5.7.0 cloudera-manager-installer.bin adduser deploy centos7.1 在安装过程时，网络配置，设置静态IP 1vim /etc/sysconfig/network-scripts/ifcfg-eth0 设置静态ip，以及指定ip地址 123456789DEVICE=&quot;eth0&quot;BOOTPROTO=&quot;static&quot;IPADDR=192.168.1.110NM_CONTROLLED=&quot;yes&quot;ONBOOT=&quot;yes&quot;TYPE=&quot;Ethernet&quot;DNS1=8.8.8.8DNS2=8.8.4.4GATEWAY=192.168.1.1 1.2 网络配置（所有节点）## 修改hostname为 cdh-server7 123 RedHat 的 hostname，就修改 /etc/sysconfig/network文件，将里面的 HOSTNAME 这一行修改成 HOSTNAME=NEWNAME，其中 NEWNAME 就是你要设置的 hostname。 Debian发行版的 hostname 的配置文件是 /etc/hostname 修改ip与主机名的对应关系 1234[root@cdh-server7 ~]# vi /etc/hosts #修改ip与主机名的对应关系:192.168.181.190 node190192.168.181.198 node198192.168.181.196 node196 重启网络服务生效 1[root@cdh-server7 ~]# service network restart 关闭SELINUX 123查看SELINUX状态[root@cdh-server7 ~]#getenforce 1234567891011121314若 SELINUX 没有关闭，按照下述方式关闭vi /etc/selinux/config修改SELinux=disabled。重启生效，可以等后面都设置完了重启主机# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - SELinux is fully disabled.SELINUX=disabled# SELINUXTYPE= type of policy in use. Possible values are:# targeted - Only targeted network daemons are protected.# strict - Full SELinux protection.SELINUXTYPE=targeted 1[root@cdh-server7 ~]# ping www.baidu.com 以上步骤执行完毕后，重启主机 1reboot 重启后再次检查下以上几点，确保环境配置正确。 1.3 卸载 openjdk (所有节点) 注意 : 如果没有openjdk, 则不需要卸载，默认 centos7 没有 1234567[root@cdh-server7 deploy]# rpm -qa | grep java[root@cdh-server7 deploy]# rpm -qa | grep jdk# if exist java or jdk, uninstall, erase it. example under this...[root@cdh-server7 deploy]# rpm -e --nodeps java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64[root@cdh-server7 deploy]# rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64[root@cdh-server7 deploy]# rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64 1.4 卸载 centOS7 默认mysql 12[root@cdh-server7 deploy]# rpm -qa | grep mariadb[root@cdh-server7 deploy]# rpm -e --nodeps mariadb-libs-5.5.41-2.el7_0.x86_64 1.5 Cloudera Manager安装 下载资源文件https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo 将cloudera-manager.repo文件拷贝到所有节点的/etc/yum.repos.d/文件夹下 123[root@node196 ]# cd /home/deploy/cdh[root@node196 cdh]# wget https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo[root@cdh-server7 cdh]# mv cloudera-manager.repo /etc/yum.repos.d/ 验证repo文件是否起效 1yum list|grep cloudera 1234567[root@cdh-server7 cdh]# yum list | grep clouderacloudera-manager-agent.x86_64 5.7.0-1.cm560.p0.54.el7 cloudera-managercloudera-manager-daemons.x86_64 5.7.0-1.cm560.p0.54.el7 cloudera-managercloudera-manager-server.x86_64 5.7.0-1.cm560.p0.54.el7 cloudera-managercloudera-manager-server-db-2.x86_64 5.7.0-1.cm560.p0.54.el7 cloudera-managerenterprise-debuginfo.x86_64 5.7.0-1.cm560.p0.54.el7 cloudera-manageroracle-j2sdk1.7.x86_64 1.7.0+update67-1 cloudera-manager 如果列出的不是你安装的版本，执行下面命令重试 12yum clean all yum list | grep cloudera 上传下列 rpm 包 到 [root@cdh-server7] 的 /home/deploy/cdh/cloudera-rpms (任意目录) 1234567cd /home/deploy/cdh/cloudera-rpmscloudera-manager-agent-5.7.0-1.cm560.p0.54.el7.x86_64.rpmcloudera-manager-daemons-5.7.0-1.cm560.p0.54.el7.x86_64.rpmcloudera-manager-server-5.7.0-1.cm560.p0.54.el7.x86_64.rpm ## agent not usecloudera-manager-server-db-2-5.7.0-1.cm560.p0.54.el7.x86_64.rpm ## agent not useenterprise-debuginfo-5.7.0-1.cm560.p0.54.el7.x86_64.rpmoracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm 说明 : 可从https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5/RPMS/x86_64/ 下载相关rpm包 切换到rpms目录下，执行 12[root@cdh-server7 cdh]# cd /home/deploy/cdh/cloudera-rpms/[root@cdh-server7 cloudera-rpms]# yum -y install *.rpm 1.6 拷贝资源包到目标目录 1从 http://archive.cloudera.com/cdh5/parcels/5.7.0/ 下载资源包 将之前下载的Parcel那3个文件拷贝到/opt/cloudera/parcel-repo目录下（如果没有该目录，请自行创建） 123[root@cdh-server7 cdh]# cp CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel /opt/cloudera/parcel-repo/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel[root@cdh-server7 cdh]# cp CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha[root@cdh-server7 cdh]# cp manifest.json /opt/cloudera/parcel-repo/manifest.json 1.7 配置 java 环境变量 设置JAVA_HOME 1234[root@cdh-server7 cdh]#vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera/export PATH=$JAVA_HOME/bin:$PATH[root@cdh-server7 cdh]#source /etc/profile 关闭防火墙 1[root@cdh-server7 deploy]#systemctl stop firewalld.service #centos7,关闭防火墙 以上步骤执行完毕后，重启主机 1reboot 1.8 安装CM (只在主节点) 以下两步骤请只在主节点上执行 : 进入该目录，给bin文件赋予可执行权限 1[root@cdh-server7 cdh]# chmod a+x ./cloudera-manager-installer.bin 安装CM (该步骤, 可能是不需要的) 1[root@cdh-server7 cdh]# ./cloudera-manager-installer.bin 开始启动server端 123456[root@cdh-server7 cdh]# cd /etc/init.d/[root@cdh-server7 init.d]# ./cloudera-scm-server-db start[root@cdh-server7 init.d]# ./cloudera-scm-server startStarting cloudera-scm-server: [ OK ][root@cdh-server7 init.d]# tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log 注意 : 机器重启之后，默认启动会导致异常 需要按照该先启动cloudera-scm-server-db，再启动cloudera-scm-server的顺序执行 1.9 浏览器访问验证(主节点) CM安装成功后浏览器输入http://ip:7180, 用户名和密码都输入admin，进入web管理界面。 通过浏览器访问验证 1http://192.168.181.190:7180/ 如果打不开改网页，等待2分钟后。这个服务启动是需要一定时间的。 选择部署的版本，这里我们选择免费版的就可以了。 如果不会设置，那么请参考 最靠谱的安装指南 http://www.jianshu.com/p/57179e03795f 安装服务时，数据库选择默认的嵌入式数据库 Part 2 安装 agent this step is similar， but I can’t be sure, exactly right. 1安装 agent ，可以在单独的机器，主节点，可以只当做主，随意你 为agent做配置,启动agent (所有节点) agent 不需要装server，其他绝大部分步骤和 安装 server 相同。 2.1 网络配置 修改ip与主机名的对应关系 1234[root@cdh-agent1 ~]# vi /etc/hosts #修改ip与主机名的对应关系:192.168.181.190 cdh-server7(node190)192.168.181.198 cdh-agent1(node198)192.168.181.196 cdh-agent2(node196) 重启网络服务生效 1[root@cdh-server7 ~]# service network restart 关闭SELINUX 123查看SELINUX状态[root@cdh-server7 ~]#getenforce 1234567891011121314若 SELINUX 没有关闭，按照下述方式关闭vi /etc/selinux/config修改SELinux=disabled。重启生效，可以等后面都设置完了重启主机# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - SELinux is fully disabled.SELINUX=disabled# SELINUXTYPE= type of policy in use. Possible values are:# targeted - Only targeted network daemons are protected.# strict - Full SELinux protection.SELINUXTYPE=targeted 1[root@cdh-server7 ~]# ping www.baidu.com 2.2 卸载 openjdk (所有节点)## 注意 : 如果没有openjdk, 则不需要卸载，默认 centos7 没有 1234567[root@cdh-server7 deploy]# rpm -qa | grep java[root@cdh-server7 deploy]# rpm -qa | grep jdk# if exist java or jdk, uninstall, erase it. example under this...[root@cdh-server7 deploy]# rpm -e --nodeps java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64[root@cdh-server7 deploy]# rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64[root@cdh-server7 deploy]# rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64 2.3 卸载centOS7默认mysql 12[root@cdh-server7 deploy]# rpm -qa | grep mariadb[root@cdh-server7 deploy]# rpm -e --nodeps mariadb-libs-5.5.41-2.el7_0.x86_64 2.4 cloudera-manager.repo 上传cloudera-manager.repo 到 cdh-agent1 [root@cdh-agent1 cdh]# cp cloudera-manager.repo /etc/yum.repos.d/ transparent_hugepage 12echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag vi /etc/rc.local 在文件尾放入 如下两条语句 12echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag 1chmod +x /etc/rc.local 调整swappiness 123echo 10 &gt; /proc/sys/vm/swappiness# vi /etc/sysctl.confvm.swappiness = 10 2.5 ~/cdh/cloudera-rpms 上传下列rpm包到cdh-agent1的/home/deploy/cdh/cloudera-rpms 1234567cloudera-manager-agent-5.7.0-1.cm560.p0.54.el7.x86_64.rpmcloudera-manager-daemons-5.7.0-1.cm560.p0.54.el7.x86_64.rpmenterprise-debuginfo-5.7.0-1.cm560.p0.54.el7.x86_64.rpmoracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm[root@cdh-agent1 init.d]# cd /home/deploy/cdh/cloudera-rpms/[root@cdh-agent1 init.d]# yum -y install *.rpm 设置JAVA_HOME 1234[root@cdh-server7 cdh]#vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera/export PATH=$JAVA_HOME/bin:$PATH[root@cdh-server7 cdh]#source /etc/profile 关闭防火墙 1[root@cdh-server7 deploy]#systemctl stop firewalld.service #centos7,关闭防火墙 以上步骤执行完毕后，重启主机 1reboot 1234567[root@cdh-agent1 init.d]# vi /etc/cloudera-scm-agent/config.ini+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# Hostname of the CM server.#server_host=localhostserver_host=cdh-server7(node190)+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 1234[root@cdh-server7 cdh]# cd /etc/init.d/[root@cdh-server7 init.d]# ./cloudera-scm-agent startStarting cloudera-scm-agent: [ OK ][root@cdh-server deploy]# tail -f /var/log//cloudera-scm-agent/cloudera-scm-agent.log 注意 : 1安装YARN NodeManager失败时，需要删除 /yarn /var/lib/hadoop-yarn 目录再重新添加 CDH最靠谱的安装指南 : http://www.jianshu.com/p/57179e03795f Part 3 恢复启动 Our 集群 3.1 确定 firewalld close 123systemctl start firewalld.service#启动firewallsystemctl stop firewalld.service#停止firewallsystemctl disable firewalld.service#禁止firewall开机启动 注意 : 操作之前确定 firewalld 是关闭的 12345678910111213141516171819[root@node19x flag]$ vim /etc/rc.local (/etc/rc.local 对应貌似相对dir /ect/init.d) 1 #!/bin/bash 2 # THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES 3 # 4 # It is highly advisable to create own systemd services or udev rules 5 # to run scripts during boot instead of using this file. 6 # 7 # In contrast to previous versions due to parallel execution during boot 8 # this script will NOT be run after all other services. 9 # 10 # Please note that you must run &#x27;chmod +x /etc/rc.d/rc.local&#x27; to ensure 11 # that this script will be executed during boot. 12 13 touch /var/lock/subsys/local 14 echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled 15 echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag 16 service ntpd start 17 service elasticsearch start 3.2 启动server端、cm only at server node 12345678[root@cdh-server7 cdh]# cd /etc/init.d/[root@cdh-server7 init.d]# ./cloudera-scm-server-db start[root@cdh-server7 init.d]# ./cloudera-scm-server startStarting cloudera-scm-server: [ OK ][root@cdh-server7 init.d]# tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log// 等待日志 7180 启动成功， 访问 : http://node190:7180/cmf/home 注意 : 机器重启之后，默认启动会导致异常 需要按照该先启动cloudera-scm-server-db，再启动cloudera-scm-server的顺序执行 一般以下 agent 是自动启动的 123456[root@node190 init.d]# ./cloudera-scm-agent startcloudera-scm-agent is already runningnode190:./cloudera-scm-agent startnode19x:./cloudera-scm-agent startnode19x:./cloudera-scm-agent start... 3.3 CM页面上启动各服务 CM 页面上重启 service monitor CM 页面上重启 host monitor CM 页面上启动各项服务 (如 : ZK, Flume, YARN, HDFS, Hive, Sqoop, Spark etc…) 3.4 各个节点启动 ES 12345678[deploy@node190 init.d]# lltotal 44-rwxr-xr-x 1 root root 8671 Apr 2 04:52 cloudera-scm-agentlrwxrwxrwx. 1 root root 58 Apr 18 16:55 elasticsearch -&gt; /home/deploy/elasticsearch-1.7.1/bin/service/elasticsearch-rw-r--r--. 1 root root 13948 Sep 16 2015 functions-rwxr-xr-x. 1 root root 2989 Sep 16 2015 netconsole-rwxr-xr-x. 1 root root 6630 Sep 16 2015 network-rw-r--r--. 1 root root 1160 Apr 1 00:45 README deploy 12345cd /home/deploy/elasticsearch-1.7.1/bin/service[deploy@node190 init.d]# ./elasticsearch start[deploy@node19x init.d]# ./elasticsearch start[deploy@node19x init.d]# ./elasticsearch start... 1http://node190:9200/_plugin/bigdesk/#cluster 等待同步数据完成，一般会很快，等待 Status 从 RED 变为 green 状态 1http://node190:9200/_plugin/head/ 3.5 启动 kibana 123[deploy@node196 ~]#cd /home/deploy/kibana-4.1.1-linux-x64 ./bin/kibana &gt; kibana.log 2&gt;&amp;1 &amp; --@deploy","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"http://www.iequa.com/tags/CDH/"}]},{"title":"Sqoop introduce","slug":"hadoop/hadoop-sqoop-learn-use01","date":"2016-02-16T07:54:16.000Z","updated":"2021-06-20T04:12:28.319Z","comments":true,"path":"2016/02/16/hadoop/hadoop-sqoop-learn-use01/","link":"","permalink":"http://www.iequa.com/2016/02/16/hadoop/hadoop-sqoop-learn-use01/","excerpt":"Sqoop 即 SQL to Hadoop, 是一款方便的在传统关系数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输.","text":"Sqoop 即 SQL to Hadoop, 是一款方便的在传统关系数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输. 1. Sqoop what ? sqoop 即 SQL to Hadoop ，是一款方便的在传统关系数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输，发展至今主要演化了二大版本，sqoop1和sqoop2。 sqoop : clouder 公司开发 生产背景 mysql 导入 Hadoop Hadoop 导入 mysql 注 : 以上 Hadoop 指 Hive、HBase、HDFS 等 2. Sqoop 特点 sqoop架构非常简单，其整合了Hive、Hbase和Oozie，通过map-reduce任务来传输数据，从而提供并发特性和容错。 Sqoop 由两部分组成：客户端(client)和服务端(server)。需要在集群的其中某个节点上安装server，该节点的服务端可以作为其他 Sqoop 客户端的入口点。 在 server 端的节点上必须安装有 Hadoop。client 可以安装在任意数量的机子上。在装有客户端的机子上不需要安装 Hadoop。 12345sqoop 官网 : https://sqoop.apache.org1.4.5官方文档 : https://sqoop.apache.org/docs/1.4.5/sqoop2不推荐的原因 : http://blog.csdn.net/robbyo/article/details/50737356 3. Sqoop 优缺点 优点 高效可控的利用资源，任务并行度，超时时间。 数据类型映射与转化，可自动进行，用户也可自定义 . 支持多种主流数据库，MySQL,Oracle，SQL Server，DB2等等 。 缺点 基于命令行的操作方式，易出错，且不安全。 数据传输和数据格式是紧耦合的，这使得connector无法支持所有的数据格式 用户名和密码暴漏出来 4. Sqoop 原理 4.1 Sqoop的import原理 Sqoop 在 import 时，需要制定 split-by 参数。 Sqoop 根据不同的 split-by参数值 来进行切分, 然后将切分出来的区域分配到不同 map 中。每个map中再处理数据库中获取的一行一行的值，写入到 HDFS 中。同时split-by 根据不同的参数类型有不同的切分方法，如比较简单的int型，Sqoop会取最大和最小split-by字段值，然后根据传入的 num-mappers来确定划分几个区域。 比如 select max(split_by),min(split-by) from 得到的 max(split-by)和 min(split-by) 分别为 1000 和 1, 而 num-mappers 为 2 的话，则会分成两个区域 (1,500) 和 (501-100), 同时也会分成 2个sql 给 2个map 去进行导入操作，分别为 select XXX from table where split-by&gt;=1 and split-by&lt;500 和 select XXX from table where split-by&gt;=501 and split-by&lt;=1000。最后每个map各自获取各自SQL中的数据进行导入工作。 4.2. Sqoop的export原理 根据 mysql 表名称，生成一个以表名称命名的 Java类，该类继承了 sqoopRecord的，是一个只有 Map 的 MR，且自定义了输出字段。 sqoop export --connect jdbc:mysql://$url:3306/$3?characterEncoding=utf8 --username $username --password $password --table $1 --export-dir $2 --input-fields-terminated-by ‘|’ --null-non-string ‘0’ --null-string ‘0’; 5. Sqoop 使用实例 环境 123sqoop: sqoop-1.4.5+cdh5.3.6+78hive : hive-0.13.1+cdh5.3.6+397hbase: hbase-0.98.6+cdh5.3.6+115 5.1. Mysql to Hadoop Mysql to Hdfs 12345678sqoop import \\ --connect $&#123;jdbc_url&#125; --username $&#123;jdbc_username&#125; --password $&#123;jdbc_passwd&#125; \\ --query &quot;$&#123;exec_sql&#125;&quot; \\ --split-by $&#123;id&#125; -m 10 \\ --target-dir $&#123;target_dir&#125; \\ --fields-terminated-by &quot;\\001&quot; --lines-terminated-by &quot;\\n&quot; \\ --hive-drop-import-delims \\ --null-string &#x27;\\\\N&#x27; --null-non-string &#x27;\\\\N&#x27; Mysql To Hive 123456sqoop import \\ --connect $&#123;jdbc_url&#125; \\ --username $&#123;jdbc_username&#125; --password $&#123;jdbc_passwd&#125; \\ --table $&#123;jdbc_table&#125; --fields-terminated-by &quot;\\001&quot; --lines-terminated-by &quot;\\n&quot; \\ --hive-import --hive-overwrite --hive-table $&#123;hive_table&#125; \\ --null-string &#x27;\\\\N&#x27; --null-non-string &#x27;\\\\N&#x27; Mysql To HBase 5.2 Hadoop to Mysql Hdfs To Mysql 123456789sqoop export -D sqoop.export.records.per.statement=10 \\--connect jdbc:mysql://192.168.***.**:3306/***?autoReconnect=true --username *** --password *** --table mds_dm_rs_shop_result \\--fields-terminated-by &#x27;\\t&#x27; --export-dir &quot;/dc_ext/xbd/dm/mds/mds_dm_rs_shop_result/dt=20170410&quot; --null-string &#x27;\\\\N&#x27; --null-non-string &#x27;\\\\N&#x27;; refence article Sqoop中文文档 Hive to Mysql 常遇九大问题总结","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"http://www.iequa.com/tags/sqoop/"}]},{"title":"Hive Introduce 1","slug":"hadoop/hadoop-hive-brief","date":"2016-02-15T07:07:21.000Z","updated":"2021-06-20T04:12:28.322Z","comments":true,"path":"2016/02/15/hadoop/hadoop-hive-brief/","link":"","permalink":"http://www.iequa.com/2016/02/15/hadoop/hadoop-hive-brief/","excerpt":"初步了解 Hadoop 生态圈 初步了解 Hive 架构图","text":"初步了解 Hadoop 生态圈 初步了解 Hive 架构图 1. Hive Introduce 1.1 Hive Preface Hadoop Hadoop 生态系统 是 处理大数据集而产生的解决方案。 Hadoop 实现计算模型 MapReduce, 可将计算任务分割成多个处理单元，这个计算模型下面是一个 HDFS。 Hive Hive 提供了一个 Hive查询语言 HiveQL, 查询转换为 MapReduce job Hive 适合做数据仓库，可离线维护海量数据，可对数据进行挖掘, 形成报告等 Hadoop、HDFS 设计本身限制了 Hive 所能胜任的工作, Hive 不支持记录级别的更新、插入 或者 删除 操作。 Hive 运行架构 使用 HQL 作为查询接口； 使用 MapReduce 作为执行层； 使用 HDFS 作为存储层； 1.2 Hadoop / Mapreduce Input -&gt; Mappers -&gt; Sort,Shuffle -&gt; Reducers -&gt; Output 1.3 Hive 系统架构 2. Hive 架构组件分析 本章重点 : 初步了解 Hive 的工作流 初步了解 hive 的工作组件 2.1 元数据存储Metastore Hive的数据由两部分组成：数据文件 和 元数据 123元数据存储，Derby只能用于一个Hive连接，一般存储在MySQL。元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。 2.2 驱动 (Driver) 编译器 优化器 执行器 用户通过下面的接口提交Hive给Driver，由Driver进行HQL语句解析，此时从Metastore中获取表的信息，先生成逻辑计划，再生成物理计划，再由Executor生成Job交给Hadoop运行，然后由Driver将结果返回给用户。 编译器（Hive的核心）：1，语义解析器（ParseDriver），将查询字符串转换成解析树表达式；2，语法解析器（SemanticAnalyzer），将解析树转换成基于语句块的内部查询表达式；3，逻辑计划生成器（Logical Plan Generator），将内部查询表达式转换为逻辑计划，这些计划由逻辑操作树组成，操作符是Hive的最小处理单元，每个操作符处理代表一道HDFS操作或者是MR作业；4，查询计划生成器（QueryPlan Generator），将逻辑计划转化成物理计划（MR Job）。 优化器：优化器是一个演化组件，当前它的规则是：列修剪，谓词下压。 执行器：编译器将操作树切分成一个Job链（DAG），执行器会顺序执行其中所有的Job；如果Task链不存在依赖关系，可以采用并发执行的方式进行Job的执行。 2.3 接口 CLI、HWI、ThriftServer CLI：为命令行工具，默认服务。bin/hive或bin/hive–service cli； HWI：为Web接口，可以用过浏览器访问Hive，默认端口9999，启动方式为bin/hive --service hwi; ThriftServer：通过Thrift对外提供服务，默认端口是10000，启动方式为bin/hive --service hiveserver; ** 连接hive-metastore(如mysql)的三种方式 ** 单用户模式。此模式连到数据库Derby，一般用于Unit Test。 多用户模式。通过网络连接到一个数据库中，是最经常使用到的模式。 远程服务器模式。用于非Java客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer访问元数据库。 2.4 其他服务 bin/hive --service -help metastore (bin/hive --service metastore) hiveserver2（bin/hive --service hiveserver2） HiveServer2 HiveServer2是HieServer改进版本，它提供给新的ThriftAPI来处理JDBC或者ODBC客户端，进行Kerberos身份验证，多个客户端并发 HS2还提供了新的CLI：BeeLine，是Hive 0.11引入的新的交互式CLI，基于SQLLine，可以作为Hive JDBC Client 端访问HievServer2，启动一个beeline就是维护了一个session. Hive下载地址 cdh-hive : hive0.13.1-cdh5.3.6 jar 包 (没用) apache-hive : Apache-Hive Hive-Beeline 试验成功 12345678910111213141516171819202122下载 apache-hive-0.13.1-bin, apache-hadoop2.5，配置 HADOOP_HOME, 启动 ➜ ./apache-hive-0.13.1-bin/bin/beelineBeeline version 0.13.1 by Apache Hivebeeline&gt; !connect jdbc:hive2://node190:10000 hdfs 1scan complete in 3msConnecting to jdbc:hive2://node190:10000Connected to: Apache Hive (version 0.13.1-cdh5.3.6)Driver: Hive JDBC (version 0.13.1)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://node190:10000&gt; select count(*) from ods_dm_shop_tmp;+-------+| _c0 |+-------+| 1091 |+-------+1 row selected (24.815 seconds)0: jdbc:hive2://node190:10000&gt;说明 : beeline 可以成功，用代码 jdbc 就可以成功安装 hadoop 参考了 《Spark大数据处理》高彦杰@著, 不用配置直接绿色简单版 Hive table table 中的一个 Partition 对应表下的一个子目录 每一个 Bucket 对应一个文件； Hive的默认数据仓库目录是/user/hive/warehouse 在hive-site.xml中由hive.metastore.warehouse.dir项定义； reference article 参考 : CSDN - Hive Server 2 调研，安装和部署 参考 : 极豆技术博客 - Beeline连接hiveserver2异常 参考 : Hive学习之HiveServer2 JDBC客户端 参考 : HiveServer2 Clients beeline 参考 : Beeline连接hiveserver2异常 参考 : Hive学习之HiveServer2服务端配置与启动 other tmp 1234567891011## Chap 7 HiveQL 视图 #### Chap 8 HiveQL 索引 #### Chap 9 模式设计 #### Chap 10 调优 #### Chap 11 其他文件格式和压缩方法 #### Chap 12 开发 #### Chap 13 函数 #### Chap 14 Streaming #### Chap 15 自定义Hive文件和记录格式 #### Chap 16 Hive 的 Thrift 服务 #### Chap 11 其他文件格式和压缩方法 ##","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://www.iequa.com/tags/hive/"}]},{"title":"Spark Introduce and Install","slug":"spark/spark-introduce-and-install","date":"2016-02-01T02:07:21.000Z","updated":"2021-06-20T04:12:28.350Z","comments":true,"path":"2016/02/01/spark/spark-introduce-and-install/","link":"","permalink":"http://www.iequa.com/2016/02/01/spark/spark-introduce-and-install/","excerpt":"介绍 Spark 的历史，介绍 Spark 的安装与部署，介绍 Spark 的代码架构 等","text":"介绍 Spark 的历史，介绍 Spark 的安装与部署，介绍 Spark 的代码架构 等 Spark 发源于 美国加州大学伯克利分校 AMPLap 大数据分析平台 Spark 立足于内存计算、从多迭代批量处理出发 Spark 兼顾数据仓库、流处理、图计算 等多种计算范式，大数据系统领域全栈计算平台 spark.apache.org University of California, Berkeley 1. Spark 的历史与发展 2009 年 : Spark 诞生于 AMPLab 2014-02 : Apache 顶级项目 2014-05 : Spark 1.0.0 发布 2. Spark 之于 Hadoop Spark 是 MapReduce 的替代方案, 且兼容 HDFS、Hive 等分布式存储层。 Spark 相比 Hadoop MapReduce 的优势如下 : 中间结果输出 数据格式和内存布局 执行策略 任务调度的开销 Spark用事件驱动类库AKKA来启动任务, 通过线程池复用线程避免进线程启动切换开销 3. Spark 能带来什么 ? 打造全栈多计算范式的高效数据流水线 轻量级快速处理, 并支持 Scala、Python、Java 与 HDFS 等 存储层 兼容 4. Spark 安装与部署 Spark 主要使用 HDFS 充当持久化层，所以完整的安装 Spark 需要先安装 Hadoop. Spark 是计算框架, 它主要使用 HDFS 充当持久化层。 Linux 集群安装 Spark 安装 JDK 安装 Scala 配置 SSH 免密码登陆 (可选) 安装 Hadoop 安装 Spark 启动 Spark 集群 Spark官网下载 4.1 安装 Spark 12345678910111213141516171819202122232425(1). download spark-1.5.2-bin-hadoop2.6.tgz(2). tar -xzvf spark-1.5.2-bin-hadoop2.6.tgz(3). 配置 conf/spark-env.sh 1) 详细复杂参数配置参见 官网 Configuration 2) vim conf/spark-env.sh export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home export SCALA_HOME=/usr/local/Cellar/scala/2.11.5 export SPARK_HOME=/usr/local/xSoft/spark export SPARK_MASTER_IP=ip export MASTER=spark://ip:7077 export SPARK_EXECUTOR_INSTANCES=2 export SPARK_EXECUTOR_CORES=1 export SPARK_WORKER_MEMORY=1000m export SPARK_EXECUTOR_MEMORY=300m export SPARK_LIBRARY_PATH=$&#123;SPARK_HOME&#125;/lib(4). 配置 conf/slaves (测试可选)(5). 一般需要 startup ssh server. 4.2 启动 Spark 集群 在 Spark 根目录启动 Spark 12./sbin/start-all.sh./sbin/stop-all.sh 启动后 jps 查看 会有 Master 进程存在 1234➜ spark-1.5.2-bin-hadoop2.6 jps11262 Jps11101 Master11221 Worker 4.3 Spark 集群初试 可以通过两种方式运行 Spark 样例 : 以 ./run-example 的方式执行 123➜ cd /usr/local/xSoft/spark➜ spark ./sbin/start-all.sh➜ spark ./bin/run-example org.apache.spark.examples.SparkPi 以 ./Spark Shell 方式执行 12345678910111213141516171819202122232425262728scala&gt; import org.apache.spark._import org.apache.spark._scala&gt; object SparkPi &#123; | | def main(args: Array[String]) &#123; | | val slices = 2 | val n = 100000 * slices | | val count = sc.parallelize(1 to n, slices).map &#123; i =&gt; | | val x = math.random * 2 - 1 | val y = math.random * 2 - 1 | | if (x * x + y * y &lt; 1) 1 else 0 | | &#125;.reduce(_ + _) | | println(&quot;Pi is rounghly &quot; + 4.0 * count / n) | | &#125; | &#125;defined module SparkPiscala&gt;// Spark Shell 已默认将 SparkContext 类初始化为对象 sc, 用户代码可直接使用。// Spark 自带的交互式的 Shell 程序，方便进行交互式编程。 通过 Web UI 查看集群状态 http：//masterIp:8080 &lt;img src=&quot;/images/spark/spark-introduce-05.png&quot; width=“740” height=“400”/img&gt; 4.4 Spark quick start quick-start : https://spark.apache.org/docs/latest/quick-start.html ./bin/spark-shell 123456789scala&gt; val textFile = sc.textFile(&quot;README.md&quot;)textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let’s start with a few actions:scala&gt; textFile.count() // Number of items in this RDDres0: Long = 126scala&gt; textFile.first() // First item in this RDDres1: String = # Apache Spark 5. Spark 生态 BDAS Spark 框架、架构、计算模型、数据管理策略 Spark BDAS 项目及其子项目进行了简要介绍 Spark 生态系统包含的多个子项目 : SparkSql、Spark Streaming、GraphX、MLlib Spark 是 BDAS 核心, 是一 大数据分布式编程框架 6. Spark 架构 Spark 的代码结构 Spark 的架构 Spark 运行逻辑 6.1 Spark 的代码结构 12345678scheduler：文件夹中含有负责整体的Spark应用、任务调度的代码。broadcast：含有Broadcast（广播变量）的实现代码，API中是Java和Python API的实现。deploy：含有Spark部署与启动运行的代码。common：不是一个文件夹，而是代表Spark通用的类和逻辑实现，有5000行代码。metrics：是运行时状态监控逻辑代码，Executor中含有Worker节点负责计算的逻辑代码。partial：含有近似评估代码。 6.2 Spark 的架构 Spark架构采用了分布式计算中的Master-Slave模型。 Role description Master 对应集群中的含有Master进程的节点, 集群的控制器 Slave 集群中含有Worker进程的节点 Client 作为用户的客户端负责提交应用 Driver 运行Application的main()函数并创建SparkContext。负责作业的调度，即Task任务的分发 Worker 管理计算节点和创建Executor，启动Executor 或 Driver. 接收主节点命令与进行状态汇报 Executor Worker node执行任务的组件,负责 Task 的执行,用于启动线程池运行任务 ClusterManager Standalone 模式中为 Master, 控制整个集群, 监控Worker SparkContext 整个应用的上下文, 控制App的生命周期 RDD Spark的基本计算单元，一组RDD可形成执行的 DAG Num Spark App 流程 1. Client 提交应用 2. Master 找到一个 Worker 启动 Driver 3. Driver 向 Master 或者 资源管理器申请资源，之后将应用转化为 RDD Graph 4. DAGScheduler 将 RDD Graph 转化为 Stage的有向无环图 提交给 TaskScheduler 5. TaskScheduler 提交 task 给Executor执行 6. 在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行 在执行阶段，Driver 会将 Task 和 Task所依赖的file 和 jar 序列化后传递给对应的 Worker机器，同时 Executor对相应数据分区的任务进行处理。 7. 小结 由于 Spark 主要使用 HDFS 充当持久化层，所以完整的使用 Spark 需要预先安装 Hadoop. Spark 将分布式的内存数据抽象为弹性分布式数据集 (RDD), 并在其上实现了丰富的算子，从而对 RDD 进行计算，最后将 算子序列 转化为 DAG 进行执行和调度。 Spark的Python API几乎覆盖了所有Scala API所能提供的功能. 但的确有些特性，比如Spark Streaming和个别的API方法，暂不支持。 具体参见《Spark编程指南》的Python部分 体会了 函数式 编程. 个人认为 scala、python 比较适合写 spark 程序.","categories":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"}]},{"title":"Kettle ETL","slug":"dataware/ops-etl-kettle","date":"2016-01-22T07:34:16.000Z","updated":"2021-06-20T04:12:28.291Z","comments":true,"path":"2016/01/22/dataware/ops-etl-kettle/","link":"","permalink":"http://www.iequa.com/2016/01/22/dataware/ops-etl-kettle/","excerpt":"Kettle 的使用初步介绍","text":"Kettle 的使用初步介绍 ETL 是数据抽取（Extract）、清洗（Cleaning）、转换（Transform）、装载（Load）的过程。 ETL 是构建 DW 的重要一环，用户从数据源抽取出数据，经 数据清洗,按照预定义好的 DW模型，将数据加载到 DW 中去。 ETL 是将业务系统的数据经过抽取、清洗转换之后加载到 DW 的过程，目的是将企业中的分散零乱、标准不统一的数据到一起，为企业的决策提供分析依据。 ETL 是 BI 项目中一个重要环节。 ETL的设计分三个部分： 数据抽取 数据的清洗转换 数据的加载 1. Kettle 开源的 ETL 工具 1-1. Kettle 的介绍 ETL（Extract-Transform-Load的缩写，即数据抽取、转换、装载的过程， 我们经常会遇到各种数据的处理，转换，迁移，所以掌握一种 ETL 工具的使用必不可少。 Kettle 支持图形化的GUI设计界面，然后可以以工作流的形式流转，熟练它可以减少非常多的研发工作量，提高工作效率。 Kettle 允许你管理来自不同数据库的数据，通过提供一个图形化的用户环境来描述你想做什么。 Kettle 中有两种脚本文件，transformation 和 job. transformation 完成针对数据的基础转换. job 则完成整个工作流的控制。 1-2. Kettle 家族产品 ** Kettle家族目前包括 4 个产品：Spoon、Pan、CHEF、Kitchen。** Spoon 允许你通过图形界面来设计 ETL 转换过程（Transformation）。 Pan 允许你批量运行由 Spoon 设计的 ETL 转换 (例如使用一个时间调度器)。Pan 是一后台执行的程序，没图界面。 Chef 允许你创建任务（Job）。 任务通过允许每个转换，任务，脚本等等，更有利于自动化更新数据仓库的复杂工作。任务通过允许每个转换，任务，脚本等等。任务将会被检查，看看是否正确地运行了。 Kitchen 允许你批量使用由 Chef 设计的任务 (例如使用一个时间调度器)。Kitchen 也是后台运行的程序。 2. 下载和部署安装 Kettle可以在http://kettle.pentaho.org/ 网站下载  下载 kettle 压缩包，因 kettle 为绿色软件，解压缩到任意本地路径即可 安装需要 : JDK、JAVA_HOME、CLASSPATH、PENTAHO_JAVA_HOME 等环境变量。 如需连接mysql，则需将 mysql-connector-java-5.1.38.jar 放入到 lib 中。 2-1 kettle windows 安装 建议在 windows 下使用操作练习 kettle windows 对图形化 支持好 直接启动 Spoon.bat 即可 2-2 kettle Linux 安装 linux 图形化不强，如需要在 linux 中查看一下 kettle 资源库是否连接正常，以及在 linux 上调度 kettle 的 job，就需要在 Linux上 配置 kettle 环境了。 验证 kettle 部署成功 12cd data-integration输入命令./kitchen.sh。如果出现帮助信息说明部署成功 如出现错误，请 chmod +x *.sh，再试。 2-3 kettle osx 安装 已经存在 3. 应用场景 这里简单概括一下几种具体的应用场景，按网络环境划分主要包括： 3-1 表视图模式： 这种情况我们经常遇到，就是在同一网络环境下，我们对各种数据源的表数据进行抽取、过滤、清洗等，例如历史数据同步、异构系统数据交互、数据对称发布或备份等都归属于这个模式；传统的实现方式一般都要进行研发（一小部分例如两个相同表结构的表之间的数据同步，如果sqlserver数据库可以通过发布/订阅实现），涉及到一些复杂的一些业务逻辑如果我们研发出来还容易出各种bug； 3-2 前置机模式 数据交换的双方 A 和 B 网络不通，但是 A 和 B 都可以和前置机 C 连接… 3-3 文件模式 数据交互的双方 A 和 B 是完全的物理隔离，这样就只能通过以文件的方式来进行数据交互了，例如 XML 格式. 4. DEMO实战 4-1 简单表同步 功能描述 : 数据库 TestDB01 中的 UsersA表 到 数据库TestDB02 的UsersB表； 实现流程 : 建立一个转换和一个作业Job； 一、建立转换 进入主界面，新建一个转换，转换的后缀名为 ktr. 创建 DB连接，选择新建 DB连接, Test按钮测试是否配置正确！ 我们需要建立两个 DB连接，分别为 TestDB01 和 TestDB02； (如报错可以 : 下载 mysql-connect jar 放入 lib 目录下) 建立步骤和步骤关系 : [输入] -&gt; [表输入] 点击核心对象，我们从步骤树中选择【表输入】, 这样拖拽一个 表输入 之后，我们双击表输入之后，我们自己可以随意写一个 sql 语句，这个语句表示 可以在这个库中随意组合，只要 sql 语句没有错误即可，我这里只是最简单的把 TestA 中的所有数据查出来，语句为 select * from usersA。 建立步骤和步骤关系 : [输出] -&gt; [插入/更新] 同上类似 建立 连接 关系 然后在【表输入】上同时按住 shift 键和鼠标左键滑向【插入/更新】，这样建立两个步骤之间的连接 运行 建立好转换之后，我们可以直接运行(点击上面的小三角形)这个转换，检查一下是否有错，如图，有错误都会在下面的控制台上输出。 二、建立作业 : 如果我们需要让这个转换定时执行怎么办呢，那么我们需要建立一个作业job 6. 新建 Job 文件-&gt;新建-&gt;Job 在 Job 中 添加 转换 在新建的作业中, 打开刚才新建的 [简单表同步] 的 transformation 添加 START 通用 -&gt; START 使 START 关联 -&gt; [简单表同步] Transformation 这样我们在【Start】步骤上面双击 设置时间间隔、定时执行 等需要的参数 这样这个作业就制定好了，点击保存之后，就可以在图形化界面上点击开始执行了。 5. win/linux 后台运行 5-1 win 后台运行 simpleTableSync.bat 1234567891011@echo off if &quot;%1&quot; == &quot;h&quot; goto begin mshta vbscript:createobject(&quot;wscript.shell&quot;).run(&quot;%~nx0 h&quot;,0)(window.close)&amp;&amp;exit :beginC:cd C:\\WorkSoft\\data-integrationkitchen /file:C:\\WorkJob\\ETL\\tSyncTestJob.kjb /level:Basic&gt;&gt;C:\\WorkJob\\ETL\\MyTest.log /level:Basic&gt;&gt;C:\\WorkJob\\ETL\\MyTest.log 5-2 linux 后台运行 simpleTableSync.sh 12345678910111213141516#!/bin/bash################################################################### @date: 2016.01.28# @desc: simpleTableSync @kettle##################################################################cd `dirname $0`/.. &amp;&amp; wk_dir=`pwd` &amp;&amp; cd -source $&#123;wk_dir&#125;/util/envecho_ex &quot;$&#123;data_integration&#125;/kitchen.sh -file=$&#123;data_dir&#125;/tSyncTestJob.kjb&quot;$&#123;data_integration&#125;/kitchen.sh -file=$&#123;data_dir&#125;/tSyncTestJob.kjbcheck_successexit 0 注意 : kjb 与 ktr 最好放在一个目录下。 12345[hdfs@node196 simpleTableSync]$ cd data/[hdfs@node196 data]$ lltotal 24-rw-rw-r--. 1 hdfs hdfs 6944 Jan 29 18:22 tSyncTestJob.kjb-rw-rw-r--. 1 hdfs hdfs 13450 Jan 29 18:22 tSyncTestTrans.ktr 从 win 拷贝过来的文件，fileformat 可能是 dos 格式，可以 :set ff=unix. Reference article kettle系列","categories":[{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"}],"tags":[{"name":"ETL","slug":"ETL","permalink":"http://www.iequa.com/tags/ETL/"}]},{"title":"Logback 入门初步","slug":"java/java-logback-indoor","date":"2015-12-27T07:54:16.000Z","updated":"2021-06-20T04:12:28.294Z","comments":true,"path":"2015/12/27/java/java-logback-indoor/","link":"","permalink":"http://www.iequa.com/2015/12/27/java/java-logback-indoor/","excerpt":"Logback 一个开源日志组件, SLF4J 这个简单的日志前端接口（Façade）来替代 Jakarta Commons-Logging 。","text":"Logback 一个开源日志组件, SLF4J 这个简单的日志前端接口（Façade）来替代 Jakarta Commons-Logging 。 Logback 一个开源日志组件。 Logback 当前分成三个模块：logback-core logback- classic 和 logback-access。 1. logback 简介 Ceki在Java日志领域世界知名。他创造了Log4J ，这个最早的Java日志框架即便在JRE内置日志功能的竞争下仍然非常流行。随后他又着手实现SLF4J 这个“简单的日志前端接口（Façade）”来替代Jakarta Commons-Logging 。 Logback，一个“可靠、通用、快速而又灵活的Java日志框架”。 官网网址 : http://logback.qos.ch/ 2. 工程使用需要的 jar 要在工程里面使用 logback , 只需要以下jar文件： (1). slf4j-api.jar (2). logback-access.jar (3). logback-classic.jar (4). logback-core.jar logback-core 是其它两个模块的基础模块。 logback-classic 是 log4j 的一个 改良版本。 logback-access 与Servlet容器集成提供通过Http来访问日志功能 logback-classic 完整实现 SLF4J API 使你可以很方便地更换成其它日志系统如 log4j 或 JDK Logging。 3. logback 常用配置详解 3.1 根节点&lt; configuration &gt; configuration 说明 scan 当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。 scanPeriod 设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 debug 当此属性设置为true时，将打印出 logback 内部日志信息，实时查看logback运行状态。默认值为false。 4. logback 配置示例 4.1 Myself resources/logback.xml example 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。--&gt;&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;3600 seconds&quot; debug=&quot;false&quot;&gt; &lt;property name=&quot;AppName&quot; value=&quot;your_app_name&quot;/&gt; &lt;property name=&quot;LogParentDir&quot; value=&quot;/home/www/logs/&quot;/&gt; &lt;contextName&gt;$&#123;AppName&#125;&lt;/contextName&gt; &lt;appender name=&quot;infoAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;$&#123;LogParentDir&#125;/$&#123;AppName&#125;/infoLogFile.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=&quot;errorAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;$&#123;LogParentDir&#125;/$&#123;AppName&#125;/errorLogFile.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--其中appender的配置表示打印到控制台--&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;logger name=&quot;com.x.dmt&quot; level=&quot;ERROR&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;errorAppender&quot;/&gt; &lt;/logger&gt; &lt;!--设置addtivity为false，将此loger的打印信息不向上级传递；--&gt; &lt;logger name=&quot;com.x.dmt.service&quot; level=&quot;INFO&quot; additivity=&quot;fasle&quot;&gt; &lt;appender-ref ref=&quot;infoAppender&quot;/&gt; &lt;/logger&gt; &lt;!-- 注意: logger 同名情况, 级别低的,需要放在下面,否则级别高的会覆盖级别低的权限,早晨级别低的打印不出来日志 --&gt;&lt;/configuration&gt; 更多参见 iteye1101260 官方网址","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"logback","slug":"logback","permalink":"http://www.iequa.com/tags/logback/"}]},{"title":"Work 常用的命令积累","slug":"devops/work-used-commands","date":"2015-05-29T10:09:44.000Z","updated":"2021-06-20T04:12:28.204Z","comments":true,"path":"2015/05/29/devops/work-used-commands/","link":"","permalink":"http://www.iequa.com/2015/05/29/devops/work-used-commands/","excerpt":"2015年工作时候的常用的命令积累","text":"2015年工作时候的常用的命令积累 awk 0. 找出 =与&amp; 之间的字符串 并输出 1234567awk &#x27;&#123;print $3&#125;&#x27; (1) awk -F&quot;[=|&amp;]&quot; &#x27;&#123;print $2&#125;&#x27; file1 &gt; file2 (2) awk -F&quot;[:|\\n]&quot; &#x27;&#123;print $2&#125;&#x27; file1 &gt; file2 awk -F&quot;[\\^|\\^]&quot; &#x27;&#123;print $2&#125;&#x27; file1 &gt; file2 (3) sed &#x27;/\\^/d&#x27; 试试awk -F&quot;[crowd_list&quot;:|&quot;attachment&quot;]&quot; &#x27;&#123;print $2&#125;&#x27; file1 &gt; file2 shell 1). shell 中 cut 命令的用法 1echo root:x:0:0:root:/root:/bin/bash | cut -d : -f 6-7 2). 如何查看一个目录占用的空间 1du -sh * 3). 后台运行脚本的命令 1nohup sh create_redis.sh &gt; create_redis.log 2&gt;&amp;1 &amp; 4). 查找目录下的所有文件中是否含有某个字符串 1find .|xargs grep -ri &quot;IBM&quot; 5). 查找目录下的所有文件中是否含有某个字符串,并且只打印出文件名 1find .|xargs grep -ri &quot;IBM&quot; -l 6). 让一个变量具有双引号 1&#x27;&quot;$&#123;plat&#125;:$version_init&quot;&#x27; 7). 批量替换字符串 1sed -i &quot;s/oldString/newString/g&quot; `grep oldString -rl ./` 8). VIM tab ^I 替换 1:set listchars=tab:\\ \\ ,eol:$ git 123456789101112git clone http://****/dm/dmp_engine.git （注意这个地址是 HTTP 不是 SSH） cd dmp_enginegit branch -rgit checkout 分支名，这是重新 down 分支的步骤git add filenamegit commit -m “a&quot;git push origin 分支名 或者git push -u origin origin/dev_dmp_online_servinggit checkout branch_name 切换到这个分支之下git checkout -b online_redis3.0 master (从 master 拉下 新分支, 新分支名叫 online_redis3.0) linux os_info 查看 内存 与 CPU 信息 1). 查看内存 1cat /proc/meminfo 2). 查看物理CPU的个数 1#cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq | wc -l 3). 查看逻辑CPU的个数 1#cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l 4). 查看CPU是几核 1#cat /proc/cpuinfo | grep &quot;cores&quot; | uniq 5). 查看CPU的主频 1#cat /proc/cpuinfo | grep MHz | uniq hive 1hive -e &quot;select attributes[&#x27;lt_its&#x27;],attributes[&#x27;ad_clicks&#x27;] from new_algo_user_attributes where dt=&#x27;20150419&#x27; and platform=&#x27;pc&#x27; and attributes[&#x27;lt_its&#x27;]&lt;&gt;&#x27;NULL&#x27; limit 10&quot; Reference","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"work-cmd","slug":"work-cmd","permalink":"http://www.iequa.com/tags/work-cmd/"}]},{"title":"Hive 中 udf、udaf 和 udtf 的使用","slug":"hadoop/hadoop-hive-udf-udaf","date":"2015-02-01T02:07:21.000Z","updated":"2021-06-20T04:12:28.322Z","comments":true,"path":"2015/02/01/hadoop/hadoop-hive-udf-udaf/","link":"","permalink":"http://www.iequa.com/2015/02/01/hadoop/hadoop-hive-udf-udaf/","excerpt":"Hive 是基于 Hadoop 中的 MapReduce，提供 HQL 查询的数据仓库. Hive 是一个很开放的系统，很多内容都支持用户定制. 如 : 文件格式、MR脚本、自定义函数、自定义聚合函数 等.","text":"Hive 是基于 Hadoop 中的 MapReduce，提供 HQL 查询的数据仓库. Hive 是一个很开放的系统，很多内容都支持用户定制. 如 : 文件格式、MR脚本、自定义函数、自定义聚合函数 等. UDF 编写 UDF函数 的时候需要注意一下几点： 自定义 UDF 需要继承 org.apache.hadoop.hive.ql.UDF 需要实现 evaluate 函数 以下是两个数求和函数的UDF。evaluate函数代表两个整型数据相加 12345678910111213package hive.connect; import org.apache.hadoop.hive.ql.exec.UDF; public final class Add extends UDF &#123; public Integer evaluate(Integer a, Integer b) &#123; if (null == a || null == b) &#123; return null; &#125; return a + b; &#125; &#125; UDAF 函数类需要继承 UDAF 类，内部类 Evaluator 需要实现 UDAFEvaluator 接口. Evaluator 需要实现 init、iterate、terminatePartial、merge、terminate 这几个函数. init函数实现接口 UDAFEvaluator 的 init 函数. iterate接收传入的参数，并进行内部的轮转。其返回类型为 boolean. terminatePartial无参数，其为 iterate 函数轮转结束后，返回轮转数据. merge 接收 terminatePartial 的返回结果，进行数据 merge 操作，其返回类型为boolean. terminate 返回最终的聚集函数结果. 下面是一个简单的 UDAF 的 demo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.x.user_bhv;import com.google.common.collect.Maps;import org.apache.hadoop.hive.ql.exec.UDAF;import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;import java.util.HashMap;import java.util.Map;public class UDAFMergeIntToIntMap extends UDAF &#123; public static class PartialResult &#123; Map&lt;Integer, Integer&gt; attributes; PartialResult() &#123; attributes = Maps.newHashMap(); &#125; &#125; public static class UnitIdUDAFEvaluator implements UDAFEvaluator &#123; private PartialResult partialResult; public UnitIdUDAFEvaluator() &#123; super(); init(); &#125; public void init() &#123; System.out.println(&quot;map init&quot;); partialResult = new PartialResult(); &#125; public boolean iterate(Map&lt;Integer, Integer&gt; attributes_args) &#123; if (attributes_args == null || attributes_args.isEmpty()) &#123; return true; &#125; for (Map.Entry&lt;Integer, Integer&gt; entry : attributes_args.entrySet()) &#123; this.partialResult.attributes.put(entry.getKey(), entry.getValue()); &#125; return true; &#125; public PartialResult terminatePartial() &#123; return this.partialResult; &#125; public boolean merge(PartialResult other) &#123; // 参数不可能为 null for (Map.Entry&lt;Integer, Integer&gt; entry : other.attributes.entrySet()) &#123; this.partialResult.attributes.put(entry.getKey(), entry.getValue()); &#125; return true; &#125; public Map&lt;Integer, Integer&gt; terminate() &#123; if (partialResult == null) &#123; return new HashMap&lt;Integer, Integer&gt;(); &#125; else &#123; return this.partialResult.attributes; &#125; &#125; &#125; public static void main(String[] args) &#123; &#125;&#125; 在 Hive 脚本中的使用示例 : 123456789hql=&quot;ADD jar $&#123;jar_dir&#125;/user_bhv_for_hive.jar; CREATE TEMPORARY FUNCTION merge_int_to_int_map AS &#x27;com.x.user_bhv.UDAFMergeIntToIntMap&#x27;; INSERT OVERWRITE TABLE $&#123;table_user_buy_category&#125; SELECT mobile_number, merge_int_to_int_map (level1_id_count_map) FROM ods_dm_e_coupon GROUP BY mobile_number Summary 重载 evaluate 函数. UDF 函数中参数类型可以为Writable，也可为java中的基本数据对象. UDF 支持变长的参数. Hive 支持隐式类型转换. 客户端退出时，创建的临时函数自动销毁. evaluate函数必须要返回类型值，空的话返回null，不能为void类型. UDF 和 UDAF 都可以重载. 查看函数 SHOW FUNCTIONS. UDAF: User Defined Aggregation Function Reference Hive 中 UDF、UDAF 和 UDTF 使用 bliar’s github hive udaf demo","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://www.iequa.com/tags/hive/"}]},{"title":"MapReduce for Python","slug":"hadoop/hadoop-mr-for-python","date":"2015-01-30T02:37:21.000Z","updated":"2021-06-20T04:12:28.318Z","comments":true,"path":"2015/01/30/hadoop/hadoop-mr-for-python/","link":"","permalink":"http://www.iequa.com/2015/01/30/hadoop/hadoop-mr-for-python/","excerpt":"我们可以用 hadoop-streaming 的方式，通过 python 等其他语言来编写 MR 程序.","text":"我们可以用 hadoop-streaming 的方式，通过 python 等其他语言来编写 MR 程序. Map阶段：mapper.py 123456789#!/usr/bin/env pythonimport sysfor line in sys.stdin: line = line.strip() words = line.split() for word in words: print(&quot;%s&quot; % word) # 这里仅仅是一个例子，只输出了第一列 为了是脚本可执行，增加mapper.py的可执行权限 当然，Map阶段， 你也可以不作处理原样输出: 只写一个 cat Reduce阶段：reducer.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#!/usr/bin/env python# -*- coding: utf-8 -*-# Copyright 2013 x Inc. All Rights Reserved__author__ = &#x27;Blair Chan&#x27;import sysimport constantfrom datetime import datetimefrom EsHelper import EsHelperdef insert_user_basic_consume_info(items, esHelper): basic_consume_info_doc = get_user_basic_consume_info_doc(items) if basic_consume_info_doc is None: return _id = basic_consume_info_doc[&#x27;mobile_number&#x27;] basic_consume_info_index = &quot;basic_consume_info_index&quot; esHelper.index(index=basic_consume_info_index, doc_type=basic_consume_info_index, id=_id, data=basic_consume_info_doc)def get_user_basic_consume_info_doc(items): doc = None try: doc = &#123; &quot;mobile_number&quot;: items[0], &quot;first_consume_time&quot;: items[1] &#125; except BaseException as e: print(&quot;Exist Exception : %s About get_user_basic_consume_info_doc, mobile_number: %s&quot; % (str(e), mobile_number)) finally: pass return docdef main(): esHelper = EsHelper(constant.ES_URL) success_sum = 0 for line in sys.stdin: line = line.strip() items = line.split(&#x27;\\001&#x27;) if len(items) &lt; 2: continue insert_user_basic_consume_info(items, esHelper) success_sum = success_sum + 1 print(&quot;Success:%d&quot; % success_sum)if __name__ == &#x27;__main__&#x27;: main() 本地测试 1cat data.txt | python mapper.py | sort | reducer.py 提交Hadoop 123456789101112131415161718192021222324252627282930#!/bin/bashcd `dirname $0`/.. &amp;&amp; wk_dir=`pwd` &amp;&amp; cd -source $&#123;wk_dir&#125;/util/envinput_file=&quot;$&#123;OSS_URL&#125;/$&#123;mds_hive_dir&#125;/$&#123;table_user_basic_consume_info&#125;/*&quot;output_file=&quot;$&#123;OSS_URL&#125;/$&#123;tmp_hive_dir&#125;/$&#123;table_user_basic_consume_info&#125;/dt=$&#123;d1&#125;&quot;reducer=&quot;reducer.py&quot;reducer_depend1=&quot;constant.py&quot;reducer_depend2=&quot;EsHelper.py&quot;archive=&quot;$&#123;OSS_URL&#125;/share/packages/elasticsearch-5.0.0.tar.gz#elasticsearch-5.0.0&quot; ## archive 表示的依赖包需要上传到 hdfs 上，#后面表示的是解压后的目录名$&#123;HADOOP&#125; fs -rmr $&#123;output_file&#125;cmd=&quot;$&#123;HADOOP&#125; jar $&#123;hadoop_streaming_jar&#125; -D mapred.map.tasks=100 -D mapred.reduce.tasks=100 -D stream.map.input.ignoreKey=true -input $&#123;input_file&#125; -output $&#123;output_file&#125; -file $&#123;reducer&#125; -file $&#123;reducer_depend1&#125; -file $&#123;reducer_depend2&#125; -mapper cat -reducer $&#123;reducer&#125; -cacheArchive $&#123;archive&#125;&quot;echo_ex &quot;$cmd&quot;$cmdcheck_success hadoop_streaming_jar=&quot;/home/data_mining/share/packages/hadoop2/hadoop-streaming-2.7.2.jar&quot; 以上仅仅是一个例子，虽然插入 ES 出现异常，但本篇仅仅说明如何用 python 写 mapreduce 程序 Reference 用python写MapReduce函数——以WordCount为例","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/tags/hadoop/"}]},{"title":"Linux, profile / bashrc Brief Introduce","slug":"devops/linux-bashrc-profile","date":"2014-05-17T23:54:16.000Z","updated":"2021-06-20T04:12:28.198Z","comments":true,"path":"2014/05/18/devops/linux-bashrc-profile/","link":"","permalink":"http://www.iequa.com/2014/05/18/devops/linux-bashrc-profile/","excerpt":"/etc/profile、/etc/bashrc、/.bash_profile、/.bashrc","text":"/etc/profile、/etc/bashrc、/.bash_profile、/.bashrc config file desc /etc/profile，/etc/bashrc 系统全局环境变量设定 /.profile，/.bashrc 用户家目录下的私有环境变量设定 1. login env steps 以下是 登入系统,环境设定档 流程 Read step desc /etc/profile /etc/profile.d 和 /etc/inputrc 。 从/etc/profile.d目录的配置文件搜集shell的设置 ~/.bash_profile ~/.bash_profile，如无则读取 ~/.bash_login，如无则读取 ~/.profile ~/.bashrc ~/.bashrc (交互式 non-login 方式进入 bash 运行的) 2. .profile 与 .bashrc ~/.profile 与 ~/.bashrc 2.1 相同点 都具有个性化定制功能 ~/.profile 可以设定本用户专有的路径，环境变量，等，它只能登入的时候执行一次 ~/.bashrc 也是某用户专有设定文档，可以设定路径，命令别名，每次shell script的执行都会使用它一次 2.2 bashrc 和 profile 的区别 交互式模式 shell等待你的输入，并且执行你提交的命令。 shell与用户进行交互 登录、执行命令、签退、shell终止 ~/.bash_profile 是交互式、login 方式进入 bash 运行的 ~/.bashrc 是交互式 non-login 方式进入 bash 运行的 非交互式模式 shell不与你进行交互，是读取存在文件中的命令,并且执行它们。当它读到文件的结尾，shell终止 Reference blog.chinaunix.net/","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"bashrc","slug":"bashrc","permalink":"http://www.iequa.com/tags/bashrc/"}]},{"title":"Qunar Training Feelings","slug":"tools/my-qunar-dev-feelings","date":"2014-05-06T22:24:16.000Z","updated":"2021-06-22T06:52:37.112Z","comments":true,"path":"2014/05/07/tools/my-qunar-dev-feelings/","link":"","permalink":"http://www.iequa.com/2014/05/07/tools/my-qunar-dev-feelings/","excerpt":"training at a good company","text":"training at a good company 时间过得真快，我在去哪儿网已经有一段时间了，在这期间我真的学会了很多，经过这次培训，我学会了java，了解了guava，学习了SVN， git版本管理工具， Maven项目管理及自动构建工具， SpringMVC的开发，mybatis 的使用，Servlet， jsp， html， http 的初步了解，动态代理与AOP， 单元测试与自动化测试，数据库建表规范， Linux系统的使用等等等等，熟悉了公司内部的开发框架，了解公司开发规范，适应公司文化, 环境。 在这些天的日子里，最初本想不走 java 路线的，觉得封装好严重，你根本不知道那个包是怎么回事，就可以拿过来一用，就出来了一个看起来比较高大上的作品，学很短的时间就可以搞出个东西来，觉得没有C、C++或者是算法，来得那么爽。但是经过培训后，我才发现了java的可爱之处与强大，以前对java有误解，完全是自己的无知与不懂，遇到了酒店事业部的 思雨大神，更是感受到了学习java这方面的知识，可以很牛很牛，学任何一门技术都具有更为广阔的空间，来公司的初期，由于我对java知识之前是没有任何经验的，所以面对严格快速正规的培训，真的感到压力很大，幸好小组的分享还有做作业遇到问题，组内外同学的帮助，才可以跟下来，由于初学，节奏很快，那么多知识，在不到2个月的时间内全部学熟是很难做到的，我现在对 SpringMVC 还有一些 Guava，web的知识，还不是很熟练，以后一定要找时间在自己补习下这方面的知识。并把自己在公司学到的一些知识，也介绍给一些还在迷茫懵懂的学妹，学弟们。在公司的生活，对我来说不仅仅是技术和能力上的提升，更多的还有思想和态度的转变。 最后，首先要感谢公司给我们新人提供的这个培训平台，可以让我学到这么多的专业知识，认识到更多的志同道合的朋友；感谢丫丫姐，辛苦操劳的尽可能给我们提供欢愉的学习环境，以及邀请很多的优秀讲师；感谢全体给我们讲课的讲师们，感谢我的团队以及周边的同事，在工作学习的繁忙之余，还可以带来各种各样的欢声笑语，感谢老何与david，让我学到了严谨，负责，认真的工作态度与生活态度。","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"health","slug":"health","permalink":"http://www.iequa.com/tags/health/"}]},{"title":"Linux 鸟哥的私房菜 读书心得","slug":"devops/linux-bird-bro","date":"2013-08-29T10:09:44.000Z","updated":"2021-06-20T04:12:28.197Z","comments":true,"path":"2013/08/29/devops/linux-bird-bro/","link":"","permalink":"http://www.iequa.com/2013/08/29/devops/linux-bird-bro/","excerpt":"Linux 鸟哥的私房菜 读书心得 ： 实践与观察才是王道","text":"Linux 鸟哥的私房菜 读书心得 ： 实践与观察才是王道 0. 计算机概论 计算机：接受用户输入的指令与数据，经过处理器的数据与逻辑单元运算处理后，以产生或存储成有用的信息。 **计算机硬件的五大单元 ** 输入单元 (鼠标，键盘等) 控制器 运算器 (算术逻辑，控制，记忆) 存储器 输出单元 (屏幕，打印机等) CPU 为一个具有特定功能的芯片，里头含有微指令集。 CPU 读取的数据都是从内存中读取来的，内存的数据是由输入单元传输进来的。 CPU 处理完毕的数据要写入内存，内存再到输出单元。 接口设备 主板 (设备连接一起，让其协调，通信) 存储设备(硬盘，软盘，光盘，磁带) 显示设备 网络设备 硬盘最小物理量512bytes,最小的组成单位为扇区sector 个人计算机的架构与设计 x86开发商 (intel,AMD) 的cpu架构并不兼容，主板芯片组设计不相同。 主板上最重要的就是芯片组，芯片组通常又分为两个桥接器来控制各组件的通信。 北桥 : 负责连接较快的 cpu,内存和显卡 等组件。 (AMD : 内存直接与cpu通信，不经过北桥) 南桥 : 负责连接硬盘，USB，网卡等。 北桥的总线称为系统总线，是内存传输的主要信道。 与总线宽度类似，cpu每次能够处理的数据量称为字组大小(word size),字组大小依据cpu的设计而有32位与64位。 显卡厂商直接在显卡上面嵌入一个3D加速的芯片，这就是GPU称谓的由来。 CMOS是电脑主板上的一块可读写的RAM芯片。因为可读写的特性. 所以在电脑主板上用来保存BIOS设置完电脑硬件参数后的数据，这个芯片仅仅是用来存放数据的。 BIOS为写入到主板上某一块闪存或EEPROM的程序，在开机的时候运行，以加载CMOS当中的参数。 软件是计算机的灵魂。 机器程序与编译程序。 C/C++ – 编译器 --&gt; 机器码 操作系统 OS 如果我们能够将硬件都驱动，并且提供一个开发软件的参考接口来给工程师开发软件的话，那就是OS OS 内核： OS其实是一组程序，这组程序的重点在于管理计算机的所有活动以及驱动系统中的所有硬件。 OS内核 – 开机后常驻内存。 系统调用： System Call – 开发软件，参考内核的相关功能。 应用程序 – 系统调用 – OS内核 – 硬件 内核功能： 系统调用接口 程序管理 内存管理 文件系统管理 设备驱动 1. Linux 介绍 GNU (Richard Mathew Stallman - 史托曼) 伟大的人物。 GNU 通用公共许可证 GPL - General Public License Emacs GCC (GNU C Compiler ) GLIBC (GNU C Library) Bash shell Linus Torvalds - Linux 参考 Minix(from 谭宁邦教授) Linux 参考了 POSIX规范(可携式操作系统接口)，重点在于规范内核与应用程序之间的接口。 Linux 如何学习： 实践再实践 – 要增加自己的体力，就只有运动；要增加自己的知识，就只有读书。 鸟哥的建议 『 （1）建议兴趣 （2）成就感 (被认可，被认同) 』 2. Linux 文件命令与权限 Linux : 多用户，多任务环境。 owner, group, others 且 各有 read, write, execute 等权限。 /etc/passwd /etc/shadow /etc/group 文件命令 文件处理命令 [ ls, cp, mv, rm, cat, ln, file] 文件内容查阅 [ cat, more, less, head, tail] 权限管理命令 [ chmod u+r g-w o=x 777, chown [-R] robby filename, chgrp 组名 filename, umask -S] 文件搜索命令 [ which, find, locate, updatedb, grep ] which -a在哪, whereis -b(数据库) locate 显示所有 压缩解压命令 [ gzip, gunzip, tar, zip, bzip2] 网络通信命令 [ write, wall, ping, ifconfig] 帮助命令 [ man, info, whatis apropos, help] whereis 与 which [不同是可以看到命令所在帮助文档位置] 文件类型 1, (普通文件，纯文本文件(ASCII)， - 二进制文件，数据格式文件) 2, 目录 d directory 3, 连接文件 ( l 软链接文件link ) 4, 设备与设备文件(b,c) 5, 套接字 6, 管道 文件权限 1234 drwxr-xr-x 2 hp hp 4096 2012-12-25 16:18 dlinux[文件类型与权限] [硬链接数] [所有者] [所属组] [文件容量] [修改日期] [文件名]4096 - 文件大小 [不是很准确，标记目录本身的大小，不是目录总大小] 数据块(512字节1单位) 特殊情况： 123hp@ubuntu:~/2014$ ls -ld /usr/bin/passwd-rwsr-xr-x 1 root root 42824 2月 11 2012 /usr/bin/passwds 与 t 这两个权限与系统的帐号以及系统的进程较为相关。以后再研究。 组织数据方式 每种OS都有自己的，比如 NTFS, ext3 … 存储数据的最小单位就叫做数据块，这一这样理解！！ 文件存储时，至少要占用一个数据块。 （你 60斤，也得做一个椅子，200斤做一个，600斤可能做两个！） 分你做什么，数据块越小存取速度越小，数据块越大存取的时候浪费空间越大。 分你做什么，调数据块多大合适。。。有个别时候！！ 3. Linux 文件搜索命令 (1) 命令所在路径 : /usr/bin/find 语法:find [搜索路径] [搜寻关键字] 1234$ find /etc -name init* 在目录/etc中查找init开头的文件$ find / -size +204800 在根目录下查找大于100MB的文件 block=512B 大于 ： +， 小于 ： - ， 等于 ： 不写 +, -；$ find / -user sam 在根目录下查找所有者为sam的文件 (2) 按照时间查找 find /etc -mmin -120 （-之内， +超过 ） 天 ctime, atime, mtime ， 分钟 cmin , amin, mmin c-change 改变， 表示文件属性被修改过，所有者，所属组，权限 a-access 访问 m-modify 修改， 表示文件内容被修改过 我们刚讲了 4 个find的选项，其实不下 40 个！要学会看文档！ (3) 连接符 -a and 逻辑与 -o or 逻辑或 1234567891011121314$ find /etc -ctime -1 在/etc下查找24小时内被修改过属性的文件和目录$ find /etc -size +163840 -a -size -204800 在/etc下找大于80MB小于100MB的文件$ find /etc -name init* -a -type f [ 二进制文件 ]$ find /etc -name init* -a -type l [ 软链接文件 ] -type 文件类型 f 二进制文件 l 软链接文件 d 目录 连接符 find ..... -exec 命令 &#123;&#125; \\; &#123;&#125; find 查询的结果 \\ 转义符 【ls \\ls】$ find /etc -name inittab -exec ls -l &#123;&#125; \\; 在/etc下查找inittab文件并显示其详细信息$ find /test -name testfile3 -exec rm &#123;&#125; \\;$ find /etc -name inittab -ok ls -l &#123;&#125; \\; 能询问一下 -ok$ find /etc -name inittab -a -type f -exec ls -l &#123;&#125; \\;$ find . -inum 16 -exec rm &#123;&#125; \\; (4) locate 功能描述:寻找文件或目录 范例: $ locate file 列出所有跟file相关的文件 (5) updatedb 执行权限:root 语法: updatedb 功能描述: 建立整个系统目录文件的数据库 范例: # updateb (6) grep 功能描述 : 在文件中搜寻字串匹配的行并输出 4. Linux 文件系统管理 123456789101112131415hp@ubuntu:/$ df -m文件系统 1M-块 已用 可用 已用% 挂载点/dev/loop0 14692 4746 9210 35% /udev 2977 1 2977 1% /devtmpfs 1195 1 1194 1% /runnone 5 1 5 1% /run/locknone 2986 1 2985 1% /run/shm/dev/sda8 20490 17188 3303 84% /host/dev/sda7 105036 24898 80139 24% /media/Studtyhp@ubuntu:/$ du -h /etc/services20K /etc/serviceshp@ubuntu:/$ du -sh ~/dlinux76K /home/hp/dlinuxhp@ubuntu:/$ file /etc/services/etc/services: ASCII English text 特殊权限: 粘着位 t 粘着位的定义: 当权限为777的目录被授予粘着位,用户只能在此目录下删除自己是所有者的文件。 查看分区使用情况:df 查看文件、目录大小:du 查看文件详细时间参数:stat 校验文件md5值:md5sum 检测修复文件系统:fsck、e2fsck (单用户模式卸载文件系统后执行) 123456789hp@ubuntu:/$ df -h 文件系统 容量 已用 可用 已用% 挂载点 /dev/loop0 15G 4.7G 9.0G 35% / udev 3.0G 4.0K 3.0G 1% /dev tmpfs 1.2G 1000K 1.2G 1% /run none 5.0M 4.0K 5.0M 1% /run/lock none 3.0G 804K 3.0G 1% /run/shm /dev/sda8 21G 17G 3.3G 84% /host /dev/sda7 103G 25G 79G 24% /media/Studty 添加硬盘分区 划分分区(fdisk) 创建文件系统 (mkfs) 尝试挂载 (mount) [mount 物理设备名 挂载点(空目录)] 写入配置文件 (/etc/fstab) 5. Linux 文件系统目录结构 12345678910111213141516171819202122/bin : 基础系统所需要的最基础的命令就是放在这里。比如 ls、cp、mkdir等命令； 功能和/usr/bin类似，这个目录中的文件都是可执行的，普通用户都可以使用的命令。/boot : Linux的内核及引导系统程序所需要的文件。 启动装载文件存放位置，如kernels,initrd,grub。一般是一个独立的分区。/dev : 一些必要的设备,声卡、磁盘等。还有如 /dev/null. /dev/console /dev/zero /dev/full 等。/etc : 系统的配置文件存放地. /home : 用户工作目录，和个人配置文件，如个人环境变量等./lib : 库文件存放地。bin和sbin需要的库文件。类似windows的DLL。/media : 可拆卸的媒介挂载点，如CD-ROMs、移动硬盘、U盘，系统默认会挂载到这里来。/mnt : 临时挂载文件系统。比如有cdrom 等目录。/opt : 可选的应用程序包。 第三方软件/proc : 操作系统运行时，进程（正在运行中的程序）信息及内核信息（比如cpu、硬盘分区、内存信息等）存放在这里。/root : Root用户的工作目录/sbin : 和bin类似，是一些可执行文件，不过不是所有用户都需要的，一般是系统管理所需要使用得到的。/tmp : 系统的临时文件，一般系统重启不会被保存。/usr : 包含了系统用户工具和程序。/usr/bin ： 非必须的普通用户可执行命令/usr/include ： 标准头文件/usr/lib : /usr/bin/ 和 /usr/sbin/的库文件/usr/sbin : 非必须的可执行文件/usr/src : 内核源码/srv : 该目录存放一些服务启动之后需要提取的数据 6. VIM 编辑器 序号 命令类型 命令详情 1. [插入命令] a, A, i, I, o, O 2. [定位命令] h, j, k, l, $, 0, H, M, L :set nu 设置行号，gg 到第一行 G 到最后一行 3. [删除命令] x, nx, dd, ndd, dG[光标处删除到文件尾], D, :n1, n2d[删除指定范围的行] 4. [复制和剪切命令] yy, Y, nyy, nY, dd[剪切]， ndd[剪切n行], p, P [粘贴在当前光标所在行下或行上] 5. [替换和取消命令] r, R, u， r 取代光标所在处字符 R 从光标所在处开始替换字符，按Esc结束 u 取消上一步操作 6. [搜索和替换命令] /string 按 n 是下一个 从前向后， N 从后向前 :%s/old/new/g 全文替换指定字符串 :n1,n2s/old/new/c 询问替换 :set ic 搜索的时候就不区分大小写了！ :set noic 7. [保存和退出命令] [ZZ 最常用 ] :wq 保存退出 8. [vi 中另外比较有趣的命令] :r !命令 导入命令执行结果到当前vi中, 在vi中执行命令 :!命令 9. 分屏 sp, vsp (水平) ctrl+w *2 10. 定义快捷键 :map 快捷键 触发命令 范例: : map ^P I# ^P = ctrl+v+p : map ^B 0x 11. vi 的配置文件 ~/.vimrc 7. 引导流程 Linux引导流程 Linux运行级别 Linux启动服务管理 GRUB配置与应用 系统引导流程: 123456789固件 firmware(CMOS/BIOS) → POST 加电自检 ↓ [CMOS是固化在主板上的那段程序， BIOS 操作CMOS的那个界面]自举程序 BootLoader(GRUB) → 载入内核 linux-grub /etc/grub.conf / win-ntldr [nt内核代号,loader] bootini [里面记载了启动信息] ↓ 载入内核，OS的核心-内核[存储CPU文件进程...管理]-心脏大脑 指定linux内核存放的位置。ls /boot载入内核 Kernel → 驱动硬件 ↓ [内核只做两件事情，1驱动硬件2启动init. 内核保存最多的是驱动程序]启动进程 init ↓ [init是第一个可以存在和启动的进程]读取执行配置文件/etc/inittab 引导流程说明: firmware自检之后，发现硬件们都没有什么问题之后，然后 firmware 读取 MBR[主引导记录]，位于 0柱面0磁头1扇区， 跳到 Master boot record 去读取数据。载入MBR 中一个很重要的数据叫做 Bootloader, 也称做自举程序或自启动程序。 下面是 Partition table 磁盘分区表，下面是 Magic Number 结束标志字 init 启动后读取 inittab文件, 执行缺省运行级别, 从而继续引导过程。在UNIX系统中, init时第一个可以存在的进程, 它的PID恒为1, 但它也必须向一个更高级的功能负责: PID为0的内核调度器(Kernelscheduler),从而获得CPU时间。 扩展 ： 在Linux里面不允许存在 孤儿进程，在linux系统中init是所有进程的父进程。 僵尸进程[Z] 儿子死了，父亲不知道，这个子进程就会变成 Z。 Linux运行级别 0 ~ 6 说明 ： 0 关机 1 字符单用户 2，3 字符界面的多用户模式[广泛使用的服务器的模式] 4 自定义 5 图形化的多用户 6 reboot 8. 学习bash Linux bash东西非常多，包括变量的设置与使用，bash操作环境，数据流重定向功能，还有好用的管道命令。 1, 硬件 2，内核程序 3，应用程序 man, chmod, chown, vi, fdisk, mkfs等命令，这些命令都是独立的应用程序。 bash的主要优点 : (1) 命令记忆功能 (history) .bash_history (注销系统后，命令记忆会记录到.bash_history) (2) 命令与文件补全功能 (tab) (3) 命令别名设置功能 (alias) alias lm=‘ls -al’ (4) 作业控制，前台，后台控制 (job control, foreground, background) (5) 程序脚本 (shell script) (6) 通配符 (Wildcard) shell 变量: echo 变量显示 如 ： echo PATH父子进程export设置成环境变量..unset取消变量的设置值。uname−r显示内核版本&quot;version={PATH} 父子进程 export 设置成环境变量.. unset 取消变量的设置值。 uname -r 显示内核版本 &quot;version=PATH父子进程export设置成环境变量..unset取消变量的设置值。uname−r显示内核版本&quot;version=(uname -r)&quot; 来替代 “version=uname -r” 比较好。 环境变量的功能: env 与 export 查看环境变量 HOME， SHELL， HISTSIZE， PATH， LANG(语系数据)， RANDOM(随机数发生器) export 自定义变量转成环境变量 export name 子进程会定义父进程的环境变量，不会继承父进程的自定义变量 bash 可以限制用户的某些系统资源，可打开的文件数量，可使用的CPU时间，可使用的内存总量等 文件系统及程序的限制关系 : ulimit ulimit -H -S -a -c -f -d -l -t -u -s 等。 数据流重定向 1, stdin 代码为 0 ，使用 &lt; 或 &lt;&lt; 2, stdout 代码为 1 ，使用 &gt; 或 &gt;&gt; 3, stderr 代码为 2 ，使用 2&gt; 或 2&gt;&gt; /dev/null 垃圾桶黑洞设备 &amp;&amp; 与 || cut 命令 例子 ： echo $PATH | cut -d ‘:’ -f 3,5 export | cut -c 12- cut 的主要用途在于将同一行里面的数据进行分解，用于分析一些日志。 grep -a -c -i -n -v ‘targe’ filename 排序命令： sort, wc, uniq last | cut -d ’ ’ -f 1 | sort | uniq -c hp@ubuntu:~$ last | tee last.list | cut -d &quot; &quot; -f1 tee 命令可以让 standard output 转存到一个文件内，并将同样的数据继续送到屏幕中去处理 字符转换命令： tr, col, join, paste, expand (1) tr : last | tr ‘[a-z]’ ‘[A-Z]’ ： cat /etc/passwd | tr -d ‘:’ (2) col -x 将tab键转换成对等的空格 -b 在文字内有 / 时，仅保留 / 后面接的那个字符 ： cat /etc/man.config | col -x | cat -A | more (3) join [-ti12] file1 file2 (4) paste [-d] file1 file2 ： -d : 后面可以接分隔字符，默认是以 [tab] 来分隔的。 ： - : 如果file部分写成 -, 代表来自 standard input 的数据的意思。 (5) expand 就是将 [tab] 按键转成空格键，可以这样做 ： expand [-t] file ： expand -t 6 - | cat -A ： unexpand 切割命令： split 参数 -b : 后面可接欲切割成的文件大小，可加单位 b, k, m 等； 12345$ split -b 300k /etc/termcap termcap$ ll -k termcap*$ cat termcap* &gt;&gt; termcapback$ ls -al / | split -l 10 - lsroot$ wc -l lsroot* 参数代换： xargs 可以读入stdin的数据，并且以空格符或断行字符进行分辨，将stdin的数据分隔成args 12345hp@ubuntu:~$ cut -d &#x27;:&#x27; -f1 /etc/passwd | head -n 3 | xargs -p/bin/echo root daemon bin ?...yroot daemon binhp@ubuntu:~$ echo &quot;3 4:asd&quot; | xargs 3 4:asd 关于减号 - 的用途 123$ tar -cvf - /home | tar -xvf - 某些命令需要用到文件名来处理，该stdin, stdout可以利用减号“-”来替代。 9. 进程管理 进程的概念 进程管理命令 计划任务 9.1 进程和程序的区别 程序是静态概念, 它作为一种软件资源长期保存; 进程是程序的执行过程, 它是动态概念, 有一定的生命期, 是动态产生和消亡的; 子进程是由一个进程所产生的进程,产生这个子进程的进程称为父进程; 在Linux系统中,使用系统调用fork创建进程。fork复制的内容包括父进程的数据和堆栈段以及父进程的进程环境; 父进程终止子进程自然终止. 9.2 前台进程和后台进程 前台进程: 在Shell提示处打入命令后, 创建一个子进程, 运行命令, Shell等待命令退出, 然后返回到对用户给出提示符。 这条命令与Shell异步运行, 即在前台运行, 用户在它完成之前不能执行另一个命令。 后台进程: 在Shell提示处打入命令, 若后随一个&amp;, Shell创建的子进程运行此命令, 但不等待命令退出, 而直接返回到对用户给出提示。 这条命令与Shell同步运行, 即在后台运行。 后台进程必须是非交互式的。 9.3 进程状态 进程状态 描述 就绪 进程已经分配到资源,但因为其它进程正占用CPU。 等待 因等待某种事件而暂时不能运行的状态。 运行 进程分配到CPU,正在处理器上运行。 进程状态细化 用户态运行 : 在CPU上执行用户代码 核心态运行 : 在CPU上执行核心代码 在内存就绪 : 具备运行条件,只等调度程序为它分配CPU 在内存睡眠 : 因等待某一事件的发生,而在内存中排队等待 在外存就绪 : 就绪进程被交换到外存上继续处于就绪状态 在外存睡眠 : 睡眠进程被交换到外存上继续等待 在内存暂停 : 因调用stop程序而进入跟踪暂停状态,等待其父进程发送命令。 在外存暂停 : 处于跟踪暂停态的进程被交换到外存上 创建态 新进程正在被创建、但尚未完毕的中间状态 终止态 进程终止自己 查看用户信息 w (命令) 12345hp@ubuntu:~$ w 22:02:45 up 22 min, 2 users, load average: 0.27, 0.27, 0.28USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAThp tty7 21:40 22:33 1:16 0.34s gnome-session -hp pts/0 :0 22:02 0.00s 0.37s 0.01s w 选项 描述 JCPU 以终端代号来区分,该终端所有相关的进程执行时,所消耗的CPU时间会显示在这里 PCPU CPU执行程序耗费的时间 WHAT 用户正在执行的操作查看个别用户信息:w 用户名 load average 分别显示系统在过去1、5、15分钟内的平均负载程度。 FROM 显示用户从何处登录系统,“:0”的显示代表该用户时从X Window下,打开文本模式窗口登录的 IDLE 用户闲置的时间。这是一个计时器,一旦用户执行任何操作,该计时器便会被重置 查看系统中的进程 ps 常用选项 a : 显示所有用户的进程 u : 显示用户名和启动时间 x : 显示没有控制终端的进程 e : 显示所有进程,包括没有控制终端的进程 l : 长格式显示 w : 宽行显示,可以使用多个w进行加宽显示 ps 常用输出信息的含义 TIME: 进程自从启动以来启用CPU的总时间 COMMAND/CMD: 进程的命令名 USER:用户名 %CPU:占用CPU时间和总时间的百分比 %MEM:占用内存与系统内存总量的百分比 ps 应用实例 ps 查看隶属于自己的进程 ps -u or -l 查看隶属于自己进程详细信息 ps -le or -aux 查看所有用户执行的进程的详细信息 ps -aux --sort pid 可按进程执行的时间、 PID、UID等对进程进行排序 1234# ps -aux | grep sam# ps -uU sam 查看系统中指定用户执行的进程# ps -le | grep init 查看指定进程信息# pstree 9.4 kill – 杀死进程 为什么要杀死进程 该进程占用了过多的CPU时间 该进程缩住了一个终端,使其他前台进程无法运行 运行时间过长,但没有预期效果 产生了过多到屏幕或磁盘文件的输出 无法正常退出, 关闭进程:kill 进程号 1). kill -9 进程号(强行关闭) kill -s 9 进程号 [前简化] 2). kill -1 进程号(重启进程) 3). 关闭图形程序: xkill 4). 结束所有进程: killall 5). 查找服务进程号: pgrep 服务名称 6). 关闭进程: pkill 进程名称 启动的程序 stop 也可以关闭 , 重启 /etc/rc.d/init.d/httpd restart 123# cat/proc/cpuinfo# pgrep httpd 检测但它所有进程的 pid# pkill httpd 也可以关闭，很方便 9.5 进程优先级，挂起与恢复 nice 指定程序的运行优先级 格式:nice -n command 例如:nice -5 myprogram renice 改变一个正在运行的进程的优先级 格式:renice n pid 例如:renice -5 777 优先级取值范围为(-20,19) nohup 使进程在用户退出登陆后仍旧继续执行,nohup命令将执行后的数据信息和 错误信息默认储存到文件 nohup.out 中 格式: nohup program &amp; 进程的挂起和恢复 进程的中止(挂起)和终止 挂起(Ctrl+Z)[类似差不多暂停] 终止(Ctrl+C) 进程的恢复 恢复到前台继续运行(fg) 复到后台继续运行(bg) 查看被挂起 /后台的进程(jobs) 9.6 top 作用 进程状态显示和进程控制,每5秒钟自动刷新一次(动态显示) 常用选项: d : 指定刷新的时间间隔 c : 显示整个命令行而不仅仅显示命令名 top常用命令: u : 查看指定用户的进程 k : 终止执行中的进程 h or ?:获得帮助 r : 重新设置进程优先级 s : 改变刷新的时间间隔 W : 将当前设置写入~/.toprc文件中 9.7 计划任务 at 安排作业在某一时刻执行一次 batch 安排作业在系统负载不重时执行一次 cron 安排周期性运行的作业 at命令的功能和格式 功能: 安排一个或多个命令在指定的时间运行一次 at 的命令格式及参数 at [-f 文件名] 时间 / at -d or atrm 删除队列中的任务 / at -l or atq 查看队列中的任务 进程处理方式 standalone 独立运行 xinetd 进程托管 atd、crond 计划任务 Linux 书架 入门类 ：《鸟哥的Linux私房菜》 编程类 ：《Advanced Linux Programming》-&gt; 《Advanced Programming in the UNIX Environment》 内核类 ：《Linux Kernel Development》 工具类 ：《Handbook of Open Source Tools》","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"linuxbird","slug":"linuxbird","permalink":"http://www.iequa.com/tags/linuxbird/"}]},{"title":"Java SE Learning Notes for Exception","slug":"java/java-se-2.4-notes-exception","date":"2013-07-25T06:54:16.000Z","updated":"2021-06-20T04:12:28.295Z","comments":true,"path":"2013/07/25/java/java-se-2.4-notes-exception/","link":"","permalink":"http://www.iequa.com/2013/07/25/java/java-se-2.4-notes-exception/","excerpt":"Java SE 异常处理部分的概要笔记: 手中无剑, 心中有剑.","text":"Java SE 异常处理部分的概要笔记: 手中无剑, 心中有剑. 第四章 异常处理 Java 异常是 Java 提供的用于处理程序中错误的一种机制。 java.lang…Exceptions 写程序有问题要有友好界面 医生开单子 { 1, 鼻腔内感觉异常 2, 体温持续升高 3, 分泌乳白色液体 直接说感冒不就得了么？ } e.printStackTrace(); 非常好！给程序员读。堆栈信息都打印出来！ java.lang.Throwable { 开车在上山走， 1, Error 山爆发 JVM 出问题。 2, Exception { 你可以处理的 – 刹车坏啦！修好再走。。。 1, … 2, RuntimeException (经常出，不用逮) 压路面上的小石子 } 一个 try 可以跟多个catch 所以 { 一个茶壶可以跟多个茶碗，一个男人可以三妻四妾。} try { // 可能抛出异常的语句 语句一； 语句二； } catch(someEx e) { 语句； } catch() { 语句 } finally { } 一 ： 打开 二 ： 关闭 finally : 一般进行资源的清除工作。。。！ 我处理不了的事情 ： 我交给上一级部门去处理！ 当时 catch 到 Ex 的时候，你至少要做出一种处理。要不那是危险的编程习惯！ main() 抛出 就是交给 java 运行时系统啦！ 它会把堆栈信息打出来！ 一个图 ： 五个关键字 { try, catch, finally, throw, throws } 一点问题 { 先逮大的，后逮小的，报错。 } 使用自定义异常 程序中可以使用 throw - 方法后 throws 如果throw抛出异常之后,方法就结束啦！ 注意 ： 重写方法需要抛出与原方法所抛出异常类型一致异常或不抛出异常。 总结 ：{ 一个图 五个关键字 先逮小的，再逮大的。 异常与重写的关系 } Reference csdn robbyo java","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"}]},{"title":"Java SE Learning Notes for OO","slug":"java/java-se-2.3-notes-oo","date":"2013-07-24T05:54:16.000Z","updated":"2021-06-20T04:12:28.292Z","comments":true,"path":"2013/07/24/java/java-se-2.3-notes-oo/","link":"","permalink":"http://www.iequa.com/2013/07/24/java/java-se-2.3-notes-oo/","excerpt":"Java SE 面向对象部分的概要笔记: 手中无剑, 心中有剑. 定义类 生成对象 ， 定义方法 被调用","text":"Java SE 面向对象部分的概要笔记: 手中无剑, 心中有剑. 定义类 生成对象 ， 定义方法 被调用 1. static static var 知道了内存，你就知道了一切. 局部变量 分配在 stack memory 成员变量 分配在 heap memory static var 为类对象共享的变量 在数据区 非静态变量 专属于某一个对象 静态方法不再是针对某一个对象进行调用, 所以不能访问非静态成员。 2. package 包名起名方法 ： 公司域名倒过来. 必须保证该类 class 文件位于正确的目录下. 必须class文件的最上层包的父目录位于classpath下. 执行一个类需要写全包名. SDK 主要的包介绍 * java.lang - 包含一些 java 语言的核心类， 如 : String, Math, Integer, System, Thread. * java.net - 包含执行与网络相关的操作的类 * java.io - 包含能提供多种输入/输出功能的类 * java.util - 包含一些实用工具类. 3. 类的继承与权限控制 * 修饰符 类内部 同一包内 子类 任何地方 * private Yes * default Yes Yes * protected Yes Yes Yes * public Yes Yes Yes Yes 分析内存 : 子类对象包含一个父类对象. 重写方法不能使用比被重写方法更严格的访问权限 – 其实这和多态有关 super 关键字 &amp; 继承中的构造方法 如果调用 super 必须写在构造方法的第一行 如果没调用，系统自动调用 super(), 如果没调，父类中又没写参数为空这个构造方法则出错。 Object 类 instanceof 是一个操作符 equals方法 J2SDK 提供的一些类 如 String , Date 重写了Object 的 equals() 方法. 对象转型 casting * 一个基类的引用类型变量可以指向 “其子类的对象”。 * 一个基类的引用不可以访问其子类新增加的成员 * 可以使用 引用 变量 instanceof 类名 来判断该引用型变量所&quot;指向&quot;的对象是否属于该类或该类的子类。 * upcasting / downcasting 内存分析 - 明白了内存你就明白了一切！ 动态绑定, 池绑定, 多态 * 动态绑定的机制 是 实际类型 new 的是！ * 深一点 -- 是对象内部有一个指针。。。。。。 * 动态绑定的机制是 ： 实际类型，还是引用类型。是调用实际类型，不是引用类型。 * 实际地址才会绑定到那个方法上。 方法在 code segment * 只有在运行期间(不是在编译期间)，运行出对象来，才能判断调用哪一个。。。。 这是面向对象核心中的核心。核心中的核心 ! 带来的莫大好处: 可扩展性达到了非常非常的极致好！ 多态总结: 要有继承 要有重写 父类引用指向子类对象 4. Abstract class abstract 修饰class时，这个类叫做抽象类. abstract 修饰方法时，该方法叫做抽象方法. abstract class 必须被继承，抽象方法必须被重写. 含有抽象方法的类必须声明为抽象类. abstract class 不能被实例化, 抽象方法只需声明，而不需要实现. 5. Final 关键字 final 的变量的值不能够被改变. final 的成员变量、局部变量(形参) final 的方法不能够被重写， final 的类不能够被继承. 系统中的 final class 例如： public final class String public final class Math public final class Boolean 6. interface: 一种特殊的抽象类 变量全是: public static final int id = 1; 方法全是: abstract function() java.lang - Comparable 我看就像 cmp 一样！(个人认为) Interface Comparable 可以扩展 interface interface 可以相互继承 class interface 只能是 implement 关系 Reference csdn robbyo java","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"}]},{"title":"Java SE Learning Notes for Hello World","slug":"java/java-se-2.2-notes-app","date":"2013-02-02T02:54:16.000Z","updated":"2021-06-20T04:12:28.295Z","comments":true,"path":"2013/02/02/java/java-se-2.2-notes-app/","link":"","permalink":"http://www.iequa.com/2013/02/02/java/java-se-2.2-notes-app/","excerpt":"第一个 Java 程序 HelloWorld.java. 手中无剑, 心中有剑.","text":"第一个 Java 程序 HelloWorld.java. 手中无剑, 心中有剑. Hello World HelloWorld.java 12345678public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println(&quot;Hello Java.&quot;); &#125;&#125;/** * 这里是注释 */ 一个源文件中最多只能有一个public类. 如果源文件 文件包含一个public class 它必需按该 class_name 命名 Java 程序设计 data type 123456789101112 -- 整数类型 (byte, short, int, long) -- 数值型 -- | -- 浮点类型 (float, double) --基本数据类型 -- 字符型 (char) | | | -- 布尔型 (boolean) 数据类型 -- | -- 类 (class) | | --引用数据类型 -- 接口 (interface) | -- 数组 (array) java 中定义了 4类 8种 基本数据类型 boolean 类型只允许取值 true / false , 不可以用 0 或 非0 替代。 char 采用 Unicode 编码 (全球语言统一编码), 每个字符占两个字节 Reference","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"}]},{"title":"Java SE Learning Notes for Environment","slug":"java/java-se-2.1-notes-env","date":"2013-02-02T00:54:16.000Z","updated":"2021-06-20T04:12:28.292Z","comments":true,"path":"2013/02/02/java/java-se-2.1-notes-env/","link":"","permalink":"http://www.iequa.com/2013/02/02/java/java-se-2.1-notes-env/","excerpt":"Java 环境的搭建 手中无剑, 心中有剑.","text":"Java 环境的搭建 手中无剑, 心中有剑. 1. JDK 12345678MS=/usr/local/xsoft/software### JAVA ###JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk/Contents/HomeJAVA_BIN=$JAVA_HOME/binPATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jarexport JAVA_HOME JAVA_BIN PATH CLASSPATH 2. Maven 1234567### Maven ###M2_HOME=/usr/local/xsoft/software/apache-mavenMAVEN_HOME=$M2_HOMEM3_HOME=$M2_HOMEPATH=$M3_HOME/bin:$PATHexport MAVEN_HOME M2_HOME PATH#MAVEN_OPTS=-Xms128m -Xmx512m 3. Tomcat 1234### Tomcat ###CATALINA_HOME=/usr/local/xsoft/software/apache-tomcatPATH=$CATALINA_HOME/bin:$PATHexport CATALINA_HOME PATH 4. IDE Intellij IDEA Reference Blair Zsh Config","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"}]},{"title":"Java SE Introduce","slug":"java/java-se-1-introduce","date":"2013-02-01T23:54:16.000Z","updated":"2021-06-20T04:12:28.293Z","comments":true,"path":"2013/02/02/java/java-se-1-introduce/","link":"","permalink":"http://www.iequa.com/2013/02/02/java/java-se-1-introduce/","excerpt":"Java是一种广泛使用的计算机编程语言，拥有跨平台、面向对象、泛型编程的特性.","text":"Java是一种广泛使用的计算机编程语言，拥有跨平台、面向对象、泛型编程的特性. Java data type OO Exception Java Array Java 常用类 Java 容器类 Collection / Generic Java I/O Stream Java Thread Java TCP/UDP, socket 1. Java 概述 Java 运行机制 JDK &amp; JRE Java env install Java Basic Content conclude : 计算机语言朝着人类易于理解的方向发展 2. Java 特点 一种 OO 语言 一种平台无关的语言, 提供程序运行的解释环境 一种健壮的语言, 吸收了C/C++语言的优点， 但去掉了其影响程序健壮性的部分(如: 指针， 内存的申请与释放等)。 3. Java程序运行机制 Java 2种核心机制 Java Virtual Machine Garbage collection JVM 可理解成一个以字节码为机器指令的CPU JVM 机制屏蔽了底层运行平台的差别, 实现了&quot;一次编译, 随处运行&quot;。 x.java --编译–&gt; x.class --执行–&gt; JVM Java语言消除了程序员回收无用内存空间的责任; 它提供一种系统级线程跟踪存储空间的分配情况，并在JVM的空闲时, 检查并释放那些可被释放的存储器空间。 4. JDK &amp; JRE &amp; env install Software Development Kit (软件开发包) 开发需要 JDK Java Runtime Environment 用户只需 JRE /etc/profile or ~/.zshrc or ~/.zprofile 123456### JAVA ###JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/HomeJAVA_BIN=$JAVA_HOME/binPATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jarexport JAVA_HOME JAVA_BIN PATH CLASSPATH classpath : java在编译和运行时要找的class所在的路径 建议你的 JDK 装在不带空格的目录里面 5. 命名规则 类名首字母大写 变量名和方法名的首字母小写 运用驼峰标识 HelloWorld.java 12345678public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println(&quot;Hello Java.&quot;); &#125;&#125;/** * 这里是注释 */ 一个源文件中最多只能有一个public类. 其它类的个数不限，如果源文件 文件包含一个public class 它必需按该 class-name 命名 6. Java 程序设计 data type 123456789101112 -- 整数类型 (byte, short, int, long) -- 数值型 -- | -- 浮点类型 (float, double) --基本数据类型 -- 字符型 (char) | | | -- 布尔型 (boolean) 数据类型 -- | -- 类 (class) | | --引用数据类型 -- 接口 (interface) | -- 数组 (array) java 中定义了 4类 8种 基本数据类型 boolean 类型只允许取值 true / false , 不可以用 0 或 非0 替代。 char 采用 Unicode 编码 (全球语言统一编码), 每个字符占两个字节 7. Array &amp; Method 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; Date[] days; days = new Date[3]; for (int i = 0; i &lt; 3; i++) &#123; days[i] = new Date(2004, 4, i+1); &#125; // int[] a = &#123;1, 2, 3, 4, 5, 6, 7&#125;; for (int i = 0; i &lt; a.length; i++) &#123; System.out.print(a[i] + &quot; &quot;); &#125; &#125; &#125; class Date &#123; int year; int month; int day; Date(int y, int m, int d) &#123; year = y; month = m; day = d; &#125; &#125; 面向过程-约瑟夫环 面向对象-约瑟夫环","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"}]},{"title":"Java SE Learning Notes for All","slug":"java/java-se-2.5-notes-all","date":"2013-02-01T23:54:16.000Z","updated":"2021-06-20T04:12:28.297Z","comments":true,"path":"2013/02/02/java/java-se-2.5-notes-all/","link":"","permalink":"http://www.iequa.com/2013/02/02/java/java-se-2.5-notes-all/","excerpt":"这篇主要是记录了我学习 Java SE 的概要笔记: 手中无剑, 心中有剑, 摘花飞叶可以伤人.","text":"这篇主要是记录了我学习 Java SE 的概要笔记: 手中无剑, 心中有剑, 摘花飞叶可以伤人. 1. 环境配置 123456### JAVA ###JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk/Contents/HomeJAVA_BIN=$JAVA_HOME/binPATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jarexport JAVA_HOME JAVA_BIN PATH CLASSPATH 2. 基础 分析设计 ： 1. 这个东西有 哪些类 2. 类的 属性和方法 3. 类与类之间的关系 4. 实现 3. 面向对象 定义类 生成对象 定义方法 被调用 3.1 static static var 知道了内存，你就知道了一切. 局部变量 分配在 stack memory 成员变量 分配在 heap memory static var 为类对象共享的变量 在数据区 非静态变量 专属于某一个对象 静态方法不再是针对某一个对象进行调用, 所以不能访问非静态成员。 3.2 package 包名起名方法 ： 公司域名倒过来. 必须保证该类 class 文件位于正确的目录下. 必须class文件的最上层包的父目录位于classpath下. 执行一个类需要写全包名. SDK 主要的包介绍 * java.lang - 包含一些 java 语言的核心类， 如 : String, Math, Integer, System, Thread. * java.net - 包含执行与网络相关的操作的类 * java.io - 包含能提供多种输入/输出功能的类 * java.util - 包含一些实用工具类. 3.3 类的继承与权限控制 * 修饰符 类内部 同一包内 子类 任何地方 * private Yes * default Yes Yes * protected Yes Yes Yes * public Yes Yes Yes Yes 分析内存 : 子类对象包含一个父类对象. 重写方法不能使用比被重写方法更严格的访问权限 – 其实这和多态有关 super 关键字 &amp; 继承中的构造方法 如果调用 super 必须写在构造方法的第一行 如果没调用，系统自动调用 super(), 如果没调，父类中又没写参数为空这个构造方法则出错。 Object 类 instanceof 是一个操作符 equals方法 J2SDK 提供的一些类 如 String , Date 重写了Object 的 equals() 方法. 对象转型 casting * 一个基类的引用类型变量可以指向 “其子类的对象”。 * 一个基类的引用不可以访问其子类新增加的成员 * 可以使用 引用 变量 instanceof 类名 来判断该引用型变量所&quot;指向&quot;的对象是否属于该类或该类的子类。 * upcasting / downcasting 内存分析 - 明白了内存你就明白了一切！ 动态绑定, 池绑定, 多态 * 动态绑定的机制 是 实际类型 new 的是！ * 深一点 -- 是对象内部有一个指针。。。。。。 * 动态绑定的机制是 ： 实际类型，还是引用类型。是调用实际类型，不是引用类型。 * 实际地址才会绑定到那个方法上。 方法在 code segment * 只有在运行期间(不是在编译期间)，运行出对象来，才能判断调用哪一个。。。。 这是面向对象核心中的核心。核心中的核心 ! 带来的莫大好处: 可扩展性达到了非常非常的极致好！ 多态总结: 要有继承 要有重写 父类引用指向子类对象 3.4 Abstract class abstract 修饰class时，这个类叫做抽象类. abstract 修饰方法时，该方法叫做抽象方法. abstract class 必须被继承，抽象方法必须被重写. 含有抽象方法的类必须声明为抽象类. abstract class 不能被实例化, 抽象方法只需声明，而不需要实现. 3.5 Final 关键字 final 的变量的值不能够被改变. final 的成员变量、局部变量(形参) final 的方法不能够被重写， final 的类不能够被继承. 系统中的 final class 例如： public final class String public final class Math public final class Boolean 3.6 接口: 一种特殊的抽象类 变量全是: public static final int id = 1; 方法全是: abstract function() java.lang - Comparable 我看就像 cmp 一样！(个人认为) Interface Comparable 可以扩展 interface interface 可以相互继承 class interface 只能是 implement 关系 第四章 异常处理 Java 异常是 Java 提供的用于处理程序中错误的一种机制。 java.lang…Exceptions 写程序有问题要有友好界面 医生开单子 { 1, 鼻腔内感觉异常 2, 体温持续升高 3, 分泌乳白色液体 直接说感冒不就得了么？ } e.printStackTrace(); 非常好！给程序员读。堆栈信息都打印出来！ java.lang.Throwable { 开车在上山走， 1, Error 山爆发 JVM 出问题。 2, Exception { 你可以处理的 – 刹车坏啦！修好再走。。。 1, … 2, RuntimeException (经常出，不用逮) 压路面上的小石子 } 一个 try 可以跟多个catch 所以 { 一个茶壶可以跟多个茶碗，一个男人可以三妻四妾。} try { // 可能抛出异常的语句 语句一； 语句二； } catch(someEx e) { 语句； } catch() { 语句 } finally { } 一 ： 打开 二 ： 关闭 finally : 一般进行资源的清除工作。。。！ 我处理不了的事情 ： 我交给上一级部门去处理！ 当时 catch 到 Ex 的时候，你至少要做出一种处理。要不那是危险的编程习惯！ main() 抛出 就是交给 java 运行时系统啦！ 它会把堆栈信息打出来！ 一个图 ： 五个关键字 { try, catch, finally, throw, throws } 一点问题 { 先逮大的，后逮小的，报错。 } 使用自定义异常 程序中可以使用 throw - 方法后 throws 如果throw抛出异常之后,方法就结束啦！ 注意 ： 重写方法需要抛出与原方法所抛出异常类型一致异常或不抛出异常。 总结 ：{ 一个图 五个关键字 先逮小的，再逮大的。 异常与重写的关系 } 5. 数组 四维空间 int[] a, int a[]; 内存分析 - 知道了内存你就明白了一切！ 本来无一物 ： 何处装数组。 动态初始化 静态初始化 int a[] = {3, 9, 8}; 内部过程屏蔽掉啦！ ipconfig ipconfig -all 这里 -all 就是命令行参数。 基础类型一般分配在栈上面！包装类，用于把基础类型包装成对象，则分配在堆上了。 例如 类 Double, Integer 约瑟夫环 - 面向过程 和 面向对象 写法 另一个比较精巧的算法 ： 用数组来模拟链表 算法和逻辑思维能力不是一朝一夕能完成的。 排序算法紧跟着的是 - 搜索算法 你这里是通过对象square1调用的方法getsquare() public static void main(String[] args){ getsquare(); //这里会出错 } 是的。其实main函数可以简单认为跟本类没什么关系，只是调用本类的 其它静态方法时不用写类名而已。所以，要调用其它非静态方法，都要 先实例化，就像别的类来调用一样。 -------- 我有些懂啦！ 但还是不太懂，我能理解啦！ (个人理解)-------- 二维数组 ： 可以看成以数组为元素的数组 * 数组的拷贝 &#123; &#125; 大公司剥削人剥削得最厉害！ 明白内存你就明白了一切！。。。 总结 { * 数组内存的布局 * 常见算法 } } 6. 常用类 本章内容 { * 字符串相关类 (String, StringBuffer) * 基本数据类型包装类 * Math类 * File类 * 枚举类 * java.lang.String 类代表不可变的字符序列 String s1 = &quot;helo&quot;; String s3 = &quot;hello&quot;; s1 == s3 true 字符串常量 - data seg 区 data segment 编译器有优化 如果是 new s1 == s3 false s1.equals(s3) true 字符串你可以看成是一个字符数组！ String 类常用方法 &#123; * 静态重载方法 public static String valueOf(...) * public String[] spllit(String regex) &#125; String.valueOf(Object obj); 多态的存在 toString java.lang.StringBuffer 代表可变的字符序列 * 基本数据类型包装类 基本数据 ： 栈 -&gt; 包装 --&gt; 堆上面 * 包装类 &#123; * 写程序要循序渐进方法 &#125; * Math 类 &#123; java.lang.Math 其中方法的参数和返回值类型都为 double &#125; * File 类 &#123; java.io.File 类代表系统文件名 (路径和文件名) File 类的常见构造方法 ： * public File(String pathname) 以 pathname 为路径创建 File 对象, 如果 pathname 是相对路径，则默认的当前路径在系统属性 user.dir 中存储 * public File(String parent, String child) * File 的静态属性 String separator 存储了当前系统的路径分隔符。 原型 ： public static final String separator 但是事实上无论在哪 你写 / 都没有问题 注意 \\ 在 java 里面是转义字符 &#125; * Enum - java.lang.Enum 枚举类型 &#123; 1, 只能够取特定值中的一个 2, 使用 enum 关键字 3, 是 java.lang.Enum 4, 举例 ： TestEnum.java &#125; 总结~~~ API 和 金庸的书差不多！ 7. 容器 第七章 容器 &#123; * 容器的概念 - 数组是么， 当然是！ * 容器API * Collection 接口 * Iterator 接口 * 增强的 for 循环 * Set 接口 * List接口 和 Comparable接口 * Collections 类 * Map 接口 * 自动打包 / 解包 * 泛型 (JDK1.5新增) ----- * J2SDk 所提供的容器位于 java.util 包内。 * 容器API的类图如下图所示： -------------------------------------------------- 1136 1136 1136 -- 一个图, 一个类, 三个知识点，六个接口 &lt;&lt;interface&gt;&gt; Collection / \\ &lt;&lt;interface&gt;&gt; &lt;&gt; &lt;&gt; ^ Set List | ^ ^ | | _| | HashSet LinkedList ArrayList HashMap 1136 1136 1136 一个图, 一个类, 三个知识点，六个接口 --------------------------------------------------- * Collection 接口 -- 定义了存取一组对象的方法, 其子接口 Set 和 List 分别定 义了存储方式。 * Set 中的数据对象没有顺序且不可以重复。 * List中的数据对象有顺序且可以重复 * Map 接口定义了存储 “键 (key) -- 值 (value) 映射&quot;对&quot;的方法。 Collection 方法举例 * 容器类对象在调用 remove, contains 等方法时需要比较对象是否相等 这会涉及到对象类型的 equals 方法和 hashCode 方法，对于自定义的 类型，需要要重写 equals 和 hashCode 方法以实现自定义的对象相等 规则。 * 注意 ： 相等的对象应该具有相等的 hashcodes * --- ArrayList 底层是一个数组 哈哈哈哈哈 ： 装入的是对象，因为对象在堆上，栈里面的内容随时可能被清空！ hashCode 能直接定位到那个对象 toyreb Iterator - 接口最小化原则 我这大管家在做操作的时候 ： 连主人做任何的操作都不让操作啦！因为 iterater 执行了锁定，谁也不让谁看！ JDK 1.5 增强的 for 循环 Set &#123; HashSet, TreeSet 一个以 hash 表实现， 一个以 树 结构实现 &#125; List &#123; Object set(int index, Object element) int indexof(Object o); int lastIndexof(Object o); &#125; *** 一个类 { Collections – java.util.Collections 提供了一些静态方法实现了基于List容器的一些常用算法 例如 &#123; void sort(List) ... ... ... &#125; LinkedList -- 逆序的时候效率较 ArrayList 高！ 对于 特定的 对象怎么确定谁大谁小。 &#123; 对象间可以比较大小 通过 接口 只能看见对象的一点】 Comparable 接口 -- 所有可以实现排序的类 都实现了 Comparable 接口 &#125; public int compareTo(Object obj) 泛型规定 - 只能传 “猫” vector / hashtable 以前遗留下来的。效率特别低 Map 接口 &#123; Map 接口的实现类有 HashMap 和 TreeMap 等。 &#123;hashmap 用 hash表来实现， TreeMap 用二叉树来实现-红黑&#125; Map 类中存储的键 - 值对通过键来标识，所以键值不能重复。&#123; 不能重复 ： 是equals() equals() 太慢， 所以我们用 hashCode() 来比较 &#125; &#125; JDK 1.5 之后 &#123; 可以自动打包和解包 - Auto-boxing / unboxing * 在合适的时机自动打包 , 解包 * 自动将基础类型转化为对象 * 自动将对象转换为基础类型 * TestMap2.java &#125; 示例练习 &#123; TestArgsWords.java &#125; JDK 1.5 泛型 起因 ： JDK 1.4 以前类型不明确 装入集合类型都被当作 Object 对待, 从而失去自己的实际类型。 从集合中取出时往往需要转型, 效率低, 容易产生错误。 解决办法 ： 在定义集合的时候同时定义集合中对象的类型 示例 ： BasicGeneric.java 可以在定义 Collection 的时候指定 也可以在循环时用 Iterator 指定 好处 ： 增强程序的可读性和稳定性 什么时候可以指定自己的类型 ： 你看 API， 他跟你就可以跟 总结 { 1136 一个图 一个类 Collections 三个知识点 For Generic Auto-boxing / unboxing 六个接口 { 1, Collection { 2, Set, 3, List } 4, Map 5, Iterator 6, Comparable } 8. IO 流 能帮助你建立文件，不能帮你建目录 读到内存区 - * 转换流 &#123; 中文 windos 编码 JBK 当前系统默认的 编码是 JBK IOS8859_1 包含的所有西欧语言 --&gt; 后来才推出 UniCode (国际标准化组织为了包含全球) JBK JB2312 JB18030 东方人自己的编码 - 国标码 - 就是汉字，你可以认为 日文，韩文 都有自己的编码 - 台湾有自己的 大五码 拉丁1， 2， 3， 4， 5. 6. 7. 8. 9 都同意啦！包括俄罗斯， 但是中文还不行 ---- &gt; UniCode FileOutputStream() 构造方法自己去查 * System.in &#123; System 类 -- in 是 InputStream 类型 public static final InputStream in 抽象类 类型， 又是父类引用指向子类对象 InputStreamReader isr = new InputStreamReader(System.in); System.in -&gt; 管道直接堆到黑窗口上 BufferedReader br = new BufferedReader(isr); wait() 运行后 ： 等待在那 - 阻塞式的方法 很多 readLine() 有点特殊 其实是 System.in 比较特殊 -- 标准输入 - 等待着标准输入 &#123; 你不输入 - 我可不就等着么，当然这个也叫做同步方法。 你不输入，我就不能干别的 同步式的 &#125; * 数据流 &#123; 请你把 long 类型的数， 写到 --&gt; 文件里面去 readUTF() UTF8 比较省空间 &#125; * 打印流 &#123; System.out out - public static final PrintStream 默认在我们的黑窗口输出 语言代表人的思维 - 能够促进人的思维 log4J 著名的日志开发包 &#125; * Object 流 &#123; 把整个 Object 全部写入硬盘被 在 VC 上叫做系列化 存盘点。 挨着排的序列化 再一点一点读出来 Serializable 接口 --- 标记性的接口 transient int k = 15; 相当于这个 k 是透明的。在序列化的时候不给于考虑，读的时候读默认值。 * Serializable 接口 * Externalizable 接口 extends Serializable &#125; }","categories":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"}]},{"title":"Trie Tree POJ 2001","slug":"leetcode/acm/Trie_Tree-POJ-2001","date":"2013-01-17T10:06:21.000Z","updated":"2021-06-20T04:12:28.276Z","comments":true,"path":"2013/01/17/leetcode/acm/Trie_Tree-POJ-2001/","link":"","permalink":"http://www.iequa.com/2013/01/17/leetcode/acm/Trie_Tree-POJ-2001/","excerpt":"trie tree poj 2001 shortest prefixes","text":"trie tree poj 2001 shortest prefixes POJ 2001 Shortest Prefixes 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include &lt;iostream&gt; #include &lt;string&gt; #include &lt;cstring&gt; #include &lt;cstdlib&gt; #include &lt;cstdio&gt; #include &lt;cmath&gt; #include &lt;vector&gt; #include &lt;stack&gt; #include &lt;deque&gt; #include &lt;queue&gt; #include &lt;bitset&gt; #include &lt;list&gt; #include &lt;map&gt; #include &lt;set&gt; #include &lt;iterator&gt; #include &lt;algorithm&gt; #include &lt;functional&gt; #include &lt;utility&gt; #include &lt;sstream&gt; #include &lt;climits&gt; #include &lt;cassert&gt; #define BUG puts(&quot;here!!!&quot;); using namespace std; const int N = 1010; const int kind = 26; char str[N][25]; struct Node &#123; int num; bool tail; Node* next[kind]; Node() : num(1), tail(false) &#123; memset(next, 0, sizeof(next)); &#125; &#125;; void insert(Node* root, char *s) &#123; Node* p = root; int i = 0, index; while(s[i]) &#123; index = s[i] - &#x27;a&#x27;; if(p-&gt;next[index] == NULL) &#123; p-&gt;next[index] = new Node(); &#125; else p-&gt;next[index]-&gt;num++; p = p-&gt;next[index]; i++; &#125; p-&gt;tail = true; &#125; void solve(Node* root, int count) &#123; for(int i = 0; i &lt; count; i++) &#123; Node* p = root; int len = strlen(str[i]); char* s = str[i]; cout &lt;&lt; s &lt;&lt; &#x27; &#x27;; for(int j = 0; j &lt; len; j++) &#123; cout &lt;&lt; s[j]; int index = s[j] - &#x27;a&#x27;; if(p-&gt;next[index]-&gt;num == 1) &#123; break; &#125; p = p-&gt;next[index]; &#125; cout &lt;&lt; endl; &#125; &#125; int main() &#123; Node* root = new Node(); // 根节点不包含任何字符 int i = 0, count = 0; while(scanf(&quot;%s&quot;, str[i]) == 1) &#123; insert(root, str[i]); i++; &#125; count = i; solve(root, count); return 0; &#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"Union-Find Sets POJ 1703","slug":"leetcode/acm/union-find_sets-POJ-1703","date":"2013-01-17T10:04:21.000Z","updated":"2021-06-20T04:12:28.276Z","comments":true,"path":"2013/01/17/leetcode/acm/union-find_sets-POJ-1703/","link":"","permalink":"http://www.iequa.com/2013/01/17/leetcode/acm/union-find_sets-POJ-1703/","excerpt":"union-find sets poj 1703 find them, catch them 帮派之争","text":"union-find sets poj 1703 find them, catch them 帮派之争 Find them, Catch them Sample Input 123456715 5A 1 2D 1 2A 1 2D 2 4A 1 4 Sample Output 123Not sure yet.In different gangs.In the same gang. Code 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;iostream&gt; #include &lt;string&gt; #include &lt;cstring&gt; #include &lt;cstdlib&gt; #include &lt;cstdio&gt; #include &lt;cmath&gt; #include &lt;vector&gt; #include &lt;stack&gt; #include &lt;deque&gt; #include &lt;queue&gt; #include &lt;bitset&gt; #include &lt;list&gt; #include &lt;map&gt; #include &lt;set&gt; #include &lt;iterator&gt; #include &lt;algorithm&gt; #include &lt;functional&gt; #include &lt;utility&gt; #include &lt;sstream&gt; #include &lt;climits&gt; #include &lt;cassert&gt; #define MID(x,y) ( ( x + y ) &gt;&gt; 1 ) #define L(x) ( x &lt;&lt; 1 ) #define R(x) ( x &lt;&lt; 1 | 1 ) #define BUG puts(&quot;here!!!&quot;); #define STOP system(&quot;pause&quot;); using namespace std; const int N = 100005; int f[N+N]; int n, m; int find(int x) &#123; if(f[x] &lt; 0) return x; return f[x] = find(f[x]); &#125; int main() &#123; int loop; cin &gt;&gt; loop; while(loop--) &#123; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); memset(f, 255, sizeof(f)); while(m--) &#123; int a, b; char s[3]; scanf(&quot;%s%d%d&quot;, s, &amp;a, &amp;b); if(s[0] == &#x27;A&#x27;) &#123; if(find(a) != find(b) &amp;&amp; find(a) != find(b+n)) &#123; printf(&quot;Not sure yet.\\n&quot;); &#125; else if(find(a) == find(b)) &#123; printf(&quot;In the same gang.\\n&quot;); &#125; else printf(&quot;In different gangs.\\n&quot;); &#125; else &#123; if(find(a) != find(b+n)) &#123; f[find(a)] = find(b+n); f[find(b)] = find(a+n); &#125; &#125; &#125; &#125; return 0; &#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"Union-Find Sets HDU 1856","slug":"leetcode/acm/union-find_sets-hdu_1856","date":"2013-01-17T07:28:21.000Z","updated":"2021-06-20T04:12:28.278Z","comments":true,"path":"2013/01/17/leetcode/acm/union-find_sets-hdu_1856/","link":"","permalink":"http://www.iequa.com/2013/01/17/leetcode/acm/union-find_sets-hdu_1856/","excerpt":"union-find sets hdu 1856 more is better","text":"union-find sets hdu 1856 more is better More is better 1Mr Wang wants some boys to help him with a project. ... Sample Input 1234567891041 23 45 61 641 23 45 67 8 Sample Output 1242 Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include &lt;iostream&gt; #include &lt;string&gt; #include &lt;cstring&gt; #include &lt;cstdlib&gt; #include &lt;cstdio&gt; #include &lt;cmath&gt; #include &lt;vector&gt; #include &lt;stack&gt; #include &lt;deque&gt; #include &lt;queue&gt; #include &lt;bitset&gt; #include &lt;list&gt; #include &lt;map&gt; #include &lt;set&gt; #include &lt;iterator&gt; #include &lt;algorithm&gt; #include &lt;functional&gt; #include &lt;utility&gt; #include &lt;sstream&gt; #include &lt;climits&gt; #include &lt;cassert&gt; #define BUG puts(&quot;here!!!&quot;); using namespace std; const int N = 100005; struct Node &#123; int par; int sum; &#125;; int SUM; Node p[2*N + 5]; void makeSet(int n) &#123; for(int i = 0; i &lt;= 2*n; i++) &#123; p[i].par = i; p[i].sum = 1; &#125; SUM = 1; &#125; int find(int a) &#123; if(a == p[a].par) return a; return p[a].par = find(p[a].par); &#125; void union1(int a, int b) &#123; int fa = find(a); int fb = find(b); if(fa != fb) &#123; p[fa].par = fb; p[fb].sum += p[fa].sum; &#125; if(p[fb].sum &gt; SUM) &#123; SUM = p[fb].sum; &#125; &#125; int main() &#123; int n, a, b; while(scanf(&quot;%d&quot;, &amp;n) == 1) &#123; makeSet(n); while(n--) &#123; scanf(&quot;%d%d&quot;, &amp;a, &amp;b); union1(a, b); &#125; printf(&quot;%d\\n&quot;, SUM); &#125; return 0; &#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"KMP / HDU 1711 找到匹配的位置并返回","slug":"leetcode/acm/kmp-for-hdu_1711","date":"2013-01-17T07:22:21.000Z","updated":"2021-06-20T04:12:28.277Z","comments":true,"path":"2013/01/17/leetcode/acm/kmp-for-hdu_1711/","link":"","permalink":"http://www.iequa.com/2013/01/17/leetcode/acm/kmp-for-hdu_1711/","excerpt":"kmp hdu 1711 number sequence","text":"kmp hdu 1711 number sequence Description Given two sequences of numbers : a[1], a[2], …… , a[N], and b[1], b[2], …… , b[M] (1 &lt;= M &lt;= 10000, 1 &lt;= N &lt;= 1000000). Your task is to find a number K which make a[K] = b[1], a[K + 1] = b[2], …… , a[K + M – 1] = b[M]. If there are more than one K exist, output the smallest one. Input The first line of input is a number T which indicate the number of cases. Each case contains three lines. The first line is two numbers N and M (1 &lt;= M &lt;= 10000, 1 &lt;= N &lt;= 1000000). The second line contains N integers which indicate a[1], a[2], …… , a[N]. The third line contains M integers which indicate b[1], b[2], …… , b[M]. All integers are in the range of [-1000000, 1000000]. Output For each test case, you should output one line which only contain K described above. If no such K exists, output -1 instead. Sample Input 2 13 5 1 2 1 2 3 1 2 3 1 3 2 1 2 1 2 3 1 3 13 5 1 2 1 2 3 1 2 3 1 3 2 1 2 1 2 3 2 1 Sample Output 6 -1 Code 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;iostream&gt; #include &lt;string&gt; #include &lt;cstring&gt; #include &lt;cstdlib&gt; #include &lt;cstdio&gt; #include &lt;cmath&gt; #include &lt;vector&gt; #include &lt;stack&gt; #include &lt;deque&gt; #include &lt;queue&gt; #include &lt;bitset&gt; #include &lt;list&gt; #include &lt;map&gt; #include &lt;set&gt; #include &lt;iterator&gt; #include &lt;algorithm&gt; #include &lt;functional&gt; #include &lt;utility&gt; #include &lt;sstream&gt; #include &lt;climits&gt; #include &lt;cassert&gt; #define BUG puts(&quot;here!!!&quot;); using namespace std; const int N = 1000005; const int M = 10005; int s[N]; int t[M]; int next[M]; void getNext(int len) &#123; int i, j; i = 0, j = -1; next[0] = -1; while(i &lt; len-1) &#123; if(j == -1 || t[i] == t[j]) &#123; i++, j++, next[i] = j; &#125; else &#123; j = next[j]; &#125; &#125; &#125; int kmp(int sl, int tl) &#123; int i = 0, j = 0; while(i &lt; sl &amp;&amp; j &lt; tl) &#123; if(j == -1 || s[i] == t[j]) &#123; i++, j++; &#125; else j = next[j]; &#125; if(j == tl) return i-j+1; else return -1; &#125; // abcabcababcabcabdef // abcabcabd int main() &#123; int T, n, m, ans; cin &gt;&gt; T; while(T--) &#123; cin &gt;&gt; n &gt;&gt; m; for(int i = 0; i &lt; n; i++) &#123; scanf(&quot;%d&quot;, s+i); &#125; for(int i = 0; i &lt; m; i++) &#123; scanf(&quot;%d&quot;, t+i); &#125; getNext(m); ans = kmp(n, m); cout &lt;&lt; ans &lt;&lt; endl; &#125; return 0; &#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"需要的气质品格","slug":"tools/being-eq-gentleman","date":"2013-01-16T22:24:16.000Z","updated":"2021-06-22T06:52:37.109Z","comments":true,"path":"2013/01/17/tools/being-eq-gentleman/","link":"","permalink":"http://www.iequa.com/2013/01/17/tools/being-eq-gentleman/","excerpt":"做一个 成熟、绅士、稳重 的人，我最近越来越觉得它非常非常非常重要…","text":"做一个 成熟、绅士、稳重 的人，我最近越来越觉得它非常非常非常重要… 一：沉稳 （1）不要随便显露你的情绪。 （2）不要逢人就诉说你的困难和遭遇。 （3）在征询别人的意见之前，自己先思考，但不要先讲。 （4）不要一有机会就唠叨你的不满。 （5）重要的决定尽量有别人商量，最好隔一天再发布。 （6）讲话不要有任何的慌张，走路也是。 二：细心 （1）对身边发生的事情，常思考它们的因果关系。 （2）对做不到位的执行问题，要发掘它们的根本症结。 （3）对习以为常的做事方法，要有改进或优化的建议。 （4）做什么事情都要养成有条不紊和井然有序的习惯。 （5）经常去找几个别人看不出来的毛病或弊端。 （6）自己要随时随地对有所不足的地方补位。 三：胆识 （1）不要常用缺乏自信的词句 （2）不要常常反悔，轻易推翻已经决定的事。 （3）在众人争执不休时，不要没有主见。 （4）整体氛围低落时，你要乐观、阳光。 （5）做任何事情都要用心，因为有人在看着你。 （6）事情不顺的时候，歇口气，重新寻找突破口，就结束也要干净利落。 四：大度 （1）不要刻意把有可能是伙伴的人变成对手。 （2）对别人的小过失、小错误不要斤斤计较。 （3）在金钱上要大方，学习三施（财施、法施、无畏施） （4）不要有权力的傲慢和知识的偏见。 （5）任何成果和成就都应和别人分享。 （6）必须有人牺牲或奉献的时候，自己走在前面。 五：诚信 （1）做不到的事情不要说，说了就努力做到。 （2）虚的口号或标语不要常挂嘴上。 （3）针对客户提出的“不诚信”问题，拿出改善的方法。 （4）停止一切“不道德”的手段。 （5）耍弄小聪明，要不得！ （6）计算一下产品或服务的诚信代价，那就是品牌成本。 六：担当 （1）检讨任何过失的时候，先从自身或自己人开始反省。 （2）事项结束后，先审查过错，再列述功劳。 （3）认错从上级开始，表功从下级启动. （4）着手一个计划，先将权责界定清楚，而且分配得当。 （5）对“怕事”的人或组织要挑明了说。 （6）因为勇于承担责任所造成的损失，公司应该承担 Reference 转自网络","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"life","slug":"life","permalink":"http://www.iequa.com/tags/life/"}]},{"title":"Linux C语言的一些知识学习","slug":"devops/linux-c","date":"2012-05-19T02:54:16.000Z","updated":"2021-06-20T04:12:28.205Z","comments":true,"path":"2012/05/19/devops/linux-c/","link":"","permalink":"http://www.iequa.com/2012/05/19/devops/linux-c/","excerpt":"Linux C语言的一些知识, 涉及 计算机程序，语言发展，编译链接，数据，函数库等等.","text":"Linux C语言的一些知识, 涉及 计算机程序，语言发展，编译链接，数据，函数库等等. 1. 程序 什么是程序？ 程序的作用是什么？ 我们必须通过指令，指挥计算机执行我们想要它做的动作。 而依照顺序执行的一组指令就是程序。 1.1 程序特征 1, 程序是与计算机沟通的语言 2, 程序是由特定语法与关键字构成 3, 程序是一行一行执行的 4, 程序的执行, 从入口点开始, 原则上是由上而下, 从左而右执行。 入口点可能是一个方法或者是函数。。。 2. 程序语言的演化 1, 机器语言 – 计算机通过电信号模式 0/1/0/1…开关开关… 2, 汇编语言 3, 高级语言 4, 第四代语言 5, 自然语言 – 智能语言，开机，它就开机。关机，它就关机。 2.1 机器语言 1, 是计算机唯一能够执行的语言。 2, 其他语言必须先转化为机器语言 3, 指令有 0 与 1 组成, 称为机器码。 4, 指令难记忆但执行速度最快。 5, 不同类型机器有不同的机器码, 不具备移植性。 2.2 高级语言 1, 离机器越来越远 2, 语法接近人类的自然语言 3, 执行的单位不是指令, 而是语句, 一行语句。 4, 开发效率越来越高。 5, 必须编译成机器码 6, 移植性比较好。 2.3 第四代语言 1, 4GL, 也称为面向问题的程序语言。 2, 仅需告诉计算机 “做什么”, 不需要指挥计算机&quot;如何做&quot; 3, 大大提高开发效率 4, 如 SQL 语言及各种查询语言。 2.4 自然语言 1, 运用人工智能, 以接近口语的指令指挥计算机，如语音。 2, 还没有成熟。 3. 编译与链接 3.1 编译 1, 把源代码转化成机器码的过程。 2, 之前还会进行前期处理。 3, 过程中会进行语法检查。 4, 编译器 : 完成编译动作的程序 3.2 连接 1, 将可执行文件与包含文件 / 用到的函数库建立关联的过程。 4. 基本概念 4.1 数据 是计算机程序处理的对象, 可以是整数, 实数, 字符, 也可以是图像, 声音等的编码表示。 4.2 数据结构 指的是数据与数据间存在一种或多种特定关系。 与数据结构密切相关的便是数据的类型和数据的存放。 程序设计 编写程序的过程。 软件 程序 + 文档。 5. Linux C 概述 Linux 和 C 天生有不解之源, Linux 的操作系统内核就主要是用 C 写的, 另外 Linux 下的很多软件也是用 C 写的，特别是一些著名的服务软件, 如何 MySQL, Apache 等。 5.1 开发环境的构成 1, 编辑器 ： 选择 VI 2, 编译器 ： 选择 GNU C/C++ 编译器 gcc (免费开源的工具 - linux发型版本多数都自动安装) 3, 调试器 ： 应用广泛的 gdb (JDK 学习的 GDB) 4, 函数库 ： glibc 5, 系统头文件 ： glibc_header 5.2 编译器 gcc gcc (GNU CCompiler) 是GNU推出的功能强大 , 性能优越的多平台编译器, gcc 编译器能将C , C++语言源程序编译, 连接成可执行文件, 以下是 gcc 支持编译的一些源文件的后缀及其解释。 .c 为后缀的文件, C语言源代码文件 .h 为后缀的文件, 是程序所包含的头文件。 .i 为后缀的文件, 是已经预处理过的 C 源代码文件; .o 为后缀的文件, 是编译后的目标文件。 .s 为后缀的文件, 是汇编语言源代码文件。 gcc -o hello hello.c 用 gcc 来编译我们的源程序 -o 选项要求编译器给我们输出的可执行文件名为 hello hello.c 是我们的源程序文件。 -c 选项 表示我们只要求编译器输出目标代码, 而不必要输出可执行文件 -g 选项表示我们要求编译器在编译的时候提供我们以后对程序进行调试的信息。 执行 ./hello 就可以看到程序的输出结构。 (在当前目录下去找) 5.3 函数库 glibc 要构建一个完整的 C 开发环境, Glibc 是必不可少的, 它是 Linux 下 C 的主要函数库. glibc 有两种安装方式： A. 安装成测试用的函数库, 在编译程序时用不同的选项来试用新的函数库。 B. 安装成主要的 C 函数库, 所有新编译程序均用的函数库 glibc 含几个附加包 : Linuxthreads， localedate 和 crypt， 他们的文件名随版本的不同而类似于下列 ： 1234glibc-2.06.tar.gzglibc-linuxthreads-2.0.6.tar.gzglibc-localedate-2.0.6.tar.gzglibc-crypt-2.0.6.tar.gz glibc 是提供系统调用和基本函数的Ｃ库, 比如 open, malloc, printf 等等。 所有动态连接的程序都要用到它. 系统头文件 ： glibc_header 缺少系统头文件，很多用到系统功能的C程序将无法编译。 如果用户在安装过程中少装了这些包，就会无法编译C程序，解决方法： 方法一：不推荐：重装一遍Linux系统。 方法二：通过找一些Rpm包来迅速安装Linux的C开发环境。 C开发环境对应的rpm包 由于gcc包需依赖 binutils和cpp包，另外make包也是在编译中常用的，所以一共需要8个包来完成安装，它们是： 12345678cpp-2.96-110.i386.rpmbinutils-2.11.93.0.2-11. i386.rpmGlibc-2.2.5-31. i386.rpmGlibc-kernheaders-2.4-7.14. i386.rpmGlibc-common-2.2.5-34Glibc-devel-2.2.5-34- i386.rpmGcc-2.96-110.i386.rpmMake-3.79.1-8. i386.rpm 查看glibc的版本：ls /lib/libc-* 查看gcc版本号： gcc –version 5.4 C 程序的组成 对于一个 C 程序, 安装完成后可以分成三个组成 ： 可执行文件 包含文件 库文件 可执行文件是最终运行的命令, 包含文件是该 C 程序 include 的定义文件, 库文件则是该 C 程序自定义的库。 比如用 RPM 安装的 Mysql 数据库 ： 可执行文件放在 /usr/bin 下 包含文件放在 /usr/include/mysql 下 库文件在 /usr/lib/mysql 下 只有系统可以找到程序对应的包含文件和库文件, 程序可执行文件才能正常运行。 6. 小结 开发环境的构成 GNU 的 gcc 编译器 glibc 函数库 Linux 下 C 程序开发过程 第一个 C 程序 C 程序结构","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"Linux_C","slug":"Linux-C","permalink":"http://www.iequa.com/tags/Linux-C/"}]},{"title":"学用 g++ (初步)","slug":"devops/g++","date":"2012-03-22T02:54:16.000Z","updated":"2021-06-20T04:12:28.197Z","comments":true,"path":"2012/03/22/devops/g++/","link":"","permalink":"http://www.iequa.com/2012/03/22/devops/g++/","excerpt":"gcc 和 g++ 都是GNU(组织)的一个编译器.","text":"gcc 和 g++ 都是GNU(组织)的一个编译器. 链接库 动态链接库 (通常以 .so 结尾) 和 静态链接库 (通常以 .a 结尾) 两者的差别仅在程序执行时所需的代码是在运行时加载的, 还是在编译时加载的, 默认情况下, g++ 在链接时优先使用动态链接库, 只有当动态链接库不存在时才考虑使用静态链接库. 如果需要的话可以在编译时加上 -static 选项, 强制使用静态链接库。 1g++ foo.cpp -L /home/xiaowp/lib -static -lfoo -o foo 代码优化 代码优化指的是编译器用过分析源代码, 找出其中尚未达到最优的部分, 然后对其重新进行组合, 目的是改善程序的执行性能. g++ 通过编译选项 -On 来控制优化代码的生成 (n 一般 0 ~ 2,3) 学用 g++ GCC (GNC Compiler Collection) 是 linux 下最主要的编译工具, GCC 不仅功能强大, 结构也异常灵活. g++ 是 gcc 中的一个工具, 专门来编译 C++ 语言的。 $ g++ hello.cpp -o hello (hello 是编译成的可执行文件) $ ./hello (运行 hello)","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"g++","slug":"g","permalink":"http://www.iequa.com/tags/g/"}]},{"title":"寄存器, 内存, 存储器, Cache 的区别","slug":"devops/linux-computer-memory","date":"2012-03-18T02:54:16.000Z","updated":"2021-06-20T04:12:28.196Z","comments":true,"path":"2012/03/18/devops/linux-computer-memory/","link":"","permalink":"http://www.iequa.com/2012/03/18/devops/linux-computer-memory/","excerpt":"计算机存储中的 寄存器 内存 存储器 cache区别， 从范围来看，它们所指的范畴就不一样。","text":"计算机存储中的 寄存器 内存 存储器 cache区别， 从范围来看，它们所指的范畴就不一样。 寄存器 中央处理器内的组成部份。它跟CPU有关。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。 存储器 存储器范围最大，它几乎涵盖了所有关于存储的范畴。你所说的寄存器，内存，都是存储器里面的一种。凡是有存储能力的硬件，都可以称之为存储器，这是自然，硬盘更加明显了，它归入外存储器行列，由此可见——。 内存 既专业名上的内存储器，它不是个什么神秘的东西，它也只是存储器中的沧海一粟，它包涵的范围也很大，一般分为只读存储器和随机存储器，以及最强悍的高速缓冲存储器（CACHE），只读存储器应用广泛，它通常是一块在硬件上集成的可读芯片，作用是识别与控制硬件，它的特点是只可读取，不能写入。随机存储器的特点是可读可写，断电后一切数据都消失，我们所说的内存条就是指它了。 Cache 在CPU中速度非常块，而容量却很小的一种存储器，它是计算机存储器中最强悍的存储器。由于技术限制，容量很难提升，一般都不过兆。 Reference 转自 : hp_carrot","categories":[{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"}],"tags":[{"name":"memory","slug":"memory","permalink":"http://www.iequa.com/tags/memory/"}]},{"title":"Graph Theory 近期小结","slug":"leetcode/acm/graph_theory_summary","date":"2011-12-29T02:26:48.000Z","updated":"2021-06-20T04:12:28.276Z","comments":true,"path":"2011/12/29/leetcode/acm/graph_theory_summary/","link":"","permalink":"http://www.iequa.com/2011/12/29/leetcode/acm/graph_theory_summary/","excerpt":"关于图论的学习总结, 在本博可以详细的体现，做的大部分都是非常经典的图论算法入门题。","text":"关于图论的学习总结, 在本博可以详细的体现，做的大部分都是非常经典的图论算法入门题。 断断续续，也要接近一个月的时间啦！我今天终于把图论的大部分经典算法 复习+学习 小完毕。 因为中间学校总会有一些杂事在影响我的进度，所以是几经周折，断断续续，但是值得庆幸的是我一直没有间断过，这方面的学习。 本人图论所涉猎的算法如下 { 拓扑排序 (Topology-h1285 ) 最小生成树 (Kruska-p1287 / Prim-p1287 ) 最短路 ( Dijkstra-p2387 / Bellman_Ford-z3033 / Bellman_Ford-p3259 / SPFA-template / SPFA-邻接矩阵-p3259 / SPFA-邻接表-p3259 / Floyd-template / Floyd-p1502 ) 二分图 (Hungary . poj 1274 / poj 2446 / zoj 1654 KM_CA_poj 2195O(n^4)/ KM_O(n^3)_p2195 ) 网络流 (EK_入门_p1273 / Dinic_入门_p1273 / 最小费用最大流(SPFA+EK)_p2195) 强连通分量与缩点(有向图) Korasaju_p2186 / Korasaju and Tarjan_p1236 。 图的割点与桥(无向图) ( Tarjan割点和桥的示例程序。 Sample : poj 144 求割点个数) 双连通分量(无向图) (Tarjan点双连通示例程序 / tarjan_边双连通-缩点p3352。 Sample : poj 3352 求加入最少的边使其变成边双连通分支) 2-sat(这个我目前只能求判定性的问题) : poj 3207 / poj 3678 / poj 2723 Sample: Sample : poj 2186 求有多少顶点是由任何顶点出发都可达。 Sample : poj 1236 至少要选几个顶点，才能做到从这些顶点出发，可以到达全部顶点 至少要加多少条边，才能使得从任何一个顶点出发，都能到达全部顶点","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"致敬奋斗的自己","slug":"tools/Chicken-soup","date":"2011-11-29T02:26:48.000Z","updated":"2021-06-22T06:52:37.116Z","comments":true,"path":"2011/11/29/tools/Chicken-soup/","link":"","permalink":"http://www.iequa.com/2011/11/29/tools/Chicken-soup/","excerpt":"这是关于一些励志的句子， 现在看那时候的我，真的是超励志， 超刻苦","text":"这是关于一些励志的句子， 现在看那时候的我，真的是超励志， 超刻苦 三军可夺帅也，匹夫不可夺志也！ 天行健，君子以自强不息！ 宝剑锋自磨砺出，梅花香自苦寒来！ 耐心候好运，好运常会来！ 不经一番寒彻骨，怎得梅花扑鼻香！ 成事不在于力量的大小，而在于能坚持多久！ 绳锯木断，水滴石穿！ 蚓无爪牙之利，筋骨之强，上食埃土，下饮黄泉，用心一也！ 疾风知劲草，烈火炼真金。 不经寒霜苦，安能香袭人？ 锋自磨砺出，玉乃雕琢成。 人而不苦练，焉能艺精深？","categories":[{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"}],"tags":[{"name":"Chicken-soup","slug":"Chicken-soup","permalink":"http://www.iequa.com/tags/Chicken-soup/"}]},{"title":"Shortest Path","slug":"leetcode/acm/shortest-path","date":"2011-10-27T07:28:21.000Z","updated":"2021-06-20T04:12:28.278Z","comments":true,"path":"2011/10/27/leetcode/acm/shortest-path/","link":"","permalink":"http://www.iequa.com/2011/10/27/leetcode/acm/shortest-path/","excerpt":"shortest path ： dijstra 、 Bellman 、 Floyd 、 SPFA","text":"shortest path ： dijstra 、 Bellman 、 Floyd 、 SPFA 1. dijstra 1234567891011121314151617181920212223242526272829int data[M][M]; // init INFint lowc[M];int vis[M];int n, m;void djst(int p) &#123; int i, j; for(i = 1; i &lt;= n; i++) &#123; vis[i] = 0; lowc[i] = data[p][i]; &#125; vis[p] = 1; for(i = 1; i &lt;= n-1; i++) &#123; int minc = INF, c = 0, lk; for(j = 1; j &lt;= n; j++) &#123; if(vis[j] == 0 &amp;&amp; lowc[j] &lt; minc) &#123; minc = lowc[j]; c = j; &#125; &#125; if(c == 1) break; vis[c] = 1; for(j = 1; j &lt;= n; j++) &#123; if(vis[j] == 0 &amp;&amp; data[c][j] + minc &gt; 0 &amp;&amp; data[c][j] + minc &lt; lowc[j]) &#123; lowc[j] = data[c][j] + minc; &#125; &#125; &#125; cout &lt;&lt; lowc[1] &lt;&lt; endl;&#125; 2. Bellman 1234567891011121314151617181920212223242526272829#define INF ((long long)(1))&lt;&lt;62#define N 301using namespace std;struct edge&#123; int u; int v; long long w; // 注意&#125;e[N*N];int m, n;long long d[1005];bool bellman_ford(int s, int di) &#123; int i, j; for(i = 1; i &lt; n; i++) &#123; d[i] = INF; &#125; d[s] = 0; for(i = 1; i &lt;= n-1; i++) &#123; for(j = 1; j &lt;= m; j++) &#123; if(d[e[j].u] != INF &amp;&amp; d[e[j].u]+e[j].w &lt; d[e[j].v]) // 对边进行操作 、松弛 d[e[j].v] = d[e[j].u] + e[j].w; &#125; &#125; for(j = 1; j &lt;= m; j++) &#123; if(d[e[j].u] != INF &amp;&amp; (d[e[j].v] &gt; d[e[j].u]+e[j].w)) // 很理解 d[e[j].v] = -INF; &#125; if(d[di] == INF || d[di] == -INF) return false; return true;&#125; 3. Floyd 4. SPFA 123456789101112131415161718192021222324252627282930313233343536373839404142434445const int INF = 0x7fffffff;const int N = 5501;struct edge &#123; int to; int w;&#125;;vector&lt;edge&gt; p[N]; // vector 实现邻接表int d[N];bool inque[N]; // 记录顶点是否在队列中，SPFA算法可以入队列多次int cnt[N]; // 记录顶点入队的次数int n, m, q;bool SPFA(int s) &#123; queue&lt;int&gt; Q; while(!Q.empty()) Q.pop(); int i; for(i = 0; i &lt;= n; i++) &#123; d[i] = INF; &#125; d[s] = 0; // 源点的距离为 0 memset(inque, 0, sizeof(inque)); memset(cnt, 0, sizeof(cnt)); Q.push(s); inque[s] = true; cnt[s]++; // 源点入队列的次数增加 while(!Q.empty()) &#123; int t = Q.front(); Q.pop(); inque[t] = false; for(i = 0; i &lt; p[t].size(); i++) &#123; int to = p[t][i].to; if(d[t] &lt; INF &amp;&amp; d[to] &gt; d[t] + p[t][i].w) &#123; d[to] = d[t] + p[t][i].w; cnt[to]++; if(cnt[to] &gt;= n) &#123; //当一个点入队的次数&gt;=n时就证明出现了负环 return false; &#125; if(!inque[to]) &#123; Q.push(to); inque[to] = true; &#125; &#125; &#125; &#125; return true;&#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"Six Kinds of Sort algorithms","slug":"leetcode/six-sort","date":"2011-10-27T02:28:21.000Z","updated":"2021-06-20T04:12:28.274Z","comments":true,"path":"2011/10/27/leetcode/six-sort/","link":"","permalink":"http://www.iequa.com/2011/10/27/leetcode/six-sort/","excerpt":"input ： 8, 5, 4, 9, 2, 3, 6","text":"input ： 8, 5, 4, 9, 2, 3, 6 1. heapSort 12345678910111213141516171819202122232425262728void heapify(int a[], int i, int size) &#123; // 堆化的维持需要用递归 int ls = 2*i, rs = 2*i + 1; int large = i; if(ls &lt;= size &amp;&amp; a[ls] &gt; a[i]) &#123; large = ls; &#125; if(rs &lt;= size &amp;&amp; a[rs] &gt; a[large]) &#123; large = rs; &#125; if(large != i) &#123; swap(a[i], a[large]); heapify(a, large, size); &#125;&#125;void buildHeap(int a[], int size) &#123; for(int i = size/2; i &gt;= 1; i--) &#123; heapify(a, i, size); &#125;&#125;void heapSort(int a[], int size) &#123; buildHeap(a, size); int len = size; for(int i = len; i &gt;= 2; i--) &#123; swap(a[i], a[1]); len--; heapify(a, 1, len); &#125;&#125; 2. quickSort 1234567891011121314void quickSort(int a[], int left, int right) &#123; if(left &lt; right) &#123; // exit. good idea! int l = left, r = right, x = a[l]; while(1) &#123; while(l &lt; r &amp;&amp; a[r] &gt;= x) r--; while(l &lt; r &amp;&amp; a[l] &lt;= x) l++; if(l &gt;= r) break; swap(a[r], a[l]); &#125; swap(a[left], a[l]); quickSort(a, left, l-1); quickSort(a, l+1, right); &#125;&#125; 3. mergeSort 1234567891011121314151617181920212223void mergeSort(int a[], int l, int r) &#123; // 8, 5, 4, 9, 2, 3, 6 if(l &gt;= r) return; // exit. int mid = (l+r) / 2; // overflow &lt;-&gt; l + (r-l)/2 mergeSort(a, l, mid); mergeSort(a, mid+1, r); int *arr = new int[r-l+1]; int k = 0; int i = l, j = mid + 1; while(i &lt;= mid &amp;&amp; j &lt;= r) &#123; if(a[i] &lt;= a[j]) &#123; arr[k++] = a[i++]; &#125; else &#123; arr[k++] = a[j++]; // ans += (mid-i+1); &#125; &#125; while(i &lt;= mid) arr[k++] = a[i++]; while(j &lt;= r) arr[k++] = a[j++]; for(int i = l; i &lt;= r; i++) &#123; a[i] = arr[i-l]; &#125; delete []arr;&#125; 4. insertSort 12345678910void insertSort(int a[], int len) &#123; int j; for(int i = 1; i &lt; len; i++) &#123;// 新抓的每张扑克牌 int temp = a[i]; for(j = i-1; a[j] &gt; temp &amp;&amp; j &gt;= 0; j--) &#123; a[j+1] = a[j]; &#125; a[j+1] = temp; &#125;&#125; 5. bubbleSort 1234567void bubbleSort(int a[], int len) &#123; for(int i = 1; i &lt; len; i++) &#123; for(int j = 0; j &lt; len-i; j++) &#123; if(a[j] &gt; a[j+1]) swap(a[j], a[j+1]); &#125; &#125;&#125; 6. selectSort 12345678910void selectSort(int a[], int len) &#123; int i, j, k; for(i = 0; i &lt; len-1; i++) &#123; k = i; for(j = i+1; j &lt; len; j++) &#123; if(a[j] &lt; a[k]) k = j; &#125; swap(a[i], a[k]); // 将第i位小的数放入i位置 &#125; &#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/tags/leetcode/"}]},{"title":"Minimum Spanning Tree","slug":"leetcode/acm/mst","date":"2011-10-27T02:28:21.000Z","updated":"2021-06-20T04:12:28.275Z","comments":true,"path":"2011/10/27/leetcode/acm/mst/","link":"","permalink":"http://www.iequa.com/2011/10/27/leetcode/acm/mst/","excerpt":"data structure - minimal spanning tree","text":"data structure - minimal spanning tree 1. prim http://acm.hdu.edu.cn/showproblem.php?pid=1233 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;using namespace std;const int INF = 0x7fffffff; // max int valueconst int N = 101;int map[N][N];int dis[N];bool vis[N];int prim(int n) &#123; memset(vis, false, sizeof(vis)); memset(dis, 0, sizeof(dis)); for(int i = 1; i &lt;= n; i++) &#123; if(map[1][i] != -1) &#123; dis[i] = map[1][i]; &#125; else dis[i] = INF; &#125; vis[1] = true; int sum = 0; for(int i = 1; i &lt;= n-1; i++) &#123; int minv = INF, c = 0; for(int j = 1; j &lt;= n; j++) &#123; if(!vis[j] &amp;&amp; dis[j] &lt; minv) &#123; minv = dis[j]; c = j; &#125; &#125; vis[c] = true; sum += minv; for(int j = 1; j &lt;= n; j++) &#123; if(!vis[j] &amp;&amp; map[c][j] != -1 &amp;&amp; map[c][j] &lt; dis[j]) &#123; dis[j] = map[c][j]; &#125; &#125; &#125; return sum;&#125;int main() &#123; int n, m; while(1 == scanf(&quot;%d&quot;, &amp;n) &amp;&amp; n != 0) &#123; memset(map, 255, sizeof(map)); int a, b, c; m = (n * (n-1)) / 2; for (int i = 0; i &lt; m; i++) &#123; scanf(&quot;%d%d%d&quot;, &amp;a, &amp;b, &amp;c); map[a][b] = map[b][a] = c; &#125; printf(&quot;%d\\n&quot;, prim(n)); &#125; return 0;&#125; 2. kruskal 123456789101112131415161718192021222324252627282930313233int pre[N];int n, m;struct Edge &#123; int u, v; int w;&#125;e[N];bool cmp(const Edge a, const Edge b) &#123; return a.w &lt; b.w;&#125;void make_set(int n) &#123; for(int i = 0; i &lt;= n; i++) pre[i] = i;&#125;int find_set(int a) &#123; if(pre[a] == a) return a; return pre[a] = find_set(pre[a]);&#125;void kruskal() &#123; int sum = 0; sort(e, e + m, cmp); make_set(n); for (int i = 0, fu, fv, cnt_e; i &lt; m; i++) &#123; fu = find_set(e[i].u); fv = find_set(e[i].v); if (fu != fv) &#123; sum += e[i].w; cnt_e++; if (cnt_e == n-1) break; pre[fv] = fu; &#125; &#125; cout &lt;&lt; sum &lt;&lt; endl;&#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"搜索的经典 POJ 2386 Lake Counting","slug":"leetcode/acm/dfs-POJ-2386","date":"2010-09-27T10:04:21.000Z","updated":"2021-06-20T04:12:28.278Z","comments":true,"path":"2010/09/27/leetcode/acm/dfs-POJ-2386/","link":"","permalink":"http://www.iequa.com/2010/09/27/leetcode/acm/dfs-POJ-2386/","excerpt":"经典的搜索，寻找水泡 poj 2386 Lake Counting","text":"经典的搜索，寻找水泡 poj 2386 Lake Counting Lake Counting 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;iostream&gt;using namespace std;const int MAX = 101;int asd[202];int n, m;int ans = 0;char c;int map[MAX][MAX];void digui(int a, int b, int k) &#123; if (a &gt;= 0 &amp; &amp; a &lt; n &amp; &amp; b &gt;= 0 &amp; &amp; b &lt; m &amp; &amp; map[a][b] == -2) &#123; map[a][b] = k; // 这是很重要的标记 digui(a + 1, b, k); digui(a, b + 1, k); digui(a - 1, b, k); digui(a, b - 1, k); digui(a + 1, b + 1, k); digui(a + 1, b - 1, k); digui(a - 1, b + 1, k); digui(a - 1, b - 1, k); &#125;&#125;int main() &#123; int i, j; cin &gt;&gt; n &gt;&gt; m; for (i = 0; i &lt; n; i++) &#123; for (j = 0; j &lt; m; j++)&#123; cin &gt;&gt; c; if (c == &#x27;W&#x27;) map[i][j] = -2; // 这的标记要慎重，最好用负数，不避免和K重复 else map[i][j] = 0; &#125; &#125; for (i = 0; i &lt; n; i++) &#123; for (j = 0; j &lt; m; j++) &#123; if (map[i][j] == -2) &#123; ans++; digui(i, j, ans); &#125; &#125; &#125; cout &lt;&lt; ans &lt;&lt; endl; return 0;&#125;","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"格式化输出 printf 与 输出流 cout","slug":"leetcode/acm/cout_vs_printf","date":"2010-09-25T11:04:21.000Z","updated":"2021-06-20T04:12:28.277Z","comments":true,"path":"2010/09/25/leetcode/acm/cout_vs_printf/","link":"","permalink":"http://www.iequa.com/2010/09/25/leetcode/acm/cout_vs_printf/","excerpt":"printf 是 格式化输出 ， cout 是输出流 今天无意中发现 cout 和 printf 是有点区别的，一个是输出语句，一个是输出函数","text":"printf 是 格式化输出 ， cout 是输出流 今天无意中发现 cout 和 printf 是有点区别的，一个是输出语句，一个是输出函数 demo 12345678910111213#include &lt;iostream.h&gt;#include &lt;stdio.h&gt;int main() &#123; for(int a=0; a&lt;24; a++) &#123; printf(&quot;++++++++++++\\n&quot;); cout &lt;&lt; &quot;============\\n&quot;; printf(&quot;############\\n&quot;); &#125; printf(&quot;@@@@@@@@@@@\\n&quot;);&#125; 运行结果如下： 12345678910111213++++++++++++++++++++++++########################@@@@@@@@@@@======================== 原因分析 cout 先是把输出结果存到缓存区，然后一次性输出，其实 cout 输出的时候也是调用了printf函数； pintf函数是每次输出结果; 这就是为了在 TIME程序中 cout运行的时间要比printf函数快的原因； 注意： cout&lt;&lt;&quot;\\n&quot;; 和 cout&lt;&lt;&quot;&quot;&lt;&lt;endl; 也有不同，\\n只是一个字符，而endl会将缓存区的数据全部输出并清零；在TIME程序中，如果采用cout&lt;&lt;&quot;============&quot;&lt;&lt;endl;结构反而会比printf慢，因为多了一道调用缓存手续 形象比喻 下面打个很形象的比喻，大家都会明白, 比如有100个鸡蛋，需要从A拿到B点，相当于程序中的输出: printf把鸡蛋一次一个用手拿到B点 cout&lt;&lt;&quot; \\n&quot; 先把鸡蛋全部放到篮子，然后一次性拿过去取出 cout&lt;&lt;&quot; &quot;&lt;&lt;endl 先把鸡蛋放到篮子，然后一次一个拿过去再取出 尤其可见哪个快那个慢显而易见","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]},{"title":"格式化输入 scanf 与 输入流 cin","slug":"leetcode/acm/cin_vs_scanf","date":"2010-09-24T10:04:21.000Z","updated":"2021-06-20T04:12:28.277Z","comments":true,"path":"2010/09/24/leetcode/acm/cin_vs_scanf/","link":"","permalink":"http://www.iequa.com/2010/09/24/leetcode/acm/cin_vs_scanf/","excerpt":"scanf 是格式化输入，printf是格式化输出。 cin是输入流，cout是输出流。效率稍低，但书写简便。","text":"scanf 是格式化输入，printf是格式化输出。 cin是输入流，cout是输出流。效率稍低，但书写简便。 格式化输出 与 流输出 格式化输出效率比较高，但是写代码麻烦。 流输出操作效率稍低，但书写简便。 cout之所以效率低，是先把要输出的东西存入缓冲区，再输出，导致效率降低。 缓冲区比较抽象，举个例子吧： 1234int i;cout &lt;&lt; &#x27;a&#x27;;cin &gt;&gt; i;cout &lt;&lt; &#x27;b&#x27;; 运行结果什么都没看到输出，输入一个整型比如3再按回车后ab同时显示出来了。 但是这样的情况并不是经常发生，是在一些比较大型的工程中偶尔出现，原因是字符a先到了缓冲区，但是没输出，等输入了i，b进入缓冲区后再一并输出的。流输入也是差不多的。 C++ 中 iostream.h 与 stdio.h iostream.h 和 stdio.h 是 C++ 的两个头文件，里面是对于一些类，操作符，函数的定义，头文件本身没有好坏之分吧~~ 只是发展到C++，都比较提倡用iostream.h罢了，因为这样写代码简单。","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"}],"tags":[{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"}]}],"categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/categories/leetcode/"},{"name":"data-warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/categories/data-warehouse/"},{"name":"hive","slug":"data-warehouse/hive","permalink":"http://www.iequa.com/categories/data-warehouse/hive/"},{"name":"data-analysis","slug":"data-analysis","permalink":"http://www.iequa.com/categories/data-analysis/"},{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/categories/spark/"},{"name":"python","slug":"python","permalink":"http://www.iequa.com/categories/python/"},{"name":"world","slug":"world","permalink":"http://www.iequa.com/categories/world/"},{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/categories/hadoop/"},{"name":"tools","slug":"tools","permalink":"http://www.iequa.com/categories/tools/"},{"name":"English","slug":"English","permalink":"http://www.iequa.com/categories/English/"},{"name":"IELTS","slug":"English/IELTS","permalink":"http://www.iequa.com/categories/English/IELTS/"},{"name":"java","slug":"java","permalink":"http://www.iequa.com/categories/java/"},{"name":"nlp","slug":"nlp","permalink":"http://www.iequa.com/categories/nlp/"},{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/categories/machine-learning/"},{"name":"devops","slug":"devops","permalink":"http://www.iequa.com/categories/devops/"},{"name":"elastic","slug":"elastic","permalink":"http://www.iequa.com/categories/elastic/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/categories/tensorflow/"},{"name":"data-science","slug":"data-science","permalink":"http://www.iequa.com/categories/data-science/"},{"name":"deeplearning","slug":"deeplearning","permalink":"http://www.iequa.com/categories/deeplearning/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.iequa.com/tags/leetcode/"},{"name":"Hive","slug":"Hive","permalink":"http://www.iequa.com/tags/Hive/"},{"name":"DWH","slug":"DWH","permalink":"http://www.iequa.com/tags/DWH/"},{"name":"SQL","slug":"SQL","permalink":"http://www.iequa.com/tags/SQL/"},{"name":"data warehouse","slug":"data-warehouse","permalink":"http://www.iequa.com/tags/data-warehouse/"},{"name":"data-analysis","slug":"data-analysis","permalink":"http://www.iequa.com/tags/data-analysis/"},{"name":"sparkSQL","slug":"sparkSQL","permalink":"http://www.iequa.com/tags/sparkSQL/"},{"name":"spark","slug":"spark","permalink":"http://www.iequa.com/tags/spark/"},{"name":"Airflow","slug":"Airflow","permalink":"http://www.iequa.com/tags/Airflow/"},{"name":"getattr","slug":"getattr","permalink":"http://www.iequa.com/tags/getattr/"},{"name":"interview","slug":"interview","permalink":"http://www.iequa.com/tags/interview/"},{"name":"NewZealand","slug":"NewZealand","permalink":"http://www.iequa.com/tags/NewZealand/"},{"name":"hdfs","slug":"hdfs","permalink":"http://www.iequa.com/tags/hdfs/"},{"name":"LoseWeight","slug":"LoseWeight","permalink":"http://www.iequa.com/tags/LoseWeight/"},{"name":"hive","slug":"hive","permalink":"http://www.iequa.com/tags/hive/"},{"name":"python","slug":"python","permalink":"http://www.iequa.com/tags/python/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://www.iequa.com/tags/MapReduce/"},{"name":"IELTS","slug":"IELTS","permalink":"http://www.iequa.com/tags/IELTS/"},{"name":"java","slug":"java","permalink":"http://www.iequa.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://www.iequa.com/tags/jvm/"},{"name":"BERT","slug":"BERT","permalink":"http://www.iequa.com/tags/BERT/"},{"name":"PCA","slug":"PCA","permalink":"http://www.iequa.com/tags/PCA/"},{"name":"Vietnam","slug":"Vietnam","permalink":"http://www.iequa.com/tags/Vietnam/"},{"name":"pyCharm","slug":"pyCharm","permalink":"http://www.iequa.com/tags/pyCharm/"},{"name":"scikit-learn","slug":"scikit-learn","permalink":"http://www.iequa.com/tags/scikit-learn/"},{"name":"Malaysia","slug":"Malaysia","permalink":"http://www.iequa.com/tags/Malaysia/"},{"name":"Systemd","slug":"Systemd","permalink":"http://www.iequa.com/tags/Systemd/"},{"name":"Anaconda","slug":"Anaconda","permalink":"http://www.iequa.com/tags/Anaconda/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.iequa.com/tags/ElasticSearch/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.iequa.com/tags/Kubernetes/"},{"name":"Stack","slug":"Stack","permalink":"http://www.iequa.com/tags/Stack/"},{"name":"Fluentd","slug":"Fluentd","permalink":"http://www.iequa.com/tags/Fluentd/"},{"name":"developer","slug":"developer","permalink":"http://www.iequa.com/tags/developer/"},{"name":"python3","slug":"python3","permalink":"http://www.iequa.com/tags/python3/"},{"name":"Docker","slug":"Docker","permalink":"http://www.iequa.com/tags/Docker/"},{"name":"WSGI","slug":"WSGI","permalink":"http://www.iequa.com/tags/WSGI/"},{"name":"Singapore","slug":"Singapore","permalink":"http://www.iequa.com/tags/Singapore/"},{"name":"dynamic programming","slug":"dynamic-programming","permalink":"http://www.iequa.com/tags/dynamic-programming/"},{"name":"keras","slug":"keras","permalink":"http://www.iequa.com/tags/keras/"},{"name":"CNN","slug":"CNN","permalink":"http://www.iequa.com/tags/CNN/"},{"name":"Japan","slug":"Japan","permalink":"http://www.iequa.com/tags/Japan/"},{"name":"estimation","slug":"estimation","permalink":"http://www.iequa.com/tags/estimation/"},{"name":"Sentiment","slug":"Sentiment","permalink":"http://www.iequa.com/tags/Sentiment/"},{"name":"Credit-Score","slug":"Credit-Score","permalink":"http://www.iequa.com/tags/Credit-Score/"},{"name":"Chatbot","slug":"Chatbot","permalink":"http://www.iequa.com/tags/Chatbot/"},{"name":"Seq2Seq","slug":"Seq2Seq","permalink":"http://www.iequa.com/tags/Seq2Seq/"},{"name":"PPL","slug":"PPL","permalink":"http://www.iequa.com/tags/PPL/"},{"name":"RNN","slug":"RNN","permalink":"http://www.iequa.com/tags/RNN/"},{"name":"MLP","slug":"MLP","permalink":"http://www.iequa.com/tags/MLP/"},{"name":"SVM","slug":"SVM","permalink":"http://www.iequa.com/tags/SVM/"},{"name":"L1-L2","slug":"L1-L2","permalink":"http://www.iequa.com/tags/L1-L2/"},{"name":"entropy","slug":"entropy","permalink":"http://www.iequa.com/tags/entropy/"},{"name":"Metric","slug":"Metric","permalink":"http://www.iequa.com/tags/Metric/"},{"name":"Rondom_Forest","slug":"Rondom-Forest","permalink":"http://www.iequa.com/tags/Rondom-Forest/"},{"name":"Logistic_Regression","slug":"Logistic-Regression","permalink":"http://www.iequa.com/tags/Logistic-Regression/"},{"name":"Taiwan","slug":"Taiwan","permalink":"http://www.iequa.com/tags/Taiwan/"},{"name":"fastText","slug":"fastText","permalink":"http://www.iequa.com/tags/fastText/"},{"name":"Kmeans","slug":"Kmeans","permalink":"http://www.iequa.com/tags/Kmeans/"},{"name":"TextCNN","slug":"TextCNN","permalink":"http://www.iequa.com/tags/TextCNN/"},{"name":"sequence_loss","slug":"sequence-loss","permalink":"http://www.iequa.com/tags/sequence-loss/"},{"name":"body","slug":"body","permalink":"http://www.iequa.com/tags/body/"},{"name":"Attention","slug":"Attention","permalink":"http://www.iequa.com/tags/Attention/"},{"name":"Glove","slug":"Glove","permalink":"http://www.iequa.com/tags/Glove/"},{"name":"CDH","slug":"CDH","permalink":"http://www.iequa.com/tags/CDH/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://www.iequa.com/tags/tensorflow/"},{"name":"numpy","slug":"numpy","permalink":"http://www.iequa.com/tags/numpy/"},{"name":"newaxis","slug":"newaxis","permalink":"http://www.iequa.com/tags/newaxis/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://www.iequa.com/tags/deeplearning-ai/"},{"name":"CS231n","slug":"CS231n","permalink":"http://www.iequa.com/tags/CS231n/"},{"name":"NLTK","slug":"NLTK","permalink":"http://www.iequa.com/tags/NLTK/"},{"name":"SkinCare","slug":"SkinCare","permalink":"http://www.iequa.com/tags/SkinCare/"},{"name":"Ensumble","slug":"Ensumble","permalink":"http://www.iequa.com/tags/Ensumble/"},{"name":"decision-tree","slug":"decision-tree","permalink":"http://www.iequa.com/tags/decision-tree/"},{"name":"machine-learning","slug":"machine-learning","permalink":"http://www.iequa.com/tags/machine-learning/"},{"name":"ITFIN","slug":"ITFIN","permalink":"http://www.iequa.com/tags/ITFIN/"},{"name":"Ensemble","slug":"Ensemble","permalink":"http://www.iequa.com/tags/Ensemble/"},{"name":"Cindy-English","slug":"Cindy-English","permalink":"http://www.iequa.com/tags/Cindy-English/"},{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://www.iequa.com/tags/Matplotlib/"},{"name":"Sklearn","slug":"Sklearn","permalink":"http://www.iequa.com/tags/Sklearn/"},{"name":"Pandas","slug":"Pandas","permalink":"http://www.iequa.com/tags/Pandas/"},{"name":"Numpy","slug":"Numpy","permalink":"http://www.iequa.com/tags/Numpy/"},{"name":"Anaconda3","slug":"Anaconda3","permalink":"http://www.iequa.com/tags/Anaconda3/"},{"name":"mysql","slug":"mysql","permalink":"http://www.iequa.com/tags/mysql/"},{"name":"seq2seq+Attention","slug":"seq2seq-Attention","permalink":"http://www.iequa.com/tags/seq2seq-Attention/"},{"name":"banwagon","slug":"banwagon","permalink":"http://www.iequa.com/tags/banwagon/"},{"name":"word2vec","slug":"word2vec","permalink":"http://www.iequa.com/tags/word2vec/"},{"name":"Guitar","slug":"Guitar","permalink":"http://www.iequa.com/tags/Guitar/"},{"name":"linux","slug":"linux","permalink":"http://www.iequa.com/tags/linux/"},{"name":"zshrc","slug":"zshrc","permalink":"http://www.iequa.com/tags/zshrc/"},{"name":"Shadowsocks","slug":"Shadowsocks","permalink":"http://www.iequa.com/tags/Shadowsocks/"},{"name":"mac","slug":"mac","permalink":"http://www.iequa.com/tags/mac/"},{"name":"help","slug":"help","permalink":"http://www.iequa.com/tags/help/"},{"name":"macos","slug":"macos","permalink":"http://www.iequa.com/tags/macos/"},{"name":"vimrc","slug":"vimrc","permalink":"http://www.iequa.com/tags/vimrc/"},{"name":"jieba","slug":"jieba","permalink":"http://www.iequa.com/tags/jieba/"},{"name":"Word2Vec","slug":"Word2Vec","permalink":"http://www.iequa.com/tags/Word2Vec/"},{"name":"Shuping Yang","slug":"Shuping-Yang","permalink":"http://www.iequa.com/tags/Shuping-Yang/"},{"name":"ALS","slug":"ALS","permalink":"http://www.iequa.com/tags/ALS/"},{"name":"sina","slug":"sina","permalink":"http://www.iequa.com/tags/sina/"},{"name":"RecommendationSystem","slug":"RecommendationSystem","permalink":"http://www.iequa.com/tags/RecommendationSystem/"},{"name":"recommendation-system","slug":"recommendation-system","permalink":"http://www.iequa.com/tags/recommendation-system/"},{"name":"spring","slug":"spring","permalink":"http://www.iequa.com/tags/spring/"},{"name":"Chinese's name","slug":"Chinese-s-name","permalink":"http://www.iequa.com/tags/Chinese-s-name/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://www.iequa.com/tags/elasticsearch/"},{"name":"math","slug":"math","permalink":"http://www.iequa.com/tags/math/"},{"name":"scala","slug":"scala","permalink":"http://www.iequa.com/tags/scala/"},{"name":"sbt","slug":"sbt","permalink":"http://www.iequa.com/tags/sbt/"},{"name":"sqoop","slug":"sqoop","permalink":"http://www.iequa.com/tags/sqoop/"},{"name":"ETL","slug":"ETL","permalink":"http://www.iequa.com/tags/ETL/"},{"name":"logback","slug":"logback","permalink":"http://www.iequa.com/tags/logback/"},{"name":"work-cmd","slug":"work-cmd","permalink":"http://www.iequa.com/tags/work-cmd/"},{"name":"hadoop","slug":"hadoop","permalink":"http://www.iequa.com/tags/hadoop/"},{"name":"bashrc","slug":"bashrc","permalink":"http://www.iequa.com/tags/bashrc/"},{"name":"health","slug":"health","permalink":"http://www.iequa.com/tags/health/"},{"name":"linuxbird","slug":"linuxbird","permalink":"http://www.iequa.com/tags/linuxbird/"},{"name":"acm","slug":"acm","permalink":"http://www.iequa.com/tags/acm/"},{"name":"life","slug":"life","permalink":"http://www.iequa.com/tags/life/"},{"name":"Linux_C","slug":"Linux-C","permalink":"http://www.iequa.com/tags/Linux-C/"},{"name":"g++","slug":"g","permalink":"http://www.iequa.com/tags/g/"},{"name":"memory","slug":"memory","permalink":"http://www.iequa.com/tags/memory/"},{"name":"Chicken-soup","slug":"Chicken-soup","permalink":"http://www.iequa.com/tags/Chicken-soup/"}]}