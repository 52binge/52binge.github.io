<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="你可能想不到，Machine Learning and Logic Programming 有一种奇妙的关系，她们就像亲姐妹. Machine Learning and Logic Programming   逻辑编程是什么 （X + 2 = 5 询问 X 的值，回答： X=3）类比 forward pass + back propagation ML框架 其实是一种 程序语言 : differe">
<meta property="og:type" content="website">
<meta property="og:title" content="Auckland New Zealand">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;ai1&#x2F;index2.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:description" content="你可能想不到，Machine Learning and Logic Programming 有一种奇妙的关系，她们就像亲姐妹. Machine Learning and Logic Programming   逻辑编程是什么 （X + 2 = 5 询问 X 的值，回答： X=3）类比 forward pass + back propagation ML框架 其实是一种 程序语言 : differe">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;ai&#x2F;nlp-1.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;pic1.zhimg.com&#x2F;80&#x2F;v2-004df09bcc2f085c72cc0938c08b1910_hd.jpg">
<meta property="og:updated_time" content="2019-10-20T04:30:35.089Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;ai&#x2F;nlp-1.jpg">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<!-- jiangting add start... @2020.08.30 -->
<!-- <div id="menu" class="duration-main" style="background-color: #e7e7e7"> -->
<!--   <p class="links-p" href="/">Blair</p> -->
<!--   <address class="icons"> -->
<!--     <a href="https://github.com/blair101" class="icon-font icon linkedin" target="_blank"></a> -->
<!--     <a href="https://cn.linkedin.com/pub/tianyu-dai/a8/818/44a" class="icon-font icon linkedin" target="_blank"></a> -->
<!--   </address> -->
<!--   <div class="hr1"></div> -->
<!--   <nav> -->
<!--     <div> -->
<!--     <a class="home" href="/">Home</a></div> -->
<!--     <p class="links-p" href="/">Home</p> -->
<!--     <nav class="tag-ath"> -->
<!--       <a class="proj" href="/categories">Category</a> -->
<!--       <a class="authors" href="/about">About</a> -->
<!--     </nav> -->
<!--   </nav> -->
<!--   <div class="hr2"></div> -->
<!--     <p class="links-p">Links</p> -->
<!--       <address class="links"> -->
<!--       <a class="proj" href="/article/Create-MyWorld">Projects</a> -->
<!--       <a class="friend">Friends</a></address><div class="hr3"> -->
<!--   </div> -->
<!--   <p class="end"></p> -->
<!--   <div id="menu-links" class="duration-main" style="top: -400px; background-color: #f5f5f5"> -->
<!--     <address> -->
<!--       <li><a target="_blank" href="http://lm7.xxxxxxxx.jp">Lm7</a></li> -->
<!--       <li><a target="_blank" href="http://www.pixiv.net/member.php?id=4933015">Domik</a></li> -->
<!--       <li><a target="_blank" href="http://hana-ui.moe">hana-ui</a></li> -->
<!--       <li><a target="_blank" href="http://fil.dtysky.moe">F-I-L</a></li> -->
<!--       <li><a target="_blank" href="http://paradise.dtysky.moe">Paradise</a></li> -->
<!--       <li><a target="_blank" href="http://moe-notes.dtysky.moe">MoeNotes</a></li> -->
<!--       <li><a target="_blank" href="http://kanata.dtysky.moe">Kanata</a></li> -->
<!--       <li><a target="_blank" href="http://blog.nekohand.moe">Nekohand</a></li> -->
<!--       <li><a target="_blank" href="http://www.jerryfu.net">JerryFu</a></li> -->
<!--       <li><a target="_blank" href="http://kawabangga.com">南史</a></li> -->
<!--       <li><a>Hide Links</a></li> -->
<!--       <li><a></a></li> -->
<!--     </address> -->
<!--   </div> -->
<!-- </div> -->
<!-- jiangting add end !  @2020.08.30 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/leetcode">LC</a>
        
          <a class="main-nav-link" href="/ai1">AI</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="page-" class="article article-type-page" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    <div class="article-meta">
      <!--<a href="/ai1/index2.html" class="article-date">
  <time datetime="2019-10-20T04:30:35.090Z" itemprop="datePublished">2019-10-20</time>
</a>-->
      <!-- 
--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/ai1/index2.html#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>你可能想不到，Machine Learning and Logic Programming 有一种奇妙的关系，她们就像亲姐妹.</p>
<p><strong>Machine Learning and Logic Programming</strong></p>
<blockquote>
<ol>
<li>逻辑编程是什么 （X + 2 = 5 询问 X 的值，回答： X=3）类比 <strong>forward pass + back propagation</strong></li>
<li><strong>ML框架</strong> 其实是一种 <strong>程序语言</strong> : differentiable programming language</li>
</ol>
<p>&nbsp;&nbsp; Pytorch 和 TensorFlow 的动态计算图和静态计算图. 对应动态静态语言。</p>
<p><strong>简单表达 &amp; 递归：</strong></p>
<ol>
<li>Feed-forward 网络，比如 CNN 一类的，对应了最简单的表达式，它只能处理图像一类具有固定长度的数据。</li>
<li>RNN（LSTM）对应的是单个递归（循环）的函数。这就是为什么 RNN 可以处理文本这类线性“链表”数据。</li>
</ol>
<p><a href="http://www.yinwang.org/blog-cn/2019/01/30/machine-learning" target="_blank" rel="noopener">machine learning and logic programming 机器学习与逻辑编程</a></p>
</blockquote>
<p><strong>NLP 发展方向</strong></p>
<p>未来十年将是 NLP 发展的黄金档：</p>
<blockquote>
<ol>
<li>来自各个行业的文本大数据将会更好地采集、加工、入库。</li>
<li>来自 搜索引擎、客服、商业智能、语音助手、翻译、教育 等领域对 NLP 的需求会大幅度上升。</li>
<li>文本数据和语音、图像数据的多模态融合成为未来机器人的刚需。</li>
</ol>
</blockquote>
<p>因此，NLP 研究将会向如下几个方面倾斜：</p>
<blockquote>
<ol>
<li>将知识和常识引入目前基于数据的学习系统中。</li>
<li>低资源的NLP任务的学习方法。</li>
<li>上下文建模、多轮语义理解。</li>
<li>基于语义分析、知识和常识的可解释 NLP。</li>
</ol>
</blockquote>
<p><img src="/images/ai/nlp-1.jpg" alt="趋势热点：值得关注的NLP技术"></p>
<p>除了备受关注的 NN Pre-Train 外，知识和常识的引入将大幅推动NLP技术的发展：</p>
<blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/53794989" target="_blank" rel="noopener">2019：迈向高能NLP之路！</a></li>
<li><a href="https://zhuanlan.zhihu.com/c_1064159241216102400" target="_blank" rel="noopener">高能NLP之路的专栏</a></li>
<li><a href="https://www.msra.cn/zh-cn/news/executivebylines/tech-bylines-nlp" target="_blank" rel="noopener">NLP将迎来黄金十年 from MSRA</a> 通篇似乎都在强调知识和常识引入对NLP乃至整个AI的重要性。</li>
</ul>
</blockquote>
<p><strong>2018 美团技术团队发表两篇重磅级文章：</strong></p>
<ul>
<li><a href="https://tech.meituan.com/2018/11/22/meituan-brain-nlp-01.html" target="_blank" rel="noopener">美团餐饮娱乐知识图谱——美团大脑揭秘</a></li>
<li><a href="https://tech.meituan.com/2018/11/01/meituan-ai-nlp.html" target="_blank" rel="noopener">美团大脑：知识图谱的建模方法及其应用</a></li>
</ul>
<blockquote>
<p>人工智能背后两大技术驱动力：<strong>深度学习和知识图谱</strong>，知识图谱就是人工智能的基础。</p>
</blockquote>
<h2 id="1-AI-算法基础"><a href="#1-AI-算法基础" class="headerlink" title="1. AI 算法基础"></a>1. AI 算法基础</h2><h3 id="1-1-防止-overfiting-的-8-条"><a href="#1-1-防止-overfiting-的-8-条" class="headerlink" title="1.1 防止 overfiting 的 8 条"></a>1.1 防止 overfiting 的 8 条</h3><blockquote>
<p>1). get more data<br>2). Data augmentation<br>3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验)<br>4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；)<br>5). Choosing Right Network Structure<br>6). Early stopping<br>7). Model Ensumble<br>8). Batch Normalization</p>
<p><a href="https://zhuanlan.zhihu.com/p/38176412" target="_blank" rel="noopener">张俊林 - Batch Normalization导读</a> 、 <a href="https://zhuanlan.zhihu.com/p/43200897" target="_blank" rel="noopener">张俊林 - 深度学习中的Normalization模型</a></p>
<p><code>Internal Covariate Shift</code> &amp; Independent and identically distributed，缩写为 <code>IID</code></p>
<p>Batch Normalization 可以有效避免复杂参数对网络训练产生的影响，也可提高泛化能力.</p>
<p>神经网路的训练过程的本质是学习数据分布，如果训练数据与测试数据分布不同，将大大降低网络泛化能力， BN 是针对每一批数据，在网络的每一层输入之前增加 BN，(均值0，标准差1)。</p>
<p>Dropout 可以抑制过拟合，作用于每份小批量的训练数据，随机丢弃部分神经元机制. bagging 原理.</p>
<p><a href="https://posts.careerengine.us/p/5cae13b2d401440a7fe047af" target="_blank" rel="noopener">ML算法： 关于防止过拟合，整理了 8 条迭代方向</a></p>
</blockquote>
<h3 id="1-2-机器学习之类别不平衡问题"><a href="#1-2-机器学习之类别不平衡问题" class="headerlink" title="1.2 机器学习之类别不平衡问题"></a>1.2 机器学习之类别不平衡问题</h3><p>机器学习之类别不平衡问题 (1) —— 各种评估指标</p>
<blockquote>
<p>1）过采样和欠采样；（1. 随机过采样(不太使用了，重采样往往会导致严重的过拟合)， 2. <strong>Border-line SMOTE</strong>）<br>2）修改权重（修改损失函数）；<br>3）集成方法：bagging，类似随机森林、自助采样；<br>4）多任务联合学习；</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/34473430" target="_blank" rel="noopener">机器学习之类别不平衡问题 (1) —— 各种评估指标</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34655990" target="_blank" rel="noopener">机器学习之类别不平衡问题 (2) —— ROC和PR曲线</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/41237940" target="_blank" rel="noopener">机器学习之类别不平衡问题 (3) —— 采样方法</a></li>
</ul>
</blockquote>
<h3 id="1-6-Linear-vs-Nonlinear-classifier"><a href="#1-6-Linear-vs-Nonlinear-classifier" class="headerlink" title="1.6 Linear vs Nonlinear classifier"></a>1.6 Linear vs Nonlinear classifier</h3><blockquote>
<p>线性和非线性是针对，模型参数和输入特征来讲的；</p>
<p>比如输入x，模型 y=ax+ax^2 那么就是 nonlinear model 如果输入是x和X^2则模型是线性的。</p>
<ol>
<li>Linear classifier 可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。</li>
<li>nonlinear classifier 拟合能力较强，不足之处是数据量不足容易 <strong>overfiting</strong> 、计算复杂度高、解释性不好。</li>
</ol>
<p>Linear classifier ：LR,贝叶斯分类，单层感知机、线性回归</p>
<p>nonlinear classifier：决策树、RF、GBDT、多层感知机</p>
<p>SVM 两种都有（看线性核还是高斯核）</p>
</blockquote>
<h2 id="2-NLP高频问题"><a href="#2-NLP高频问题" class="headerlink" title="2. NLP高频问题"></a>2. NLP高频问题</h2><p><a href="https://www.infoq.cn/article/66vicQt*GTIFy33B4mu9" target="_blank" rel="noopener">NLP 神经网络发展历史中最重要的 8 个里程碑</a></p>
<blockquote>
<ol>
<li><p>Language Model (语言模型就是要看到上文预测下文, So NNLM)</p>
</li>
<li><p>n-gram model（n元模型）（基于 马尔可夫假设 思想）</p>
</li>
<li><p>2001 - <strong>NNLM</strong> , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。</p>
</li>
<li><p>2008 - Multi-task learning</p>
</li>
<li><p>2013 - Word2Vec (Word Embedding的工具word2vec : CBOW 和 Skip-gram)</p>
</li>
<li><p>2014 - Glove</p>
</li>
<li><p>2014 - sequence-to-sequence</p>
</li>
<li><p>2015 - Attention</p>
</li>
<li><p>2015 - Memory-based networks</p>
</li>
<li><p>2017 - fastText</p>
</li>
<li><p>2018 - Pretrained language models</p>
</li>
</ol>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/29488930" target="_blank" rel="noopener">Word2Vec介绍: 为什么使用负采样（negtive sample）？</a></p>
<blockquote>
<p>1）其本质都可以看作是 Language Model；</p>
<p>2）词向量只不过 NNLM 一个产物， word2vec 虽然其本质也是 Language Model，但是其专注于<strong>词向量本身</strong>，因此做了许多优化来提高计算效率：</p>
<p>与 NNLM 相比，词向量直接sum，不再拼接，并舍弃隐层；<br>考虑到 sofmax归一化 需要遍历整个词汇表，采用 hierarchical softmax 和 negative sampling 进行优化</p>
<ol>
<li>hierarchical softmax 实质上生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；</li>
<li>negative sampling 更为直接，实质上对每一个样本中每一个词都进行负例采样；</li>
</ol>
</blockquote>
<p><strong>Hierarchical Softmax 缺点:</strong></p>
<blockquote>
<p>如果我们的训练样本里的中心词w是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了.</p>
</blockquote>
<h3 id="2-2-negative-sampling"><a href="#2-2-negative-sampling" class="headerlink" title="2.2 negative sampling"></a>2.2 negative sampling</h3><blockquote>
<p>1）如果通过一个正例和neg个负例进行二元逻辑回归呢？ 2） 如何进行负采样呢？</p>
</blockquote>
<p>负采样这个点引入 word2vec 非常巧妙，两个作用，</p>
<blockquote>
<ol>
<li>加速了模型计算</li>
<li>保证了模型训练的效果</li>
</ol>
<p>第一，model 每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢。</p>
<p>第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点聪明.</p>
</blockquote>
<ul>
<li><a href="https://www.cnblogs.com/pinard/p/7249903.html" target="_blank" rel="noopener">good good, word2vec Negative Sampling 刘建平Pinard</a></li>
</ul>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/46430775" target="_blank" rel="noopener">知乎: 哈夫曼树</a></p>
<p>给定n权值作为n个叶子节点，构造一棵二叉树，若这棵二叉树的带权路径长度达到最小，则称这样的二叉树为最优二叉树，也称为Huffman树。</p>
</blockquote>
<h3 id="2-3-word2vec-vs-fastText"><a href="#2-3-word2vec-vs-fastText" class="headerlink" title="2.3 word2vec vs fastText"></a>2.3 word2vec vs fastText</h3><blockquote>
<ol>
<li>都可以无监督学习词向量， fastText 训练词向量时会考虑 subword；</li>
<li>fastText 还可以进行有监督学习进行文本分类</li>
</ol>
</blockquote>
<p>其主要特点：</p>
<blockquote>
<ul>
<li>结构与CBOW类似，但学习目标是人工标注的分类结果；</li>
<li>采用 hierarchical softmax 对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径；</li>
<li>引入 N-gram，考虑词序特征；</li>
<li>引入 subword 来处理长词，处理未登陆词问题；</li>
</ul>
</blockquote>
<h3 id="2-4-word2vec-vs-glove"><a href="#2-4-word2vec-vs-glove" class="headerlink" title="2.4 word2vec vs glove"></a>2.4 word2vec vs glove</h3><ol>
<li>目标函数不同 （crossentrpy vs 平方损失函数）</li>
<li>glove 全局统计固定语料信息</li>
</ol>
<blockquote>
<ul>
<li>word2vec 是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。</li>
</ul>
<ul>
<li>word2vec 是无监督学习，同样由于不需要人工标注；glove通常被认为是无监督学习，但实际上glove还是有label的，即共现次数log(X_{ij})。</li>
</ul>
<ul>
<li>word2vec 损失函数实质上是带权重的<strong>crossentrpy</strong>，权重固定；glove的损失函数是最小<strong>平方损失函数</strong>，权重可以做映射变换。</li>
</ul>
<ul>
<li>总体来看，glove 可以被看作是更换了目标函数和权重函数的全局 word2vec。</li>
</ul>
</blockquote>
<h3 id="2-5-ELMO-vs-GPT-vs-BERT"><a href="#2-5-ELMO-vs-GPT-vs-BERT" class="headerlink" title="2.5 ELMO vs GPT vs BERT"></a>2.5 ELMO vs GPT vs BERT</h3><p><img src="https://pic1.zhimg.com/80/v2-004df09bcc2f085c72cc0938c08b1910_hd.jpg" alt=""></p>
<p>之前介绍词向量均是静态的词向量，无法解决一次多义等问题。下面介绍三种elmo、GPT、bert词向量，它们都是基于语言模型的动态词向量。下面从几个方面对这三者进行对比：</p>
<p>（1）<strong>特征提取器</strong>：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。</p>
<p>（2）<strong>单/双向语言模型</strong>：</p>
<p>GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。<br>GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子。</p>
<p><strong>1. 进退维谷的 RNN</strong></p>
<blockquote>
<ol>
<li>RNN (包括LSTM、GRU + Attention) 效果与 Transformer 差距很明显</li>
<li>RNN 很难并行计算。 由于 RNN 特点 ： 线形序列收集前面的信息。</li>
</ol>
<p>一个严重阻碍RNN将来继续走红的问题是：RNN本身的序列依赖结构对于大规模并行计算来说相当之不友好。通俗点说，就是RNN很难具备高效的并行计算能力，这个乍一看好像不是太大的问题，其实问题很严重。</p>
<p>对于小数据集 RNN 可能速度更快些， Transformer 慢些， 但是可以改进 Transformer 缓解：</p>
<ol>
<li>可把Block数目降低，减少参数量；</li>
<li>引入Bert两阶段训练模型，那么对于小数据集合来说会极大缓解效果问题。</li>
</ol>
</blockquote>
<p><strong>2. 一希尚存的 CNN</strong></p>
<blockquote>
<ol>
<li>CNN 天生自带的高并行计算能力</li>
<li>一些深度网络的优化trick，CNN在NLP领域里的深度逐步能做起来了。dilated CNN</li>
</ol>
<p>早期CNN做不好NLP的一个很大原因是网络深度做不起来。 原生的CNN在很多方面仍然是比不过Transformer的，典型的还是长距离特征捕获能力方面，而这点在NLP界算是比较严重的缺陷。</p>
<p>对于远距离特征，单层怀旧版CNN是无法捕获到的，如果滑动窗口k最大为2，而如果有个远距离特征距离是5，那么无论上多少个卷积核，都无法覆盖到长度为5的距离的输入，所以它是无法捕获长距离特征的</p>
<p>滑动窗口从左到右滑动，捕获到的特征也是如此顺序排列，所以它在结构上已经记录了相对位置信息了。但是如果卷积层后面立即接上Pooling层的话，Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。所以在NLP领域里，目前CNN的一个发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度。</p>
<p>怀旧版 CNN模型 一直处于被 RNN模型 压制到抑郁症早期的尴尬局面。</p>
<p><strong>CNN的进化</strong>：物竞天择的模型斗兽场</p>
<p>摩登CNN（使用Skip Connection来辅助优化）、Dilated CNN </p>
<p>想方设法把CNN的深度做起来，随着深度的增加，很多看似无关的问题就随之解决了。</p>
</blockquote>
<p><strong>3. Transformer</strong></p>
<p>Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。</p>
<blockquote>
<p>Transformer作为新模型，并不是完美无缺的。它也有明显的缺点：首先，对于长输入的任务，典型的比如篇章级别的任务（例如文本摘要），因为任务的输入太长，Transformer会有巨大的计算复杂度，导致速度会急剧变慢。</p>
</blockquote>
<h3 id="2-8-Attention"><a href="#2-8-Attention" class="headerlink" title="2.8 Attention"></a>2.8 Attention</h3><p>除此之外模型为了取得比较好的效果还是用了下面三个小技巧来改善性能：</p>
<blockquote>
<p>深层次的LSTM：作者使用了4层LSTM作为encoder和decoder模型，并且表示深层次的模型比shallow的模型效果要好（单层，神经元个数多）。</p>
<p>将source进行反序输入：输入的时候将“ABC”变成“CBA”，这样做的好处是解决了长序列的long-term依赖，使得模型可以学习到更多的对应关系，从而达到比较好的效果。</p>
<p>Beam Search：这是在test时的技巧，也就是在训练过程中不会使用。</p>
<p>一般来讲我们会采用greedy贪婪式的序列生成方法，也就是每一步都取概率最大的元素作为当前输出，但是这样的缺点就是一旦某一个输出选错了，可能就会导致最终结果出错，所以使用beam search的方法来改善。也就是每一步都取概率最大的k个序列（beam size），并作为下一次的输入。更详细的解释和例子可以参考下面这个链接：<a href="https://zhuanlan.zhihu.com/p/28048246" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28048246</a></p>
</blockquote>
<h3 id="2-9-文本分类任务-tricks"><a href="#2-9-文本分类任务-tricks" class="headerlink" title="2.9 文本分类任务 tricks"></a>2.9 文本分类任务 tricks</h3><p>在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的tricks？</p>
<blockquote>
<ol>
<li>数据预处理时vocab的选取（前N个高频词或者过滤掉出现次数小于3的词等等）</li>
<li>词向量的选择，可以使用预训练好的词向量如谷歌、facebook开源出来的，当训练集比较大的时候也可以进行微调或者随机初始化与训练同时进行。训练集较小时就别微调了</li>
<li>结合要使用的模型，这里可以把数据处理成char、word或者都用等</li>
<li>有时将词性标注信息也加入训练数据会收到比较好的效果</li>
<li>至于PAD的话，取均值或者一个稍微比较大的数，但是别取最大值那种应该都还好</li>
<li>神经网络结构的话到没有什么要说的，可以多试几种比如fastText、TextCNN、RCNN、char-CNN/RNN、HAN等等。哦，对了，加上dropout和BN可能会有意外收获。反正模型这块还是要具体问题具体分析吧，根据自己的需求对模型进行修改（比如之前参加知乎竞赛的时候，最终的分类标签也有文本描述，所以就可以把这部分信息也加到模型之中等等）</li>
<li>超参数的话，推荐看看之前TextCNN的一篇论文，个人感觉足够了“A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification”</li>
<li>之前还见别人在文本领域用过数据增强的方法，就是对文本进行随机的shuffle和drop等操作来增加数据量</li>
</ol>
</blockquote>
<h2 id="3-其他算法问题"><a href="#3-其他算法问题" class="headerlink" title="3. 其他算法问题"></a>3. 其他算法问题</h2><p>1、怎么进行单个样本的学习？<br>2、 决策树 bagging boosting adaboost 区别？RF的特征随机目的是什么？<br>3、transformer各部分怎么用？Q K V怎么计算；Attention怎么用？<br>4、HMM 假设是什么？CRF解决了什么问题？CRF做过特征工程吗？HMM中的矩阵意义？5、说以一下空洞卷积？膨胀卷积怎么理解？什么是Piece-CNN？<br>6、怎么解决beam-search局部最优问题？global embedding 怎么做？<br>7、数学题：什么是半正定矩阵？机器学习中有什么应用？<br>8、卷积的物理意义是什么？傅里叶变换懂吗？<br>9、说一下Bert？<br>10、推导word2vec？<br>11、怎么理解传统的统计语言模型？现在的神经网络语言模型有什么不同？<br>12、神经网络优化的难点是什么？这个问题要展开来谈。<br>13、attention你知道哪些？<br>14、自动文章摘要抽取时，怎么对一篇文章进行分割？（从序列标注、无监督等角度思考）<br>15、在做NER任务时，lstm后面可以不用加CRF吗？<br>16、通过画图描述TextRank？<br>17、LDA和pLSA有什么区别？<br>18、Transformer在实际应用中都会有哪些做法？<br>19、讲出过拟合的解决方案？<br>20、说一下transforemr、LSTM、CNN间的区别？从多个角度进行讲解？<br>21、梯度消失的原因和解决办法有哪些？<br>22、数学题：贝叶斯计算概率？<br>23、数学题：25只兔子赛跑问题，共5个赛道，最少几次比赛可以选出前5名？<br>24、数学题：100盏灯问题？</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2>
      
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-AI-算法基础"><span class="toc-text">1. AI 算法基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-防止-overfiting-的-8-条"><span class="toc-text">1.1 防止 overfiting 的 8 条</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-机器学习之类别不平衡问题"><span class="toc-text">1.2 机器学习之类别不平衡问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-Linear-vs-Nonlinear-classifier"><span class="toc-text">1.6 Linear vs Nonlinear classifier</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-NLP高频问题"><span class="toc-text">2. NLP高频问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-negative-sampling"><span class="toc-text">2.2 negative sampling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-word2vec-vs-fastText"><span class="toc-text">2.3 word2vec vs fastText</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-word2vec-vs-glove"><span class="toc-text">2.4 word2vec vs glove</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-ELMO-vs-GPT-vs-BERT"><span class="toc-text">2.5 ELMO vs GPT vs BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-8-Attention"><span class="toc-text">2.8 Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-文本分类任务-tricks"><span class="toc-text">2.9 文本分类任务 tricks</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-其他算法问题"><span class="toc-text">3. 其他算法问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        

      </footer>
    
  </div>
  
    
  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/ai1/index2.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
