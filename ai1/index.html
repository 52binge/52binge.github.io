<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="NLP 的发展 NLP 神经网络发展历史中最重要的 8 个里程碑    Language Model (语言模型就是要看到上文预测下文, So NNLM)   n-gram model（n元模型）（基于 马尔可夫假设 思想）上下文相关的特性 建立数学模型。   2001 - NNLM , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。   200">
<meta property="og:type" content="website">
<meta property="og:title" content="Auckland New Zealand">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;ai1&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:description" content="NLP 的发展 NLP 神经网络发展历史中最重要的 8 个里程碑    Language Model (语言模型就是要看到上文预测下文, So NNLM)   n-gram model（n元模型）（基于 马尔可夫假设 思想）上下文相关的特性 建立数学模型。   2001 - NNLM , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。   200">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;tensorflow&#x2F;tf-google-9.1.2_1-equation.svg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;tensorflow&#x2F;tf-google-9.1.2_2-equation.svg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;RNN-01.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;RNN-02.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;tensorflow&#x2F;tf-google-8-1.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;word2vec-nnlm.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;word2vec-CBOW_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;word2vec-neg.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;fastText-4.webp">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Seq2Seq-03.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Attention-01.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Attention-04.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Attention-03.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;bert-zh-3.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;bert-zh-4.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;bert-zh-14.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;bert-zh-17.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;bert-11.gif">
<meta property="og:updated_time" content="2019-10-20T04:30:35.090Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;tensorflow&#x2F;tf-google-9.1.2_1-equation.svg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/ai1">AI</a>
        
          <a class="main-nav-link" href="/tensorflow">TF/Keras</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="page-" class="article article-type-page" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    <div class="article-meta">
      <!--<a href="/ai1/index.html" class="article-date">
  <time datetime="2019-10-20T04:30:35.090Z" itemprop="datePublished">2019-10-20</time>
</a>-->
      <!-- 
--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/ai1/index.html#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="nlp-的发展"><a class="markdownIt-Anchor" href="#nlp-的发展"></a> NLP 的发展</h2>
<p><a href="https://www.infoq.cn/article/66vicQt*GTIFy33B4mu9" target="_blank" rel="noopener">NLP 神经网络发展历史中最重要的 8 个里程碑</a></p>
<blockquote>
<ol>
<li>
<p>Language Model (语言模型就是要看到上文预测下文, So NNLM)</p>
</li>
<li>
<p>n-gram model（n元模型）（基于 马尔可夫假设 思想）<strong>上下文相关的特性 建立数学模型</strong>。</p>
</li>
<li>
<p>2001 - <strong>NNLM</strong> , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。</p>
</li>
<li>
<p>2008 - Multi-task learning</p>
</li>
<li>
<p>2013 - Word2Vec (Word Embedding的工具word2vec : CBOW 和 Skip-gram)</p>
</li>
<li>
<p>2014 - sequence-to-sequence</p>
</li>
<li>
<p>2015 - Attention</p>
</li>
<li>
<p>2015 - Memory-based networks</p>
</li>
<li>
<p>2018 - Pretrained language models</p>
</li>
</ol>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">good 张俊林: 深度学习中的注意力模型（2017版）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">good 张俊林: 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></p>
<h2 id="1-language-model"><a class="markdownIt-Anchor" href="#1-language-model"></a> 1. Language Model</h2>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mi>N</mi><mo>+</mo><mn>1</mn></mrow><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mi>N</mi><mo>+</mo><mn>2</mn></mrow><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w\_{i}|w\_{1}, w\_{2}, ..., w\_{i-1}) = P(w\_i | w\_{i-N+1}, w\_{i-N+2}, ..., w\_{i-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord">2</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span></span></span></p>
<h2 id="2-perplexity"><a class="markdownIt-Anchor" href="#2-perplexity"></a> 2. Perplexity</h2>
<p>计算perplexity的公式如下：</p>
<img src="/images/tensorflow/tf-google-9.1.2_1-equation.svg" width="600" />
<p><strong>perplexity</strong> 刻画的是语言模型预测一个语言样本的能力. 比如已经知道 (w1,w2,w3,…,wm) 这句话会出现在语料库之中，那么通过语言模型计算得到的这句话的概率越高，说明语言模型对这个语料库拟合得越好。</p>
<p>perplexity 实际是计算每一个单词得到的概率倒数的 几何平均(<strong>geometric mean</strong>) ，因此 perplexity 可以理解为平均分支系数（average branching factor），即模型预测下一个词时的平均可选择数量。</p>
<p><a href="https://zhuanlan.zhihu.com/p/23809612" target="_blank" rel="noopener">参见： arithmetic average vs geometric mean</a></p>
<p>在语言模型的训练中，通常采用 perplexity 的 <strong><code>log</code></strong> 表达形式：</p>
<img src="/images/tensorflow/tf-google-9.1.2_2-equation.svg" width="600" />
<blockquote>
<p>相比较乘积求平方根的方式，加法的形式可加速计算，同时避免概率乘积数值过小而导致浮点数向下溢出的问题.</p>
<p>在数学上，log perplexity 可以看作真实分布与预测分布之间的交叉熵 Cross Entropy, 交叉熵描述了两个概率分布之间的一种距离. log perplexity 和 Cross Entropy 是等价的</p>
</blockquote>
<h2 id="2-recurrent-neural-networks"><a class="markdownIt-Anchor" href="#2-recurrent-neural-networks"></a> 2. Recurrent Neural Networks</h2>
<ul>
<li><strong>输入和输出的长度不尽相同</strong></li>
<li><strong>无法共享从其他位置学来的特征</strong></li>
</ul>
<img src="/images/deeplearning/RNN-01.png" width="500" />
<blockquote>
<ul>
<li>很多数据是以序列形式存在的，例如文本、语音、视频、点击流等等。</li>
</ul>
</blockquote>
<p><strong>Typical RNN Structure:</strong></p>
<p>在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mi mathvariant="normal">_</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">h\_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span> 后面直接接一个 Softmax 层，输出文本所属类别的预测概率 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span>，就可以实现文本分类.</p>
<img src="/images/deeplearning/RNN-02.png" width="650" />
<p>可应用于多种具体任务：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mi>e</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo>=</mo><mi>U</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo>+</mo><mi>W</mi><mi>h</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">net\_{t}=U x\_{t}+W h\_{t-1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9250799999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.99333em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mord mathdefault">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo>=</mo><mi>f</mi><mrow><mo fence="true">(</mo><mtext>net</mtext><mi mathvariant="normal">_</mi><mi>t</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">h\_{t}=f\left(\text {net}\_{t}\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord text"><span class="mord">net</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>g</mi><mrow><mo fence="true">(</mo><mi>V</mi><mi>h</mi><mi mathvariant="normal">_</mi><mi>T</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">y=g\left(V h\_{T}\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord mathdefault">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span></span></span></span> 为激活函数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span></span></span></span> 为输入层到隐含层的权重矩阵，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span> 为隐含层从上一时刻到下一时刻状态转移的权重矩阵。在文本分类任务中，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span> 可以选取 Tanh 函数或者 ReLU 函数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span></span></span></span> 可以采用 Softmax 函数。</p>
<p><strong>TensorFlow RNN</strong></p>
<img src="/images/tensorflow/tf-google-8-1.jpg" width="700" alt="Forward Propagation" />
<p><a href="/2018/11/08/tensorflow/tf-google-8-rnn-1/">更多详情参见本博： TensorFlow：第8章 Recurrent Neural Networks 1</a></p>
<blockquote>
<ol>
<li>RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。</li>
<li>RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。</li>
</ol>
</blockquote>
<ul>
<li><a href="/2019/06/14/deeplearning/RNN-LSTM-GRU/">Recurrent Neural Networks</a></li>
</ul>
<h2 id="3-nnlm"><a class="markdownIt-Anchor" href="#3-nnlm"></a> 3. NNLM</h2>
<p>NNLM,直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程.</p>
<img src="/images/nlp/word2vec-nnlm.png" width="600" />
<p><strong>使用词嵌三步</strong></p>
<blockquote>
<ol>
<li>获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库</li>
<li>应用词嵌：将获得的词嵌应用在我们的训练任务中</li>
<li>可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）</li>
</ol>
</blockquote>
<h2 id="4-word2vec"><a class="markdownIt-Anchor" href="#4-word2vec"></a> 4. word2vec</h2>
<p>word2vec 并不是一个模型， 而是一个 2013年 google 发表的工具.</p>
<p>该工具包含2个模型：</p>
<ol>
<li>Skip-Gram</li>
<li>CBOW.</li>
</ol>
<p>该工具包含2种高效训练方法：</p>
<ol>
<li>negative sampling</li>
<li>hierarchicam softmax.</li>
</ol>
<blockquote>
<p>词向量（词的特征向量）既能够降低维度，又能够capture到当前词在本句子中上下文的信息</p>
</blockquote>
<p><strong>CBOW</strong> (context(W)-&gt;center)</p>
<img src="/images/nlp/word2vec-CBOW_1.png" width="600" />
<blockquote>
<p>纠错 : 上图”目标函数“的第一个公式，应该是 连乘 公式，不是 连加 运算。</p>
<p>理解 : 背景词向量与 中心词向量 内积 等部分，你可考虑 softmax <span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \* at position 3: w \̲*̲ x+b'>w \* x+b</span> 中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 的关系来理解.</p>
</blockquote>
<h3 id="41-negative-sampling"><a class="markdownIt-Anchor" href="#41-negative-sampling"></a> 4.1 Negative Sampling</h3>
<blockquote>
<p>1）如何通过一个正例和neg个负例进行二元逻辑回归呢？ 2） <strong><code>如何进行负采样呢</code></strong>？</p>
</blockquote>
<ul>
<li><strong>如何进行 negative sampling</strong>？</li>
</ul>
<p>每个词𝑤的线段长度由下式决定：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∑</mo><mi mathvariant="normal">_</mi><mrow><mi>u</mi><mo>∈</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi></mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">len(w) = \frac{count(w)}{\sum\limits\_{u \in vocab} count(u)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.423em;vertical-align:-0.996em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">u</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">o</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault">b</span></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault">u</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.996em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li>在word2vec中，分子和分母都取了3/4次幂如下：</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>w</mi><msup><mo stretchy="false">)</mo><mrow><mn>3</mn><mi mathvariant="normal">/</mi><mn>4</mn></mrow></msup></mrow><mrow><mo>∑</mo><mi mathvariant="normal">_</mi><mrow><mi>u</mi><mo>∈</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi></mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>u</mi><msup><mo stretchy="false">)</mo><mrow><mn>3</mn><mi mathvariant="normal">/</mi><mn>4</mn></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">len(w) = \frac{count(w)^{3/4}}{\sum\limits\_{u \in vocab} count(u)^{3/4}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.5789999999999997em;vertical-align:-1.014em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.565em;"><span style="top:-2.296em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">u</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">o</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault">b</span></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault">u</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.814em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">/</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">/</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.014em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<img src="/images/nlp/word2vec-neg.png" width="600" />
<p>负采样这个点引入 word2vec 非常巧妙，两个作用，</p>
<blockquote>
<ol>
<li>加速了模型计算</li>
<li>保证了模型训练的效果</li>
</ol>
<p>第一，model 每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢。</p>
<p>第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点聪明.</p>
</blockquote>
<ul>
<li><a href="https://www.cnblogs.com/pinard/p/7249903.html" target="_blank" rel="noopener">good, word2vec Negative Sampling 刘建平Pinard</a></li>
</ul>
<h3 id="42-hierarchicam-softmax"><a class="markdownIt-Anchor" href="#42-hierarchicam-softmax"></a> 4.2 Hierarchicam Softmax</h3>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/46430775" target="_blank" rel="noopener">知乎: Huffman Tree</a></p>
<p>给定n权值作为n个叶子节点，构造一棵二叉树，若这棵二叉树的带权路径长度达到最小，则称这样的二叉树为最优二叉树，也称为Huffman树。</p>
</blockquote>
<p><strong>word2vec vs glove</strong></p>
<ol>
<li>目标函数不同 （crossentrpy vs 平方损失函数）</li>
<li>glove 全局统计固定语料信息</li>
</ol>
<blockquote>
<ul>
<li>
<p>word2vec 是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。</p>
</li>
<li>
<p>总体来看，glove 可以被看作是更换了目标函数和权重函数的全局 word2vec。</p>
</li>
</ul>
</blockquote>
<h2 id="5-fasttext"><a class="markdownIt-Anchor" href="#5-fasttext"></a> 5. fastText</h2>
<p>FastText是一个快速文本分类算法，在使用标准多核CPU的情况下，在10分钟内可以对超过10亿个单词进行训练。 不需要使用预先训练好的词向量，因为FastText会自己训练词向量。</p>
<img src="/images/nlp/fastText-4.webp" width="500" />
<p>fastText 能够做到效果好，速度快，主要依靠两个秘密武器：</p>
<blockquote>
<ol>
<li>结构与CBOW类似，但学习目标是人工标注的分类结果；</li>
<li>用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick.</li>
<li>引入 N-gram，考虑词序特征；</li>
<li>引入 subword 来处理长词，处理未登陆词问题；</li>
</ol>
</blockquote>
<h2 id="6-seq2seq"><a class="markdownIt-Anchor" href="#6-seq2seq"></a> 6. Seq2Seq</h2>
<img src="/images/deeplearning/Seq2Seq-03.jpg" width="600" alt="Encoder-Decoder" />
<p>Source 和 Target 分别由各自的单词序列构成：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mi>o</mi><mi>u</mi><mi>r</mi><mi>c</mi><mi>e</mi><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>x</mi><mi mathvariant="normal">_</mi><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Source = ({x}\_1, {x}\_2, ..., {x}\_m)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">m</span><span class="mclose">)</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>=</mo><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Target = ({y}\_1, {y}\_2, ..., {y}\_n)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span></p>
<p>Encoder 顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><mo>=</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>x</mi><mi mathvariant="normal">_</mi><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C = F({x}\_1, {x}\_2, ..., {x}\_m)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">m</span><span class="mclose">)</span></span></span></span></span></p>
<p>对于 Decoder 来说，其任务是根据句子 Source 的 中间语义表示 C 和 之前已经生成的历史信息</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">({y}\_1, {y}\_2, ..., {y}\_{i-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span></span></span></p>
<p>来生成 i时刻 要生成的单词 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">{y}\_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>C</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y\_{i} = g(C, {y}\_1, {y}\_2, ..., {y}\_{i-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span></span></span></p>
<blockquote>
<p>每个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">y\_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span></span></span></span> 都依次这么产生，那么看起来就是整个系统根据输入 句子Source 生成了目标句子Target。</p>
<p>(1). 如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题；<br />
(2). 如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要；<br />
(3). 如果Source是一句问句，Target是一句回答，那么这是问答系统。</p>
</blockquote>
<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。</p>
<p><strong>train seq2seq model</strong></p>
<p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>
<p class='katex-block katex-error' title='ParseError: KaTeX parse error: No such environment: split at position 7: \begin{̲s̲p̲l̲i̲t̲}̲\begin{aligned}…'>\begin{split}\begin{aligned}
{P}(y\_1, \ldots, y\_{T&#039;} \mid x\_1, \ldots, x\_T)
&amp;= \prod\_{t&#039;=1}^{T&#039;} {P}(y\_{t&#039;} \mid y\_1, \ldots, y\_{t&#039;-1}, x\_1, \ldots, x\_T)\\\\
&amp;= \prod\_{t&#039;=1}^{T&#039;} {P}(y\_{t&#039;} \mid y\_1, \ldots, y\_{t&#039;-1}, \boldsymbol{c}),
\end{aligned}\end{split}
</p>
<p>并得到该输出序列的损失</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><msup><mi>T</mi><mo mathvariant="normal">′</mo></msup><mo>∣</mo><mi>x</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>x</mi><mi mathvariant="normal">_</mi><mi>T</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><msup><mi>t</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mn>1</mn></mrow><msup><mi>T</mi><mo mathvariant="normal">′</mo></msup></msup><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">_</mi><msup><mi>t</mi><mo mathvariant="normal">′</mo></msup><mo>∣</mo><mi>y</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mrow><msup><mi>t</mi><mo mathvariant="normal">′</mo></msup><mo>−</mo><mn>1</mn></mrow><mo separator="true">,</mo><mi mathvariant="bold-italic">c</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex"> - \log{P}(y\_1, \ldots, y\_{T&#x27;} \mid x\_1, \ldots, x\_T) = -\sum\_{t&#x27;=1}^{T&#x27;} \log {P}(y\_{t&#x27;} \mid y\_1,  \ldots, y\_{t&#x27;-1}, \boldsymbol{c}),
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.111892em;vertical-align:-0.31em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.684282em;vertical-align:-0.55001em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1342720000000002em;"><span style="top:-3.254792em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278285714285715em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.111892em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">c</span></span></span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span></p>
<blockquote>
<p>在 train 中，所有输出序列损失的均值通常作为需要最小化的损失函数。</p>
<p>在 predict 中，我们需要将decode在上一个时间步的输出作为当前时间步的输入 Or <strong>teacher forcing</strong>。</p>
</blockquote>
<p><strong>summary</strong></p>
<blockquote>
<ul>
<li>Encoder—Decoder（seq2seq）可以输入并输出不定长的序列。Encoder—Decoder 使用了两个 RNN .</li>
<li>Encoder—Decoder 的训练中，我们可以采用 teacher forcing。(这也是 Seq2Seq 2 的内容)</li>
</ul>
<p>将source进行反序输入：输入的时候将“ABC”变成“CBA”，这样做的好处是解决了长序列的long-term依赖，使得模型可以学习到更多的对应关系，从而达到比较好的效果。</p>
<p>Beam Search：这是在test时的技巧，也就是在训练过程中不会使用。</p>
</blockquote>
<h2 id="7-attention"><a class="markdownIt-Anchor" href="#7-attention"></a> 7. Attention</h2>
<p>请务必要阅读： <a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">张俊林 深度学习中的注意力模型（2017版）</a></p>
<img src="/images/deeplearning/Attention-01.jpg" width="600" />
<p><strong>Attention 本质思想</strong></p>
<p>把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易懂:</p>
<img src="/images/deeplearning/Attention-04.jpg" width="600" />
<p><strong>Attention 的三阶段</strong></p>
<blockquote>
<ol>
<li>第一个阶段根据Query和Key计算两者的相似性或者相关性；</li>
<li>第二个阶段对第一阶段的原始分值进行归一化处理；</li>
<li>根据权重系数对Value进行加权求和。</li>
</ol>
</blockquote>
<img src="/images/deeplearning/Attention-03.jpg" width="600" />
<h2 id="8-gpt-elmo"><a class="markdownIt-Anchor" href="#8-gpt-elmo"></a> 8. GPT &amp; ELMO</h2>
<p><strong>ELMO: Embedding from Language Models</strong></p>
<blockquote>
<p>ELMO的论文题目：“Deep contextualized word representation”</p>
<p>NAACL 2018 最佳论文 - ELMO： Deep contextualized word representation</p>
<p>ELMO 本身是个根据当前上下文对Word Embedding动态调整的思路。</p>
<p><strong>ELMO 有什么缺点？</strong></p>
<ol>
<li>LSTM 抽取特征能力远弱于 Transformer</li>
<li>拼接方式双向融合特征能力偏弱</li>
</ol>
</blockquote>
<p>**GPT (Generative Pre-Training) **</p>
<blockquote>
<ol>
<li>第一个阶段是利用 language 进行 Pre-Training.</li>
<li>第二阶段通过 Fine-tuning 的模式解决下游任务。</li>
</ol>
<p><strong>GPT: 有什么缺点？</strong></p>
<ol>
<li>要是把 language model 改造成双向就好了</li>
<li>不太会炒作，GPT 也是非常重要的工作.</li>
</ol>
</blockquote>
<p><strong>Bert 亮点 : 效果好 和 普适性强</strong></p>
<blockquote>
<ol>
<li>Transformer 特征抽取器</li>
<li>Language Model 作为训练任务 (双向)</li>
</ol>
<p>Bert 采用和 GPT 完全相同的 <strong>两阶段</strong> 模型：</p>
<ol>
<li>Pre-Train Language Model；</li>
<li>Fine-&gt; Tuning模式解决下游任务。</li>
</ol>
</blockquote>
<h2 id="9-transformer"><a class="markdownIt-Anchor" href="#9-transformer"></a> 9. Transformer</h2>
<p>Transformer 改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。</p>
<p>Transformer 可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。</p>
<p><a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">务必阅读： The Illustrated Transformer 中文版</a></p>
<p>Q、K、V 它们都是有助于计算和理解注意力机制的抽象概念</p>
<!--![](/images/nlp/bert-zh-3.jpg)-->
<img src="/images/nlp/bert-zh-3.jpg" width="600" />
<p>所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。</p>
<!--![](/images/nlp/bert-zh-4.jpg)-->
<img src="/images/nlp/bert-zh-4.jpg" width="600" />
<p>解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。</p>
<p>一个公式来计算自注意力层的输出</p>
<img src="/images/nlp/bert-zh-14.jpg" width="600" />
<!--<img src="/images/nlp/bert-zh-17.jpg" width="500" />
-->
<p>解码组件</p>
<img src="/images/nlp/bert-11.gif" width="650" />
<!--![](/images/nlp/bert-zh-12.jpg)-->
<!--![](/images/nlp/bert-zh-17.jpg)-->
<!--![](/images/nlp/bert-11.gif)-->
<blockquote>
<p>Transformer作为新模型，并不是完美无缺的。它也有明显的缺点：首先，对于长输入的任务，典型的比如篇章级别的任务（例如文本摘要），因为任务的输入太长，Transformer会有巨大的计算复杂度，导致速度会急剧变慢。</p>
</blockquote>
<h2 id="10-task-tricks"><a class="markdownIt-Anchor" href="#10-task-tricks"></a> 10. Task tricks</h2>
<p>在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的tricks？</p>
<blockquote>
<ol>
<li>数据预处理时vocab的选取（前N个高频词或者过滤掉出现次数小于3的词等等）</li>
<li>词向量的选择，可以使用预训练好的词向量如谷歌、facebook开源出来的，当训练集比较大的时候也可以进行微调或者随机初始化与训练同时进行。训练集较小时就别微调了</li>
<li>结合要使用的模型，这里可以把数据处理成char、word或者都用等</li>
<li>有时将词性标注信息也加入训练数据会收到比较好的效果</li>
<li>至于PAD的话，取均值或者一个稍微比较大的数，但是别取最大值那种应该都还好</li>
<li>神经网络结构的话到没有什么要说的，可以多试几种比如fastText、TextCNN、RCNN、char-CNN/RNN、HAN等等。加上dropout和BN可能会有意外收获。反正模型这块还是要具体问题具体分析吧，（比如之前参加知乎竞赛的时候，最终的分类标签也有文本描述，所以就可以把这部分信息也加到模型之中等等）</li>
</ol>
</blockquote>
<p><strong>Overfiting 8 条</strong></p>
<blockquote>
<p>1). get more data<br />
2). Data augmentation<br />
3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验)<br />
4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；)<br />
5). Choosing Right Network Structure<br />
6). Early stopping<br />
7). Model Ensumble<br />
8). Batch Normalization</p>
</blockquote>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/38176412" target="_blank" rel="noopener">张俊林 - Batch Normalization导读</a> 、 <a href="https://zhuanlan.zhihu.com/p/43200897" target="_blank" rel="noopener">张俊林 - 深度学习中的Normalization模型</a></p>
<p><code>Internal Covariate Shift</code> &amp; Independent and identically distributed，缩写为 <code>IID</code></p>
<p>Batch Normalization 可以有效避免复杂参数对网络训练产生的影响，也可提高泛化能力.</p>
<p>神经网路的训练过程的本质是学习数据分布，如果训练数据与测试数据分布不同，将大大降低网络泛化能力， BN 是针对每一批数据，在网络的每一层输入之前增加 BN，(均值0，标准差1)。</p>
<p>Dropout 可以抑制过拟合，作用于每份小批量的训练数据，随机丢弃部分神经元机制. bagging 原理.</p>
<p><a href="https://posts.careerengine.us/p/5cae13b2d401440a7fe047af" target="_blank" rel="noopener">ML算法： 关于防止过拟合，整理了 8 条迭代方向</a></p>
</blockquote>
<h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2>
<ul>
<li><a href="/2019/06/16/nlp/Language-Model-and-Word-Embedding/">Language Model and Perplexity</a></li>
<li><a href="https://blog.csdn.net/blmoistawinde/article/details/80816179" target="_blank" rel="noopener">sklearn: TfidfVectorizer 中文处理及一些使用参数</a></li>
<li><a href="https://blog.csdn.net/binglingzy666/article/details/79241486" target="_blank" rel="noopener">sklearn.feature_extraction.text.TfidfVectorizer函数说明</a></li>
</ul>

      
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#nlp-的发展"><span class="toc-text"> NLP 的发展</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-language-model"><span class="toc-text"> 1. Language Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-perplexity"><span class="toc-text"> 2. Perplexity</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-recurrent-neural-networks"><span class="toc-text"> 2. Recurrent Neural Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-nnlm"><span class="toc-text"> 3. NNLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-word2vec"><span class="toc-text"> 4. word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-negative-sampling"><span class="toc-text"> 4.1 Negative Sampling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-hierarchicam-softmax"><span class="toc-text"> 4.2 Hierarchicam Softmax</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-fasttext"><span class="toc-text"> 5. fastText</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-seq2seq"><span class="toc-text"> 6. Seq2Seq</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-attention"><span class="toc-text"> 7. Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-gpt-elmo"><span class="toc-text"> 8. GPT &amp; ELMO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-transformer"><span class="toc-text"> 9. Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-task-tricks"><span class="toc-text"> 10. Task tricks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-text"> Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        

      </footer>
    
  </div>
  
    
  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/ai1/index.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
