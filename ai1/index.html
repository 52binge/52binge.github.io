<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="NLP 的发展NLP 神经网络发展历史中最重要的 8 个里程碑


Language Model (语言模型就是要看到上文预测下文, So NNLM)

n-gram model（n元模型）（基于 马尔可夫假设 思想）上下文相关的特性 建立数学模型。

2001 - NNLM , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。 

2008 - M">
<meta property="og:type" content="website">
<meta property="og:title" content="Auckland New Zealand">
<meta property="og:url" content="http://shopee.ai/ai1/index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:description" content="NLP 的发展NLP 神经网络发展历史中最重要的 8 个里程碑


Language Model (语言模型就是要看到上文预测下文, So NNLM)

n-gram model（n元模型）（基于 马尔可夫假设 思想）上下文相关的特性 建立数学模型。

2001 - NNLM , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。 

2008 - M">
<meta property="og:image" content="http://shopee.ai/images/tensorflow/tf-google-9.1.2_1-equation.svg">
<meta property="og:image" content="http://shopee.ai/images/tensorflow/tf-google-9.1.2_2-equation.svg">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/RNN-01.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/RNN-02.png">
<meta property="og:image" content="http://shopee.ai/images/tensorflow/tf-google-8-1.jpg">
<meta property="og:image" content="http://shopee.ai/images/nlp/word2vec-nnlm.png">
<meta property="og:image" content="http://shopee.ai/images/nlp/word2vec-CBOW_1.png">
<meta property="og:image" content="http://shopee.ai/images/nlp/fastText-4.webp">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/Seq2Seq-03.jpg">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/Attention-01.jpg">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/Attention-04.jpg">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/Attention-03.jpg">
<meta property="og:updated_time" content="2019-07-29T09:32:10.178Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Auckland New Zealand">
<meta name="twitter:description" content="NLP 的发展NLP 神经网络发展历史中最重要的 8 个里程碑


Language Model (语言模型就是要看到上文预测下文, So NNLM)

n-gram model（n元模型）（基于 马尔可夫假设 思想）上下文相关的特性 建立数学模型。

2001 - NNLM , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。 

2008 - M">
<meta name="twitter:image" content="http://shopee.ai/images/tensorflow/tf-google-9.1.2_1-equation.svg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/ai1">AI</a>
        
          <a class="main-nav-link" href="/english">English</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shopee.ai"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="page-undefined" class="article article-type-page" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    <div class="article-meta">
      <!--<a href="/ai1/index.html" class="article-date">
  <time datetime="2019-07-29T09:32:10.190Z" itemprop="datePublished">2019-07-29</time>
</a>-->
      <!-- 
--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://shopee.ai/ai1/index.html#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="NLP-的发展"><a href="#NLP-的发展" class="headerlink" title="NLP 的发展"></a>NLP 的发展</h2><p><a href="https://www.infoq.cn/article/66vicQt*GTIFy33B4mu9" target="_blank" rel="external">NLP 神经网络发展历史中最重要的 8 个里程碑</a></p>
<blockquote>
<ol>
<li><p>Language Model (语言模型就是要看到上文预测下文, So NNLM)</p>
</li>
<li><p>n-gram model（n元模型）（基于 马尔可夫假设 思想）<strong>上下文相关的特性 建立数学模型</strong>。</p>
</li>
<li><p>2001 - <strong>NNLM</strong> , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。 </p>
</li>
<li><p>2008 - Multi-task learning</p>
</li>
<li><p>2013 - Word2Vec (Word Embedding的工具word2vec : CBOW 和 Skip-gram)</p>
</li>
<li><p>2014 - sequence-to-sequence </p>
</li>
<li><p>2015 - Attention</p>
</li>
<li><p>2015 - Memory-based networks</p>
</li>
<li><p>2018 - Pretrained language models</p>
</li>
</ol>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="external">good 张俊林: 深度学习中的注意力模型（2017版）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="external">good 张俊林: 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a> </p>
<h2 id="1-Language-Model"><a href="#1-Language-Model" class="headerlink" title="1. Language Model"></a>1. Language Model</h2><p>$$<br>P(w_{i}|w_{1}, w_{2}, …, w_{i-1}) = P(w_i | w_{i-N+1}, w_{i-N+2}, …, w_{i-1})<br>$$</p>
<h2 id="2-Perplexity"><a href="#2-Perplexity" class="headerlink" title="2. Perplexity"></a>2. Perplexity</h2><p>计算perplexity的公式如下：</p>
<p><img src="/images/tensorflow/tf-google-9.1.2_1-equation.svg" width="600"></p>
<p><strong>perplexity</strong> 刻画的是语言模型预测一个语言样本的能力. 比如已经知道 (w1,w2,w3,…,wm) 这句话会出现在语料库之中，那么通过语言模型计算得到的这句话的概率越高，说明语言模型对这个语料库拟合得越好。</p>
<p>perplexity 实际是计算每一个单词得到的概率倒数的几何平均，因此 perplexity 可以理解为平均分支系数（average branching factor），即模型预测下一个词时的平均可选择数量。</p>
<p>在语言模型的训练中，通常采用 perplexity 的对数表达形式：</p>
<p><img src="/images/tensorflow/tf-google-9.1.2_2-equation.svg" width="600"></p>
<blockquote>
<p>相比较乘积求平方根的方式，加法的形式可加速计算，同时避免概率乘积数值过小而导致浮点数向下溢出的问题.</p>
<p>在数学上，log perplexity 可以看作真实分布与预测分布之间的交叉熵 Cross Entropy, 交叉熵描述了两个概率分布之间的一种距离. log perplexity 和 Cross Entropy 是等价的</p>
</blockquote>
<h2 id="2-Recurrent-Neural-Networks"><a href="#2-Recurrent-Neural-Networks" class="headerlink" title="2. Recurrent Neural Networks"></a>2. Recurrent Neural Networks</h2><ul>
<li><strong>输入和输出的长度不尽相同</strong></li>
<li><strong>无法共享从其他位置学来的特征</strong></li>
</ul>
<p><img src="/images/deeplearning/RNN-01.png" width="500"></p>
<blockquote>
<ul>
<li>很多数据是以序列形式存在的，例如文本、语音、视频、点击流等等。</li>
</ul>
</blockquote>
<p><strong>Typical RNN Structure:</strong></p>
<p>在 $h_T$ 后面直接接一个 Softmax 层，输出文本所属类别的预测概率 $y$，就可以实现文本分类.</p>
<p><img src="/images/deeplearning/RNN-02.png" width="650"></p>
<p>可应用于多种具体任务：</p>
<p>$$<br>net_{t}=U x_{t}+W h_{t-1}<br>$$</p>
<p>$$<br>h_{t}=f\left(\text {net}_{t}\right)<br>$$</p>
<p>$$<br>y=g\left(V h_{T}\right)<br>$$</p>
<p>其中 $f$ 和 $g$ 为激活函数，$U$ 为输入层到隐含层的权重矩阵，$W$ 为隐含层从上一时刻到下一时刻状态转移的权重矩阵。在文本分类任务中，$f$ 可以选取 Tanh 函数或者 ReLU 函数，$g$ 可以采用 Softmax 函数。</p>
<p><strong>TensorFlow RNN</strong></p>
<p><img src="/images/tensorflow/tf-google-8-1.jpg" width="700" alt="Forward Propagation"></p>
<p><a href="/2018/11/08/tensorflow/tf-google-8-rnn-1/">更多详情参见本博： TensorFlow：第8章 Recurrent Neural Networks 1</a></p>
<blockquote>
<ol>
<li>RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。</li>
<li>RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。</li>
</ol>
</blockquote>
<ul>
<li><a href="/2019/06/14/deeplearning/RNN-LSTM-GRU/">Recurrent Neural Networks</a></li>
</ul>
<h2 id="3-NNLM"><a href="#3-NNLM" class="headerlink" title="3. NNLM"></a>3. NNLM</h2><p>NNLM,直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程.</p>
<p><img src="/images/nlp/word2vec-nnlm.png" width="600"></p>
<p><strong>使用词嵌三步</strong></p>
<blockquote>
<ol>
<li>获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库</li>
<li>应用词嵌：将获得的词嵌应用在我们的训练任务中</li>
<li>可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）</li>
</ol>
</blockquote>
<h2 id="4-word2vec"><a href="#4-word2vec" class="headerlink" title="4. word2vec"></a>4. word2vec</h2><p>word2vec 并不是一个模型， 而是一个 2013年 google 发表的工具. 该工具包含2个模型： Skip-Gram 和 CBOW. 及两种高效训练方法： negative sampling 和 hierarchicam softmax.</p>
<blockquote>
<p>词向量（词的特征向量）既能够降低维度，又能够capture到当前词在本句子中上下文的信息</p>
</blockquote>
<p><strong>CBOW</strong></p>
<p><img src="/images/nlp/word2vec-CBOW_1.png" width="600"></p>
<blockquote>
<p>纠错 : 上图”目标函数“的第一个公式，应该是 连乘 公式，不是 连加 运算。</p>
<p>理解 : 背景词向量与 中心词向量 内积 等部分，你可考虑 softmax $w * x+b$ 中 $x$ 和 $w$ 的关系来理解.</p>
</blockquote>
<h2 id="5-fastText"><a href="#5-fastText" class="headerlink" title="5. fastText"></a>5. fastText</h2><p>FastText是一个快速文本分类算法，在使用标准多核CPU的情况下，在10分钟内可以对超过10亿个单词进行训练。 不需要使用预先训练好的词向量，因为FastText会自己训练词向量。</p>
<p><img src="/images/nlp/fastText-4.webp" width="500"></p>
<p>fastText 能够做到效果好，速度快，主要依靠两个秘密武器：</p>
<blockquote>
<ol>
<li>利用了 词内的n-gram信息 (subword n-gram information)</li>
<li>用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick.</li>
</ol>
</blockquote>
<h2 id="6-Seq2Seq"><a href="#6-Seq2Seq" class="headerlink" title="6. Seq2Seq"></a>6. Seq2Seq</h2><p><img src="/images/deeplearning/Seq2Seq-03.jpg" width="600" alt="Encoder-Decoder"></p>
<p>Source 和 Target 分别由各自的单词序列构成：</p>
<p>$$<br>Source = ({x}_1, {x}_2, …, {x}_m)<br>$$</p>
<p>$$<br>Target = ({y}_1, {y}_2, …, {y}_n)<br>$$</p>
<p>Encoder 顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p>
<p>$$<br>C = F({x}_1, {x}_2, …, {x}_m)<br>$$</p>
<p>对于 Decoder 来说，其任务是根据句子 Source 的 中间语义表示 C 和 之前已经生成的历史信息</p>
<p>$$<br>({y}_1, {y}_2, …, {y}_{i-1})<br>$$</p>
<p>来生成 i时刻 要生成的单词 ${y}_{i}$</p>
<p>$$<br>y_{i} = g(C, {y}_1, {y}_2, …, {y}_{i-1})<br>$$</p>
<blockquote>
<p>每个 $y_i$ 都依次这么产生，那么看起来就是整个系统根据输入 句子Source 生成了目标句子Target。</p>
<p>(1). 如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题；<br>(2). 如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要；<br>(3). 如果Source是一句问句，Target是一句回答，那么这是问答系统。</p>
</blockquote>
<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。</p>
<p><strong>train seq2seq model</strong></p>
<p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>
<p>$$<br>\begin{split}\begin{aligned}<br>{P}(y_1, \ldots, y_{T’} \mid x_1, \ldots, x_T)<br>&amp;= \prod_{t’=1}^{T’} {P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, x_1, \ldots, x_T)\\<br>&amp;= \prod_{t’=1}^{T’} {P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c}),<br>\end{aligned}\end{split}<br>$$</p>
<p>并得到该输出序列的损失</p>
<p>$$ - \log{P}(y_1, \ldots, y_{T’} \mid x_1, \ldots, x_T) = -\sum_{t’=1}^{T’} \log {P}(y_{t’} \mid y_1,  \ldots, y_{t’-1}, \boldsymbol{c}),<br>$$</p>
<blockquote>
<p>在 train 中，所有输出序列损失的均值通常作为需要最小化的损失函数。</p>
<p>在 predict 中，我们需要将decode在上一个时间步的输出作为当前时间步的输入 Or <strong>teacher forcing</strong>。</p>
</blockquote>
<p><strong>summary</strong></p>
<blockquote>
<ul>
<li>Encoder—Decoder（seq2seq）可以输入并输出不定长的序列。Encoder—Decoder 使用了两个 RNN .</li>
<li>Encoder—Decoder 的训练中，我们可以采用 teacher forcing。(这也是 Seq2Seq 2 的内容)</li>
</ul>
</blockquote>
<h2 id="7-Attention"><a href="#7-Attention" class="headerlink" title="7. Attention"></a>7. Attention</h2><p>请务必要阅读： <a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="external">张俊林 深度学习中的注意力模型（2017版）</a></p>
<p><img src="/images/deeplearning/Attention-01.jpg" width="600"></p>
<p><strong>Attention 本质思想</strong></p>
<p>把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易懂:</p>
<p><img src="/images/deeplearning/Attention-04.jpg" width="600"></p>
<p><strong>Attention 的三阶段</strong></p>
<blockquote>
<ol>
<li>第一个阶段根据Query和Key计算两者的相似性或者相关性；</li>
<li>第二个阶段对第一阶段的原始分值进行归一化处理；</li>
<li>根据权重系数对Value进行加权求和。</li>
</ol>
</blockquote>
<p><img src="/images/deeplearning/Attention-03.jpg" width="600"></p>
<h2 id="8-GPT-amp-ELMO"><a href="#8-GPT-amp-ELMO" class="headerlink" title="8. GPT &amp; ELMO"></a>8. GPT &amp; ELMO</h2><p><strong>ELMO: Embedding from Language Models</strong></p>
<blockquote>
<p>ELMO的论文题目：“Deep contextualized word representation”</p>
<p>NAACL 2018 最佳论文 - ELMO： Deep contextualized word representation</p>
<p>ELMO 本身是个根据当前上下文对Word Embedding动态调整的思路。</p>
<p><strong>ELMO 有什么缺点？</strong></p>
<ol>
<li>LSTM 抽取特征能力远弱于 Transformer</li>
<li>拼接方式双向融合特征能力偏弱</li>
</ol>
</blockquote>
<p><strong>GPT (Generative Pre-Training) </strong></p>
<blockquote>
<ol>
<li>第一个阶段是利用 language 进行 Pre-Training.</li>
<li>第二阶段通过 Fine-tuning 的模式解决下游任务。</li>
</ol>
<p><strong>GPT: 有什么缺点？</strong></p>
<ol>
<li>要是把 language model 改造成双向就好了</li>
<li>不太会炒作，GPT 也是非常重要的工作.</li>
</ol>
</blockquote>
<p><strong>Bert 亮点 : 效果好 和 普适性强</strong></p>
<blockquote>
<ol>
<li>Transformer 特征抽取器</li>
<li>Language Model 作为训练任务 (双向)</li>
</ol>
<p>Bert 采用和 GPT 完全相同的 <strong>两阶段</strong> 模型：</p>
<ol>
<li>Pre-Train Language Model；</li>
<li>Fine-&gt; Tuning模式解决下游任务。</li>
</ol>
</blockquote>
<h2 id="9-Transformer"><a href="#9-Transformer" class="headerlink" title="9. Transformer"></a>9. Transformer</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="/2019/06/16/nlp/Language-Model-and-Word-Embedding/">Language Model and Perplexity</a></li>
<li><a href="https://blog.csdn.net/blmoistawinde/article/details/80816179" target="_blank" rel="external">sklearn: TfidfVectorizer 中文处理及一些使用参数</a></li>
<li><a href="https://blog.csdn.net/binglingzy666/article/details/79241486" target="_blank" rel="external">sklearn.feature_extraction.text.TfidfVectorizer函数说明</a></li>
</ul>

      
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#NLP-的发展"><span class="toc-number"></span> <span class="toc-text">NLP 的发展</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Language-Model"><span class="toc-number"></span> <span class="toc-text">1. Language Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Perplexity"><span class="toc-number"></span> <span class="toc-text">2. Perplexity</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Recurrent-Neural-Networks"><span class="toc-number"></span> <span class="toc-text">2. Recurrent Neural Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-NNLM"><span class="toc-number"></span> <span class="toc-text">3. NNLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-word2vec"><span class="toc-number"></span> <span class="toc-text">4. word2vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-fastText"><span class="toc-number"></span> <span class="toc-text">5. fastText</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Seq2Seq"><span class="toc-number"></span> <span class="toc-text">6. Seq2Seq</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Attention"><span class="toc-number"></span> <span class="toc-text">7. Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-GPT-amp-ELMO"><span class="toc-number"></span> <span class="toc-text">8. GPT & ELMO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Transformer"><span class="toc-number"></span> <span class="toc-text">9. Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number"></span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        

      </footer>
    
  </div>
  
    
  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://shopee.ai/ai1/index.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
