<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="0. ML and NLP你可能想不到，Machine Learning and Logic Programming 有一种奇妙的关系，她们就像亲姐妹.
ML and LP

逻辑编程是什么 （X + 2 = 5 询问 X 的值，回答： X=3）类比 forward pass + back propagation
ML框架 其实是一种 程序语言 : differentiable programmi">
<meta property="og:type" content="website">
<meta property="og:title" content="Home">
<meta property="og:url" content="http://shopee.ai/ai1/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="0. ML and NLP你可能想不到，Machine Learning and Logic Programming 有一种奇妙的关系，她们就像亲姐妹.
ML and LP

逻辑编程是什么 （X + 2 = 5 询问 X 的值，回答： X=3）类比 forward pass + back propagation
ML框架 其实是一种 程序语言 : differentiable programmi">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-c79815588a512fc8a9256bc7582b02e3_hd.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=g%28s%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-s%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-a1c3ce43528dbc274be8952c06d2b9b4_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-569009cc3ccd3e9922b77c1e4cbf4ca0_hd.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-4713a5b63da71ef5afba3fcd3a65299d_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-a384924b89b1bdd581cef7d75b56e226_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-aca3644ddd56abe1e47c0f45601587c3_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-492ac29bbed274a282eee069c0b63c93_hd.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-3ee1faa3a371c7667fdca01e960dd294_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-5c649d8fbb03dae0703a1b70413ae82d_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-947f270aaae4164a14c9093859cf0cce_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/50/v2-296b158ebb205a2b90d05f5d2074bbe9_hd.gif">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E5%99%A8%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E8%AF%A6%E8%A7%A3/BV-Tradeoff.png">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-004df09bcc2f085c72cc0938c08b1910_hd.jpg">
<meta property="og:updated_time" content="2019-04-21T10:05:58.433Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Home">
<meta name="twitter:description" content="0. ML and NLP你可能想不到，Machine Learning and Logic Programming 有一种奇妙的关系，她们就像亲姐妹.
ML and LP

逻辑编程是什么 （X + 2 = 5 询问 X 的值，回答： X=3）类比 forward pass + back propagation
ML框架 其实是一种 程序语言 : differentiable programmi">
<meta name="twitter:image" content="https://pic4.zhimg.com/80/v2-c79815588a512fc8a9256bc7582b02e3_hd.jpg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/chatbot">SPO</a>
        
          <a class="main-nav-link" href="/deeplearning">DeepLearning</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shopee.ai"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="page-undefined" class="article article-type-page" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    <div class="article-meta">
      <!--<a href="/ai1/index.html" class="article-date">
  <time datetime="2019-04-21T10:05:58.448Z" itemprop="datePublished">2019-04-21</time>
</a>-->
      <!-- 
--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://shopee.ai/ai1/index.html#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="0-ML-and-NLP"><a href="#0-ML-and-NLP" class="headerlink" title="0. ML and NLP"></a>0. ML and NLP</h2><p>你可能想不到，Machine Learning and Logic Programming 有一种奇妙的关系，她们就像亲姐妹.</p>
<h3 id="ML-and-LP"><a href="#ML-and-LP" class="headerlink" title="ML and LP"></a>ML and LP</h3><blockquote>
<ol>
<li>逻辑编程是什么 （X + 2 = 5 询问 X 的值，回答： X=3）类比 <strong>forward pass + back propagation</strong></li>
<li><strong>ML框架</strong> 其实是一种 <strong>程序语言</strong> : differentiable programming language</li>
</ol>
<p>&nbsp;&nbsp; Pytorch 和 TensorFlow 的动态计算图和静态计算图. 对应动态静态语言。</p>
<p><strong>简单表达 &amp; 递归：</strong></p>
<ol>
<li>Feed-forward 网络，比如 CNN 一类的，对应了最简单的表达式，它只能处理图像一类具有固定长度的数据。</li>
<li>RNN（LSTM）对应的是单个递归（循环）的函数。这就是为什么 RNN 可以处理文本这类线性“链表”数据。</li>
</ol>
<p><a href="http://www.yinwang.org/blog-cn/2019/01/30/machine-learning" target="_blank" rel="external">machine learning and logic programming 机器学习与逻辑编程</a></p>
</blockquote>
<h3 id="NLP-发展方向"><a href="#NLP-发展方向" class="headerlink" title="NLP 发展方向"></a>NLP 发展方向</h3><p>未来十年将是 NLP 发展的黄金档：</p>
<blockquote>
<ol>
<li>来自各个行业的文本大数据将会更好地采集、加工、入库。</li>
<li>来自 搜索引擎、客服、商业智能、语音助手、翻译、教育 等领域对 NLP 的需求会大幅度上升。</li>
<li>文本数据和语音、图像数据的多模态融合成为未来机器人的刚需。</li>
</ol>
</blockquote>
<p>因此，NLP 研究将会向如下几个方面倾斜：</p>
<blockquote>
<ol>
<li>将知识和常识引入目前基于数据的学习系统中。</li>
<li>低资源的NLP任务的学习方法。</li>
<li>上下文建模、多轮语义理解。</li>
<li>基于语义分析、知识和常识的可解释 NLP。</li>
</ol>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-c79815588a512fc8a9256bc7582b02e3_hd.jpg" alt="趋势热点：值得关注的NLP技术"></p>
<p>除了备受关注的 NN Pre-Train 外，知识和常识的引入将大幅推动NLP技术的发展：</p>
<blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/53794989" target="_blank" rel="external">2019：迈向高能NLP之路！</a></li>
<li><a href="https://zhuanlan.zhihu.com/c_1064159241216102400" target="_blank" rel="external">高能NLP之路的专栏</a></li>
<li><a href="https://www.msra.cn/zh-cn/news/executivebylines/tech-bylines-nlp" target="_blank" rel="external">NLP将迎来黄金十年 from MSRA</a> 通篇似乎都在强调知识和常识引入对NLP乃至整个AI的重要性。</li>
</ul>
</blockquote>
<p><strong>2018 美团技术团队发表两篇重磅级文章：</strong></p>
<ul>
<li><a href="https://tech.meituan.com/2018/11/22/meituan-brain-nlp-01.html" target="_blank" rel="external">美团餐饮娱乐知识图谱——美团大脑揭秘</a></li>
<li><a href="https://tech.meituan.com/2018/11/01/meituan-ai-nlp.html" target="_blank" rel="external">美团大脑：知识图谱的建模方法及其应用</a></li>
</ul>
<blockquote>
<p>人工智能背后两大技术驱动力：<strong>深度学习和知识图谱</strong>，知识图谱就是人工智能的基础。</p>
</blockquote>
<h2 id="1-AI-算法基础"><a href="#1-AI-算法基础" class="headerlink" title="1. AI 算法基础"></a>1. AI 算法基础</h2><p><strong>1.1 防止 overfiting 的 8 条</strong></p>
<blockquote>
<p>1). get more data<br>2). Data augmentation<br>3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验)<br>4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；)<br>5). Choosing Right Network Structure<br>6). Early stopping<br>7). Model Ensumble<br>8). Batch Normalization</p>
<p>Batch Normalization 可以有效避免复杂参数对网络训练产生的影响，也可提高泛化能力.</p>
<p>神经网路的训练过程的本质是学习数据分布，如果训练数据与测试数据分布不同，将大大降低网络泛化能力， BN 是针对每一批数据，在网络的每一层输入之前增加 BN，(均值0，标准差1)。</p>
<p>Dropout 可以抑制过拟合，作用于每份小批量的训练数据，随机丢弃部分神经元机制. bagging 原理.</p>
<p><a href="https://posts.careerengine.us/p/5cae13b2d401440a7fe047af" target="_blank" rel="external">ML算法： 关于防止过拟合，整理了 8 条迭代方向</a></p>
</blockquote>
<p><strong>1.2 机器学习之类别不平衡问题</strong></p>
<p>机器学习之类别不平衡问题 (1) —— 各种评估指标</p>
<blockquote>
<p>1）过采样和欠采样；（1. 随机过采样(不太使用了，重采样往往会导致严重的过拟合)， 2. <strong>Border-line SMOTE</strong>）<br>2）修改权重（修改损失函数）；<br>3）集成方法：bagging，类似随机森林、自助采样；<br>4）多任务联合学习；</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/34473430" target="_blank" rel="external">机器学习之类别不平衡问题 (1) —— 各种评估指标</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34655990" target="_blank" rel="external">机器学习之类别不平衡问题 (2) —— ROC和PR曲线</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/41237940" target="_blank" rel="external">机器学习之类别不平衡问题 (3) —— 采样方法</a></li>
</ul>
</blockquote>
<p><strong>1.3 CrossEntropy 与 最大似然？</strong></p>
<blockquote>
<p>1）CrossEntropy lossFunction <img src="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D" alt=""></p>
<p>二分类: <img src="https://www.zhihu.com/equation?tex=g%28s%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-s%7D%7D" alt=""></p>
<p>意义：能表征 真实样本标签 和 预测概率 之间的差值</p>
<p>2）最小化交叉熵的本质就是对数似然函数的最大化；</p>
<p>3）对数似然函数的本质就是衡量在某个参数下，整体的估计和真实情况一样的概率，越大代表越相近；</p>
<p>4）损失函数的本质就是衡量预测值和真实值之间的差距，越大代表越不相近。</p>
</blockquote>
<p>Reference Article</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/38241764" target="_blank" rel="external">知乎： 简单的交叉熵损失函数，你真的懂了吗？</a></p>
<p>我们希望 log P(y|x) 越大越好，反过来，只要 log P(y|x) 的负值 -log P(y|x) 越小就行了。那我们就可以引入损失函数，且令 Loss = -log P(y|x)即可。则得到损失函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D" alt=""></p>
<p>图可以帮助我们对 CrossEntropy lossFunction 有更直观的理解。无论真实样本标签 y 是 0 还是 1，L 都表征了预测输出与 y 的差距。</p>
<p><strong>重点一提：</strong></p>
<p>预测输出与 y 差得越多，L 的值越大，也就是说对当前模型的 “ 惩罚 ” 越大，而且是非线性增大，是一种类似指数增长的级别。这是由 log 函数本身的特性所决定的。这样的好处是 模型会倾向于让预测输出更接近真实样本标签 y。</p>
<p><a href="https://zhuanlan.zhihu.com/p/26614750" target="_blank" rel="external">知乎：一文搞懂极大似然估计</a><br><a href="https://blog.csdn.net/u011508640/article/details/72815981" target="_blank" rel="external">CSDN：详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解</a></p>
<p>就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的 <font color="#c7254e"><strong>模型参数值</strong>！</font></p>
<p>对于这个函数： $P(x|θ)$</p>
<p>输入有两个：$x$ 表示某一个具体的数据；$θ$ 表示模型的参数。</p>
<p>如果 $θ$ 是已知确定的，$x$ 是变量，这个函数叫做概率函数 (probability function)，它描述对于不同的样本点 $x$，其出现概率是多少。</p>
<p>如果 $x$ 是已知确定的，$θ$ 是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现 $x$ 这个样本点的概率是多少。</p>
<p>MLE 提供了一种 <strong>给定观察数据来评估模型参数</strong> 的方法，即：“模型已定，参数未知”。</p>
<p>MLE 中 <strong>采样</strong> 需满足一个重要的假设，就是所有的采样都是 <strong>独立同分布</strong> 的.</p>
<p>一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。</p>
</blockquote>
<p><strong>1.4 SVM 和 LR 的区别与联系？</strong></p>
<blockquote>
<p>1). 对非线性表达上，LR 只能通过人工的特征组合来实现，而 SVM 可以很容易引入非线性核函数来实现非线性表达，当然也可以通过特征组合。</p>
<p>2). LR 产出的是概率值，而SVM只能产出是正类还是负类，不能产出概率。LR 的损失函数是 log loss，而 SVM 使用的是 hinge loss。</p>
<p>3). SVM 不直接依赖数据分布，而LR则依赖, SVM 主要关注的是“支持向量”，也就是和分类最相关的少数点，即关注局部关键信息；而 LR 是在全局进行优化的。这导致 SVM 天然比 LR 有<strong>更好的泛化能力</strong>，防止过拟合。</p>
<p>4). 损失函数的优化方法不同，LR 是使用 GD 来求解 <strong>对数似然函数</strong> 的最优解；SVM 使用 (Sequnential Minimal Optimal) 顺序最小优化，来求解条件约束损失函数的对偶形式。</p>
<hr>
<p>一般用线性核和高斯核，也就是Linear核与RBF核需要注意的是需要对 <strong>数据归一化处理</strong>.</p>
<p>一般情况下RBF效果是不会差于Linear但是时间上RBF会耗费更多</p>
</blockquote>
<p><strong>Andrew Ng 的见解：</strong></p>
<blockquote>
<ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ol>
</blockquote>
<p><strong>如何量化 feature number 和 sample number：</strong></p>
<blockquote>
<p>n 是feature的数量, m是样本数   </p>
<p>1). feature number &gt;&gt; sample number，则使用LR算法或者不带核函数的SVM（线性分类）<br>  &nbsp;&nbsp;&nbsp;&nbsp; feature number = 1W， sample number = 1K</p>
<p>2). <strong>fn</strong> 小， sample number <strong>一般</strong>1W，使用带有 <strong>kernel函数</strong> 的 SVM算法.  </p>
<p>3). <strong>fn</strong> 小， sample number <strong>很大</strong>5W+（n=1-1000，m=50000+）<br>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; 增加更多的 feature 然后使用LR 算法或者 not have kernel 的 SVM</p>
</blockquote>
<p><strong>1.5 ERM / SRM</strong></p>
<p>Supervised Learning Obj</p>
<p>$$<br>w^*=argmin_w\sum_iL(y_i,f(x_i;w))+\lambda\Omega(w)<br>$$</p>
<blockquote>
<ol>
<li>第一项对应模型的训练损失函数 (Square Loss、Hinge loss、Exp loss、Log loss)</li>
<li>第二项对应模型的正则化项 （模型参数向量的范数）</li>
</ol>
<p>经验风险最小化 empirical risk minimization, 结构风险最小化 structural risk minimization</p>
</blockquote>
<p>李沐曾经说过：</p>
<blockquote>
<p>model是用离散特征还是连续特征，其实是“<strong>海量离散特征+简单模型</strong>” 同 “<strong>少量连续特征+复杂模型</strong>”的权衡。</p>
<p>既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾 <strong>feature</strong> 还是折腾 <strong>model</strong> 了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</p>
</blockquote>
<p><strong>1.6 Linear vs Nonlinear classifier</strong></p>
<blockquote>
<p>线性和非线性是针对，模型参数和输入特征来讲的；</p>
<p>比如输入x，模型 y=ax+ax^2 那么就是 nonlinear model 如果输入是x和X^2则模型是线性的。</p>
<ol>
<li>Linear classifier 可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。</li>
<li>nonlinear classifier 拟合能力较强，不足之处是数据量不足容易 <strong>overfiting</strong> 、计算复杂度高、解释性不好。</li>
</ol>
<p>Linear classifier ：LR,贝叶斯分类，单层感知机、线性回归</p>
<p>nonlinear classifier：决策树、RF、GBDT、多层感知机</p>
<p>SVM 两种都有（看线性核还是高斯核）</p>
</blockquote>
<p><strong>1.7 Random Forest</strong></p>
<blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/34534004" target="_blank" rel="external">RF、bagging、boosting、GBDT、xgboost算法总结</a></li>
</ul>
<p>RF 是一个典型的多个决策树的组合分类器。</p>
<p>1). 数据的随机性选取<br>2). 待选特征的随机选取</p>
</blockquote>
<p>Sample Random： </p>
<p><img src="https://pic1.zhimg.com/80/v2-a1c3ce43528dbc274be8952c06d2b9b4_hd.jpg" alt=""></p>
<p>Feature Random：</p>
<blockquote>
<p>与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。</p>
</blockquote>
<p><img src="https://pic1.zhimg.com/80/v2-569009cc3ccd3e9922b77c1e4cbf4ca0_hd.jpg" alt=""></p>
<p><strong>1.8 GBDT</strong></p>
<p>GBDT 是以决策树（CART）为基学习器的 GB算法，是迭代树，而不是分类树。</p>
<p>一般 Boosting 算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。</p>
<p><img src="https://pic2.zhimg.com/80/v2-4713a5b63da71ef5afba3fcd3a65299d_hd.jpg" alt=""></p>
<p>GBDT 的核心就在于：<strong>每一棵树学的是之前所有树结论和的残差</strong>，这个残差就是一个加预测值后能得真实值的累加量。</p>
<p><img src="https://pic3.zhimg.com/80/v2-a384924b89b1bdd581cef7d75b56e226_hd.jpg" alt=""></p>
<p><strong>1.9 RF vs GBDT 区别</strong></p>
<blockquote>
<ol>
<li>组成 RF 的树可以是分类树，也可以是回归树；而GBDT只由回归树组成 </li>
<li>组成 RF 的树可以并行生成；而GBDT只能是串行生成 </li>
<li>对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来 </li>
<li>RF 对异常值不敏感，GBDT对异常值非常敏感 </li>
<li>RF 对训练集一视同仁，GBDT是基于权值的弱分类器的集成 </li>
<li>RF 是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能</li>
</ol>
</blockquote>
<p><strong>1.10 GBDT vs Xgboost</strong></p>
<p>Xgboost相比于GBDT来说，更加有效应用了数值优化，最重要是<strong>对损失函数</strong>（预测值和真实值的误差）<strong>变得更复杂</strong>。目标函数依然是所有树的预测值相加等于预测值。</p>
<blockquote>
<ol>
<li>二阶泰勒展开，同时用到了一阶和二阶导数</li>
<li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度</li>
<li>Shrinkage（缩减），相当于学习速率（xgboost中的eta）</li>
<li>列抽样（column subsampling）。xgboost借鉴RF做法，支持列抽样（即每次的输入特征不是全部特征)</li>
<li>并行化处理： 预先对每个特征内部进行了排序找出候选切割点.各个<strong>feature</strong>的增益计算就可以开多线程进行.</li>
</ol>
</blockquote>
<a href="!--![](https://pic2.zhimg.com/80/v2-1c0706e463f78b6036b3923048ac9149_hd.jpg)--">!--![](https://pic2.zhimg.com/80/v2-1c0706e463f78b6036b3923048ac9149_hd.jpg)--</a>
<blockquote>
<p>好的模型需要具备两个基本要素：</p>
<ol>
<li>是要有好的精度（即好的拟合程度）</li>
<li>是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定）</li>
</ol>
<p>因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项）</p>
<p>常用的误差项有平方误差和逻辑斯蒂误差，常见的惩罚项有l1，l2正则，l1正则是将模型各个元素进行求和，l2正则是对元素求平方。</p>
<p><a href="https://zhuanlan.zhihu.com/p/34534004" target="_blank" rel="external">ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结</a></p>
</blockquote>
<p><font color="#c7254e">Bagging &amp; Boosting</font> 的理念：</p>
<p>Bagging 的思想比较简单，即每一次从原始数据中根据 <strong>均匀概率分布有放回的抽取和原始数据大小相同的样本集合</strong>，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。</p>
<p>boosting 的每一次抽样的 <strong>样本分布都是不一样</strong> 的。每一次迭代，都根据上一次迭代的结果，<strong>增加被错误分类的样本的权重</strong>，使得模型能在之后的迭代中更加注意到 <strong>难以分类的样本</strong>，这是一个 <strong>不断学习的过程，也是一个不断提升</strong> 的过程，这也就是boosting思想的本质所在。 迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。</p>
<p><img src="https://pic4.zhimg.com/80/v2-aca3644ddd56abe1e47c0f45601587c3_hd.jpg" alt=""></p>
<p><strong>1.11 Evaluation Metric 对比 ?</strong></p>
<p>Confusion Matrix</p>
<p><strong>1). accuracy</strong> (正反比例严重失衡，则没意义，存在 accuracy paradox 现象)</p>
<p><img src="https://pic1.zhimg.com/80/v2-492ac29bbed274a282eee069c0b63c93_hd.jpg" alt=""></p>
<blockquote>
<p>accuracy 准确率 = (TP+TN)/(TP+TN+FP+FN), <strong>准确率可以判断总的正确率</strong></p>
</blockquote>
<p><strong>2). precision</strong> 查准率 (80% = 你一共预测了100个正例，80个是对的正例)</p>
<p><img src="https://pic2.zhimg.com/80/v2-3ee1faa3a371c7667fdca01e960dd294_hd.jpg" alt=""></p>
<blockquote>
<p>Precision = TP/(TP+FP)</p>
</blockquote>
<p><strong>3). recall</strong> (样本中的正例有多少被预测正确 TPR = TP/(TP+FN))</p>
<p><img src="https://pic1.zhimg.com/80/v2-5c649d8fbb03dae0703a1b70413ae82d_hd.jpg" alt=""></p>
<p><strong>4). F1-score</strong> （precision 和 recall 的 metric）</p>
<blockquote>
<p>2*precision*recall / (precision + recall)</p>
</blockquote>
<p><strong>5). P-R（precision-recall）PRC</strong></p>
<blockquote>
<p>依靠 LR 举例:</p>
<p>这条曲线是根据什么变化的？为什么是这个形状的曲线？</p>
<p>这个阈值是我们随便定义的，我们并不知道这个阈值是否符合我们的要求</p>
<p>遍历 0 到 1 之间所有的阈值, 得到了这条曲线</p>
</blockquote>
<p><strong>5). ROC curve</strong> （TPR 纵轴，FPR 横轴，TP（真正率）和 FP（假正率），设一个阈值）</p>
<blockquote>
<p>ROC（Receiver Operating Characteristic）曲线。 ROC 曲线 是基于混淆矩阵得出的。 </p>
<p>TPR = recall = 灵敏度 = P（X=1 | Y=1）<br>FPR = 特异度 = P（X=0 | Y=0）</p>
</blockquote>
<p><img src="https://pic3.zhimg.com/80/v2-947f270aaae4164a14c9093859cf0cce_hd.jpg" alt=""></p>
<p><strong>ROC曲线的阈值问题:</strong></p>
<blockquote>
<p>与前面的P-R曲线类似，ROC曲线也是通过遍历所有阈值来绘制整条曲线的。 </p>
</blockquote>
<p><img src="https://pic4.zhimg.com/50/v2-296b158ebb205a2b90d05f5d2074bbe9_hd.gif" alt=""></p>
<p><strong>ROC曲线无视样本不平衡</strong></p>
<blockquote>
<ul>
<li><a href="https://www.zhihu.com/question/30643044" target="_blank" rel="external">精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34655990" target="_blank" rel="external">机器学习之类别不平衡问题 (2) —— ROC和PR曲线</a></li>
</ul>
</blockquote>
<p><strong>6). AUC</strong>  (AUC = 0.5，跟随机猜测一样， ROC 纵轴 TPR 越大， 横轴 FPR 越小 模型越好）</p>
<blockquote>
<p>0.5 - 0.7：效果较低，但用于预测股票已经很不错了<br>0.7 - 0.85：效果一般<br>0.85 - 0.95：效果很好</p>
<p>real world data 经常会面临 class imbalance 问题，即正负样本比例失衡。</p>
<p>根据计算公式可以推知，在 testing set 出现 imbalance 时 ROC曲线 能保持不变，而 PR 则会出现大变化。</p>
</blockquote>
<p><strong>7). multi-class classification</strong> 如果非要用一个综合考量的 metric 的话，</p>
<blockquote>
<ol>
<li>macro-average（宏平均）- 分布计算每个类别的F1，然后做平均（各类别F1的权重相同）</li>
<li>micro-average（微平均）- 通过先计算总体的TP，FN和FP的数量，再计算F1</li>
</ol>
<p>macro-average（宏平均） 会比 micro-average（微平均）好一些哦，因为 macro 会受 minority class 影响更大，也就是说更能体现在 small class 上的 performance.</p>
<p><a href="https://www.cnblogs.com/techengin/p/8962024.html" target="_blank" rel="external">sklearn中 F1-micro 与 F1-macro 区别和计算原理</a></p>
</blockquote>
<p>precision &amp; recall</p>
<blockquote>
<p>precision 是相对你自己的模型预测而言<br>recall 是相对真实的答案而言</p>
</blockquote>
<p><strong>1.12 Bias-Variance Tradeoff</strong></p>
<p><img src="https://charlesliuyx.github.io/2017/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E5%99%A8%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E8%AF%A6%E8%A7%A3/BV-Tradeoff.png" width="450"></p>
<blockquote>
<ul>
<li><a href="https://charlesliuyx.github.io" target="_blank" rel="external">Bias-Variance Tradeoff</a></li>
</ul>
</blockquote>
<p><strong>1.13 激活函数的对比？</strong></p>
<blockquote>
<ol>
<li>Sigmoid 和 Tanh 为什么会导致 Vanishing/Exploding gradients ? </li>
<li>Tanh 值域 (-1,1) Sigmoid 值域 (0,1)</li>
<li>ReLU 的优点，和局限性分别是什么? </li>
<li><a href="https://zhuanlan.zhihu.com/p/48776056" target="_blank" rel="external">谈谈激活函数 Sigmoid,Tanh,ReLu,softplus,softmax</a></li>
</ol>
</blockquote>
<p><strong>1.14 sigmoid 用作激活函数时，分类为什么要用 crossentropy loss，而不用均方损失？</strong></p>
<blockquote>
<ol>
<li>softmax函数可以看做是Sigmoid函数的一般化，可以进行多分类。</li>
<li>非常适合用于<code>分类</code>问题： <code>Cross Entropy</code> 交叉熵损失函数</li>
<li>Square error loss function 与 Cross Entropy Error Function 分别适合什么景？</li>
</ol>
</blockquote>
<p><strong>1.15 InfoEntropy vs Crossentropy</strong></p>
<blockquote>
<p>InfoEntropy，代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。</p>
<p>Crossentropy，用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力值。</p>
</blockquote>
<h2 id="2-NLP高频问题"><a href="#2-NLP高频问题" class="headerlink" title="2. NLP高频问题"></a>2. NLP高频问题</h2><h3 id="2-1-word2vec-vs-NNLM"><a href="#2-1-word2vec-vs-NNLM" class="headerlink" title="2.1 word2vec vs NNLM"></a>2.1 word2vec vs NNLM</h3><blockquote>
<p>1）其本质都可以看作是 Language Model；</p>
<p>2）词向量只不过 NNLM 一个产物， word2vec 虽然其本质也是 Language Model，但是其专注于<strong>词向量本身</strong>，因此做了许多优化来提高计算效率：</p>
<p>与 NNLM 相比，词向量直接sum，不再拼接，并舍弃隐层；<br>考虑到 sofmax归一化 需要遍历整个词汇表，采用 hierarchical softmax 和 negative sampling 进行优化</p>
<ol>
<li>hierarchical softmax 实质上生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；</li>
<li>negative sampling 更为直接，实质上对每一个样本中每一个词都进行负例采样；</li>
</ol>
</blockquote>
<h3 id="2-2-negative-sampling"><a href="#2-2-negative-sampling" class="headerlink" title="2.2 negative sampling"></a>2.2 negative sampling</h3><p>负采样这个点引入 word2vec 非常巧妙，两个作用，</p>
<blockquote>
<ol>
<li>加速了模型计算</li>
<li>保证了模型训练的效果</li>
</ol>
<p>第一，model 每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢。</p>
<p>第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点聪明.</p>
</blockquote>
<ul>
<li><a href="https://www.cnblogs.com/pinard/p/7249903.html" target="_blank" rel="external">word2vec Negative Sampling 刘建平Pinard</a></li>
</ul>
<h3 id="2-3-word2vec-vs-fastText"><a href="#2-3-word2vec-vs-fastText" class="headerlink" title="2.3 word2vec vs fastText"></a>2.3 word2vec vs fastText</h3><blockquote>
<ol>
<li>都可以无监督学习词向量， fastText 训练词向量时会考虑 subword；</li>
<li>fastText 还可以进行有监督学习进行文本分类</li>
</ol>
</blockquote>
<p>其主要特点：</p>
<blockquote>
<ul>
<li>结构与CBOW类似，但学习目标是人工标注的分类结果；</li>
<li>采用 hierarchical softmax 对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径；</li>
<li>引入 N-gram，考虑词序特征；</li>
<li>引入 subword 来处理长词，处理未登陆词问题；</li>
</ul>
</blockquote>
<h3 id="2-4-word2vec-vs-glove"><a href="#2-4-word2vec-vs-glove" class="headerlink" title="2.4 word2vec vs glove"></a>2.4 word2vec vs glove</h3><blockquote>
<ul>
<li>word2vec 是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。</li>
</ul>
<ul>
<li>word2vec 是无监督学习，同样由于不需要人工标注；glove通常被认为是无监督学习，但实际上glove还是有label的，即共现次数log(X_{ij})。</li>
</ul>
<ul>
<li>word2vec 损失函数实质上是带权重的<strong>crossentrpy</strong>，权重固定；glove的损失函数是最小<strong>平方损失函数</strong>，权重可以做映射变换。</li>
</ul>
<ul>
<li>总体来看，glove 可以被看作是更换了目标函数和权重函数的全局 word2vec。</li>
</ul>
</blockquote>
<h3 id="2-5-ELMO-vs-GPT-vs-BERT"><a href="#2-5-ELMO-vs-GPT-vs-BERT" class="headerlink" title="2.5 ELMO vs GPT vs BERT"></a>2.5 ELMO vs GPT vs BERT</h3><p><img src="https://pic1.zhimg.com/80/v2-004df09bcc2f085c72cc0938c08b1910_hd.jpg" alt=""></p>
<p>之前介绍词向量均是静态的词向量，无法解决一次多义等问题。下面介绍三种elmo、GPT、bert词向量，它们都是基于语言模型的动态词向量。下面从几个方面对这三者进行对比：</p>
<p>（1）<strong>特征提取器</strong>：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。</p>
<p>（2）<strong>单/双向语言模型</strong>：</p>
<p>GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。<br>GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子。</p>
<h3 id="2-6-RNN-vs-LSTM-vs-GRU"><a href="#2-6-RNN-vs-LSTM-vs-GRU" class="headerlink" title="2.6 RNN vs LSTM vs GRU"></a>2.6 RNN vs LSTM vs GRU</h3><ul>
<li><p>GRU 和 LSTM 的性能在很多任务上不分伯仲。</p>
</li>
<li><p>GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。</p>
</li>
</ul>
<blockquote>
<p>从结构上来说：</p>
<ul>
<li>GRU 只有两个门（update和reset），LSTM 有三个门（forget，input，output）</li>
<li>GRU 直接将 hidden state 传给下一个单元，而 LSTM 则用 memory cell 把hidden state 包装起来。</li>
</ul>
</blockquote>
<h3 id="2-7-RNN-vs-CNN"><a href="#2-7-RNN-vs-CNN" class="headerlink" title="2.7 RNN vs CNN"></a>2.7 RNN vs CNN</h3><blockquote>
<ol>
<li>RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。</li>
<li>RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。</li>
<li>CNN 优点： 提取数据中的局部位置的特征，然后再拼接池化层。 CNN可以更好地捕捉文本的语义。是O(n)</li>
<li>CNN 优点： 一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。</li>
</ol>
</blockquote>
<h3 id="2-8-Attention"><a href="#2-8-Attention" class="headerlink" title="2.8 Attention"></a>2.8 Attention</h3><p>除此之外模型为了取得比较好的效果还是用了下面三个小技巧来改善性能：</p>
<blockquote>
<p>深层次的LSTM：作者使用了4层LSTM作为encoder和decoder模型，并且表示深层次的模型比shallow的模型效果要好（单层，神经元个数多）。</p>
<p>将source进行反序输入：输入的时候将“ABC”变成“CBA”，这样做的好处是解决了长序列的long-term依赖，使得模型可以学习到更多的对应关系，从而达到比较好的效果。</p>
<p>Beam Search：这是在test时的技巧，也就是在训练过程中不会使用。</p>
<p>一般来讲我们会采用greedy贪婪式的序列生成方法，也就是每一步都取概率最大的元素作为当前输出，但是这样的缺点就是一旦某一个输出选错了，可能就会导致最终结果出错，所以使用beam search的方法来改善。也就是每一步都取概率最大的k个序列（beam size），并作为下一次的输入。更详细的解释和例子可以参考下面这个链接：<a href="https://zhuanlan.zhihu.com/p/28048246" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/28048246</a></p>
</blockquote>
<h3 id="2-9-文本分类任务-tricks"><a href="#2-9-文本分类任务-tricks" class="headerlink" title="2.9 文本分类任务 tricks"></a>2.9 文本分类任务 tricks</h3><p>在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的tricks？</p>
<blockquote>
<ol>
<li>数据预处理时vocab的选取（前N个高频词或者过滤掉出现次数小于3的词等等）</li>
<li>词向量的选择，可以使用预训练好的词向量如谷歌、facebook开源出来的，当训练集比较大的时候也可以进行微调或者随机初始化与训练同时进行。训练集较小时就别微调了</li>
<li>结合要使用的模型，这里可以把数据处理成char、word或者都用等</li>
<li>有时将词性标注信息也加入训练数据会收到比较好的效果</li>
<li>至于PAD的话，取均值或者一个稍微比较大的数，但是别取最大值那种应该都还好</li>
<li>神经网络结构的话到没有什么要说的，可以多试几种比如fastText、TextCNN、RCNN、char-CNN/RNN、HAN等等。哦，对了，加上dropout和BN可能会有意外收获。反正模型这块还是要具体问题具体分析吧，根据自己的需求对模型进行修改（比如之前参加知乎竞赛的时候，最终的分类标签也有文本描述，所以就可以把这部分信息也加到模型之中等等）</li>
<li>超参数的话，推荐看看之前TextCNN的一篇论文，个人感觉足够了“A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification”</li>
<li>之前还见别人在文本领域用过数据增强的方法，就是对文本进行随机的shuffle和drop等操作来增加数据量</li>
</ol>
</blockquote>
<p>这个看似不重要，其实确实很重要的点。一开我以为 padding 的最大长度取整个评论平均的长度的2倍差不多就可以啦(对于char level 而言，max_length 取 400左右)，但是会发现效果上不去，当时将 max_length 改为 1000 之后，macro f-score提示明显，我个人认为是在多分类问题中，那些长度很长的评论可能会有部分属于那些样本数很少的类别，padding过短会导致这些长评论无法被正确划分。</p>
<h2 id="3-其他算法问题"><a href="#3-其他算法问题" class="headerlink" title="3. 其他算法问题"></a>3. 其他算法问题</h2><p>1、怎么进行单个样本的学习？<br>2、 决策树 bagging boosting adaboost 区别？RF的特征随机目的是什么？<br>3、transformer各部分怎么用？Q K V怎么计算；Attention怎么用？<br>4、HMM 假设是什么？CRF解决了什么问题？CRF做过特征工程吗？HMM中的矩阵意义？5、说以一下空洞卷积？膨胀卷积怎么理解？什么是Piece-CNN？<br>6、怎么解决beam-search局部最优问题？global embedding 怎么做？<br>7、数学题：什么是半正定矩阵？机器学习中有什么应用？<br>8、卷积的物理意义是什么？傅里叶变换懂吗？<br>9、说一下Bert？<br>10、推导word2vec？<br>11、怎么理解传统的统计语言模型？现在的神经网络语言模型有什么不同？<br>12、神经网络优化的难点是什么？这个问题要展开来谈。<br>13、attention你知道哪些？<br>14、自动文章摘要抽取时，怎么对一篇文章进行分割？（从序列标注、无监督等角度思考）<br>15、在做NER任务时，lstm后面可以不用加CRF吗？<br>16、通过画图描述TextRank？<br>17、LDA和pLSA有什么区别？<br>18、Transformer在实际应用中都会有哪些做法？<br>19、讲出过拟合的解决方案？<br>20、说一下transforemr、LSTM、CNN间的区别？从多个角度进行讲解？<br>21、梯度消失的原因和解决办法有哪些？<br>22、数学题：贝叶斯计算概率？<br>23、数学题：25只兔子赛跑问题，共5个赛道，最少几次比赛可以选出前5名？<br>24、数学题：100盏灯问题？</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/57153934" target="_blank" rel="external">【NLP/AI算法面试必备-2】NLP/AI面试全记录（持续更新）</a></li>
<li><a href="https://www.zhihu.com/people/lou-jie-9/posts" target="_blank" rel="external">【NLP/AI算法面试必备-1】学习NLP/AI，必须深入理解“神经网络及其优化问题”</a></li>
<li><a href="https://www.zhihu.com/people/lou-jie-9/posts" target="_blank" rel="external">JayLouNLP算法工程师</a></li>
</ul>

      
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-ML-and-NLP"><span class="toc-number"></span> <span class="toc-text">0. ML and NLP</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-and-LP"><span class="toc-number"></span> <span class="toc-text">ML and LP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NLP-发展方向"><span class="toc-number"></span> <span class="toc-text">NLP 发展方向</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-AI-算法基础"><span class="toc-number"></span> <span class="toc-text">1. AI 算法基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-NLP高频问题"><span class="toc-number"></span> <span class="toc-text">2. NLP高频问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-word2vec-vs-NNLM"><span class="toc-number"></span> <span class="toc-text">2.1 word2vec vs NNLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-negative-sampling"><span class="toc-number"></span> <span class="toc-text">2.2 negative sampling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-word2vec-vs-fastText"><span class="toc-number"></span> <span class="toc-text">2.3 word2vec vs fastText</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-word2vec-vs-glove"><span class="toc-number"></span> <span class="toc-text">2.4 word2vec vs glove</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-ELMO-vs-GPT-vs-BERT"><span class="toc-number"></span> <span class="toc-text">2.5 ELMO vs GPT vs BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-RNN-vs-LSTM-vs-GRU"><span class="toc-number"></span> <span class="toc-text">2.6 RNN vs LSTM vs GRU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-RNN-vs-CNN"><span class="toc-number"></span> <span class="toc-text">2.7 RNN vs CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-8-Attention"><span class="toc-number"></span> <span class="toc-text">2.8 Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-文本分类任务-tricks"><span class="toc-number"></span> <span class="toc-text">2.9 文本分类任务 tricks</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-其他算法问题"><span class="toc-number"></span> <span class="toc-text">3. 其他算法问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number"></span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        

      </footer>
    
  </div>
  
    
  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://shopee.ai/ai1/index.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
