<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>理论 seq2seq+Attention 机制模型详解 - Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="理论 seq2seq+Attention 机制模型详解">
<meta property="og:url" content="http://iequa.com/2017/11/17/nlp/seq2seq+Attention/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。">
<meta property="og:image" content="http://iequa.com/images/chatbot/seq2seq-1.jpg">
<meta property="og:image" content="http://iequa.com/images/chatbot/seq2seq-5.jpeg">
<meta property="og:image" content="http://iequa.com/images/chatbot/seq2seq-6.png">
<meta property="og:image" content="http://iequa.com/images/chatbot/seq2seq-2.jpg">
<meta property="og:image" content="http://iequa.com/images/chatbot/seq2seq-3.jpg">
<meta property="og:image" content="http://iequa.com/images/chatbot/seq2seq-4.jpg">
<meta property="og:image" content="http://iequa.com/images/chatbot/seq2seq-7.jpeg">
<meta property="og:updated_time" content="2018-12-28T14:04:22.828Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="理论 seq2seq+Attention 机制模型详解">
<meta name="twitter:description" content="从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。">
<meta name="twitter:image" content="http://iequa.com/images/chatbot/seq2seq-1.jpg">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/chatbot">Bot</a>
        
          <a class="main-nav-link" href="/deeplearning">Deep Learning</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-nlp/seq2seq+Attention" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      理论 seq2seq+Attention 机制模型详解
      <small class=article-detail-date-index>&nbsp; 2017-11-17</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2017/11/17/nlp/seq2seq+Attention/" class="article-date">
  <time datetime="2017-11-17T12:00:21.000Z" itemprop="datePublished">2017-11-17</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/nlp/">nlp</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2017/11/17/nlp/seq2seq+Attention/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。</p>
<a id="more"></a>
<ul>
<li>Seq-to-Seq 框架1</li>
<li>Seq-to-Seq 框架2（teacher forcing）</li>
<li>Seq-to-Seq with Attention（NMT）</li>
<li>Seq-to-Seq with Attention 各种变形</li>
<li>Seq-to-Seq with Beam-Search</li>
</ul>
<p>当输入输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder）<a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">1</a> 或者 seq2seq 模型 <a href="https://zh.gluon.ai/chapter_recurrent-neural-networks/gru.html" target="_blank" rel="external">2</a>。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。</p>
<h2 id="1-Seq2Seq-框架1"><a href="#1-Seq2Seq-框架1" class="headerlink" title="1. Seq2Seq 框架1"></a>1. Seq2Seq 框架1</h2><p><img src="/images/chatbot/seq2seq-1.jpg" width="300"></p>
<p><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p>
<h3 id="1-1-encode-编码器"><a href="#1-1-encode-编码器" class="headerlink" title="1.1 encode 编码器"></a>1.1 encode 编码器</h3><p>编码器的作用是把一个不定长的输入序列变换成一个定长的背景变量 $c$，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。</p>
<p>让我们考虑批量大小为 1 的时序数据样本。假设输入序列是 $x_1,\ldots,x_T$, 例如 $x_i$ 是输入句子中的第 $i$ 个词。在时间步 $t$，循环神经网络将输入 $x_t$ 的特征向量 $x_t$ 和上个时间步的隐藏状态 $\boldsymbol{h}_{t-1}$ 变换为当前时间步的隐藏状态 $h_t$。我们可以用函数 $f$ 表达循环神经网络隐藏层的变换：</p>
<p>$$<br>\boldsymbol{h}_t = f(\boldsymbol{x}_t, \boldsymbol{h}_{t-1}).<br>$$</p>
<p>接下来编码器通过自定义函数 $q$ 将各个时间步的隐藏状态变换为背景变量</p>
<p>$$<br>\boldsymbol{c} =  q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T).<br>$$</p>
<p>例如，当选择 $q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) = \boldsymbol{h}_T$ 时，背景变量是输入序列最终时间步的隐藏状态 $\boldsymbol{h}_T$。</p>
<p>以上描述的编码器是一个单向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用 Bi-RNN 构造编码器。这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。</p>
<p><img src="/images/chatbot/seq2seq-5.jpeg" width="800"></p>
<h3 id="1-2-decode-解码器"><a href="#1-2-decode-解码器" class="headerlink" title="1.2 decode 解码器"></a>1.2 decode 解码器</h3><p>Encode 编码器输出的背景变量 $c$ 编码了整个输入序列 $x_1, \ldots, x_T$ 的信息。给定训练样本中的输出序列 $y_1, y_2, \ldots, y_{T’}$，对每个时间步 $t’$（符号与输入序列或编码器的时间步 $t$ 有区别）， 解码器输出 $y_{t’}$ 的条件概率将基于之前的输出序列 $y_1,\ldots,y_{t’-1}$ 和背景变量 $c$，<strong>即</strong> $\mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c})$。</p>
<p>为此，我们可以使用<code>另一个RNN</code>作为解码器。 在输出序列的时间步 $t^\prime$，解码器将上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $c$ 作为输入，并将它们与上一时间步的隐藏状态 $\boldsymbol{h}_{t^\prime-1}$ 变换为当前时间步的隐藏状态 $\boldsymbol{h}_{t^\prime}$。因此，我们可以用函数 $g$ 表达解码器隐藏层的变换：</p>
<p>$$<br>\boldsymbol{h}_{t^\prime} = g(y_{t^\prime-1}, \boldsymbol{c}, \boldsymbol{h}_{t^\prime-1}).<br>$$</p>
<p>有了decode的隐藏状态后，我们可以使用自定义的输出层和 softmax 运算来计算 $\mathbb{P}(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})$，例如基于当前时间步的解码器隐藏状态 $\boldsymbol{h}_{t^\prime}$、上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $c$ 来计算当前时间步输出 $y_{t^\prime}$ 的概率分布。</p>
<h3 id="1-3-train-模型训练"><a href="#1-3-train-模型训练" class="headerlink" title="1.3 train 模型训练"></a>1.3 train 模型训练</h3><p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>
<p>$$<br>\begin{split}\begin{aligned}<br>\mathbb{P}(y_1, \ldots, y_{T’} \mid x_1, \ldots, x_T)<br>&amp;= \prod_{t’=1}^{T’} \mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, x_1, \ldots, x_T)\\<br>&amp;= \prod_{t’=1}^{T’} \mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c}),<br>\end{aligned}\end{split}<br>$$</p>
<p>并得到该输出序列的损失</p>
<p>$$ - \log\mathbb{P}(y_1, \ldots, y_{T’} \mid x_1, \ldots, x_T) = -\sum_{t’=1}^{T’} \log \mathbb{P}(y_{t’} \mid y_1,  \ldots, y_{t’-1}, \boldsymbol{c}),<br>$$</p>
<p><img src="/images/chatbot/seq2seq-6.png" width="800"></p>
<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图中所描述的模型预测中，我们需要将decode在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列在上一个时间步的标签作为decode在当前时间步的输入。这叫做强制教学（teacher forcing）。</p>
<h3 id="1-4-小结"><a href="#1-4-小结" class="headerlink" title="1.4 小结"></a>1.4 小结</h3><ul>
<li>编码器 - 解码器（seq2seq）可以输入并输出不定长的序列。</li>
<li>编码器—解码器使用了两个循环神经网络。</li>
<li>在编码器—解码器的训练中，我们可以采用强制教学。 （这也是 Seq2Seq 2 的内容）</li>
</ul>
<h2 id="2-Seq2Seq-框架2"><a href="#2-Seq2Seq-框架2" class="headerlink" title="2. Seq2Seq 框架2"></a>2. Seq2Seq 框架2</h2><p>第二个要讲的Seq-to-Seq模型来自于 “<a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a>”，其模型结构图如下所示：</p>
<p><img src="/images/chatbot/seq2seq-2.jpg" width="600"></p>
<p>与上面模型最大的区别在于其source编码后的 向量$C$ 直接作为Decoder阶段RNN的初始化state，而不是在每次decode时都作为<code>RNN cell</code>的输入。此外，decode时RNN的输入是目标值，而不是前一时刻的输出。首先看一下编码阶段：</p>
<p><img src="/images/chatbot/seq2seq-3.jpg" width="500"></p>
<p>就是简单的RNN模型，每个词经过RNN之后都会编码为hidden state（e0,e1,e2），并且source序列的编码向量e就是最终的hidden state e2。接下来再看一下解码阶段：</p>
<p><img src="/images/chatbot/seq2seq-4.jpg" width="500"></p>
<p>e向量仅作为RNN的初始化状态传入decode模型。接下来就是标准的循环神经网络，每一时刻输入都是前一时刻的正确label。直到最终输入<eos>符号截止滚动。</eos></p>
<h2 id="3-Seq2Seq-Attention"><a href="#3-Seq2Seq-Attention" class="headerlink" title="3. Seq2Seq Attention"></a>3. Seq2Seq Attention</h2><p><strong>decode</strong> 在各个时间步依赖相同的 <strong>背景变量 $c$</strong> 来获取输入序列信息。当 <strong>encode</strong> 为 RNN 时，<strong>背景变量$c$</strong> 来自它最终时间步的隐藏状态。</p>
<blockquote>
<p>英语输入：“They”、“are”、“watching”、“.”<br>法语输出：“Ils”、“regardent”、“.”</p>
<p>翻译例子：输入为英语序列“They”、“are”、“watching”、“.”，输出为法语序列“Ils”、“regardent”、“.”。，<strong>decode</strong> 在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步 1，解码器可以主要依赖“They”、“are”的信息来生成“Ils”，在时间步 2 则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步 3 则直接映射句号“.”。这看上去就像是在 <strong>decode</strong> 的每一时间步对输入序列中不同时间步的编码信息分配不同的注意力一样。这也是注意力机制的由来 <a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">1</a>。</p>
<p>仍以 RNN 为例，Attention 通过对 Encode 所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量$c$。Decode 在每一时间步调整这些权重，即 Attention weight，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量$c$。本节我们将讨论 Attention机制 是怎么工作的。</p>
</blockquote>
<p>在“编码器—解码器（seq2seq）”, 解码器在时间步 $t’$ 的隐藏状态</p>
<p>$$<br>\boldsymbol{s}_{t’} = g(\boldsymbol{y}_{t’-1}, \boldsymbol{c}, \boldsymbol{s}_{t’-1})<br>$$</p>
<p>在 Attention机制 中, 解码器的每一时间步将使用可变的背景变量$c$</p>
<p>$$<br>\boldsymbol{s}_{t’} = g(\boldsymbol{y}_{t’-1}, \boldsymbol{c}_{t’}, \boldsymbol{s}_{t’-1}).<br>$$</p>
<p>关键是如何计算背景变量 $\boldsymbol{c}_{t’}$ 和如何利用它来更新隐藏状态 $\boldsymbol{s}_{t’}$。以下将分别描述这两个关键点。</p>
<h3 id="3-1-计算背景变量-c"><a href="#3-1-计算背景变量-c" class="headerlink" title="3.1 计算背景变量 c"></a>3.1 计算背景变量 c</h3><p>$$<br>\boldsymbol{c}_{t’} = \sum_{t=1}^T \alpha_{t’ t} \boldsymbol{h}_t,<br>$$</p>
<p>其中给定 $t’$ 时，权重 $\alpha_{t’ t}$ 在 $t=1,\ldots,T$ 的值是一个概率分布。为了得到概率分布，可以使用 softmax 运算:</p>
<p>$$<br>\alpha_{t’ t} = \frac{\exp(e_{t’ t})}{ \sum_{k=1}^T \exp(e_{t’ k}) },\quad t=1,\ldots,T.<br>$$</p>
<p>现在，我们需要定义如何计算上式中 softmax 运算的输入 $e_{t’ t}$。由于 $e_{t’ t}$ 同时取决于decode的时间步 $t’$ 和encode的时间步 $t$，我们不妨以解码器在时间步 $t’−1$ 的隐藏状态 $\boldsymbol{s}_{t’ - 1}$ 与编码器在时间步 $t$ 的隐藏状态 $h_t$ 为输入，并通过函数 $a$ 计算 $e_{t’ t}$：</p>
<p>$$<br>e_{t’ t} = a(\boldsymbol{s}_{t’ - 1}, \boldsymbol{h}_t).<br>$$</p>
<p>这里函数 a 有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积 $a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出Attention机制的论文则将输入连结后通过含单隐藏层的多层感知机MLP 变换 </p>
<p>$$<br>a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),<br>$$</p>
<p>其中 $v、W_s、W_h$ 都是可以学习的模型参数。</p>
<h3 id="3-2-更新隐藏状态"><a href="#3-2-更新隐藏状态" class="headerlink" title="3.2 更新隐藏状态"></a>3.2 更新隐藏状态</h3><p>以门控循环单元为例，在解码器中我们可以对门控循环单元的设计稍作修改。解码器在时间步 $t’$ 的隐藏状态为</p>
<p>$$<br>\boldsymbol{s}_{t’} = \boldsymbol{z}_{t’} \odot \boldsymbol{s}_{t’-1}  + (1 - \boldsymbol{z}_{t’}) \odot \tilde{\boldsymbol{s}}_{t’},<br>$$</p>
<p>其中的重置门、更新门和候选隐含状态分别为 :</p>
<p>$$<br>\begin{split}\begin{aligned}<br>\boldsymbol{r}_{t’} &amp;= \sigma(\boldsymbol{W}_{yr} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{sr} \boldsymbol{s}_{t’ - 1} + \boldsymbol{W}_{cr} \boldsymbol{c}_{t’} + \boldsymbol{b}_r),\\<br>\boldsymbol{z}_{t’} &amp;= \sigma(\boldsymbol{W}_{yz} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{sz} \boldsymbol{s}_{t’ - 1} + \boldsymbol{W}_{cz} \boldsymbol{c}_{t’} + \boldsymbol{b}_z),\\<br>\tilde{\boldsymbol{s}}_{t’} &amp;= \text{tanh}(\boldsymbol{W}_{ys} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{ss} (\boldsymbol{s}_{t’ - 1} \odot \boldsymbol{r}_{t’}) + \boldsymbol{W}_{cs} \boldsymbol{c}_{t’} + \boldsymbol{b}_s),<br>\end{aligned}\end{split}<br>$$</p>
<p>其中含下标的 W 和 b 分别为门控循环单元的权重参数和偏差参数。</p>
<p><img src="/images/chatbot/seq2seq-7.jpeg" width="800"></p>
<h3 id="3-3-小结"><a href="#3-3-小结" class="headerlink" title="3.3 小结"></a>3.3 小结</h3><ul>
<li>可以在decode的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。</li>
<li>Attention机制可以采用更为高效的矢量化计算。</li>
</ul>
<h2 id="4-Seq2Seq-Attention各种变形"><a href="#4-Seq2Seq-Attention各种变形" class="headerlink" title="4. Seq2Seq Attention各种变形"></a>4. Seq2Seq Attention各种变形</h2><p>第四个Seq-to-Seq模型，来自于论文 <a href="http://link.zhihu.com/?target=http%3A//aclweb.org/anthology/D15-1166" target="_blank" rel="external">Effective Approaches to Attention-based Neural Machine Translation</a> 这篇论文提出了两种 Seq2Seq模型 分别是global Attention 和 local Attention。</p>
<h2 id="5-Seq2Seq-with-Beam-Search"><a href="#5-Seq2Seq-with-Beam-Search" class="headerlink" title="5. Seq2Seq with Beam-Search"></a>5. Seq2Seq with Beam-Search</h2><p>上面讲的几种Seq2Seq模型都是从模型结构上进行的改进，也就说为了从训练的层面上改善模型的效果，但这里要介绍的beam-search是在测试的时候才用到的技术。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制</a></li>
<li><a href="https://zh.gluon.ai/chapter_recurrent-neural-networks/gru.html" target="_blank" rel="external">门控循环单元（GRU）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32092871" target="_blank" rel="external">seq2seq+Attention机制模型详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/38064637" target="_blank" rel="external">三分钟带你对 Softmax 划重点</a></li>
<li><a href="https://zh.gluon.ai/chapter_deep-learning-basics/softmax-regression.html" target="_blank" rel="external">Softmax 回归</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_weibo_icon.png);background-size: contain;display: inline-block; width:50px; height:50px" href="https://service.weibo.com/share/share.php?url" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support-pay-blog/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support-pay-blog">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Seq2Seq-框架1"><span class="toc-number"></span> <span class="toc-text">1. Seq2Seq 框架1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-encode-编码器"><span class="toc-number"></span> <span class="toc-text">1.1 encode 编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-decode-解码器"><span class="toc-number"></span> <span class="toc-text">1.2 decode 解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-train-模型训练"><span class="toc-number"></span> <span class="toc-text">1.3 train 模型训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-小结"><span class="toc-number"></span> <span class="toc-text">1.4 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Seq2Seq-框架2"><span class="toc-number"></span> <span class="toc-text">2. Seq2Seq 框架2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Seq2Seq-Attention"><span class="toc-number"></span> <span class="toc-text">3. Seq2Seq Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-计算背景变量-c"><span class="toc-number"></span> <span class="toc-text">3.1 计算背景变量 c</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-更新隐藏状态"><span class="toc-number"></span> <span class="toc-text">3.2 更新隐藏状态</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-小结"><span class="toc-number"></span> <span class="toc-text">3.3 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Seq2Seq-Attention各种变形"><span class="toc-number"></span> <span class="toc-text">4. Seq2Seq Attention各种变形</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Seq2Seq-with-Beam-Search"><span class="toc-number"></span> <span class="toc-text">5. Seq2Seq with Beam-Search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number"></span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/nlp/">nlp</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/seq2seq-Attention/">seq2seq+Attention</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/11/17/chatbot/chatbot-research8/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Chatbot Research 8 - 理论 seq2seq+Attention 机制模型详解
        
      </div>
    </a>
  
  
    <a href="/2017/11/14/nlp/hidden-markov-model/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Hidden Markov Model&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Libin Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2017/11/17/nlp/seq2seq+Attention/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
