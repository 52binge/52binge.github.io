<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>NLP 简介 &amp; 统计语言模型 - Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP 简介 & 统计语言模型">
<meta property="og:url" content="http://iequa.com/2017/11/13/nlp-pre-statistics-language-model/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
<meta property="og:updated_time" content="2017-12-30T15:12:58.738Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP 简介 & 统计语言模型">
<meta name="twitter:description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/english">English</a>
        
          <a class="main-nav-link" href="/datascience">Science</a>
        
          <a class="main-nav-link" href="/nlp">Algorithm</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/project_frame">Project</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-nlp-pre-statistics-language-model" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      NLP 简介 &amp; 统计语言模型
      <small class=article-detail-date-index>&nbsp; 2017-11-13</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2017/11/13/nlp-pre-statistics-language-model/" class="article-date">
  <time datetime="2017-11-13T02:08:21.000Z" itemprop="datePublished">2017-11-13</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/nlp/">nlp</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2017/11/13/nlp-pre-statistics-language-model/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

<p>1946年 计算机出现之后，计算机很多事情比人类做得好，那么机器是否能懂<strong>自然语言</strong>?</p>
<a id="more"></a>
<h2 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h2><p>NLP是一门集计算机科学，人工智能，语言学三者于一身的交叉性学科。她的终极研究目标是让计算机能够处理甚至是“理解”人类的自然语言，进而帮助人类解决一些现实生活中遇到的实际问题。这里的语言“理解”是一个很抽象也很哲学的概念。在 NLP 中，我们将对语言的“理解”定义为是学习一个能够解决具体问题的复杂函数的过程。</p>
<p>一些NLP技术的应用:</p>
<ul>
<li>简单的任务：拼写检查，关键词检索，同义词检索等</li>
<li>复杂的任务：信息提取、情感分析、文本分类等</li>
<li>更复杂任务：机器翻译、人机对话、QA系统</li>
</ul>
<h2 id="From-rules-to-statistics"><a href="#From-rules-to-statistics" class="headerlink" title="From rules to statistics"></a>From rules to statistics</h2><blockquote>
<p><strong>为什么要把NLP从机器学习的任务列表中单独抽取出来做为一门研究的对象？</strong></p>
<p>根本原因在于语言用于表达客观世界的内在复杂性和多义性。举一个简单的例子：”Jane hit June and then she [fell/ran]”。当she所伴随的动作不同（fell or ran），其所指代的对象也发生了变化（June or Jane）。这样的例子太多，显然我们无法通过枚举所有的规则来解决语言内在的复杂性。另一个多义性的例子是：”I made her duck”。我们可以理解为：”I cooked her a duck”，或是”I curved her a wooden duck”，也可以理解为：”I transformed her into a duck with some magic”。</p>
</blockquote>
<h2 id="Statistics-Language-Model"><a href="#Statistics-Language-Model" class="headerlink" title="Statistics Language Model"></a>Statistics Language Model</h2><p>自然语言逐渐演变成一种上下文信息表达和传递的方式，让计算机处理自然语言，一个基本的问题就是为自然语言这种上下文相关的特性建立数学模型。 这个数学模型也就是NLP说的 Statistics Language Model.</p>
<blockquote>
<ol>
<li>美联储主席昨天告诉媒体 7000 亿美金的救助资金将借给上百家银行、汽车公司。</li>
<li>美联储主席昨天 7000 亿美金的救助资金告诉媒体将借给上百家银行、汽车公司。</li>
<li>美联储主席昨天 告媒诉体 70 亿00美金的救助资金上百家银行将借给、汽车公司。</li>
</ol>
<p>上世纪70年代科学家们试图用规则文法判断句子是否合理。贾里尼克用统计模型解决方法更有效。</p>
</blockquote>
<p>如果 S 表示一连串特定顺序排列的词 $w_1$， $w_2$，…， $w_n$ ，换句话说，S 表示的是一个有意义的句子。机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为：</p>
<p>$$<br>P(S) = P(w_1)P(w_2|w_1)P(w_3| w_1 w_2)…P(w_n|w_1 w_2…w_{n-1})<br>$$</p>
<p>马尔可夫假设</p>
<p>$$<br>P(S) = P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_i|w_{i-1})…<br>$$</p>
<p>接下来如何估计 $P (w_i|w_{i-1})$。只要机器数一数这对词 $(w_i{-1}, w_i)$ 在统计的文本中出现了多少次，以及 $w_{i-1}$ 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,</p>
<p>$$<br>P(w_i|w_{i-1}) = \frac {P(w_{i-1}, w_i)} {P(w_{i-1})}<br>$$</p>
<blockquote>
<p>如何计算 ： </p>
<p>$$<br> \frac {P(w_{i-1}, w_i)} {P(w_{i-1})}<br>$$</p>
<p>根据大数定理，只要统计量足够，相对频度就等于概率</p>
</blockquote>
<h3 id="higher-order-language-model"><a href="#higher-order-language-model" class="headerlink" title="higher order language model"></a>higher order language model</h3><p>假定文本中的每个词 $w_i$ 和 前面N-1个词有关，而和更前面的词无关，这样当前词 $w_i$ 的概率值取决于前面 N-1个词 $P(w_{i-N+1}, w_{i-N+2}, …, w_{i-1})$</p>
<p>因此，</p>
<p>$$<br>P(w_{i}|w_{1}, w_{2}, …, w_{i-1}) = P(w_i | w_{i-N+1}, w_{i-N+2}, …, w_{i-1})<br>$$</p>
<blockquote>
<p>N元模型， N=2 时，为二元模型。 在实际中应用最多的是 N=3 的三元模型</p>
</blockquote>
<h3 id="zero-probability-smoothing-method"><a href="#zero-probability-smoothing-method" class="headerlink" title="zero probability, smoothing method"></a>zero probability, smoothing method</h3><p>解决统计样本不足的概率估计问题， 不可信的统计数据打折扣的一种概率估计方法。</p>
<blockquote>
<p>1953年有古德（I.J.Good）图灵（Turing）的方法而提出来的。其基本思想是：对于没有看见的事件，我们不能认为它发生的概率就是零，因此我们从概率的总量（Probability Mass）中，分配一个很小的比例给这些没有看见的事件。这样一来看的见概率总和就要小于1了，因此，需要将所有看见的事件概率调小一点。至于小多少，要根据“越是不可信的统计折扣越多”的方法进行。</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>《数学之美》 读书笔记 </li>
<li><a href="https://whiskytina.github.io/word2vec.html" target="_blank" rel="external">word2vec前世今生</a></li>
<li><a href="https://whiskytina.github.io/14947653164873.html" target="_blank" rel="external">CS224N NLP with Deep Learning: Lecture 1 课程笔记</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_weibo_icon.png);background-size: contain;display: inline-block; width:50px; height:50px" href="https://service.weibo.com/share/share.php?url" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/2017/11/05/support-pay-blog/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/2017/11/05/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  原创文章，转载请注明： 转载自<a href="http://www.iequa.com"> Blair Chan's Blog</a>，作者：
  <a href="http://www.iequa.com/about">Blair Chan</a> <br>
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>

 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Natural-Language-Processing"><span class="toc-number"></span> <span class="toc-text">Natural Language Processing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#From-rules-to-statistics"><span class="toc-number"></span> <span class="toc-text">From rules to statistics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Statistics-Language-Model"><span class="toc-number"></span> <span class="toc-text">Statistics Language Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#higher-order-language-model"><span class="toc-number"></span> <span class="toc-text">higher order language model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zero-probability-smoothing-method"><span class="toc-number"></span> <span class="toc-text">zero probability, smoothing method</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number"></span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/nlp/">nlp</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/word2vec/">word2vec</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/11/14/nlp-pre-hidden-markov-model/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Hidden Markov Model
        
      </div>
    </a>
  
  
    <a href="/2017/11/12/ops-bandwagonhost-ssh-openvpn/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">搬瓦工VPS 配置 SSH 与 OpenVpn&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Libin Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2017/11/13/nlp-pre-statistics-language-model/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
