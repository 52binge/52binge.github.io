<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Tensorflow Useful Links]]></title>
      <url>http://sggo.me/2019/02/15/tensorflow/tf-help-links/</url>
      <content type="html"><![CDATA[<p>Here are some useful links about Tensorflow</p>
<a id="more"></a>
<h2 id="1-GPU设备指定"><a href="#1-GPU设备指定" class="headerlink" title="1. GPU设备指定"></a>1. GPU设备指定</h2><p><a href="https://blog.csdn.net/dcrmg/article/details/79091941" target="_blank" rel="external">tensorflow中使用tf.ConfigProto()配置Session运行参数&amp;&amp;GPU设备指定</a></p>
<h2 id="2-gensim-models"><a href="#2-gensim-models" class="headerlink" title="2. gensim.models"></a>2. gensim.models</h2><p><a href="https://blog.csdn.net/sinat_26917383/article/details/69803018" target="_blank" rel="external">gensim训练word2vec及相关函数与功能理解</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow： 第6章 图片识别与CNN]]></title>
      <url>http://sggo.me/2019/01/28/tensorflow/tf-google-6-cnn-1/</url>
      <content type="html"><![CDATA[<p>实战Google深度学习框架 笔记-第6章 图片识别 与 CNN</p>
<p>介绍 CNN 在图片识别的应用 和 CNN 基本原理 以及 如何使用 TensorFlow 来实现 CNN .</p>
<a id="more"></a>
<h2 id="1-图像识别介绍与经典数据集"><a href="#1-图像识别介绍与经典数据集" class="headerlink" title="1. 图像识别介绍与经典数据集"></a>1. 图像识别介绍与经典数据集</h2><p>视觉是人类认识世界非常重要的一种知觉.</p>
<p>经典数据集 </p>
<ol>
<li>MNIST</li>
<li><a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR</a></li>
<li><a href="http://www.image-net.org/challenges/LSVRC/" target="_blank" rel="external">ImageNet</a></li>
</ol>
<blockquote>
<p>CIFAR 数据集是一个影响力很大的图像分类数据集, 是图像词典项目（Visual Dictionary） (7) 中800万张图片的一个子集， 都是 32×32的彩色图片, 由Alex Krizhevsky教授、Vinod Nair博士和Geoffrey Hinton教授整理的.</p>
<p><a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR-10</a> 问题收集了来自 10个 不同种类的 60000张图片, Cifar-10中的图片大小都是固定的且每一张图片中仅包含一个种类的实体 </p>
<p><a href="http://www.image-net.org/challenges/LSVRC/" target="_blank" rel="external">ImageNet</a> 是为了更加贴近真实环境下的图像识别问题，基于 WordNet， 由斯坦福大学（Stanford University）的李飞飞（Feifei Li）带头整理的ImageNet很大程度地解决了这两个问题。 ILSVRC2012 包含来自 1000 个类别 120万 张图片.</p>
</blockquote>
<h2 id="2-CNN-介绍与常用结构"><a href="#2-CNN-介绍与常用结构" class="headerlink" title="2. CNN 介绍与常用结构"></a>2. CNN 介绍与常用结构</h2><p>这部分内容详见 <a href="/deeplearning/#4-Convolutional-Neural-Networks">Convolutional Neural Networks</a></p>
<p><a href="/2018/08/24/deeplearning/Convolutional-Neural-Networks-week2/">如何搭建一个神经网络，包括最新的变体，如: ResNet</a></p>
<h2 id="3-经典-CNN-模型"><a href="#3-经典-CNN-模型" class="headerlink" title="3. 经典 CNN 模型"></a>3. 经典 CNN 模型</h2><ul>
<li>LeNet-5</li>
<li>Inception</li>
<li>ResNet</li>
</ul>
<h3 id="3-1-LeNet-5-模型"><a href="#3-1-LeNet-5-模型" class="headerlink" title="3.1 LeNet-5 模型"></a>3.1 LeNet-5 模型</h3><p>该网络 1980s 提出，主要针对灰度图像训练的，用于识别手写数字。</p>
<h3 id="3-2-Inception-v3-模型"><a href="#3-2-Inception-v3-模型" class="headerlink" title="3.2 Inception-v3 模型"></a>3.2 Inception-v3 模型</h3><h2 id="4-CNN-迁移学习"><a href="#4-CNN-迁移学习" class="headerlink" title="4. CNN 迁移学习"></a>4. CNN 迁移学习</h2><h3 id="4-1-迁移学习介绍"><a href="#4-1-迁移学习介绍" class="headerlink" title="4.1 迁移学习介绍"></a>4.1 迁移学习介绍</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/31534286" target="_blank" rel="external">知乎：《TensorFlow：实战Google深度学习框架》笔记、代码及勘误-第6章 图像识别与卷积神经网络-1</a></li>
<li><a href="http://b.7dtime.com/B076DGNXP1/11/0.html" target="_blank" rel="external">7天时间读书 Tensorflow 实战 Google 深度学习框架</a></li>
<li><a href="https://blog.csdn.net/u011389706/article/details/81455750" target="_blank" rel="external">MNIST识别自己手写的数字–进阶篇（CNN）</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[ELMo 最好用的词向量《Deep Contextualized Word Representations》]]></title>
      <url>http://sggo.me/2018/12/23/nlp/ELMo/</url>
      <content type="html"><![CDATA[<p>ELMo 模型在非常多的 NLP task上都提高了state-of-the-art 方法的performance.</p>
<p>论文链接：<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="external">[1802.05365] Deep contextualized word representations</a></p>
<p>这文章同时被 <strong>ICLR 2018</strong> 和 <strong>NAACL 2018</strong> 接收, 后来获得了 <strong>NAACL best paper award</strong>.</p>
<a id="more"></a>
<blockquote>
<p>ACL、EMNLP、NAACL（北美分会）、COLING 是 NLP 领域 的 四大顶会。</p>
</blockquote>
<h2 id="1-ELMo的优势"><a href="#1-ELMo的优势" class="headerlink" title="1. ELMo的优势"></a>1. ELMo的优势</h2><ul>
<li><p>ELMo 能够学习到词汇用法的复杂性，比如语法、语义。</p>
</li>
<li><p>ELMo 能够学习不同上下文情况下的词汇多义性。</p>
</li>
</ul>
<h2 id="2-ELMo的模型简介"><a href="#2-ELMo的模型简介" class="headerlink" title="2. ELMo的模型简介"></a>2. ELMo的模型简介</h2><p>基于大量文本，ELMo模型是从深层的双向语言模型（deep bidirectional language model）中的内部状态(internal state)学习而来的，而这些词向量很容易加入到QA、文本对齐、文本分类等模型中，后面会展示一下ELMo词向量在各个任务上的表现。</p>
<h2 id="3-双向语言模型"><a href="#3-双向语言模型" class="headerlink" title="3. 双向语言模型"></a>3. 双向语言模型</h2><h2 id="4-ELMo"><a href="#4-ELMo" class="headerlink" title="4. ELMo"></a>4. ELMo</h2><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/38254332" target="_blank" rel="external">文本情感分类：使用卷积神经网络（textCNN）</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[FastText 用于高效文本分类的技巧]]></title>
      <url>http://sggo.me/2018/12/19/nlp/fastText/</url>
      <content type="html"><![CDATA[<p>fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。</p>
<p>fastText 是 智慧与美貌并重的 <strong>文本分类</strong> and <strong>向量化工具</strong> 的项目，它是有两部分组成的。 </p>
<p>论文1链接： <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="external">Bag of Tricks for Efficient Text Classification</a></p>
<p>论文2链接： <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a></p>
<a id="more"></a>
<p>github链接： <a href="https://github.com/facebookresearch/fastText" target="_blank" rel="external">facebookresearch/fastText</a></p>
<p>fastText 能够做到效果好，速度快，主要依靠两个秘密武器：</p>
<blockquote>
<ol>
<li>利用了词内的n-gram信息(subword n-gram information)</li>
<li>用到了层次化Softmax回归(Hierarchical Softmax) 的训练 trick.</li>
</ol>
</blockquote>
<p><strong>fastText 背景</strong></p>
<p>英语单词通常有其内部结构和形成方式。例如我们可以从“dog”、“dogs”和“dogcatcher”的字面上推测他们的关系。这些词都有同一个词根“dog”，这个关联可以推广至其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。很多词根据场景不同有多种不同的形态。构词学（morphology）作为语言学的一个重要分支，研究的正是词的内部结构和形成方式。</p>
<p>在 word2vec 中，我们并没有直接利用构词学中的信息。无论是在 <strong>skip-gram</strong> 还是 <strong>CBOW</strong> 中，我们将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。有鉴于此，fastText 提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入 word2vec 中的 <strong>skip-gram</strong>。</p>
<p><strong>子词嵌入 subword embedding</strong></p>
<p>在 fastText 中，每个中心词被表示成子词的集合。下面我们用单词“where”作为例子来了解子词是如何产生的。首先，我们在单词的首尾分别添加特殊字符“&lt;”和“&gt;”以区分作为前后缀的子词。然后，将单词当成一个由字符构成的序列来提取 $n$ 元语法。例如当 $n=3$ 时，我们得到所有长度为 3 的子词：</p>
<p>$$&lt;.wh ， whe ， her ， ere ， re&gt;$$</p>
<p>以及特殊子词 “&lt;.where&gt;”。</p>
<p>在 fastText 中，对于一个词 $w$，将它所有长度在 3 到 6 的子词和特殊子词的并集记为 $\mathcal{G}_w$。那么词典则是所有词的子词集合的并集。假设词典中子词 $g$ 的向量为 $\boldsymbol{z}_g$，那么跳字模型中词 $w$ 的作为中心词的向量 $\boldsymbol{v}_w$ 则表示成</p>
<p>$$<br>\boldsymbol{v}_w = \sum_{g\in\mathcal{G}_w} \boldsymbol{z}_g.<br>$$</p>
<p>FastText 的其余部分同 <strong>skip-gram</strong> 一致，不在此重复。可以看到，同 <strong>skip-gram</strong> 相比，fastText 中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。但与此同时，较生僻的复杂单词，甚至是词典中没有的单词，可能会从同它结构类似的其他词那里获取更好的词向量表示。</p>
<h2 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h2><h3 id="1-1-Softmax-Regression"><a href="#1-1-Softmax-Regression" class="headerlink" title="1.1 Softmax Regression"></a>1.1 Softmax Regression</h3><p>Softmax Regression (回归) 又被称作多项LR（multinomial logistic regression），它是LR在多类别任务上的推广。</p>
<p><img src="/images/nlp/fastText2.jpg" width="850" img=""></p>
<h3 id="1-2-Hierarchical-Softmax"><a href="#1-2-Hierarchical-Softmax" class="headerlink" title="1.2 Hierarchical Softmax"></a>1.2 Hierarchical Softmax</h3><h3 id="1-3-n-gram’s-feature"><a href="#1-3-n-gram’s-feature" class="headerlink" title="1.3 n-gram’s feature"></a>1.3 n-gram’s feature</h3><p>在文本特征提取中，常常能看到 n-gram 的身影。它是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为 N 的滑动窗口操作，最终形成长度为 N 的字节片段序列。看下面的例子：</p>
<p><strong>字粒度</strong></p>
<blockquote>
<p>我来到达观数据参观</p>
<ul>
<li><p>相应的bigram特征为：我来 来到 到达 达观 观数 数据 据参 参观</p>
</li>
<li><p>相应的trigram特征为：我来到 来到达 到达观 达观数 观数据 数据参 据参观</p>
</li>
</ul>
</blockquote>
<p><strong>词粒度</strong></p>
<blockquote>
<p>我 来到 达观数据 参观</p>
<ul>
<li><p>相应的bigram特征为：我/来到 来到/达观数据 达观数据/参观</p>
</li>
<li><p>相应的trigram特征为：我/来到/达观数据 来到/达观数据/参观 </p>
</li>
</ul>
</blockquote>
<p><strong>小结：</strong></p>
<p>n-gram中的gram根据粒度不同。它可以是字粒度，也可以是词粒度的。</p>
<p>n-gram 产生的特征只是作为<strong>文本特征的候选集</strong>，你后面可能会采用信息熵、卡方统计、IDF等文本特征选择方式筛选出比较重要特征。</p>
<h2 id="2-word2vec-架构原理"><a href="#2-word2vec-架构原理" class="headerlink" title="2. word2vec 架构原理"></a>2. word2vec 架构原理</h2><h3 id="2-1-CBOW-模型架构"><a href="#2-1-CBOW-模型架构" class="headerlink" title="2.1 CBOW 模型架构"></a>2.1 CBOW 模型架构</h3><h3 id="2-2-前向传播"><a href="#2-2-前向传播" class="headerlink" title="2.2 前向传播"></a>2.2 前向传播</h3><h3 id="2-3-反向传播"><a href="#2-3-反向传播" class="headerlink" title="2.3 反向传播"></a>2.3 反向传播</h3><h2 id="3-fastText-核心思想"><a href="#3-fastText-核心思想" class="headerlink" title="3. fastText 核心思想"></a>3. fastText 核心思想</h2><h3 id="3-1-字符级-n-gram"><a href="#3-1-字符级-n-gram" class="headerlink" title="3.1 字符级 n-gram"></a>3.1 字符级 n-gram</h3><h3 id="3-2-模型架构"><a href="#3-2-模型架构" class="headerlink" title="3.2 模型架构"></a>3.2 模型架构</h3><h3 id="3-3-核心思想"><a href="#3-3-核心思想" class="headerlink" title="3.3 核心思想"></a>3.3 核心思想</h3><p>仔细观察模型的后半部分，即从隐含层输出到输出层输出，会发现它就是一个softmax线性多类别分类器，分类器的输入是一个用来表征当前文档的向量；模型的前半部分，即从输入层输入到隐含层输出部分，主要在做一件事情：生成用来表征文档的向量。那么它是如何做的呢？叠加构成这篇文档的所有词及n-gram的词向量，然后取平均。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。</p>
<p>于是fastText的核心思想就是：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。</p>
<h3 id="3-4-分类效果"><a href="#3-4-分类效果" class="headerlink" title="3.4 分类效果"></a>3.4 分类效果</h3><p>还有个问题，就是为何fastText的分类效果常常不输于传统的非线性分类器？</p>
<p><strong>假设我们有两段文本：</strong></p>
<blockquote>
<p>我 来到 达观数据</p>
<p>俺 去了 达而观信息科技</p>
</blockquote>
<p>这两段文本意思几乎一模一样，如果要分类，肯定要分到同一个类中去。但在传统的分类器中，用来表征这两段文本的向量可能差距非常大。传统的文本分类中，你需要计算出每个词的权重，比如tfidf值， “我”和“俺” 算出的tfidf值相差可能会比较大，其它词类似，于是，VSM（向量空间模型）中用来表征这两段文本的文本向量差别可能比较大。</p>
<p>但是fastText就不一样了，它是用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度，于是，在fastText模型中，这两段文本的向量应该是非常相似的，于是，它们很大概率会被分到同一个类中。</p>
<p><strong>fastText效果好的原因：</strong></p>
<blockquote>
<ol>
<li>使用词embedding而非词本身作为特征</li>
<li>字符级n-gram特征的引入对分类效果会有一些提升 </li>
</ol>
</blockquote>
<h2 id="4-fastText-keras-实战"><a href="#4-fastText-keras-实战" class="headerlink" title="4. fastText keras 实战"></a>4. fastText keras 实战</h2><h2 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a>6. 小结</h2><ul>
<li>FastText 提出了子词嵌入方法。在 word2vec <strong>skip-gram</strong> 基础上，将中心词向量表示成单词的子词向量之和。</li>
<li>子词嵌入（subword embedding）利用构词上的规律，通常可以提升生僻词表示的质量。</li>
<li>fastText 训练时复杂度 采用层次化 softmax 之后，减少为 O(hlogK) 级别, 预测时还是 O(Kh) </li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zh.gluon.ai/chapter_natural-language-processing/fasttext.html" target="_blank" rel="external">子词嵌入（fastText）</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-05-3" target="_blank" rel="external">fastText，智慧与美貌并重的文本分类及向量化工具</a></li>
<li><a href="https://blog.csdn.net/sinat_26917383/article/details/54850933" target="_blank" rel="external">NLP︱高级词向量表达（二）——FastText（简述、学习笔记）</a></li>
<li><a href="https://blog.csdn.net/sinat_26917383/article/details/83041424" target="_blank" rel="external">如何在python 非常简单训练FastText</a></li>
<li><a href="http://www.52nlp.cn/fasttext" target="_blank" rel="external">我爱自然语言处理-fastText原理及实践</a></li>
<li><a href="https://www.jianshu.com/p/2acc49549af6" target="_blank" rel="external">FastText文本分类算法学习笔记（好文）</a></li>
<li><a href="https://blog.csdn.net/fendouaini/article/details/81086575" target="_blank" rel="external">FastText的内部机制</a></li>
<li><a href="https://blog.csdn.net/joleoy/article/details/84987230" target="_blank" rel="external">利用skift实现fasttext模型</a></li>
<li><a href="https://blog.csdn.net/sxllllwd/article/details/81914447" target="_blank" rel="external">CSDN 层次softmax</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TextCNN 文本情感分类的卷积神经网络]]></title>
      <url>http://sggo.me/2018/12/16/nlp/textCNN/</url>
      <content type="html"><![CDATA[<p>textCNN 是 2014年 提出的用来做文本分类的卷积神经网络，结构简单、效果好.</p>
<p>论文链接： <a href="https://arxiv.org/abs/1510.03820" target="_blank" rel="external">Convolutional Neural Networks for Sentence Classification</a> 在文本分类等 NLP 领域应用广泛. </p>
<p>一般结构： 降维 -&gt; conv -&gt; 最大池化 -&gt; 完全连接层 -&gt; softmax .</p>
<a id="more"></a>
<blockquote>
<p>将文本当做是一维图像，从而可以用一维卷积神经网络来捕捉临近词之间的关联。</p>
</blockquote>
<h2 id="1-一维卷积层"><a href="#1-一维卷积层" class="headerlink" title="1. 一维卷积层"></a>1. 一维卷积层</h2><p>在介绍模型前我们先来解释一维卷积层的工作原理。和二维卷积层一样，一维卷积层使用一维的互相关运算。在一维互相关运算中，卷积窗口从输入数组的最左方开始，按从左往右的顺序，依次在输入数组上滑动。</p>
<p>当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。</p>
<h3 id="1-1-多输入通道-一维互相关"><a href="#1-1-多输入通道-一维互相关" class="headerlink" title="1.1 多输入通道 * 一维互相关"></a>1.1 多输入通道 * 一维互相关</h3><p>如下图所示，输入是一个宽为 7 的一维数组，核数组的宽为 2。可以看到输出的宽度为 <strong>7−2+1=6</strong>，且第一个元素是由输入的最左边的宽为 2 的子数组与核数组按元素相乘后再相加得到的。</p>
<p><img src="/images/nlp/conv1d.svg" width="650" img=""></p>
<blockquote>
<p>一维互相关运算。阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：0×1+1×2=2。</p>
</blockquote>
<h3 id="1-2-多输入通道-一维互相关"><a href="#1-2-多输入通道-一维互相关" class="headerlink" title="1.2 多输入通道 * 一维互相关"></a>1.2 多输入通道 * 一维互相关</h3><p>多输入通道的一维互相关运算也与多输入通道的二维互相关运算类似：在每个通道上，将核与相应的输入做一维互相关运算，并将通道之间的结果相加得到输出结果。下图展示了含 3 个输入通道的一维互相关运算。</p>
<p><img src="/images/nlp/conv1d-channel.svg" width="650" img=""></p>
<blockquote>
<p>含3个输入通道的一维互相关运算。阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：</p>
<p>$$0×1+1×2+1×3+2×4+2×(−1)+3×(−3)=2$$</p>
</blockquote>
<h3 id="1-3-单输入通道-二维互相关"><a href="#1-3-单输入通道-二维互相关" class="headerlink" title="1.3 单输入通道 * 二维互相关"></a>1.3 单输入通道 * 二维互相关</h3><p><img src="/images/nlp/conv1d-2d.svg" width="650" img=""></p>
<blockquote>
<p>$$2×(−1)+3×(−3)+1×3+2×4+0×1+1×2=2$$</p>
<p>结论 ： <strong>多输入通道</strong> <strong>一维互相关</strong> &lt;=&gt; <strong>单输入通道</strong> <strong>二维互相关</strong></p>
</blockquote>
<h3 id="1-4-多输入通道-二维互相关"><a href="#1-4-多输入通道-二维互相关" class="headerlink" title="1.4 多输入通道 * 二维互相关"></a>1.4 多输入通道 * 二维互相关</h3><p><img src="/images/nlp/conv_multi_in.svg" width="650" img=""></p>
<blockquote>
<p>$$<br>(1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56<br>$$</p>
</blockquote>
<h3 id="1-5-多输出通道-1×1-卷积层"><a href="#1-5-多输出通道-1×1-卷积层" class="headerlink" title="1.5 多输出通道 1×1 卷积层"></a>1.5 多输出通道 1×1 卷积层</h3><p>当输入通道有多个时，由于我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为 1</p>
<p><img src="/images/nlp/conv_1x1.svg" width="650" img=""></p>
<h3 id="1-6-小结"><a href="#1-6-小结" class="headerlink" title="1.6 小结"></a>1.6 小结</h3><ul>
<li>使用多通道可以拓展卷积层的模型参数.</li>
<li>假设将通道维当做是特征维，将高和宽维度上的元素当成数据样本，那么 1×1 卷积层的作用与全连接层等价.</li>
<li>1×1 卷积层通常用来调整网络层之间的通道数，并控制模型复杂度.</li>
</ul>
<h2 id="2-时序最大池化层"><a href="#2-时序最大池化层" class="headerlink" title="2. 时序最大池化层"></a>2. 时序最大池化层</h2><p>类似地，我们有一维池化层。TextCNN 中使用的时序最大池化层（max-over-time pooling）实际上对应一维全局最大池化层：假设输入包含多个通道，各通道由不同时间步上的数值组成，各通道的输出即该通道所有时间步中最大的数值。因此，时序最大池化层的输入在各个通道上的时间步数可以不同。</p>
<blockquote>
<p>为提升计算性能，我们常常将不同长度的时序样本组成一个小<strong>Batch</strong>，并通过在较短序列后附加<strong>特殊字符（例如 0）</strong>令批量中各时序样本长度相同。这些人为添加的特殊字符当然是无意义的。由于时序最大池化的主要目的是抓取时序中最重要的特征，它通常能使模型不受人为添加字符的影响。</p>
</blockquote>
<h2 id="3-TextCNN-模型"><a href="#3-TextCNN-模型" class="headerlink" title="3. TextCNN 模型"></a>3. TextCNN 模型</h2><p>TextCNN 主要使用了一维卷积层和时序最大池化层。假设输入的文本序列由 n 个词组成，每个词用 d 维的词向量表示。那么输入样本的宽为 n，高为 1，输入通道数为 d。</p>
<p>textCNN 的计算主要分为以下几步：</p>
<blockquote>
<p>(1). <strong>卷积层</strong>： 定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。</p>
<p>(2). <strong>池化层</strong>： 对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。</p>
<p>(3). <strong>全连层</strong>： 通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合。</p>
</blockquote>
<p><img src="/images/nlp/conv_textcnn.svg" width="700" img=""></p>
<p>用一个例子解释了 textCNN 的设计。这里的输入是一个有 11 个词的句子，每个词用 6 维词向量表示。因此输入序列的宽为 11，输入通道数为 6。给定 2 个一维卷积核，核宽分别为 2 和 4，输出通道数分别设为 4 和 5。因此，一维卷积计算后，4 个输出通道的宽为 11−2+1=10，而其他 5 个通道的宽为 11−4+1=8。尽管每个通道的宽不同，我们依然可以对各个通道做时序最大池化，并将 9 个通道的池化输出连结成一个 9 维向量。最终，我们使用全连接将 9 维向量变换为 2 维输出：正面情感和负面情感的预测。</p>
<h2 id="4-CNN-的特点"><a href="#4-CNN-的特点" class="headerlink" title="4. CNN 的特点"></a>4. CNN 的特点</h2><p>CNN的三个优点：</p>
<ol>
<li>sparse interaction(稀疏的交互)</li>
<li>parameter sharing(参数共享)</li>
<li>equivalent respresentation(等价表示)。</li>
</ol>
<p>经典的简化卷积公式表示如下：</p>
<p><img src="/images/nlp/textcnn-1.webp" width="700" img=""></p>
<p>假设每个词用三维向量表示，左边是4个词，右边是卷积矩阵，那么得到输出为：</p>
<p><img src="/images/nlp/textcnn-2.webp" width="700" img=""></p>
<p>如果基于这个结果做1-MaxPool池化(最大值池化)，那么就取卷积层结果 o 中的最大值，即提取最显著的特征。</p>
<p>针对海量的文本多分类数据，也可以尝试一下浅层的深度学习模型FastText模型，该模型的分类效率更高。</p>
<p><img src="/images/nlp/textcnn-3.webp" width="700" img=""></p>
<p>整个模型由四部分构成： <strong>输入层</strong>、<strong>卷积层</strong>、<strong>池化层</strong>、<strong>全连接层</strong>。</p>
<h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a>5. 小结</h2><ul>
<li>我们可以使用一维卷积来处理和分析时序数据。</li>
<li>多输入通道的一维互相关运算可以看作是单输入通道的二维互相关运算。</li>
<li>时序最大池化层的输入在各个通道上的时间步数可以不同。</li>
<li>TextCNN 主要使用了一维卷积层和时序最大池化层。</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zh.gluon.ai/chapter_natural-language-processing/sentiment-analysis-cnn.html" target="_blank" rel="external">文本情感分类：使用卷积神经网络（textCNN）</a></li>
<li><a href="http://www.52nlp.cn/tag/textcnn" target="_blank" rel="external">我爱自然语言处理</a></li>
<li><a href="https://www.jianshu.com/p/f69e8a306862" target="_blank" rel="external">吾爱NLP(4)—基于Text-CNN模型的中文文本分类实战</a></li>
<li><a href="https://www.cnblogs.com/DjangoBlog/p/7511979.html" target="_blank" rel="external">fastText、TextCNN、TextRNN……这里有一套NLP文本分类深度学习方法库供你选择</a></li>
<li><a href="https://blog.csdn.net/u012762419/article/details/79561441" target="_blank" rel="external">大规模文本分类网络TextCNN介绍</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 13 - 理论篇： MMI 模型理论]]></title>
      <url>http://sggo.me/2018/12/05/chatbot/chatbot-research13/</url>
      <content type="html"><![CDATA[<p>本文提出了两种模型（其实就是改了下目标函数，而且训练过程中仍然使用likelihood，仅在测试的时候使用新的目标函数将有意义的响应的概率变大~~），MMI-antiLM和MMI-bidi，下面分别进行介绍。</p>
<a id="more"></a>
<p>本文是李纪为的论文“A Diversity-Promoting Objective Function for Neural Conversation Models”阅读笔记。违章提出使用MMI代替原始的maximum likelihood作为目标函数，目的是使用互信息减小“I don’t Know”这类无聊响应的生成概率。一般的seq2seq模型，倾向于生成安全、普适的响应，因为这种响应更符合语法规则，在训练集中出现频率也较高，最终生成的概率也最大，而有意义的响应生成概率往往比他们小。通过MMI来计算输入输出之间的依赖性和相关性，可以减少模型对他们的生成概率。</p>
<h2 id="新的目标函数"><a href="#新的目标函数" class="headerlink" title="新的目标函数"></a>新的目标函数</h2><p>…</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 12 - 理论篇： 评价指标介绍]]></title>
      <url>http://sggo.me/2018/12/01/chatbot/chatbot-research12/</url>
      <content type="html"><![CDATA[<p>对话系统之所以没有取得突破性的进展，很大程度是因为没有一个可以准确表示回答效果好坏的评价标准。对话系统中大都使用机器翻译、摘要生成领域提出来的评价指标，但是很明显对话系统的场景和需求与他们是存在差别的.</p>
<a id="more"></a>
<h2 id="1-评价指标-概览"><a href="#1-评价指标-概览" class="headerlink" title="1. 评价指标*概览"></a>1. 评价指标*概览</h2><p><strong>对于某一轮对话而言:</strong></p>
<blockquote>
<p>可使用响应的适当性、流畅度、相关性；</p>
</blockquote>
<p><strong>对于多轮对话而言:</strong></p>
<blockquote>
<p>关注流畅性、对话深度、多样性、一致连贯性等指标</p>
</blockquote>
<p><strong>对于整个对话系统:</strong></p>
<blockquote>
<p>我们则希望他可以涵盖更多的话题、回复真实可信等等。</p>
</blockquote>
<p>这些都是我们想要对话系统所拥有的能力，但是往往在一个具体的任务中我们只能关注某一项或者几项指标，这里我们主要针对开放域生成式对话模型的评价指标进行总结。</p>
<h2 id="2-词重叠评价指标"><a href="#2-词重叠评价指标" class="headerlink" title="2. 词重叠评价指标"></a>2. 词重叠评价指标</h2><p>首先来看词重叠评价指标，他们认为有效地回答应该和真实回答之间存在大量的词重叠<br>（但是对话系统的答案空间往往是发散的，也就是一个问题的答案可能是完全不同的两句话，这种情况下该评价指标效果不好），也就是说这是一个非常强的假设。（以下环节中r表示真是响应，r^表示系统生成响应）</p>
<h2 id="3-BLEU"><a href="#3-BLEU" class="headerlink" title="3. BLEU"></a>3. BLEU</h2><p>该评价指标有IBM在2002年提出，参考论文“BLEU: a Method for Automatic Evaluation of Machine Translation”，常作为机器翻译系统评价指标。其实就是统计生成响应和真实响应中的n-gram词组在整个训练语料中出现次数。公式如下所示：</p>
<p><img src="/images/chatbot/chatbot-12.1.jpg" width="400" img=""></p>
<blockquote>
<p>ROUGE : 该指标常用于文本摘要领域<br>METEOR: BLEU 的升级版</p>
</blockquote>
<h2 id="4-词向量评价指标"><a href="#4-词向量评价指标" class="headerlink" title="4. 词向量评价指标"></a>4. 词向量评价指标</h2><p>上面的词重叠评价指标基本上都是n-gram方式，去计算生成响应和真实响应之间的重合程度，共现程度等指标。而词向量则是通过Word2Vec等方法将句子转换为向量表示，这样一个句子就被映射到一个低维空间，句向量在一定程度上表征了其含义，在通过余弦相似度等方法就可以计算两个句子之间的相似程度。使用词向量的好处是，可以一定程度上增加答案的多样性，因为这里大多采用词语相似度进行表征，相比词重叠中要求出现完全相同的词语，限制降低了很多。</p>
<h2 id="5-perplexity困惑度"><a href="#5-perplexity困惑度" class="headerlink" title="5. perplexity困惑度"></a>5. perplexity困惑度</h2><p>perplexity是语言模型中的指标，用于评价语言模型的好坏，其实就是估算一句话出现的概率，看一句话是否通顺。也经常会在对话系统中出现评价生成的响应是否符合语言规则，计算方法也很简单，如下图所示：</p>
<p><img src="/images/chatbot/chatbot-12.3.jpg" width="400" img=""></p>
<p>所以当我们使用tf.contrib.seq2seq.sequence_loss()函数计算模型loss的时候，perplexity的计算就显得很简单了，直接对计算出来的loss取个指数就行了，命令如下所示：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">train_perp = math.exp(float(mean_loss)) <span class="keyword">if</span> mean_loss &lt; <span class="number">300</span> <span class="keyword">else</span> math.inf</span><br></pre></td></tr></table></figure>
<p>现在我训练的对话系统，一般都只是用了perplexity来评判模型的效果，最终perplexity可以降到20左右（越小越好，说明越接近于自然语言）。</p>
<h2 id="6-人工指标"><a href="#6-人工指标" class="headerlink" title="6. 人工指标"></a>6. 人工指标</h2><p>最后说一下人工评价，首先来讲，上面说了这么多的评价指标，并没有一个可以很好的解决对话系统的问题，就像“How NOT To Evaluate Your Dialogue System”论文中说到的那样，当下的这些评价指标都跟人工评价成弱相关或者完全没有关系，相关程度跟具体的数据集有关。</p>
<p>以下摘自徐阿衡的回答：</p>
<ul>
<li>在闲聊性质的数据集上，上述 metric 和人工判断有一定微弱的关联 (only a small positive correlation on chitchat oriented Twitter dataset)</li>
<li>在技术类的数据集上，上述 metric 和人工判断完全没有关联(no correlation at all on the technical UDC)</li>
<li>当局限于一个特别具体的领域时，BLEU 会有不错的表现</li>
</ul>
<p>随着发展，还逐渐有了一些别的评价方法，比如使用GAN网络来评价生成的回复是否跟人类回复相似等等。。。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Common Useful Links]]></title>
      <url>http://sggo.me/2018/12/01/chatbot/chatbot-common-links/</url>
      <content type="html"><![CDATA[<p>Here are some useful links about Chatbot</p>
<a id="more"></a>
<h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><ul>
<li><a href="https://blog.csdn.net/leviopku/article/details/78508951" target="_blank" rel="external">TensorFlow中global_step的简单分析</a></li>
<li><a href="https://feisky.xyz/machine-learning/tensorflow/hello.html" target="_blank" rel="external">Tensorflow 入门， 很好的教程</a></li>
<li><a href="https://applenob.github.io/tf_10.html#tf.sequence_mask" target="_blank" rel="external">Tensorflow 学习笔记 数据处理 sequence_mask</a></li>
<li><a href="https://blog.csdn.net/qq_28808697/article/details/80648657" target="_blank" rel="external">tensorflow 学习笔记– tf.reduce_max、tf.sequence_mask</a></li>
<li><a href="https://blog.csdn.net/UESTC_C2_403/article/details/72779417" target="_blank" rel="external">tf.nn.embedding_lookup函数的用法</a></li>
<li><a href="https://ask.hellobi.com/blog/wenwen/11367" target="_blank" rel="external">使用Seq2Seq+attention实现简单的Chatbot</a></li>
<li><a href="https://blog.csdn.net/banana1006034246/article/details/75092388" target="_blank" rel="external">tf.strided_slice函数</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27685060" target="_blank" rel="external">Tensorlow 中文API: 常量(constants) - 5. tf.fill</a></li>
<li><a href="https://blog.csdn.net/mao_xiao_feng/article/details/53366163" target="_blank" rel="external">【TensorFlow】tf.concat的用法</a></li>
<li><a href="https://ithelp.ithome.com.tw/articles/10187814" target="_blank" rel="external">[第 28 天] 深度學習（2）TensorBoard</a></li>
<li><a href="https://blog.csdn.net/hu_guan_jie/article/details/78495297" target="_blank" rel="external">tf.identity的意义以及用例</a></li>
<li><a href="https://www.jianshu.com/p/c0c5f1bdbb88" target="_blank" rel="external">Tensorflow动态seq2seq使用总结（r1.3）- Loss Function</a></li>
<li><a href="https://blog.csdn.net/u012436149/article/details/53184847" target="_blank" rel="external">tensorflow学习笔记(二十一):tensorflow可视化</a></li>
<li><a href="https://blog.csdn.net/hustqb/article/details/80260002" target="_blank" rel="external">tensorflow—tf.gradients()简单实用教程</a></li>
<li><a href="https://blog.csdn.net/u013713117/article/details/56281715" target="_blank" rel="external">tf.clip_by_global_norm理解</a></li>
<li><a href="https://applenob.github.io/tf_6.html" target="_blank" rel="external">Tensorflow 学习笔记（六） ———— Optimizer</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/47929039" target="_blank" rel="external">Tensorflow中的Seq2Seq全家桶</a></li>
<li><a href="https://blog.csdn.net/weixin_41700555/article/details/85011957" target="_blank" rel="external">map() 与 nest.map_structure（） 的区别及用法</a></li>
</ul>
<h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><ul>
<li><a href="https://blog.csdn.net/wuzqChom/article/details/77073246" target="_blank" rel="external">sampled softmax Csdn wuzqChom</a></li>
<li><a href="https://www.zhihu.com/question/62070907/answer/218745719" target="_blank" rel="external">如何通俗理解sampled softmax机制？</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow Sequence_loss]]></title>
      <url>http://sggo.me/2018/12/01/tensorflow/sequence_loss/</url>
      <content type="html"><![CDATA[<p>sequence_loss 是 nlp算法 中非常重要的一个函数. rnn,lstm,attention都要用到这个函数.看下面代码:</p>
<a id="more"></a>
<h2 id="1-特殊的🌰"><a href="#1-特殊的🌰" class="headerlink" title="1. 特殊的🌰"></a>1. 特殊的🌰</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.seq2seq <span class="keyword">import</span> sequence_loss</span><br><span class="line"></span><br><span class="line">logits_np = np.array([</span><br><span class="line">    [[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]],</span><br><span class="line">    [[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]]</span><br><span class="line">])</span><br><span class="line">targets_np = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">], dtype=np.int32)</span><br><span class="line"></span><br><span class="line">logits = tf.convert_to_tensor(logits_np)</span><br><span class="line">targets = tf.convert_to_tensor(targets_np)</span><br><span class="line">cost = sequence_loss(logits=logits,</span><br><span class="line">                     targets=targets,</span><br><span class="line">                     weights=tf.ones_like(targets, dtype=tf.float64))</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    r = sess.run(cost)</span><br><span class="line">    print(r)</span><br></pre></td></tr></table></figure>
<p>先对每个[0.5,0.5,0.5,0.5]取softmax. softmax([0.5,0.5,0.5,0.5])=(0.25,0.25,0.25,0.25)然后再计算-ln(0.25)*6/6=1.38629436112.</p>
<h2 id="2-一般的🌰"><a href="#2-一般的🌰" class="headerlink" title="2. 一般的🌰"></a>2. 一般的🌰</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.seq2seq <span class="keyword">import</span> sequence_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2个句子，3个时刻，4个值(词汇表)</span></span><br><span class="line">output_np = np.array(</span><br><span class="line">    [</span><br><span class="line">        [[<span class="number">0.6</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>], [<span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>], [<span class="number">1.0</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>]],</span><br><span class="line">        [[<span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>], [<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>], [<span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>]]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">print(output_np.shape)</span><br><span class="line"><span class="comment"># 2个句子，</span></span><br><span class="line">target_np = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                      [<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>]],</span><br><span class="line">                     dtype=np.int32)</span><br><span class="line">print(target_np.shape)</span><br><span class="line">output = tf.convert_to_tensor(output_np, np.float32)</span><br><span class="line">target = tf.convert_to_tensor(target_np, np.int32)</span><br><span class="line"></span><br><span class="line">cost = sequence_loss(output,</span><br><span class="line">                     target,</span><br><span class="line">                     tf.ones_like(target, dtype=np.float32))</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    cost_r = sess.run(cost)</span><br><span class="line">    print(cost_r)</span><br></pre></td></tr></table></figure>
<p>这个代码作用和下面的tf.reduce_mean(softmax_cross_entropy_with_logits)作用一致.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(a)</span>:</span></span><br><span class="line">    max_index = np.max(a)</span><br><span class="line">    b = np.zeros((a.shape[<span class="number">0</span>], max_index + <span class="number">1</span>))</span><br><span class="line">    b[np.arange(a.shape[<span class="number">0</span>]), a] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line">logits_ph = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>))</span><br><span class="line">labels_ph = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>))</span><br><span class="line">output_np = np.array([</span><br><span class="line">    [<span class="number">0.6</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">1.0</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_ph, logits=logits_ph))</span><br><span class="line">target_np = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    cost_r = sess.run(cost, feed_dict=&#123;logits_ph: output_np, labels_ph: to_onehot(target_np)&#125;)</span><br><span class="line">    print(cost_r)</span><br></pre></td></tr></table></figure>
<p>再取交叉熵,再取平均.</p>
<h2 id="seq2seq-的应用"><a href="#seq2seq-的应用" class="headerlink" title="seq2seq 的应用"></a>seq2seq 的应用</h2><p>chatbot 应用 seq2seq 需要用到 sequence_loss</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[工程师应该如何注意身体健康？]]></title>
      <url>http://sggo.me/2018/11/26/tools/body-health/</url>
      <content type="html"><![CDATA[<p>IT 工程师群体是职业病高发人群（不说了，都是泪…）</p>
<a id="more"></a>
<p><strong>程序员常见的职业病</strong></p>
<ul>
<li>颈椎病</li>
<li>腰椎病</li>
<li>久坐对前列腺的危害以及肥胖问题</li>
<li>眼疲劳、用眼过度</li>
<li>饮食、作息不规律导致的胃病等一系列问题</li>
</ul>
<p><strong>全文目录：</strong></p>
<p>颈椎、腰椎病防治、久坐对前列腺、肛门的危害以及肥胖问题</p>
<ul>
<li>换一把人体工学椅，附不同价位品牌推荐</li>
<li>站立式办公</li>
<li>人体工学椅和站立式办公，应该选择哪个？</li>
</ul>
<p>用眼过度、眼疲劳</p>
<ul>
<li>护眼宝 软件</li>
<li>Gunnar 防蓝光眼镜</li>
<li>f.lux 软件</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.zhihu.com/question/20402689" target="_blank" rel="external">程序员应该如何注意身体健康？可能患哪些职业病？如何防治？</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Glove 和 fastText]]></title>
      <url>http://sggo.me/2018/11/16/nlp/glove-1/</url>
      <content type="html"><![CDATA[<p>本节介绍两种更新一点的词向量。分别是2014年Stanford发表的<a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="external">Glove</a>和2017年由Facebook发表的<a href="https://fasttext.cc/" target="_blank" rel="external">fastText</a>.</p>
<a id="more"></a>
<p>让我们先回顾一下 word2vec 中的跳字模型。将跳字模型中使用 softmax 运算表达的条件概率 $\mathbb{P}(w_j\mid w_i)$. 记作 $q_{ij}$，即</p>
<p>$$<br>q_{ij}=\frac{\exp(\mathbf{u}_j^\top \mathbf{v}_i)}{ \sum_{k \in \mathcal{V}} \text{exp}(\mathbf{u}_k^\top \mathbf{v}_i)},<br>$$</p>
<h2 id="GloVe-模型"><a href="#GloVe-模型" class="headerlink" title="GloVe 模型"></a>GloVe 模型</h2><p>有鉴于此，作为在 word2vec 之后提出的词嵌入模型，GloVe 采用了平方损失，并基于该损失对跳字模型做了三点改动 </p>
<h2 id="从条件概率比值理解-GloVe"><a href="#从条件概率比值理解-GloVe" class="headerlink" title="从条件概率比值理解 GloVe"></a>从条件概率比值理解 GloVe</h2><p>我们还可以从另外一个角度来理解 GloVe 词嵌入。沿用本节前面的符号，$\mathbb{P}(w_j \mid w_i)$ 表示数据集中以 $w_i$ 为中心词生成背景词 $w_j$ 的条件概率，并记作 $p_{ij}$。作为源于某大型语料库的真实例子，以下列举了两组分别以“ice”（“冰”）和“steam”（“蒸汽”）为中心词的条件概率以及它们之间的比值 [1]：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$w_k$=</th>
<th style="text-align:center">“solid”</th>
<th style="text-align:center">“gas”</th>
<th style="text-align:center">“water”</th>
<th style="text-align:center">“fashion”</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$p_1=\mathbb{P}(w_k\mid\text{“ice”})$</td>
<td style="text-align:center">0.00019</td>
<td style="text-align:center">0.000066</td>
<td style="text-align:center">0.003</td>
<td style="text-align:center">0.000017</td>
</tr>
<tr>
<td style="text-align:center">$p_2=\mathbb{P}(w_k\mid\text{“steam”})$</td>
<td style="text-align:center">0.000022</td>
<td style="text-align:center">0.00078</td>
<td style="text-align:center">0.0022</td>
<td style="text-align:center">0.000018</td>
</tr>
<tr>
<td style="text-align:center">$p_1/p_2$</td>
<td style="text-align:center">8.9</td>
<td style="text-align:center">0.085</td>
<td style="text-align:center">1.36</td>
<td style="text-align:center">0.96</td>
</tr>
</tbody>
</table>
<p>我们可以观察到以下现象：</p>
<blockquote>
<ul>
<li><p>对于与“ice”相关而与“steam”不相关的词 $w_k$，例如 $w_k=$“solid”（“固体”），我们期望条件概率比值较大，例如上表最后一行中的值 8.9；</p>
</li>
<li><p>对于与“ice”不相关而与 steam 相关的词 $w_k$，例如 $w_k=$“gas”（“气体”），我们期望条件概率比值较小，例如上表最后一行中的值 0.085；</p>
</li>
<li><p>对于与“ice”和“steam”都相关的词 $w_k$，例如 $w_k=$“water”（“水”），我们期望条件概率比值接近 1，例如上表最后一行中的值 1.36；</p>
</li>
<li><p>对于与“ice”和“steam”都不相关的词 $w_k$，例如 $w_k=$“fashion”（“时尚”），我们期望条件概率比值接近 1，例如上表最后一行中的值 0.96。</p>
</li>
</ul>
</blockquote>
<p>由此可见，条件概率比值能比较直观地表达词与词之间的关系。我们可以构造一个词向量函数使得它能有效拟合条件概率比值。我们知道，任意一个这样的比值需要三个词 $w_i$、$w_j$ 和 $w_k$。以 $w_i$ 作为中心词的条件概率比值为 ${p_{ij}}/{p_{ik}}$。我们可以找一个函数，它使用词向量来拟合这个条件概率比值</p>
<p>$$f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) \approx \frac{p_{ij}}{p_{ik}}.$$</p>
<p>这里函数 $f$ 可能的设计并不唯一，我们只需考虑一种较为合理的可能性。注意到条件概率比值是一个标量，我们可以将 $f$ 限制为一个标量函数：$f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) = f\left((\boldsymbol{u}_j - \boldsymbol{u}_k)^\top {\boldsymbol{v}}_i\right)$。交换索引 $j$ 和 $k$ 后可以看到函数 $f$ 应该满足 $f(x)f(-x)=1$，因此一个可能是 $f(x)=\exp(x)$，于是</p>
<p>$$f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) = \frac{\exp\left(\boldsymbol{u}_j^\top {\boldsymbol{v}}_i\right)}{\exp\left(\boldsymbol{u}_k^\top {\boldsymbol{v}}_i\right)} \approx \frac{p_{ij}}{p_{ik}}.$$</p>
<p>满足最右边约等号的一个可能是 $\exp\left(\boldsymbol{u}_j^\top {\boldsymbol{v}}_i\right) \approx \alpha p_{ij}$，这里 $\alpha$ 是一个常数。考虑到 $p_{ij}=x_{ij}/x_i$，取对数后 $\boldsymbol{u}_j^\top {\boldsymbol{v}}_i \approx \log\,\alpha + \log\,x_{ij} - \log\,x_i$。我们使用额外的偏差项来拟合 $- \log\,\alpha + \log\,x_i$，例如中心词偏差项 $b_i$ 和背景词偏差项 $c_j$：</p>
<p>$$\boldsymbol{u}_j^\top \boldsymbol{v}_i + b_i + c_j \approx \log(x_{ij}).$$</p>
<p>对上式左右两边取平方误差并加权，我们可以得到 GloVe 的损失函数。</p>
<p><img src="/images/nlp/glove-1.jpeg" width="900" img=""></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>在有些情况下，交叉熵损失函数有劣势。GloVe 采用了平方损失，并通过词向量拟合预先基于整个数据集计算得到的全局统计信息。</li>
<li>任意词的中心词向量和背景词向量在 GloVe 中是等价的。</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.youtube.com/watch?v=ioSnNLZSQq0&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax&amp;index=17" target="_blank" rel="external">动手学深度学习第十七课：GloVe、fastText和使用预训练的词向量</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow：第8章 LSTM & Bi-RNN & Deep RNN]]></title>
      <url>http://sggo.me/2018/11/10/tensorflow/tf-google-8-rnn-2/</url>
      <content type="html"><![CDATA[<p>LSTM 可以学习到距离很远的信息，解决了RNN无法长期依赖的问题。 </p>
<p>Bidirectional RNN 解决的是 当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关。</p>
<p>Deep RNNs 是 为了增强模型的表达能力，可以在网络中设置多个循环层，将每层 RNN 的输出传给下一层处理。</p>
<a id="more"></a>
<h2 id="1-LSTM"><a href="#1-LSTM" class="headerlink" title="1. LSTM"></a>1. LSTM</h2><p><img src="/images/tensorflow/tf-google-8-2.jpg" width="600"></p>
<h3 id="单层LSTM结构实现"><a href="#单层LSTM结构实现" class="headerlink" title="单层LSTM结构实现"></a>单层LSTM结构实现</h3><p>Tensorflow中实现了以下模块 :tf.nn.rnn_cell，包括了10个类：</p>
<ol>
<li>class BasicLSTMCell: Basic LSTM recurrent network cell.</li>
<li>class BasicRNNCell: The most basic RNN cell.</li>
<li>class DeviceWrapper: Operator that ensures an RNNCell runs on a particular device.</li>
<li>class DropoutWrapper: Operator adding dropout to inputs and outputs of the given cell.</li>
<li>class GRUCell: Gated Recurrent Unit cell (cf. <a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="external">http://arxiv.org/abs/1406.1078</a>).</li>
<li>class LSTMCell: Long short-term memory unit (LSTM) recurrent network cell.</li>
<li>class LSTMStateTuple: Tuple used by LSTM Cells for state_size, zero_state, and output state.</li>
<li>class MultiRNNCell: RNN cell composed sequentially of multiple simple cells.</li>
<li>class RNNCell: Abstract object representing an RNN cell.</li>
<li>class ResidualWrapper: RNNCell wrapper that ensures cell inputs are added to the outputs.</li>
</ol>
<p>在基本的 LSTM cell 中我们用第一个类来进行实现，他是 tf.contrib.rnn.BasicLSTMCell 同名类，定义在 <a href="https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/ops/rnn_cell_impl.py" target="_blank" rel="external">tensorflow/python/ops/rnn_cell_impl.py</a> 中</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">_init__(</span><br><span class="line">    num_units,</span><br><span class="line">    forget_bias=<span class="number">1.0</span>,</span><br><span class="line">    state_is_tuple=<span class="keyword">True</span>,</span><br><span class="line">    activation=<span class="keyword">None</span>,</span><br><span class="line">    reuse=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>其中参数表示：</p>
<ul>
<li>num_units 表示神经元的个数</li>
<li>forget_bias 就是LSTM们的忘记系数，如果等于1，就是不会忘记任何信息。如果等于0，就都忘记</li>
<li>state_is_tuple 默认就是True，表示返回的状态是一个 2-tuple (c_state, m_state)</li>
<li>activation 表示内部状态的激活函数，默认是 tanh</li>
<li>name 表示这一层的名字，同样名字的层会共享权重，如果为了避免这样的情况需要设置reuse=True</li>
</ul>
<p><img src="/images/tensorflow/tf-google-8-5.jpg" width="600"></p>
<p>采用<strong>BasicLSTMCell来声明LSTM结构如下所示</strong>，我们用伪代码和注释来进行说明。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个lstm结构，在tensorflow中通过一句话就能实现一个完整的lstm结构</span></span><br><span class="line"><span class="comment"># lstm_hidden_size 表示 LSTM cell 中神经元的数量。 cell其实就是一个RNN的网络。</span></span><br><span class="line">lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将lstm中的状态初始化为全0数组，BasicLSTMCell提供了zero_state来生成全0数组</span></span><br><span class="line"><span class="comment"># 在优化RNN时每次也会使用一个batch的训练样本，batch_size给出了一个batch的大小</span></span><br><span class="line">state = lstm.zero_state(batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = <span class="number">0.0</span></span><br><span class="line"><span class="comment"># 为了在训练中避免梯度弥散的情况，规定一个最大的序列长度num_steps</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    <span class="comment"># 在第一个时刻声明lstm结构中使用的变量，在之后的时刻都需要重复使用之前定义好的变量</span></span><br><span class="line">    <span class="keyword">if</span> i&gt;<span class="number">0</span>:</span><br><span class="line">        tf.get_variable_scope().reuse_variables()</span><br><span class="line">    <span class="comment"># 每一步处理时间序列中的一个时刻，将当前输入current_input和前一时刻状态state传入LSTM结构</span></span><br><span class="line">    <span class="comment"># 就可以得到当前lstm结构的输出lstm_output和更新后的状态state</span></span><br><span class="line">    lstm_output, state = lstm(current_input, state)</span><br><span class="line">    <span class="comment"># 将当前时刻lstm输出传入一个全连接层得到最后的输出</span></span><br><span class="line">    final_output = fully_connected(lstm_output)</span><br><span class="line">    <span class="comment"># 计算当前时刻输出的损失</span></span><br><span class="line">    loss += calc_loss(final_output, expected_output)</span><br></pre></td></tr></table></figure>
<h2 id="2-Bidirectional-RNN"><a href="#2-Bidirectional-RNN" class="headerlink" title="2. Bidirectional RNN"></a>2. Bidirectional RNN</h2><ul>
<li>Bidirectional RNN 双向递归神经网络. 该神经网络首先从正面理解一遍这句话，再从反方向理解一遍.</li>
</ul>
<p><img src="/images/tensorflow/tf-google-8-3.jpg" width="600"></p>
<h2 id="3-Deep-RNNs"><a href="#3-Deep-RNNs" class="headerlink" title="3. Deep RNNs"></a>3. Deep RNNs</h2><ul>
<li>Deep RNNs 深层，顾名思义就是层次增。 横向表示时间展开，纵向则是层次展开。</li>
</ul>
<!--<img src="/images/tensorflow/tf-google-8-4.jpg" width="600" />
-->
<p><img src="/images/deeplearning/C5W1-47_1.png" width="750"></p>
<p>MultiRNNCell的初始化方法如下</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    cells,</span><br><span class="line">    state_is_tuple=<span class="keyword">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>其中</p>
<ul>
<li>cells 表示 RNNCells 的 list，按照顺序从输入到输出来表示不同层的循环层</li>
<li>state_is_tuple 表示 接受和返回的状态都是 n-tuples, 其中 n = len(cells)，建议采用True</li>
</ul>
<p>同样MultiRNNCell提供了状态初始化的函数</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">zero_state(</span><br><span class="line">    batch_size,</span><br><span class="line">    dtype</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们接下来用伪代码和注释来说明Deep RNN如何实现</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个基本的LSTM结构作为循环体的基础结构，当然也支持使用其他的循环体结构</span></span><br><span class="line">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell</span><br><span class="line"><span class="comment"># 通过MultiRNNCells类来实现Deep RNN，其中number_of_layers表示有多少层，lstm_size表示每层的单元数量</span></span><br><span class="line">stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(lstm_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(number_of_layers)])</span><br><span class="line"><span class="comment"># 初始化并获取初始状态</span></span><br><span class="line">state = stacked_lstm.zeros_state(batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line">foor i <span class="keyword">in</span> range(len(num_steps)):</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">        tf.get_variable_scope().reuse_variables()</span><br><span class="line">    <span class="comment"># 根据当前输入current_input(x_t) 和前一阶段状态state(h_(t-1), s_(t-1)) 来前向计算得到当前状态state(h_t, s_t) 和输出stacked_lstm_output (h_t)</span></span><br><span class="line">    stacked_lstm_output, state = stacked_lstm(current_input, state)</span><br><span class="line">    <span class="comment"># 输出喂给全联接层</span></span><br><span class="line">    final_output = fully_connected(stacked_lstm_output)</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss += calc_loss(final_output, expected_output)</span><br><span class="line">    <span class="comment"># 进行优化</span></span><br><span class="line">    .......</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.ctolib.com/docs-Tensorflow-c-Tensorflow5.html" target="_blank" rel="external">深入浅出Tensorflow（五）：循环神经网络简介</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37070414" target="_blank" rel="external">Tensorflow实战(1): 实现深层循环神经网络</a></li>
<li><a href="https://zh.gluon.ai/" target="_blank" rel="external">zh.gluon.ai 动手学深度学习</a></li>
<li><a href="https://www.zhihu.com/question/272049149" target="_blank" rel="external">正确理解 cell 与 hidden size 的区别</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow： 第8章 循环神经网络 1]]></title>
      <url>http://sggo.me/2018/11/08/tensorflow/tf-google-8-rnn-1/</url>
      <content type="html"><![CDATA[<p>实战Google深度学习框架 笔记-第8章 循环神经网络-1-前向传播。 <a href="https://github.com/blair101/deep-learning-action/tree/master/tf.tutorials/Chapter8" target="_blank" rel="external">Github: RNN-1-Forward_Propagation.ipynb</a></p>
<a id="more"></a>
<p>运算的流程图可参考下面这张图</p>
<p><img src="/images/tensorflow/tf-google-8-1.jpg" width="800"></p>
<h2 id="RNN-Forward-Propagation"><a href="#RNN-Forward-Propagation" class="headerlink" title="RNN Forward Propagation"></a>RNN Forward Propagation</h2><p>RNN 前向传播知识回顾</p>
<p><img src="/images/deeplearning/C5W1-10_1.png" width="750"></p>
<blockquote>
<p>$a^{<0>}=\vec{0}$</0></p>
<p>$a^{<1>}=g_1(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$</1></0></1></p>
<p>$y^{<1>}=g_2(W_{ya}a^{<1>}+b_y)$</1></1></p>
<p>$a^{&lt;{t}&gt;}=g_1(W_{aa}a^{&lt;{t-1}&gt;}+W_{ax}x^{&lt;{t}&gt;}+b_a)$</p>
<p>$y^{&lt;{t}&gt;}=g_2(W_{ya}a^{&lt;{t}&gt;}+b_y)$</p>
<p>激活函数：<strong>$g_1$</strong> 一般为 <strong><code>tanh</code>函数</strong> (或者是 <strong><code>Relu</code>函数</strong>)，<strong>$g_2$</strong> 一般是 <strong><code>Sigmod</code>函数</strong>.</p>
<p>注意: 参数的下标是有顺序含义的，如 $W_{ax}$ 下标的第一个参数表示要计算的量的类型，即要计算 $a$ 矢量，第二个参数表示要进行乘法运算的数据类型，即需要与 $x$ 矢量做运算。如 $W_{ax} x^{t}\rightarrow{a}$</p>
</blockquote>
<h2 id="1-定义RNN的参数"><a href="#1-定义RNN的参数" class="headerlink" title="1. 定义RNN的参数"></a>1. 定义RNN的参数</h2><p>这个例子是用np写的，没用到tensorflow</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化， state = a^&#123;&lt;0&gt;&#125; 与 定义 X 时间序列参数</span></span><br><span class="line">X = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">state = [<span class="number">0.0</span>, <span class="number">0.0</span>] <span class="comment"># a^&#123;&lt;0&gt;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分开定义不同输入部分的权重以方便操作</span></span><br><span class="line">w_cell_state = np.asarray([[<span class="number">0.1</span>, <span class="number">0.2</span>], [<span class="number">0.3</span>, <span class="number">0.4</span>]]) <span class="comment"># W_&#123;aa&#125;</span></span><br><span class="line">w_cell_input = np.asarray([[<span class="number">0.5</span>, <span class="number">0.6</span>]]) <span class="comment"># W_&#123;ax&#125;</span></span><br><span class="line"></span><br><span class="line">b_cell = np.asarray([<span class="number">0.1</span>, <span class="number">-0.1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义用于输出的全连接层参数， 与 state = a^&#123;&lt;i&gt;&#125; 的 shape 相反置</span></span><br><span class="line">w_output = np.asarray([[<span class="number">0.1</span>], [<span class="number">2.0</span>]])</span><br><span class="line">b_output = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>
<h2 id="2-执行前向传播的过程"><a href="#2-执行前向传播的过程" class="headerlink" title="2. 执行前向传播的过程"></a>2. 执行前向传播的过程</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按照时间顺序执行循环审计网络的前向传播过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">    <span class="comment"># 计算循环体中的全连接层神经网络</span></span><br><span class="line">    before_activation = np.dot(state, w_cell_state) + X[i] * w_cell_input + b_cell</span><br><span class="line">    </span><br><span class="line">    state = np.tanh(before_activation)</span><br><span class="line">    final_output = np.dot(state, w_output) + b_output</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"iteration round:"</span>, i+<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">"before activation: "</span>, before_activation)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"state: "</span>, state)</span><br><span class="line">    print(<span class="string">"output: "</span>, final_output)</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">iteration round: 1</span><br><span class="line">before activation:  [[0.95107374 1.0254142 ]]</span><br><span class="line">state:  [[0.74026877 0.7720626 ]]</span><br><span class="line">output:  [[1.71815207]]</span><br><span class="line">iteration round: 2</span><br><span class="line">before activation:  [[1.40564566 1.55687879]]</span><br><span class="line">state:  [[0.88656589 0.91491336]]</span><br><span class="line">output:  [[2.0184833]]</span><br></pre></td></tr></table></figure>
<p>和其他神经网络类似，在定义完损失函数之后，套用第4章中介绍的优化框架TensorFlow就可以<strong>自动完成模型训练</strong>的过程。这里唯一需要特别指出的是，理论上循环神经网络可以支持任意长度的序列，然而在实际中，如果序列过长会导致优化时出现梯度消散的问题（<strong>the vanishing gradient problem</strong>） (8) ，所以实际中一般会<strong>规定一个最大长度</strong>，当序列长度超过规定长度之后会对序列进行截断。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/31539492" target="_blank" rel="external">知乎：《TensorFlow：实战Google深度学习框架》笔记、代码及勘误-第8章 循环神经网络-1-前向传播</a></li>
<li><a href="http://b.7dtime.com/B076DGNXP1/13/0.html" target="_blank" rel="external">7天时间： 循环神经网络简介 (1)</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow - tf.contrib.learn 创建 Estimator]]></title>
      <url>http://sggo.me/2018/11/04/tensorflow/tf-5.4-contrib-learn-Estimator/</url>
      <content type="html"><![CDATA[<p>tf.contrib.learn 框架可以通过其高级别的 Estimator API 轻松构建和训练机器学习模型.</p>
<p>Estimator 提供您可以实例化的类以快速配置常见的模型类型，如 regressors 和 classifiers：</p>
<a id="more"></a> 
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearClassifier" target="_blank" rel="external">tf.contrib.learn.LinearClassifier</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearRegressor" target="_blank" rel="external">tf.contrib.learn.LinearRegressor</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier" target="_blank" rel="external">tf.contrib.learn.DNNClassifier</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNRegressor" target="_blank" rel="external">tf.contrib.learn.DNNRegressor</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn" target="_blank" rel="external">tf.contrib.learn……</a></li>
</ul>
<p><strong>完整代码</strong>：</p>
<ul>
<li><a href="https://github.com/blair101/deep-learning-action/tree/master/tf.contrib.learn/tf-5.4-Estimator" target="_blank" rel="external">Github 鲍鱼年龄预测器 r1.11 Abalone Age Predictor</a></li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029584" target="_blank" rel="external">在tf.contrib.learn中创建估算器</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow - tf.contrib.learn 基础的记录和监控教程]]></title>
      <url>http://sggo.me/2018/11/04/tensorflow/tf-5.3-contrib-learn-MonitorAPI/</url>
      <content type="html"><![CDATA[<p>训练模型时，实时跟踪和评估进度通常很有价值。</p>
<p>学习使用TensorFlow的日志记录功能和MonitorAPI来监督正在用神经网络分类器分类irises的训练情况。</p>
<a id="more"></a> 
<p><strong>完整代码：</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data sets</span></span><br><span class="line">IRIS_TRAINING = <span class="string">"iris_training.csv"</span></span><br><span class="line">IRIS_TEST = <span class="string">"iris_test.csv"</span></span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># First download iris_training.csv and iris_test.csv</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load datasets.</span></span><br><span class="line">    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">        filename=IRIS_TRAINING,</span><br><span class="line">        target_dtype=np.int,</span><br><span class="line">        features_dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">        filename=IRIS_TEST,</span><br><span class="line">        target_dtype=np.int,</span><br><span class="line">        features_dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Specify that all features have real-value data</span></span><br><span class="line">    feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</span><br><span class="line">    <span class="comment"># [_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float32, normalizer=None)]</span></span><br><span class="line"></span><br><span class="line">    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(</span><br><span class="line">        test_set.data,</span><br><span class="line">        test_set.target,</span><br><span class="line">        every_n_steps=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></span><br><span class="line">    classifier = tf.contrib.learn.DNNClassifier(</span><br><span class="line">        feature_columns=feature_columns,</span><br><span class="line">        hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</span><br><span class="line">        n_classes=<span class="number">3</span>,</span><br><span class="line">        model_dir=<span class="string">"/tmp/iris_model"</span>,</span><br><span class="line">        config=tf.contrib.learn.RunConfig(save_checkpoints_secs=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fit model.</span></span><br><span class="line">    classifier.fit(x=training_set.data,</span><br><span class="line">                   y=training_set.target,</span><br><span class="line">                   steps=<span class="number">2000</span>,</span><br><span class="line">                   monitors=[validation_monitor])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><a href="https://github.com/blair101/TensorFlowExamples/tree/master/tf.contrib.learn/tf-5.3-validationMonitor-Iris" target="_blank" rel="external">代码参见 Blair‘s Github - tf.contrib.learn 基础的记录和监控教程</a></li>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029489" target="_blank" rel="external">教程参见 cwiki.apachecn.org tf.contrib.learn 基础的记录和监控教程</a></li>
</ul>
</blockquote>
<h2 id="1-使用TensorFlow启用日志记录"><a href="#1-使用TensorFlow启用日志记录" class="headerlink" title="1. 使用TensorFlow启用日志记录"></a>1. 使用TensorFlow启用日志记录</h2><p>默认情况下，TensorFlow被配置在WARN日志级别，但是当跟踪模型训练时，您需要将级别调整为INFO。</p>
<p>代码的开头（在import导入之后）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tf.logging.set_verbosity(tf.logging.INFO)</span><br></pre></td></tr></table></figure>
<p>运行代码时，会看到如下所示的其他日志输出：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">INFO:tensorflow:loss = 1.18812, step = 1</span><br><span class="line">INFO:tensorflow:loss = 0.210323, step = 101</span><br><span class="line">INFO:tensorflow:loss = 0.109025, step = 201</span><br></pre></td></tr></table></figure>
<p>使用INFO级别日志记录，tf.contrib.learn会在每100步之后自动将training-loss metrics输出到stderr。</p>
<h2 id="2-配置验证监视器进行流评估"><a href="#2-配置验证监视器进行流评估" class="headerlink" title="2. 配置验证监视器进行流评估"></a>2. 配置验证监视器进行流评估</h2><p>记录训练损失有助于了解您的模型是否收敛，但如果您想进一步了解训练中发生的情况怎么办？tf.contrib.learn提供了几个高级别Monitor，您可以附加到您的fit操作，以进一步跟踪metrics/调试模型训练期间的更低级别TensorFlow操作</p>
<h3 id="2-1-每隔N步评估"><a href="#2-1-每隔N步评估" class="headerlink" title="2.1 每隔N步评估"></a>2.1 每隔N步评估</h3><p>对于iris神经网络分类器，在记录训练损失时，您可能还需要同时对测试数据进行评估，以了解该模型的泛化程度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(</span><br><span class="line">    test_set.data,</span><br><span class="line">    test_set.target,</span><br><span class="line">    every_n_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p>将此代码放在实例化classifier那行之前。</p>
<p>ValidationMonitor依靠保存的checkpoints执行评估操作，因此您需要添加包含save_checkpoints_secs的<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig" target="_blank" rel="external">tf.contrib.learn.RunConfig</a>去修改classifier的实例化，该参数指定在训练期间经过多少秒保存checkpoint。</p>
<p>由于iris数据集相当小，因此训练速度很快，设置save_checkpoints_secs为1（每1秒保存checkpoint）：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">classifier = tf.contrib.learn.DNNClassifier(</span><br><span class="line">    feature_columns=feature_columns,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</span><br><span class="line">    n_classes=<span class="number">3</span>,</span><br><span class="line">    model_dir=<span class="string">"/tmp/iris_model"</span>,</span><br><span class="line">    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>validation_monitor，更新包快调用monitors参数的fit，该参数在模型训练期间生成包含所有monitors的list：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">classifier.fit(x=training_set.data,</span><br><span class="line">               y=training_set.target,</span><br><span class="line">               steps=<span class="number">2000</span>,</span><br><span class="line">               monitors=[validation_monitor])</span><br></pre></td></tr></table></figure>
<p>重新运行代码时，您应该在日志输出中看到验证metrics，例如： (但是我这里试验的时候，并没有出现)</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">INFO:tensorflow:Validation (step <span class="number">50</span>): loss = <span class="number">1.71139</span>, global_step = <span class="number">0</span>, accuracy = <span class="number">0.266667</span></span><br><span class="line">...</span><br><span class="line">INFO:tensorflow:Validation (step <span class="number">300</span>): loss = <span class="number">0.0714158</span>, global_step = <span class="number">268</span>, accuracy = <span class="number">0.966667</span></span><br><span class="line">...</span><br><span class="line">INFO:tensorflow:Validation (step <span class="number">1750</span>): loss = <span class="number">0.0574449</span>, global_step = <span class="number">1729</span>, accuracy = <span class="number">0.966667</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-使用MetricSpec定义Evaluation-Metrics"><a href="#2-2-使用MetricSpec定义Evaluation-Metrics" class="headerlink" title="2.2 使用MetricSpec定义Evaluation Metrics"></a>2.2 使用MetricSpec定义Evaluation Metrics</h3><ul>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029489" target="_blank" rel="external">详情参见 cwiki.apachecn.org tf.contrib.learn 基础的记录和监控教程</a></li>
</ul>
<h3 id="2-3-通过ValidationMonitor提前停止训练"><a href="#2-3-通过ValidationMonitor提前停止训练" class="headerlink" title="2.3 通过ValidationMonitor提前停止训练"></a>2.3 通过ValidationMonitor提前停止训练</h3><ul>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029489" target="_blank" rel="external">详情参见 cwiki.apachecn.org tf.contrib.learn 基础的记录和监控教程</a></li>
</ul>
<h2 id="3-用TensorBoard可视化日志数据"><a href="#3-用TensorBoard可视化日志数据" class="headerlink" title="3. 用TensorBoard可视化日志数据"></a>3. 用TensorBoard可视化日志数据</h2><p>读取通过ValidationMonitor在训练期间产生大量关于模型性能的原始数据的日志，此数据的可视化，对进一步了解趋势可能会有帮助，例如准确性如何随着步数而变化。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tensorboard --logdir=/tmp/iris_model/</span><br><span class="line">Starting TensorBoard 39 on port 6006</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029489" target="_blank" rel="external">tf.contrib.learn基础的记录和监控教程</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[大数据平台CDH6.0集群在线安装]]></title>
      <url>http://sggo.me/2018/11/02/hadoop/ops-install-CDH6.0.1/</url>
      <content type="html"><![CDATA[<p>介绍了 CDH 集群的搭建与安装</p>
<p>标签： Cloudera-Manager CDH Hadoop 部署 集群</p>
<a id="more"></a>
<blockquote>
<p>目前Hadoop比较流行的主要有2个版本，Apache和Cloudera版本。</p>
<ul>
<li>Apache Hadoop：维护人员比较多，更新频率比较快，但是稳定性比较差。</li>
<li>Cloudera Hadoop（CDH）：CDH：Cloudera公司的发行版本，基于Apache Hadoop的二次开发，优化了组件兼容和交互接口、简化安装配置、增加Cloudera兼容特性。</li>
</ul>
</blockquote>
<h2 id="1-操作环境"><a href="#1-操作环境" class="headerlink" title="1. 操作环境"></a>1. 操作环境</h2><ul>
<li>CentOS 7.3 x64 （4C/10G/50G） </li>
<li>Cloudera Manager：6.0.1  </li>
<li>CDH: 6.0.1</li>
</ul>
<p>相关包地址</p>
<p>Cloudera Manager下载地址：<a href="https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/" target="_blank" rel="external">https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/</a></p>
<blockquote>
<ul>
<li>cloudera-manager-agent-6.0.0-530873.el7.x86_64.rpm</li>
<li>cloudera-manager-daemons-6.0.0-530873.el7.x86_64.rpm</li>
<li>cloudera-manager-server-6.0.0-530873.el7.x86_64.rpm</li>
<li>cloudera-manager-server-db-2-6.0.0-530873.el7.x86_64.rpm</li>
<li>oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm</li>
</ul>
</blockquote>
<p>CDH安装包地址：<a href="https://archive.cloudera.com/cdh6/6.0.0/parcels/" target="_blank" rel="external">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a></p>
<blockquote>
<ul>
<li>CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel</li>
<li>CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256</li>
<li>manifest.json</li>
</ul>
</blockquote>
<p>注意：以下操作均用root用户操作。</p>
<h2 id="2-网络配置-所有节点"><a href="#2-网络配置-所有节点" class="headerlink" title="2. 网络配置(所有节点)"></a>2. 网络配置(所有节点)</h2><p><strong>在所有节点上把IP和主机名的对应关系写入</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注释掉原有的语句, 增加：</span></span><br><span class="line">192.192.0.25 server</span><br><span class="line">192.192.0.26 chdagent1</span><br><span class="line">192.192.0.27 chdagent2</span><br></pre></td></tr></table></figure>
<p><strong>在相应的节点主机上修改主机名</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br><span class="line"></span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=cdhserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改或者添加 HOSTNAME=cdhserver</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>cdhserver 是你起的的主机名字</p>
</blockquote>
<p><strong>执行命令</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hostname cdhserver</span><br></pre></td></tr></table></figure>
<p>CentOS7要多执行以下这步：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hostnamectl set-hostname cdhserver</span><br></pre></td></tr></table></figure>
<h2 id="3-打通SSH"><a href="#3-打通SSH" class="headerlink" title="3. 打通SSH"></a>3. 打通SSH</h2><p>设置ssh无密码登陆（所有节点）</p>
<h2 id="4-关闭防火墙和SELinux"><a href="#4-关闭防火墙和SELinux" class="headerlink" title="4. 关闭防火墙和SELinux"></a>4. 关闭防火墙和SELinux</h2><p>注意： 需要在所有的节点上执行，因为涉及到的端口太多了，临时关闭防火墙是为了安装起来更方便，安装完毕后可以根据需要设置防火墙策略，保证集群安全。</p>
<p>关闭防火墙并关闭自启动：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br></pre></td></tr></table></figure>
<h2 id="5-所有节点配置NTP服务"><a href="#5-所有节点配置NTP服务" class="headerlink" title="5. 所有节点配置NTP服务"></a>5. 所有节点配置NTP服务</h2><p>集群中所有主机必须保持时间同步，如果时间相差较大会引起各种问题。 具体思路如下：</p>
<p>master节点作为ntp服务器与外界对时中心同步时间，随后对所有datanode节点提供时间同步服务。</p>
<p>所有datanode节点以master节点为基础同步时间。</p>
<p>所有节点安装相关组件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ntp</span><br></pre></td></tr></table></figure>
<p>启动服务： </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start ntpd</span><br></pre></td></tr></table></figure>
<p>配置开机启动：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> ntpd</span><br></pre></td></tr></table></figure>
<h2 id="6-安装-python-2-7"><a href="#6-安装-python-2-7" class="headerlink" title="6. 安装 python 2.7"></a>6. 安装 python 2.7</h2><p>必须是python2.7版本，CentOS 7 系统可以不用装，系统自带的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#下载并安装EPEL，安装python-pip，psycopg2有依赖</span><br><span class="line">[root@localhost ~]# wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">[root@localhost ~]# rpm -ivh epel-release-latest-7.noarch.rpm</span><br><span class="line">[root@localhost ~]# yum repolist  #检查是否已添加至源列表</span><br></pre></td></tr></table></figure>
<p>升级软件依赖版本</p>
<p>Starting with CDH 6, PostgreSQL-backed Hue requires the Psycopg2 version to be at least 2.5.4</p>
<p>首先安装epel扩展源：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install epel-release</span><br><span class="line">yum -y install python-pip</span><br><span class="line">pip install --upgrade psycopg2</span><br></pre></td></tr></table></figure>
<h2 id="7-准备Parcels，用以安装CDH6"><a href="#7-准备Parcels，用以安装CDH6" class="headerlink" title="7. 准备Parcels，用以安装CDH6"></a>7. 准备Parcels，用以安装CDH6</h2><p>将CHD6相关的Parcel包放到主节点的/opt/cloudera/parcel-repo/目录中，如果没有此目录，可以自己创建。</p>
<blockquote>
<ul>
<li>CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel</li>
<li>CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256</li>
<li>manifest.json</li>
</ul>
</blockquote>
<p>注意：最后将CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256，重命名为CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha</p>
<p><strong>安装repo</strong>: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/</span><br></pre></td></tr></table></figure>
<p><strong>导入GPG key</strong>: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm --import https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPM-GPG-KEY-cloudera</span><br></pre></td></tr></table></figure>
<p><strong>JDK install</strong>: </p>
<blockquote>
<p>yum install oracle-j2sdk1.8</p>
<p>注意 ： </p>
<ul>
<li>使用 yum 下载，需要确定版本与安装CDH6官方要求的需要的版本一致</li>
<li>也可不使用 yum 安装，使用自己下载 JDK，然后手动绿色安装配置</li>
<li>也可在安装 CM 的时候，再根据提示来安装需要的 JDK</li>
</ul>
<p>三种方式任选其一便可</p>
</blockquote>
<p><strong>yum安装CM</strong>: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install cloudera-manager-server</span><br></pre></td></tr></table></figure>
<h2 id="8-安装MySql"><a href="#8-安装MySql" class="headerlink" title="8. 安装MySql"></a>8. 安装MySql</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">yum update</span><br><span class="line">yum install mysql-server</span><br><span class="line">systemctl start mysqld</span><br><span class="line">systemctl <span class="built_in">enable</span> mysqld</span><br></pre></td></tr></table></figure>
<p>初始化Mysql</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/bin/mysql_secure_installation</span><br></pre></td></tr></table></figure>
<p>配置JDBC</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz</span><br><span class="line">tar zxvf mysql-connector-java-5.1.46.tar.gz</span><br><span class="line">mkdir -p /usr/share/java/</span><br><span class="line"><span class="built_in">cd</span> mysql-connector-java-5.1.46</span><br><span class="line">cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>
<p>建库：根据官方文档提供的命名建库，方便记忆。(在CM配置CDH的时候会用到这些库名)</p>
<blockquote>
<p>Set up the Cloudera Manager Database：/opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm</p>
</blockquote>
<p>出现如下日志：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_141-cloudera</span><br><span class="line">Verifying that we can write to /etc/cloudera-scm-server</span><br><span class="line">Creating SCM configuration file <span class="keyword">in</span> /etc/cloudera-scm-server</span><br><span class="line">Executing:  /usr/java/jdk1.8.0_141-cloudera/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.</span><br><span class="line">[main] DbCommandExecutor INFO  Successfully connected to database.</span><br><span class="line">All <span class="keyword">done</span>, your SCM database is configured correctly!</span><br></pre></td></tr></table></figure>
<h2 id="9-启动CM服务"><a href="#9-启动CM服务" class="headerlink" title="9. 启动CM服务"></a>9. 启动CM服务</h2><p>启动：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start cloudera-scm-server</span><br></pre></td></tr></table></figure>
<p>查看日志：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tail <span class="_">-f</span> /var/<span class="built_in">log</span>/cloudera-scm-server/cloudera-scm-server.log</span><br></pre></td></tr></table></figure>
<blockquote>
<p>出现：INFO WebServerImpl:com.cloudera.server.cmf.WebServerImpl: Started Jetty server.则表示服务正常启动</p>
</blockquote>
<p>登录 http://<server_host>:7180 账号：admin</server_host></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/caolijun1166/article/details/82714387" target="_blank" rel="external">CDH 6.0.0 搭建</a></li>
<li><a href="http://blog.51cto.com/pizibaidu/2174297" target="_blank" rel="external">CDH6.0.0详细安装教程及所遇到的问题</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/installation.html" target="_blank" rel="external">官方文档 - Cloudera Installation Guide</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow - tf.contrib.learn 构建输入函数]]></title>
      <url>http://sggo.me/2018/11/01/tensorflow/tf-5.2-contrib-learn-Input-fn/</url>
      <content type="html"><![CDATA[<p>介绍如何在tf.contrib.learn中创建输入函数。了解如何构建input_fn去预处理和将数据输到模型中的概述。</p>
<p>实现利用input_fn将训练，评估和预测数据提供给神经网络回归器，用于预测房价中位数。</p>
<a id="more"></a> 
<h2 id="1-利用input-fn自定义输入Pipelines"><a href="#1-利用input-fn自定义输入Pipelines" class="headerlink" title="1. 利用input_fn自定义输入Pipelines"></a>1. 利用input_fn自定义输入Pipelines</h2><p>当使用 tf.contrib.learn 训练神经网络，它可以直接通过您的特征和目标数据进行训练，分析或预测操作。这是一个从tf.contrib.learn快速入门教程中获取的示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)</span><br><span class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)</span><br><span class="line">...</span><br><span class="line"> </span><br><span class="line">classifier.fit(x=training_set.data,</span><br><span class="line">               y=training_set.target,</span><br><span class="line">               steps=<span class="number">2000</span>)</span><br></pre></td></tr></table></figure>
<p>当需要对源数据进行少量操作时，这种方法运行良好。但是在需要更多特征工程的情况下， tf.contrib.learn支持使用自定义输入函数（input_fn）将预处理和pipeline数据的逻辑封装到模型中。</p>
<h3 id="1-1-解析input-fn"><a href="#1-1-解析input-fn" class="headerlink" title="1.1 解析input_fn"></a>1.1 解析input_fn</h3><p>以下代码阐述了输入函数的基本框架：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">()</span>:</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Preprocess your data here...</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># ...then return 1) a mapping of feature columns to Tensors with</span></span><br><span class="line">    <span class="comment"># the corresponding feature data, and 2) a Tensor containing labels</span></span><br><span class="line">    <span class="keyword">return</span> feature_cols, labels</span><br></pre></td></tr></table></figure>
<p>输入函数部分包含用于预处理输入数据的特定逻辑，例如擦除不良示例或feature scaling。</p>
<p>输入函数必须返回以下两个值，其中包含要馈送到模型中的最终特征和标签数据（如上述代码框架所示）：</p>
<blockquote>
<p>feature_cols</p>
<ul>
<li>将特征列名称映射到相应特征数据的Tensors（或SparseTensors）的keys/values对的字典。</li>
</ul>
<p>labels</p>
<ul>
<li>包含您的标签（目标）值的张量：您的模型预测的值。</li>
</ul>
</blockquote>
<h3 id="1-2-将特征数据转换为张量"><a href="#1-2-将特征数据转换为张量" class="headerlink" title="1.2 将特征数据转换为张量"></a>1.2 将特征数据转换为张量</h3><p>如果特征/标签数据存储在pandas 数据架构或numpy数组，你需要将其转换为Tensor在它从input_fn返回之前。</p>
<p>对于连续数据，您可以使用tf.constant创建和填充Tensor：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">feature_column_data = [<span class="number">1</span>, <span class="number">2.4</span>, <span class="number">0</span>, <span class="number">9.9</span>, <span class="number">3</span>, <span class="number">120</span>]</span><br><span class="line">feature_tensor = tf.constant(feature_column_data)</span><br></pre></td></tr></table></figure>
<h2 id="2-详见代码"><a href="#2-详见代码" class="headerlink" title="2. 详见代码"></a>2. 详见代码</h2><ul>
<li><a href="https://github.com/blair101/TensorFlowExamples/blob/master/tf.contrib.learn/tf.contrib.learn构建输入函数.ipynb" target="_blank" rel="external">Blair’s Github tf.contrib.learn构建输入函数</a></li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义数据集中的列名COLUMNS。 为了区分标签的 feature，还要定义 FEATURES 和 LABEL。</span></span><br><span class="line"></span><br><span class="line">COLUMNS = [<span class="string">"crim"</span>, <span class="string">"zn"</span>, <span class="string">"indus"</span>, <span class="string">"nox"</span>, <span class="string">"rm"</span>, <span class="string">"age"</span>,</span><br><span class="line">           <span class="string">"dis"</span>, <span class="string">"tax"</span>, <span class="string">"ptratio"</span>, <span class="string">"medv"</span>]</span><br><span class="line">FEATURES = [<span class="string">"crim"</span>, <span class="string">"zn"</span>, <span class="string">"indus"</span>, <span class="string">"nox"</span>, <span class="string">"rm"</span>,</span><br><span class="line">            <span class="string">"age"</span>, <span class="string">"dis"</span>, <span class="string">"tax"</span>, <span class="string">"ptratio"</span>]</span><br><span class="line">LABEL = <span class="string">"medv"</span></span><br><span class="line"> </span><br><span class="line">training_set = pd.read_csv(<span class="string">"boston_train.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</span><br><span class="line">                           skiprows=<span class="number">1</span>, names=COLUMNS)</span><br><span class="line">test_set = pd.read_csv(<span class="string">"boston_test.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</span><br><span class="line">                       skiprows=<span class="number">1</span>, names=COLUMNS)</span><br><span class="line">prediction_set = pd.read_csv(<span class="string">"boston_predict.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</span><br><span class="line">                             skiprows=<span class="number">1</span>, names=COLUMNS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(training_set.head(2))</span></span><br><span class="line"><span class="comment"># print(test_set.head(2))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义FeatureColumns并创建回归</span></span><br><span class="line">feature_cols = [tf.contrib.layers.real_valued_column(k) <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES]</span><br><span class="line">feature_cols</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 实例化一个DNNRegressor神经网络回归模型</span></span><br><span class="line">regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,</span><br><span class="line">                                          hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">                                          model_dir=<span class="string">"/tmp/boston_model"</span>)</span><br><span class="line">                                          </span><br><span class="line"><span class="comment"># 4. 构建input_fn                                          </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(data_set)</span>:</span></span><br><span class="line">    feature_cols = &#123;k: tf.constant(data_set[k].values)</span><br><span class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES&#125;</span><br><span class="line">    labels = tf.constant(data_set[LABEL].values)</span><br><span class="line">    <span class="keyword">return</span> feature_cols, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 训练回归器</span></span><br><span class="line">regressor.fit(input_fn=<span class="keyword">lambda</span>: input_fn(training_set), steps=<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 评估模型</span></span><br><span class="line">ev = regressor.evaluate(input_fn=<span class="keyword">lambda</span>: input_fn(test_set), steps=<span class="number">1</span>)</span><br><span class="line">loss_score = ev[<span class="string">"loss"</span>]</span><br><span class="line">print(<span class="string">"Loss: &#123;0:f&#125;"</span>.format(loss_score))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 预测</span></span><br><span class="line">y = regressor.predict_scores(input_fn=<span class="keyword">lambda</span>: input_fn(prediction_set), batch_size=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># .predict() returns an iterator; convert to a list and print predictions</span></span><br><span class="line">predictions = list(itertools.islice(y, <span class="number">6</span>))</span><br><span class="line">print(<span class="string">"Predictions: &#123;&#125;"</span>.format(str(predictions)))</span><br></pre></td></tr></table></figure>
<h2 id="3-其他资源"><a href="#3-其他资源" class="headerlink" title="3. 其他资源"></a>3. 其他资源</h2><p>为神经网络回归器创建一个input_fn。要了解有关将input_fn用于其他类型模型的更多信息，请查看以下资源：</p>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/linear" target="_blank" rel="external">TensorFlow的大规模线性模型</a>：介绍TensorFlow中线性模型，提供转换输入数据的特征列和技术的高级概述。</li>
<li><a href="https://www.tensorflow.org/tutorials/wide" target="_blank" rel="external">TensorFlow线性模型教程</a>：FeatureColumns和 input_fn，线性分类模型，据人口财产普查数据预测收入范围。</li>
<li><a href="https://www.tensorflow.org/tutorials/wide_and_deep" target="_blank" rel="external">TensorFlow Wide＆Deep Learning教程</a>：基于<a href="https://www.tensorflow.org/tutorials/wide" target="_blank" rel="external">线性模型教程</a>，本教程涵盖 FeatureColumn 和 input_fn，创建了一个“宽而深”的模型，它融合了一个线性模型和使用 DNNLinearCombinedClassifier 的神经网络 。</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029487" target="_blank" rel="external">使用tf.contrib.learn构建输入函数</a></li>
<li><a href="http://funhacks.net/2017/02/13/itertools/" target="_blank" rel="external">高效的 itertools 模块</a></li>
<li><a href="http://www.cnblogs.com/datablog/p/6127000.html" target="_blank" rel="external">pandas.read_csv参数整理</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow - tf.contrib.learn 快速入门]]></title>
      <url>http://sggo.me/2018/10/31/tensorflow/tf-5.1-contrib-learn-Quickstart/</url>
      <content type="html"><![CDATA[<p>TensorFlow 的高级机器学习API（tf.contrib.learn）可以轻松配置，训练和评估各种机器学习模型。</p>
<p>使用tf.contrib.learn构建 神经网络 分类器并在<strong>Iris</strong>数据集上进行训练. 基于花萼/花瓣几何形状来预测花种。</p>
<a id="more"></a> 
<p>依照以下五个步骤编写代码：</p>
<ol>
<li>将包含Iris训练/测试数据的CSV加载到TensorFlow数据集中</li>
<li>构建神经网络分类器</li>
<li>使用训练数据拟合模型</li>
<li>评估模型的准确性</li>
<li>分类新样本</li>
</ol>
<h2 id="1-完整的神经网络源代码"><a href="#1-完整的神经网络源代码" class="headerlink" title="1. 完整的神经网络源代码"></a>1. 完整的神经网络源代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data sets</span></span><br><span class="line">IRIS_TRAINING = <span class="string">"iris_training.csv"</span></span><br><span class="line">IRIS_TRAINING_URL = <span class="string">"http://download.tensorflow.org/data/iris_training.csv"</span></span><br><span class="line"></span><br><span class="line">IRIS_TEST = <span class="string">"iris_test.csv"</span></span><br><span class="line">IRIS_TEST_URL = <span class="string">"http://download.tensorflow.org/data/iris_test.csv"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># First download iris_training.csv and iris_test.csv</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load datasets.</span></span><br><span class="line">    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">        filename=IRIS_TRAINING,</span><br><span class="line">        target_dtype=np.int,</span><br><span class="line">        features_dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">        filename=IRIS_TEST,</span><br><span class="line">        target_dtype=np.int,</span><br><span class="line">        features_dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Specify that all features have real-value data</span></span><br><span class="line">    feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></span><br><span class="line">    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</span><br><span class="line">                                                hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</span><br><span class="line">                                                n_classes=<span class="number">3</span>,</span><br><span class="line">                                                model_dir=<span class="string">"/tmp/iris_model"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define the training inputs</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_train_inputs</span><span class="params">()</span>:</span></span><br><span class="line">        x = tf.constant(training_set.data)</span><br><span class="line">        y = tf.constant(training_set.target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fit model.</span></span><br><span class="line">    classifier.fit(input_fn=get_train_inputs, steps=<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define the test inputs</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_test_inputs</span><span class="params">()</span>:</span></span><br><span class="line">        x = tf.constant(test_set.data)</span><br><span class="line">        y = tf.constant(test_set.target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line">    <span class="string">'''Evaluate accuracy'''</span> </span><br><span class="line">    <span class="comment"># &#123;'loss': 0.098150678, 'accuracy': 0.96666664, 'global_step': 4000&#125;</span></span><br><span class="line">    accuracy_score = classifier.evaluate(input_fn=get_test_inputs,</span><br><span class="line">                                         steps=<span class="number">1</span>)[<span class="string">"accuracy"</span>]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\nTest Accuracy: &#123;0:f&#125;\n"</span>.format(accuracy_score))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Classify two new flower samples.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">new_samples</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.array(</span><br><span class="line">            [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>],</span><br><span class="line">             [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    predictions = list(classifier.predict(input_fn=new_samples))</span><br><span class="line"></span><br><span class="line">    print(</span><br><span class="line">        <span class="string">"New Samples, Class Predictions:    &#123;&#125;\n"</span></span><br><span class="line">            .format(predictions))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="2-将Iris-CSV数据加载到TF中"><a href="#2-将Iris-CSV数据加载到TF中" class="headerlink" title="2. 将Iris CSV数据加载到TF中"></a>2. 将Iris CSV数据加载到TF中</h2><p>该<a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" target="_blank" rel="external">Iris data set</a>包含150行数据，包括来自每三个相关Iris种类的50个样品： ris setosa, Iris virginica, 以及 Iris versicolor。</p>
<p><img src="/images/tensorflow/tf-5.1-Iris-data_1.jpg" width="700"></p>
<blockquote>
<p>从左到右， Iris setosa（ Radomil，CC BY-SA 3.0）， Iris versicolor（Dlanglois，CC BY-SA 3.0）和Iris virginica（Frank Mayfield，CC BY-SA 2.0））。</p>
<p>每行包含每个花样品的以下数据： 花萼长度，花萼宽度， 花瓣长度，花瓣宽度和花种。花种以整数表示，0表示Iris setosa，1表示Iris versicolor，2表示Iris virginica。</p>
</blockquote>
<p><img src="/images/tensorflow/tf-5.1-Iris-data_2.jpg" width="400"></p>
<blockquote>
<p>Iris数据已被随机分为两个独立的CSV：</p>
<ul>
<li>含120个样本的训练集（iris_training.csv）</li>
<li>含30个样本的测试集（iris_test.csv）。</li>
</ul>
</blockquote>
<p>接下来，使用learn.datasets.base中的<a href="https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/learn/python/learn/datasets/base.py" target="_blank" rel="external">load_csv_with_header()</a> 方法将训练集和测试集加载到datasets中。该load_csv_with_header()方法需要三个必不可少的参数：</p>
<ul>
<li>filename，带有文件路径的CSV文件。</li>
<li>target_dtype，数据集的形式为numpy 数据类型。</li>
<li>features_dtype，数据特征集的形式为numpy 数据类型。</li>
</ul>
<p>在这里，目标（你正在训练预测模型的值）是花种，它是0-2的整数，所以适当的numpy数据类型是np.int：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load datasets.</span></span><br><span class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename=IRIS_TRAINING,</span><br><span class="line">    target_dtype=np.int,</span><br><span class="line">    features_dtype=np.float32)</span><br><span class="line">    </span><br><span class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename=IRIS_TEST,</span><br><span class="line">    target_dtype=np.int,</span><br><span class="line">    features_dtype=np.float32)</span><br></pre></td></tr></table></figure>
<p>test_set 数据形式</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">Dataset(</span><br><span class="line">       data=array([</span><br><span class="line">          [ <span class="number">5.9000001</span> ,  <span class="number">3.</span> ,  <span class="number">4.19999981</span>,  <span class="number">1.5</span>],</span><br><span class="line">          [ <span class="number">6.9000001</span> ,  <span class="number">3.0999999</span> ,  <span class="number">5.4000001</span> ,  <span class="number">2.0999999</span> ],</span><br><span class="line">          ......</span><br><span class="line">          [ <span class="number">6.69999981</span>,  <span class="number">3.29999995</span>,  <span class="number">5.69999981</span>,  <span class="number">2.5</span>       ],</span><br><span class="line">          [ <span class="number">6.4000001</span> ,  <span class="number">2.9000001</span> ,  <span class="number">4.30000019</span>,  <span class="number">1.29999995</span>]</span><br><span class="line">       ], dtype=float32), </span><br><span class="line">   		</span><br><span class="line">       target=array(</span><br><span class="line">          [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>,<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>test_set.data 数据形式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[ 5.9000001   3.          4.19999981  1.5       ]</span><br><span class="line"> [ 6.9000001   3.0999999   5.4000001   2.0999999 ]</span><br><span class="line"> ......</span><br><span class="line"> [ 6.4000001   2.9000001   4.30000019  1.29999995]]</span><br></pre></td></tr></table></figure>
<p>test_set.target 数据形式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 2 0 1 1 1 0 2 1 2 2 0 2 1 1 0 1 0 0 2 0 1 2 1 1 1 0 1 2 1]</span><br></pre></td></tr></table></figure>
<p>tf.contrib.learn中的datasets被命名为 tuples ; 您可以通过data和target 属性访问特征数据和目标值。这里的training_set.data，training_set.target包含训练集的特征数据和目标值，test_set.data ，test_set.target包含测试集的特征数据和目标值。</p>
<p>在 “在Iris训练数据中拟合DNNC分类器”， 您将使用training_set.data和 training_set.target训练您的模型，在 “评估模型精度” 时，您将使用test_set.data和 test_set.target。</p>
<h2 id="3-构建深层神经网络分类器"><a href="#3-构建深层神经网络分类器" class="headerlink" title="3. 构建深层神经网络分类器"></a>3. 构建深层神经网络分类器</h2><p>tf.contrib.learn提供了各种预定义的模型，称为 <a href="https://www.tensorflow.org/api_guides/python/contrib.learn#estimators" target="_blank" rel="external">Estimators</a>，您可以使用“out of the box”方式对数据进行训练和评估操作。</p>
<p>在这里，您将配置深层神经网络分类器模型以拟合Iris数据。利用tf.contrib.learn，您可以使用几行代码实例化<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier" target="_blank" rel="external">tf.contrib.learn.DNNClassifier</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Specify that all features have real-value data</span></span><br><span class="line">feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></span><br><span class="line">classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</span><br><span class="line">                                            hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</span><br><span class="line">                                            n_classes=<span class="number">3</span>,</span><br><span class="line">                                            model_dir=<span class="string">"/tmp/iris_model"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-1-feature-columns-形式"><a href="#3-1-feature-columns-形式" class="headerlink" title="3.1 feature_columns 形式"></a>3.1 feature_columns 形式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[_RealValuedColumn(column_name=<span class="string">''</span>, dimension=4, default_value=None, dtype=tf.float32, normalizer=None)]</span><br></pre></td></tr></table></figure>
<p>上面的代码首先定义了模型的特征列，它们指定数据集中的特征数据类型。所有的特征数据是连续的，所以tf.contrib.layers.real_valued_column使用相应的函数来构造特征列。数据集中有四个特征（花萼宽度，花萼高度，花瓣宽度和花瓣高度），因此dimension 必须设置为4保存所有数据。</p>
<h3 id="3-2-classifier-形式"><a href="#3-2-classifier-形式" class="headerlink" title="3.2 classifier 形式"></a>3.2 classifier 形式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">DNNClassifier(params=&#123;</span><br><span class="line"><span class="string">'head'</span>: &lt;tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x1089c0c18&gt;, </span><br><span class="line"><span class="string">'hidden_units'</span>: [10, 20, 10], </span><br><span class="line"><span class="string">'feature_columns'</span>: (_RealValuedColumn(column_name=<span class="string">''</span>, dimension=4, default_value=None, dtype=tf.float32, normalizer=None),), </span><br><span class="line"><span class="string">'optimizer'</span>: None, </span><br><span class="line"><span class="string">'activation_fn'</span>: &lt;<span class="keyword">function</span> relu at 0x11590ad08&gt;, </span><br><span class="line"><span class="string">'dropout'</span>: None, </span><br><span class="line"><span class="string">'gradient_clip_norm'</span>: None, </span><br><span class="line"><span class="string">'embedding_lr_multipliers'</span>: None, </span><br><span class="line"><span class="string">'input_layer_min_slice_size'</span>: None&#125;)</span><br></pre></td></tr></table></figure>
<p>然后，代码使用以下参数创建一个DNNClassifier模型：</p>
<ul>
<li>feature_columns=feature_columns。上面定义的特征列集合。</li>
<li>hidden_units=[10, 20, 10]。三个隐含层，分别含有10,20和10个神经元。</li>
<li>n_classes=3。三个目标类别，代表三种Iris物种。</li>
<li>model_dir=/tmp/iris_model。TensorFlow将在模型训练期间保存检查点数据的目录。有关使用TensorFlow进行日志记录和监视的更多信息，请参阅Logging and Monitoring Basics with tf.contrib.learn.。</li>
</ul>
<h2 id="4-训练的输入流"><a href="#4-训练的输入流" class="headerlink" title="4. 训练的输入流"></a>4. 训练的输入流</h2><p>tf.contrib.learnAPI使用输入函数，创建为模型生成数据的TensorFlow操作。本例中，数据足够小，可以TensorFlow constants 存储。以下代码生成最简单的输入：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define the test inputs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_inputs</span><span class="params">()</span>:</span></span><br><span class="line">  x = tf.constant(training_set.data)</span><br><span class="line">  y = tf.constant(training_set.target)</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure>
<h2 id="5-在Iris训练数据上拟合DNN分类器"><a href="#5-在Iris训练数据上拟合DNN分类器" class="headerlink" title="5. 在Iris训练数据上拟合DNN分类器"></a>5. 在Iris训练数据上拟合DNN分类器</h2><p>现在您已经配置了DNN classifier模型，您可以使用该<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/BaseEstimator#fit" target="_blank" rel="external">fit</a>方法将其拟合Iris训练数据。将get_train_inputs传递给input_fn，指定训练的步骤（这里取2000）：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Fit model.</span></span><br><span class="line">classifier.fit(input_fn=get_train_inputs, steps=<span class="number">2000</span>)</span><br></pre></td></tr></table></figure>
<p>模型的状态保留在classifier，这意味着如果你喜欢，你可以分布训练。例如，以上代码相当于：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">classifier.fit(x=training_set.data, y=training_set.target, steps=<span class="number">1000</span>)</span><br><span class="line">classifier.fit(x=training_set.data, y=training_set.target, steps=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>但是，如果您希望在训练时跟踪模型，则可能需要使用TensorFlow <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/monitors" target="_blank" rel="external">monitor</a> 来执行日志记录操作。有关此主题的更多信息，请参阅<br>“Logging and Monitoring Basics with tf.contrib.learn”教程 。</p>
<h2 id="6-评估模型精度"><a href="#6-评估模型精度" class="headerlink" title="6. 评估模型精度"></a>6. 评估模型精度</h2><p>您已经在Iris训练数据上拟合DNNClassifier模型; 现在，您可以使用该<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/BaseEstimator#evaluate" target="_blank" rel="external">evaluate</a>方法检查其对Iris测试数据的准确性 。正如fit， evaluate需要一个构建其输入渠道的输入函数。evaluate 返回一个评估结果dict。下面的代码通过Iris测试数据- test_set.data和test_set.target进行evaluate并打印结果的精度：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define the test inputs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_inputs</span><span class="params">()</span>:</span></span><br><span class="line">  x = tf.constant(test_set.data)</span><br><span class="line">  y = tf.constant(test_set.target)</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">return</span> x, y</span><br><span class="line"> </span><br><span class="line"><span class="string">'''Evaluate accuracy.'''</span> </span><br><span class="line"><span class="comment"># &#123;'loss': 0.098150678, 'accuracy': 0.96666664, 'global_step': 4000&#125;</span></span><br><span class="line">accuracy_score = classifier.evaluate(input_fn=get_test_inputs,</span><br><span class="line">                                     steps=<span class="number">1</span>)[<span class="string">"accuracy"</span>]</span><br><span class="line"> </span><br><span class="line">print(<span class="string">"\nTest Accuracy: &#123;0:f&#125;\n"</span>.format(accuracy_score))</span><br></pre></td></tr></table></figure>
<p>注意：这里的steps参数对evaluate很重要。 evaluate直到它到达输入的末尾才停止运行。</p>
<h2 id="7-分类新样本"><a href="#7-分类新样本" class="headerlink" title="7. 分类新样本"></a>7. 分类新样本</h2><p>使用estimator的predict()方法对新样本进行分类。例如，说你有这两个新的花朵样例：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Classify two new flower samples.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_samples</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">return</span> np.array(</span><br><span class="line">    [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>],</span><br><span class="line">     [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=np.float32)</span><br><span class="line"> </span><br><span class="line">predictions = list(classifier.predict(input_fn=new_samples))</span><br><span class="line"> </span><br><span class="line">print(</span><br><span class="line">    <span class="string">"New Samples, Class Predictions:    &#123;&#125;\n"</span></span><br><span class="line">    .format(predictions))</span><br></pre></td></tr></table></figure>
<p>您可以使用该predict()方法预测其物种。predict返回一个生成器，可以很容易地转换成一个列表。以下代码取得并打印分类的预测结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">New Samples, Class Predictions:    [1 2]</span><br></pre></td></tr></table></figure>
<h2 id="8-其他资源"><a href="#8-其他资源" class="headerlink" title="8. 其他资源"></a>8. 其他资源</h2><ul>
<li>其他资源有关tf.contrib.learn的更多参考资料，请参阅官方 <a href="https://www.tensorflow.org/api_guides/python/contrib.learn" target="_blank" rel="external">API文档</a>。</li>
<li>有关使用tf.contrib.learn创建线性模型的更多信息，请参阅 <a href="https://www.tensorflow.org/tutorials/linear" target="_blank" rel="external">Large-scale Linear Models with TensorFlow.</a></li>
<li>要使用tf.contrib.learn API构建自己的Estimator，请查看在 <a href="https://www.tensorflow.org/extend/estimators" target="_blank" rel="external">tf.contrib.learn中创建估计器</a>。</li>
<li>要在浏览器中实验神经网络建模和可视化，请查看<a href="http://playground.tensorflow.org/" target="_blank" rel="external">Deep Playground</a>。</li>
<li>有关神经网络的更多高级教程，请参阅 <a href="https://www.tensorflow.org/tutorials/images/deep_cnn" target="_blank" rel="external">卷积神经网络</a>和<a href="https://www.tensorflow.org/tutorials/sequences/recurrent" target="_blank" rel="external">循环神经网络</a>。</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://mli.github.io/gpu/2016/01/17/build-gpu-clusters/" target="_blank" rel="external">GPU集群折腾手记——2015</a></li>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029485" target="_blank" rel="external">TensorFlow R1.2 中文文档</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow 踩坑记之 tf.metrics]]></title>
      <url>http://sggo.me/2018/10/23/tensorflow/tf-metrics_learn/</url>
      <content type="html"><![CDATA[<p>总结一下 tf.metrics 遇到的一些坑。</p>
<a id="more"></a>
<p><strong>精确率的计算公式</strong></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/qq_37747262/article/details/82223155" target="_blank" rel="external">Tensorflow踩坑记之tf.metrics</a></li>
<li><a href="https://github.com/blair101/tensorflow_metrics_learn/blob/master/tensorflow_metrics_learn.ipynb" target="_blank" rel="external">TensorFlow Github tf.metrics 实践</a></li>
<li><a href="http://ronny.rest/blog/post_2017_09_11_tf_metrics/" target="_blank" rel="external">Ronny Restrepo - tf.metrics.accuracy()讲解滴很清楚</a></li>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029489" target="_blank" rel="external">ApacheCN 开源组织 tf.contrib.learn基础的记录和监控教程</a></li>
<li><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029377" target="_blank" rel="external">ApacheCN 开源组织 TensorFlow入门</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow tf.app.run()与命令行参数解析]]></title>
      <url>http://sggo.me/2018/10/23/tensorflow/tf-app_run_&_tf_flags/</url>
      <content type="html"><![CDATA[<p>tf.app.run() 与 命令行参数解析 tf.flags</p>
<a id="more"></a>
<p>首先给出一段常见的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
<p>找到 Tensorflow 中关于上述 函数<code>run()</code> 的源码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(main=None, argv=None)</span>:</span></span><br><span class="line">  <span class="string">"""Runs the program with an optional 'main' function and 'argv' list."""</span></span><br><span class="line">  f = flags.FLAGS</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Extract the args from the optional `argv` list.</span></span><br><span class="line">  args = argv[<span class="number">1</span>:] <span class="keyword">if</span> argv <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Parse the known flags from that list, or from the command</span></span><br><span class="line">  <span class="comment"># line otherwise.</span></span><br><span class="line">  <span class="comment"># pylint: disable=protected-access</span></span><br><span class="line">  flags_passthrough = f._parse_flags(args=args)</span><br><span class="line">  <span class="comment"># pylint: enable=protected-access</span></span><br><span class="line"></span><br><span class="line">  main = main <span class="keyword">or</span> _sys.modules[<span class="string">'__main__'</span>].main</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Call the main function, passing through any arguments</span></span><br><span class="line">  <span class="comment"># to the final program.</span></span><br><span class="line">  _sys.exit(main(_sys.argv[:<span class="number">1</span>] + flags_passthrough))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_allowed_symbols = [</span><br><span class="line">    <span class="string">'run'</span>,</span><br><span class="line">    <span class="comment"># Allowed submodule.</span></span><br><span class="line">    <span class="string">'flags'</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">remove_undocumented(__name__, _allowed_symbols)</span><br></pre></td></tr></table></figure>
<p>可以看到源码中的过程是首先加载 <code>flags</code> 的参数项，然后执行 <code>main</code> 函数。参数是使用<code>tf.app.flags.FLAGS</code> 定义的。</p>
<h2 id="tf-app-flags-FLAGS"><a href="#tf-app-flags-FLAGS" class="headerlink" title="tf.app.flags.FLAGS"></a>tf.app.flags.FLAGS</h2><p>关于 <code>tf.app.flags.FLAGS</code> 的使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fila_name: temp.py</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'string'</span>, <span class="string">'train'</span>, <span class="string">'This is a string'</span>)</span><br><span class="line">tf.app.flags.DEFINE_float(<span class="string">'learning_rate'</span>, <span class="number">0.001</span>, <span class="string">'This is the rate in training'</span>)</span><br><span class="line">tf.app.flags.DEFINE_boolean(<span class="string">'flag'</span>, <span class="keyword">True</span>, <span class="string">'This is a flag'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'string: '</span>, FLAGS.string)</span><br><span class="line">print(<span class="string">'learning_rate: '</span>, FLAGS.learning_rate)</span><br><span class="line">print(<span class="string">'flag: '</span>, FLAGS.flag)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(<span class="string">'string: '</span>, <span class="string">'train'</span>)</span><br><span class="line">(<span class="string">'learning_rate: '</span>, 0.001)</span><br><span class="line">(<span class="string">'flag: '</span>, True)</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/TwT520Ly/article/details/79759448" target="_blank" rel="external">tf.app.run()与命令行参数解析</a></li>
<li><a href="https://blog.csdn.net/spring_willow/article/details/80111993" target="_blank" rel="external">TensorFlow中的小知识：tf.flags.DEFINE_xxx()</a></li>
<li><a href="https://www.jianshu.com/p/7ccfe8cf4aa1" target="_blank" rel="external">Tensorflow 1.0：老司机立下的Flag</a></li>
<li><a href="https://blog.csdn.net/yanqianglifei/article/details/83020992" target="_blank" rel="external">Tensorflow教程(十四) 命令行参数tf.flags的使用</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[从 Encoder 到 Decoder 实现 Seq2Seq 模型]]></title>
      <url>http://sggo.me/2018/10/15/tensorflow/tf-seq2seq/</url>
      <content type="html"><![CDATA[<p>简单的 Seq2Seq 实现，我们将使用 TensorFlow 来实现个基础版的 Seq2Seq，主要帮助理解 Seq2Seq 中的基础架构。</p>
<a id="more"></a>
<p><img src="/images/tensorflow/tf-nlp-seq2seq.jpg" width="800"></p>
<p>自己做了一个示意图，希望帮助初学者更好地理解. </p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://coolshell.cn/articles/17583.html" target="_blank" rel="external">技术人员的发展之路</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27608348" target="_blank" rel="external">从 Encoder 到 Decoder 实现 Seq2Seq 模型</a></li>
<li><a href="https://github.com/NELSONZHAO/zhihu/tree/master/basic_seq2seq?1521452873816" target="_blank" rel="external">zhihu/basic_seq2seq/</a></li>
<li><a href="https://www.zhihu.com/question/41949741" target="_blank" rel="external">隔壁小王 LSTM 神经网络输入输出究竟是怎样的？</a></li>
<li><a href="https://colab.research.google.com" target="_blank" rel="external">colab.research.google</a></li>
<li><a href="https://zh.gluon.ai/" target="_blank" rel="external">zh.gluon.ai 动手学深度学习</a></li>
<li><a href="http://discuss.gluon.ai/" target="_blank" rel="external">discuss.gluon.ai 论坛</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[简单前馈网络实现 mnist 分类]]></title>
      <url>http://sggo.me/2018/10/04/tensorflow/tf-mnist-1-beginners/</url>
      <content type="html"><![CDATA[<p>我们来实现一个非常简单的两层 FC 全连接网络来完成 MNIST数据 的分类</p>
<a id="more"></a> 
<p>输入 [-1,28*28]， FC1 有 1024 个neurons， FC2 有 10 个neurons。</p>
<blockquote>
<p>这么简单的一个全连接网络，结果测试准确率达到了 0.98。还是非常棒的！！！</p>
<p>MNIST 数据集 包含了 60000 张图片来作为训练数据，10000 张图片作为测试数据。每张图片都代表了 0~9 中的一个数字。图片大小都为 28*28，处理后的每张图片是一个长度为 784 的一维数组，这个数组中的元素对应图片像素矩阵提供给神经网络的输入层，像素矩阵中元素的取值范围 [0, 1]， 它代表了颜色的深浅。其中 0 表示白色背景(background)，1 表示黑色前景(foreground)。</p>
<p>为了方便使用随机梯度下降， input_data.read_data_sets 函数生成的类还提供了 mnist.train.next.batch 函数，它可以从所有训练数据中读取一小部分作为一个训练 batch。</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">MNIST 数据下载地址和内容</th>
<th style="text-align:center">内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Extracting MNIST_data/train-images-idx3-ubyte.gz</td>
<td style="text-align:center">训练数据图片</td>
</tr>
<tr>
<td style="text-align:center">Extracting MNIST_data/train-labels-idx1-ubyte.gz</td>
<td style="text-align:center">训练数据答案</td>
</tr>
<tr>
<td style="text-align:center">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</td>
<td style="text-align:center">测试数据图片</td>
</tr>
<tr>
<td style="text-align:center">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</td>
<td style="text-align:center">测试数据答案</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置按需使用 GPU</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">sess = tf.InteractiveSession(config=config)</span><br></pre></td></tr></table></figure>
<h2 id="1-导入数据"><a href="#1-导入数据" class="headerlink" title="1. 导入数据"></a>1. 导入数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用tensorflow 导入数据</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># input_data.read_data_sets 自动将 MNIST 数据集划分为 train、validation、test 三个数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train 集合有 55000 张图片</span></span><br><span class="line"><span class="comment"># validation 集合有 5000 张图片</span></span><br><span class="line"><span class="comment"># test 集合有 10000 张图片，图片来自 MNIST 提供的测试数据集</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'training data shape '</span>, mnist.train.images.shape)</span><br><span class="line">print(<span class="string">'training label shape '</span>, mnist.train.labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training data shape  (55000, 784)</span></span><br><span class="line"><span class="comment"># training label shape  (55000, 10)</span></span><br></pre></td></tr></table></figure>
<h2 id="2-构建网络"><a href="#2-构建网络" class="headerlink" title="2. 构建网络"></a>2. 构建网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 权值初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="comment"># 用正态分布来初始化权值</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="comment"># 本例中用relu激活函数，所以用一个很小的正偏置较好</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># input_layer</span></span><br><span class="line">X_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># FC1</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">784</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(X_, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FC2</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">y_pre = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)</span><br></pre></td></tr></table></figure>
<h2 id="3-训练和评估"><a href="#3-训练和评估" class="headerlink" title="3. 训练和评估"></a>3. 训练和评估</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.损失函数：cross_entropy</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_ * tf.log(y_pre))</span><br><span class="line"><span class="comment"># 2.优化函数：AdamOptimizer, 优化速度要比 GradientOptimizer 快很多</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.预测结果评估</span></span><br><span class="line"><span class="comment">#　预测值中最大值（１）即分类结果，是否等于原始标签中的（１）的位置。argmax()取最大值所在的下标</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_pre, <span class="number">1</span>), tf.arg_max(y_, <span class="number">1</span>))  </span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始运行</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="comment"># 这大概迭代了不到 10 个 epoch， 训练准确率已经达到了0.98</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">    X_batch, y_batch = mnist.train.next_batch(batch_size=<span class="number">100</span>)</span><br><span class="line">    train_step.run(feed_dict=&#123;X_: X_batch, y_: y_batch&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">        train_accuracy = accuracy.eval(feed_dict=&#123;X_: mnist.train.images, y_: mnist.train.labels&#125;)</span><br><span class="line">        print(<span class="string">"step %d, training acc %g"</span> % (i+<span class="number">1</span>, train_accuracy))</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        test_accuracy = accuracy.eval(feed_dict=&#123;X_: mnist.test.images, y_: mnist.test.labels&#125;)</span><br><span class="line">        print(<span class="string">"= "</span> * <span class="number">10</span>, <span class="string">"step %d, testing acc %g"</span> % (i+<span class="number">1</span>, test_accuracy))</span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">step 200, training acc 0.937364</span><br><span class="line">step 400, training acc 0.965818</span><br><span class="line">step 600, training acc 0.973364</span><br><span class="line">step 800, training acc 0.977709</span><br><span class="line">step 1000, training acc 0.981528</span><br><span class="line">= = = = = = = = = =  step 1000, testing acc 0.9688</span><br><span class="line">step 1200, training acc 0.988437</span><br><span class="line">step 1400, training acc 0.988728</span><br><span class="line">step 1600, training acc 0.987491</span><br><span class="line">step 1800, training acc 0.993873</span><br><span class="line">step 2000, training acc 0.992527</span><br><span class="line">= = = = = = = = = =  step 2000, testing acc 0.9789</span><br><span class="line">step 2200, training acc 0.995309</span><br><span class="line">step 2400, training acc 0.995455</span><br><span class="line">step 2600, training acc 0.9952</span><br><span class="line">step 2800, training acc 0.996073</span><br><span class="line">step 3000, training acc 0.9964</span><br><span class="line">= = = = = = = = = =  step 3000, testing acc 0.9778</span><br><span class="line">step 3200, training acc 0.996709</span><br><span class="line">step 3400, training acc 0.998109</span><br><span class="line">step 3600, training acc 0.997455</span><br><span class="line">step 3800, training acc 0.995055</span><br><span class="line">step 4000, training acc 0.997291</span><br><span class="line">= = = = = = = = = =  step 4000, testing acc 0.9808</span><br><span class="line">step 4200, training acc 0.997746</span><br><span class="line">step 4400, training acc 0.996073</span><br><span class="line">step 4600, training acc 0.998564</span><br><span class="line">step 4800, training acc 0.997946</span><br><span class="line">step 5000, training acc 0.998673</span><br><span class="line">= = = = = = = = = =  step 5000, testing acc 0.98</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/jerr__y/article/category/6747409" target="_blank" rel="external">大学之道，在明明德 永永夜 Tensorflow学习之路</a></li>
<li><a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-c1ov28so.html" target="_blank" rel="external">W3cschool MNIST数据集 來龍去脈講解的清清楚楚</a></li>
<li><a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="external">Visual-Information 交叉熵</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[RNN 的语言模型 TensorFlow 实现]]></title>
      <url>http://sggo.me/2018/10/02/tensorflow/tf-nlp-9.2.3/</url>
      <content type="html"><![CDATA[<p>上篇 <code>PTB</code> 数据集 batching 中我们介绍了如何对 <code>PTB</code> 数据集进行 连接、切割 成多个 batch，作为 NNLM 的输入。</p>
<p>本文将介绍如何采用 TensorFlow 实现 RNN-based NNLM。 </p>
<a id="more"></a>
<p><img src="/images/tensorflow/tf-nlp-9.2.3_1.jpg" width="700"></p>
<h2 id="1-Embedding-层"><a href="#1-Embedding-层" class="headerlink" title="1. Embedding 层"></a>1. Embedding 层</h2><p>将 word 编号 转化为 word embedding 两大作用 :</p>
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th style="text-align:center">function</th>
<th style="text-align:center">desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td style="text-align:center">降低输入的维度</td>
<td style="text-align:center">词向量的维度通常在 200 ~ 1000 之间, 大大减少 RNN 网络的参数数量 与 计算量</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td style="text-align:center">增加语义信息</td>
<td style="text-align:center">简单的单词编号是不包含任何语义信息的.</td>
</tr>
</tbody>
</table>
<blockquote>
<p>词向量维度: EMB_SIZE，词汇表大小: VOCAB_SIZE</p>
<p>所有单词的词向量可以放入一个大小为 <strong>(EMB_SIZE, VOCAB_SIZE)</strong> 的矩阵内</p>
<p>在读取词向量时，可以调用 <strong>tf.nn.embedding_lookup</strong> 方法。</p>
<p>用 tf.Variable 来表示词向量，这样就可以采用任意初始化的词向量，学习过程中也会优化词向量。</p>
</blockquote>
<p><img src="/images/tensorflow/tf-nlp-9.2.3_2.jpg" width="700"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义单词的词向量矩阵</span></span><br><span class="line">embedding = tf.get_variable(<span class="string">"embedding"</span>, [VOCAB_SIZE, EMB_SIZE])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为词向量表示</span></span><br><span class="line">inputs = tf.nn.embedding_lookup(embedding, input_data)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>其中输入数据 input_data 的维度是 (batch_size * num_steps)</p>
<p>而输出的 input_embedding 的维度成为 (batch_size * num_steps * EMB_SIZE). </p>
<p>在本文中，我们输入数据维度是 ($20 \times 35$) ，EMB_SIZE = 300, 输入词向量维度时 ($20 \times 35 \times 300$) .</p>
</blockquote>
<h2 id="2-Softmax-层"><a href="#2-Softmax-层" class="headerlink" title="2. Softmax 层"></a>2. Softmax 层</h2><p>Softmax层 的作用是将 RNN 的输出 转化为一个单词表中每个单词的输出概率，为此需要两个步骤：</p>
<h3 id="2-1-第一步"><a href="#2-1-第一步" class="headerlink" title="2.1 第一步"></a>2.1 第一步</h3><p>使用一个线性映射将 RNN 的输出映射为一个维度与词汇表大小相同的向量，这一步的输出叫做 <strong>logits</strong>. 代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先定义映射用到的参数</span></span><br><span class="line"><span class="comment"># HIDDEN_SIZE 是 RNN 的隐藏状态维度，VOCAB_SIZE 是词汇表大小</span></span><br><span class="line">weight = tf.get_variable(<span class="string">"weight"</span>, [HIDDEN_ZIZE, VOCAB_SIZE])</span><br><span class="line">bias = tf.get_variable(<span class="string">"bias"</span>, [VOCAB_SIZE])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算线性映射</span></span><br><span class="line">logits = tf.matmul(output, weight) + bias</span><br></pre></td></tr></table></figure>
<blockquote>
<p>其中 output 是 RNN 的输出，维度是 [batch_size * num_steps, <strong>HIDDEN_SIZE</strong>]</p>
<p>经过线性映射后，输出结果是 [batch_size * num_steps, <strong>VOCAB_SIZE</strong>].</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://www.zhihu.com/question/52250059/answer/146260654" target="_blank" rel="external">tf.nn.embedding_lookup函数原理？</a></li>
<li><a href="https://www.zhihu.com/question/48107602/answer/159801895" target="_blank" rel="external">求通俗讲解下tensorflow的embedding_lookup接口的意思？</a></li>
<li><a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/" target="_blank" rel="external">Tomas Mikolov PTB 数据</a></li>
<li><a href="https://www.zhihu.com/question/60751553" target="_blank" rel="external">如何理解深度学习源码里经常出现的logits？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37886740" target="_blank" rel="external">基于循环神经网络的语言模型的介绍与TensorFlow实现(4)：TensorFlow实现RNN-based语言模型</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PTB 数据的 batching 方法]]></title>
      <url>http://sggo.me/2018/10/01/tensorflow/tf-nlp-9.2.2/</url>
      <content type="html"><![CDATA[<p><code>PTB</code> 数据集 batching 介绍, 如何对 <code>PTB</code> 数据集进行 连接、切割 成多个 batch。</p>
<p>重点了解 <strong>batch_size</strong>、<strong>num_batch</strong>、<strong>num_step</strong> 这三个概念。</p>
<a id="more"></a>
<h2 id="batching"><a href="#batching" class="headerlink" title="batching"></a>batching</h2><p>先看下面这段，摘取自 <a href="https://www.zhihu.com/question/278485204/answer/402066718" target="_blank" rel="external">知乎 作者：dalida</a></p>
<p>画图很弱，直接就徒手画了。以朱自清的《背影》中的节选段落为例。字有点难看，请忽略～(￣▽￣～)~</p>
<p>这是单个字符级别的 RNN，换成词语也一样。</p>
<p>num_batch 其实就是 batch 的数量，这里只画了三个，可以一直接下去….这样一次完成 6个字符 (num_step=6) 的处理，而且能保证 batch 之间的连续性。但是行与行之间的连续性确实是丢失了。</p>
<p><img src="/images/tensorflow/tf-nlp-9.2.2_2.jpg" width="700"></p>
<p><strong>结论 :</strong></p>
<ul>
<li>batch_size = 3 </li>
<li>num_batch = 3</li>
<li>num_step = 6</li>
</ul>
<blockquote>
<p>batch_size 也是一次输入的句子数</p>
</blockquote>
<h2 id="朱自清《背影》"><a href="#朱自清《背影》" class="headerlink" title="朱自清《背影》"></a>朱自清《背影》</h2><p>我们过了江，进了车站。我买票，他忙着照看行李。行李太多了，得向脚夫行些小费，才可过去。他便又忙着和他们讲价钱。我那时真是聪明过分，总觉他说话不大漂亮，非自己插嘴不可。但他终于讲定了价钱；就送我上车。他给我拣定了靠车门的一张椅子；我将他给我做的紫毛大衣铺好坐位。他嘱我路上小心，夜里警醒些，不要受凉。又嘱托茶房好好照应我。我心里暗笑他的迂；他们只认得钱，托他们直是白托！而且我这样大年纪的人，难道还不能料理自己么？唉，我现在想想，那时真是太聪明了！</p>
<p>我说道，“爸爸，你走吧。”他望车外看了看，说，“我买几个橘子去。你就在此地，不要走动。”我看那边月台的栅栏外有几个卖东西的等着顾客。走到那边月台，须穿过铁道，须跳下去又爬上去。父亲是一个胖子，走过去自然要费事些。我本来要去的，他不肯，只好让他去。我看见他戴着黑布小帽，穿着黑布大马褂，深青布棉袍，蹒跚地走到铁道边，慢慢探身下去，尚不大难。可是他穿过铁道，要爬上那边月台，就不容易了。他用两手攀着上面，两脚再向上缩；他肥胖的身子向左微倾，显出努力的样子。这时我看见他的背影，我的泪很快地流下来了。我赶紧拭干了泪，怕他看见，也怕别人看见。我再向外看时，他已抱了朱红的橘子望回走了。过铁道时，他先将橘子散放在地上，自己慢慢爬下，再抱起橘子走。到这边时，我赶紧去搀他。他和我走到车上，将橘子一股脑儿放在我的皮大衣上。于是扑扑衣上的泥土，心里很轻松似的，过一会说，“我走了；到那边来信！”我望着他走出去。他走了几步，回过头看见我，说，“进去吧，里边没人。”等他的背影混入来来往往的人里，再找不着了，我便进来坐下，我的眼泪又来了。</p>
<p>近几年来，父亲和我都是东奔西走，家中光景是一日不如一日。他少年出外谋生，独力支持，做了许多大事。那知老境却如此颓唐！他触目伤怀，自然情不能自已。情郁于中，自然要发之于外；家庭琐屑便往往触他之怒。他待我渐渐不同往日。但最近两年的不见，他终于忘却我的不好，只是惦记着我，惦记着我的儿子。我北来后，他写了一信给我，信中说道，“我身体平安，惟膀子疼痛利害，举箸提笔，诸多不便，大约大去之期不远矣。”我读到此处，在晶莹的泪光中，又看见那肥胖的，青布棉袍，黑布马褂的背影。唉！我不知何时再能与他相见！</p>
<p><strong>再看这个图</strong>：</p>
<p><img src="/images/tensorflow/tf-nlp-9.2.2_1.jpg" width="700"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.zhihu.com/question/278485204/answer/402066718" target="_blank" rel="external">关于batching多句子切割batch疑问？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/40809517" target="_blank" rel="external">[L2]使用LSTM实现语言模型-数据batching</a></li>
<li><a href="https://blog.csdn.net/jerr__y/article/details/61195257" target="_blank" rel="external">TensorFlow入门（五）多层 LSTM 通俗易懂版</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[numpy.random.normal 函数]]></title>
      <url>http://sggo.me/2018/09/11/python/py-numpy-9-numpy.random.normal/</url>
      <content type="html"><![CDATA[<p>numpy.random.normal 函数，有三个参数（<strong>loc, scale, size</strong>），代表生成的高斯分布随机数的均值、方差以及输出的 size. </p>
<a id="more"></a>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, (<span class="number">7</span>,<span class="number">1</span>)).astype(np.float32)</span><br></pre></td></tr></table></figure>
<p>Output :</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[-0.05229944],</span><br><span class="line">       [ 0.01754326],</span><br><span class="line">       [ 0.01764081],</span><br><span class="line">       [-0.03058357],</span><br><span class="line">       [-0.05406121],</span><br><span class="line">       [-0.07284269],</span><br><span class="line">       [ 0.00289147]], dtype=float32)</span><br></pre></td></tr></table></figure>
<p><strong>Scipy Help</strong>: <strong>numpy.random.normal</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Parameters:	</span><br><span class="line">loc : float <span class="keyword">or</span> array_like of floats</span><br><span class="line">Mean (“centre”) of the distribution.</span><br><span class="line"></span><br><span class="line">scale : float <span class="keyword">or</span> array_like of floats</span><br><span class="line">Standard deviation (spread <span class="keyword">or</span> “width”) of the distribution.</span><br><span class="line"></span><br><span class="line">size : int <span class="keyword">or</span> tuple of ints, optional</span><br><span class="line">Output shape. If the given shape <span class="keyword">is</span>, e.g., (m, n, k), then m * n * k samples are drawn. If size <span class="keyword">is</span> <span class="keyword">None</span> (default), a single value <span class="keyword">is</span> returned <span class="keyword">if</span> loc <span class="keyword">and</span> scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.</span><br><span class="line"></span><br><span class="line">Returns:	</span><br><span class="line">out : ndarray <span class="keyword">or</span> scalar</span><br><span class="line">Drawn samples <span class="keyword">from</span> the parameterized normal distribution.</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html" target="_blank" rel="external">numpy.random.normal</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[numpy.newaxis 转变矩阵的形狀]]></title>
      <url>http://sggo.me/2018/09/11/python/py-numpy-9-newaxis/</url>
      <content type="html"><![CDATA[<p>有一個<strong>一維陣列 x1</strong>，我分別想要把它變成一個 3*1 的矩陣 <strong>x2</strong>，以及 1*3 的矩陣 <strong>x3</strong>，作法如下。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x1 = np.array([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>], float)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有一個一維陣列x1，我分別想要把它變成一個 3*1 的矩陣x2，以及 1*3 的矩陣x3，作法如下。</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"shape of x1 is "</span>, x1.shape)</span><br><span class="line">print(x1)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"-------------"</span>)</span><br><span class="line"></span><br><span class="line">x2 = x1[:, np.newaxis]</span><br><span class="line">print(<span class="string">"shape of x2 is "</span>, x2.shape)</span><br><span class="line">print(x2)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"-------------"</span>)</span><br><span class="line"></span><br><span class="line">x3 = x1[np.newaxis, :]</span><br><span class="line">print(<span class="string">"shape of x3 is "</span>, x3.shape)</span><br><span class="line">print(x3)</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">shape of x1 is  (3,)</span><br><span class="line">[ 10.  20.  30.]</span><br><span class="line">-------------</span><br><span class="line">shape of x2 is  (3, 1)</span><br><span class="line">[[ 10.]</span><br><span class="line"> [ 20.]</span><br><span class="line"> [ 30.]]</span><br><span class="line">-------------</span><br><span class="line">shape of x3 is  (1, 3)</span><br><span class="line">[[ 10.  20.  30.]]</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.ben-do.github.io/2016/09/15/change-shape-of-matrix-by-numpy/" target="_blank" rel="external">利用numpy的newaxis轉變矩陣的形狀</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[CNN (week4) - Face recognition & Neural style transfer]]></title>
      <url>http://sggo.me/2018/09/08/deeplearning/Convolutional-Neural-Networks-week4/</url>
      <content type="html"><![CDATA[<p>Face recognition &amp; Neural style transfer 能够在图像、视频以及其他 2D 或 3D 数据上应用这些算法。</p>
<a id="more"></a>
<h2 id="1-What-is-face-recognition"><a href="#1-What-is-face-recognition" class="headerlink" title="1. What is face recognition?"></a>1. What is face recognition?</h2><p>这一节中的人脸识别技术的演示的确很NB…, 演技不错，😄</p>
<p><img src="/images/deeplearning/C4W4-1_1.png" width="750"></p>
<h2 id="2-One-Shot-Learning"><a href="#2-One-Shot-Learning" class="headerlink" title="2. One Shot Learning"></a>2. One Shot Learning</h2><p>作为老板希望与时俱进，所以想使用人脸识别技术来实现打卡。</p>
<p>假如我们公司只有4个员工，按照之前的思路我们训练的神经网络模型应该如下：</p>
<p><img src="/images/deeplearning/C4W4-2.jpg" width="550"></p>
<blockquote>
<p>如图示，输入一张图像，经过CNN，最后再通过 Softmax 输出 5 个可能值的大小 (4个员工中的一个，或者都不是，所以共5种可能性)。</p>
<p>看起来好像没什么毛病，但是我们要相信我们的公司会越来越好啊，所以难道公司每增加一个人就要重新训练<strong>CNN</strong> 及 最后一层的输出数量吗 ？</p>
</blockquote>
<p><strong>one-shot：</strong></p>
<p>这显然有问题，所以有人提出了一次学习(one-shot)，更具体地说是通过一个函数来求出输入图像与数据库中的图像的差异度，用 $d(img1,img2)$ 表示。</p>
<p><img src="/images/deeplearning/C4W4-3_1.png" width="750"></p>
<p>如上图示，如果两个图像之间的差异度不大于某一个阈值 <strong>τ</strong>，那么则认为两张图像是同一个人。反之，亦然。</p>
<blockquote>
<p>下一小节介绍了如何计算差值。</p>
</blockquote>
<h2 id="3-Siamese-Network"><a href="#3-Siamese-Network" class="headerlink" title="3. Siamese Network"></a>3. Siamese Network</h2><p>注意：下图中两个网络参数是一样的。</p>
<p>先看上面的网络。记输入图像为 $x^{(1)}$，经过卷积层，池化层 和 全连接层 后得到了箭头所指位置的数据 (一般后面还会接上 $softmax$ 层，但在这里暂时不用管)，假设有 <strong>128</strong> 个节点，该层用 $f(x^{(1)})$ 表示，可以理解为输入 $x^{(1)}$ 的编码。</p>
<p>那么下一个网络同理，不再赘述。</p>
<p>因此上一节中所说的差异度函数即为</p>
<p>$$<br>d(x^{(1)},x^{(2)})=||f(x^{(1)})-f(x^{(2)})||^2<br>$$</p>
<p><img src="/images/deeplearning/C4W4-4_1.png" width="750"></p>
<p>问题看起来好像解决了，但感觉还漏了点什么。。<strong>神经网络的参数咋确定啊？也就是说 $f(x^{(i)})$ 的参数怎么计算呢？</strong></p>
<p>首先可以很明确的是如果两个图像是同一个人，那所得到的参数应该使得 $||f(x^{(1)})-f(x^{(2)})||^2$ 的值较小，反之较大。</p>
<p><img src="/images/deeplearning/C4W4-5_1.png" width="750"></p>
<h2 id="4-Triplet-Loss"><a href="#4-Triplet-Loss" class="headerlink" title="4. Triplet Loss"></a>4. Triplet Loss</h2><h3 id="4-1-Learning-Objective"><a href="#4-1-Learning-Objective" class="headerlink" title="4.1 Learning Objective"></a>4.1 Learning Objective</h3><p>这里首先介绍一个三元组，即 <strong>(Anchor, Positive, Negative)，简写为(A,P,N)</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">(A,P,N)</th>
<th style="text-align:center">三元组 各个含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Anchor</td>
<td style="text-align:center">可以理解为用于识别的图像 （锚）</td>
</tr>
<tr>
<td style="text-align:center">Positive</td>
<td style="text-align:center">表示是这个人</td>
</tr>
<tr>
<td style="text-align:center">Negative</td>
<td style="text-align:center">表示不是同一个人</td>
</tr>
</tbody>
</table>
<p>由上一节中的思路，我们可以得到如下不等式：</p>
<blockquote>
<p>$d(A,P)\leqq d(A,N)$, 即 $||f(A)-f(P)||^2-||f(A)-f(N)||^2\leqq0$ (如下图示)</p>
</blockquote>
<p><img src="/images/deeplearning/C4W4-6_1.png" width="750"></p>
<p>但是这样存在一个问题，即如果神经网络什么都没学到，返回的值是0，也就是说如果 $f(x)=\vec{0}$ 的话，那么这个不等式是始终成立的。(如下图示)</p>
<p><img src="/images/deeplearning/C4W4-7_1.png" width="750"></p>
<p>为了避免上述特殊情况，且左边值必须小于0，所以在右边减去一个变量<strong>α</strong>，但按照惯例是加上一个值，所以将<strong>α</strong>加在左边。</p>
<p><img src="/images/deeplearning/C4W4-8_1.png" width="750"></p>
<p><img src="/images/deeplearning/C4W4-9_1.png" width="750"></p>
<p>综上，所得到的参数需要满足如下不等式</p>
<p>$$<br>||f(A)-f(P)||^2-||f(A)-f(N)||^2+α\leqq0<br>$$</p>
<h3 id="4-2-Lost-function"><a href="#4-2-Lost-function" class="headerlink" title="4.2 Lost function"></a>4.2 Lost function</h3><p>介绍完三元组后，我们可以对单个图像定义如下的损失函数(如下图示)</p>
<p>$$<br>L(A,P,N)=max(||f(A)-f(P)||^2-||f(A)-f(N)||^2+α,0)<br>$$</p>
<blockquote>
<p>解释一下为什么用<strong>max</strong>函数，因为如果只要满足 $||f(A)-f(P)||^2-||f(A)-f(N)||^2+α\leqq0$，我们就认为已经正确识别出了图像中的人，所以对于该图像的损失值是 0.</p>
</blockquote>
<p><img src="/images/deeplearning/C4W4-10_1.png" width="750"></p>
<p>所以总的损失函数是 : $J=\sum{L(A^{(i)},P^{(i)},N^{(i)})}$</p>
<p>要注意的是使用这种方法要保证每一个人不止有一张图像，否则无法训练。另外要注意与前面的 <strong>One-shot</strong> 区分开来，这里是在训练模型，所以训练集的数量要多一些，每个人要有多张照片。而One-shot是进行测试了，所以只需一张用于输入的照片即可。</p>
<h3 id="4-3-Choosing-the-triplets-A-P-N"><a href="#4-3-Choosing-the-triplets-A-P-N" class="headerlink" title="4.3 Choosing the triplets(A,P,N)"></a>4.3 Choosing the triplets(A,P,N)</h3><p>还有一个很重要的问题就是如何选择三元组 <strong>(A,P,N)</strong>。因为实际上要满足不等式 $d(A,P)+α\leqq d(A,N)$ 是比较简单的,即只要将 Negative 选择的比较极端便可，比如 Anchor 是一个小女孩，而 Negative 选择一个老大爷。</p>
<p>所以还应该尽量满足 $d(A,N)\approx{d(A,N)}$</p>
<p><img src="/images/deeplearning/C4W4-11_1.png" width="750"></p>
<h2 id="5-Face-Verification-and-Binary-Classification"><a href="#5-Face-Verification-and-Binary-Classification" class="headerlink" title="5. Face Verification and Binary Classification"></a>5. Face Verification and Binary Classification</h2><p>通过以上内容，我们可以确定下图中的网络的参数了，那么现在开始进行面部验证了。</p>
<p><strong>上面的是测试图，下面的是数据库中的一张照片</strong></p>
<p>和之前一样假设 $f(x^{(i)})$ 有 128个节点，之后这两个数据作为输入数据输入到后面的逻辑回归模型中去，即</p>
<p>$$<br>\hat{y} = σ(\sum_{k=1}^{128}w_i|f(x^{(i)})_k-f(x^{(j)})_k|+b_i)<br>$$</p>
<p>若 $\hat{y}=1$, 为同一人。反之，不是。</p>
<p>如下图示，绿色下划线部分可以用其他公式替换，即有</p>
<p>$$<br>\hat{y}=σ(\sum_{k=1}^{128}w_i \frac{(f(x^{(i)})_k-f(x^{(j)})_k)^2}{f(x^{(i)})_k+f(x^{(j)})_k}+b_i)<br>$$</p>
<p><img src="/images/deeplearning/C4W4-12_1.png" width="750"></p>
<p>当然数据库中的图像不用每次来一张需要验证的图像都重新计算，其实可以提前计算好，将结果保存起来，这样就可以加快运算的速度了。</p>
<p><img src="/images/deeplearning/C4W4-13_1.png" width="750"></p>
<h2 id="6-What-is-neural-style-transfer"><a href="#6-What-is-neural-style-transfer" class="headerlink" title="6. What is neural style transfer?"></a>6. What is neural style transfer?</h2><p><img src="/images/deeplearning/C4W4-14_1.png" width="750"></p>
<h2 id="7-What-are-deep-ConvNets-learning"><a href="#7-What-are-deep-ConvNets-learning" class="headerlink" title="7. What are deep ConvNets learning?"></a>7. What are deep ConvNets learning?</h2><p><img src="/images/deeplearning/C4W4-15_1.png" width="750"></p>
<blockquote>
<p>第一层只能看到小部分卷积神经.</p>
<p>你选择一个隐藏单元，发现有9个图片，最大化了单元激活，你可能找到类似这样的图片浅层区域.</p>
</blockquote>
<p><img src="/images/deeplearning/C4W4-16_1.png" width="750"></p>
<h2 id="8-Cost-Function"><a href="#8-Cost-Function" class="headerlink" title="8. Cost Function"></a>8. Cost Function</h2><p>如下图示：</p>
<p><img src="/images/deeplearning/C4W4-17_1.png" width="750"></p>
<p>左上角的包含 Content 的图片简称为 C，右上角包含 Style 的简称 S，二者融合后得到的图片简称为 G。</p>
<p>我们知道计算问题须是有限的，所以融合的标准是什么？也就是说 Content 的保留程度和 Style 的运用程度如何取舍呢？</p>
<p>此时引入损失函数，并对其进行最优化，这样便可得到最优解。</p>
<p>$$<br>J(G)=αJ_{Content}(C,G)+βJ_{Style}(S,G)<br>$$</p>
<blockquote>
<p>$J(G)$ 定义用来生成图像的好坏，$J_{Content}(C,G)$ 表示 图像$C$ 和 图像$G$ 之间的差异，$J_{Style}(S,G)$ 同理。</p>
</blockquote>
<p><strong>计算过程示例</strong>：</p>
<blockquote>
<p>随机初始化图像 $G$，假设为 100*100*3 （maybe 500*500*3） (如下图右边四个图像最上面那个所示)</p>
<p>使用梯度下降不断优化 $J(G)$。 (优化过程如下图右边下面3个图像所示)</p>
</blockquote>
<p><img src="/images/deeplearning/C4W4-18_1.png" width="750"></p>
<blockquote>
<p>下面一小节将具体介绍 <strong>Cost Function</strong> 的计算。</p>
</blockquote>
<h2 id="9-Content-Cost-Function"><a href="#9-Content-Cost-Function" class="headerlink" title="9. Content Cost Function"></a>9. Content Cost Function</h2><p>首先假设我们使用 <strong>第 $l$ 层</strong> 隐藏层 来计算 $J_{Content}(C,G)$，注意这里的 <strong>$l$</strong> 一般取在中间层，而不是最前面的层，或者最后层</p>
<blockquote>
<p>原因如下：</p>
<ul>
<li>假如取<strong>第1层</strong>，那么得到的 $G$ 图像 将会与 图像$C$ 像素级别的相似，这显然不行。</li>
<li>假如取很深层，那么该层已经提取出了比较重要的特征，例如 图像$C$ 中有一条狗，那么得到的 图像$G$ 会过度的保留这个特征。</li>
</ul>
</blockquote>
<ul>
<li>然后使用预先训练好的卷积神经网络，如 VGG网络。这样我们就可以得到 图像$C$ 和 图像$G$ 在第$l$层的激活函数值，分别记为 $a^{[l][C]},a^{[l][G]}$</li>
<li>内容损失函数 $J_{Content}(C,G) = \frac{1}{2} || a^{[l][C]} - a^{[l][G]} ||^2$</li>
</ul>
<p><img src="/images/deeplearning/C4W4-19_1.png" width="750"></p>
<h2 id="10-Style-Cost-Function"><a href="#10-Style-Cost-Function" class="headerlink" title="10. Style Cost Function"></a>10. Style Cost Function</h2><h3 id="10-1-什么是“风格”"><a href="#10-1-什么是“风格”" class="headerlink" title="10.1 什么是“风格”"></a>10.1 什么是“风格”</h3><p>要计算风格损失函数，我们首先需要知道“风格(Style)”是什么。</p>
<p>我们使用 $l$ 层的激活来度量“Style”，将“Style”定义为通道间激活值之间的<strong>相关系数</strong>。(<strong>Define style as correlation between activation across channels</strong>)</p>
<p><img src="/images/deeplearning/C4W4-20_1.png" width="750"></p>
<p>那么我们如何计算这个所谓的相关系数呢？</p>
<p>下图是我们从上图中所标识的第 $l$ 层，为方便说明，假设只有 5 层通道。</p>
<p><img src="/images/deeplearning/C4W4-21_1.png" width="350"></p>
<p>如上图示，红色通道和黄色通道对应位置都有激活项，而我们要求的便是它们之间的<strong>相关系数</strong>。</p>
<p>但是为什么这么求出来是有效的呢？为什么它们能够反映出风格呢？</p>
<p>继续往下看↓</p>
<h3 id="10-2-图像风格的直观理解"><a href="#10-2-图像风格的直观理解" class="headerlink" title="10.2 图像风格的直观理解"></a>10.2 图像风格的直观理解</h3><p>如图风格图像有 <strong>5</strong> 层通道，且该图像的可视化特征如 <font color="blue">左下角图</font> 所示。</p>
<p><img src="/images/deeplearning/C4W4-22_1.png" width="800"></p>
<p>其中红色通道可视化特征如图中<strong>箭头</strong>所指是<strong>垂直条纹</strong>，而<strong>黄色通道的特征则是橘色背景</strong>。<br><!--<img src="/images/deeplearning/C4W4-22_2.png" width="750" />
--><br>那么通过计算这两层通道的相关系数有什么用呢？</p>
<blockquote>
<p>其实很好理解，如果<strong>二者相关系数性强，那么如果出现橘色背景，那么就应该很大概率出现垂直条纹</strong>。反之，亦然。</p>
</blockquote>
<h3 id="10-3-风格相关系数矩阵"><a href="#10-3-风格相关系数矩阵" class="headerlink" title="10.3 风格相关系数矩阵"></a>10.3 风格相关系数矩阵</h3><p>令 $a_{i,j,k}^{[l]}$ 表示 <strong>(i,j,k)</strong> 的激活项，其中 <strong>i,j,k</strong> 分别表示高度值(H)，宽度值(W) 及 所在通道层次(C)。</p>
<p>风格矩阵(也称为“<strong>Gram Matrix</strong>”)用 $G^{[l]}$ 表示，其大小为 $n_c^{l]}*n_c^{l]}$.</p>
<p>因此风格图像的风格矩阵为：</p>
<p>$$<br>G_{kk’}^{<a href="S">l</a>}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a_{i,j,k}^{<a href="S">l</a>}a_{i,j,k’}^{<a href="S">l</a>}<br>$$</p>
<p>生成图像的相关系数矩阵</p>
<p>$$<br>G_{kk’}^{<a href="G">l</a>}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a_{i,j,k}^{<a href="G">l</a>}a_{i,j,k’}^{<a href="G">l</a>}<br>$$</p>
<p><img src="/images/deeplearning/C4W4-23_1.png" width="750"></p>
<h3 id="10-4-风格损失函数"><a href="#10-4-风格损失函数" class="headerlink" title="10.4 风格损失函数"></a>10.4 风格损失函数</h3><p><img src="/images/deeplearning/C4W4-24_1.png" width="750"></p>
<p>第 $l$ 层的风格损失函数为：</p>
<p>$$<br>J_{Style}^{[l]} (S, G) = \frac {1} { (2n_H^{[l]} n_W^{[l]} n_C^{[l]})^2 } \sum_{k}\sum_{k’} (G_{kk’}^{[l](S)} - G_{kk’}^{[l](G)})<br>$$</p>
<p>总的风格损失函数：</p>
<p>$$<br>J_{Style}(S,G) = \sum_{l}λ^{[l]}J_{Style}C4^{[l]}(S,G)<br>$$</p>
<h2 id="11-1D-and-3D-Generalizations"><a href="#11-1D-and-3D-Generalizations" class="headerlink" title="11. 1D and 3D Generalizations"></a>11. 1D and 3D Generalizations</h2><p>1D generalizations of models</p>
<p><img src="/images/deeplearning/C4W4-25_1.png" width="750"></p>
<p>3D generalizations of models</p>
<p><img src="/images/deeplearning/C4W4-26_1.png" width="750"></p>
<blockquote>
<p>医学图像 与 视频检测 都是 3D 的.</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[深度学习与计算机视觉 - 历史回顾与介绍]]></title>
      <url>http://sggo.me/2018/09/01/deeplearning/deeplearning-CS231n-P1/</url>
      <content type="html"><![CDATA[<p>1966年是计算机视觉的诞生年</p>
<p>CVPR、ICCV</p>
<a id="more"></a>
<p>第一领悟 : Hubel 和 Wiesel  从简单的形状开始<br>第二领悟 : David Marr 《视觉》 视觉是分层的</p>
<p>深度学习架构的基石</p>
<p>感知分组</p>
<p>ImageNet</p>
<p>2012年是历史性时刻的一年，提出了 CNN .</p>
<p>ImageNet 竞赛 2010年 开始举办竞赛<br>ImageNet 竞赛 2012年 7层 CNN 获胜<br>ImageNet 竞赛 2015年 151层 CNN 获胜， MS 深度残差网络.</p>
<p>计算机视觉智能比物体识别要更为任重而道远</p>
<p>计算机视觉的愿景是 ： 可以看图讲故事</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21353567" target="_blank" rel="external">斯坦福CS231N课程学习笔记（一）.课程简介与准备</a></li>
<li><a href="https://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/78537000" target="_blank" rel="external">李飞飞CS231n2017课程双语字幕版上线 !（附课程链接）</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Convolutional Neural Networks (week3) - Object detection]]></title>
      <url>http://sggo.me/2018/09/01/deeplearning/Convolutional-Neural-Networks-week3/</url>
      <content type="html"><![CDATA[<p>知道如何将卷积网络应用到视觉检测和识别任务。 知道如何使用神经风格迁移生成艺术。</p>
<a id="more"></a>
<h2 id="1-Object-Localization"><a href="#1-Object-Localization" class="headerlink" title="1. Object Localization"></a>1. Object Localization</h2><p>这一小节视频主要介绍了我们在实现目标定位时标签该如何定义</p>
<p><img src="/images/deeplearning/C4W3-1_1.png" width="750"></p>
<p>输出 softmax 的同时，在输出4个值 $b_x,b_y,b_w,b_h$, 边界框的具体位置 （图片左上角坐标为 (0, 0)， 右下角为 (1, 1) )</p>
<blockquote>
<p>在上面的例子中，$b_x$ 的值大约为 0.5，$b_y$ 的值大约为 0.7，最佳位置.</p>
</blockquote>
<p><strong>how we define the target label $y$ for this as a supervised learning task.</strong></p>
<p><img src="/images/deeplearning/C4W3-2_1.png" width="750"></p>
<blockquote>
<p>上图左下角给出了损失函数的计算公式 (这里使用的是平方差)</p>
<p>注意: 我们假设图像中存在这三者 (<strong>pedestrian</strong>，<strong>car</strong>，<strong>motorcycles</strong>) 中的一种 或者 都不存在，所以共有四种可能.</p>
</blockquote>
<p>$P_c=1$ <strong>表示有三者中的一种</strong></p>
<ul>
<li><p>$C_1=1$ 表示有 pedestrian，反之没有</p>
</li>
<li><p>$C_2=1$ 表示有 car</p>
</li>
<li><p>$C_3=1$ 表示有 motorcycles</p>
</li>
</ul>
<p>$b_*$ <strong>用于标识所识别食物的位置</strong></p>
<ul>
<li><p>$b_x,b_y$: 表示识别物体的中心坐标</p>
</li>
<li><p>$b_w,b_h$: 表示识别物体的宽和高</p>
</li>
</ul>
<p><img src="/images/deeplearning/C4W3-3.png" width="450"></p>
<blockquote>
<p>注意: $P_c=0$ 表示三者都没有，所以此时 $C_*,b_*$ 的值我们并不在乎了.</p>
<p>为了让大家便于了解对象定位的细节，这里我用平方误差简化了描述过程. (实际应用中 $P_c$ 更多是应用逻辑回归函数)</p>
</blockquote>
<h2 id="2-Landmark-Detection"><a href="#2-Landmark-Detection" class="headerlink" title="2. Landmark Detection"></a>2. Landmark Detection</h2><p>特征点检测，这一节的内容和上一节感觉很类似.</p>
<p><img src="/images/deeplearning/C4W3-4_1.png" width="750"></p>
<blockquote>
<p>要输出眼角的位置，你可以让神经网络输出的最后一层，多输出 2 个数字 $l_x, l_y$.</p>
<p>假设 脸部 有 64 个特征点. 具体做法是准备一个 CNN 和 一些关键特征集. 将人脸图片输入 CNN. 输出 1 或者 0. </p>
<p>1 表示有 人脸。 这里一共有 <strong>128 + 1</strong> 个输出单元，因为有 64 个特征 64 * 2， 来实现对人脸的检测和定位.</p>
<p>Snapchat 应用，AR (Augmented Reality) 增强现实 Filter 有所了解，Snapchat Filter 实现了在人脸上画皇冠 还有一些其他效果。检测脸部特征也是计算机图形学的关键构造模块.</p>
<p>最后一个例子是，检测人体姿态.</p>
</blockquote>
<h2 id="3-Object-Detection"><a href="#3-Object-Detection" class="headerlink" title="3. Object Detection"></a>3. Object Detection</h2><p>假设你想构造一个汽车检测算法:</p>
<ol>
<li>首先，创建一个标签训练集 Training Set， 也就是 $x$ 和 $y$</li>
<li>选定一个大小确定的窗口，输入 CNN，CNN 开始预测红色方框中是否有汽车.</li>
<li>然后，滑动窗口继续, 红色方框稍向右滑动之后的区域, 处理下一个，输入 CNN..</li>
<li>依次重复操作.., 知道这个红色的窗口滑过每一个角落。</li>
<li>如果上面到最后角落还是没有检测到汽车， 那么选用更大的窗口，然后还是以固定步长..依次重复..滑过..检测</li>
</ol>
<p>这个算法叫做 : <strong>滑动窗口目标检测</strong></p>
<p><img src="/images/deeplearning/C4W3-5_1.png" width="750"></p>
<p>目标检测常使用的是滑动窗口技术检测，即使用一定大小的窗口按照指定的步长对图像进行遍历</p>
<p><img src="/images/deeplearning/C4W3-6.jpg" width="750"></p>
<p>因为图像中车辆的大小我们是不知道的，所以可以更改窗口大小，从而识别并定位出车辆的位置。</p>
<p><img src="/images/deeplearning/C4W3-7_1.png" width="750"></p>
<p><img src="/images/deeplearning/C4W3-8_1.png" width="750"></p>
<blockquote>
<p><strong>滑动窗口目标检测</strong> 算法的缺点就是计算成本的问题.</p>
<ol>
<li>如果你把窗口调大，显然会减少输入CNN的窗口个数，但是粗粒度可能会影响性能.</li>
<li>如果采用小粒度，那么输入给CNN的窗口个数就会特别多。这意味着超高计算成本.</li>
</ol>
<p>人们通常采用更简单的分类器做对象检测. 比如简单的线性分类器. 计算成本就会很低. 滑动窗口算法表现良好. 然而 CNN 运行单个分类任务的成本确高得多。 这样的滑动窗口太慢了。</p>
<p>庆幸的是，人们已经找到了方法解决计算成本，下一节见.</p>
</blockquote>
<h2 id="4-Convolutional-Implementation-of-Sliding-Windows"><a href="#4-Convolutional-Implementation-of-Sliding-Windows" class="headerlink" title="4. Convolutional Implementation of Sliding Windows"></a>4. Convolutional Implementation of Sliding Windows</h2><blockquote>
<p>注意：该节视频的例子和上一节一样，都是识别图像中是否有 pedestrian，car，motorcycles，background，所以最后输出y是 4个节点</p>
</blockquote>
<h3 id="4-1-全连接层→卷积层"><a href="#4-1-全连接层→卷积层" class="headerlink" title="4.1 全连接层→卷积层"></a>4.1 全连接层→卷积层</h3><p>在介绍卷积滑动窗口之前我们首先要知道如何把神经网络的<strong>全连接层</strong>转化成<strong>卷积层</strong>，下面是使用了全连接层的网络结构</p>
<p><img src="/images/deeplearning/C4W3-9_1.png" width="750"></p>
<p>那么如何将<strong>全连接层转化成卷积层</strong>呢？如下图示</p>
<p><img src="/images/deeplearning/C4W3-10_1.png" width="750"></p>
<p>我们可以看到经过 Max Pooling 之后的数据大小是 (5, 5, 16), 第一个FC层是 400 个节点。我们可以使用 400 个 5*5 的过滤器进行卷积运算，随后我们就得到了 (1, 1, 400) 的矩阵。</p>
<p>第二个 FC层 也是 400 个节点，由之前的 1*1 过滤器的特点，我们可以使用 400 个 1*1 的过滤器，也可以得到 (1,1,400) 的矩阵。至此，我们已经成功将全连接层转化成了卷积层。</p>
<blockquote>
<p>以上就是用卷积层代替全连接层的过程， 下面再看如何通过卷积实现滑动窗口对象检测算法，借鉴 OverFeat Paper。</p>
</blockquote>
<h3 id="4-2-卷积滑动窗口实现"><a href="#4-2-卷积滑动窗口实现" class="headerlink" title="4.2 卷积滑动窗口实现"></a>4.2 卷积滑动窗口实现</h3><p>目标检测一节中介绍了滑动窗口。要实现窗口遍历，那么就需要很大的计算量，But 大神们自然已经有了解决办法。</p>
<p>注意: 下面的第一幅图，Andrew Ng 为了方便只花了平面图，就没有画出3D的效果了。</p>
<p><strong>首先我们先看下图，这就是上面提到的将全连接层转化成卷积层的示意图:</strong></p>
<p><img src="/images/deeplearning/C4W3-11_1.png" width="750"></p>
<p>下面，假设我们的测试图大小是 16*16，并令滑动窗口大小是 14*14 的 (为了方便理解，下图用蓝色清楚地表明了 14*14 窗口的大小), 步长是 2，所以这个测试图可以被窗口划分成 4 个部分。随后和上面执行一样的操作，最后可以得到 (2,2,4) 的矩阵（这样发现很多计算部分是重复的），此时我们不难看出<strong>测试图被滑动窗口选取的左上角部分对应的结果也是输出矩阵的左上角部分，其他3个部分同理</strong>。</p>
<p><img src="/images/deeplearning/C4W3-12_1.png" width="750"></p>
<blockquote>
<p>所以这说明了什么？</p>
<p>说明我们没有必要用滑动窗口截取一部分，然后带入卷积网络运算。相反我们可以整体进行运算，这样速度就快很多了。</p>
</blockquote>
<p>下图很清楚的展示了卷积滑动窗口的实现。我们可以看到图片被划分成了 64 块</p>
<p><img src="/images/deeplearning/C4W3-13_1.png" width="750"></p>
<p><img src="/images/deeplearning/C4W3-14_1.png" width="750"></p>
<blockquote>
<p>目前这个算法还有一个缺点，就是边界框的位置可能不够准确，下一节，我们将学习解决这个缺点。</p>
</blockquote>
<h2 id="5-Bounding-Box-Predictions"><a href="#5-Bounding-Box-Predictions" class="headerlink" title="5. Bounding Box Predictions"></a>5. Bounding Box Predictions</h2><p>上面介绍的滑动窗口方法存在一个问题就是很多情况下滑动窗口并不能很好的切割出车体，如下图示：</p>
<p><img src="/images/deeplearning/C4W3-15.png" width="400"></p>
<blockquote>
<p>上面中的蓝色框可能是滑动窗口的时候，最佳的匹配位置了，但是其实我们人看后，其实很明显发现，这并不是，这就是我们要解决的问题所在. 其实最佳的匹配位置应该是红色的框，稍微有点长方形，长宽比有点向水平方向延伸.</p>
<p>为了解决这个问题，就有了 <strong>YOLO (you only look once)</strong> 算法，即只需要计算一次便可确定需要识别物体的位置的大小。</p>
</blockquote>
<p><strong>原理如下：</strong></p>
<p>首先将图像划分成 3*3 (即9份)，每一份最后由一个向量表示，这个向量在本文最前面介绍过，即</p>
<p>$$<br>y=[P_c,b_x,b_y,b_h,b_w,c_1,c_2,c_3]<br>$$</p>
<p><img src="/images/deeplearning/C4W3-16.png" width="450"></p>
<blockquote>
<p>如果一个格子中有两个对象，那么就当做这个格子中不存在对象. 按照 $P_c = 0$ 处理. </p>
<p>(如果是 19 * 19 的情况，两个对象的中点分配到一个格子的情况，其实特别低)</p>
</blockquote>
<p>因为有 9 份，所以最后输出矩阵大小是 (3,3,8) , 如下图示：</p>
<p><img src="/images/deeplearning/C4W3-17_1.png" width="750"></p>
<p>那么如何构建卷积网络呢？</p>
<p>输入矩阵是 (100, 100, 3), 然后是 Conv，Maxpool 层，……，最后只要确保输出矩阵大小是 (3,3,8) 即可。</p>
<p><img src="/images/deeplearning/C4W3-18_1.png" width="750"></p>
<blockquote>
<p>这是一个卷积实现，你并没有在 3 * 3 网格上跑 9 次算法，19 * 19 同样你不需要网格上跑 361 次，相反这是单次卷积的实现，但你<strong>使用了一个卷积网络</strong>，有很多共享计算步骤，比如在处理这 3 * 3 计算中很多计算步骤是共享的. 所以这个算法的效率很高.</p>
<p>YOLO 算法有一个好处是，它的计算特别快，可以达到实时识别。</p>
</blockquote>
<p><strong>下图是以右边的车辆作为示</strong>例介绍该车辆所在框的输出矩阵</p>
<blockquote>
<ul>
<li>很显然 $P_c=1$</li>
<li>然后 $b_x,b_y$ 的值是右边车辆的中心点相对于该框的位置,所以它们的值是一定小于 1 的，我们可以很容易的得到近似值 $b_x=0.4, b_y=0.3$</li>
<li>$b_h,b_w$ 的值同理也是车辆的宽高相对于其所在框的比例，但是要注意的是这两个值是可以大于 1 的，因为有可能部分车身在框外。但是也可以使用 sigmoid函数 将值控制在 1 以内。</li>
</ul>
</blockquote>
<p><img src="/images/deeplearning/C4W3-19_1.png" width="750"></p>
<blockquote>
<p>这里我用 3 * 3 的网格说明，在实践中可能 19 * 19 的网格会精细的多.</p>
<p>避免把 多个 对象分配到同一个格子中，观察这个对象的中点，然后将图像对象分配到其中点所在的格子. 19 * 19 的网格, 两个对象的中点处于同一个格子的概率会更低.</p>
<p>上面吴大大给出的是一个合理的约定，使用起来应该没什么问题. 其实还有其他更复杂的参数化形式.</p>
<p>YOLO 的论文是相对难度较高的论文, Andrew Ng 看的时候也看不懂很多.</p>
</blockquote>
<h2 id="6-Intersection-Over-Union"><a href="#6-Intersection-Over-Union" class="headerlink" title="6. Intersection Over Union"></a>6. Intersection Over Union</h2><p>如何评价对象检测算法呢，IOU 交并比函数可以用来评价对象检测算法。</p>
<p>前面说到了实现目标定位时可能存在 <strong>滑动窗口</strong> 与 <strong>真实边框</strong> 存在出入，如下图示：</p>
<p><img src="/images/deeplearning/C4W3-20.jpg" width="340"></p>
<blockquote>
<p>红色框是车身边界，紫色框是滑动窗口，那么此窗口返回的值是有车还是无车呢？</p>
<p>为了解决上面的问题引入了交并比(IoU)，也就是两个框之间的交集与并集之比，依据这个值可以评价定位算法是否精准。</p>
</blockquote>
<p><img src="/images/deeplearning/C4W3-21_1.png" width="750"></p>
<blockquote>
<p>上图黄色区域表示紫色框和红色框的交集，绿色区域表示紫色框和红色框的并集，交并比(IoU)就等于 黄色区域大小 比上 绿色区域大小。</p>
<p>如果 $IoU\geq0.5$，则表示紫色框中有车辆，反之没有。</p>
<p>当然 0.5 这个阈值是人为设定的，没有深入的科学探究，所以如果希望结果更加精确，也可以用 0.6 或 0.7 设为阈值，但是不建议用小于 0.5 的阈值。</p>
<p>下一节讨论这个 Non-max Suppression 工具，可以让 YOLO 工作的更好</p>
</blockquote>
<h2 id="7-Non-max-Suppression"><a href="#7-Non-max-Suppression" class="headerlink" title="7. Non-max Suppression"></a>7. Non-max Suppression</h2><blockquote>
<p><strong>非极大值抑制可以确保你对每个对象只检测一次</strong>。</p>
</blockquote>
<p>算法大致思路</p>
<p>前面 Bounding Box 一节中介绍到将图片划分成若干等分，例如 3*3，那么一共就有9块，如下图示，我们可以很清楚的看到第二行第一块和第三块都有车，所以可以标出一个中心点坐标 $(b_x,b_y)$，这样我们就能通过最终的输出结果知道这两个框中有车。</p>
<p><img src="/images/deeplearning/C4W3-22_1.png" width="550"></p>
<p>但是如果我们划分的数量变多之后呢？如下图示划分成了 19*19，图中标出的 3 个黄框和 3 个绿框最终结果都会都会返回[$P_x=1,b_x=,b_y=……$]，但是最后我们该信谁的呢？是这三个框真的有车，而且还不是同一辆车？还是只是同一辆车？所以就有了非极大值抑制来解决这个问题。</p>
<p><img src="/images/deeplearning/C4W3-23_1.png" width="550"></p>
<blockquote>
<p>Non-max Suppression 做的就是清理这些检测结果，这样一辆车只检测一次，而不是每辆车都出发多次检测。</p>
</blockquote>
<p>其思路大致如下 (为了方便说明和理解，我们不使用 19*19 的方框)：</p>
<p>首先每个框会对是否有目标返回一个 $P_c$ 的概率值 (也可以是 $P_c*C_1*C_2*C_3$ 的概率之积)，如下图示：</p>
<p><img src="/images/deeplearning/C4W3-24.png" width="550"></p>
<p>然后找到 $P_c$ 最大的一个框，显然 0.9 的框有车的概率最大，所以该边框颜色高亮</p>
<p><img src="/images/deeplearning/C4W3-25.jpg" width="550"></p>
<p><strong>然后算法遍历其他边框，找出与上一个边框的交并比大于 0.5 的边框</strong>，很显然右边剩余两个边框符合条件，所以这两个边框变暗</p>
<p><img src="/images/deeplearning/C4W3-26.jpg" width="550"></p>
<p>左边的车同理，不加赘述</p>
<p><img src="/images/deeplearning/C4W3-27.jpg" width="550"></p>
<p>下面结合一个例子总结一下 <strong>非极大值抑制</strong> 算法的实现步骤：</p>
<p>在这里假设只需要识别定位车辆即可，所以输出格式为 $P_c,b_x,b_y,b_h,b_w$</p>
<p><img src="/images/deeplearning/C4W3-28.png" width="750"></p>
<p>这个例子中将图像划分成 19*19 方格，假设每个方格都已经计算出 $P_c$ 的概率值</p>
<blockquote>
<ol>
<li>去掉所有满足 $P_c ≤ 0.6$ 的方格 (0.6也可以进行人为修改)</li>
<li>对剩下的方格进行如下循环操作：</li>
</ol>
<ul>
<li>从剩下的方格中选取 $P_c$ 最大的一个作为预测值输出，假设这个方格为 $A$</li>
<li>将与 $A$ 方格交并比大于 0.5 的剔除</li>
</ul>
</blockquote>
<h2 id="8-Anchor-Boxes"><a href="#8-Anchor-Boxes" class="headerlink" title="8. Anchor Boxes"></a>8. Anchor Boxes</h2><p>前面介绍了那么多，都只是识别单个物体，如果要同时识别多个物体该怎么办呢？而且识别的不同物体的中心点在同一个框中又该怎么呢 (如下图示，人和车的中心都在红点位置，处于同一个框中)？这时就需要使用 <strong>Anchor Boxes</strong> 了。</p>
<p><img src="/images/deeplearning/C4W3-29.jpg" width="400"></p>
<p>Anchor Boxes 思路是对于不同物体事先采用不同的框，例如人相对于车属于瘦高的，所以使用下图中的 <strong>Anchor Box 1</strong>，相反车辆就使用<strong> Anchor Box 2</strong>.</p>
<p>之前的输出值的格式都是 $y=[P_x,b_x,b]_y,b_h,b_w,C_1,C_2,C_3]$，最后输出的矩阵大小(以该图为例) 是 (3,3,8), 但是这样只能确定一个物体。</p>
<p>所以为了同时检测不同物体，很自然的我们可以重复输出这个上面的值即可，即 $y = [P_x, b_x, b_y, b_h, b_w, C_1, C_2, C_3, P_x, b_x, b_y, b_h, b_w, C_1, C_2, C_3]$, 所以输出矩阵是 (3,3,16), 也可以是(3,3,2,8)。</p>
<blockquote>
<p>要注意的是我们需要提前设定好输出值前面的值对应 <strong>Anchor Box 1</strong>，后面的对应 <strong>Anchor Box 2</strong>.</p>
<p>例如我们得到了图中人的边框信息值，然后经过计算发现其边框与 Anchor Box 1更为接近，所以最后将人的边框信息对应在前面，同理车辆边框信息对应在后面。</p>
</blockquote>
<p><img src="/images/deeplearning/C4W3-30_1.png" width="750"></p>
<p>总结起来 <strong>Anchor Box</strong>算法 和 之前的算法区别如下：</p>
<p><strong>之前的算法：</strong></p>
<blockquote>
<p>对于训练集图像中的每个对象，都根据那个对象的中点位置分配到对应的格子中,所以在上面的示例中输出y就是(3,3,8)</p>
</blockquote>
<p><strong>Anchor Boxes 算法</strong></p>
<blockquote>
<p>现在每个对象都和之前一样分配到同一个格子中, 即对象中心所在的格子。不同的是也需要分配到和对象形状交并比最高的 <strong>Anchor Box</strong>。</p>
</blockquote>
<p>例如下图中的红色框不仅要分配到其中心所在的图像上的格子中，而且还需要分配到与其交并比最大的 <strong>Anchor Box</strong> 中，即竖条的紫色方格。</p>
<p><img src="/images/deeplearning/C4W3-31_1.png" width="750"></p>
<p>回到本小节最开始的例子，最后的输出值如下图示：</p>
<p>图中人的对应 <strong>Anchor Box 1</strong>， 输出值对应图中的黄色字体；车辆同理，对应绿色字体</p>
<p><img src="/images/deeplearning/C4W3-32_1.png" width="750"></p>
<blockquote>
<p>如果两个对象所在一个格子，且两个对象圈出来的框也是一样的，这种情况非常非常少见，我们用其他方法来特殊处理.</p>
</blockquote>
<h2 id="9-YOLO-Algorithm"><a href="#9-YOLO-Algorithm" class="headerlink" title="9. YOLO Algorithm"></a>9. YOLO Algorithm</h2><p><img src="/images/deeplearning/C4W3-33_1.png" width="750"></p>
<p><img src="/images/deeplearning/C4W3-34_1.png" width="750"></p>
<p><img src="/images/deeplearning/C4W3-35_1.png" width="750"></p>
<blockquote>
<p>YOLO Algorithm, that also encompasses many of the best ideas across the entire computer vision literature the relate to object detection.</p>
<p>YOLO 计算机视觉对象检测领域文献中最精妙的思路。</p>
</blockquote>
<h2 id="10-Region-Proposals-Optional"><a href="#10-Region-Proposals-Optional" class="headerlink" title="10. Region Proposals (Optional)"></a>10. Region Proposals (Optional)</h2><p>候选区域，这些目前工作其实用的很少，但是还是非常有意义的，所以我作为可选视频可学习.​​​​</p>
<p><img src="/images/deeplearning/C4W3-36_1.png" width="750"> </p>
<p>吴大大认为 YOLO 这种只看一次的算法，长远而言是CV领域更有希望的方向</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Convolutional Neural Networks (week2) - deep CNN]]></title>
      <url>http://sggo.me/2018/08/24/deeplearning/Convolutional-Neural-Networks-week2/</url>
      <content type="html"><![CDATA[<p>理解如何搭建一个神经网络，包括最新的变体，例如残余网络。</p>
<a id="more"></a>
<h2 id="1-Why-look-at-case-studies"><a href="#1-Why-look-at-case-studies" class="headerlink" title="1. Why look at case studies?"></a>1. Why look at case studies?</h2><p>通过他人的实例可以更好的理解如何构建卷积神经网络，本周课程主要会介绍如下网络</p>
<ul>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>VGG</li>
<li>ResNet (有152层)</li>
<li>Inception</li>
</ul>
<h2 id="2-Classic-networks"><a href="#2-Classic-networks" class="headerlink" title="2. Classic networks"></a>2. Classic networks</h2><h3 id="2-1-LeNet-5"><a href="#2-1-LeNet-5" class="headerlink" title="2.1 LeNet-5"></a>2.1 LeNet-5</h3><p>该网络 1980s 提出，主要针对灰度图像训练的，用于识别手写数字。</p>
<p><img src="/images/deeplearning/C4W2-1_1.png" width="750"></p>
<blockquote>
<ol>
<li>当时很少用到 Padding，所以看到随着网络层次增加，图像的高度和宽度都是逐渐减小的，深度则不断增加.</li>
<li>当时人们会更倾向于使用 Average Pooling，但是现在则更推荐使用 Max Pooling.</li>
<li>最后的预测没有使用 softmax，而是使用了一般的方法.</li>
</ol>
<p>论文中你会发现，过去人们使用 Sigmoid函数 和 Tanh函数，而不是 ReLu， 这种网路结构的特别之处还在于各网络层之间是有关联的.</p>
</blockquote>
<h3 id="2-2-AlexNet"><a href="#2-2-AlexNet" class="headerlink" title="2.2 AlexNet"></a>2.2 AlexNet</h3><p>AlexNet 其实和 LetNet-5 有很多相似的地方，如大致的网络结构。不同的地方主要有如下：</p>
<p><img src="/images/deeplearning/C4W2-2_1.png" width="750"></p>
<ul>
<li>激活函数使用的是 <strong>Relu</strong>，最后一层使用的是 <strong>Softmax</strong></li>
<li>参数更多，有6000万个参数，而 LeNet-5 只有6万个左右</li>
<li>使用 Max Pooling</li>
</ul>
<blockquote>
<p>Local Response Normalization 局部响应归一化 - LRN层，不重要划掉.</p>
<p>这篇论文之后，深度学习逐渐在 CV 方面的关注，与日俱增. </p>
<p>AlexNet 比较复杂，包含大量超参数.</p>
</blockquote>
<h3 id="2-3-VGG-16"><a href="#2-3-VGG-16" class="headerlink" title="2.3 VGG-16"></a>2.3 VGG-16</h3><p><img src="/images/deeplearning/C4W2-3_1.png" width="750"></p>
<p>这个网络太牛了，因为它有将近 1.38亿个参数，即使放到现在也是一个很大的网络，但是这个网络的结构并不复杂。下面主要介绍一下上图网络。</p>
<p>首先该网络使用的是 Same卷积，即保证高度和宽度不变，另外因为总共有16层卷积操作，所以就不把每一层都用图像的方式表现出来了，例如 [CONV 64 X2] 表示的是用 64个 过滤器进行 Same卷积 操作2次，即右上角所画的示意图，(224,224,3) -&gt; (224,224,64) -&gt; (224,224,64)</p>
<blockquote>
<p><strong>Andrew Ng</strong> : 我最喜欢它的一点是，随着网络的加深，图像的 Height 和 Width 都在以一定的规律不断缩小，每次池化之后刚好缩小一半，而信道数量在不断增加. 而刚好也是在每组卷积操作后增加一倍. 也就是说 ： 图像缩小的比例和信道增加的比例是有规律的.</p>
<p>上面三个是比较经典的网络，可阅读其论文，Ng<strong>吴大师</strong> 建议的阅读顺序是 AlexNet-&gt;VGG-&gt;LeNet。</p>
</blockquote>
<h2 id="3-Residual-Network-ResNets"><a href="#3-Residual-Network-ResNets" class="headerlink" title="3. Residual Network (ResNets)"></a>3. Residual Network (ResNets)</h2><p>ResNets 发明者是 何恺明、张翔宇、任少卿、孙剑</p>
<p>吴大师表示 “非常深的网络是很难训练的，因为存在梯度消失和梯度爆炸的问题”，为了解决这个问题，引入了 <strong>Skip Connection</strong> (跳远链接)，残差网络正是使用了这个方法。</p>
<h3 id="3-1-残差块-Residual-Block"><a href="#3-1-残差块-Residual-Block" class="headerlink" title="3.1 残差块 (Residual Block)"></a>3.1 残差块 (Residual Block)</h3><p>首先介绍组成残差网络的单元：残差块(<strong>Residual Block</strong>)，如下图示：</p>
<p><img src="/images/deeplearning/C4W2-4.png" width="550"></p>
<p>残差块是由两层网络节点组成的, $a^{[l]}$ 经过线性变化，再通过Relu激活函数后得到 $a^{[l+1]}$， $a^{[l+2]}$ 也同理，具体过程如下图示：</p>
<p><img src="/images/deeplearning/C4W2-5_1.png" width="750"></p>
<p>特别注意上图中的<strong>紫色线</strong>连接，$a^{[{l}]}$ 通过这条线直接将数据传递给 $a^{[l+2]}$， 所以 $a^{[l+2]}=g(z^{[l+1]}+a^{[l]})$ ，这条紫色线也叫作<strong>short cut</strong>(或skip connection)</p>
<h3 id="3-2-残差网络"><a href="#3-2-残差网络" class="headerlink" title="3.2 残差网络"></a>3.2 残差网络</h3><p><img src="/images/deeplearning/C4W2-6_1.png" width="750"></p>
<p>如图示，残差网络每两层网络节点组成一个残差块，这也就是其与普通网络(Plain Network)的差别。</p>
<p>随着网络深度的加深，优化算法会越来越难训练，训练错误会越来越多，但是有了 ResNets 就不一样了. 也可以在一定程度上缓解梯度消失和梯度爆炸问题. 另一角度，网络越深会比较臃肿，但是 ResNet 确实在训练深度网络方面非常有效.</p>
<p>结合之前的课程我们知道如果使用普通网络训练模型，训练误差会随着网络层次加深先减小，而后会开始增加，而残差网络则不会有这种情况，反而它会随着层次增加，误差也会越来越小，这与理论相符。</p>
<p>随着网络深度的加深，优化算法会越来越难训练，训练错误会越来越多，但是有了 ResNets 就不一样了. 也可以在一定程度上缓解梯度消失和梯度爆炸问题. 另一角度，网络越深会比较臃肿，但是 ResNet 确实在训练深度网络方面非常有效.</p>
<h2 id="4-Why-ResNets-work"><a href="#4-Why-ResNets-work" class="headerlink" title="4. Why ResNets work"></a>4. Why ResNets work</h2><p>吴大表示: 网络在训练集上表现好，才能 Hold-out 交叉验证集 或 dev集、测试集上表现好，所以训练集上表现好是第一步.</p>
<p>为了直观解释残差网络为什么有用，假设我们已经通过一个很大的神经网络得到了 $a^{[l]}$。 而现在我们又需要添加两层网络进去，我们看看如果添加的是残差块会有什么效果。如下图示：</p>
<p><img src="/images/deeplearning/C4W2-7.jpg" width="650"></p>
<p>由 <strong>残差块Residual Block</strong> 的特点我们知道 $a^{[l+2]}=g(z^{[l+1]}+a^{[l]})=g(W^{[l+1]}a^{[l]}+b^{[l+1]}+a^{[l]})$</p>
<p>我们先考虑一个极端情况，即 $W^{[l+1]}=0,b^{[l+1]}=0$， 那么 $a^{[l+2]}=g(a^{[l]})=a^{[l]}$ <strong>(因为激活函数是Relu)</strong>，所以在添加了额外的两层网络后，即使最坏情况也是保持和之前结果一样。(而如果只是<strong>加上普通的两层网络</strong>，可能结果会更好，但是也很有可能结果会越来越糟糕, 因为普通网络就算是选择用来学习恒等函数的参数都很困难)，残差网络起作用的主要原因是这些残差块学习恒等函数非常容易, 这也就是为什么残差网络能够保证深度网络依旧有用的原因了。</p>
<blockquote>
<p><strong>注意</strong> ： 各层网络的<strong>维度</strong>，因为 $a^{[l+2]}=g(z^{[l+1]}+a^{[l]})$, 那么就要求 $z^{[l+1]}$ 要和 $a^{[l]}$ 保持相同的维度所以残差网络使用的是<strong>Same卷积</strong>。</p>
<p>但是如果唯独不一样也没关系，可以给 $a^{[l]}$ 乘上一个 $W_s$ 来保持相同维度。 $W_s$ 的值可以通过学习获得</p>
<p>普通网络 和 ResNets 常用的结构是 Conv -&gt; Conv -&gt; Conv -&gt; Pool -&gt; Conv -&gt; Conv -&gt; Conv -&gt; Pool 依次重复之.. 直到有一个通过 Softmax 进行预测的全连接层.</p>
</blockquote>
<h2 id="5-in-Network-and-1×1-convolutions"><a href="#5-in-Network-and-1×1-convolutions" class="headerlink" title="5. in Network and 1×1 convolutions"></a>5. in Network and 1×1 convolutions</h2><p>$1 * 1$ 卷积乍看起来好像很没用，如下图上，​但是如果这个 $1 * 1$ 的卷积有深度呢？​</p>
<p><img src="/images/deeplearning/C4W2-8.png" width="750"></p>
<p>说个更加直观的理解就是使用 $1*1$ 卷积可以很方便的减少深度，而不改变高度和宽度，如下图所示：</p>
<p><img src="/images/deeplearning/C4W2-9.png" width="700"></p>
<p>只需要用 32 个 ($1*1*192$) 的 Filter 即可, 如果不用 $1 * 1$ 卷积，例如采用 $2*2$ 卷积,要想实现只改变深度，那么还需要使用 padding，相比起来更加麻烦了.</p>
<h2 id="6-Inception-network-motivation"><a href="#6-Inception-network-motivation" class="headerlink" title="6. Inception network motivation"></a>6. Inception network motivation</h2><p><img src="/images/deeplearning/C4W2-10.jpg" width="700"></p>
<p>如上图示，我们使用了各种过滤器，也是用了 Max Pooling。但是这些并不需要人工的选择其个数，这些都可以通过学习来确定下来。所以这种方法很好的帮助我们选择何种 <strong>Filter</strong> 的问题，这也就是 <strong>Inception</strong>网络。</p>
<h3 id="6-1-计算成本"><a href="#6-1-计算成本" class="headerlink" title="6.1 计算成本"></a>6.1 计算成本</h3><p>注意随之而来的计算成本，尤其是 $5*5$ 的 Filter，下面以这个 Filter 举例进行说明：</p>
<p><img src="/images/deeplearning/C4W2-11.jpg" width="700"></p>
<p>如上图示，使用 32 个 $(5*5*192)$ 的 <strong>Filter</strong>，对 $(28,28,192)$ 进行 Same卷积 运算得到 $(28,28,32)$ 的输出矩阵，该卷积需要执行的乘法运算有多少次呢？</p>
<p>输出矩阵中的一个数据是经过 $(5*5*192)$ 次乘法得到的，那么总共的乘法运算次数则是 $(5*5*192*28*28*32=1.2)$ 亿</p>
<h3 id="6-2-瓶颈层-Bottleneck-layer"><a href="#6-2-瓶颈层-Bottleneck-layer" class="headerlink" title="6.2 瓶颈层(Bottleneck layer)"></a>6.2 瓶颈层(Bottleneck layer)</h3><p>上面运算次数多大1.2亿次，运算量相当大，因此有另一种网络结构对此进行优化，且可以达到同样效果，即采用 1 * 1 卷积</p>
<p><img src="/images/deeplearning/C4W2-12.png" width="700"></p>
<p>如图示进行了两次卷积，我们计算一下总共的乘法次数。</p>
<p>第一次卷积：28 * 28 * 16 * 192 = 2.4 million<br>第二次卷积：28 * 28 * 32 * 5 * 5 * 16 = 10 million<br>总共乘法次数是 12.4 million，这与上面直接用 5 * 5 Filter 的运算次数整整少了十倍。</p>
<h2 id="7-Inception-network"><a href="#7-Inception-network" class="headerlink" title="7. Inception network"></a>7. Inception network</h2><p><img src="/images/deeplearning/C4W2-12_1.png" width="750"></p>
<p><img src="/images/deeplearning/C4W2-13_1.png" width="750"></p>
<blockquote>
<p>为了可以防止过拟合，还有个特别的 inception network，是一个 Google 员工开发的叫做 GoogLeNet，这个名字是为了向 LeNet 致敬. 这样非常好，深度学习的研究人员如何重视协作，深度学习工作者对彼此的工作成果，都有一种强烈的敬意.</p>
</blockquote>
<h2 id="8-Using-open-source-impl"><a href="#8-Using-open-source-impl" class="headerlink" title="8. Using open-source impl"></a>8. Using open-source impl</h2><p>Practical advice for using ConvNets</p>
<h2 id="9-Transfer-Learning"><a href="#9-Transfer-Learning" class="headerlink" title="9. Transfer Learning"></a>9. Transfer Learning</h2><p>简单说就是在他人的基础上实现自己想要的模型，举个🌰，假如我们现在需要识别家里养的两只猫，分别叫 <strong>小花</strong> 和 <strong>小白</strong>，但是我们只有比较少的图片。幸运的是网上已经有一个已经训练好的模型，是用来区分1000个不同事物的(包括猫)，其网络模型如下：</p>
<p><img src="/images/deeplearning/C4W2-15.png" width="750"></p>
<p>我们的需求是最后结果有三种：是 <strong>小花</strong>，or <strong>小白</strong>，or <strong>都不是</strong>。​所以需要对 <strong>softmax</strong> 做如下图修改.</p>
<p>由于数据较少，所以可以对他人的模型的前面的结构中的参数进行冻结，即 <strong>权重 weight</strong> 和 <strong>偏差 bias</strong> 不做改动。</p>
<p><img src="/images/deeplearning/C4W2-16.png" width="750"></p>
<p>​当然，如果我们有一定量的数据，那么 <strong>freeze</strong> 的范围也可以随之减少，即拿来训练的层次可以增多<br>​<br>​<img src="/images/deeplearning/C4W2-17.png" width="750"></p>
<blockquote>
<p>you find that for a lot of computer vision applications, you just do much better if you download someone else’s open source weights and use that as initialization for your problem.</p>
<p>I think that computer vision is one where transfer learning is something that you should almost always do. （unless you actually have a very very large data set..）</p>
</blockquote>
<h2 id="10-Data-augmentation"><a href="#10-Data-augmentation" class="headerlink" title="10. Data augmentation"></a>10. Data augmentation</h2><h3 id="10-1-Common-augmentation"><a href="#10-1-Common-augmentation" class="headerlink" title="10.1 Common augmentation"></a>10.1 Common augmentation</h3><ul>
<li>旋转(rotation)</li>
<li>修剪(shearing)</li>
<li>局部变形(local warping)</li>
<li>镜像(mirroring)</li>
</ul>
<p>​<img src="/images/deeplearning/C4W2-18.jpg" width="750"></p>
<blockquote>
<p>以上介绍的方法，同时使用并没有什么坏处，但是在实践中，因为太复杂了，所以使用的很少。</p>
<p>更经常使用的方法可能下面要介绍的 <strong>Color shifting</strong> 。</p>
</blockquote>
<h3 id="10-2-Color-shifting"><a href="#10-2-Color-shifting" class="headerlink" title="10.2 Color shifting"></a>10.2 Color shifting</h3><p>我们都知道图像是由 <strong>RGB</strong> 三种颜色构成的，所以该数据扩充方法常采用 <strong>PCA color augmentation</strong>，即假如一个图片的 <strong>R</strong> 和 <strong>G</strong> 成分较多，那么该算法则 <strong>R,G</strong> 的值会减少很多，而 B 的值变化会少一些，所以使得总体的颜色保持一致.</p>
<p>​<img src="/images/deeplearning/C4W2-19_2.png" width="750"></p>
<blockquote>
<p>如果你看不懂这些，那么没关系，可以看看 AlexNet 论文中的细节，你也能找到 PCA 颜色增强的开源实现方法.</p>
</blockquote>
<h2 id="11-The-state-of-CV"><a href="#11-The-state-of-CV" class="headerlink" title="11. The state of CV"></a>11. The state of CV</h2><p>​<img src="/images/deeplearning/C4W2-21_1.png" width="750"></p>
<p>​<img src="/images/deeplearning/C4W2-22_1.png" width="750"></p>
<p>​<img src="/images/deeplearning/C4W2-23_1.png" width="750"><br>​</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Convolutional Neural Networks (week1) - CNN]]></title>
      <url>http://sggo.me/2018/08/21/deeplearning/Convolutional-Neural-Networks-week1/</url>
      <content type="html"><![CDATA[<ul>
<li>理解如何搭建一个神经网络，包括最新的变体，例如残余网络。</li>
<li>知道如何将卷积网络应用到视觉检测和识别任务。</li>
<li>知道如何使用神经风格迁移生成艺术。</li>
<li>能够在图像、视频以及其他2D或3D数据上应用这些算法。</li>
</ul>
<a id="more"></a>
<h2 id="1-Computer-vision"><a href="#1-Computer-vision" class="headerlink" title="1. Computer vision"></a>1. Computer vision</h2><p><img src="/images/deeplearning/C4W1-1_1.png" width="700"></p>
<p><img src="/images/deeplearning/C4W1-2_1.png" width="700"></p>
<blockquote>
<p>如图示，之前课程中介绍的都是 64 * 64 * 3的图像 (<strong>3 代表</strong>:因为每个图片都有3个颜色通道 channels, 12288 So $X$, <strong>the input features has dimension 12288</strong>)，而一旦图像质量增加，例如变成 1000 * 1000 * 3 的时候那么此时的神经网络的计算量会巨大，显然这不现实。所以需要引入其他的方法来解决这个问题.</p>
</blockquote>
<h2 id="2-Edge-detection-example"><a href="#2-Edge-detection-example" class="headerlink" title="2. Edge detection example"></a>2. Edge detection example</h2><p>使用边缘检测作为入门样例, you see how the convolution operation works.</p>
<p><img src="/images/deeplearning/C4W1-3_1.png" width="700"></p>
<blockquote>
<p>边缘检测可以是垂直边缘检测，也可以是水平边缘检测，如上图所示.</p>
</blockquote>
<p>至于算法如何实现，下面举一个比较直观的例子：</p>
<p><img src="/images/deeplearning/C4W1-4_1.png" width="700"></p>
<blockquote>
<p>可以很明显的看出原来 6 * 6 的矩阵有明显的垂直边缘，通过 3 * 3 的过滤器 <strong>filter</strong> (也叫做 “核”)卷积之后，仍然保留了原来的垂直边缘特征，虽然这个边缘貌似有点粗，这是因为数据不够大的原因，如果输入数据很大的话这个不是很明显了.</p>
<p>关于用编程语言实现：python / tensorflow / keras 等，都有一些函数来实现卷积运算.</p>
</blockquote>
<h2 id="3-More-edge-detection"><a href="#3-More-edge-detection" class="headerlink" title="3. More edge detection"></a>3. More edge detection</h2><p><img src="/images/deeplearning/C4W1-5_1.png" width="700"></p>
<p>除了上面的垂直，水平边缘检测，其实也可以检测初颜色过度变化，例如是亮变暗，还是暗变亮？</p>
<p><img src="/images/deeplearning/C4W1-5_2.png" width="600"></p>
<p><strong>在计算机视觉的历史上，层曾经公平的争论过哪些什么样的 <code>filter</code> 数字组合才是最好的:
</strong></p>
<blockquote>
<p><strong>下面是一些常见的过滤器，第二个是<code>Sobel filter</code>，具有较强的鲁棒性，第三个是<code>Schoss filter</code>.</strong></p>
</blockquote>
<p><img src="/images/deeplearning/C4W1-6_1.png" width="700"></p>
<table>
<thead>
<tr>
<th style="text-align:center">Filter</th>
<th style="text-align:center">desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Sobel Filter</td>
<td style="text-align:center">it puts a little bit more weight to the central row <br> 增加了中间一行的权重</td>
</tr>
<tr>
<td style="text-align:center">Schoss Filter</td>
<td style="text-align:center">实际它也是一种垂直边缘检测 <br> 翻转90度，它就变为水平边缘检测 </td>
</tr>
<tr>
<td style="text-align:center">Other Filter</td>
<td style="text-align:center">9个参数也可以通过学习(反向传播)的方式获得 </td>
</tr>
</tbody>
</table>
<blockquote>
<p>其实过滤器的9个参数也可以通过学习(反向传播)的方式获得，虽然比较费劲，但是可能会学到很多其他除了垂直，水平的边缘特征，例如  45°，70° 等各种特征.</p>
</blockquote>
<h2 id="4-Padding"><a href="#4-Padding" class="headerlink" title="4. Padding"></a>4. Padding</h2><blockquote>
<p><strong>由前面的例子, 卷积的方法，有2个缺点:</strong></p>
<ol>
<li><p>每经过一次卷积计算，原数据都会减小，但有时我们并不希望这样。举个比较极端的例子：假设原数据是 30 * 30 的一只猫的图像，经过10次卷积 (过滤器是3 * 3) 后，最后图像只剩下了 10 * 10 了 😳😳</p>
</li>
<li><p>由卷积的计算方法可知，图像边缘特征计算次数显然少于图像中间位置的像素点，如下图示 (绿色的位置明显是冷宫), 图像边缘的大部分信息都丢失了.</p>
</li>
</ol>
</blockquote>
<h3 id="4-1-运用-Padding-的原因"><a href="#4-1-运用-Padding-的原因" class="headerlink" title="4.1 运用 Padding 的原因"></a>4.1 运用 Padding 的原因</h3><p><img src="/images/deeplearning/C4W1-7_1.png" width="500"></p>
<p>原来的 6 * 6 填充后变成了 8 * 8，此时在经过一次卷积得到的仍旧是 6 * 6 的矩阵。</p>
<p>下面总结一下卷积之后得到矩阵大小的计算方法，假设：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Title</th>
<th style="text-align:center">Size</th>
<th style="text-align:center">desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">原数据</td>
<td style="text-align:center">$n * n$</td>
<td style="text-align:center">矩阵 $n * n$ </td>
</tr>
<tr>
<td style="text-align:center">Filter</td>
<td style="text-align:center">$f * f$</td>
<td style="text-align:center">过滤器</td>
</tr>
<tr>
<td style="text-align:center">Padding</td>
<td style="text-align:center">$p * p$</td>
<td style="text-align:center">填充数量</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>综上：</strong></td>
<td style="text-align:center"><strong>$n+2p-f+1$</strong></td>
<td style="text-align:center">得到的矩阵大小</td>
</tr>
</tbody>
</table>
<blockquote>
<p>padding 后，虽然边缘像素点仍旧计算的比较少，但是这个缺点至少一定程度上被削弱了.</p>
</blockquote>
<h3 id="4-2-如何-padding-的大小"><a href="#4-2-如何-padding-的大小" class="headerlink" title="4.2 如何 padding 的大小"></a>4.2 如何 padding 的大小</h3><table>
<thead>
<tr>
<th style="text-align:center">Type</th>
<th style="text-align:center">desc</th>
<th style="text-align:center">Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Valid convolutions</td>
<td style="text-align:center">不添加 padding</td>
<td style="text-align:center">$n - f + 1$</td>
</tr>
<tr>
<td style="text-align:center">Same convolutions</td>
<td style="text-align:center">Pad so that output size is the same as the input size. <br><br> 保持原图像矩阵的大小</td>
<td style="text-align:center">满足 $n+2p-f+1 = n$ <br><br> 即 $p=\frac{f-1}{2}$, 为了满足上式，$f$ 一般奇数</td>
</tr>
</tbody>
</table>
<h2 id="5-Strided-convolutions"><a href="#5-Strided-convolutions" class="headerlink" title="5. Strided convolutions"></a>5. Strided convolutions</h2><p>过滤器 纵向、横向 都需要按 步长 $S$ 来移动，如图示:</p>
<p><img src="/images/deeplearning/C4W1-8_1.png" width="700"></p>
<p>结合之前的内容，输出矩阵大小计算公式方法为，假设：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Title</th>
<th style="text-align:center">Size</th>
<th style="text-align:center">desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">原数据</td>
<td style="text-align:center">$n * n$</td>
<td style="text-align:center">矩阵 $n * n$ </td>
</tr>
<tr>
<td style="text-align:center">Filter</td>
<td style="text-align:center">$f * f$</td>
<td style="text-align:center">过滤器</td>
</tr>
<tr>
<td style="text-align:center">Padding</td>
<td style="text-align:center">$p * p$</td>
<td style="text-align:center">填充数量</td>
</tr>
<tr>
<td style="text-align:center">Stride</td>
<td style="text-align:center">$s * s$</td>
<td style="text-align:center">步长</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>综上：</strong></td>
<td style="text-align:center">得到的矩阵大小是</td>
<td style="text-align:center"><strong>⌊$\frac{n+2p-f}{s}$⌋ * ⌊$\frac{n+2p-f}{s}$⌋</strong></td>
</tr>
</tbody>
</table>
<blockquote>
<p>⌊⌋: 向下取整符号 ⌊59/60⌋ = 0</p>
<p>⌈⌉: 向上取整符号 ⌈59/60⌉ = 1</p>
</blockquote>
<h2 id="6-Convolutions-Over-Volumes"><a href="#6-Convolutions-Over-Volumes" class="headerlink" title="6. Convolutions Over Volumes"></a>6. Convolutions Over Volumes</h2><p>这一节用立体卷积来解释</p>
<p><img src="/images/deeplearning/C4W1-9_1.png" width="700"></p>
<blockquote>
<p><strong>如图</strong>:</p>
<ul>
<li>输入矩阵是 $6 * 6 * 3$ (height * width * channels), 过滤器是 $3 * 3 * 3$，计算方法是一一对应相乘相加</li>
<li>最后得到 $4 * 4$ 的二维矩阵.</li>
</ul>
<p>有时可能需要检测 水平边缘 或 垂直边缘，或 其他特征，所以我们可以使用多个过滤器。上图则使用了两个过滤器 (黄色和橘黄色)，得到的特征矩阵大小为 $4 * 4 * 2$.</p>
<p><strong>Filter</strong> 数字组合参数的选择不同，你可以得到不同的特征检测器.</p>
</blockquote>
<h2 id="7-One-Layer-of-CNN-Example"><a href="#7-One-Layer-of-CNN-Example" class="headerlink" title="7. One Layer of CNN Example"></a>7. One Layer of CNN Example</h2><p><img src="/images/deeplearning/C4W1-10_1.png" width="700"></p>
<p>如图示得到 <strong>$4 * 4$</strong> 的矩阵后还需要加上一个偏差 <strong>$b_n$</strong> (Python 广播机制)，之后还要进行非线性转换，即用 ReLU 函数.</p>
<p><img src="/images/deeplearning/C4W1-10_2.png" width="700"></p>
<p>因此假如在某一卷积层中使用了 10 个 <strong>$3 * 3$</strong> 的 <strong>Filter</strong> 过滤器，那么一共有 $(3*3+1)*10=280$ 个参数.</p>
<p><img src="/images/deeplearning/C4W1-11_1.png" width="700"></p>
<p>下面总结了各项参数的大小和表示方法：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Title</th>
<th style="text-align:center">Formula</th>
<th style="text-align:center">desc </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>输入矩阵</strong></td>
<td style="text-align:center">$n_H^{l-1} * n_W^{l-1} * n_c^{l-1}$</td>
<td style="text-align:center">$height * width * channels = 6 * 6 * 3$</td>
</tr>
<tr>
<td style="text-align:center">过滤器大小</td>
<td style="text-align:center">$f^{[l]}$</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Padding</td>
<td style="text-align:center">$p^{[l]}$</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Stride</td>
<td style="text-align:center">$s^l$</td>
<td style="text-align:center">步长</td>
</tr>
<tr>
<td style="text-align:center">Each Filter is</td>
<td style="text-align:center">$f^{l} * f^{l} * n_c^{[l-1]}$</td>
<td style="text-align:center">$3 * 3 * 3$  ,  $n_c^{[l-1]} = 3$ <br><br> 每一卷积层的过滤器的通道的大小 = 输入层的通道大小 $n_c^{[l-1]}$</td>
</tr>
<tr>
<td style="text-align:center">Activations 激活函数</td>
<td style="text-align:center">$a^{l}$ = $n_H^{l} * n_W^{l} * n_c^{l}$</td>
<td style="text-align:center">$A^{l}$ = $m * n_H^{l} * n_W^{l} * n_c^{l}$，($m$个例子)</td>
</tr>
<tr>
<td style="text-align:center">权重 weight</td>
<td style="text-align:center">$f^{l} * f^{l} * n_c^{[l-1]} * n_c^{[l]}$</td>
<td style="text-align:center">$Filter * 过滤器个数$ <br><br> 过滤器的个数 = 输出层的通道的大小 $n_c^{l}$</td>
</tr>
<tr>
<td style="text-align:center">偏差 bias</td>
<td style="text-align:center">$1 * 1 * 1 * n_c^{[l]}$</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>输出矩阵</strong></td>
<td style="text-align:center">$n_H^{l} * n_W^{l} * n_c^{l}$</td>
<td style="text-align:center">$height * width * channels$</td>
</tr>
<tr>
<td style="text-align:center">Output</td>
<td style="text-align:center">$n_{H/W}^{[l]}=[\frac{n_{H/W}^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]$</td>
<td style="text-align:center">输出层与输入层计算公式</td>
</tr>
</tbody>
</table>
<h2 id="8-A-Simple-Convolution-Network"><a href="#8-A-Simple-Convolution-Network" class="headerlink" title="8. A Simple Convolution Network"></a>8. A Simple Convolution Network</h2><p><img src="/images/deeplearning/C4W1-12_1.png" width="750"></p>
<p>上图简单介绍了卷积网络的计算过程，需要再介绍的一点是最后一层的全连接层，即将 $7 * 7 * 40$ 的输出矩阵展开，得到 $1960$ 个节点，然后再采用 <strong>Softmax</strong> 来进行预测.</p>
<blockquote>
<p>一般的 <strong>CNN</strong> 中，每一层矩阵的 <strong>height</strong> 和 <strong>width</strong> 是逐渐减小的，而 <strong>channel</strong> 则是增加的 ($n_c$ 在增加，$n_H$ 和 $n_W$ 在减少).</p>
</blockquote>
<p><strong>CNN 中常见的 3 种类型的 layer：</strong></p>
<ol>
<li>Convolution (Conv 卷积层)</li>
<li>Pooling (Pool 池化层)</li>
<li>Fully connected (FC 全连接层)</li>
</ol>
<h2 id="9-Pooling-Layers"><a href="#9-Pooling-Layers" class="headerlink" title="9. Pooling Layers"></a>9. Pooling Layers</h2><p>使用池化层来缩减模型的大小，提高计算速度，同时提高提取特征的鲁棒性.</p>
<p><img src="/images/deeplearning/C4W1-13_1.png" width="700"></p>
<p><strong><font color="wathet">最大池化的直观理解</font>:</strong></p>
<p>你可以把上面 $4 * 4$ 输入看做是某些特征的集合 (也许不是)，也就是神经网络中某一层的反激活值，数字大意味着可能提取了某些特定特征. 左上象限具有这个特征，可能是一个垂直边缘. or maybe an eye， 显然左上象限具有这个特征.</p>
<p>最大化操作的功能就是只要在任何一个象限内提取到某个特征，它就会保留在最大池化的输出里. 最大化操作的实际作用就是：如果在 <strong>Filter</strong> 中提取到某个特征 那么保留其最大值. 如果没有提取到这个特征，比如说 右上象限 中，那么其最大值也还是很小. </p>
<blockquote>
<p><code>Max Pooling</code> 的超级参数 $f=2 / 3$，$s=2$ 是最常用的，效果相当于高度和宽度缩减一半. <code>Max Pooling</code> 很少用 padding， by far, is $p = 0$，Average Pooling 这个用的不多，这个也会加入更多的计算量.</p>
</blockquote>
<h2 id="10-A-CNN-Example"><a href="#10-A-CNN-Example" class="headerlink" title="10. A CNN Example"></a>10. A CNN Example</h2><p>在 Andrew Ng 的课件中将 Conv layer 和 Pooling layer 合并在一起视为一层，因为池化层没有参数 (因为池化层的过滤器 无参数，有超参数，而且其大小可以事先确定好)。 但是在其他文献中有可能会把池化层算成单独的层，所以视情况而定。</p>
<p><img src="/images/deeplearning/C4W1-14_1.png" width="750"></p>
<p><img src="/images/deeplearning/C4W1-15.jpg" width="750"></p>
<blockquote>
<p>池化层参数个数为 0，卷积层参数个数一般多，<strong>fully connected layer</strong> 全连接层 参数很多.</p>
</blockquote>
<h2 id="11-Why-Convolutions"><a href="#11-Why-Convolutions" class="headerlink" title="11. Why Convolutions ?"></a>11. Why Convolutions ?</h2><p><img src="/images/deeplearning/C4W1-16_1.png" width="750"></p>
<p>卷积 相比于 FC全连接 的好处最直观的就是使用的参数更少，<strong>参数共享</strong> 和 <strong>稀疏连接</strong>：</p>
<h3 id="11-1-Parameter-sharing"><a href="#11-1-Parameter-sharing" class="headerlink" title="11.1 Parameter sharing"></a>11.1 Parameter sharing</h3><p>特征检测如垂直边缘检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域，也就是说如果你用一个 $3 * 3$ 的 Filter 检测垂直边缘. 那么图片的左上角区域以及旁边的各个区域都可以使用这个 $3 * 3$ 的过滤器. 每个特征检测器以及输出都可以在输入图片的不同区域中使用同样的参数，以便提取垂直边缘或其他特征，它不仅适用于边缘特征这样的低阶特征，同样适用于高阶特征. 例如：提取脸上的眼睛 等或者其他对象. 这种是整张图片共享特征检测器 提取效果也很好</p>
<p><img src="/images/deeplearning/C4W1-17_1.png" width="750"></p>
<h3 id="11-2-Sparsity-of-Connections"><a href="#11-2-Sparsity-of-Connections" class="headerlink" title="11.2 Sparsity of Connections"></a>11.2 Sparsity of Connections</h3><p>右边图 的边缘，仅与 36 个输入特征中的 9 个相连接, 而且其他像素值都不会对输出产生任何影响, 这就是稀疏连接的概念. 某个输出点，看上去只有这 9 个输入特征与输出相连接. 其他像素对输出没有任何影响. 神经网络可以通过这两种机制减少参数. 以便于我们用更小的训练集训练它，从而预防过度拟合. (卷积有一个平移不变属性)</p>
<p>综上这些就是卷积神经网络 CNN 表现良好的原因.</p>
<p><img src="/images/deeplearning/C4W1-18_1.png" width="750"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31657315" target="_blank" rel="external">CNN入门讲解：卷积层是如何提取特征的？</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Models (week3) - Attention mechanism]]></title>
      <url>http://sggo.me/2018/08/14/deeplearning/Sequence-Models-week3/</url>
      <content type="html"><![CDATA[<p>能够将序列模型应用到自然语言问题、音频应用 等，包括文字合成、语音识别和音乐合成。</p>
<a id="more"></a>
<h2 id="1-Basic-models"><a href="#1-Basic-models" class="headerlink" title="1. Basic models"></a>1. Basic models</h2><p>假设需要翻译下面这句话</p>
<blockquote>
<p>“简将要在 9 月访问中国”</p>
</blockquote>
<p>我们希望得到的结果是</p>
<blockquote>
<p>“<strong>Jane is visiting China in September</strong>”</p>
</blockquote>
<p>在这个例子中输入的数量是 10 个中文汉字，输出为 6 个单词， $T_x$ 与 $T_y$ 数量不一致，就需要用到 Sequence to sequence model <strong>RNN</strong></p>
<p><img src="/images/deeplearning/C5W3-1.jpg" width="750"></p>
<p>类似的例子还有用机器为下面这张图片生成描述</p>
<p><img src="/images/deeplearning/C5W3-2.png" width="600"></p>
<p>只需要将 encoder 部分用一个 CNN模型 替换就可以了，比如 AlexNet，就可以得到“一只（可爱的）猫躺在楼梯上”</p>
<h2 id="2-Picking-the-most-likely-sentence"><a href="#2-Picking-the-most-likely-sentence" class="headerlink" title="2. Picking the most likely sentence"></a>2. Picking the most likely sentence</h2><p>下面将之前学习的语言模型和机器翻译模型做一个对比, P 为概率</p>
<p>语言模型:</p>
<p><img src="/images/deeplearning/C5W3-3.png" width="700"></p>
<p>机器翻译模型:</p>
<p><img src="/images/deeplearning/C5W3-4.png" width="750"></p>
<p>可以看到，机器翻译模型的后半部分其实就是语言模型，Andrew 将其称之为 “<strong>条件语言模型</strong>”，在语言模型之前有一 个条件也就是被翻译的句子:</p>
<p>$$<br>P(y^{<1>},…,y^{&lt;{T_y}&gt;}|x^{<1>},…,x^{&lt;{T_x}&gt;})<br>$$</1></1></p>
<blockquote>
<p>但是我们知道翻译是有很多种方式的，同一句话可以翻译成很多不同的句子，那么我们如何判断哪一个句子是最好的呢？</p>
<p>还是翻译上面那句话，有如下几种翻译结果：</p>
<ul>
<li>“Jane is visiting China in September.”</li>
<li>“Jane is going to visit China in September.”</li>
<li>“In September, Jane will visit China”</li>
<li>“Jane’s Chinese friend welcomed her in September.”</li>
<li>….</li>
</ul>
<p>与语言模型不同的是，机器模型在输出部分不再使用 <strong>softmax</strong> 随机分布的形式进行取样，因为很容易得到一个不准确的翻译，取而代之的是使用 <code>Beam Search</code> 做最优化的选择。这个方法会在后下一小节介绍，在此之前先介绍一下<strong>贪婪搜索(Greedy Search)</strong>及其弊端，这样才能更好地了解 <code>Beam Search</code> 的优点。</p>
</blockquote>
<h3 id="2-1-Greedy-Search"><a href="#2-1-Greedy-Search" class="headerlink" title="2.1 Greedy Search"></a>2.1 Greedy Search</h3><p>得到最好的翻译结果，转换成数学公式就是:</p>
<p>$$<br>argmax P(y^{<1>},…,y^{&lt;{T_y}&gt;}|x^{<1>},…,x^{&lt;{T_x}&gt;})<br>$$</1></1></p>
<p>那么 Greedy Search 是什么呢？</p>
<p>通俗解释就是每次输出的那个都必须是最好的。还是以翻译那句话为例。</p>
<p>现在假设通过贪婪搜索已经确定最好的翻译的前两个单词是：”Jane is “</p>
<p>然后因为 “going” 这个单词出现频率较高和其它原因，所以根据贪婪算法得出此时第三个单词的最好结果是 “going”。</p>
<p>所以据贪婪算法最后的翻译结果可能是下图中的第二个句子，但第一句可能会更好(不服气的话，我们就假设第一句更好).</p>
<p><img src="/images/deeplearning/C5W3-5.png" width="700"></p>
<p>所以贪婪搜索的缺点是局部最优并不代表全局最优，就好像五黑，一队都是很牛逼的，但是各个都太优秀，就显得没那么优秀了，而另一队虽然说不是每个都是最优秀，但是凑在一起就是能 carry 全场。</p>
<p>更形象的理解可能就是 <code>Greedy Search</code> 更加短视，看的不长远，而且也更加耗时。假设字典中共有 10000 个单词，如果使用 <code>Greedy Search</code>，那么可能的组合有 1000010 种，所以还是挺恐怖的 2333~~</p>
<h2 id="3-Beam-Search"><a href="#3-Beam-Search" class="headerlink" title="3. Beam Search"></a>3. Beam Search</h2><p><strong>Beam Search</strong> 是 greedy search 的加强版本，首先要预设一个值 beam width，这里等于 <code>3</code> (如果等于 <strong>1</strong> 就是 <strong>greedy search</strong>)。然后在每一步保存最佳的 3 个结果进行下一步的选择，以此直到遇到句子的终结符.</p>
<h3 id="3-1-步骤一"><a href="#3-1-步骤一" class="headerlink" title="3.1 步骤一"></a>3.1 步骤一</h3><p>如下图示，因为beam width=3，所以根据输入的需要翻译的句子选出 3 个 $y^{<1>}$最可能的输出值，即选出$P(y^{<1>}|x)$最大的前3个值。假设分别是”in”,”jane”,”september”</1></1></p>
<p><img src="/images/deeplearning/C5W3-6_1.png" width="700"></p>
<h3 id="3-2-步骤二"><a href="#3-2-步骤二" class="headerlink" title="3.2 步骤二"></a>3.2 步骤二</h3><p>以”<strong>in</strong>“为例进行说明，其他同理.</p>
<p>如下图示，在给定被翻译句子 $x$ 和确定 $y^{<1>}$ = “<strong>in</strong>“ 的条件下，下一个输出值的条件概率是 $P(y^{<2>}|x,”in”)$。此时需要从 10000 种可能中找出条件概率最高的前 3 个.</2></1></p>
<p>又由公式 $P(y^{<1>},y^{<2>}|x)=P(y^{<1>}|x) P(y^{<2>}|x, y^{<1>})$, 我们此时已经得到了给定输入数据，前两个输出值的输出概率比较大的组合了.</1></2></1></2></1></p>
<p><img src="/images/deeplearning/C5W3-7_1.png" width="700"></p>
<p>另外 2 个单词也做同样的计算</p>
<p><img src="/images/deeplearning/C5W3-8_1.png" width="700"></p>
<p>此时我们得到了 9 组 $P(y^{<1>},y^{<2>}|x)$, 此时我们再从这 9组 中选出概率值最高的前 3 个。如下图示，假设是这3个：</2></1></p>
<ul>
<li>“in september”</li>
<li>“jane is”</li>
<li>“jane visits”</li>
</ul>
<p><img src="/images/deeplearning/C5W3-9_1.png" width="550"></p>
<h3 id="3-3-步骤三"><a href="#3-3-步骤三" class="headerlink" title="3.3 步骤三"></a>3.3 步骤三</h3><p>继续步骤2的过程，根据 $P(y^{<3>}|x,y^{<1>},y^{<2>})$ 选出 $P(y^{<1>},y^{<2>},y^{<3>}|x)$ 最大的前3个组合.</3></2></1></2></1></3></p>
<p>后面重复上述步骤得出结果.</p>
<h3 id="3-4-总结"><a href="#3-4-总结" class="headerlink" title="3.4 总结"></a>3.4 总结</h3><p>总结一下上面的步骤就是：</p>
<blockquote>
<ul>
<li><p>(1). 经过 encoder 以后，decoder 给出最有可能的三个开头词依次为 “in”, “jane”, “september”<br>$$P(y^{<1>}|x)$$</1></p>
</li>
<li><p>(2). 经过将第一步得到的值输入到第二步中，最有可能的三个翻译为 “in september”, “jane is”, “jane visits” </p>
</li>
</ul>
<p>$$P(y^{<2>}|x,y^{<1>})$$</1></2></p>
<p>(这里，september开头的句子由于概率没有其他的可能性大，已经失去了作为开头词资格)</p>
<ul>
<li>(3). 继续这个过程… </li>
</ul>
<p>$$P(y^{<3>}|x,y^{<1>},y^{<2>})$$</2></1></3></p>
</blockquote>
<p><img src="/images/deeplearning/C5W3-10_1.png" width="750"></p>
<h2 id="4-Refinements-to-beam-search"><a href="#4-Refinements-to-beam-search" class="headerlink" title="4. Refinements to beam search"></a>4. Refinements to beam search</h2><p>$$<br>P(y^{<1>},….,P(y^{T_y})|x)=P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})…P(y^{&lt;{T_y}&gt;}|x,y^{<1>},…y^{&lt;{T_y-1}&gt;})<br>$$</1></1></2></1></1></p>
<p>所以要满足 $argmax P(y^{<1>},….,P(y^{T_y})|x)$, 也就等同于要满足</1></p>
<p>$$<br>argmax \prod_{t=1}^{T_y}P(y^{&lt;{t}&gt;}|x,y^{<1>},…y^{&lt;{t-1}&gt;})<br>$$</1></p>
<p>但是上面的公式存在一个问题，因为概率都是小于1的，累乘之后会越来越小，可能小到计算机无法精确存储，所以可以将其转变成 log 形式（因为 log 是单调递增的，所以对最终结果不会有影响），其公式如下：</p>
<p>$$<br>argmax \sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{<1>},…y^{&lt;{t-1}&gt;})<br>$$</1></p>
<blockquote>
<p>But！！！上述公式仍然存在bug，观察可以知道，概率值都是小于1的，那么log之后都是负数，所以为了使得最后的值最大，那么只要保证翻译的句子越短，那么值就越大，所以如果使用这个公式，那么最后翻译的句子通常都是比较短的句子，这显然不行。</p>
</blockquote>
<p>所以我们可以通过归一化的方式来纠正，即保证平均到每个单词都能得到最大值。其公式如下：</p>
<p>$$<br>argmax \frac{1}{T_y}\sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{<1>},…y^{&lt;{t-1}&gt;})<br>$$</1></p>
<p>通过归一化的确能很好的解决上述问题，但是在实际运用中，会额外添加一个参数 $α$, 其大小介于 0 和 1 之间，公式如下:</p>
<p>$$<br>argmax \frac{1}{T_y^α}\sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{<1>},…y^{&lt;{t-1}&gt;})<br>$$</1></p>
<p><img src="/images/deeplearning/C5W3-11_1.png" width="700"></p>
<blockquote>
<p>$T_y$ 为输出句子中单词的个数，$α$ 是一个超参数 (可以设置为 0.7)</p>
<p>$α$ == 1. 则代表 完全用句子长度归一化<br>$α$ == 0. 则代表 没有归一化<br>$α$ == 0~1. 则代表 在 句子长度归一化 与 没有归一化 之间的折中程度.</p>
<p>beam width = B = 3~<strong>10</strong>~100 是会有一个明显的增长，但是 B 从 1000 ~ 3000 是并没有一个明显增长的.</p>
</blockquote>
<h2 id="5-Error-analysis-on-beam-search"><a href="#5-Error-analysis-on-beam-search" class="headerlink" title="5. Error analysis on beam search"></a>5. Error analysis on beam search</h2><p>仔细想想 <strong>beam search</strong>，我们会发现其实它是近似搜索，也就是说可能使用这种方法最终得到的结果并不是最好的。当然也有可能是因为使用的 <strong>RNN</strong> 模型有缺陷导致结果不是最好的。</p>
<p><strong>所以我们如何判断误差是出在哪个地方呢？</strong></p>
<blockquote>
<p>还是以翻译这句话为例：“<strong>简在9月访问中国</strong>”。</p>
<ul>
<li>假设按照人类的习惯翻译成英文是“Jane visits China in September.”,该结果用 $y^*$ 表示。</li>
<li>假设通过算法得出的翻译结果是：“Jane visited China in September.”,该结果用 $\hat{y}$ 表示。</li>
</ul>
<p>要判断误差出在哪，只需要比较 $P(y^*|x)$ 和 $P(\hat{y}|x)$ 的大小即可.</p>
</blockquote>
<p>下面分两种情况讨论：</p>
<p><img src="/images/deeplearning/C5W3-12_1.png" width="750"></p>
<blockquote>
<p>RNN 实际上是 encode 和 decode 的过程.</p>
</blockquote>
<p>两种情况：</p>
<p>(1). $<br>P(y^*|x)&gt;P(\hat{y}|x)<br>$</p>
<p>上面的不等式的含义是 beam search 最后选出的结果不如人类，也就是 beam search 并没有选出最好的结果，所以问题出在 beam search</p>
<p>(2). $<br>P(y^*|x)≤P(\hat{y}|x)<br>$</p>
<p>上面不等式表示 beam search 最后选出的结果要比人类的更好，也就是说 beam search 已经选出了最好的结果，但是模型对各个组合的预测概率值并不符合人类的预期，所以 RNN模型 at fault.</p>
<blockquote>
<p>上面已经介绍了误差分析的方式，但时仅凭一次误差分析就判定谁该背锅肯定也不行，所以还需要进行多次误差分析多次。</p>
<p>如下图示已经进行了多次的误差分析，每次分析之后都判定了锅该谁背，最后计算出beam search和模型背锅的比例，根据比例作出相应的调整。</p>
<p>例如:</p>
<ul>
<li>如果 beam search 更高，可以相应调整 beam width.</li>
<li>如果模型背锅比例更高，那么可以考虑增加正则化，增加数据等操作.</li>
</ul>
</blockquote>
<p><img src="/images/deeplearning/C5W3-13_1.png" width="750"></p>
<h2 id="6-Bleu-score-optional"><a href="#6-Bleu-score-optional" class="headerlink" title="6. Bleu score (optional)"></a>6. Bleu score (optional)</h2><p>主要介绍了如何给机器翻译结果打分，因为是选修内容, 所以 balabala…</p>
<h2 id="7-Attention-model-intuition"><a href="#7-Attention-model-intuition" class="headerlink" title="7. Attention model intuition"></a>7. Attention model intuition</h2><p>之前介绍的 RNN 翻译模型存在一个很明显的问题就是:</p>
<blockquote>
<p>机器翻译的翻译过程是首先将所有需要翻译的句子输入到 <strong>Encoder</strong> 中，之后再通过 <strong>Decoder</strong> 输出翻译语句.</p>
</blockquote>
<h3 id="7-1-Why-Attention-model"><a href="#7-1-Why-Attention-model" class="headerlink" title="7.1 Why Attention model"></a>7.1 Why Attention model</h3><p>如下图示机器算法将法语翻译成英语的模型.</p>
<p><img src="/images/deeplearning/C5W3-14_1.png" width="750"></p>
<p>机器翻译与人类的翻译过程不太相同。因为人类翻译一般是逐句翻译，或者是讲一段很长的句子分解开来进行翻译。</p>
<p>所以上述模型的翻译结果的 Bleu评分 与被翻译句子的长短有很大关系，句子较短时，模型可能无法捕捉到关键信息，所以翻译结果不是很高；但是当句子过长时，模型又抓不到重点等原因使得结果也不是很高。</p>
<p><img src="/images/deeplearning/C5W3-15_1.png" width="750"></p>
<blockquote>
<p>​见上图，如果机器能像人一样逐句或者每次将注意力只集中在一小部分进行翻译，那么翻译结果将不受句子长度的影响。下图中的绿色线即为使用了注意力模型后的翻译句子得分。</p>
</blockquote>
<h3 id="7-2-Attention-model-intro"><a href="#7-2-Attention-model-intro" class="headerlink" title="7.2 Attention model intro"></a>7.2 Attention model intro</h3><p>下图展示了普通的翻译模型双向 RNN 结构，该结构可根据输入 $x^{&lt;{t}&gt;}$ 直接得到输出 $y^{&lt;{t}&gt;}$.</p>
<p><img src="/images/deeplearning/C5W3-16_1.png" width="750"></p>
<p>Attention model 在此基础上做进一步处理。</p>
<p>为避免误解，使用另一个符号 $s$ 来表示节点。</p>
<p>如下图示，根据下面一层的 双向RNN 计算结果可得到节点 $s^{<1>}$ 与其他节点权重 $α^{<1,1>},α^{<1,2>},…$ 通过这些权重可以知道该节点与其他节点的相关联程度，从而可以达到将注意力集中到部分区域的效果。</1,2></1,1></1></p>
<p><img src="/images/deeplearning/C5W3-17_1.png" width="750"></p>
<p>​其他节点同理。整个注意力模型结构如下图示</p>
<p><img src="/images/deeplearning/C5W3-18_1.png" width="750"></p>
<h2 id="8-Attention-model"><a href="#8-Attention-model" class="headerlink" title="8. Attention model"></a>8. Attention model</h2><p>特别要区分 $a$ (字母a) 和 $α$ (alpha)。前者表示特征节点，后者表示注意力权重。</p>
<h3 id="8-1-参数介绍"><a href="#8-1-参数介绍" class="headerlink" title="8.1 参数介绍"></a>8.1 参数介绍</h3><p>如下图示，注意力模型采用双向 RNN 结构，所以每个节点有两个值，用 $\overrightarrow{a}^{&lt;{t’}&gt;},\overleftarrow{a}^{&lt;{t’}&gt;}$ 表示，为了使公式更简化，令 $a^{&lt;{t’}&gt;}=(\overrightarrow{a}^{&lt;{t’}&gt;},\overleftarrow{a}^{&lt;{t’}&gt;})$ 。其中 $t’$ 表示输入数据的索引。</p>
<p><img src="/images/deeplearning/C5W3-19_1.png" width="750"></p>
<p>上一节已经介绍了注意力权重 $α^{&lt;{t,t’}&gt;}$，以第一个节点为例，它的权重值可以用 $α^{&lt;{1,t’}&gt;}$ 表示，且所有权重值满足 $\sum{α^{&lt;{1,t’}&gt;}}=1$</p>
<p>所有权重与对应节点的线性之和用 $c^{&lt;{t’}&gt;}$ 表示（为方便书写，用 $c$ 表示）,$c$ 表示 context，即上下文变量.</p>
<p>还是以第一个节点为例，c 的计算公式如下：</p>
<p>$$<br>c^{<1>}=\sum_{t’}α^{&lt;{1,t’}&gt;}a^{&lt;{t’}&gt;}<br>$$</1></p>
<p><img src="/images/deeplearning/C5W3-20_1.png" width="750"></p>
<h3 id="8-2-注意力权值计算公式"><a href="#8-2-注意力权值计算公式" class="headerlink" title="8.2 注意力权值计算公式"></a>8.2 注意力权值计算公式</h3><p>$$<br>\alpha^{&lt;{t,t’}&gt;}=\frac{exp(e^{&lt;{t,t’}&gt;})}{\sum_{t’’=1}^{T_x}{exp(e^{t,t’’})}}<br>$$</p>
<p>上面公式中的 $e^{&lt;{t,t’}&gt;}$ 计算图如下：</p>
<p>其中 $s^{&lt;{t-1}&gt;}$ 表示上一个状态的值, $a^{&lt;{t’}&gt;}$ 表示第 $t’$ 个特征节点.</p>
<p><img src="/images/deeplearning/C5W3-21_1.png" width="500"></p>
<blockquote>
<p><strong>Andrew Ng</strong> 并没有详细的介绍上面的网络，只是一笔带过，说反向传播和梯度下降会自动学习，emmm。。那就这样吧。</p>
<p>结合下图可以独自参考一下上面的公式是什么意思.</p>
</blockquote>
<p><img src="/images/deeplearning/C5W3-22_1.png" width="600"></p>
<h3 id="8-3-大数据文摘"><a href="#8-3-大数据文摘" class="headerlink" title="8.3 大数据文摘"></a>8.3 大数据文摘</h3><p>下面的笔记是《大数据文摘》的笔记，感觉他写的清楚一些:</p>
<p>通过之前的学习可以看到机器翻译是将所有要翻译的内容统一输入然后再开始生成结果，但这样有一个弊端就是在句子特别长的时候后面的内容有的时候无法翻译的特别的准确。通过搭建 attention model 可以解决这个问题:</p>
<p><img src="/images/deeplearning/C5W3-23_1.png" width="800"></p>
<p>如图所示，这是一个 BRNN，并且在普通 RNN 的基础上增加 attention层，将阶段性的输入部分转化为输出，这样的方式也更符合人类的翻译过程。</p>
<p>让我们拿出细节部分仔细的理解一下，首先是 <strong>attention</strong> 层，也就是下图中 $context^{&lt;{t}&gt;}$ ，每一个 attention 单元接受 三个单词的输入所以也称作语境单元（context）， α 是每单个输入词在语境单元中占得权重。对每一个语境单元 t 来说，因为 α 是通过 softmax 决定的，所以 $\sum_{i=1}^{T_x}α^{t,i}=1$. 这里决定终每一个单词占得语境权重仍然是通过一 个小型的神经网络来进行计算并且后得到的。 </p>
<p><img src="/images/deeplearning/C5W3-24_1.png" width="800"></p>
<p>输出的 $context^{&lt;{t}&gt;}$ 进入到下一层 Post LSTM 这一步就和之前学习过的那样子，将前一步的输出与这一步经过重重分析的输入综合到一起产生这一步的输出。</p>
<p>让我们评估一下 attention model： 由于结构的复杂，计算量与时间比普通的语言模型要多和慢许多。不过对于机器翻译来说，由于每一句话并不会特别特比的长，所以有的时候稍微慢一点也不是完全无法接受.</p>
<blockquote>
<p>一个重要 attention model 应用就是语音识别，人通过麦克风输入一句话让机器来翻译输入的内容，来看一下是如何实现的</p>
</blockquote>
<h2 id="9-Speech-recognition"><a href="#9-Speech-recognition" class="headerlink" title="9. Speech recognition"></a>9. Speech recognition</h2><p>一般语音识别过程是如下图示的，即首先将原音频 (黑白的，纵轴表示振幅) 转化成纵轴为频率的音谱图，并且通过人工预先设定的音素(phonemes)再来识别.</p>
<p><img src="/images/deeplearning/C5W3-25_1.png" width="750"></p>
<p>当人对着麦克风录入一句话，麦克风记录下来的是空气细微的震动的强度，以及频率。人耳在听到一句话的时候其 实做的是类似的处理。在深度学习没有特别流行之前，比较流行的是用音节做语音识别，但现在因为有了强大的 <strong>attention model</strong>，得到的结果比音节的效果更好。</p>
<p><img src="/images/deeplearning/C5W3-26_1.png" width="750"></p>
<p>CTC(connectionist temporal classiﬁcation)是之前较为常用的方法。</p>
<p>具体原理如下：</p>
<p>假设每秒音频可以提取出 100 个特征值，那么假设 10秒 的音频就有 1000 个特征值，那么输出值也有 1000 个，但是说出的话并没有这么多啊，那该怎么处理呢？</p>
<p>方法很简单，只需要把“<em>”进行压缩即可，注意需要将 “</em>“和空额区分开来，因为空格也是占一个字符的。</p>
<p><img src="/images/deeplearning/C5W3-27_1.png" width="750"></p>
<h2 id="10-Trigger-word-detection"><a href="#10-Trigger-word-detection" class="headerlink" title="10. Trigger word detection"></a>10. Trigger word detection</h2><p><img src="/images/deeplearning/C5W3-28_1.png" width="700"></p>
<p>假设下图式训练集中的一段音频，其中包含了两次唤醒词:</p>
<p><img src="/images/deeplearning/C5W3-29_1.png" width="750"></p>
<p>搭建一个 attention model，在听到唤醒词之前一直输出的是 0，在听到唤醒词以后输出 1，但因为一个唤醒词会持续半秒左右所以我们也不仅仅只输出一次 1，而是将输出的 1 持续一段时间，通过这样的方式训练出的 RNN 就可以很 有效的检测到唤醒词了。</p>
<h2 id="11-Summary-and-thank-you"><a href="#11-Summary-and-thank-you" class="headerlink" title="11. Summary and thank you"></a>11. Summary and thank you</h2><p>终于学完了。虽然并不能说明什么~~~233333</p>
<p>感谢吴恩达和他的团队给我们带来这么好的教程</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
<li><a href="https://www.ctolib.com/Yukong-Deeplearning-ai-Solutions.html" target="_blank" rel="external">deeplearning.ai深度学习课程字幕翻译项目</a></li>
<li><a href="https://blog.csdn.net/Jerr__y/article/details/53749693" target="_blank" rel="external">seq2seq学习笔记</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Models (week2) - NLP - Word Embeddings]]></title>
      <url>http://sggo.me/2018/08/02/deeplearning/Sequence-Models-week2/</url>
      <content type="html"><![CDATA[<p>能够将序列模型应用到自然语言问题中，包括文字合成。</p>
<a id="more"></a>
<h2 id="1-Word-Representation"><a href="#1-Word-Representation" class="headerlink" title="1. Word Representation"></a>1. Word Representation</h2><p>上周的学习中，学习了如何用独热编码来代表一个词，这一节我们来探究一下词和词之间的联系。比如有下面这句话：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">“I want a glass of orange ________”</span><br></pre></td></tr></table></figure>
<p>假如我们的 RNN 的模型通过训练已经学会了短语 “orange juice”，并准确的预测了这句话的空格部分，那么如果遇到了另一句话时，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">“I want a glass of apple _________”</span><br></pre></td></tr></table></figure>
<p>是否需要从头学习短语 “apple juice” 呢？能否通过构建 “<code>apple</code>” 与 “<code>orange</code>” 的联系让它不需要重学就能进行判断呢？</p>
<blockquote>
<p>能否通过构建 “apple” 与 “orange” 的联系让它不需要重学就能进行判断呢？<br>所以下面给出了一种改进的表示方法，称之为“词嵌入(<strong>Word Embedding</strong>)”</p>
</blockquote>
<h3 id="1-1-词汇的特性"><a href="#1-1-词汇的特性" class="headerlink" title="1.1 词汇的特性"></a>1.1 词汇的特性</h3><p>单词与单词之间是有很多共性的，或在某一特性上相近，比如“苹果”和“橙子”都是水果；或者在某一特性上相反，比如“父亲”在性别上是男性，“母亲”在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率，这里用一个简化的表格说明:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Man (5391)</th>
<th style="text-align:center">Woman (9853)</th>
<th style="text-align:center">Apple (456)</th>
<th style="text-align:center">Orange (6257)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">性别</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.1</td>
</tr>
<tr>
<td style="text-align:center">年龄</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">-0.01</td>
<td style="text-align:center">0.00</td>
</tr>
<tr>
<td style="text-align:center">食物</td>
<td style="text-align:center">0.04</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.95</td>
<td style="text-align:center">0.97</td>
</tr>
<tr>
<td style="text-align:center">颜色</td>
<td style="text-align:center">0.03</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.70</td>
<td style="text-align:center">0.30</td>
</tr>
</tbody>
</table>
<p>在表格中可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。括号里的数字代表这个单词在独热编码中的位置，可以用这个数字代表这个单词比如 Man = ，Man 的特性用 ，也就是那一纵列。</p>
<p>在实际的应用中，特性的数量远不止 4 种，可能有几百种，甚至更多。对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。</p>
<blockquote>
<p>这里还介绍了一个 <code>t-SNE</code> 算法，因为词性表本身是一个很高维度的空间，通过这个算法压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个就是 “Word Embeddings” 的概念.</p>
</blockquote>
<p><img src="/images/deeplearning/C5W2-2.png" width="500"></p>
<blockquote>
<p>上图通过聚类将词性相类似的单词在二维空间聚为一类.</p>
</blockquote>
<h2 id="2-Using-Word-Embeddings"><a href="#2-Using-Word-Embeddings" class="headerlink" title="2. Using Word Embeddings"></a>2. Using Word Embeddings</h2><p>先下一个非正规定义 “词嵌 - 描述了词性特征的总量，也是在高维词性空间中嵌入的位置，拥有越多共性的词，词嵌离得越近，反之则越远”。值得注意的是，表达这个“位置”，需要使用所有设定的词性特征，假如有 300 个特征（性别，颜色，…），那么词嵌的空间维度就是 300.</p>
<h3 id="2-1-使用词嵌三步"><a href="#2-1-使用词嵌三步" class="headerlink" title="2.1 使用词嵌三步"></a>2.1 使用词嵌三步</h3><ol>
<li>获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库</li>
<li>应用词嵌：将获得的词嵌应用在我们的训练任务中</li>
<li>可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）</li>
</ol>
<h3 id="2-2-词嵌实用场景"><a href="#2-2-词嵌实用场景" class="headerlink" title="2.2 词嵌实用场景"></a>2.2 词嵌实用场景</h3><table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th style="text-align:center">sencentce</th>
<th style="text-align:center">replace word</th>
<th style="text-align:center">target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">Sally Johnson is an <code>orange</code> farmer.</td>
<td style="text-align:center">orange</td>
<td style="text-align:center">Sally Johnson</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">Robert Lin is an <code>apple</code> farmer.</td>
<td style="text-align:center">apple</td>
<td style="text-align:center">Robert Lin</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">Robert Lin is a <code>durian cultivator</code>.</td>
<td style="text-align:center">durian cultivator</td>
<td style="text-align:center">Robert Lin</td>
</tr>
</tbody>
</table>
<blockquote>
<p>我们继续替换，我们将 apple farmer 替换成不太常见的 durian cultivator (榴莲繁殖员)。此时词嵌入中可能并没有 durian 这个词，cultivator 也是不常用的词汇。这个时候怎么办呢？我们可以用到迁移学习。</p>
</blockquote>
<p><strong>词嵌入迁移学习步骤如下：</strong></p>
<blockquote>
<ol>
<li>学习含有大量文本语料库的词嵌入 (一般含有 10亿 到 1000亿 单词)，或者下载预训练好的词嵌入</li>
<li>将学到的词嵌入迁移到相对较小规模的训练集 (例如 10万 词汇).</li>
<li>(可选) 这一步骤就是对新的数据进行 fine-tune。</li>
</ol>
</blockquote>
<h2 id="3-Properties-of-Word-Embeddings"><a href="#3-Properties-of-Word-Embeddings" class="headerlink" title="3. Properties of Word Embeddings"></a>3. Properties of Word Embeddings</h2><p><strong>假设有如下的问题：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;Man&quot; -&gt; &quot;Woman&quot; 那么 &quot;King&quot; -&gt; ？</span><br></pre></td></tr></table></figure>
<p>这个问题被称作词汇的类比问题，通过研究词嵌的特征可以解决这样的问题.</p>
<p><img src="/images/deeplearning/C5W2-3_1.png" width="750"></p>
<p>数学的表达式为：</p>
<p>$$<br>e_{man} - e_{woman} \, \approx \, e_{king}-e_w<br>$$</p>
<p>$e_w$ 是什么呢？ 在高纬度空间中（300D）</p>
<p>$$<br>argmax_w \;\, Similarity(e_w, e_{king}-e_{man}+e_{woman})<br>$$</p>
<p>这个公式相当于在算两个向量(vector)的cos相似度</p>
<p>$$<br>Similarity(u,v) = \frac {u^Tv} {||u||_2||v||_2}<br>$$</p>
<blockquote>
<p>当然也可以用其他距离公式, 但是多数是用这个余弦相似度.</p>
</blockquote>
<p>如下图用几何方式能够更容易理解，即只要找到与向量 $\vec{AB}$ 最接近平行的向量 $\vec{CD}$ 即可.</p>
<p><img src="/images/deeplearning/C5W2-4_1.png" width="750"></p>
<h2 id="4-Embedding-Matrix"><a href="#4-Embedding-Matrix" class="headerlink" title="4. Embedding Matrix"></a>4. Embedding Matrix</h2><p>这一节中主要讲了词嵌矩阵的shape，如果词嵌（词性特征的总量）是300，独热编码的长度是10000，那么词嵌矩阵的的shape就是 <code>300 * 10000</code> 。所以就有了下面的式子：</p>
<blockquote>
<p>词嵌矩阵 * 单词的独热编码 = 单词的词嵌</p>
<p>(300, 10000) * (10000, 1) = (300, 1)</p>
</blockquote>
<h2 id="5-Learning-Word-Embeddings"><a href="#5-Learning-Word-Embeddings" class="headerlink" title="5. Learning Word Embeddings"></a>5. Learning Word Embeddings</h2><p>可以通过训练神经网络的方式构建词嵌表 <code>E</code> .</p>
<p>下图展示了预测单词的方法，即给出缺少一个单词的句子：</p>
<p>“<strong>I want a glass of orange ___</strong>”</p>
<blockquote>
<p>计算方法是将已知单词的特征向量都作为输入数据送到神经网络中去，然后经过一系列计算到达 Softmax分类层，在该例中输出节点数为10000个。经过计算juice概率最高，所以预测为</p>
<p>“I want a glass of orange <code>juice</code>”</p>
</blockquote>
<p><img src="/images/deeplearning/C5W2-5_1.png" width="750"></p>
<p>在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表 $E$</p>
<blockquote>
<p>假设要预测的单词为 $W$，词嵌表仍然为 $E$，需要注意的是训练词嵌表和预测 $W$ 是两个不同的任务。</p>
<p>如果任务是预测 $W$，最佳方案是使用 $W$ 前面 $n$ 个单词构建语境。</p>
<p>如果任务是训练 $E$，除了使用 $W$ 前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 Skip Gram 算法），这些方法都能提供很好的结果。</p>
</blockquote>
<h2 id="6-Word2Vec"><a href="#6-Word2Vec" class="headerlink" title="6. Word2Vec"></a>6. Word2Vec</h2><p>视频中一直没有给 Word2Vec 下一个明确的定义，我们再次下一个非正式定义便于理解:</p>
<p>“<strong>word2vec</strong>” 是指将词语 word 变成向量vector 的过程，这一过程通常通过浅层的神经网络完成，例如 CBOW 或者skip gram，这一过程同样可以视为构建词嵌表 $E$ 的过程”。</p>
<h3 id="6-1-Skip-grams"><a href="#6-1-Skip-grams" class="headerlink" title="6.1 Skip-grams"></a>6.1 Skip-grams</h3><p>这里着重介绍了<strong>skip gram model</strong>，这是一个用一个随机词预测其他词的方法。比如下面这句话中</p>
<blockquote>
<p>“I want a glass of orange juice.”</p>
</blockquote>
<p>我们可以选 <strong>orange</strong>作为随机词 c(<strong>Context</strong>)，通过设置窗口值例如前后 5 个单词以监督学习的方式去预测其中的词t(<strong>Target</strong>) 例如 “juice, glass, a, of” 但需要注意的是，这个过程仍然是为了搭建（更新）词嵌表 $E$ 而不是为了真正<br>的去预测，所以如果预测效果不好并不用担心，表达式：</p>
<p>$$<br>O_{c}\rightarrow E \rightarrow e_{c} \rightarrow \underset{Softmax}{Output} \rightarrow \hat{y}<br>$$</p>
<p><strong>Softmax</strong>公式为(假设输出节点数为10000)：</p>
<p>$$<br>p(t|c)=\frac{e^{θ_t^Te_c}}{\sum_{j=1}^{10000}e^{θ_j^Te_c}}<br>$$</p>
<blockquote>
<p>$θ_t$ 表示与$t$有关的参数</p>
</blockquote>
<p>损失函数：</p>
<p>$$<br>l(\hat{y},y)=\sum_{i=1}^{10000}y_ilog\hat{y_i}<br>$$</p>
<p><img src="/images/deeplearning/C5W2-6_1.png" width="750"></p>
<p>在skip gram中有一个不足是 <strong>softmax</strong> 作为激活函数需要的运算量太大，在上限为10000个单词的词库中就已经比较慢了。一种补救的办法是用一个它的变种 “<strong>Hierachical Softmax</strong> (分层的Softmax)”，通过类似二叉树的方法提高训练的效率</p>
<p>例如一些常见的单词，如<strong>the</strong>、<strong>of</strong>等就可以在很浅的层次得到，而像<strong>durian</strong>这种少用的单词则在较深的层次得到</p>
<p><img src="/images/deeplearning/C5W2-7_1.png" width="750"></p>
<h2 id="7-Negative-Sampling-负采样"><a href="#7-Negative-Sampling-负采样" class="headerlink" title="7. Negative Sampling 负采样"></a>7. Negative Sampling 负采样</h2><p>对于skip gram model而言，还要解决的一个问题是如何取样（选择）有效的随机词 $c$ 和目标词 $t$ 呢？如果真的按照自然随机分布的方式去选择，可能会大量重复的选择到出现次数频率很高的单词比如说 “the, of, a, it, I, …” 重复的训练这样的单词没有特别大的意义.</p>
<p>如何有效的去训练选定的词如 orange 呢？在设置训练集时可以通过“负取样”的方法, 下表中第一行是通过和上面一样的窗口法得到的“正”（1）结果，其他三行是从字典中随机得到的词语，结果为“负”（0）。通过这样的负取样法可以更有效地去训练 skip gram model.</p>
<table>
<thead>
<tr>
<th style="text-align:center">context</th>
<th style="text-align:center">word</th>
<th style="text-align:center">target?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">orange</td>
<td style="text-align:center">juice</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">orange</td>
<td style="text-align:center">king</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">orange</td>
<td style="text-align:center">book</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">orange</td>
<td style="text-align:center">the</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
<p><img src="/images/deeplearning/C5W2-8_1.png" width="700"></p>
<p>负取样的个数 <strong>k</strong> 由数据量的大小而定，上述例子中为3. 实际中数据量大则 k = 2 ~ 5，数据量小则可以相对大一些 k = 5 ~ 20</p>
<blockquote>
<p>通过负取样，我们的神经网络训练从 <strong>softmax</strong> 预测每个词出现的频率变成了经典 binary logistic regression 问题，概率公式用 <strong>sigmoid</strong> 代替 <strong>softmax</strong> 从而大大提高了速度.</p>
</blockquote>
<p>$$<br>x_1=(orange, juice) \rightarrow y_1=1 \\<br>x_2=(orange, king) \rightarrow y_2=0 \\<br>… \\<br>P(y=1|c,t)=\sigma(\theta_t^Te_c)<br>$$</p>
<p>最后我们通过一个并没有被理论验证但是实际效果很好的方式来确定每个被负选样选中的概率为：</p>
<p>$$<br>P(w_i)=\frac{f(w_i^{\frac{3}{4}})} {\sum_{j=1}^{10000}f(w_j^{\frac{3}{4}})}<br>$$</p>
<p><img src="/images/deeplearning/C5W2-9_1.png" width="750"></p>
<h2 id="8-GloVe-Word-Vectors"><a href="#8-GloVe-Word-Vectors" class="headerlink" title="8. GloVe Word Vectors"></a>8. GloVe Word Vectors</h2><p>GloVe(Global vectors for word representation)虽不像Word2Vec模型那样流行，但是它也有自身的优点，即简单.</p>
<h2 id="9-Sentiment-Classification"><a href="#9-Sentiment-Classification" class="headerlink" title="9. Sentiment Classification"></a>9. Sentiment Classification</h2><p>平时上淘宝我们都会对买的东西给出文字评价和对应的星级评价，如下图示。</p>
<p>商家可以通过对这些数据来构建一个情绪分类器，从而可以在一些社交平台上如微博、QQ等大家的文字评论然后对应输出相应的星级等级，这样就可以更容易知道自家店是蒸蒸日上还是日落西山了,hehehe。</p>
<p><img src="/images/deeplearning/C5W2-10_1.png" width="750"></p>
<p>可以看到下图中的模型先将评语中各个单词通过 词嵌表(数据量一般比较大，例如有100Billion的单词数) 转化成对应的特征向量，然后对所有的单词向量做求和或者做平均，然后构建Softmax分类器，最后输出星级评级。</p>
<p><img src="/images/deeplearning/C5W2-11_1.png" width="750"></p>
<p>但是上面的模型存在一个问题，一般而言如果评语中有像”good、excellent”这样的单词，一般都是星级评分较高的评语，但是该模型对下面这句评语就显得无能为力了：</p>
<p>“<strong>Completely lacking in good taste, good service, and good ambience</strong>.”</p>
<p>该评语中出现大量的good，如果直接做求和或者平均运算，经过分类器得到的输出很大概率上是高星级评分的，但这显然与该评语的本意不符.</p>
<p><img src="/images/deeplearning/C5W2-12_1.png" width="750"></p>
<p>之所以上面的模型存在那样的缺点，就是因为它没有把单词的时序考虑进去，所以我们可以使用RNN构建模型来解决这种问题。RNN模型如下图示：</p>
<p><img src="/images/deeplearning/C5W2-13_1.png" width="750"></p>
<p>另外使用RNN模型还有另一个好处，假设测试集中的评语是这样的</p>
<p>“Completely absent of good taste, good service, and good ambience.”</p>
<p>该评语只是将<strong>lacking in</strong>替换成了<strong>absent of</strong>，而且我们即使假设<strong>absent</strong>并没有出现在训练集中，但是因为词嵌表很庞大，所以词嵌表中包含<strong>absent</strong>，所以算法依旧可以知道<strong>absent</strong>和<strong>lacking</strong>有相似之处，最后输出的结果也依然可以保持正确。</p>
<p><img src="/images/deeplearning/C5W2-14_1.png" width="750"></p>
<h2 id="10-Debiasing-Word-Embeddings"><a href="#10-Debiasing-Word-Embeddings" class="headerlink" title="10. Debiasing Word Embeddings"></a>10. Debiasing Word Embeddings</h2><p>现如今机器学习已经被用到了很多领域，例如银行贷款决策，简历筛选。但是因为机器是向人们学习，所以好的坏的都会学到. 因为 <strong>RNN</strong> 通常是通过大量的网络数据文本集进行训练得到的，所以很多时候文本集中的偏见会反映在词嵌以及最终 的结果中，例如</p>
<blockquote>
<p>当说到Man：程序员的时候，算法得出Woman：家庭主妇，这显然存在偏见。</p>
<p>又如Man：Doctor，算法认为Woman：Nurse。这显然也存在其实和偏见。</p>
</blockquote>
<p>这种带有偏见的结果是应该尽力避免的，这类偏见大量存在于网络数据文本中，包括 性别偏见，种族偏见，年龄偏见，等等… 人类在这方面已经做的不对了，所以机器应当做出相应的调整来减少歧视.  </p>
<p><strong>给词嵌去偏见主要分三步</strong>(在词嵌的高维空间中完成):</p>
<ol>
<li>找到偏见的方向(确定偏见的x，y轴)</li>
<li>将非定义化的词平移到x=0(父亲，母亲这类词就是定义化的词，本身就带有了性别的暗示) </li>
<li>使定义化的词据离移动的词距离相等</li>
</ol>
<blockquote>
<p>So word embeddings can reflect the gender, ethnicity, age, sexual, orientation, and other biases of the text used to train the model. One that I’m especially passionate about is bias relating to socioeconomic status. I think that every person, whether you come from a wealthy family, or a low income family, or anywhere in between, I think everyone should have a great opportunities.</p>
</blockquote>
<p><img src="/images/deeplearning/C5W2-15_1.png" width="750"></p>
<p>下面将主要从性别歧视上来举例说明如何让机器学习消除偏见。</p>
<p>下图展示了一些单词，你可以在心里先想想你看到这些单词的第一时间认为他们所对应的性别是什么吧~~~</p>
<p><img src="/images/deeplearning/C5W2-16_1.png" width="450"></p>
<h3 id="1-识别偏见方向"><a href="#1-识别偏见方向" class="headerlink" title="1. 识别偏见方向"></a>1. 识别偏见方向</h3><p>因为该例子是以消除性别歧视为目的，所以我们需要计算出图中这些单词之间的距离的平均值，进而作为偏见方向(bias direction)</p>
<p>$$<br>e_{he}-e_{she} \\<br>e_{boy}-e_{girl} \\<br>e_{grandmother}-e_{grandfather}<br>$$</p>
<p>将上面所求做平均运算，得到的向量方向即为偏见方向</p>
<p>为方便理解，已在图中画出偏见方向，其余299D(除gender以外的其他单词特征)向量与偏见方向正交，也在下图中画出.</p>
<p><img src="/images/deeplearning/C5W2-17_1.png" width="700"></p>
<h3 id="2-词性中和"><a href="#2-词性中和" class="headerlink" title="2. 词性中和"></a>2. 词性中和</h3><p>像“ <strong>boy, girl</strong> ”这类词在性别词性上是很明确的，而且不存在歧视，所以无需中和(Neutralize).</p>
<p>而图中的 <strong>babysister、doctor</strong> 则需要中和，具体方法就是将该词像非偏见方向投影得到一个新的坐标.</p>
<p><img src="/images/deeplearning/C5W2-18_1.png" width="700"></p>
<h3 id="3-单词对等距离化"><a href="#3-单词对等距离化" class="headerlink" title="3. 单词对等距离化"></a>3. 单词对等距离化</h3><p>如下图示，虽然 <strong>babysister</strong> 中和化，但是它还是离 <strong>grandmother</strong> 更近，所以依旧带有偏见</p>
<p><img src="/images/deeplearning/C5W2-19_1.png" width="700"></p>
<p>所以我们还需要将grandmother、grandfather这类与性别有关的对应词等距分布在非偏见方向的两侧(红色剪头表示移动方向，红色点表示移动后的新坐标)，如下图示。</p>
<p><img src="/images/deeplearning/C5W2-20_1.png" width="700"></p>
<h2 id="11-Reference"><a href="#11-Reference" class="headerlink" title="11. Reference"></a>11. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
<li><a href="https://www.ctolib.com/Yukong-Deeplearning-ai-Solutions.html" target="_blank" rel="external">deeplearning.ai深度学习课程字幕翻译项目</a></li>
<li><a href="https://blog.csdn.net/Jerr__y/article/details/53749693" target="_blank" rel="external">seq2seq学习笔记</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Models (week1) - Recurrent Neural Networks]]></title>
      <url>http://sggo.me/2018/07/26/deeplearning/Sequence-Models-week1/</url>
      <content type="html"><![CDATA[<p>这次我们要学习专项课程中第五门课 <strong>Sequence Models</strong>.</p>
<p><strong>第一周:  Recurrent Neural Networks</strong> 已被证明在时间数据上表现好，它有几个变体，包括 LSTM、GRU 和双向神经网络.</p>
<a id="more"></a>
<h2 id="1-Why-sequence-models"><a href="#1-Why-sequence-models" class="headerlink" title="1. Why sequence models?"></a>1. Why sequence models?</h2><p>为什么要学习序列模型呢? 序列模型, 普遍称为 RNN (递归神经网络 - Recurrent Neural Network), 做为深度学习中非常重要的一环，有着比普通神经网络更广的宽度与更多的可能性，其应用领域包括但不限于“语音识别”， “NLP”， “DNA序列分析”，“Machine Translation”， “视频动作分析”，等等… 有这样一种说法，也许并不严谨，但有助于我们理解RNN，大意是这样的:</p>
<blockquote>
<p>普通神经网络处理的是一维的数据，CNN 处理的是二维的数据，RNN 处理的是三维的数据</p>
<p>最直观的理解是在 CNN 对图片的分析基础上，RNN 可以对视频进行分析，这里也就引入了第三维“时间”的概念</p>
</blockquote>
<p><img src="/images/deeplearning/C5W1-1_1.png" width="700"></p>
<p>这一小节通过一个小例子为我们打开序列模型的大门，例子如下:</p>
<blockquote>
<p>给出这样一个句子 “Harry Potter and Herminone Granger invented a new spell.”(哈利波特与赫敏格兰杰发明了 一个新的咒语。)， 我们的任务是在这个句子中准确的定位到人名 Harry Potter 和 Herminone Granger. 用深度学习的语言来描述如下图 - 每一个单词对应一个输出 0 或者 1，1 代表着是人名，0 代表不是。</p>
</blockquote>
<p><img src="/images/deeplearning/C5W1-2_2.png" width="750"></p>
<blockquote>
<p>接下来我们要解决的一个问题是<code>如何才能代表一个单词?</code>，比如我们例子中的 “Harry”，这里我们介绍一种新的编码方式， 就是用另一种方式来代表每一个单词 - 独热编码（<strong>One-Hot Encoding</strong>）。 具体流程是这样，假设我们有 10000 个常用词，为其构建一个10000*1 的矩阵(column matrix)，假如第一个词是苹果(apple), 那么对应的第一个位置为 1，其他都为 0，所以称之为独热。这样每个单词都有对应的矩阵进行表示，如果这个词没有出现在我们的字典中，那么我们可以给一个特殊的符号代替，常用的是 <unk> (unknown)</unk></p>
</blockquote>
<h2 id="2-Notation"><a href="#2-Notation" class="headerlink" title="2. Notation"></a>2. Notation</h2><p>为了后面方便说明，先将会用到的数学符号进行介绍. 以下图为例，假如我们需要定位一句话中人名出现的位置.</p>
<p><img src="/images/deeplearning/C5W1-2_1.png" width="750"></p>
<blockquote>
<ul>
<li>红色框中的为输入、输出值。可以看到人名输出用 1 表示，反之用 0 表示；</li>
<li><p>绿色框中的 $x^{&lt; t >}$,$y^{&lt; t >}$ 表示对应红色框中的输入输出值的数学表示，注意从 1 开始.</p>
</li>
<li><p>灰色框中的 $T_x,T_y$ 分别表示输入输出序列的长度，在该例中，$T_x=9,T_y=9$</p>
</li>
<li><p>黄色框中 $X^{(i)&lt; t >}$ 上的表示<strong>第 $i$ 个输入样本的第 $t$ 个输入值</strong>，$T_x^{ (i) }$ 则表示第 <strong>$i$</strong> 个输入样本的长度。输出 <strong>$y$</strong> 也同理.</p>
</li>
</ul>
</blockquote>
<p>输入值中每个单词使用 <strong>One-Hot</strong> 来表示。即首先会构建一个字典(Dictionary), 假设该例中的字典维度是 10000*1 (如图示)。第一个单词 “Harry” 的数学表示形式即为 [0,0,0,……,1 (在第4075位) ,0,……,0]，其他单词同理。</p>
<p>但是如果某一个单词并没有被包含在字典中怎么办呢？此时我们可以添加一个新的标记，也就是一个叫做 Unknown Word 的伪造单词，用 &lt;<strong>UNK</strong>&gt; 表示。具体的细节会在后面介绍。</p>
<p><img src="/images/deeplearning/C5W1-3_1.png" width="750"></p>
<h2 id="3-Recurrent-Neural-Network-Model"><a href="#3-Recurrent-Neural-Network-Model" class="headerlink" title="3. Recurrent Neural Network Model"></a>3. Recurrent Neural Network Model</h2><p>在介绍 RNN 之前，首先解释一下为什么之前的标准网络不再适用了。因为它有两个缺点：</p>
<ul>
<li>输入和输出的长度不尽相同</li>
<li>无法共享从其他位置学来的特征<blockquote>
<p>例如上一节中的 <strong>Harry</strong> 这个词是用 $x^{<1>}$ 表示的，网络从该位置学习了它是一个人名。但我们希望无论 <strong>Harry</strong> 在哪个位置出现网络都能识别出这是一个人名的一部分，而标准网络无法做到这一点.</1></p>
</blockquote>
</li>
</ul>
<p><img src="/images/deeplearning/C5W1-4_1.png" width="750"></p>
<blockquote>
<p>输入层，比如每个 $x^{<1>}$ 都是一个 1000 维的向量，这样输入层很庞大, 那么第一层的权重矩阵就有着巨大的参数.</1></p>
</blockquote>
<h3 id="3-1-RNN-结构"><a href="#3-1-RNN-结构" class="headerlink" title="3.1 RNN 结构"></a>3.1 RNN 结构</h3><p><img src="/images/deeplearning/C5W1-50_1.png" width="550"></p>
<p>还是以识别人名为例,第一个单词 $x^{<1>}$ 输入神经网络得到输出 $y^{<1>}$</1></1></p>
<p><img src="/images/deeplearning/C5W1-5_1.png" width="90"></p>
<p>同理, 由 $x^{<2>}$ 将得到 $y^{<2>}$,以此类推。但是这就是传统网络存在的问题，即单词之间没有联系</2></2></p>
<p><img src="/images/deeplearning/C5W1-6_1.png" width="90"></p>
<p>为了将单词之间关联起来，所以将前一层的结果也作为下一层的输入数据。如下图示</p>
<p><img src="/images/deeplearning/C5W1-7_1.png" width="250"></p>
<p>整体的 RNN 结构有两种表示形式，如下图示, 左边是完整的表达形式，注意第一层的 $a^{<0>}$ 一般设置为 0向量.</0></p>
<p><img src="/images/deeplearning/C5W1-8_1.png" width="750"></p>
<blockquote>
<p>要开始整个流程, 需要编造一个激活值, 这通常是 0向量, 有些研究人员会用其他方法随机初始化 $a^{<0>}=\vec{0}$. 不过使用 0向量，作为 0时刻 的伪激活值 是最常见的选择. 因此我们把它输入神经网络.</0></p>
<p>(右边的示意图是 RNN 的简写示意图)</p>
</blockquote>
<hr>
<p>介绍完结构之后，我们还需要知道网络中参数的表达方式及其含义。如下图示，$x^{&lt;{i}&gt;}$ 到网络的参数用 $W_{ax}$ 表示，$a^{&lt;{i}&gt;}$ 到网络的参数用 $W_{aa}$ 表示，$y^{&lt;{i}&gt;}$ 到网络的参数用 $W_{ya}$ 表示，具体含义将在下面进行说明.</p>
<p><img src="/images/deeplearning/C5W1-9_1.png" width="750"></p>
<blockquote>
<p>$x^{<1>}$ 通过网络可以传递到 $y^{<3>}$</3></1></p>
<p>但是这存在一个问题，即每个输出只与前面的输入有关，而与后面的无关。这个问题会在后续内容中进行改进. </p>
<p>举个🌰: He said, “Teddy Roosevelt was a great President.”</p>
<p>对于这句话，只知道 <strong>He said</strong> 前面两个词，来判断 Teddy 是否是人名是不够的，还需后面的信息.（BRNN 可处理这问题）</p>
</blockquote>
<h3 id="3-2-RNN-Forward-Propagation"><a href="#3-2-RNN-Forward-Propagation" class="headerlink" title="3.2 RNN Forward Propagation"></a>3.2 RNN Forward Propagation</h3><p>看 RNN Forward Propagation 之前，先看下基本的标准网络</p>
<p><img src="/images/deeplearning/C1W3-1_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W3-4_1.png" width="750"></p>
<p>RNN 在正向传播的过程中可以看到 <code>a</code> 的值随着时间的推移被传播了出去，也就一定程度上保存了单词之间的特性:</p>
<p><img src="/images/deeplearning/C5W1-10_1.png" width="750"></p>
<blockquote>
<p>$a^{<0>}=\vec{0}$</0></p>
<p>$a^{<1>}=g_1(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$</1></0></1></p>
<p>$y^{<1>}=g_2(W_{ya}a^{<1>}+b_y)$</1></1></p>
<p>$a^{&lt;{t}&gt;}=g_1(W_{aa}a^{&lt;{t-1}&gt;}+W_{ax}x^{&lt;{t}&gt;}+b_a)$</p>
<p>$y^{&lt;{t}&gt;}=g_2(W_{ya}a^{&lt;{t}&gt;}+b_y)$</p>
<p>激活函数：<strong>$g_1$</strong> 一般为 <strong><code>tanh</code>函数</strong> (或者是 <strong><code>Relu</code>函数</strong>)，<strong>$g_2$</strong> 一般是 <strong><code>Sigmod</code>函数</strong>.</p>
<p>注意: 参数的下标是有顺序含义的，如 $W_{ax}$ 下标的第一个参数表示要计算的量的类型，即要计算 $a$ 矢量，第二个参数表示要进行乘法运算的数据类型，即需要与 $x$ 矢量做运算。如 $W_{ax} x^{t}\rightarrow{a}$</p>
</blockquote>
<p><img src="/images/deeplearning/C5W1-19_1.png" width="750"></p>
<p><strong>Tx</strong> ， <strong>Ty</strong> 是时间单位, 这里统称为“时刻”，在这例子中对应不同时刻是输入的第几个单词， <strong>x</strong> 是“输入值”，例子中是当前时刻的单词（以独热编码的形式）， <strong>y</strong> 是“输出值” <strong>0</strong> 或者 <strong>1</strong>， <strong>a</strong> 称为激活值用于将前一个单元的输出结果传递到下一个单元， <strong>Wax</strong> <strong>Way</strong> <strong>Waa</strong> 是不同的“权重矩阵”也就是我们神经网络 update 的值。每一个单元有两个输入，$a^{&lt;{T_x-1}&gt;}$ 和 <strong>x</strong> ，有两个输出 $a^{&lt;{T_x}&gt;}$ 和 <strong>y</strong> . 图中没有出现的 g 是“激活函数”。</p>
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">名字</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$x$</td>
<td style="text-align:center">输入值</td>
</tr>
<tr>
<td style="text-align:center">$a$</td>
<td style="text-align:center">激活值</td>
</tr>
<tr>
<td style="text-align:center">$T_x$, $T_y$</td>
<td style="text-align:center">$x$,$y$ 时刻</td>
</tr>
<tr>
<td style="text-align:center">Wax, Way, Waa</td>
<td style="text-align:center">权重矩阵</td>
</tr>
</tbody>
</table>
<h3 id="3-3-Simplified-RNN-notation"><a href="#3-3-Simplified-RNN-notation" class="headerlink" title="3.3 Simplified RNN notation"></a>3.3 Simplified RNN notation</h3><p>下面将对如下公式进行化简：</p>
<p><img src="/images/deeplearning/C5W1-11_1.png" width="400"></p>
<p><strong>1. 简化 $a^{&lt;{t}&gt;}$</strong></p>
<p>$$<br>\begin{align}<br>a^{&lt;{t}&gt;}&amp;= g(W_{aa}a^{&lt;{t-1}&gt;}+W_{ax}x^{&lt;{t}&gt;}+b_a) \notag \\<br>&amp;= g(W_a [a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^{T}+b_a) \notag<br>\end{align}<br>$$</p>
<p><img src="/images/deeplearning/C5W1-12_1.png" width="750"></p>
<blockquote>
<p>注意，公式中使用了两个矩阵进行化简，分别是 $W_a$ 和 $[a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^T$ (使用转置符号更易理解),下面分别进行说明：</p>
</blockquote>
<p>$W_a = [ W_{aa}, W_{ax} ]$, 假设 $W_{aa}$ 是 (100,100) 的矩阵，$W_{ax}$ 是 (100,10000) 的矩阵,那么 $W$ 则是 (100,10100) 的矩阵.</p>
<p><img src="/images/deeplearning/C5W1-13_1.png" width="550"></p>
<p>$[a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^T$ 是下图示意:</p>
<p><img src="/images/deeplearning/C5W1-14_1.png" width="550"></p>
<p>故 $W_a [a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^{T}$ 矩阵计算如下图示:</p>
<p><img src="/images/deeplearning/C5W1-15_1.png" width="550"></p>
<p><strong>2. 简化 $y^{&lt;{t}&gt;}$</strong></p>
<p><img src="/images/deeplearning/C5W1-16_1.png" width="550"></p>
<p>该节PPT内容：</p>
<p><img src="/images/deeplearning/C5W1-17_1.png" width="750"></p>
<p>再回顾下干净的前向传播概览图:</p>
<p><img src="/images/deeplearning/C5W1-20_1.png" width="750"></p>
<h2 id="4-Backpropagation-through-time"><a href="#4-Backpropagation-through-time" class="headerlink" title="4. Backpropagation through time"></a>4. Backpropagation through time</h2><p>RNN 的反向传播通常都由类似 Tensorflow、Torch 之类的库或者框架帮你完成，不过感官上和普通神经网络类似，算梯度值然后更新权重矩阵.</p>
<p>但是下面这里依然会对<strong>反向传播</strong>进行详细的介绍，跟着下面一张一张的图片走起来 😄😄:</p>
<h3 id="4-1-整体感受"><a href="#4-1-整体感受" class="headerlink" title="4.1 整体感受"></a>4.1 整体感受</h3><p>首先再回顾一下 RNN 的整体结构:</p>
<p><img src="/images/deeplearning/C5W1-20_1.png" width="750"></p>
<p>要进行反向传播，首先需要前向传播，传播方向如蓝色箭头所示，其次再按照红色箭头进行反向传播</p>
<p><img src="/images/deeplearning/C5W1-18_1.png" width="750"></p>
<h3 id="4-2-前向传播"><a href="#4-2-前向传播" class="headerlink" title="4.2 前向传播"></a>4.2 前向传播</h3><p>首先给出所有输入数据，即从 $x^{<1>}$ 到 $x^{&lt;{T_x}&gt;}$, $T_x$ 表示输入数据的数量.</1></p>
<p><img src="/images/deeplearning/C5W1-21_1.png" width="650"></p>
<p>初始化参数 $W_a$, $b_a$，将输入数据输入网络得到对应的 $a^{&lt;{t}&gt;}$</p>
<p><img src="/images/deeplearning/C5W1-22_1.png" width="650"></p>
<p>再通过与初始化参数 $W_y$, $b_y$ 得到 $y^{&lt;{t}&gt;}$</p>
<p><img src="/images/deeplearning/C5W1-23_1.png" width="650"></p>
<h3 id="4-3-损失函数定义"><a href="#4-3-损失函数定义" class="headerlink" title="4.3 损失函数定义"></a>4.3 损失函数定义</h3><p>要进行反向传播，必须得有损失函数嘛，所以我们将损失函数定义如下：</p>
<p><strong>每个节点的损失函数:</strong></p>
<p>$$<br>L^{&lt;{t}&gt;}(\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;})=y^{&lt;{t}&gt;}log(y^{&lt;{t}&gt;})-(1-y^{&lt;{t}&gt;})log(1-\hat{y}^{&lt;{t}&gt;})<br>$$</p>
<p><strong>整个网络的损失函数:</strong></p>
<p>$$<br>L(\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;)}) = \sum_{t=1}^{T_y}L^{&lt;{t}&gt;}(\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;})<br>$$</p>
<p><img src="/images/deeplearning/C5W1-24_1.png" width="750"></p>
<h3 id="4-4-反向传播"><a href="#4-4-反向传播" class="headerlink" title="4.4 反向传播"></a>4.4 反向传播</h3><p><img src="/images/deeplearning/C5W1-25_1.png" width="750"></p>
<h3 id="4-5-整个流程图"><a href="#4-5-整个流程图" class="headerlink" title="4.5 整个流程图"></a>4.5 整个流程图</h3><p><img src="/images/deeplearning/C5W1-26_1.png" width="750"></p>
<h2 id="5-Different-types-of-RNNs"><a href="#5-Different-types-of-RNNs" class="headerlink" title="5. Different types of RNNs"></a>5. Different types of RNNs</h2><p><strong>RNN 的不同应用领域:</strong></p>
<p>序列模型对输入与输出的长度没有要求，在常见的例子中，机器翻译就是多个输入与多个输出，简称“多对多”， 语音识别可视为“单对多”， 它的反例是音乐生成-“多对单”。课程中介绍了多种可能的 RNN 模式，我们用下面一张图概括：</p>
<p><img src="/images/deeplearning/C5W1-27_1.png" width="800"></p>
<p>RNN 不同的结构给了我们更多的可能性.</p>
<h2 id="6-Language-model-and-sequence-generation"><a href="#6-Language-model-and-sequence-generation" class="headerlink" title="6. Language model and sequence generation"></a>6. Language model and sequence generation</h2><p>语言模型和序列生成</p>
<h3 id="6-1-什么是语言模型"><a href="#6-1-什么是语言模型" class="headerlink" title="6.1 什么是语言模型"></a>6.1 什么是语言模型</h3><p>凡事开头举个🌰，一切都好说：</p>
<p>假设一个语音识别系统听一句话得到了如下两种选择，作为正常人肯定会选择第二种。但是机器才如何做判断呢？</p>
<p><img src="/images/deeplearning/C5W1-28_1.png" width="600"></p>
<p>此时就需要通过语言模型来预测每句话的概率：</p>
<p><img src="/images/deeplearning/C5W1-29_1.png" width="600"></p>
<h3 id="6-2-如何使用-RNN-构建语言模型"><a href="#6-2-如何使用-RNN-构建语言模型" class="headerlink" title="6.2 如何使用 RNN 构建语言模型"></a>6.2 如何使用 RNN 构建语言模型</h3><ol>
<li>首先我们需要一个很大的语料库 (<strong>Corpus</strong>)</li>
<li>将每个单词字符化 (<strong>Tokenize</strong>，<strong>即使用One-shot编码</strong>) 得到词典,，假设有 10000 个单词</li>
<li>还需要添加两个特殊的单词<blockquote>
<ul>
<li>end of sentence. 终止符，表示句子结束.<br><img src="/images/deeplearning/C5W1-30_1.png" width="600"></li>
<li>UNknown, 之前的笔记已介绍过<br><img src="/images/deeplearning/C5W1-31_1.png" width="600"></li>
</ul>
</blockquote>
</li>
</ol>
<h3 id="6-3-构建语言模型示例"><a href="#6-3-构建语言模型示例" class="headerlink" title="6.3 构建语言模型示例"></a>6.3 构建语言模型示例</h3><p>假设要对这句话进行建模：<strong>Cats average 15 hours of sleep a day. <eos></eos></strong></p>
<p><strong>1. 初始化</strong></p>
<blockquote>
<p>这一步比较特殊，即 $x^{<1>}$ 和 $a^{<0>}$ 都需要初始化为 $\vec{0}$ .<br>此时 $\hat{y}^{<1>}$ 将会对第一个字可能出现的每一个可能进行概率的判断,即 $\hat{y}^{<1>}=[p(a),…,p(cats),…]$.</1></1></0></1></p>
<p>当然在最开始的时候没有任何的依据，可能得到的是完全不相干的字，因为只是根据初始的值和激活函数做出的取样。</p>
<p><img src="/images/deeplearning/C5W1-32_1.png" width="500"></p>
</blockquote>
<p><strong>2. 将真实值作为输入值:</strong></p>
<blockquote>
<p>之所以将真实值作为输入值很好理解，如果我们一直传错误的值，将永远也无法得到字与字之间的关系</p>
</blockquote>
<p>如下图示，将 $y^{<1>}$ 所表示的真实值 Cats 作为输入，即 $x^{<2>}=y^{<1>}$ 得到 $\hat{y}^{<2>}$</2></1></2></1></p>
<p>此时的 $\hat{y}^{<2>}=[p(a|cats),…,p(average|cats),…]$</2></p>
<p>同理有 $\hat{y}^{<3>}=[p(a|cats\, average),…,p(average|cats\,average),…]$</3></p>
<p>另外输入值满足： $x^{&lt;{t}&gt;}=y^{&lt;{t-1}&gt;}$</p>
<p><img src="/images/deeplearning/C5W1-33_1.png" width="600"></p>
<p><strong>3. 计算出损失值:</strong></p>
<p>下图给出了构建模型的过程以及损失值计算公式:</p>
<p><img src="/images/deeplearning/C5W1-34_1.png" width="700"></p>
<blockquote>
<p>随着训练的次数的增多，或者常用词出现的频率的增多，语言模型便慢慢的会开始掌握简单的词语比如“平均”，“每天”，“小时”。一个完善的语言模型看到类似“ 10 个小”的时候，应该就能准确的判定下一个字是“时”。</p>
<p>（当然也许实际情况是“ 10 个小朋友”，所以通常会有更多的判断因素，这里只是一个例子）</p>
</blockquote>
<h2 id="7-Sampling-novel-sequences"><a href="#7-Sampling-novel-sequences" class="headerlink" title="7. Sampling novel sequences"></a>7. Sampling novel sequences</h2><p>当训练得到了一个模型之后，如想知道这个<strong>模型学到了些什么</strong>，一个非正式的方法就是对新序列进行采样。具体方法如下：</p>
<p>在每一步输出 $\hat{y}$ 时，通常使用 softmax 作为激活函数，然后根据输出的分布，随机选择一个值，也就是对应的一个字 或 英文单词。 然后将这个值作为下一个单元的 $x$ 输入进去 (即 $x^{&lt;{t}&gt;}=\hat{y}^{&lt;{t-1}&gt;}$, 直到我们输出了终结符，或者输出长度超过了提前的预设值 n 才停止采样.</p>
<p>上述步骤具体如图示：</p>
<p><img src="/images/deeplearning/C5W1-35_1.png" width="700"></p>
<p>下图给出了采样之后得到的效果：</p>
<blockquote>
<ul>
<li>左边是对训练得到新闻信息模型进行采样得到的内容；</li>
<li>右边是莎士比亚模型采样得到的内容.</li>
</ul>
</blockquote>
<p><img src="/images/deeplearning/C5W1-36_1.png" width="750"></p>
<h2 id="8-Vanishing-gradients-with-RNNs"><a href="#8-Vanishing-gradients-with-RNNs" class="headerlink" title="8. Vanishing gradients with RNNs"></a>8. Vanishing gradients with RNNs</h2><blockquote>
<p>现在你已经学会了 基本的 RNN 如何应用在 比如 语言模型 还有 如何用反向传播来训练你的 RNN 模型, 但是还有一个问题就是 梯度消失 与 梯度爆炸 问题.</p>
<p>目前这种基本的 RNN 也不擅长捕获这种长期依赖效应. </p>
<p>梯度爆炸可以用梯度消减解决、梯度消失就有点麻烦了，需要用 GRU 来解决.</p>
</blockquote>
<p><strong>RNN 的梯度消失、爆炸问题:</strong></p>
<p>梯度值在 RNN 中也可能因为反向传播的层次太多导致过小 或 过大</p>
<blockquote>
<ul>
<li>当梯度值过小的时候，神经网络将无法有效地调整自己的权重矩阵导致训练效果不佳，称之为<strong>“梯度消失问题”(gradient vanishing problem)</strong>；</li>
<li>过大时可能直接影响到程序的运作因为程序已经无法存储那么大的值，直接返回 NaN ，称之为<strong>“梯度爆炸问题”(gradient exploding problem)</strong></li>
</ul>
</blockquote>
<p>当梯度值过大的时候有一个比较简便的解决方法，每次将返回的梯度值进行检查，如果超出了预定的范围，则手动设置为范围的边界值.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (gradient &gt; max) &#123;</span><br><span class="line">    gradient = max</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但梯度值过小的解决方案要稍微复杂一点，比如下面两句话：</p>
<blockquote>
<p>“The <strong>cat</strong>，which already ate apple，yogurt，banana，…, <strong>was</strong> full.”<br>“The <strong>cats</strong>，which already ate apple，yogurt，banana，…, <strong>were</strong> full.”</p>
</blockquote>
<p>重点标出的 <strong>cat(s)</strong> 和 be 动词（<strong>was, were</strong>） 是有很重要的关联的，但是中间隔了一个 which 引导的定语从句，对于前面所介绍的基础的 RNN网络 很难学习到这个信息，尤其是当出现梯度消失时，而且这种情况很容易发生.</p>
<p>我们知道一旦神经网络层次很多时，反向传播很难影响前面层次的参数。所以为了 <strong>解决梯度消失</strong> 问题，提出了 <strong>GRU</strong>单元，下面一节具体介绍.</p>
<blockquote>
<p>将在接下来的两个章节介绍两种方法来解决 <strong>梯度过小</strong> 问题，目标是当一些重要的单词离得很远的时候，比如例子中的 “<strong>cat</strong>” 和 “<strong>was</strong>”，能让语言模型准确的输出单数人称过去时的 “<strong>was</strong>”，而不是 “<strong>is</strong>” 或者 “<strong>were</strong>”. 两个方法都将引入“记忆”的概念，也就是为 RNN 赋予一个记忆的功能.</p>
</blockquote>
<h2 id="9-GRU-Gated-Recurrent-Unit"><a href="#9-GRU-Gated-Recurrent-Unit" class="headerlink" title="9. GRU - Gated Recurrent Unit"></a>9. GRU - Gated Recurrent Unit</h2><p>GRU（Gated Recurrent Unit）是一种用来解决梯度值过小的方法，首先来看下在一个时刻下的 RNN单元，激活函数为 tanh</p>
<h3 id="9-1-回顾普通-RNN单元-的结构"><a href="#9-1-回顾普通-RNN单元-的结构" class="headerlink" title="9.1 回顾普通 RNN单元 的结构"></a>9.1 回顾普通 RNN单元 的结构</h3><p>如图示，输入数据为 $a^{&lt;{t-1}&gt;}$ 和 $x^{&lt;{t}&gt;}$, 与参数 $W_a$ 进行线性运算后再使用 $tanh$ 函数 转化得到 $a^{&lt;{t}&gt;}$. 当然再使用 softmax 函数处理可以得到预测值.</p>
<p><img src="/images/deeplearning/C5W1-37_1.png" width="750"></p>
<h3 id="9-2-GRU结构"><a href="#9-2-GRU结构" class="headerlink" title="9.2 GRU结构"></a>9.2 GRU结构</h3><p><strong>记忆细胞:</strong></p>
<p>在 GRU中 会用到 “记忆细胞(Memory cell)” 这个概念, 我们用变量<code>C</code>表示。这个记忆细胞提供了记忆功能，例如它能够帮助记住 cat 对应 was, cats 对应 were.</p>
<p>而在 $t$ 时刻，记忆细胞所包含的值其实就是 Activation function 值，即 $c^{&lt;{t}&gt;}=a^{&lt;{t}&gt;}$</p>
<blockquote>
<p>注意：在这里两个变量的值虽然一样，但是含义不同。</p>
<p>另外在下节将介绍的 LSTM 中，二者值的大小有可能是不一样的，所以有必要使用这两种变量进行区分</p>
</blockquote>
<p>为了更新记忆细胞的值，我们引入 $\tilde{c}$ 来作为候选值从而来更新 $c^{&lt;{t}&gt;}$，其公式为：</p>
<p>$$<br>\tilde{c}=tanh(W_c [c^{&lt;{t-1}&gt;}, x^{&lt;{t}&gt;}]+b_c)<br>$$</p>
<p><strong>更新门 (update gate):</strong></p>
<p>更新门是 GRU 的核心概念，它的作用是用于判断是否需要进行更新.</p>
<p>更新门用 $\Gamma_u$ 表示，其公式为：</p>
<p>$$<br>\Gamma_u=σ(W_u [c^{&lt;{t-1}&gt;}, x^{&lt;{t}&gt;}]+b_u)<br>$$</p>
<p><img src="/images/deeplearning/C5W1-38_1.png" width="550"></p>
<p>如上图示，$\Gamma_u$ 值的大小大多分布在 0 或者 1，所以可以将其值的大小粗略的视为 0 或者 1。这就是为什么我们就可以将其理解为一扇门，如果 $\Gamma_u=1$ , 就表示此时需要更新值，反之不用.</p>
<p><strong>$t$ 时刻记忆细胞:</strong></p>
<p>有了更新门公式后，我们则可以给出 $t$ 时刻 <code>Memory cell</code> 的值的计算公式了:</p>
<p>$$<br>c^{&lt;{t}&gt;} =  \Gamma_u * \tilde{c} + (1-\Gamma_u) * c^{&lt;{t-1}&gt;}<br>$$</p>
<blockquote>
<p>注意：上面公式中的 * 表示元素之间进行乘法运算，而其他公式是 矩阵运算</p>
</blockquote>
<p>公式很好理解，如果 $\Gamma_u=1$，那么 $t$ 时刻 记忆细胞的值就等于候选值 $\tilde{c}$, 反之等于前一时刻记忆细胞的值.</p>
<p>下图给出了该公式很直观的解释：</p>
<p>在读到 “cat” 的时候 ，其他时候一直为 0，知道要 输出 “was” 的时刻我们仍然知道 “cat” 的存在，也就知道它为单数了</p>
<p><img src="/images/deeplearning/C5W1-39_1.png" width="550"></p>
<p><strong>GRU 结构示意图</strong></p>
<p><img src="/images/deeplearning/C5W1-40_1.png" width="550"></p>
<h3 id="9-3-完整版-GRU"><a href="#9-3-完整版-GRU" class="headerlink" title="9.3 完整版 GRU"></a>9.3 完整版 GRU</h3><p>上简化了的 GRU，在完整版中还存在另一个符号 ，这符号的意义是控制 $\tilde{c}$ 和 $c^{&lt;{t-1}&gt;}$ 之间的联系强弱，完整版公式如下：</p>
<p><img src="/images/deeplearning/C5W1-41_1.png" width="550"></p>
<blockquote>
<p>注意，完整公式中多出了一个 $\Gamma_r$, 这个符号的作用是控制 $\tilde{c}^{&lt;{t}&gt;}$ 和 $c^{&lt;{t}&gt;}$ 之间联系的强弱.</p>
</blockquote>
<h2 id="10-LSTM（long-short-term-memory）unit"><a href="#10-LSTM（long-short-term-memory）unit" class="headerlink" title="10. LSTM（long short term memory）unit"></a>10. LSTM（long short term memory）unit</h2><p>介绍完 GRU 后，再介绍 LSTM 会更加容易理解。下图是二者公式对比：</p>
<p>GRU 只有两个门，而 LSTM 有三个门，分别是更新门 $\Gamma_u$ (是否需要更新为 $\tilde{c}^{&lt;{t}&gt;}$，遗忘门 $\Gamma_f$ (是否需要丢弃上一个时刻的值)，输出门 $\Gamma_o$ (是否需要输出本时刻的值)</p>
<p><img src="/images/deeplearning/C5W1-42_1.png" width="650"></p>
<p><img src="/images/deeplearning/C5W1-43_1.png" width="650"></p>
<p>虽然 LSTM 比 GRU 更复杂，但是它比 GRU 更早提出哇😄。另外一般而言 LSTM 的表现要更好，但是计算量更大，毕竟多了一个门嘛。而 GRU 实际上是对 LSTM 的简化，它的表现也不错，能够更好地扩展到深层网络。所以二者各有优势。</p>
<p>下图是 LSTM 的结构示意图：</p>
<p><img src="/images/deeplearning/C5W1-44_1.png" width="700"></p>
<h2 id="11-Bidirectional-RNN"><a href="#11-Bidirectional-RNN" class="headerlink" title="11. Bidirectional RNN"></a>11. Bidirectional RNN</h2><p>前面介绍的都是单向的 RNN 结构，在处理某些问题上得到的效果不尽人意</p>
<p>如下面两句话，我们要从中标出人名：</p>
<blockquote>
<p><code>He</code> said, “Teddy Roosevelt was a great President”.<br><code>He</code> said, “Teddy bears are on sale”.</p>
</blockquote>
<ol>
<li>第一句中的 Teddy Roosevelt 是人名</li>
<li>第二句中的 Teddy bears 是泰迪熊，同样都是单词 <strong>Teddy</strong> 对应的输出在第一句中应该是 1，第二句中应该是 0</li>
</ol>
<p>像这样的例子如果想让我们的序列模型明白就需要借助不同的结构比如 - 双向递归神经网络(Bidirectional RNN).<br>该神经网络首先从正面理解一遍这句话，再从反方向理解一遍.</p>
<p>双向递归神经网络结构如下：</p>
<p><img src="/images/deeplearning/C5W1-45_1.png" width="750"></p>
<p>下图摘自大数据文摘整理</p>
<p><img src="/images/deeplearning/C5W1-46_1.png" width="750"></p>
<h2 id="12-Deep-RNNs"><a href="#12-Deep-RNNs" class="headerlink" title="12. Deep RNNs"></a>12. Deep RNNs</h2><p>深层，顾名思义就是层次增加。如下图是深层循环神经网络的示意图</p>
<p>横向表示时间展开，纵向则是层次展开。</p>
<p><img src="/images/deeplearning/C5W1-47_1.png" width="750"></p>
<p>注意激活值的表达形式有所改变，以 $a^{[1]<0>}$ 为例进行解释：</0></p>
<ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">1</a> 表示第一层</li>
<li><0> 表示第一个激活值</0></li>
</ul>
<p>另外各个激活值的计算公式也略有不同，以 $a^{[2]<3>}$ 为例，其计算公式如下：</3></p>
<p><img src="/images/deeplearning/C5W1-48_1.png" width="550"></p>
<h2 id="13-Reference"><a href="#13-Reference" class="headerlink" title="13. Reference"></a>13. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/tag/DeepLearning/" target="_blank" rel="external">DeepLearning.ai 学习笔记汇总</a></li>
<li><a href="https://github.com/theBigDataDigest/Andrew-Ng-deeplearning-part-5-Course-notes-in-Chinese/blob/master/Andrew-Ng-deeplearning.ai-part-5-Course%20notes.pdf" target="_blank" rel="external">大数据文摘 DeepLearning.ai 学习笔记</a></li>
<li><a href="https://kulbear.github.io/pdf/sequence-models.pdf" target="_blank" rel="external">Sequence Models 英文版笔记</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Structured Machine Learning Projects (week2) - ML Strategy 2]]></title>
      <url>http://sggo.me/2018/07/25/deeplearning/Structured-Machine-Learning-Projects-week2/</url>
      <content type="html"><![CDATA[<p>如何进行 误差分析、标注错误数据、定位数据不匹配偏差与方差</p>
<p>知道如何应用端到端学习、迁移学习以及多任务学习</p>
<a id="more"></a>
<h2 id="1-Carrying-out-error-analysis"><a href="#1-Carrying-out-error-analysis" class="headerlink" title="1. Carrying out error analysis"></a>1. Carrying out error analysis</h2><blockquote>
<p>很多时候我们发现训练出来的模型有误差后，就会一股脑的想着法子去减少误差。想法固然好，但是有点 headlong 。。</p>
<p>这节视频中 Andrew Ng 介绍了一个比较科学的方法，具体的看下面的例子</p>
</blockquote>
<p>还是以猫分类器为例，假设我们的模型表现的还不错，但是依旧存在误差，预测后错误标记的数据中有一部分狗图片被错误的标记成了猫。这个时候按照一般的思路可能是想通过训练出狗分类器模型来提高猫分类器，或者其他的办法，反正就是要让分类器更好地区分狗和猫。</p>
<p>但是现在的问题是，假如错误分类的100个样本中，只有5个狗样本被错误的标记成了猫，那么你费尽千辛万苦也最多只能提高一丢丢的准确度。所以对误差进行分析就显得比较重要，而且可以帮助我们在未来的工作中指明优化方向，节省时间。具体的方法按吴大大的说法是可以人工的对错误标记的样本进行再处理、分析。</p>
<p>下面以一个例子来介绍一下操作步骤</p>
<p><strong>1. 人工标记</strong></p>
<blockquote>
<p>将错误标记样本以表格的形式列举出来，然后人工的标记处样本的分类，统计各分类(或者说错误标记的原因)所占比例.</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">Image</th>
<th style="text-align:center">Dog</th>
<th style="text-align:center">Great cats(大型猫科动物，如狮子)</th>
<th style="text-align:center">Blurry(图片模糊)</th>
<th style="text-align:center">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center">眯着眼</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">在动物园，且下着雨</td>
</tr>
<tr>
<td style="text-align:center">……</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">% of total</td>
<td style="text-align:center">8%</td>
<td style="text-align:center">43%</td>
<td style="text-align:center">61%</td>
<td style="text-align:center">-</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注意:上面的分类并不是互相独立的，只是举个例子。。。抄下 Andrew Ng 的PPT</p>
</blockquote>
<p><strong>2. 分析误差</strong></p>
<p>上面的结果可以知道，误差样本中只有8%是狗狗的图片，而43%是大型猫科动物，61%是因为图片模糊。很显然此时你即使用毕生所学去优化区别狗和猫的算法，整个模型的准确率提升的空间也远不如后两个特征高。所以如果人手够的话，也是可以选择几个特征进行优化的。</p>
<h2 id="2-Cleaning-up-Incorrectly-labeled-data"><a href="#2-Cleaning-up-Incorrectly-labeled-data" class="headerlink" title="2. Cleaning up Incorrectly labeled data"></a>2. Cleaning up Incorrectly labeled data</h2><p>机器预测可能会出错，那么人当然也有可能会出错。所以如果训练集和验证集中认为添加的标签Y出现误差该怎么处理呢？</p>
<p><img src="/images/deeplearning/C3W2-2_1.png" width="700"></p>
<p>这里分两种情况：</p>
<p><strong>1.随机误差</strong></p>
<p>这种情况比较好，因为如果人为误差比较接近随机误差，那么可以睁一只眼闭一只眼，因为深度学习算法对于随机误差还是有一定的健壮性的</p>
<p><strong>2.非随机误差</strong></p>
<p>PS:不知道有没有非随机误差这个词。。我只是为了行文方便取的一个名字。</p>
<p>对于<strong>随机误差</strong>正常人可能都会问“what？我怎么知道是不是接近随机误差”，所以视频里 Andrew Ng 也给咱们提供了一个方法，这个方法和上一节中的表格法一样一样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Image</th>
<th style="text-align:center">Dog</th>
<th style="text-align:center">Great cats(大型猫科动物，如狮子)</th>
<th style="text-align:center">Blurry(图片模糊)</th>
<th style="text-align:center">Incorrectly labeled</th>
<th style="text-align:center">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center">只是一只手画的的猫，不是真的猫</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center">背景的角落里有一只猫</td>
</tr>
<tr>
<td style="text-align:center">……</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">% of total</td>
<td style="text-align:center">8%</td>
<td style="text-align:center">43%</td>
<td style="text-align:center">61%</td>
<td style="text-align:center">6%</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<p>有了上面这个表格，那么问题来了，此时我还需要修正这6%标记错误的样本吗？还是举个例子：</p>
<p>假设我们有如下数据：</p>
<blockquote>
<ul>
<li>总体验证集误差：10%</li>
<li>由人工错误标记引起的错误样本比例： 0.6%</li>
<li>由其他原因引起的错误样本比例：10%-0.6%=9.4%</li>
</ul>
<p>所以这种情况下我们应该集中精力找出引起9.4%误差的原因，并进行修正，当然如果有余力也还是可以休整一下人工错误标记的数据的。</p>
<p>假如你通过优化算法，减少了因其他原因引起的误差，并且使得总体验证集误差降到了2%，此时我们再分析一下：</p>
<p>很显然，因为并没有对人工误差进行优化，<strong>所以由人工错误标记引起的错误样本比例依旧是0.6%(这个数据可能有点不能理解，要注意这个0.6%是相对于整体验证集而言的，所以不会变)</strong>, 那么人工误差在总误差中所占的比例则达到了0.6%/2%=30%,相比于之前的6%影响力打了不小哦，所以此时则应该考虑对人工误差动手了.</p>
</blockquote>
<h2 id="3-Build-your-first-system-quickly-then-iterate"><a href="#3-Build-your-first-system-quickly-then-iterate" class="headerlink" title="3. Build your first system quickly, then iterate"></a>3. Build your first system quickly, then iterate</h2><p>还是有一个步骤流程的：</p>
<blockquote>
<ol>
<li>建立训练集，验证集，测试集</li>
<li>迅速搭建初始化系统</li>
<li>使用前面提到的Bias/Variance分析和误差分析来确定接下来的优化方向</li>
</ol>
</blockquote>
<h2 id="4-Training-and-testing-on-different-distributions"><a href="#4-Training-and-testing-on-different-distributions" class="headerlink" title="4. Training and testing on different distributions"></a>4. Training and testing on different distributions</h2><p><img src="/images/deeplearning/C3W2-3_1.png" width="700"></p>
<h2 id="5-Bias-and-Variance-with-mismatched-data-distributions"><a href="#5-Bias-and-Variance-with-mismatched-data-distributions" class="headerlink" title="5. Bias and Variance with mismatched data distributions"></a>5. Bias and Variance with mismatched data distributions</h2><p><img src="/images/deeplearning/C3W2-4_1.png" width="700"></p>
<p>对上面的PPT截图进行解释：</p>
<p><strong>左边:</strong></p>
<blockquote>
<p>首先还是以喵咪分类器作为例子，假设人类的误差接近贝叶斯误差0%。而训练集误差和开发集误差分别为1%和10%，二者相差9%，而且<strong>如果两个数据集来自同一个分布</strong>，那么我们就可以说模型训练结果方差较大。<br><strong>但是当两个数据集来自不同的分布时，我们就不能得出上面的结论了</strong>。另外，这9%的方差可能有两个原因导致的.</p>
<ol>
<li>是我们自己实现的代码有问题</li>
<li>是数据分布不同，所以你很难确定哪个是更主要的原因.</li>
</ol>
<p>因此为了找出是哪个原因我们做如下的事情：</p>
<p>  创建<strong>Training-dev set(训练-开发集)</strong>，其实就是从原来的训练集中抽取一部分数据出来，但是不喂给模型。（如上图所示）</p>
</blockquote>
<p><strong>右边:</strong></p>
<p>那怎么操作呢？很简单，下面以几个例子来说明：</p>
<blockquote>
<p>1.因为Training-dev set(训练-开发集)和Training set同分布，所以假设训练出来的结果如下： </p>
<ul>
<li>training error: 1%</li>
<li>training-dev error: 9%</li>
<li>dev error: 10%</li>
</ul>
<p>此时可以看到来自同分布数据的训练误差和训练-开发误差存在较大的方差，所以我们就可以确定肯定是我们滴代码还需要完善.</p>
<p>2.假设训练出来的结果如下：</p>
<ul>
<li>training error: 1%</li>
<li>training-dev error: 1.5%</li>
<li>dev error: 10%</li>
</ul>
<p>此时就可以说不是我程序员的问题了，而是发生了data mismatch(数据不匹配问题)</p>
</blockquote>
<p><strong>上图右下角：</strong></p>
<blockquote>
<p>1.假设人类的误差接近贝叶斯误差0，且训练误差如下：</p>
<ul>
<li>training error: 10%</li>
<li>training-dev error: 11%</li>
<li>dev error: 12%</li>
</ul>
<p>此时我们会认为模型与人类误差相比存在较大的偏差。所以就朝着 <strong>减小偏差的方向努力</strong> 吧少年.</p>
<p>2.同样假设人类的误差接近贝叶斯误差0，且训练误差如下：</p>
<p>training error: 10%<br>training-dev error: 11%<br>dev error: 20%</p>
<p>此时我们会认为存在两个问题：</p>
<ol>
<li>高偏差</li>
<li>数据不匹配问题</li>
</ol>
<p>祝福你，继续修改代码吧..</p>
</blockquote>
<h2 id="6-Addressing-data-mismatch"><a href="#6-Addressing-data-mismatch" class="headerlink" title="6. Addressing data mismatch"></a>6. Addressing data mismatch</h2><p><img src="/images/deeplearning/C3W2-5_1.png" width="700"></p>
<blockquote>
<p>虽然我们使用数据合成已经在语音识别方面取得了不错的效果提升</p>
<p>可以使用数据合成，但是要注意你的神经网络可能过拟合，过分学习了你这小部分数据集了。</p>
</blockquote>
<h2 id="7-Transfer-learning"><a href="#7-Transfer-learning" class="headerlink" title="7. Transfer learning"></a>7. Transfer learning</h2><p>简单的解释就是假如我们之前训练好了一个喵咪分类器，后来我们有了新任务 — 做一个海豚分类器，那么就可以将之前创建的喵咪分类器模型运用到新任务中去</p>
<p>举个栗子，假设我们对信号灯的红、绿灯进行了大量数据的学习，现在有了新任务，即需要识别黄灯,此时我们就不需要从头搭建模型，我们可以继续使用红绿灯网络框架，只需修改神经网络最后一层，即输出层，然后用已经训练好的权重参数初始化这个模型，对黄灯数据进行训练学习。</p>
<p>为什么可以这么做呢？因为尽管最后的标签不一致，但是之前学习的红绿灯模型已经捕捉和学习了很多有用的特征和细节，这对于黄灯的学习十分有帮助，而且这么做也可以大大的加快模型的构建速度。</p>
<blockquote>
<p><strong>想将<code>A</code>模型运用到<code>B</code>模型, 一般来说是有条件限制的，如下：</strong> </p>
<ol>
<li>A 和 B 需要有相类似的输入数据集，例如要么都是图像识别，要么是语音识别</li>
<li>A 的数据集要足够多，即远多于B</li>
<li>A 中学到一些 <strong>low level features</strong> 要对 B 有所帮助</li>
</ol>
</blockquote>
<h2 id="8-Multi-task-learning"><a href="#8-Multi-task-learning" class="headerlink" title="8. Multi-task learning"></a>8. Multi-task learning</h2><p>在 <strong>Transfer learning</strong> 中，整个过程是串行的，即咱们首先得实现A模型，然后在运用到B模型。</p>
<p>在 <strong>Multi-task learning</strong> 中，可以是同时开始学习.</p>
<blockquote>
<p><strong>举个🌰：</strong></p>
<p>现在很火的无人驾驶汽车，在行驶路上需要识别很多类型的物体，如行人、红绿灯、指路标志等等，所以此时可以使用 <strong>Multi-task learning</strong> 来实现。神经网络示意图如下：</p>
</blockquote>
<p><img src="/images/deeplearning/C3W2-6_2.png" width="750"></p>
<blockquote>
<p>如图示，最后的 $\hat{y}$ 是一个有4元素的向量，假设分别是行人、汽车、停车标志、信号灯。如果识别出图片中有哪一个元素，对应位置则输出1。</p>
<p><strong>注意</strong>：这要与softmax进行区分，softmax 只是一次识别一种物体，比如说识别出是行人，则输出[1,0,0,0],而不会说同时识别出行人和信号灯.</p>
</blockquote>
<p><strong>适用情况：</strong></p>
<p><img src="/images/deeplearning/C3W2-7_1.png" width="750"></p>
<blockquote>
<p>最后，Andrew Ng 说在实际中迁移学习使用频率要远高于多任务学习，有个例外就是视觉检测项目中多任务学习比较多.</p>
</blockquote>
<h2 id="9-End-to-end-deep-learning"><a href="#9-End-to-end-deep-learning" class="headerlink" title="9. End-to-end deep learning"></a>9. End-to-end deep learning</h2><p>首先以现在广泛使用的人脸识别技术解释一下什么是端到端的深度学习.</p>
<p><strong>What is end-to-end learning?</strong></p>
<blockquote>
<p>假如咱们走进一个摄像头，最开始离得较远的时候摄像头捕捉到的是我们的全身，此时系统不会将这种照片喂给模型，而是通过算法找到人脸的位置，然后切割放大，最后喂给模型进行识别.</p>
</blockquote>
<p><img src="/images/deeplearning/C3W2-8_1.png" width="700"></p>
<blockquote>
<p><strong>总结起来就是：</strong></p>
<ol>
<li>找人脸位置</li>
<li>将人脸图像切割放大，并喂给模型</li>
</ol>
<p>Notes: 端到端的深度学习其实就不是像将问题细分化，流水线化，每个步骤各司其职，下一层依赖上一层</p>
<p>And you need a large data set before the end-to-end approach really shines. （你需要大量的数据，端到端的深度学习，才能发挥耀眼的光芒.）</p>
</blockquote>
<h2 id="10-Reference"><a href="#10-Reference" class="headerlink" title="10. Reference"></a>10. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Structured Machine Learning Projects (week1) - ML Strategy 1]]></title>
      <url>http://sggo.me/2018/07/24/deeplearning/Structured-Machine-Learning-Projects-week1/</url>
      <content type="html"><![CDATA[<p>这次我们要学习专项课程中第三门课 <strong>Structured Machine Learning Projects</strong></p>
<p>学完这门课之后，你将会:</p>
<blockquote>
<ul>
<li>理解如何诊断机器学习系统中的错误</li>
<li>能够优先减小误差最有效的方向</li>
<li>理解复杂ML设定，例如训练/测试集不匹配，比较并/或超过人的表现</li>
<li>知道如何应用端到端学习、迁移学习以及多任务学习</li>
</ul>
</blockquote>
<p>很多团队浪费数月甚至数年来理解这门课所教授的准则，也就是说，这门两周的课可以为你节约数月的时间</p>
<a id="more"></a>
<h2 id="1-Why-ML-Strategy"><a href="#1-Why-ML-Strategy" class="headerlink" title="1. Why ML Strategy?"></a>1. Why ML Strategy?</h2><p><img src="/images/deeplearning/C3W1-1_1.png" width="700"></p>
<blockquote>
<p>如上图示，假如我们在构建一个喵咪分类器，数据集就是上面几个图，训练之后准确率达到90%。虽然看起来挺高的，但是这显然并不具一般性，因为数据集太少了。那么此时可以想到的ML策略有哪些呢？总结如上图中 <strong><code>Ideas</code></strong>.</p>
</blockquote>
<h2 id="2-Orthogonalization"><a href="#2-Orthogonalization" class="headerlink" title="2. Orthogonalization"></a>2. Orthogonalization</h2><blockquote>
<p>Orthogonalization [ɔ:θɒɡənəlaɪ’zeɪʃn] 正交化</p>
</blockquote>
<p><img src="/images/deeplearning/C3W1-2_1.png" width="600"></p>
<blockquote>
<p>And when I train a neural network，I tend not to use early shopping.</p>
<p>因为 Early Stropping，这个按钮能同时影响两件事情. 就像一个按钮同时影响电视机的宽度和高度. 如果你有更多的正交化(Orthogonalization)的手段，用这些手段调网络会简单不少.<br>When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal.</p>
</blockquote>
<p><img src="/images/deeplearning/C3W1-3_1.png" width="600"></p>
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th style="text-align:center">strategy</th>
<th style="text-align:center">solutions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td style="text-align:center">Fit training set well in cost function</td>
<td style="text-align:center">If it doesn’t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td style="text-align:center">Fit development set well on cost function</td>
<td style="text-align:center">If it doesn’t fit well, regularization or using bigger training set might help.</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td style="text-align:center">Fit test set well on cost function</td>
<td style="text-align:center">If it doesn’t fit well, the use of a bigger development set might help</td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td style="text-align:center">Performs well in real world</td>
<td style="text-align:center">If it doesn’t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing</td>
</tr>
</tbody>
</table>
<h2 id="3-Single-number-evaluation-metric"><a href="#3-Single-number-evaluation-metric" class="headerlink" title="3. Single number evaluation metric"></a>3. Single number evaluation metric</h2><p><img src="/images/deeplearning/C3W1-4_1.png" width="700"></p>
<blockquote>
<p>大致的思想就是首先按照单一数字评估指标对模型进行评价和优化。以精确率和召回率为例，这二者一般来说是一个不可兼得的指标，所以为了更好的衡量模型的好坏，引入F1算法来综合精确率和召回率对模型进行评估.</p>
</blockquote>
<!--<img src="/images/deeplearning/C3W1-6_1.png" width="700" />
-->
<p><img src="/images/deeplearning/C3W1-7_1.png" width="700"></p>
<p><a href="https://www.cnblogs.com/techengin/p/8962024.html" target="_blank" rel="external">Ref: sklearn中 F1-micro 与 F1-macro区别和计算原理</a></p>
<h2 id="4-Satisficing-and-optimizing-metrics"><a href="#4-Satisficing-and-optimizing-metrics" class="headerlink" title="4. Satisficing and optimizing metrics"></a>4. Satisficing and optimizing metrics</h2><p>It’s not always easy into a single real number evaluation metric</p>
<p><img src="/images/deeplearning/C3W1-9_1.png" width="750"></p>
<blockquote>
<p>So more generally, if you have N metrics that you care about, it’s sometimes reasonable to pick one of them to be optimizing. So you want to do as well as is possible on that one. And then N minus 1 to be satisficing.</p>
<p>满足和优化指标是很重要的</p>
</blockquote>
<h2 id="5-Train-dev-test-distributions"><a href="#5-Train-dev-test-distributions" class="headerlink" title="5. Train/dev/test distributions"></a>5. Train/dev/test distributions</h2><p><img src="/images/deeplearning/C3W1-10_1.png" width="700"></p>
<p><strong>Training, development and test distributions</strong></p>
<blockquote>
<p>Setting up the training, development and test sets have a huge impact on productivity. It is important to<br>choose the development and test sets from the same distribution and it must be taken randomly from all<br>the data.</p>
</blockquote>
<p><strong>Guideline</strong></p>
<blockquote>
<p>Choose a development set and test set to reflect data you expect to get in the future and consider important to do well.</p>
</blockquote>
<p><strong>所以为了实现服从同一分布，我们可以这样做:</strong></p>
<blockquote>
<p>首先将所有国家和地区的数据打散，混合, 按照一定的比例将上面混合打散后的数据划分为 <strong>development and test sets</strong></p>
</blockquote>
<h2 id="6-Size-of-dev-and-test-sets"><a href="#6-Size-of-dev-and-test-sets" class="headerlink" title="6. Size of dev and test sets"></a>6. Size of dev and test sets</h2><p><img src="/images/deeplearning/C3W1-11_1.png" width="750"></p>
<h2 id="7-When-to-change-dev-test-sets-and-metrics"><a href="#7-When-to-change-dev-test-sets-and-metrics" class="headerlink" title="7. When to change dev/test sets and metrics"></a>7. When to change dev/test sets and metrics</h2><p><strong>举个🌰:</strong> 假设现在一个公司在做一个喵咪图片推送服务（即给用户推送喵咪的照片），部署的有两个算法:</p>
<blockquote>
<ul>
<li>算法A: 喵咪图片识别误差是3%，但是可能会一不小心就给用户发了一些少儿不宜的图片</li>
<li>算法B：误差是5%，但是不会给用户推送不健康的图片</li>
</ul>
<p>所以对于技术人员来说可能希望准确性高一些的算法A，而用户可能会非常在意你给他推送了某些不想看的东西, 也许更喜欢算法B。所以总的来说就是根据实际需要来 改变开发/测试集合指标.</p>
</blockquote>
<p><img src="/images/deeplearning/C3W1-12_1.png" width="750"></p>
<h2 id="8-Why-human-level-performance"><a href="#8-Why-human-level-performance" class="headerlink" title="8. Why human-level performance?"></a>8. Why human-level performance?</h2><p><img src="/images/deeplearning/C3W1-14_1.png" width="750"></p>
<blockquote>
<p>如图示：</p>
<ul>
<li>蓝色虚线：表示人类识别的准确率</li>
<li>紫色曲线：表示机器学习不断训练过程中准确率的变化</li>
<li>绿色虚线：表示最高的准确率，即100%</li>
</ul>
<p>其中紫色曲线在末尾收敛后与绿色虚线之间的差距称为贝叶斯优化误差(Bayse Optima Error)</p>
</blockquote>
<!--<img src="/images/deeplearning/C3W1-13_1.png" width="750" />-->
<p>因此在实际操作过程中，我们可以以人类准确率为指标来评判我们训练的模型好坏程度</p>
<p><img src="/images/deeplearning/C3W1-15_1.png" width="750"></p>
<h2 id="9-Avoidable-bias"><a href="#9-Avoidable-bias" class="headerlink" title="9. Avoidable bias"></a>9. Avoidable bias</h2><p><img src="/images/deeplearning/C3W1-16_1.png" width="750"></p>
<blockquote>
<p>Humans error 与 Training Error 之间的差距我们成为 Avoidable bias<br>Training Error 与 Dev Error 之间的差距我们成为 Variance</p>
</blockquote>
<h2 id="10-Understanding-human-level-performance"><a href="#10-Understanding-human-level-performance" class="headerlink" title="10. Understanding human-level performance"></a>10. Understanding human-level performance</h2><p><img src="/images/deeplearning/C3W1-18_1.png" width="750"></p>
<blockquote>
<p><strong>解释说明 Example 1</strong>:</p>
<p>假如一个医院需要对一个医学影像进行分类识别，普通人，普通医生，有经验的医生和一群有经验的医生识别错误率分别为3%，1%，0.7%，0.5%。上一节中提到过Human Error，那此时的该如何确定Human Error呢？你可能会说取平均值，只能说Too Naive！当然是取最好的结果啦，也就是由一群经验丰富的医生组成的团体得到的结果作为Human Error。另外贝叶斯误差一定小于0.5%。</p>
</blockquote>
<p><img src="/images/deeplearning/C3W1-19_1.png" width="750"></p>
<blockquote>
<p><strong>解释说明 Example 2</strong>:</p>
<p>还是以医学影像分类识别为例，假如现在分成了三种情况：</p>
<p>Scenario A<br>让三类人群来划分后得到的误差分别为1%，0.7%，0.5%，而训练集和测试集误差分别为5%，6%。很显然此时的Avoidable Bias=4%~4.5%，Variance=1%，bias明显大于variance，所以此时应该将重心放到减小bias上去。</p>
<p>Scenario Bayse<br>同理此情况下的Avoidable Bias=0%~0.5%，Variance=4%，所以需要减小variance。</p>
<p>Scenario C<br>Avoidable Bias=0.2%，Variance=0.1%，二者相差无几，但是此时训练的模型准确率还是不及人类，所以没办法咱们还得继续优化，都说枪打出头鸟，所以继续优化bias~</p>
</blockquote>
<h2 id="11-Surpassing-human-level-performance"><a href="#11-Surpassing-human-level-performance" class="headerlink" title="11. Surpassing human-level performance"></a>11. Surpassing human-level performance</h2><p><img src="/images/deeplearning/C3W1-20_1.png" width="750"></p>
<blockquote>
<p><strong>Scenario A</strong></p>
<ul>
<li>Avoidable Bias=0.1%，Variance=0.2%，所以此时应该将重心放到减小Variance上去</li>
</ul>
<p><strong>Scenario B</strong></p>
<ul>
<li>Avoidable Bias=-0.2%，Variance=0.1%.乍一看可能会有点不知所措，而且训练集准确度也超过了人的最好成绩，不知道应该选择优化哪一项了，或者说这是不是就说明可以不用再优化了呢？</li>
</ul>
<p>（还是可以继续优化的。不可否认在图像识别方面人类的确其优于机器的方面，但是在其他方面，如在线广告推送，贷款申请评测等方面机器人要远远比人类优秀，所以如果是在上面课件中提到的一些领域，即使机器准确度超过了人类，也还有很大的优化空间。具体怎么优化。。。以后再探索。。。）</p>
</blockquote>
<h2 id="12-Improving-your-model-performance"><a href="#12-Improving-your-model-performance" class="headerlink" title="12. Improving your model performance"></a>12. Improving your model performance</h2><p><img src="/images/deeplearning/C3W1-21_1.png" width="750"></p>
<h2 id="13-Reference"><a href="#13-Reference" class="headerlink" title="13. Reference"></a>13. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7681619.html" target="_blank" rel="external">DeepLearning.ai学习笔记（三）结构化机器学习项目–week1 机器学习策略</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Improving DNN (week3) - Hyperparameter、Batch Regularization]]></title>
      <url>http://sggo.me/2018/07/23/deeplearning/Improving-Deep-Neural-Networks-week3/</url>
      <content type="html"><![CDATA[<p>Hyperparameter Tuning process、Normalizing Activations in a network</p>
<p>Fitting Batch Norm into a neural network、Why does Batch Norm work?、Batch Norm at test time</p>
<p>Softmax regression、TensorFlow</p>
<a id="more"></a>
<h2 id="1-Hyperparameter-Tuning-process"><a href="#1-Hyperparameter-Tuning-process" class="headerlink" title="1. Hyperparameter Tuning process"></a>1. Hyperparameter Tuning process</h2><p>正常情况有如下超参数:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Hyperparameter</th>
<th style="text-align:center">Desc</th>
<th style="text-align:center">Importance level</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><font color="red">α</font></td>
<td style="text-align:center">最重要</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><font color="orange">hidden units</font></td>
<td style="text-align:center"></td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center"><font color="orange">mini-batch size</font></td>
<td style="text-align:center"></td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center"><font color="orange">β</font></td>
<td style="text-align:center"></td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center"><font color="purple">layers</font></td>
<td style="text-align:center"></td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center"><font color="purple">learning rate decay</font></td>
<td style="text-align:center"></td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">$β_1,β_2,ε$</td>
<td style="text-align:center">最不重要</td>
<td style="text-align:center">4</td>
</tr>
</tbody>
</table>
<blockquote>
<p>颜色表示重要性，以及调试过程中可能会需要修改的程度.</p>
</blockquote>
<h3 id="那么如何选择超参数的值呢？"><a href="#那么如何选择超参数的值呢？" class="headerlink" title="那么如何选择超参数的值呢？:"></a>那么如何选择超参数的值呢？:</h3><ul>
<li>首先是粗略地随机地寻找最优参数</li>
</ul>
<p><img src="/images/deeplearning/C2W3-1_1.png" width="700"></p>
<p><strong>建议使用图右的方式，原因如下：</strong></p>
<blockquote>
<p>对于图左的超参数分布而言，可能会使得参考性降低，我们假设超参1是学习率α，超参2是ε，根据week2中Adam算法的介绍，我们知道ε的作用几乎可以忽略，所以对于图左25中参数分布来说，其本质只有5种参数分布。而右边则是25种随机分布，更能帮助我们选择合适的超参数.</p>
</blockquote>
<p><strong>其次在上面找到的最优参数分布周围再随机地寻找最有参数</strong></p>
<p><img src="/images/deeplearning/C2W3-2_1.png" width="700"></p>
<h2 id="2-Using-an-appropriate-scale-to-pick-hyperparameters"><a href="#2-Using-an-appropriate-scale-to-pick-hyperparameters" class="headerlink" title="2. Using an appropriate scale to pick hyperparameters"></a>2. Using an appropriate scale to pick hyperparameters</h2><p>上一节提到的的随机采样虽然能帮助我们寻找最优参数分布，但是这有点像大海捞针，如果能够指出参数取值的范围，然后再去寻找最优的参数分布岂不是更加的美滋滋？那如何为超参数选择合适的范围呢？</p>
<blockquote>
<p>$n^{[l]}=50,……,100$</p>
<p>$layers=2~4$</p>
<p>$α=0.0001，……,1$</p>
</blockquote>
<p>此时注意: 如按照线性划分的话(如下图)，那么随机采样的值 90% 的数据来自 [0.1,1] 这个区间, 这显然与不太符合随机性.</p>
<p><img src="/images/deeplearning/C2W3-3_1.png" width="700"></p>
<blockquote>
<p>所以为了改进这一问题，我们需要将区间对数化来采样.</p>
<p><strong>举个🌰：</strong> 我们将 [0.0001,1] 转化成四个区间 [0.0001,0.001], [0.001,0.01], [0.01,0.1], [0.1,1], 再转化成对数就是 [-4,-3], [-3,-2], [-2,-1], [-1,0].</p>
<p>($10^{−4}=0.0001$，其他同理取指数).</p>
</blockquote>
<p>然后我们可以用 Python 中提供的方法来实现随机采样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = <span class="number">-4</span>*np.random.rand() <span class="comment"># rand()表示在[0,1]上均匀采样, 最后的采样区间是[-4, 0]</span></span><br><span class="line">a = pow(<span class="number">10</span>, r)</span><br></pre></td></tr></table></figure>
<p><img src="/images/deeplearning/C2W3-4_1.png" width="700"></p>
<p><strong>$β=0.9,……,0.999$</strong></p>
<p>同理这里也不能使用线性轴来采样数据，我们可以通过对 <strong>1-β=0.1,……,0.001</strong> 来间接采样。转化成 [0.1, 0.01], [0.01,0.001], 转化成对数指数 [-1,-2],[-2,-3]。</p>
<p>即: $r∈[-3,-1], 1-β=10^r, β=1-10^r$</p>
<blockquote>
<p>当 β 接近 1 时, β 就会对细微的变化变得很敏感.</p>
<p>for example : 0.999, 0.9995 =&gt; 1000 -&gt; 2000</p>
<p>所以你需要更加密集的取值，在 β 接近 1 的时候.</p>
</blockquote>
<h2 id="3-Hyperparameters-tuning-in-practice-Pandas-vs-Caviar"><a href="#3-Hyperparameters-tuning-in-practice-Pandas-vs-Caviar" class="headerlink" title="3. Hyperparameters tuning in practice: Pandas vs Caviar"></a>3. Hyperparameters tuning in practice: Pandas vs Caviar</h2><p><img src="/images/deeplearning/C2W3-5_1.png" width="700"></p>
<p><strong>Babysitting one model:</strong></p>
<p>这种方法适用于有足够的数据集，但是 GPU，CPU 资源有限的情况，所以可能只能训练一个模型，然后每天对模型做某一项超参数的修改，查看效果是否变得更好.</p>
<blockquote>
<p>例如第一天令所有超参数随机初始化。到了第二天发现效果还不错，此时可以去增加学习率(也可以修改其他参数)。……，到了某一天加入修改了mini-batch size，结果效果明显减弱，这时则需要重新恢复到前一天的状态。</p>
<p>总的来说这一过程就像熊猫一样，只照顾一个宝宝，多的照顾不过来.</p>
</blockquote>
<p><strong>Train many models in parallel:</strong></p>
<blockquote>
<p>这种方法适用于财大气粗的情况，即并行训练多个模型，最后选出效果最好的一个即可。这就像鱼子酱一样，一下生多大一亿的孩子.</p>
</blockquote>
<h2 id="4-Normalizing-Activations-in-a-network"><a href="#4-Normalizing-Activations-in-a-network" class="headerlink" title="4. Normalizing Activations in a network"></a>4. Normalizing Activations in a network</h2><p>不仅要归一化输入数据 <strong>$X$</strong>,隐藏层的数据也是要归一化的. 一般来说隐藏层数据有 $Z$ 和 $a$ 两种，Andrew Ng 推荐归一化 <strong>$z$</strong>.</p>
<blockquote>
<p>Batch 归一化 由 Sergey loffe 和 Christian Szegedy 两位研究者创造.</p>
</blockquote>
<p>Batch 归一化，会使你的参数搜索变得容易, 使神经网络对超参数的搜索更加稳定. 这样也会使得你容易训练深层神经网络。</p>
<p><strong>输入数据 $X$ 归一化方法:</strong></p>
<p>$$<br>μ=\frac{1}{m}\sum_{i}{x^{(i)}}<br>$$</p>
<p>$$<br>σ^2=\frac{1}{m}\sum_{i}x^{(i)^2}<br>$$</p>
<p>$$<br>x=\frac{x-μ}{σ^2}<br>$$</p>
<blockquote>
<p>m 为 mini-batch 中的 m， 而不是整个训练集</p>
</blockquote>
<p><strong>隐藏层数据归一化方法:</strong></p>
<p>$$<br>μ=\frac{1}{m}\sum_{i}{z^{(i)}-μ}<br>$$</p>
<p>$$<br>σ^2=\frac{1}{m}\sum_{i}(z^{(i)^2}-μ)^2<br>$$</p>
<p>$$<br>z^{(i)}_{norm}=\frac{z^{(i)}-μ}{\sqrt{σ^2+ε}}<br>$$</p>
<p>上面的归一化后的数据 $z$ 都是服从均值为 0，方差为 1 的，显然这样不能满足咱们的需求，所以还需要做进一步处理，如下：</p>
<p>$$<br>\tilde{z}^{(i)}=γz^{(i)} + β<br>$$</p>
<p>上式中的 $γ$ 可以设置方差，$β$ 可以设置均值.</p>
<blockquote>
<p>你也许不想隐层单元值必须是平均值0 和 方差1, 比如你有一个 sigmoid 函数，你不想让它的值完全集中在这里, 你不想使他们平均值和方差一直是0和1， 这样可以更好的利用非线性的 Sigmoid 函数， 而不是所有值都集中在线性的区域, $γ$ 和 $β$ 可以确保所有的 $Z^{(i)}$ 值，可以是你想赋予的任意值. 或者 它的作用是保证隐藏的单元已使均值和方差标准化. 那里均值和方差由两参数控制. $γ$ 和 $β$ 学习算法可以设置为任何值. 所以它的真正作用是使均值和方差标准化. $Z^{(i)}$ 有固定的均值和方差，均值和方差可以是0和1，也可以是其他值，由 $γ$ 和 $β$ 确定.</p>
<p>In practice， normlizing $Z^{[2]}$ is done much more often.</p>
</blockquote>
<h2 id="5-Fitting-Batch-Norm-into-a-neural-network"><a href="#5-Fitting-Batch-Norm-into-a-neural-network" class="headerlink" title="5. Fitting Batch Norm into a neural network"></a>5. Fitting Batch Norm into a neural network</h2><p><strong>adding batch Norm to a network</strong></p>
<p><img src="/images/deeplearning/C2W3-6_1.png" width="750"></p>
<p><strong>working with mini-batches</strong></p>
<p>一般的方法中</p>
<p>$$<br>z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}<br>$$</p>
<p>在上面归一化数据过程中需要减去均值，所以 $b^{[l]}$ 这一项可以省略掉,所以归一化后是</p>
<p>$$<br>z_{norm}^{[l]}=w^{[l]}a^{[l-1]}<br>$$</p>
<p>为了能够使数据分布更加满足我们的要求，可以用如下公式</p>
<p>$$<br>\tilde{z}^{[l]}=γ^{[l]}z_{norm}^{[l]}+β^{[l]}<br>$$</p>
<p><strong>Implementing gradient descent</strong></p>
<p>for t= 1,……,numMinBatches</p>
<ul>
<li>计算基于第 $t$ 批数据的前向传播</li>
<li>在计算反向传播时使用 $\tilde{z}^{[l]}$, 得到 $dw^{[l]},dβ^{[l]},dγ^{[l]}$</li>
<li>更新参数</li>
</ul>
<p>$$<br>w^{[l]}=w^{[l]}-αdw^{[l]} \\<br>β^{[l]}=β^{[l]}-αdβ^{[l]} \\<br>γ^{[l]}=γ^{[l]}-αdγ^{[l]}<br>$$</p>
<h2 id="6-Why-does-Batch-Norm-work"><a href="#6-Why-does-Batch-Norm-work" class="headerlink" title="6. Why does Batch Norm work?"></a>6. Why does Batch Norm work?</h2><p><strong>原因一:</strong></p>
<p><strong>batch norm</strong> 可以使得权重比你的网络更滞后或更深层，为了更好地理解可以看下面的例子:</p>
<p><img src="/images/deeplearning/C2W3-7_1.png" width="700"></p>
<p>如上图所示，假设我们现在要计算第三层隐藏层的值，很显然该层的计算结果依赖第二层的数据，但是第二层的数据如果未归一化之前是不可知的，分布是随机的。而如果进行归一化后，即 $\tilde{z}^{[2]}=γ^{[2]}z_{norm}^{[2]}+β^{[2]}$ 可以将第二层数据限制为均值为 $β^{[2]}$, 方差为 $γ^{[2]}$ 的分布,注意这两个参数并不需要人为设置，它会自动学习的。所以即使输入数据千变万化，但是经过归一化后分布都是可以满足我们的需求的，更简单地说就是归一化数据可以减弱前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习。</p>
<p><strong>原因二:</strong></p>
<p>batch norm 奏效的另一个原因则是它具有正则化的效果。其与dropout有异曲同工之妙，我们知道dropout会随机的丢掉一些节点，即数据，这样使得模型训练不会过分依赖某一个节点或某一层数据。batch norm也是如此，通过归一化使得各层之间的依赖性降低，并且会给每层都加入一些噪声，从而达到正则化的目的</p>
<blockquote>
<p>Batch 它限制了在前层的参数更新，会影响数值分布的程度，第三层看到的这种情况，因此得学习. <strong>batch 归一化减少了输入值改变的问题</strong>, 它的确是这些值变得更稳定. 神经网络的之后层就会有更坚实的基础. 即使输入分布改变了一些，它会改变得更少，它做的是 当前层保持学习，当层改变时，迫使后层, 适应的程度减少了，你可以这样想，它减弱了前层参数的作用，与后层参数的作用之间的联系，它使得网络每层都可以自己学习. 稍稍独立于其它层，这有助于加速整个网络的学习.</p>
<p>batch norm 中有一个作用，可以起到轻微 正则化 的作用. (因为添加的噪音很微小，所以并不是巨大的正则化)， 你可以将 batch norm 和 dropout 一起使用.</p>
<p>dropout, 你应用较大的 mini-batch 比如 512，那么可以减少噪音也, 因此减少了正则化的效果. 这是 dropout 的一个奇怪的性质.</p>
<p>batch norm 是一个正则化的规则，而不要把它当做目的. 但是有时候，它会对你的算法有额外的期望和非期望效果.</p>
<p>batch norm 一次只能处理 一个 mini-batch 的数据. 它在 mini-batch 上计算期望与方差.</p>
</blockquote>
<h2 id="7-Batch-Norm-at-test-time"><a href="#7-Batch-Norm-at-test-time" class="headerlink" title="7. Batch Norm at test time"></a>7. Batch Norm at test time</h2><p>前面提到的 batch norm 都是基于训练集的，但是在测试集上，有时候可能我们的测试数据很少，例如只有1个，在这个时候进行归一化则显得没多大意义了。那么该怎么办呢？均值$μ$ 和 方差$σ^2$该如何确定呢？</p>
<blockquote>
<p>方法还是有的，而且已经在上面提到过了, 就是第三节所介绍的<strong>指数加权平均</strong>啦，原理是类似的</p>
</blockquote>
<p>假设一共有如下 $x^1,x^2,……,x^5000$ 的批量数据，每组mini-batch 都得到了对应的均值$μ$, (方差同理，不详细说明了)，即 $μ^1,μ^2,……,μ^5000$, 如果测试集数据很少，那么就可以使用指数加权平均的方法来得到测试集的均值和方差。</p>
<p>之后就根据<strong>指数加权平均</strong>计算得到的值来计算归一化后的输入值即可.</p>
<p><img src="/images/deeplearning/C2W3-8_1.png" width="750"></p>
<blockquote>
<p><strong>Andrew Ng 语录:</strong></p>
<p>如果将你的神经网络用于<strong>测试</strong>，你需要单独估算 $μ$ 和 $σ^2$, 在典型的 Batch 归一化运用中，你需要用一个指数加权平均来估算，整个平均数覆盖了所有的 mini-batch .</p>
<p>$$<br>z^{(i)}_{norm}=\frac{z^{(i)}-μ}{\sqrt{σ^2+ε}}<br>$$</p>
<p>上个式子 $z^{(i)}_{norm}$ 中的，$μ$, $σ^2$ 是类似加权平均出来的值.</p>
<p>注意：测试集的均值和方差生成的方式不一定非得是上面提到的指数加权平均，也可以是简单粗暴的计算所有训练集的均值和方差，视频中 Andrew Ng 说这也是可行的.</p>
</blockquote>
<h2 id="8-Softmax-regression"><a href="#8-Softmax-regression" class="headerlink" title="8. Softmax regression"></a>8. Softmax regression</h2><p><img src="/images/deeplearning/C2W3-9_1.png" width="750"></p>
<p>假设第 $l$ 层有 $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$, 激活函数为 $a^{[l]}=\frac{e^{z^{[l]}}}{\sum_{j=1}^{n_l}e^{z^{[l]}_j}}$</p>
<p>该节视频中 Andrew Ng 并没有很详细的介绍 softmax 的原理和公式推导，感兴趣的可以戳如下链接进行进一步了解：</p>
<ul>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/Softmax回归" target="_blank" rel="external">ufldl: Softmax 回归</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21485970" target="_blank" rel="external">softmax 公式推导&amp;算法实现</a></li>
</ul>
<h2 id="9-Trying-a-softmax-classifier"><a href="#9-Trying-a-softmax-classifier" class="headerlink" title="9. Trying a softmax classifier"></a>9. Trying a softmax classifier</h2><blockquote>
<p><a href="http://www.cnblogs.com/marsggbo/p/7467347.html" target="_blank" rel="external">转载: 具体实践项目可参见softmax分类算法原理(用python实现)</a></p>
<p>上面的转载实现 softmax 需要再仔细研究.</p>
</blockquote>
<h2 id="10-Deep-learning-frameworks"><a href="#10-Deep-learning-frameworks" class="headerlink" title="10. Deep learning frameworks"></a>10. Deep learning frameworks</h2><p><img src="/images/deeplearning/C2W3-9_1.png" width="750"></p>
<h2 id="11-TensorFlow-Example"><a href="#11-TensorFlow-Example" class="headerlink" title="11. TensorFlow Example"></a>11. TensorFlow Example</h2><p>Andrew Ng 演示了 TensorFlow 使用方法.</p>
<blockquote>
<p>我推荐一个比较好的 TensorFlow 的练手项目：<a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="external">TensorFlow Example</a></p>
</blockquote>
<h2 id="12-Reference"><a href="#12-Reference" class="headerlink" title="12. Reference"></a>12. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="https://daniellaah.github.io/2017/deeplearning-ai-Improving-Deep-Neural-Networks-week1.html" target="_blank" rel="external">deeplearning.ai 专项课程二第一周</a></li>
<li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="external">Coursera - Deep Learning Specialization</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
<li><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="external">TensorFlow-Examples</a></li>
<li><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="external">吴恩达老师的深度学习课程笔记及资源</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Improving Deep Neural Networks (week2) - Optimization Algorithm]]></title>
      <url>http://sggo.me/2018/07/21/deeplearning/Improving-Deep-Neural-Networks-week2/</url>
      <content type="html"><![CDATA[<p>Mini-batch、指数加权平均-偏差修正、Momentum、RMSprop、Adam、学习率衰减、局部最优</p>
<p>这节课每一节的知识点都很重要，所以本次笔记几乎涵盖了全部小视频课程的记录</p>
<a id="more"></a>
<h2 id="1-Mini-batch"><a href="#1-Mini-batch" class="headerlink" title="1. Mini-batch"></a>1. Mini-batch</h2><blockquote>
<p>随机梯度下降法的一大缺点是, 你会失去所有向量化带给你的加速，因为一次性只处理了一个样本，这样效率过于低下, 所以实践中最好 选择不大不小 的 Mini-batch 尺寸. 实际上学习率达到最快，你会发现2个好处，你得到了大量向量化，另一方面 你不需要等待整个训练集被处理完，你就可以开始进行后续工作.</p>
<p>它不会总朝着最小值靠近，但它比随机梯度下降要更持续地靠近最小值的方向. 它也不一定在很小的范围内收敛，如出现这个问题，你可以减小 学习率.</p>
<p>样本集比较小，就没必要使用 mini-batch.</p>
<p><strong>经验值</strong> ： 如果 m &lt;= 2000, 可以使用 batch， 不然样本数目 m 较大，一般 mini-batch 大小设置为 64 or 128 or.. or 512..</p>
</blockquote>
<p><strong>算法初步</strong></p>
<blockquote>
<p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有 500万 或 5000万 的训练数据，处理速度就会比较慢。</p>
<p>但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 Mini-batch。</p>
</blockquote>
<p><img src="/images/deeplearning/C2W2-1_1.png" width="700"></p>
<blockquote>
<p>如图，以 1000 为单位，将数据划分，令 $x^{\{1\}}=\{x^{(1)},x^{(2)}……x^{(1000)}\}$, 一般用 $x^{ \{ t \} }$, $y^{ \{t\} }$ 表示划分后的 mini-batch.</p>
<p>注意区分该系列教学视频的符号标记：</p>
<ul>
<li>小括号() 表示具体的某一个元素，指一个具体的值，例如 $x^{(i)}$</li>
<li>中括号[] 表示神经网络中的某一层, 例如 $Z^{[l]}$</li>
<li>大括号{} 表示将数据细分后的一个集合, 例如 $x^{\{1\}} = \{x^{(1)},x^{(2)}……x^{(1000)}\}$</li>
</ul>
</blockquote>
<p><strong>算法核心</strong></p>
<p><img src="/images/deeplearning/C2W2-2_1.png" width="700"></p>
<blockquote>
<p>假设我们有 5,000,000 个数据，每 1000 作为一个集合，计入上面所提到的 $x^{\{1\}}=\{x^{(1)},x^{(2)}……x^{(5000)}\},……$</p>
<ol>
<li>需要迭代运行 5000次 神经网络运算.</li>
<li>每一次迭代其实与之前笔记中所提到的计算过程一样，首先是前向传播，但是每次计算的数量是 1000.</li>
<li>计算损失函数，如果有 Regularization ，则记得加上 Regularization Item</li>
<li>Backward propagation</li>
</ol>
<p>注意，mini-batch 相比于之前一次性计算所有数据不仅速度快，而且反向传播需要计算 5000次，所以效果也更好.</p>
</blockquote>
<p>epoch</p>
<blockquote>
<ul>
<li>对于普通的梯度下降法，一个 epoch 只能进行一次梯度下降；</li>
<li>对于 Mini-batch 梯度下降法，一个 epoch 可以进行 Mini-batch 的个数次梯度下降;</li>
</ul>
<p><strong>epoch</strong> : 当一个<code>完整的数据集</code>通过了神经网络一次并且返回了一次，这个过程称为一个 epoch。</p>
<p>比如对于一个有 2000 个训练样本的数据集。将 2000 个样本分成大小为 500 的 batch，那么完成一个 epoch 需要 4 个 iteration。</p>
</blockquote>
<h3 id="不同-size-大小的比较"><a href="#不同-size-大小的比较" class="headerlink" title="不同 size 大小的比较"></a>不同 size 大小的比较</h3><p>普通的 batch 梯度下降法 和 Mini-batch梯度下降法 代价函数的变化趋势，如下图所示：</p>
<p><img src="/images/deeplearning/C2W2-3_1.png" width="700"></p>
<p><strong>Batch梯度下降</strong> （如下图中蓝色）:</p>
<blockquote>
<ul>
<li>对所有 m 个训练样本执行一次梯度下降，每一次迭代时间较长；</li>
<li>Cost function 总是向减小的方向下降。</li>
</ul>
<p>说明: mini-batch size = m，此时即为 Batch gradient descent $(x^,y^)=(X,Y)$</p>
</blockquote>
<p><strong>随机梯度下降</strong> （如下图中紫色）:</p>
<blockquote>
<p>-对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；</p>
<ul>
<li>Cost function 总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式.</li>
</ul>
<p>说明: mini-batch size = 1，此时即为 Stochastic gradient descent $(x^{\{t\}},y^{\{t\}})=(x^{(i)},y^{(i)})$</p>
</blockquote>
<p><strong>Mini-batch梯度下降</strong> （如下图中绿色）:</p>
<blockquote>
<ul>
<li>选一个 $1&lt;size&lt;m$ 的合适的 size 进行 Mini-batch 梯度下降，可实现快速学习，也应用了向量化带来的好处</li>
<li>Cost function 的下降处于前两者之间</li>
</ul>
</blockquote>
<p><img src="/images/deeplearning/C2W2-4_1.png" width="700"></p>
<h3 id="Mini-batch-大小的选择"><a href="#Mini-batch-大小的选择" class="headerlink" title="Mini-batch 大小的选择"></a>Mini-batch 大小的选择</h3><blockquote>
<ul>
<li>如果训练样本的大小比较小时，如 $m⩽2000$ 时 — 选择 batch 梯度下降法；</li>
<li>如果训练样本的大小比较大时，典型的大小为：$2^{6}、2^{7}、\cdots、2^{10}$</li>
<li>Mini-batch 的大小要符合 CPU/GPU 内存， 运算起来会更快一些.</li>
</ul>
</blockquote>
<h2 id="2-Exponentially-weighted-averages"><a href="#2-Exponentially-weighted-averages" class="headerlink" title="2. Exponentially weighted averages"></a>2. Exponentially weighted averages</h2><p>为了理解后面会提到的各种优化算法，我们需要用到指数加权平均，在统计学中也叫做指数加权移动平均.</p>
<p>指数加权平均的关键函数： </p>
<p>$$<br>v_{t} = \beta v_{t-1}+(1-\beta)\theta_{t}<br>$$</p>
<p>首先我们假设有一年的温度数据，如下图所示</p>
<p><img src="/images/deeplearning/C2W2-5_0.jpg" width="500"></p>
<p>我们现在需要计算出一个温度趋势曲线，计算方法(<code>指数加权平均实现</code>)如下：</p>
<p>$$<br>v_{0} =0 \\<br>v_{1}= \beta v_{0}+(1-\beta)\theta_{1} \\<br>v_{2}= \beta v_{1}+(1-\beta)\theta_{2} \\<br>v_{3}= \beta v_{2}+(1-\beta)\theta_{3} \\<br>\ldots<br>$$</p>
<blockquote>
<p>上面的 $θ_t$ 表示第 $t$ 天的温度，β 是可调节的参数，$V_t$ 表示 $\frac{1}{1-β}$ 天的每日温度</p>
</blockquote>
<!--下图是一个关于天数和温度的散点图：
<img src="/images/deeplearning/C2W2-5_1.png" width="600" />
-->
<h3 id="当-β-0-9、0-98、0-5-的情况"><a href="#当-β-0-9、0-98、0-5-的情况" class="headerlink" title="当 β=0.9、0.98、0.5 的情况"></a>当 β=0.9、0.98、0.5 的情况</h3><p>当 β=0.9 时，指数加权平均最后的结果如下图中<strong>红色</strong>线所示；</p>
<p><img src="/images/deeplearning/C2W2-5_2.jpg" width="600"></p>
<p>当 β=0.98 时，指数加权平均最后的结果如下图中<strong>绿色</strong>线所示, 绿线相比较红线要平滑一些，是因为对过去温度的权重更大，所以当天天气温度的影响降低，在温度变化时，适应得更缓慢一些；</p>
<p><img src="/images/deeplearning/C2W2-5_3.jpg" width="600"></p>
<p>当 β=0.5 时，指数加权平均最后的结果如下图中<strong>黄色</strong>线所示；</p>
<p><img src="/images/deeplearning/C2W2-5_4.jpg" width="600"></p>
<!--<img src="/images/deeplearning/C2W2-6_1.png" width="700" />
-->
<blockquote>
<p> Notes: The most common value for $\beta$ is 0.9.</p>
</blockquote>
<h3 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h3><p>例子，当 β=0.9 时： </p>
<p>$$<br>v_{100} = 0.9v_{99}+0.1\theta_{100} \\ v_{99} = 0.9v_{98}+0.1\theta_{99} \\ v_{98} = 0.9v_{97}+0.1\theta_{98} \\<br>\ldots<br>$$</p>
<p>展开：</p>
<p>$$<br>v_{100}=0.1\theta_{100}+0.9(0.1\theta_{99}+0.9(0.1\theta_{98}+0.9v_{97})) \\ v_{100}=0.1\theta_{100}+0.1\times0.9\theta_{99}+0.1\times(0.9)^{2}\theta_{98}+0.1\times(0.9)^{3}\theta_{97}+\cdots<br>$$</p>
<p>上式中所有 $θ$ 前面的系数相加起来为 1 或者 接近于 1，称之为偏差修正.</p>
<blockquote>
<p>总体来说存在，$(1-\varepsilon)^{1/\varepsilon}=\dfrac{1}{e}$, 在我们的例子中，$1-\varepsilon=\beta=0.9$, 即 $0.9^{10}\approx 0.35\approx\dfrac{1}{e}$ . 相当于大约10天后，系数的峰值（这里是0.1）下降到原来的 $\dfrac{1}{e}$，只关注了过去10天的天气.</p>
</blockquote>
<h3 id="指数加权平均的偏差修正-Bias-correction"><a href="#指数加权平均的偏差修正-Bias-correction" class="headerlink" title="指数加权平均的偏差修正 Bias correction"></a>指数加权平均的偏差修正 Bias correction</h3><p>在我们执行指数加权平均的公式时，当 β=0.98 时，得到的并不是图中的<strong>绿色</strong>曲线，而是下图中的<strong>紫色</strong>曲线，其起点比较低。</p>
<p><img src="/images/deeplearning/C2W2-7.png" width="650"></p>
<p><strong>原因</strong>： </p>
<blockquote>
<p>$$<br>v_{0}=0\\v_{1}=0.98v_{0}+0.02\theta_{1}=0.02\theta_{1}\\v_{2}=0.98v_{1}+0.02\theta_{2}=0.98\times0.02\theta_{1}+0.02\theta_{2}=0.0196\theta_{1}+0.02\theta_{2}<br>$$</p>
<p>如果第一天的值为如40，则得到的 v1=0.02×40=0.8，则得到的值要远小于实际值，后面几天的情况也会由于初值引起的影响，均低于实际均值.</p>
</blockquote>
<p><strong>偏差修正</strong>： </p>
<blockquote>
<p>使用 $\dfrac{v_{t}}{1-\beta^{t}}$</p>
<ul>
<li>当 t=2 时：</li>
</ul>
<p>$$<br>1-\beta^{t}=1-(0.98)^{2}=0.0396<br>$$</p>
<p>$$<br>\dfrac{v_{2}}{0.0396}=\dfrac{0.0196\theta_{1}+0.02\theta_{2}}{0.0396}<br>$$</p>
<p>偏差修正得到了绿色的曲线，在开始的时候，能够得到比紫色曲线更好的计算平均的效果。随着 t 逐渐增大，$\beta^{t}$ 接近于 0，所以后面绿色的曲线和紫色的曲线逐渐重合了.</p>
<p>虽然存在这种问题，但是在实际过程中，一般会忽略前期均值偏差的影响</p>
</blockquote>
<p><strong>偏差修正 举个🌰, 以便于大家理解:</strong></p>
<blockquote>
<p>首先我们假设的是 $β=0.98, V_0=0$, 然后由 $V_t=βV_{t-1}+(1-β)θ_t$ 可知</p>
<ul>
<li><p>$V_1=0.98V_0+0.02θ_1=0.02θ_1$</p>
</li>
<li><p>$V_2=0.98V_1+0.02θ_2=0.0196θ_1+0.02θ_2$</p>
</li>
</ul>
<p>当进行指数加权平均计算时，第一个值 $v_0$ 被初始化为 0，这样将在前期的运算用产生一定的偏差。为了矫正偏差，需要在每一次迭代后进行偏差修正（Bias Correction）</p>
<p>假设 $θ_1=40℃$,那么 $V_1=0.02*40=0.8℃$，这显然相差太大，同理对于后面的温度的计算也只会是变差越来越大. 所以我们需要进行偏差修正，具体方法如下：</p>
<p>$$<br>V_t=\frac{βV_{t-1}+(1-β)θ_t}{1-β^t}<br>$$</p>
<p>注意 ：！！！上面公式中的 $V_{t-1}$ 是未修正的值.</p>
<p>为方便说明，令 $β=0.98,θ_1=40℃,θ_2=39℃$, 则</p>
<ul>
<li>当 $t=1,θ_1=40℃$ 时，$V_1=\frac{0.02*40}{1-0.98}=40$ ,哇哦, 有没有很巧的感觉，再看</li>
<li>当 $t=2,θ_2=39℃$ 时，$V_2 = \frac{0.98*V_{t-1} + 0.02*θ_2}{1-0.98^2}$ $=\frac{0.98*(0 + 0.02*θ_1)+0.02*39}{1-0.98^2}=39.49$</li>
</ul>
<p><code>注意点</code> : 所以，<strong>记住你如果直接用修正后的 $V_{t−1}$ 值代入计算就大错特错了</strong>.</p>
</blockquote>
<h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3. Momentum"></a>3. Momentum</h2><p>动量梯度下降的基本思想就是<code>计算梯度的指数加权平均数</code>，并利用该梯度来更新权重</p>
<h3 id="Momentum-解释版"><a href="#Momentum-解释版" class="headerlink" title="Momentum 解释版"></a>Momentum 解释版</h3><p>通常情况我们在训练深度神经网络的时候把数据拆解成一小批一小批地进行训练，这就是我们常用的 mini-batch SGD 训练算法，然而虽然这种算法能够带来很好的训练速度，但是在到达最优点的时候并不能够总是真正到达最优点，而是在最优点附近徘徊。另一个缺点就是这种算法需要我们挑选一个合适的学习率，当我们采用小的学习率的时候，会导致网络在训练的时候收敛太慢；当我们采用大的学习率的时候，会导致在训练过程中优化的幅度跳过函数的范围，也就是可能跳过最优点。我们所希望的仅仅是网络在优化的时候网络的损失函数有一个很好的收敛速度同时又不至于摆动幅度太大。</p>
<p>所以 Momentum 优化器 刚好可以解决我们所面临的问题，它主要是基于梯度的移动指数加权平均。假设在当前的迭代步骤第 t 步中，那么基于 Momentum 优化算法 可以写成下面的公式： </p>
<p>$$<br>{v_{dw}} = \beta {v_{dw}} + (1 - \beta )dW \\ {v_{db}} = \beta {v_{db}} + (1 - \beta )db \\ W = W - \alpha {v_{dw}} \\ b = b - \alpha {v_{db}}<br>$$</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/30743067" target="_blank" rel="external">引用知乎机器学习小菜鸟的流水账</a> ：</p>
<p>动量梯度下降与梯度下降相比，就是对梯度使用指数加权平均，其他的都保存一致。</p>
<p>dw 与 db 表示本次迭代的梯度，Vdw 和 Vdb 表示指数加权平均的梯度。</p>
<p>如果不用指数加权平均的话，每次迭代更新使用的梯度都只与本次迭代的样本有关，每次迭代的样本有好有坏，会使迭代接近最小值的不断波动，导致下降速度慢。加入指数加权平均后，本次梯度影响减少，波动情况也就会减小，直观上面理解就是左右波动抵消，那么下降速度也就自然更快。动量梯度下降比梯度下降收敛速度要快。</p>
<p>物理意义理解：在下降的过程中， $(1 - \beta )dW$ 相当于加速度， $\beta {v_{dw}}$ 相当于摩擦，加速度可以是下降加快，而摩擦不会让加速一直进行下去。（不是很理解）</p>
</blockquote>
<hr>
<blockquote>
<p>在上面的公式中 ${v_{dw}}$ 和 ${v_{db}}$ 分别是损失函数在前 $t−1$ 轮迭代过程中累积的梯度梯度动量，$\beta$ 是梯度累积的一个指数，这里我们一般设置值为 0.9。所以 Momentum 优化器 的主要思想就是利用了类似与移动指数加权平均的方法来对网络的参数进行平滑处理的，让梯度的摆动幅度变得更小。 </p>
<p>dW 和 db 分别是损失函数反向传播时候所求得的梯度，下面两个公式是网络权重向量和偏置向量的更新公式，α 是网络的学习率。当我们使用 Momentum优化算法的时候，可以解决 mini-batch SGD 优化算法更新幅度摆动大的问题，同时可以使得网络的收敛速度更快。</p>
</blockquote>
<h3 id="Momentum-详细版"><a href="#Momentum-详细版" class="headerlink" title="Momentum 详细版"></a>Momentum 详细版</h3><p>在我们优化 Cost function 的时候，以下图所示的函数图为例：</p>
<p>首先介绍一下一般的梯度算法收敛情况是这样的</p>
<p><img src="/images/deeplearning/C2W2-8_1.png" width="750"></p>
<blockquote>
<p>可以看到，在前进的道路上十分曲折，走了不少弯路，在纵向我们希望走得慢一点，横向则希望走得快一点，所以才有了动量梯度下降算法.</p>
</blockquote>
<p><strong>Momentum算法的第 t 次迭代：</strong></p>
<blockquote>
<ul>
<li>计算出 dw, db</li>
<li>这个计算式子与上一届提到的指数加权平均有点类似，即<br>$ V_{dw}=βV_{dw}+(1-β)dw \\ V_{db}=βV_{db}+(1-β)db $</li>
<li>$W=W-αV_{dw},b=b-αV_{db}$</li>
</ul>
</blockquote>
<p><img src="/images/deeplearning/C2W2-10_1.png" width="600"></p>
<p>最终得到收敛的效果如下图的红色曲线所示.</p>
<p><img src="/images/deeplearning/C2W2-9_1.png" width="750"></p>
<p>在利用梯度下降法来最小化该函数的时候，每一次迭代所更新的代价函数值如图中蓝色线所示在上下波动，而这种幅度比较大波动，减缓了梯度下降的速度，而且我们只能使用一个较小的学习率来进行迭代.</p>
<p>如果用较大的学习率，结果可能会如紫色线一样偏离函数的范围，所以为了避免这种情况，只能用较小的学习率.</p>
<blockquote>
<p>该算法中涉及到的超参数有两个，分别是 α，β，其中一般 β=0.9 是比较常取的值</p>
<ul>
<li>一般将参数设为 0.5, 0.9，或者 0.99，分别表示最大速度 2倍，10倍，100倍 于 SGD 的算法;</li>
<li>通过速度v，来积累了之间梯度指数级衰减的平均，并且继续延该方向移动;</li>
</ul>
</blockquote>
<p>再看看算法： </p>
<p><img src="/images/deeplearning/C2W2-11.png" width="700"></p>
<h2 id="4-RMSprop-Root-Mean-Square-Prop"><a href="#4-RMSprop-Root-Mean-Square-Prop" class="headerlink" title="4. RMSprop (Root Mean Square Prop)"></a>4. RMSprop (Root Mean Square Prop)</h2><p>RMSProp 算法的全称叫 Root Mean Square Prop，是 Geoffrey E. Hinton 在 Coursera 课程中提出的一种优化算法，在上面的 Momentum 优化算法中，虽然初步解决了优化中摆动幅度大的问题。所谓的摆动幅度就是在优化中经过更新之后参数的变化范围，如下图所示，<strong>蓝色的为 Momentum 优化算法所走的路线</strong>，<strong>绿色的为 RMSProp 优化算法所走的路线</strong>。</p>
<p><img src="/images/deeplearning/C2W2-12_1.png" width="700"></p>
<p>为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp 算法对权重 W 和偏置 b 的梯度使用了微分平方加权平均数。 </p>
<p>其中，假设在第 t 轮迭代过程中，各个公式如下所示：</p>
<p>$$<br>{s_{dw}} = \beta {s_{dw}} + (1 - \beta )d{W^2} \\<br>{s_{db}} = \beta {s_{db}} + (1 - \beta )d{b^2}<br>$$</p>
<p>$$<br>W = W - \alpha \frac {dW} { \sqrt {s_{dw}} + \varepsilon } \\<br>b = b - \alpha \frac {db} { \sqrt{s_{db}}  + \varepsilon }<br>$$</p>
<blockquote>
<p>这样做，能给保留微分平方的加权平均数</p>
<p>算法的主要思想就用上面的公式表达完毕了。在上面的公式中 ${s_{dw}}$ 和 ${s_{db}}$ 分别是损失函数在前 t−1 轮迭代过程中累积的梯度梯度动量，β 是梯度累积的一个指数。所不同的是，RMSProp 算法对梯度计算了<strong>微分平方加权平均数</strong>。这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快。（比如当 dW 或者 db 中有一个值比较大的时候，那么我们在更新权重或者偏置的时候除以它之前累积的梯度的平方根，这样就可以使得更新幅度变小）。为了防止分母为零，使用了一个很小的数值 $\epsilon$ 来进行平滑，一般取值为 $10^{-8}$。</p>
</blockquote>
<h2 id="5-Adam-Adaptive-Moment-Estimation"><a href="#5-Adam-Adaptive-Moment-Estimation" class="headerlink" title="5. Adam (Adaptive Moment Estimation)"></a>5. Adam (Adaptive Moment Estimation)</h2><p>有了上面两种优化算法，一种可以使用类似于物理中的动量来累积梯度，另一种可以使得收敛速度更快同时使得波动的幅度更小。那么讲两种算法结合起来所取得的表现一定会更好。Adam（Adaptive Moment Estimation）算法是将 Momentum算法 和 RMSProp算法 结合起来使用的一种算法，我们所使用的参数基本和上面讲的一致，在训练的最开始我们需要初始化梯度的累积量和平方累积量。 </p>
<p>$$<br>{v_{dw}} = 0,{v_{db}} = 0;{s_{dw}} = 0,{s_{db}} = 0<br>$$</p>
<p>假设在训练的第 $t$ 轮训练中，我们首先可以计算得到 Momentum 和 RMSProp 的参数更新： </p>
<p>$$<br>{v_{dw}} = {\beta _1}{v_{dw}} + (1 - {\beta _1})dW \\<br>{v_{db}} = {\beta _1}{v_{db}} + (1 - {\beta _1})db \\<br>{s_{dw}} = {\beta _2}{s_{dw}} + (1 - {\beta _2})d{W^2} \\<br>{s_{db}} = {\beta _2}{s_{db}} + (1 - {\beta _2})d{b^2} \\<br>$$</p>
<p>由于移动指数平均在迭代开始的初期会导致和开始的值有较大的差异，所以我们需要对上面求得的几个值做偏差修正</p>
<p>$$<br>v_{dw}^c = \frac {v_{dw}} {1 - \beta _1^t} \\<br>v_{db}^c = \frac {v_{db}} {1 - \beta _1^t} \\<br>s_{dw}^c = \frac {s_{dw}} {1 - \beta _2^t} \\<br>s_{db}^c = \frac {s_{db}} {1 - \beta _2^t}<br>$$</p>
<p>通过上面的公式，我们就可以求得在第 $t$ 轮迭代过程中，参数梯度累积量的修正值，从而接下来就可以根据 Momentum 和 RMSProp 算法的结合来对权重和偏置进行更新.</p>
<p>$$<br>W = W - \alpha \frac {v_{dw}^c} {\sqrt {s_{dw}^c}  + \varepsilon } \\<br>b = b - \alpha \frac {v_{db}^c} {\sqrt {s_{db}^c}  + \varepsilon }<br>$$</p>
<p>上面的所有步骤就是Momentum算法和RMSProp算法结合起来从而形成Adam算法。在Adam算法中，参数 ${\beta_1}$ 所对应的就是Momentum算法中的 ${\beta}$ 值，一般取0.9，参数 ${\beta_2}$ 所对应的就是RMSProp算法中的 ${\beta}$ 值，一般我们取0.999，而 $\epsilon$ 是一个平滑项，我们一般取值为 ${10^{ - 8}}$，而学习率 $\alpha$ 则需要我们在训练的时候进行微调。</p>
<blockquote>
<p>通过上面的三个算法基本讲述了神经网络中的优化器，理解了这三个算法其他的算法也就引刃而解了.</p>
<p>Adam 优化算法 我会毫不犹豫的推荐给你， 它是 Momentum 和 RMSprop 的结合. 事实证明，它其实解决了很多问题. </p>
<p>Adam 中的超参数 ${\beta_1}$ 、${\beta_2}$  一般不需要调整，业内经常很少有人会调整他们.  $\epsilon$ 是一个平滑项，一般取值为 ${10^{ - 8}}$, 基本更不需要调整.</p>
</blockquote>
<p>Adaptive Moment Estimation</p>
<blockquote>
<p>${\beta_1}$ 用于计算这个微分 (computing the mean of the derivatives, this is called the first moment).</p>
<p>${\beta_2}$ 用于计算平方数的指数加权平均数 (compute exponentially weighted average of the squares. this is called the second moment)</p>
<p>So that gives rise to the name <code>Adaptive Moment Estimation</code>.</p>
</blockquote>
<h2 id="6-Learning-rate-decay"><a href="#6-Learning-rate-decay" class="headerlink" title="6. Learning rate decay"></a>6. Learning rate decay</h2><p>之前算法中提到的学习率α都是一个常数，这样有可能会一个问题，就是刚开始收敛速度刚刚好，可是在后面收敛过程中学习率偏大，导致不能完全收敛，而是在最低点来回波动。所以为了解决这个问题，需要让学习率能够随着迭代次数的增加进行衰减，常见的计算公式有如下几种:</p>
<p>$$<br>α = \frac {1} {1+decay_rate*epoch_num} α_0<br>$$</p>
<blockquote>
<p>对我而言，学习率衰减并不是我尝试的要点, 设置一个固定的 α， 然后好好调整，会有很大影响， 学习率衰减的确大有裨益, 有时候它可以加快训练, 但这并不是我会率先尝试的内容. 下周我们会重点介绍 如何管理和高效搜索 超参数.</p>
<p>For me，I would say that learning rate decay usually lower down on the list of things I try. learning rate decay does help. Sometimes it can really help speed up training. it is a little bit lower down my list in terms of the thingsI would try.</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="https://blog.csdn.net/u014595019/article/details/52989301" target="_blank" rel="external">深度学习笔记：优化方法总结(BGD,SGD,Momentum,AdaGrad,RMSProp,Adam)</a></li>
<li><a href="https://blog.csdn.net/BVL10101111/article/details/72614711" target="_blank" rel="external">Deep Learning 之 最优化方法</a></li>
<li><a href="https://blog.csdn.net/willduan1/article/details/78070086" target="_blank" rel="external">深度学习优化算法解析(Momentum, RMSProp, Adam)</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2017-09-25-3" target="_blank" rel="external">机器之心 - 神经网络训练中，傻傻分不清Epoch、Batch Size和迭代</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Improving Deep Neural Networks (week1) - 深度学习的实用层面]]></title>
      <url>http://sggo.me/2018/07/19/deeplearning/Improving-Deep-Neural-Networks-week1/</url>
      <content type="html"><![CDATA[<p>这次我们要学习专项课程中第二门课 Improving Deep Neural Networks</p>
<p>学完这门课之后，你将会:</p>
<blockquote>
<ul>
<li>能够高效地使用神经网络<strong>通用</strong>的技巧，包括 <code>初始化、L2和dropout正则化、Batch归一化、梯度检验</code>。</li>
<li>能够实现并应用各种<strong>优化</strong>算法，例如 <code>Mini-batch、Momentum、RMSprop、Adam，并检查它们的收敛程度</code>。</li>
<li>理解深度学习时代关于如何 <strong>构建训练/开发/测试集</strong> 以及 <strong>偏差/方差分析</strong> 最新最有效的方法.</li>
<li>能够用TensorFlow实现一个神经网络</li>
</ul>
</blockquote>
<p>这门课将会详尽地介绍深度学习的基本原理，而不仅仅只进行理论概述.</p>
<a id="more"></a>
<p>本周主要内容包括:</p>
<blockquote>
<ol>
<li>Data set partition</li>
<li>Bias / Variance</li>
<li>Regularization</li>
<li>Normalization</li>
<li>Gradient Checking</li>
</ol>
</blockquote>
<h2 id="1-Train-dev-test"><a href="#1-Train-dev-test" class="headerlink" title="1. Train/dev/test"></a>1. Train/dev/test</h2><p>在上一周的内容中, 介绍了神经网络中的常用符号以及各种变量的维度. 不清楚的可以回顾上周的笔记内容.</p>
<h3 id="1-1-Data-set-partition"><a href="#1-1-Data-set-partition" class="headerlink" title="1.1 Data set partition"></a>1.1 Data set partition</h3><p>在训练完一个模型时, 我们需要知道这个模型预测的效果. 此时就需要一个额外的数据集, 我们称为 dev/hold out/validation set, 这里我们就统一称之为<code>验证集</code>. </p>
<blockquote>
<p>如果我们需要知道模型最终效果的无偏估计, 那么我们还需要一个测试集. </p>
<p>在以往传统的机器学习中, 我们通常按照 70/30 来数据集分为 <code>Train set</code>/<code>Validation set</code>, 或者按照 60/20/20 的比例分为 <code>Train/Validation/Test</code>. </p>
<p>但在今天机器学习问题中, 我们可用的<strong>数据集的量级非常大</strong> (例如有 100W 个样本). 这时我们就<strong>不需要给验证集和测试集太大的比例, 例如 98/1/1</strong>.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-1_1.png" width="750"></p>
<h3 id="1-2-Data-src-distribution"><a href="#1-2-Data-src-distribution" class="headerlink" title="1.2 Data src distribution"></a>1.2 Data src distribution</h3><p>在划分数据集中, 有一个比较常见的错误就是不小心使得在<code>训练集</code>中的数据和<code>验证</code>或<code>测试</code>集中的数据来自于不同的分布. 例如我们想要做一个猫的分类器, 在划分数据的时候发现<code>训练集</code>中的图片全都是来自于网页, 而<code>验证集</code>和<code>测试集</code>中的数据全都来自于用户. 这是一种完全错误的做法, 在实际中一定要杜绝.</p>
<p><img src="/images/deeplearning/C2W1-2_1.png" width="750"></p>
<h2 id="2-Bias-Variance"><a href="#2-Bias-Variance" class="headerlink" title="2. Bias / Variance"></a>2. Bias / Variance</h2><p>关于 Bias / Variance 相比大家都很熟悉了, 在机器学习的课程中也已经学习到. 下面祭出 Andrew Ng 经典的图例解释:</p>
<p><img src="/images/deeplearning/C2W1-3_1.png" width="750"></p>
<p>我们该如何定位模型所处的问题? 如下图所示, 这里举了四中情况下的训练集和验证集误差.</p>
<ul>
<li>当 训练误差很小, 验证误差很大时 为 High Variance</li>
<li>当 训练误差 和 验证误差 接近 且 都很大 时为 High Bias</li>
<li>当 训练误差很大, 验证误差更大时为 High Variance &amp;&amp; High Bias</li>
<li>当 训练误差 和 验证误差接近且都很小时为 Low Variance &amp;&amp; Low Bias</li>
</ul>
<p><img src="/images/deeplearning/C2W1-4_1.png" width="750"></p>
<p>关于高方差高偏差可能是第一次听过, 如下图所示, 整体上模型处于高偏差, 但是对于一些噪声又拟合地很好. 此时就处于高偏差高方差的状态.</p>
<p><img src="/images/deeplearning/C2W1-5_1.png" width="750"></p>
<p>当我们学会定位模型的问题后, 那么该怎样解决对应的问题呢? 见下图:</p>
<p><img src="/images/deeplearning/C2W1-6_1.png" width="750"></p>
<blockquote>
<p>若 <strong>High bias</strong>, 我们可以增加模型的复杂度<strong>例如使用一个”更大”的网络结构或者训练更久一点</strong>.<br>如 <strong>High variance</strong>, 我们可以想办法 <strong>get more data</strong>, 或者使用接下来我们要讲的 <code>Regularization</code>.</p>
</blockquote>
<h2 id="3-Regularization"><a href="#3-Regularization" class="headerlink" title="3. Regularization"></a>3. Regularization</h2><p>为什么正则化没有加 $\frac{\lambda}{2m} b^2$:</p>
<blockquote>
<p>因为 $w$ 通常是一个高维参数矢量, 已经可以表达 <strong>High bias</strong> 的问题, $w$ 可能含有很多参数，我们不可能拟合所有参数, 而 $b$ 只是单个数字, 所以 $w$ 几乎覆盖了所有参数，而不是 $b$, 如果加了 $b$ 也没有影响，因为 $b$ 只是众多参数中的一个.</p>
</blockquote>
<p>关于 L1 regularization :</p>
<blockquote>
<p>如果用的是 L1 regularization, then $w$ will end up being sprase 稀疏的, 也就是说 $w$ 向量中有很多 0. 有人说这样有利于压缩模型，但是我觉得不是很合适. 越来越多的人使用 L2.</p>
<p>Notes: 不称为:矩阵 L2 范数， 按照惯例我们称为: <strong>Frobenius norm of a matrix</strong>, 其实就是 : 矩阵 L2 范数。</p>
</blockquote>
<h3 id="3-1-L2-regularization"><a href="#3-1-L2-regularization" class="headerlink" title="3.1 L2 regularization"></a>3.1 L2 regularization</h3><blockquote>
<p>L2 regularization 下的 Cost Function 如下所示, 只需要添加正则项 <strong>$\frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2_F$</strong>, 其中 F 代表 Frobenius Norm. 在添加了正则项之后, 相应的梯度也要变化, 所以在更新参数的时候需要加上对应的项. 这里注意一点, 我们只对参数 $w$ 正则, 而不对 $b$. 因为对于每一层来说, $w$ 有很高的维度, 而 $b$ 只是一个标量. $w$ 对整个模型的影响远大于 $b$.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-7_1.png" width="750"></p>
<p>下面给出添加 regularization 为什么能防止过拟合给出直观的解释. 如下图所示:</p>
<blockquote>
<p>当我们的 λ 比较大的时候, 模型就会加大对 w 的惩罚, 这样有些 w 就会变得很小 (L2 Regularization 也叫权重衰减, <strong>weights decay</strong>). 从下图左边的神经网络来看, 效果就是整个神经网络变得简单了(一些隐藏层甚至 $w$ 趋向于 0), 从而降低了过拟合的风险.</p>
<p>那些 隐藏层 并没有被消除，只是影响变得更小了，神经网络变得简单了.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-8_1.png" width="750"></p>
<blockquote>
<p>从另一个角度来看. 以 tanh激活函数 为例, 当 $λ$ 增加时, $w$ 会偏小, 这样 $z = wa +b$ 也会偏小, 此时的激活函数大致是线性的. 这样模型的复杂度也就降低了, 即降低了过拟合的风险.</p>
<p>如果神经网络每层都是线性的，其实整个还是一个线性的, 即使是一个很深的网络，因为线性激活函数的特征，最终我们只能计算线性函数.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-9_1.png" width="750"></p>
<h3 id="3-2-Dropout"><a href="#3-2-Dropout" class="headerlink" title="3.2 Dropout"></a>3.2 Dropout</h3><p>dropout 也是一种正则化的手段, 在训练时以 1-keep_prob 随机地”丢弃”一些节点. 如下图所示.</p>
<p><img src="/images/deeplearning/C2W1-10_1.png" width="600"></p>
<blockquote>
<p>具体可参考如下实现方式, 在前向传播时将 $a$ 中的某些值置为0, 为了保证大概的大小不受添加 dropout 影响, 再将处理后的 $a$ 除以 keep_prob.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-11_1.png" width="750"></p>
<blockquote>
<p>dropout 将产生收缩权重的平方范数的效果, 和 L2 类似，实施 dropout 的结果是它会压缩权重，并完成一些预防过拟合的外层正则化，事实证明 dropout 被正式地作为一种正则化的替代形式</p>
<p>L2 对不同权重的衰减是不同的，它取决于倍增的激活函数的大小.</p>
<p>dropout 的功能类似于 L2 正则化. 甚至 dropout 更适用于不同的输入范围.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-11_2.png" width="700"></p>
<blockquote>
<p>Notes: 每一层的 keep_prob 可能是不同的, keep_prob 取 1， 则是该层保留所有单元. </p>
<p>输出层的 keep_prob 经常设置为 1，有时候也可以设置为 1.9 (&gt;1). &lt; 1 通常在输出层是不太可能的. </p>
<p>输入层的 keep_prob 经常设置为 1，有时候也可以设置为 0.9， 如果是 0.5 消减一半，通常是不可能的.</p>
<p>其他 : 计算机视觉的人员非常钟情 dropout 函数.</p>
<p>Notes: dropout 的一大缺点就是 J 不会被明确定义. 每次迭代都会被随机删除一些节点. 如果再三检查梯度下降的性能，实际上是很难复查的.</p>
<p>定义明确的代价函数，每次迭代都会下降. 因为 dropout 使得 J 没有被明确定义，或者在某种程度上很难计算. 所以我们失去了调试工具，我通常会关闭 dropout. keep_prob 设置为 1， 运行代码，确保 J 函数单调递减, 然后在打开 dropout, 在 dropout 的过程中，代码并未引入bug.</p>
</blockquote>
<p>实现代码(未完成)</p>
<h3 id="3-3-Other-Regularization"><a href="#3-3-Other-Regularization" class="headerlink" title="3.3 Other Regularization"></a>3.3 Other Regularization</h3><ul>
<li>Data augmentation</li>
</ul>
<p><img src="/images/deeplearning/C2W1-12_1.png" width="600"></p>
<ul>
<li>Early stopping</li>
</ul>
<p><img src="/images/deeplearning/C2W1-13_1.png" width="600"></p>
<blockquote>
<p>W 开始是变小的，之后会随着迭代越来越大. early stopping 就是在中间点停止迭代过程.</p>
<p>Notes: </p>
<ol>
<li>early stopping  缺点是 提早停止，w 是防止了过拟合，但是 J 没有被继续下降.</li>
<li>L2 正则化 的缺点是，要用大量精力搜索合适的 λ .</li>
</ol>
<p>我个人也是更倾向于使用 L2，如果你可以负担大量的计算代价.</p>
</blockquote>
<h2 id="4-Normalization"><a href="#4-Normalization" class="headerlink" title="4. Normalization"></a>4. Normalization</h2><p><img src="/images/deeplearning/C2W1-14_1.png" width="600"></p>
<blockquote>
<ol>
<li>0 均值化 </li>
<li>归一化 方差</li>
</ol>
<p>上图2， 特征 x1 的方差 比 特征 x2 的方差 大很多<br>上图3， 特征 x1 和 特征 x2 的 方差 都是 1</p>
<p>注意: 不论 训练集 和 测试集，都是通过相同的 $\mu$ 和 ${\sigma}^2$ 定义的相同数据转换, 其中 $\mu$ 和 ${\sigma}^2$ 是由训练数据计算而来.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-15_1.png" width="700"></p>
<h2 id="5-Vanishing-Exploding-gradients"><a href="#5-Vanishing-Exploding-gradients" class="headerlink" title="5. Vanishing/Exploding gradients"></a>5. Vanishing/Exploding gradients</h2><p>Vanishing/Exploding gradients 指的是随着前向传播不断地进行, 激活单元的值会逐层指数级地增加或减小, 从而导致梯度无限增大或者趋近于零, 这样会严重影响神经网络的训练. 如下图.</p>
<p><img src="/images/deeplearning/C2W1-16_1.png" width="750"></p>
<p>为了直观理解梯度消失和梯度爆炸，我们假设所有激活函数为线性激活函数，即 $g(z)=z$。 并假设前 L−1 个权重矩阵都相等, 即为 $W_{linear}$，所以可以得到 $y_{hat}=W_{linear}^{L-1}W_{L}X$</p>
<p>假设 $W_{linear}$ 都等于这个: <img src="/images/deeplearning/C2W1-16_2.jpg" alt=""></p>
<p>那么则有 $y_{hat}=1.5^{L-1}W_LX$，很显然当 L 很大时则会出现梯度爆炸。</p>
<p>同理若将权重的值设置为小于1，那么则会出现梯度消失。</p>
<blockquote>
<p>一个可以减小这种情况发生的方法, 就是用有效的参数初始化 (该方法并不能完全解决这个问题). 但是也是有意义的</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-17_1.png" width="750"></p>
<blockquote>
<p>设置合理的权重，希望你设置的权重矩阵，既不会增长过快，也不会下降过快到 0.</p>
<p>想更加了解如何初始化权重可以看下这篇文章 <a href="http://www.cnblogs.com/marsggbo/p/7462682.html" target="_blank" rel="external">神经网络权重初始化问题</a>，其中很详细的介绍了权重初始化问题。</p>
</blockquote>
<h2 id="6-Gradient-checking-implementation"><a href="#6-Gradient-checking-implementation" class="headerlink" title="6. Gradient checking implementation"></a>6. Gradient checking implementation</h2><p><img src="/images/deeplearning/C2W1-18_1.png" width="700"></p>
<p><img src="/images/deeplearning/C2W1-19_1.png" width="700"></p>
<p><img src="/images/deeplearning/C2W1-20_1.png" width="700"></p>
<blockquote>
<p>很难用梯度检验来双重检验 dropout 的计算， 所以我不同时使用梯度检验和 dropout，除非 dropout keep.prob 设置为 1.</p>
<p>我建议关闭 dropout 用梯度检验进行双重检查.</p>
<p>在没有 dropout 的情况下，确保你的算法是正确的，然后再打开 dropout.</p>
<p>现实中 几乎不会出现, 当 w 和 b 接近 0 时，梯度下降的实施是正确的.</p>
</blockquote>
<h2 id="8-Reference"><a href="#8-Reference" class="headerlink" title="8. Reference"></a>8. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="https://daniellaah.github.io/2017/deeplearning-ai-Improving-Deep-Neural-Networks-week1.html" target="_blank" rel="external">deeplearning.ai 专项课程二第一周</a></li>
<li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="external">Coursera - Deep Learning Specialization</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Neural Networks and Deep Learning (week4) - Deep Neural Networks]]></title>
      <url>http://sggo.me/2018/07/15/deeplearning/Neural-Networks-and-Deep-Learning-week4/</url>
      <content type="html"><![CDATA[<p>本周重点任务是使用Python要实现一个任意层的神经网络, 并在cat数据上测试.</p>
<a id="more"></a>
<h2 id="1-深度神经网络中的常用符号回顾"><a href="#1-深度神经网络中的常用符号回顾" class="headerlink" title="1. 深度神经网络中的常用符号回顾"></a>1. 深度神经网络中的常用符号回顾</h2><p>在上一周的内容中, 介绍了神经网络中的常用符号以及各种变量的维度. 不清楚的可以回顾上周的笔记内容.</p>
<p><img src="/images/deeplearning/C1W4-1_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W4-2_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W4-3_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W4-4_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W4-5_1.png" width="750"></p>
<h2 id="2-Intuition-about-deep-representation"><a href="#2-Intuition-about-deep-representation" class="headerlink" title="2. Intuition about deep representation"></a>2. Intuition about deep representation</h2><p>关于深度神经网络直观地解释这部分笔记暂略, 请直接观看课程视频内容: Why deep representation?.</p>
<p><img src="/images/deeplearning/C1W4-6_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W4-7_1.png" width="750"></p>
<h2 id="3-深度神经网络中的前向-反向传播"><a href="#3-深度神经网络中的前向-反向传播" class="headerlink" title="3. 深度神经网络中的前向/反向传播"></a>3. 深度神经网络中的前向/反向传播</h2><p>在第三周的笔记中详细介绍了神经网络的前向/反向传播, 这里完全套用, 只是多了层数而已. </p>
<blockquote>
<p>需要再详细了解手推的同学可以仔细研究上周的笔记内容</p>
</blockquote>
<p><img src="/images/deeplearning/C1W4-8_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W4-9_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W4-10_1.png" width="750"></p>
<h2 id="4-参数与超参数"><a href="#4-参数与超参数" class="headerlink" title="4. 参数与超参数"></a>4. 参数与超参数</h2><p>在神经网络中参数指的是 $W$, $b$, 这两个参数是通过梯度下降算法不断优化的. 而超参数指的是学习率, 迭代次数, 决定神经网络结构的参数以及激活函数的选择等等, 在后面我们还会提到 momentum, minibatch size, regularization等等. 这些都属于超参数, 需要我们手动设定.</p>
<p><img src="/images/deeplearning/C1W4-11_1.png" width="750"></p>
<blockquote>
<p>这些超参数也决定了最终的参数 $W$, $b$. 不同的超参数的选择会导致模型很大的差别. </p>
<p>所以超参数的选择也非常重要 (后面的课程会讲解如何选择超参数).</p>
</blockquote>
<p><img src="/images/deeplearning/C1W4-12_1.png" width="750"></p>
<h2 id="5-使用Python实现深度神经网络"><a href="#5-使用Python实现深度神经网络" class="headerlink" title="5. 使用Python实现深度神经网络"></a>5. 使用Python实现深度神经网络</h2><p>DeepNeuralNetwork.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z)</span>:</span></span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z)</span><br><span class="line">    <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leaky_relu</span><span class="params">(Z)</span>:</span></span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z)</span><br><span class="line">    A[Z &lt; <span class="number">0</span>] = <span class="number">0.01</span> * Z</span><br><span class="line">    <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepNeuralNetwork</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers_dim, activations)</span>:</span></span><br><span class="line">        <span class="comment"># assert (layers_dim[-1] == 1)</span></span><br><span class="line">        <span class="comment"># assert (activations[-1] == 'sigmoid')</span></span><br><span class="line">        <span class="comment"># assert (len(activations) == len(layers_dims)-1)</span></span><br><span class="line">        np.random.seed(<span class="number">1</span>)</span><br><span class="line">        self.layers_dim = layers_dim</span><br><span class="line">        self.__num_layers = len(layers_dim)</span><br><span class="line">        self.activations = activations</span><br><span class="line">        self.input_size = layers_dim[<span class="number">0</span>]</span><br><span class="line">        self.parameters = self.__parameters_initializer(layers_dim)</span><br><span class="line">        self.output_size = layers_dim[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__parameters_initializer</span><span class="params">(self, layers_dim)</span>:</span></span><br><span class="line">        <span class="comment"># special initialzer with np.sqrt(layers_dims[l-1])</span></span><br><span class="line">        L = len(layers_dim)</span><br><span class="line">        parameters = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">            parameters[<span class="string">'W'</span>+str(l)] = np.random.randn(layers_dim[l], layers_dim[l<span class="number">-1</span>]) / np.sqrt(layers_dims[l<span class="number">-1</span>])</span><br><span class="line">            parameters[<span class="string">'b'</span>+str(l)] = np.zeros((layers_dim[l], <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__one_layer_forward</span><span class="params">(self, A_prev, W, b, activation)</span>:</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'sigmoid'</span>:</span><br><span class="line">            A = sigmoid(Z)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'relu'</span>:</span><br><span class="line">            A = relu(Z)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'leaky_relu'</span>:</span><br><span class="line">            A = leaky_relu(Z)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'tanh'</span>:</span><br><span class="line">            A = np.tanh(Z)</span><br><span class="line">        cache = &#123;<span class="string">'Z'</span>: Z, <span class="string">'A'</span>: A&#125;</span><br><span class="line">        <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__forward_propagation</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        caches = []</span><br><span class="line">        A_prev = X</span><br><span class="line">        caches.append(&#123;<span class="string">'A'</span>: A_prev&#125;)</span><br><span class="line">        <span class="comment"># forward propagation by laryer</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, len(self.layers_dim)):</span><br><span class="line">            W, b = self.parameters[<span class="string">'W'</span>+str(l)], self.parameters[<span class="string">'b'</span>+str(l)]</span><br><span class="line">            A_prev, cache = self.__one_layer_forward(A_prev, W, b, self.activations[l<span class="number">-1</span>])</span><br><span class="line">            caches.append(cache)</span><br><span class="line">        AL = caches[<span class="number">-1</span>][<span class="string">'A'</span>]</span><br><span class="line">        <span class="keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__compute_cost</span><span class="params">(self, AL, Y)</span>:</span></span><br><span class="line">        m = Y.shape[<span class="number">1</span>]</span><br><span class="line">        cost = -np.sum(Y*np.log(AL) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-AL)) / m</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="comment"># use the result from forward propagation and the label Y to compute cost</span></span><br><span class="line">        <span class="keyword">assert</span> (self.input_size == X.shape[<span class="number">0</span>])</span><br><span class="line">        AL, _ = self.__forward_propagation(X)</span><br><span class="line">        <span class="keyword">return</span> self.__compute_cost(AL, Y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(self, dA, Z)</span>:</span></span><br><span class="line">        s = sigmoid(Z)</span><br><span class="line">        dZ = dA * s*(<span class="number">1</span>-s)</span><br><span class="line">        <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(self, dA, Z)</span>:</span></span><br><span class="line">        dZ = np.array(dA, copy=<span class="keyword">True</span>)</span><br><span class="line">        dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">leaky_relu_backward</span><span class="params">(self, dA, Z)</span>:</span></span><br><span class="line">        dZ = np.array(dA, copy=<span class="keyword">True</span>)</span><br><span class="line">        dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0.01</span></span><br><span class="line">        <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tanh_backward</span><span class="params">(self, dA, Z)</span>:</span></span><br><span class="line">        s = np.tanh(Z)</span><br><span class="line">        dZ = <span class="number">1</span> - s*s</span><br><span class="line">        <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__linear_backward</span><span class="params">(self, dZ, A_prev, W)</span>:</span></span><br><span class="line">        <span class="comment"># assert(dZ.shape[0] == W.shape[0])</span></span><br><span class="line">        <span class="comment"># assert(W.shape[1] == A_prev.shape[0])</span></span><br><span class="line">        m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line">        dW = np.dot(dZ, A_prev.T) / m</span><br><span class="line">        db = np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) / m</span><br><span class="line">        dA_prev = np.dot(W.T, dZ)</span><br><span class="line">        <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__activation_backward</span><span class="params">(self, dA, Z, activation)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> (dA.shape == Z.shape)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'sigmoid'</span>:</span><br><span class="line">            dZ = self.sigmoid_backward(dA, Z)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'relu'</span>:</span><br><span class="line">            dZ = self.relu_backward(dA, Z)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'leaky_relu'</span>:</span><br><span class="line">            dZ = self.leaky_relu_backward(dA, Z)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'tanh'</span>:</span><br><span class="line">            dZ = self.tanh_backward(dA, Z)</span><br><span class="line">        <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__backward_propagation</span><span class="params">(self, caches, Y)</span>:</span></span><br><span class="line">        m = Y.shape[<span class="number">1</span>]</span><br><span class="line">        L = self.__num_layers</span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        <span class="comment"># backward propagate last layer</span></span><br><span class="line">        AL, A_prev = caches[L<span class="number">-1</span>][<span class="string">'A'</span>], caches[L<span class="number">-2</span>][<span class="string">'A'</span>]</span><br><span class="line">        dAL =  - (Y/AL - (<span class="number">1</span>-Y)/(<span class="number">1</span>-AL))</span><br><span class="line">        grads[<span class="string">'dZ'</span>+str(L<span class="number">-1</span>)] = self.__activation_backward(dAL, caches[L<span class="number">-1</span>][<span class="string">'Z'</span>], self.activations[<span class="number">-1</span>])</span><br><span class="line">        grads[<span class="string">'dA'</span>+str(L<span class="number">-2</span>)], \</span><br><span class="line">        grads[<span class="string">'dW'</span>+str(L<span class="number">-1</span>)], \</span><br><span class="line">        grads[<span class="string">'db'</span>+str(L<span class="number">-1</span>)] = self.__linear_backward(grads[<span class="string">'dZ'</span>+str(L<span class="number">-1</span>)],</span><br><span class="line">                                                      A_prev, self.parameters[<span class="string">'W'</span>+str(L<span class="number">-1</span>)])</span><br><span class="line">        <span class="comment"># backward propagate by layer</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(<span class="number">1</span>, L<span class="number">-1</span>)):</span><br><span class="line">            grads[<span class="string">'dZ'</span>+str(l)] = self.__activation_backward(grads[<span class="string">'dA'</span>+str(l)],</span><br><span class="line">                                                            caches[l][<span class="string">'Z'</span>],</span><br><span class="line">                                                            self.activations[l<span class="number">-1</span>])</span><br><span class="line">            A_prev = caches[l<span class="number">-1</span>][<span class="string">'A'</span>]</span><br><span class="line">            grads[<span class="string">'dA'</span>+str(l<span class="number">-1</span>)], \</span><br><span class="line">            grads[<span class="string">'dW'</span>+str(l)], \</span><br><span class="line">            grads[<span class="string">'db'</span>+str(l)] = self.__linear_backward(grads[<span class="string">'dZ'</span>+str(l)], A_prev, self.parameters[<span class="string">'W'</span>+str(l)])</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__update_parameters</span><span class="params">(self, grads, learning_rate)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, self.__num_layers):</span><br><span class="line">            <span class="comment"># assert (self.parameters['W'+str(l)].shape == grads['dW'+str(l)].shape)</span></span><br><span class="line">            <span class="comment"># assert (self.parameters['b'+str(l)].shape == grads['db'+str(l)].shape)</span></span><br><span class="line">            self.parameters[<span class="string">'W'</span>+str(l)] -= learning_rate * grads[<span class="string">'dW'</span>+str(l)]</span><br><span class="line">            self.parameters[<span class="string">'b'</span>+str(l)] -= learning_rate * grads[<span class="string">'db'</span>+str(l)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, num_iterations, learning_rate, print_cost=False, print_num=<span class="number">100</span>)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">            <span class="comment"># forward propagation</span></span><br><span class="line">            AL, caches = self.__forward_propagation(X)</span><br><span class="line">            <span class="comment"># compute cost</span></span><br><span class="line">            cost = self.__compute_cost(AL, Y)</span><br><span class="line">            <span class="comment"># backward propagation</span></span><br><span class="line">            grads = self.__backward_propagation(caches, Y)</span><br><span class="line">            <span class="comment"># update parameters</span></span><br><span class="line">            self.__update_parameters(grads, learning_rate)</span><br><span class="line">            <span class="comment"># print cost</span></span><br><span class="line">            <span class="keyword">if</span> i % print_num == <span class="number">0</span> <span class="keyword">and</span> print_cost:</span><br><span class="line">                    <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_prob</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        A, _ = self.__forward_propagation(X)</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        pred_prob = self.predict_prob(X)</span><br><span class="line">        threshold_func = np.vectorize(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        Y_prediction = threshold_func(pred_prob)</span><br><span class="line">        <span class="keyword">return</span> Y_prediction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        pred = self.predict(X)</span><br><span class="line">        <span class="keyword">return</span> len(Y[pred == Y]) / Y.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>main.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v2 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Explore your dataset</span></span><br><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>]</span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x_orig shape: "</span> + str(train_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_y shape: "</span> + str(train_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x_orig shape: "</span> + str(test_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_y shape: "</span> + str(test_y.shape))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape the training and test examples</span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T   <span class="comment"># The "-1" makes reshape flatten the remaining dimensions</span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape))</span><br><span class="line"><span class="comment"># Please note that the above code is from the programming assignment</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> DeepNeuralNetwork</span><br><span class="line">layers_dims = (<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># layers_dims = (12288, 10, 1)</span></span><br><span class="line"><span class="comment"># layers_dims = [12288, 20, 7, 5, 1] #  5-layer model</span></span><br><span class="line">activations = [<span class="string">'relu'</span>, <span class="string">'relu'</span>, <span class="string">'relu'</span>,<span class="string">'sigmoid'</span>]</span><br><span class="line">num_iter = <span class="number">2500</span></span><br><span class="line">learning_rate = <span class="number">0.0075</span></span><br><span class="line"></span><br><span class="line">clf = DeepNeuralNetwork(layers_dims, activations)\</span><br><span class="line">            .fit(train_x, train_y, num_iter, learning_rate, <span class="keyword">True</span>, <span class="number">100</span>)</span><br><span class="line">print(<span class="string">'train accuracy: &#123;:.2f&#125;%'</span>.format(clf.accuracy_score(train_x, train_y)*<span class="number">100</span>))</span><br><span class="line">print(<span class="string">'test accuracy: &#123;:.2f&#125;%'</span>.format(clf.accuracy_score(test_x, test_y)*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># Cost after iteration 0: 0.771749</span></span><br><span class="line"><span class="comment"># Cost after iteration 100: 0.672053</span></span><br><span class="line"><span class="comment"># Cost after iteration 200: 0.648263</span></span><br><span class="line"><span class="comment"># Cost after iteration 300: 0.611507</span></span><br><span class="line"><span class="comment"># Cost after iteration 400: 0.567047</span></span><br><span class="line"><span class="comment"># Cost after iteration 500: 0.540138</span></span><br><span class="line"><span class="comment"># Cost after iteration 600: 0.527930</span></span><br><span class="line"><span class="comment"># Cost after iteration 700: 0.465477</span></span><br><span class="line"><span class="comment"># Cost after iteration 800: 0.369126</span></span><br><span class="line"><span class="comment"># Cost after iteration 900: 0.391747</span></span><br><span class="line"><span class="comment"># Cost after iteration 1000: 0.315187</span></span><br><span class="line"><span class="comment"># Cost after iteration 1100: 0.272700</span></span><br><span class="line"><span class="comment"># Cost after iteration 1200: 0.237419</span></span><br><span class="line"><span class="comment"># Cost after iteration 1300: 0.199601</span></span><br><span class="line"><span class="comment"># Cost after iteration 1400: 0.189263</span></span><br><span class="line"><span class="comment"># Cost after iteration 1500: 0.161189</span></span><br><span class="line"><span class="comment"># Cost after iteration 1600: 0.148214</span></span><br><span class="line"><span class="comment"># Cost after iteration 1700: 0.137775</span></span><br><span class="line"><span class="comment"># Cost after iteration 1800: 0.129740</span></span><br><span class="line"><span class="comment"># Cost after iteration 1900: 0.121225</span></span><br><span class="line"><span class="comment"># Cost after iteration 2000: 0.113821</span></span><br><span class="line"><span class="comment"># Cost after iteration 2100: 0.107839</span></span><br><span class="line"><span class="comment"># Cost after iteration 2200: 0.102855</span></span><br><span class="line"><span class="comment"># Cost after iteration 2300: 0.100897</span></span><br><span class="line"><span class="comment"># Cost after iteration 2400: 0.092878</span></span><br><span class="line"><span class="comment"># train accuracy: 98.56%</span></span><br><span class="line"><span class="comment"># test accuracy: 80.00%</span></span><br></pre></td></tr></table></figure>
<h2 id="6-本周内容回顾"><a href="#6-本周内容回顾" class="headerlink" title="6. 本周内容回顾"></a>6. 本周内容回顾</h2><ul>
<li>深度神经网络中的前向/反向传播</li>
<li>参数与超参数</li>
<li>使用Python实现深度神经网络</li>
</ul>
<h2 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://daniellaah.github.io/2017/deeplearning-ai-Neural-Networks-and-Deep-Learning-week4.html" target="_blank" rel="external">deeplearning.ai 专项课程一第四周</a></li>
<li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="external">Coursera - Deep Learning Specialization</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Neural Networks and Deep Learning (week3) - Shallow Neural Networks]]></title>
      <url>http://sggo.me/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/</url>
      <content type="html"><![CDATA[<p>正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。</p>
<p>在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。</p>
<a id="more"></a>
<h2 id="1-常用符号与基本概念"><a href="#1-常用符号与基本概念" class="headerlink" title="1. 常用符号与基本概念"></a>1. 常用符号与基本概念</h2><p><img src="/images/deeplearning/C1W3-1_1.png" width="750"></p>
<blockquote>
<p>该神经网络完全可以使用上一周所讲的计算图来表示, 和 $LR$ 计算图的区别仅仅在于多了一个 $z$ 和 $a$ 的计算而已. </p>
<p>如果你已经完全掌握了上一周的内容, 那么其实你已经知道了神经网络的前向传播, 反向传播(梯度计算)等等.</p>
<p>要注意的是各种参数, 中间变量 $(a, z)$ 的维度问题. 关于神经网络的基本概念, 这里就不赘述了. 见下图回顾一下:</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-2_1.png" width="750"></p>
<h2 id="2-神经网络中的前向传播"><a href="#2-神经网络中的前向传播" class="headerlink" title="2. 神经网络中的前向传播"></a>2. 神经网络中的前向传播</h2><blockquote>
<p>我们先以一个训练样本来看神经网络中的前向传播.<br>我们只看这个神经网络中的输入层和隐藏层的第一个激活单元(如下图右边所示). 其实这就是一个Logistic Regression. </p>
<ol>
<li>神经网络中输入层和隐藏层 (不看输出层), 这就不就是四个LR放在一起吗? </li>
<li>在 LR 中 $z$ 和 $a$ 的计算我们已经掌握了, 那么在神经网络中 $z$ 和 $a$ 又是什么呢? </li>
</ol>
<p><strong>我们记隐藏层第一个 $z$ 为 $z_1$, 第二个 $z$ 记为 $z_2$ 以此类推</strong>.<br>只要将这四个 $z$ 纵向叠加在一起称为一个<strong><code>列向量</code> 即可得到神经网络中这一层的 $z$</strong> ($a$同理).</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-3_1.png" width="750"></p>
<p>那么这一层的 $w, b$ 又是如何得到的? 别忘了, 对于参数 $w$ 来说, 它本身就是一个列项量, 那么它是如何做纵向叠加的呢? 我们只需要将其转置变成一个横向量, 再纵向叠加即可.</p>
<p><img src="/images/deeplearning/C1W3-4_1.png" width="750"></p>
<p>得到隐藏层的 $a$ 之后, 我们可以将其视为输入, 现只看神经网络的隐藏层和输出层, 我们发现这不就是个 $LR$ 嘛.</p>
<p><img src="/images/deeplearning/C1W3-5_1.png" width="750"></p>
<p>这里总结一下各种变量的维度 (注意: 这里是针对一个训练样本来说的, $n_L$ 代表的 $L$ 层的节点个数):</p>
<ul>
<li>$w.shape : (n_L, n_{(L-1)})$</li>
<li>$b.shape : (n_L, 1)$</li>
<li>$z.shape : (n_L, 1)$</li>
<li>$a.shape : (n_L, 1)$</li>
</ul>
<p>那么如果有 $m$ 个训练样本这些变量的维度又是怎样的呢. 我们思考哪些变量的维度会随着样本数的变化而变化. $w$ 是参数显然它的维度是不会变的. 而输入每一个样本都会有一个 $z$ 和 $a$, 还记得 $X$ 的形式吗? 同样地, $Z$ 就是将每个样本算出来的 $z$ 横向叠加(A同理). 具体计算过程如下图:</p>
<p><img src="/images/deeplearning/C1W3-6_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W3-7_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W3-8_1.png" width="750"></p>
<h2 id="3-神经网络中的激活函数"><a href="#3-神经网络中的激活函数" class="headerlink" title="3. 神经网络中的激活函数"></a>3. 神经网络中的激活函数</h2><p>四种常用的激活函数: Sigmoid, Tanh, ReLU, Leaky ReLU.</p>
<p>其中 sigmoid 我们已经见过了, 它的输出可以看成一个概率值, 往往用在输出层. <strong>对于中间层来说, 往往是<code>ReLU</code>的效果最好.</strong></p>
<blockquote>
<p>Tanh 数据平均值为 0，具有数据中心化的效果，几乎在任何场合都优于 Sigmoid</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-9_1.png" width="750"></p>
<p>以上激活函数的导数请自行在草稿纸上推导.</p>
<p><img src="/images/deeplearning/C1W3-10_1.png" width="750"></p>
<blockquote>
<p>derivative of <strong><code>sigmoid</code></strong> activation function</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-11_1.png" width="750"></p>
<blockquote>
<p>derivative of <strong><code>tanh</code></strong> activation function</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-12_1.png" width="750"></p>
<blockquote>
<p>derivative of <strong><code>ReLU and Leaky ReLU</code></strong> activation function</p>
</blockquote>
<p>为什么需要激活函数? 如果没有激活函数, 那么不论多少层的神经网络都只相当于一个LR. 证明如下:</p>
<blockquote>
<p><strong>it turns out that if you use a linear activation function or alternatively if you don’t have an activation function, then no matter how many layers your neural network has, always doing just computing a linear activation function, so you might as well not have any hidden layers.</strong></p>
<p>so unless you throw a non-linearity in there, then you’re not computing more interesting functions.</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-13_1.png" width="750"></p>
<blockquote>
<p>你可以在隐藏层用 tanh，输出层用 sigmoid，说明不同层的激活函数可以不一样。</p>
<p>现实情况是 : <strong>the tanh is pretty much stricly superior. never use sigmoid</strong></p>
</blockquote>
<p><strong>ReLU</strong> (rectified linear unit 矫正线性单元)</p>
<blockquote>
<p>tanh 和 sigmoid 都有一个缺点，就是 z 非常大或者非常小，函数的斜率(导数梯度)就会非常小, 梯度下降很慢.</p>
<p><strong>the slope of the function you know ends up being close to zero, and so this can slow down gradient descent</strong></p>
<p><strong>ReLU (rectified linear unit) is well</strong>, z = 0 的时候，你可以给导数赋值为 0 or 1，虽然这个点是不可微的. 但<strong>实现</strong>没有影响.</p>
<p>虽然 z &lt; 0, 的时候，斜率为0， 但在实践中，有足够多的隐藏单元 令 z &gt; 0, 对大多数训练样本来说是很快的.</p>
</blockquote>
<p>Notes:</p>
<blockquote>
<p>so the one place you might use as linear activation function, others usually in the output layer.</p>
</blockquote>
<h2 id="4-神经网络中的反向传播-back-propagation"><a href="#4-神经网络中的反向传播-back-propagation" class="headerlink" title="4. 神经网络中的反向传播 back propagation"></a>4. 神经网络中的反向传播 back propagation</h2><blockquote>
<p>反向传播最主要的就是计算梯度, 在上一周的内容中, 我们已经知道了LR梯度的计算. </p>
<p>同样的方式, 我们使用<strong>计算图</strong>来计算<strong>神经网络中的各种梯度</strong>.</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-14.png" width="750"></p>
<p>$$<br>dz^{[2]} = \frac{dL}{dz}= \frac{dL}{da^{[2]}}\frac{da^{[2]}}{dz^{[2]}}=a^{[2]}-y<br>$$</p>
<p>$$<br>dW^{[2]}=\frac{dL}{dW^{[2]}}=\frac{dL}{dz^{[2]}}\frac{dz^{[2]}}{dW^{[2]}}=dz^{[2]}a^{[1]}<br>$$</p>
<p>$$<br>db^{[2]}=\frac{dL}{db^{[2]}}=\frac{dL}{dz^{[2]}}\frac{dz^{[2]}}{db^{[2]}}=dz^{[2]}<br>$$</p>
<blockquote>
<p><strong>backward propagation :</strong></p>
</blockquote>
<p>$$<br>dz^{[1]} = \frac{dL}{dz^{[2]}}\frac{dz^{[2]}}{da^{[1]}}\frac{da^{[1]}}{dz^{[1]}}=W^{[2]T}dz^{[2]}*g^{[1]’}(z^{[1]})<br>$$</p>
<p>$$<br>dW^{[1]}=\frac{dL}{dW^{[1]}}=\frac{dL}{dz^{[1]}}\frac{dz^{[1]}}{dW^{[1]}}=dz^{[1]}x^T<br>$$</p>
<p>$$<br>db^{[1]}=\frac{dL}{db^{[1]}}=\frac{dL}{dz^{[1]}}\frac{dz^{[1]}}{db^{[1]}}=dz^{[1]}<br>$$</p>
<blockquote>
<p>Notes: $\frac{dL}{dz^{[2]}} = dz^{[2]}$ ， $\frac{dz^{[2]}}{da^{[1]}} = W^{[2]}$ ， $\frac{da^{[1]}}{dz^{[1]}}=g^{[1]’}(z^{[1]})$</p>
</blockquote>
<p>下图右边为在$m$个训练样本上的向量化表达:</p>
<p><img src="/images/deeplearning/C1W3-15_1.png" width="750"></p>
<blockquote>
<p>Notes: </p>
<ul>
<li>$n^[0]$ = input features</li>
<li>$n^[1]$ = hidden units</li>
<li>$n^[2]$ = output units</li>
</ul>
</blockquote>
<h2 id="5-神经网络中的参数初始化"><a href="#5-神经网络中的参数初始化" class="headerlink" title="5. 神经网络中的参数初始化"></a>5. 神经网络中的参数初始化</h2><p>在 LR 中我们的参数 $w$ 初始化为 0, 如果在神经网络中也是用相同的初始化, 那么一个隐藏层的每个节点都是相同的, 不论迭代多少次. 这显然是不合理的, 所以我们应该<font color="red"> <strong>随机地初始化</strong></font> $w$ 从而解决这个 sysmmetry breaking problem. 破坏对称问题</p>
<p><img src="/images/deeplearning/C1W3-16_1.png" width="750"></p>
<blockquote>
<p>具体初始化代码可参见下图, 其中 <strong>乘以 0.01</strong> 是为了让参数 $w$ 较小, 加速梯度下降 </p>
<p>如激活函数为 tanh 时, 若参数较大则 $z$ 也较大, 此时的梯度接近于 0, 更新缓慢. 如不是 tanh or sigmoid 则问题不大.</p>
<p>this is a relatively shallow neural network without too many hidden layers, so 0.01 maybe work ok.</p>
<p>finally it turns out that sometimes there can be better constants than 0.01.</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-17_1.png" width="750"></p>
<blockquote>
<p>$b$ 并没有这个 sysmmetry breaking problem, 所以可以 $np.zeros((2, 1))$</p>
</blockquote>
<h2 id="6-用Python搭建简单神经网络"><a href="#6-用Python搭建简单神经网络" class="headerlink" title="6. 用Python搭建简单神经网络"></a>6. 用Python搭建简单神经网络</h2><p>使用Python+Numpy实现一个简单的神经网络. 以下为参考代码</p>
<p>SimpleNeuralNetwork.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNeuralNetwork</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># simple neural network with one hidden layer</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_layer_size)</span>:</span></span><br><span class="line">        self.paramters = self.__parameter_initailizer(input_size, hidden_layer_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__parameter_initailizer</span><span class="params">(self, n_x, n_h)</span>:</span></span><br><span class="line">        <span class="comment"># W cannot be initialized with zeros</span></span><br><span class="line">        W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">        b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">        W2 = np.random.randn(<span class="number">1</span>, n_h) * <span class="number">0.01</span></span><br><span class="line">        b2 = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'W1'</span>: W1,<span class="string">'b1'</span>: b1,<span class="string">'W2'</span>: W2,<span class="string">'b2'</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__forward_propagation</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        W1 = self.paramters[<span class="string">'W1'</span>]</span><br><span class="line">        b1 = self.paramters[<span class="string">'b1'</span>]</span><br><span class="line">        W2 = self.paramters[<span class="string">'W2'</span>]</span><br><span class="line">        b2 = self.paramters[<span class="string">'b2'</span>]</span><br><span class="line">        <span class="comment"># forward propagation</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.tanh(Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = sigmoid(Z2)</span><br><span class="line">        cache = &#123;<span class="string">'Z1'</span>: Z1,<span class="string">'A1'</span>: A1,<span class="string">'Z2'</span>: Z2,<span class="string">'A2'</span>: A2&#125;</span><br><span class="line">        <span class="keyword">return</span> A2, cache</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__compute_cost</span><span class="params">(self, A2, Y)</span>:</span></span><br><span class="line">        m = A2.shape[<span class="number">1</span>]</span><br><span class="line">        cost = -np.sum(Y*np.log(A2) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A2)) / m</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="comment"># use the result from forward propagation and the label Y to compute cost</span></span><br><span class="line">        A2, cache = self.__forward_propagation(X)</span><br><span class="line">        cost = self.__compute_cost(A2, Y)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__backward_propagation</span><span class="params">(self, cache, Y)</span>:</span></span><br><span class="line">        A1, A2 = cache[<span class="string">'A1'</span>], cache[<span class="string">'A2'</span>]</span><br><span class="line">        W2 = self.paramters[<span class="string">'W2'</span>]</span><br><span class="line">        m = X.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># backward propagation computes gradients</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">        db2 = np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) / m</span><br><span class="line">        dZ1 = np.dot(W2.T, dZ2) * (<span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">        dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">        db1 = np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) / m</span><br><span class="line">        grads = &#123;<span class="string">'dW1'</span>: dW1,<span class="string">'db1'</span>: db1,<span class="string">'dW2'</span>: dW2,<span class="string">'db2'</span>: db2&#125;</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__update_parameters</span><span class="params">(self, grads, learning_rate)</span>:</span></span><br><span class="line">        self.paramters[<span class="string">'W1'</span>] -= learning_rate * grads[<span class="string">'dW1'</span>]</span><br><span class="line">        self.paramters[<span class="string">'b1'</span>] -= learning_rate * grads[<span class="string">'db1'</span>]</span><br><span class="line">        self.paramters[<span class="string">'W2'</span>] -= learning_rate * grads[<span class="string">'dW2'</span>]</span><br><span class="line">        self.paramters[<span class="string">'b2'</span>] -= learning_rate * grads[<span class="string">'db2'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, num_iterations, learning_rate, print_cost=False, print_num=<span class="number">100</span>)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">            <span class="comment"># forward propagation</span></span><br><span class="line">            A2, cache = self.__forward_propagation(X)</span><br><span class="line">            <span class="comment"># compute cost</span></span><br><span class="line">            cost = self.cost_function(X, Y)</span><br><span class="line">            <span class="comment"># backward propagation</span></span><br><span class="line">            grads = self.__backward_propagation(cache, Y)</span><br><span class="line">            <span class="comment"># update parameters</span></span><br><span class="line">            self.__update_parameters(grads, learning_rate)</span><br><span class="line">            <span class="comment"># print cost</span></span><br><span class="line">            <span class="keyword">if</span> i % print_num == <span class="number">0</span> <span class="keyword">and</span> print_cost:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_prob</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># result of forward_propagation is the probability</span></span><br><span class="line">        A2, _ = self.__forward_propagation(X)</span><br><span class="line">        <span class="keyword">return</span> A2</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        pred_prob = self.predict_prob(X)</span><br><span class="line">        threshold_func = np.vectorize(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        Y_prediction = threshold_func(pred_prob)</span><br><span class="line">        <span class="keyword">return</span> Y_prediction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        pred = self.predict(X)</span><br><span class="line">        <span class="keyword">return</span> len(Y[pred == Y]) / Y.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>main.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br><span class="line">X, Y = load_planar_dataset()</span><br><span class="line"><span class="comment"># Please note that the above code is from the programming assignment</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> SimpleNeuralNetwork</span><br><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line">num_iter = <span class="number">10001</span></span><br><span class="line">learning_rate = <span class="number">1.2</span></span><br><span class="line">input_size = X.shape[<span class="number">0</span>]</span><br><span class="line">hidden_layer_size = <span class="number">4</span></span><br><span class="line">clf = SimpleNeuralNetwork(input_size=input_size,</span><br><span class="line">                          hidden_layer_size=hidden_layer_size)\</span><br><span class="line">        .fit(X, Y, num_iter, learning_rate, <span class="keyword">True</span>, <span class="number">1000</span>)</span><br><span class="line">train_acc = clf.accuracy_score(X, Y)</span><br><span class="line">print(<span class="string">'training accuracy: &#123;&#125;%'</span>.format(train_acc*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># Cost after iteration 0: 0.693162</span></span><br><span class="line"><span class="comment"># Cost after iteration 1000: 0.258625</span></span><br><span class="line"><span class="comment"># Cost after iteration 2000: 0.239334</span></span><br><span class="line"><span class="comment"># Cost after iteration 3000: 0.230802</span></span><br><span class="line"><span class="comment"># Cost after iteration 4000: 0.225528</span></span><br><span class="line"><span class="comment"># Cost after iteration 5000: 0.221845</span></span><br><span class="line"><span class="comment"># Cost after iteration 6000: 0.219094</span></span><br><span class="line"><span class="comment"># Cost after iteration 7000: 0.220628</span></span><br><span class="line"><span class="comment"># Cost after iteration 8000: 0.219400</span></span><br><span class="line"><span class="comment"># Cost after iteration 9000: 0.218482</span></span><br><span class="line"><span class="comment"># Cost after iteration 10000: 0.217738</span></span><br><span class="line"><span class="comment"># training accuracy: 90.5%</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> hidden_layer_size <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]:</span><br><span class="line">    clf = SimpleNeuralNetwork(input_size=input_size,</span><br><span class="line">                               hidden_layer_size=hidden_layer_size)\</span><br><span class="line">            .fit(X, Y, num_iter, learning_rate, <span class="keyword">False</span>)</span><br><span class="line">    print(<span class="string">'&#123;&#125; hidden units, cost: &#123;&#125;, accuracy: &#123;&#125;%'</span></span><br><span class="line">           .format(hidden_layer_size,</span><br><span class="line">                   clf.cost_function(X, Y),</span><br><span class="line">                   clf.accuracy_score(X, Y)))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># 1 hidden units, cost: 0.6315593779798304, accuracy: 67.5%</span></span><br><span class="line"><span class="comment"># 2 hidden units, cost: 0.5727606525435293, accuracy: 67.25%</span></span><br><span class="line"><span class="comment"># 3 hidden units, cost: 0.2521014374551156, accuracy: 91.0%</span></span><br><span class="line"><span class="comment"># 4 hidden units, cost: 0.24703039056643344, accuracy: 91.25%</span></span><br><span class="line"><span class="comment"># 5 hidden units, cost: 0.17206481441467936, accuracy: 91.5%</span></span><br><span class="line"><span class="comment"># 20 hidden units, cost: 0.16003869681611513, accuracy: 92.25%</span></span><br><span class="line"><span class="comment"># 50 hidden units, cost: 0.16000569403994763, accuracy: 92.5%</span></span><br></pre></td></tr></table></figure>
<h2 id="7-本周内容回顾"><a href="#7-本周内容回顾" class="headerlink" title="7. 本周内容回顾"></a>7. 本周内容回顾</h2><ul>
<li>学习了神经网络的基本概念</li>
<li>掌握了神经网络中各种变量的维度</li>
<li>掌握了神经网络中的前向传播与反向传播</li>
<li>了解了神经网络中的激活函数</li>
<li>学习了神经网络中参数初始化的重要性</li>
<li>掌握了使用Python实现简单的神经网络</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://daniellaah.github.io/2017/deeplearning-ai-Neural-Networks-and-Deep-Learning-week3.html" target="_blank" rel="external">deeplearning.ai 专项课程一第三周</a></li>
<li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="external">Coursera - Deep Learning Specialization</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[L1、L2 正则化小记 (not finish)]]></title>
      <url>http://sggo.me/2018/07/11/ml/1-L1-L2/</url>
      <content type="html"><![CDATA[<ul>
<li>奥卡姆剃刀: 在所有可能选择的模型中，我们应该选择能够很好的解释已知数据并且十分简单的模型</li>
<li>贝叶斯估计: 规则化项对应于模型的先验概率</li>
<li>结构风险最小化: 规则化项是结构风险最小化策略的实现，即在经验风险上加一个正则化项或惩罚项</li>
</ul>
<a id="more"></a>
<p>Supervised Learning 就是 “minimizeyour error while regularizing your parameters”，就是在规则化参数的同时最小化误差</p>
<blockquote>
<p><strong>最小化误差</strong>是为了让我们的模型拟合我们的训练数据，<strong>regularized parameters</strong>是防止我们的模型<strong>过分拟合</strong>我们的<strong>训练数据</strong>。 因为参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。但训练误差小并不是我们的最终目标，我们的<strong>目标是希望模型的测试误差小</strong>，也就是能准确的预测新的样本。</p>
<p>我们需要<strong>保证模型“<code>简单</code>”的基础上最小化训练误差</strong>，<strong>这样得到的参数才具有好的泛化性能（也就是测试误差也小）</strong>.</p>
<p>而模型<strong>“简单”就是通过规则函数来实现的</strong>。<strong>regularized item</strong>的使用还可以约束我们的模型的特性。<strong>这样就可以将人对这个模型的先验知识融入到模型的学习当中</strong>，强行地让学习到的模型具有人想要的特性，例如 稀疏、低秩、平滑 等等。</p>
<p>人的先验是非常重要的。前人的经验会让你少走很多弯路，这就是为什么我们平时学习最好找个大牛带带的原因。一句点拨可以为我们拨开眼前乌云，还我们一片晴空万里，醍醐灌顶。对机器学习也是一样，如果被我们人稍微点拨一下，它肯定能更快的学习相应的任务。只是由于人和机器的交流目前还没有那么直接的方法，目前只能由规则项来担当了.</p>
</blockquote>
<h2 id="Supervised-Learning-目标函数"><a href="#Supervised-Learning-目标函数" class="headerlink" title="Supervised Learning 目标函数"></a>Supervised Learning 目标函数</h2><p>$$<br>w^*=argmin_w\sum_iL(y_i,f(x_i;w))+\lambda\Omega(w)<br>$$</p>
<blockquote>
<p>我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小.</p>
<p>第二项 $\lambda\Omega(w)$，也就是对参数$w$的规则化函数 $Ω(w)$ 去约束我们的模型尽量的简单.</p>
</blockquote>
<p><strong>第一项对应模型的训练损失函数:</strong></p>
<blockquote>
<ul>
<li>Square Loss –&gt; 最小二乘</li>
<li>Hinge Loss –&gt; SVM</li>
<li>Exp Loss –&gt; AdaBoost</li>
<li>Log Loss –&gt; LR</li>
</ul>
</blockquote>
<p><strong>第二项对应模型的正则化项:</strong> (一般是模型复杂度的单调递增函数)</p>
<blockquote>
<ul>
<li>模型参数向量的范数，不同的选择对参数的约束不同，取得的效果也不同</li>
</ul>
<p>论文中常都聚集在：零范数、一范数、二范数、核范数等等。这么多范数，到底它们表达啥意思？具有啥能力？</p>
</blockquote>
<h2 id="L1-可以实现稀疏，为什么要稀疏？"><a href="#L1-可以实现稀疏，为什么要稀疏？" class="headerlink" title="L1 可以实现稀疏，为什么要稀疏？"></a>L1 可以实现稀疏，为什么要稀疏？</h2><p>让我们的参数稀疏有什么好处呢？这里扯两点：</p>
<p><strong>特征选择(Feature Selection)：</strong></p>
<blockquote>
<p>大家对<strong>稀疏规则化</strong>趋之若鹜的一个关键原因在于它能实现特征的自动选择。$x_i$ 的大部分元素（也就是特征）都是和最终的输出 $y_i$ 没有关系或者不提供任何信息的，在最小化目标函数的时候考虑$x_i$这些额外的特征，虽然可以获得更小的训练误差，<strong>但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确$y_i$的预测</strong>。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。</p>
</blockquote>
<p><strong>可解释性(Interpretability)：</strong></p>
<blockquote>
<p>模型更容易解释。例如患某种病的概率是$y$，我们收集到的数据$x$是 1000 维的，也就是我们需要寻找这 1000种 因素到底是怎么影响患上这种病的概率的。</p>
<p>假设我们这个是个回归模型：$y=w_1<em>x_1+w_2</em>x_2+w_{1000}*x_{1000}+b$（当然了，为了让$y$限定在[0,1]的范围，一般还得加个Logistic函数）。</p>
<p>通过学习，如果最后学习到的$w*$就只有很少的非零元素，例如只有5个非零的$w_i$，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。</p>
<p>患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个$w_i$都非0，医生面对这1000种因素，累觉不爱.</p>
</blockquote>
<h2 id="L2-范数"><a href="#L2-范数" class="headerlink" title="L2 范数"></a>L2 范数</h2><h3 id="为什么L2范数能够防止过拟合？"><a href="#为什么L2范数能够防止过拟合？" class="headerlink" title="为什么L2范数能够防止过拟合？"></a>为什么L2范数能够防止过拟合？</h3><p>通过L2范数的规则项最小来使参数值都较小、甚至趋于0(但不会为0)，模型参数值越小则对应的特征对于模型的影响就比较小，这样相当于对这部分无关特征做了一个惩罚，即使他们的值波动比较大，受限于参数值很小，也不会对模型的输出结果造成太大影响，也就使得模型不会习得这部分特征而发生过拟合</p>
<h3 id="L2范数的好处"><a href="#L2范数的好处" class="headerlink" title="L2范数的好处"></a>L2范数的好处</h3><ul>
<li>学习理论的角度：可以防止过拟合，提升模型的泛化能力</li>
<li>优化、数值计算的角度：L2范数能够让我们的优化求解变得稳定和快速，有助于处理condition number不好的情况下矩阵求逆很困难的问题</li>
</ul>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li>周志华《机器学习》</li>
<li><a href="http://izhaoyi.top/2017/09/15/l1-l2/" target="_blank" rel="external">懒死骆驼</a></li>
<li><a href="https://blog.csdn.net/zouxy09/article/details/24971995" target="_blank" rel="external">机器学习中的范数规则化之（一）L0、L1与L2范数</a></li>
<li><a href="https://plushunter.github.io/2017/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8828%EF%BC%89%EF%BC%9AL1%E3%80%81L2%E6%AD%A3%E5%88%99%E5%8C%96/" target="_blank" rel="external">机器学习算法系列（28）：L1、L2正则化</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Neural Networks and Deep Learning (week2) - Neural Networks Basics]]></title>
      <url>http://sggo.me/2018/07/07/deeplearning/Neural-Networks-and-Deep-Learning-week2/</url>
      <content type="html"><![CDATA[<p>本周我们将要学习 Logistic Regression, 它是神经网络的基础. </p>
<p>Logistic Regression 可以看成是一种只有输入层和输出层(没有隐藏层)的神经网络. </p>
<a id="more"></a>
<!--## Binary Classification
-->
<!--<img src="/images/deeplearning/C1W2-1.jpg" width="750" />

<img src="/images/deeplearning/C1W2-2.jpg" width="750" />
-->
<h2 id="一-基本概念回顾"><a href="#一-基本概念回顾" class="headerlink" title="一. 基本概念回顾"></a>一. 基本概念回顾</h2><p>这次 Andrew 系列课程在符号上有所改动 (和机器学习课程中有所区别, 主要是为了后面代码实现方便), 如下图所示:</p>
<p><img src="/images/deeplearning/C1W2-3_1.jpg" width="700"></p>
<h2 id="1-Notation"><a href="#1-Notation" class="headerlink" title="1. Notation"></a>1. Notation</h2><p>更多关于本系列课程的符号点<a href="http://7xrrje.com1.z0.glb.clouddn.com/deeplearningnotation.pdf" target="_blank" rel="external">这里</a>同样地, 参数也有所变化 ($bias$ 单独拿出来作为$b$, 而不是添加 $\theta_0$)</p>
<h2 id="2-Logistic-Regression"><a href="#2-Logistic-Regression" class="headerlink" title="2. Logistic Regression"></a>2. Logistic Regression</h2><p><img src="/images/deeplearning/C1W2-4_1.jpg" width="750"></p>
<ul>
<li>一个是 <strong>Loss function</strong>, 即损失函数, 它代表了对于一个样本估计值与真实值之间的误差; </li>
<li>一个是 <strong>Cost function</strong>, 它代表了所有样本loss的平均值.</li>
</ul>
<p><img src="/images/deeplearning/C1W2-6_1.jpg" width="750"></p>
<h2 id="3-Logistic-Regression-Cost-Function"><a href="#3-Logistic-Regression-Cost-Function" class="headerlink" title="3. Logistic Regression Cost Function"></a>3. Logistic Regression Cost Function</h2><p><img src="/images/deeplearning/C1W2-8_1.jpg" width="750"></p>
<h2 id="4-Gradient-Descent"><a href="#4-Gradient-Descent" class="headerlink" title="4. Gradient Descent"></a>4. Gradient Descent</h2><p><img src="/images/deeplearning/C1W2-9_1.jpg" width="750"></p>
<p><img src="/images/deeplearning/C1W2-10_1.jpg" width="750"></p>
<h2 id="5-Derivatives"><a href="#5-Derivatives" class="headerlink" title="5. Derivatives"></a>5. Derivatives</h2><p><img src="/images/deeplearning/C1W2-11_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W2-12_1.png" width="750"></p>
<h2 id="7-Computation-Graph"><a href="#7-Computation-Graph" class="headerlink" title="7. Computation Graph"></a>7. Computation Graph</h2><p>神经网络中, forward propagation 用来计算输出, backward propagation 用来计算梯度, 得到梯度后就可更新对应的参数了. </p>
<p><img src="/images/deeplearning/C1W2-13_1.jpg" width="750"></p>
<p>如上图所示通过前向传播, 我们得到 $J = 33$. </p>
<blockquote>
<p>这里说明一下, 在后面代码实现中, 这些导数都可以用 $dvar$ 来表示, 例如 dw1, db1 等等.</p>
</blockquote>
<h2 id="8-Computation-Graph-Derivatives"><a href="#8-Computation-Graph-Derivatives" class="headerlink" title="8. Computation Graph Derivatives"></a>8. Computation Graph Derivatives</h2><p>反向传播本质上就是通过链式法则不断求出前面各个变量的导数的过程.</p>
<p><img src="/images/deeplearning/C1W2-14_1.png" width="750"></p>
<h2 id="9-Logistic-regression-recap"><a href="#9-Logistic-regression-recap" class="headerlink" title="9. Logistic regression recap"></a>9. Logistic regression recap</h2><p>有了计算图的概念之后, 我们将其运用到 Logistic Regression 上. </p>
<p><img src="/images/deeplearning/C1W2-16_1.png" width="700"></p>
<p>上面的式子可以用下面的计算图来表达:</p>
<p><img src="/images/deeplearning/C1W2-16_2.png" width="700"></p>
<p>有了上面的图之后, 我们现在来计算反向传播.</p>
<p>首先我们来计算 $\frac{dL}{da}$:</p>
<p>$$<br>\begin{align} \frac{dL}{da} &amp; = - (\frac{y}{a} - \frac{(1-y)}{(1-a)}) \end{align}<br>$$</p>
<p>通过链式法则, 计算 $\frac{dL}{dz}$:</p>
<p>$$<br>\begin{align} \frac{dL}{dz} &amp; = \frac{dL}{da}\frac{da}{dz} \\ \\ &amp; = - (\frac{y}{a} - \frac{(1-y)}{(1-a)})\sigma(z)(1-\sigma(z)) \\ \\ &amp; = - (\frac{y}{a} - \frac{(1-y)}{(1-a)})a(1-a)) \\ \\ &amp; = -y(1-a) + (1-y)a \\ \\ &amp; = a - y \end{align}<br>$$</p>
<p>最后计算 $\frac{dL}{dw1}, \frac{dL}{dw2}, \frac{dL}{db}$:</p>
<p>$$<br>\frac{dL}{dw_1} = \frac{dL}{dz}\frac{dz}{dw_1} = (a - y)x_1<br>$$</p>
<p>$$<br>\frac{dL}{dw_2} = \frac{dL}{dz}\frac{dz}{dw_2} = (a - y)x_2<br>$$</p>
<p>$$<br>\frac{dL}{db} = \frac{dL}{dz}\frac{dz}{db} = a - y<br>$$</p>
<p>怎么样? 是不是很简单呢? 这里我们所有的计算都是针对一个训练样本的. 当然我们不可能只有一个样本, 那么对于整个训练集, 我们应该怎么做呢? 其实很简单, 我们只需要将 $J(w, b)$ 拆开来写就很清晰.</p>
<p>$$<br>J(w, b) = \frac{1}{m}(L(a^{(1)}, y^{(1)}) + L(a^{(2)}, y^{(2)}) + … + L(a^{(m)}, y^{m)}))<br>$$</p>
<p><img src="/images/deeplearning/C1W2-17_1.jpg" width="750"></p>
<p>对于每一个样本都有一个对应的 $dz^{(i)}$, 而对于 $dw, db$ 来说是对于所有求平均.</p>
<p><img src="/images/deeplearning/C1W2-18_1.png" width="750"></p>
<blockquote>
<p>如果用以上节的伪代码来实现梯度计算的话, 效率会非常低. 需要两个显式的 for 循环.<br>下一节介绍 向量化. 向量化就是用来解决计算效率问题. </p>
</blockquote>
<h2 id="11-Vectorization"><a href="#11-Vectorization" class="headerlink" title="11. Vectorization"></a>11. Vectorization</h2><p>这次的深度学习系列课程作业, 一律要求使用向量化来实现代码. 配合强大的Numpy, 向量化其实很简单. 来看一个例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">x = <span class="number">1000000</span></span><br><span class="line"><span class="comment">#x = 10</span></span><br><span class="line"></span><br><span class="line">a = np.random.rand(x)</span><br><span class="line">b = np.random.rand(x)</span><br><span class="line"></span><br><span class="line">tic = time.time() <span class="comment"># Python time time() 返回当前时间的时间戳（1970纪元后经过的浮点秒数）</span></span><br><span class="line"></span><br><span class="line">c = np.dot(a, b)</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Vectorized version:&#123;&#125;ms'</span>.format(<span class="number">1000</span>*(toc-tic)))</span><br><span class="line"></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x):</span><br><span class="line">    c += a[i] * b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'For loop:&#123;&#125;ms'</span>.format(<span class="number">1000</span>*(toc-tic)))</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Vectorized version:<span class="number">0.881195068359375</span>ms</span><br><span class="line">For loop:<span class="number">363.94405364990234</span>ms</span><br></pre></td></tr></table></figure>
<blockquote>
<p>两个版本效率上差了400多倍. 神经网络本身计算就比较复杂, 加之深度学习训练样本往往都很大, 效率尤为重要. 任何时候都要尽可能避免使用for循环.</p>
</blockquote>
<p>首先我们进行第一步优化, 将 $w$ 写成向量的形式 $dw=np.zeros((n_x, 1))$, 这样就省去了内层关于 $w$ 的循环.</p>
<p><img src="/images/deeplearning/C1W2-19_1.png" width="750"></p>
<p>接下来我们来看看如何优化关于$m$个训练样本的循环. 回顾下第一节中所说的$X$:</p>
<p><img src="/images/deeplearning/C1W2-3_1.jpg" width="600"></p>
<blockquote>
<p>将$X$用如上的矩阵表达后, 通过 $W^T+b$ 也就得到了 $z$ 的向量化表达. $a$ 的向量化表达也就是 $z$ 每个元素进行 $\sigma$操作了.<br>简单吧. 想要把握住向量化一定要清楚每个变量的维度(即python代码里ndarray的shape), 那些是<strong>矩阵操作</strong>, 那些是<strong>element-wise</strong>操作等等. </p>
<p>把握住上面的之后, 在代码实现里还要注意哪里会产生 <code>broadcasting</code>. 例如这里的 $b$ 实际上是一个scalar, 但在进行 $W^T+b$ 操作的时候, $b$ 被numpy自动<code>broadcasting</code> 成和 $W^T$ 维度一样的横向量.</p>
</blockquote>
<p><img src="/images/deeplearning/C1W2-20_1.png" width="750"></p>
<blockquote>
<p>接下来我们看一下梯度的向量化. 前面我们知道 $dz^{(1)}, dz^{(2)}, …, dz^{(m)}$, 这样得到 $dZ$ .</p>
<p>$A$、$Y$ 的向量化前面已知了，这样关于 $z$ 的梯度如下所示. 有了 $dZ$之后 $db$ 就很简单了, 它是所有 $dZ$ 中元素的均值. 在python中的代码表示为 <code>np.mean(dZ)</code> 或者 <code>1/m * np.sum(a)</code>. $dW$ 通过观察向量的维度得到. $X$ 为 $(n, m)$, $dZ$ 为 $(1, m)$, 而 $dW$ 的维度和 $W$ 的维度一样为 $(n,1)$, 这样就得到了 $dW=\frac{1}{m}XdZ^T$.</p>
<p>db 是一个均值？</p>
</blockquote>
<p><img src="/images/deeplearning/C1W2-21_1.png" width="750"></p>
<p>通过上面的努力, 我们将之前for循环的版本改成了完全向量化的表示, 这样向量化实现的代码效率会大大提高. </p>
<blockquote>
<p>(注意: ppt里的for iter in range(1000) 是迭代次数, 这个循环是不可避免的)</p>
</blockquote>
<p><img src="/images/deeplearning/C1W2-22_1.png" width="750"></p>
<h2 id="使用Python实现Logistic-Regression进行猫咪识别"><a href="#使用Python实现Logistic-Regression进行猫咪识别" class="headerlink" title="使用Python实现Logistic Regression进行猫咪识别"></a>使用Python实现Logistic Regression进行猫咪识别</h2><p>LogisticRegression.py:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__parameters_initializer</span><span class="params">(self, input_size)</span>:</span></span><br><span class="line">        <span class="comment"># initial parameters with zeros</span></span><br><span class="line">        w = np.zeros((input_size, <span class="number">1</span>), dtype=float)</span><br><span class="line">        b = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__forward_propagation</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        m = X.shape[<span class="number">1</span>]</span><br><span class="line">        A = sigmoid(np.dot(self.w.T, X) + self.b)</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__compute_cost</span><span class="params">(self, A, Y)</span>:</span></span><br><span class="line">        m = A.shape[<span class="number">1</span>]</span><br><span class="line">        cost = -np.sum(Y*np.log(A) + (<span class="number">1</span>-Y)*(np.log(<span class="number">1</span>-A))) / m</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="comment"># use the result from forward propagation and the label Y to compute cost</span></span><br><span class="line">        A = self.__forward_propagation(X)</span><br><span class="line">        cost = self.__compute_cost(A, Y)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__backward_propagation</span><span class="params">(self, A, X, Y)</span>:</span></span><br><span class="line">        m = X.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># backward propagation computes gradients</span></span><br><span class="line">        dw = np.dot(X, (A-Y).T) / m</span><br><span class="line">        db = np.sum(A-Y) / m</span><br><span class="line">        grads = &#123;<span class="string">"dw"</span>: dw, <span class="string">"db"</span>: db&#125;</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__update_parameters</span><span class="params">(self, grads, learning_rate)</span>:</span></span><br><span class="line">        self.w -= learning_rate * grads[<span class="string">'dw'</span>]</span><br><span class="line">        self.b -= learning_rate * grads[<span class="string">'db'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, num_iterations, learning_rate, print_cost=False, print_num=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.w, self.b = self.__parameters_initializer(X.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">            <span class="comment"># forward_propagation</span></span><br><span class="line">            A = self.__forward_propagation(X)</span><br><span class="line">            <span class="comment"># compute cost</span></span><br><span class="line">            cost = self.__compute_cost(A, Y)</span><br><span class="line">            <span class="comment"># backward_propagation</span></span><br><span class="line">            grads = self.__backward_propagation(A, X, Y)</span><br><span class="line">            dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">            db = grads[<span class="string">"db"</span>]</span><br><span class="line">            <span class="comment"># update parameters</span></span><br><span class="line">            self.__update_parameters(grads, learning_rate)</span><br><span class="line">            <span class="comment"># print cost</span></span><br><span class="line">            <span class="keyword">if</span> i % print_num == <span class="number">0</span> <span class="keyword">and</span> print_cost:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after iteration &#123;&#125;: &#123;:.6f&#125;"</span>.format(i, cost))</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_prob</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># result of forward_propagation is the probability</span></span><br><span class="line">        A = self.__forward_propagation(X)</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        pred_prob = self.predict_prob(X)</span><br><span class="line">        threshold_func = np.vectorize(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        Y_prediction = threshold_func(pred_prob)</span><br><span class="line">        <span class="keyword">return</span> Y_prediction</span><br><span class="line">	</span><br><span class="line">     // 分类准确率分数是指所有分类正确的百分比, 分类准确率这一衡量分类器的标准比较容易理解</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        pred = self.predict(X)</span><br><span class="line">        <span class="keyword">return</span> len(Y[pred == Y]) / Y.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>本周课程剩下部分四个视频分别讲解Broadcasting, Numpy Vector, Jupyter Notebook以及Logistic Regression的概率解释. 如果对于Numpy以及Jupyter Notebook不熟悉的同学需要好好看看这三个视频</p>
<p><strong>Python Broadcasting example:</strong></p>
<p><img src="/images/deeplearning/C1W2-23_1.png" width="500"></p>
<blockquote>
<p>Notes: 多使用 reshape 来保证你用的向量或矩阵是正确的，不要害怕使用 reshape.</p>
</blockquote>
<p><img src="/images/deeplearning/C1W2-24_1.png" width="400"></p>
<blockquote>
<p>Notes: 多使用 assert(a.shape == (5,1)) </p>
<h2 id="本周内容回顾"><a href="#本周内容回顾" class="headerlink" title="本周内容回顾"></a>本周内容回顾</h2></blockquote>
<ul>
<li>了解了深度学习系列课程中使用到的各种符号.</li>
<li>回顾了Logistic Regression.</li>
<li>掌握了loss和cost的区别与联系.</li>
<li>重新认识了前向反向传播, 即计算图.</li>
<li>学习了深度学习中必要的求导知识.</li>
<li>熟悉了Numpy, Jupyter Notebook的使用</li>
<li>掌握了使用Python以神经网络的方式实现Logistic Regression模型, 并使用强大的Numpy来向量化.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://daniellaah.github.io/2017/deeplearning-ai-Neural-Networks-and-Deep-Learning-week2.html" target="_blank" rel="external">deeplearning.ai 专项课程一第二周</a></li>
<li><a href="https://blog.csdn.net/dcrmg/article/details/52416832" target="_blank" rel="external">向量点乘（内积）和叉乘（外积、向量积）概念及几何意义解读</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31914064" target="_blank" rel="external">CPU会被GPU替代吗？SIMD和SIMT谁更好？</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="external">刘建平Pinard - 梯度下降（Gradient Descent）小结</a></li>
<li><a href="https://github.com/daniellaah/deeplearning.ai-step-by-step-guide/tree/master/01-Neural-Networks-and-Deep-Learning/week2" target="_blank" rel="external">Github 01-Neural-Networks-and-Deep-Learning/week2</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[模型评估 Precision、Recall、ROC、AUC 总结]]></title>
      <url>http://sggo.me/2018/07/05/ml/1-roc-auc-summary/</url>
      <content type="html"><![CDATA[<p>实际上非常简单，Precision 是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是对的。那么预测为正就有两种可能了，一种就是把正类预测为正类(<strong>TP</strong>)，另一种就是把负类预测为正类(<strong>FP</strong>).</p>
<a id="more"></a>
<p>$$<br>P = TP/(TP+FP)<br>$$</p>
<blockquote>
<p>Precision 准后 P</p>
</blockquote>
<p>而召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN).</p>
<p>$$<br>R = TP/(TP+FN)<br>$$</p>
<blockquote>
<p>召原正</p>
</blockquote>
<p><img src="/images/ml/model/Precision_Recall-1.png" width="300"></p>
<h2 id="ROC-曲线"><a href="#ROC-曲线" class="headerlink" title="ROC 曲线"></a>ROC 曲线</h2><blockquote>
<p>In signal detection theory, a receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied.</p>
<p>比如在逻辑回归里面，我们会设一个阈值，大于这个值的为正类，小于这个值为负类。如果我们减小这个阀值，那么更多的样本会被识别为正类。这会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了形象化这一变化，在此引入 ROC ，ROC 曲线可以用于评价一个分类器好坏。</p>
</blockquote>
<p>ROC 关注两个指标，</p>
<blockquote>
<p>直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（<code>真正率</code>）和 FP（<code>假正率</code>）间的 trade-off2。</p>
</blockquote>
<p><img src="/images/ml/model/ROC.png" width="780"></p>
<blockquote>
<p>ROC曲线，如果为 y=x 表示模型的预测能力与随机结果没有差别.</p>
</blockquote>
<h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于 1.</p>
<p>由于ROC曲线一般都处于 $y=x$ 这条直线的上方，所以 AUC 的取值范围在 0.5~1 之间.</p>
<p>简单说：AUC值越大的分类器，正确率越高.</p>
<p><img src="/images/ml/model/AUC.png" width="780"></p>
<p>AUC值为ROC曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好。
　　</p>
<blockquote>
<ul>
<li>AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li>
<li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li>
<li>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li>
<li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测</li>
</ul>
</blockquote>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="https://blog.csdn.net/qq_36330643/article/details/79522537" target="_blank" rel="external">模型评估准确率、召回率、ROC曲线、AUC总结</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25993786" target="_blank" rel="external">ROC、AUC、K-S</a></li>
<li><a href="http://cda.pinggu.org/view/21012.html" target="_blank" rel="external">关于模型检验的ROC值和KS值的异同_ROC曲线和KS值</a></li>
<li><a href="https://www.cnblogs.com/techengin/p/8962024.html" target="_blank" rel="external">（原创）sklearn中 F1-micro 与 F1-macro区别和计算原理</a></li>
<li><a href="https://blog.csdn.net/sinat_28576553/article/details/80258619" target="_blank" rel="external">分类问题的几个评价指标（Precision、Recall、F1-Score、Micro-F1、Macro-F1）</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Boosting - Xgboost (not finish)]]></title>
      <url>http://sggo.me/2018/07/03/ml/8-xgboost/</url>
      <content type="html"><![CDATA[<p>Github上和机器学习工具包（如sklearn）中有很多优秀的开源boosting实现。在这里重点介绍<a href="https://weibo.com/u/2397265244?is_all=1" target="_blank" rel="external">@陈天奇怪</a>同学的 <a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="external">Xgboost</a>.</p>
<p>Kaggle上的许多数据挖掘竞赛，Boosting 类方法都帮助参赛者取得了好成绩. 其中很多优秀战果都是用的 <a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="external">Xgboost</a>神器</p>
<a id="more"></a>
<p>Xgboost 提供了Graident Boosting算法的框架，给出了GBDT，GBRT，GBM具体实现。提供了多语言接口（C++, Python, Java, R等），供大家方便使用。</p>
<p>最新版本的xgboost是基于分布式通信协议rabit开发的，可部署在分布式资源调度系统上（如yarn，s3等）。我们完全可以利用最新版的xgboost在分布式环境下解决分类、预估等场景问题。</p>
<blockquote>
<p>XGBoost是DMLC（即分布式机器学习社区）下面的一个子项目，由@陈天奇怪，@李沐等机器学习大神发起。<br>Rabit是一个为分布式机器学习提供Allreduce和Broadcast编程范式和容错功能的开源库（也是@陈天奇同学的又一神器）。它主要是解决MPI系统机器之间无容错功能的问题，并且主要针对Allreduce和Broadcast接口提供可容错功能。</p>
</blockquote>
<h3 id="1-Overfitting-过拟合"><a href="#1-Overfitting-过拟合" class="headerlink" title="1. Overfitting 过拟合"></a>1. Overfitting 过拟合</h3><p>XGBoost里可以使用两种方式防止 Overfitting</p>
<p>直接控制模型复杂度</p>
<ul>
<li><strong>max_depth</strong>,基学习器的深度，增加该值会使基学习器变得更加复杂，荣易过拟合，设为0表示不设限制，对于<code>depth-wise</code>的基学习器学习方法需要控制深度</li>
<li><strong>min_child_weight</strong>，子节点所需的样本权重和(hessian)的最小阈值，若是基学习器切分后得到的叶节点中样本权重和低于该阈值则不会进一步切分，在线性模型中该值就对应每个节点的最小样本数，该值越大模型的学习约保守，同样用于防止模型过拟合</li>
<li><strong>gamma</strong>，叶节点进一步切分的最小损失下降的阈值(超过该值才进一步切分)，越大则模型学习越保守，用来控制基学习器的复杂度(有点LightGBM里的leaf-wise切分的意味)</li>
</ul>
<p>给模型训练增加随机性使其对噪声数据更加鲁棒</p>
<ul>
<li>行采样：subsample</li>
<li>列采样：colsample_bytree</li>
<li>步长：eta即shrinkage</li>
</ul>
<h3 id="2-数据类别分布不均"><a href="#2-数据类别分布不均" class="headerlink" title="2. 数据类别分布不均"></a>2. 数据类别分布不均</h3><h3 id="3-调参"><a href="#3-调参" class="headerlink" title="3. 调参"></a>3. 调参</h3><h3 id="4-一般参数"><a href="#4-一般参数" class="headerlink" title="4. 一般参数"></a>4. 一般参数</h3><h3 id="5-基学习器参数"><a href="#5-基学习器参数" class="headerlink" title="5. 基学习器参数"></a>5. 基学习器参数</h3><h3 id="6-任务参数"><a href="#6-任务参数" class="headerlink" title="6. 任务参数"></a>6. 任务参数</h3><h3 id="7-命令行参数"><a href="#7-命令行参数" class="headerlink" title="7. 命令行参数"></a>7. 命令行参数</h3><h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="http://www.cnblogs.com/fengfenggirl/p/classsify_decision_tree.html" target="_blank" rel="external">逗比算法工程师</a></li>
<li><a href="http://www.52caml.com/" target="_blank" rel="external">算法杂货铺</a></li>
<li><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="external">52caml</a></li>
<li>《机器学习导论》</li>
<li>《统计学习方法》</li>
<li><a href="http://izhaoyi.top/2017/06/19/Decision-Tree" target="_blank" rel="external">懒死骆驼</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Ensumble 集成学习小记]]></title>
      <url>http://sggo.me/2018/07/03/ml/9-ensumble-part1/</url>
      <content type="html"><![CDATA[<p>本文主要基于周志华《机器学习》一书第八章 集成学习内容做的整理笔记，此外查阅了网上的一些博客和问答网站</p>
<a id="more"></a>
<h2 id="1-Ensumble-概念"><a href="#1-Ensumble-概念" class="headerlink" title="1. Ensumble 概念"></a>1. Ensumble 概念</h2><p>Ensumble 是通过构建并结合多个<code>学习器</code>来完成学习任务。</p>
<blockquote>
<p>曾经听过一句话，”Feature为主，Ensemble为后”。</p>
</blockquote>
<p>Feature 决定了模型效果的上限，而 Ensemble 就是让你更接近这个上限。</p>
<blockquote>
<p>Ensemble 讲究“好而不同”，不同是指模型的学习到的侧重面不一样。</p>
<p>举个直观的例子，比如数学考试，A的函数题做的比B好，B的几何题做的比A好，那么他们合作完成的分数通常比他们各自单独完成的要高。</p>
<p>常见的Ensemble方法有Bagging、Boosting、Stacking、Blending.</p>
<p>Notes:  Stacking 与 Blending 类似，<a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="external">区别可参见</a></p>
</blockquote>
<p><strong>ensumble 中的 好而不同</strong></p>
<blockquote>
<p>如何使得集成学习性能比最好的单一学习器更好？</p>
<ul>
<li>准确性</li>
<li>多样性</li>
</ul>
<p>好而不同</p>
<p>如何产生并结合 “好而不同” 的个体学习器 ?</p>
</blockquote>
<p>集成学习研究的核心, 当前按照个体学习器的生成方式划分:</p>
<blockquote>
<p>bagging（及其变体随机森林）— 个体学习器间不存在强依赖关系，可同时生成的并行化方法<br>boosting - 个体学习器间存在强依赖关系，必须串行生成的序列化方法</p>
</blockquote>
<p>从偏差-方差分解的角度: </p>
<blockquote>
<p>bagging 关注降低方差<br>boosting 关注降低偏差</p>
</blockquote>
<p>也可以按照机器学习技法的两张图来理解</p>
<p><img src="/images/ml/ensumble/under_over.png" width="750"></p>
<p>第一幅图(对应boosting)可看作进行了一个特征转换来防止欠拟合，第二幅图(对应bagging)则进行了一个正则化来防止过拟合</p>
<h2 id="2-Boosting"><a href="#2-Boosting" class="headerlink" title="2. Boosting"></a>2. Boosting</h2><blockquote>
<p>首先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T,最终将这T个基学习器进行加权组合.</p>
</blockquote>
<h3 id="2-1-adaboost"><a href="#2-1-adaboost" class="headerlink" title="2.1 adaboost"></a>2.1 adaboost</h3><blockquote>
<p>推导方式，基于“加性模型”，即基于基学习器的线性组合<br>如何在每一轮修改样本分布</p>
<ul>
<li>重赋权法：在每一轮训练中，根据样本分布为每一个训练样本重新赋予一个权重（比如《机器学习实战》一书中就是利用这种方法构建提升树算法，通过修改权重来计算每一轮的损失）</li>
<li>重采样法：利用重采样的训练集来对基学习器进行训练–&gt;重启动：避免训练过程过早停止</li>
</ul>
</blockquote>
<p>Notes:</p>
<blockquote>
<p>西瓜书上的算法还提到训练的每一轮开始都要检查当前学习器是否比随机猜测好，若条件不满足则抛弃当前学习器，这种情形可能会导致学习过程未达到T轮即停止，所以有重采样的方法来进行重启动避免出现此种情况；但是另一方面《统计学习方法》以及《机器学习实战》中的算法并未有这条判断语句；</p>
<p>《机器学习》一书中提到，从统计学的出发认为AdaBoost实质上是基于加性模型（后续指出这一视角阐释的是一个与AdaBoost很相似的过程而非其本身），以类似牛顿迭代法来优化指数损失函数，通过将迭代优化过程替换为其他优化方法产生了GradientBoosting、LPBoost等；而这里也提到每一种变体针对不同的问题（噪声、数据不平衡等）而拥有不同的权重更新规则.</p>
</blockquote>
<h3 id="2-2-特点"><a href="#2-2-特点" class="headerlink" title="2.2 特点"></a>2.2 特点</h3><blockquote>
<p>从bias-variance分解的角度来看，Boosting主要关注降低 bias，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成（与bagging不同）</p>
</blockquote>
<p>Notes: boosting 对于噪音数据较为敏感</p>
<h2 id="3-Bagging-Bootstrap-aggregating"><a href="#3-Bagging-Bootstrap-aggregating" class="headerlink" title="3. Bagging (Bootstrap aggregating )"></a>3. Bagging (Bootstrap aggregating )</h2><p>出发点依然是“好而不同”</p>
<blockquote>
<ul>
<li>“不同” — 不同基学习器基于不同的样本子集</li>
<li>“好” — 使用相互有交叠的采样子集</li>
</ul>
</blockquote>
<h3 id="3-1-工作机制"><a href="#3-1-工作机制" class="headerlink" title="3.1 工作机制"></a>3.1 工作机制</h3><p>基于自助采样法（bootstrap sampling）— “有放回地全抽”</p>
<blockquote>
<p>从训练集从进行<strong>子抽样组成每个基模型所需要的子训练集</strong>，对所有基模型预测的结果进行综合产生最终的预测结果,至于为什么叫bootstrap aggregation，因为它抽取训练样本的时候采用的就是bootstrap的方法！</p>
</blockquote>
<h3 id="3-2-优点（相对于boosting）"><a href="#3-2-优点（相对于boosting）" class="headerlink" title="3.2 优点（相对于boosting）"></a>3.2 优点（相对于boosting）</h3><ul>
<li>高效 - 训练一个bagging集成与直接使用基学习器算法训练一个学习器的复杂度同阶</li>
<li>baggign能不经修改地用于多分类、回归任务（标准AdaBoosting只能适用于二分类任务）</li>
<li>包外估计——自助采样过程中剩余的样本可以作为验证集来对泛化性能进行“包外估计”</li>
</ul>
<h3 id="3-3-特点"><a href="#3-3-特点" class="headerlink" title="3.3 特点"></a>3.3 特点</h3><blockquote>
<p>从偏差-方差角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用明显.</p>
</blockquote>
<h2 id="4-Random-Forest-随机森林"><a href="#4-Random-Forest-随机森林" class="headerlink" title="4. Random Forest 随机森林"></a>4. Random Forest 随机森林</h2><p>Bagging 的一个扩展变体</p>
<h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h3><ul>
<li>以决策树为基学习器构建Bagging集成</li>
<li>在决策树的训练过程中引入了随机属性选择</li>
</ul>
<blockquote>
<p>传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在随机森林中，对基决策树的每个节点，<strong>先从该节点属性集合中随机选择一个包含k个属性的子集</strong>，然后再从这个子集中选择一个最优属性用于划分，其中<code>k</code>控制了随机性的引入程度</p>
</blockquote>
<h3 id="4-2-特点"><a href="#4-2-特点" class="headerlink" title="4.2 特点"></a>4.2 特点</h3><blockquote>
<ul>
<li>简单、易于实现、计算开销小</li>
<li>样本扰动+属性扰动</li>
</ul>
</blockquote>
<h3 id="4-3-bagging-vs-随机森林"><a href="#4-3-bagging-vs-随机森林" class="headerlink" title="4.3 bagging vs. 随机森林"></a>4.3 bagging vs. 随机森林</h3><ul>
<li>两者收敛性相似，随机森林的起始性能往往相对较差，但会收敛到更低的泛化误差</li>
<li>随机森林的训练效率常优于Bagging，主要是决策树划分属性时，原始baggin需要对属性全集进行考虑，而 <strong>RF</strong> 是针对一个子集</li>
</ul>
<h2 id="5-结合策略"><a href="#5-结合策略" class="headerlink" title="5. 结合策略"></a>5. 结合策略</h2><h3 id="5-1-学习器结合的好处"><a href="#5-1-学习器结合的好处" class="headerlink" title="5.1 学习器结合的好处"></a>5.1 学习器结合的好处</h3><blockquote>
<ul>
<li>统计的角度：假设空间很大时，可能存在多个假设在训练集上达到同等性能，但学习其可能误选导致泛化性能不佳，结合多个学习器可以减小该风险</li>
<li>计算的角度：降低陷入糟糕局部极小点的风险</li>
<li>表示的角度：结合多个学习器可扩大假设空间，对于真实假设在假设空间之外的情形可能学得更好的近似</li>
</ul>
</blockquote>
<h3 id="5-2-策略"><a href="#5-2-策略" class="headerlink" title="5.2 策略"></a>5.2 策略</h3><p><strong>平均法(Averaging)</strong></p>
<blockquote>
<ul>
<li>简单平均法</li>
<li>加权平均法</li>
</ul>
</blockquote>
<p><strong>投票法(Voting)</strong></p>
<blockquote>
<ul>
<li>多数投票法</li>
<li>加权投票法</li>
<li>若按个体学习器输出值类型划分 (硬投票：预测为0/1 | 软投票：相当于对后验概率的一个估计)</li>
</ul>
</blockquote>
<p><strong>学习法</strong></p>
<blockquote>
<p>Stacking:先从初始数据集训练出初级学习器，然后生成一个新的数据集用于训练元学习器，在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记；一般使用交叉验证法或留一法来用训练初级学习器未使用的样本来产生元学习器的训练样本</p>
</blockquote>
<p><strong>Notes</strong>: 关于Stacking</p>
<blockquote>
<p>-《机器学习》作者也指出Stacking本身是一种著名的集成方法，且有不少变体和特例，但他这里是作为一种特殊的结合策略看待</p>
<ul>
<li>关于Stacking的细节详述，<a href="https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/" target="_blank" rel="external">如何在 Kaggle 首战中进入前 10%</a> 以一幅图来说说明5折Stacking的过程</li>
<li>推荐一个Python的实现了Stacking集成的库mlxtend</li>
</ul>
</blockquote>
<p><img src="/images/ml/ensumble/stacking-1.jpg" width="800"></p>
<p>原作者举了一个5折stacking的例子，基本方法是，</p>
<ul>
<li>每一折取训练集80%的数据训练一个基模型并对剩下的20%的数据进行预测，同时将该模型对测试集做出预测，保留训练子集的预测结果和测试集的预测结果</li>
<li>将5折的训练子集预测结果结合起来构成第二层元模型的输入特征进行训练得到元分类器</li>
<li>将前面每一折在测试集预测得到的结果取均值作为最终元分类器的预测输入(最终的测试数据)，并使用训练好的元分类器在该数据上作出最终预测</li>
</ul>
<p>此外<a href="https://zhuanlan.zhihu.com/p/27424282" target="_blank" rel="external">知乎上的一篇文章还提到</a></p>
<blockquote>
<p>可以将K个模型对Test Data的预测结果求平均，也可以用所有的Train Data重新训练一个新模型来预测Test Data</p>
</blockquote>
<h2 id="6-多样性"><a href="#6-多样性" class="headerlink" title="6. 多样性"></a>6. 多样性</h2><h3 id="6-1-误差-分歧分解"><a href="#6-1-误差-分歧分解" class="headerlink" title="6.1 误差-分歧分解"></a>6.1 误差-分歧分解</h3><ul>
<li>集成学习“好而不同”的理论分析，见《机器学习》P185~186</li>
<li>寻找 集成泛化误差、个体学习器泛化误差、个体学习器间 的分歧三者之间的关系</li>
</ul>
<h3 id="6-2-多样性度量"><a href="#6-2-多样性度量" class="headerlink" title="6.2 多样性度量"></a>6.2 多样性度量</h3><blockquote>
<p>多样性度量是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度，典型做法是考虑个体分类器的两两相似/不相似性</p>
<p>度量方法 {不合度量、相关系数、Q-统计量、k-统计量} </p>
<p>Notes: 目前我还没有做过这种类似测试 😢</p>
</blockquote>
<h3 id="6-3-多样性增强"><a href="#6-3-多样性增强" class="headerlink" title="6.3 多样性增强"></a>6.3 多样性增强</h3><p>如何增强多样性？— 在学习过程中引入随机性</p>
<ol>
<li>数据样本扰动</li>
<li>输入属性扰动</li>
<li>输出表示扰动</li>
<li>算法参数扰动</li>
</ol>
<p><strong>数据样本扰动</strong></p>
<blockquote>
<p>给定初始数据集，可从中产生不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，通常基于采样法</p>
</blockquote>
<p><strong>输入属性扰动</strong></p>
<blockquote>
<p>从初始属性集中抽取若干个属性子集、基于每个属性子集训练一个基学习器（如随机子空间算法），最后结合</p>
</blockquote>
<p><strong>算法参数扰动</strong></p>
<p>通常可以通过随机设置不同的参数，从而产生差别较大的个体学习器。</p>
<p><strong>Notes:</strong></p>
<blockquote>
<ul>
<li><p>数据样本扰动中相对的稳定基学习器包括：线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等</p>
</li>
<li><p>输入属性扰动 : 感觉典型的是 Random Forest 就做了这件事.</p>
</li>
<li><p>对于算法参数扰动，与交叉验证做比较，交叉验证常常是在不同参数组合模型里选择最优参数组合模型，而集成则是将这些不同参数组合的模型结合起来，所以集成学习技术的实际计算开销并不比使用单一学习器大很多</p>
</li>
</ul>
</blockquote>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li>周志华《机器学习》</li>
<li><a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="external">kaggle-ensembling-guide</a></li>
<li><a href="https://www.quora.com/What-are-the-differences-between-the-three-commonly-ensemble-learning-techniques-stacking-boosting-and-bagging" target="_blank" rel="external">Bagging, Boosting &amp; Stacking</a></li>
<li><a href="https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning/19053#19053" target="_blank" rel="external">stackexchange及评论区</a></li>
<li><a href="https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/" target="_blank" rel="external">如何在 Kaggle 首战中进入前 10%</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27424282" target="_blank" rel="external">分分钟带你杀入Kaggle Top 1%</a></li>
<li><a href="http://izhaoyi.top/2017/07/03/ensemble/" target="_blank" rel="external">懒死骆驼</a></li>
<li>机器学习技法</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[学习笔记(八) - Boosting]]></title>
      <url>http://sggo.me/2018/06/29/ml/8-gradient-boosting-part1/</url>
      <content type="html"><![CDATA[<p>我们需要对多个模型进行融合以提高效果时，常常会用到 Bagging，Boosting，Stacking 等这几个框架算法.</p>
<p>Boosting 有很多种，比如 Adaptive Boosting、Gradient Boosting等，这里以AdaBoost，Gradient Boosting 为典型.</p>
<a id="more"></a>
<blockquote>
<p>在分类问题中，Boosting通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。</p>
</blockquote>
<p><strong>三个臭皮匠，顶个诸葛亮</strong></p>
<blockquote>
<p>🌰🌰🌰 :</p>
<p>关于boosting，机器学习技法一课中开头举的一个小学生认苹果的例子比较传神，假设给定一堆水果，让一群小学生来识别其中的苹果，逐个的询问小学生根据什么样的规则来认为一个水果是苹果，比如学生A认为形状为圆的是苹果，第二个学生认为颜色为红的为苹果等等，逐一询问学生的过程中，每问一个学生之后，按照他的规则对当前水果分类，记录其中的错分的水果然后让下一个同学重点根据这些被错分的水果来找找规则，这样一步一步的最后全班同学得到一个较为完备的规则来判定出苹果.</p>
<p>🌰🌰🌰 总结 :</p>
<p>这个过程中单个学生扮演的较色就是基分类器，比如常用的决策树桩，类似某一维的的感知机，在该维度将空间一分为二的划分样本，而教师的角色则是在这一过程中不断引导学生关注被上一个学生错分的样本来做出判定规则，并最终得出一个较为完备的输出结果</p>
</blockquote>
<p><strong>Boosting试图解决这么一个问题</strong>：</p>
<blockquote>
<p>由于发现弱学习算法通常比发现强学习算法容易，所以Boosting试图解决这么一个问题：“如果已经发现了‘弱学习算法’，那么能否将它提升为‘强学习算法’？”</p>
<p><strong>提升方法</strong>就是从弱学习算法出发，反复学习(改变训练数据的权值分布)，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器；</p>
</blockquote>
<p><strong>提升方法需要解决的两个问题</strong>：</p>
<blockquote>
<ul>
<li>在每一轮训练中，如何改变训练数据的权值/概率分布？</li>
<li>在得到一系列弱分类器后，如何将弱分类器组合成一个强分类器？</li>
</ul>
</blockquote>
<h2 id="1-AdaBoost-Adaptive-Boosting"><a href="#1-AdaBoost-Adaptive-Boosting" class="headerlink" title="1. AdaBoost: Adaptive Boosting"></a>1. AdaBoost: Adaptive Boosting</h2><h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="http://www.cnblogs.com/fengfenggirl/p/classsify_decision_tree.html" target="_blank" rel="external">逗比算法工程师</a></li>
<li><a href="http://www.52caml.com/" target="_blank" rel="external">算法杂货铺</a></li>
<li><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="external">52caml</a></li>
<li>《机器学习导论》</li>
<li>《统计学习方法》</li>
<li><a href="http://izhaoyi.top/2017/06/19/Decision-Tree" target="_blank" rel="external">懒死骆驼</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[学习笔记(五) - Decision Tree]]></title>
      <url>http://sggo.me/2018/06/27/ml/5-decisionTree-part2/</url>
      <content type="html"><![CDATA[<p><img src="/images/ml/decision-tree/decision-tree-2.png" width="330" img=""></p>
<a id="more"></a>
<h2 id="1-决策树模型"><a href="#1-决策树模型" class="headerlink" title="1. 决策树模型"></a>1. 决策树模型</h2><p>决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）</p>
<blockquote>
<p>关于 ID3 与 C4.5 的具体计算流程示例，请参见 <a href="/2016/08/16/ml-5-decisionTree-part1/">desicion tree (part1)</a></p>
</blockquote>
<h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1. 定义"></a>1.1. 定义</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构，其中包含两种类型的节点</p>
<ol>
<li>内部节点: 表示一个 feature（属性）</li>
<li>叶节点: 表示一个 class</li>
</ol>
<blockquote>
<p>一般决策树可以根据以下四个方面划分归类 :</p>
<ol>
<li>分支数</li>
<li>划分策略</li>
<li>终止策略</li>
<li>基分类器</li>
</ol>
</blockquote>
<h3 id="1-2-if-then规则集合"><a href="#1-2-if-then规则集合" class="headerlink" title="1.2. if-then规则集合"></a>1.2. if-then规则集合</h3><blockquote>
<ol>
<li>一条由根节点到叶节点的路径 –&gt; <strong>一条规则</strong></li>
<li>路径上内部节点的特征 –&gt; 规则的条件</li>
<li>叶节点的类 –&gt; 规则的结论 class</li>
<li>性质：互斥且完备</li>
</ol>
</blockquote>
<h3 id="1-3-条件概率分布"><a href="#1-3-条件概率分布" class="headerlink" title="1.3. 条件概率分布"></a>1.3. 条件概率分布</h3><blockquote>
<p>给定 feature 条件下 class 的条件概率分布</p>
</blockquote>
<h2 id="2-决策树的学习"><a href="#2-决策树的学习" class="headerlink" title="2. 决策树的学习"></a>2. 决策树的学习</h2><p>决策树学习本质是从 train datasets 中归纳出一组分类规则，另一个角度，学习是由 train datasets 估计条件概率模型</p>
<h3 id="2-1-目的"><a href="#2-1-目的" class="headerlink" title="2.1 目的"></a>2.1 目的</h3><blockquote>
<p>得到一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力</p>
</blockquote>
<h3 id="2-2-策略"><a href="#2-2-策略" class="headerlink" title="2.2 策略"></a>2.2 策略</h3><blockquote>
<p>以损失函数（通常为正则化的极大似然函数）为目标函数的最小化，并在损失函数确定后，选择最优决策树</p>
</blockquote>
<h3 id="2-3-学习算法"><a href="#2-3-学习算法" class="headerlink" title="2.3 学习算法"></a>2.3 学习算法</h3><blockquote>
<ol>
<li>理论上：<del>从所有可能的决策树中选取最优决策树，NP完全问题</del></li>
<li>实际中：采用启发式方法，近似求解（得到次最优决策树）–&gt; 递归的选择最优特征，并根据该最优特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。</li>
<li>主要步骤： (1). 特征选择 (2). 决策树的生成 (3). 决策树的剪枝</li>
</ol>
</blockquote>
<p>Notes: 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。</p>
<h2 id="3-特征选择"><a href="#3-特征选择" class="headerlink" title="3. 特征选择"></a>3. 特征选择</h2><ul>
<li><p>实质 ： 选取对于训练数据具有分类能力的特征（决定用哪个 <strong>feature</strong> 来划分 <strong>feature space</strong>）</p>
</li>
<li><p>常用准则</p>
</li>
</ul>
<blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">Algorithm</th>
<th style="text-align:center">Feature 选择方法</th>
<th style="text-align:center">Author</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ID3</td>
<td style="text-align:center">Information gain</td>
<td style="text-align:center">Quinlan. 1986</td>
</tr>
<tr>
<td style="text-align:center">C4.5</td>
<td style="text-align:center">Gain ratio</td>
<td style="text-align:center">Quinlan. 1993.</td>
</tr>
<tr>
<td style="text-align:center">CART</td>
<td style="text-align:center">回归树： 最小二乘<br>分类树： 基尼指数 Gini index</td>
<td style="text-align:center">Breiman. 1984<br>(Classification and Regression Tree 分类与回归树)</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="3-1-信息增益-Information-gain"><a href="#3-1-信息增益-Information-gain" class="headerlink" title="3.1 信息增益 Information gain"></a>3.1 信息增益 Information gain</h3><p>$$<br>g(D, A)=H(D)-H(D|A)<br>$$</p>
<blockquote>
<p>$g(D, A)$即信息增益，表示得知特征$A$的信息而使得类$D$的信息的不确定性减少的程度</p>
<p>$H(D)$ 为集合 $D$ 的信息熵</p>
<ul>
<li><p>其中假设 $D$ 是一个取有限个值的离散随机变量，概率分布为 $P(X=x_i)=p_i, i=1, 2,…,n$</p>
</li>
<li><p>熵是表示随机变量不确定性的度量，定义 $H(D)=- \sum_{i=1}^n p_ilogp_i$，熵越大，随机变量的不确定性就越大，$0 \leq H(D) \leq logn$</p>
</li>
<li><p>$H(D|A)$ 经验条件熵在已知随机变量$A$（特征）的条件下随机变量 $D$ 的不确定性$H(D|A)= \sum_{i=1}^{n}p_iH(D|A=a_i)$</p>
</li>
<li><p>一般将熵 $H(D)$ 与条件熵 $H(D|A)$ 之差称为互信息，决策树学习中的信息增益等价于类与特征的互信息</p>
</li>
</ul>
</blockquote>
<p><strong>小结:</strong></p>
<blockquote>
<ul>
<li><p>给定训练数据集 $D$ 和特征 $A$，经验熵 $H(D)$ 表示对数据集 $D$ 进行分类的不确定性，而经验条件熵 $H(D|A)$ 表示在特征 $A$ 给定的条件下对数据集进行分类的不确定性，因此两者之差即<code>信息增益</code>表示由于特征 $A$ 而使得数据集 $D$ 的分类的不确定性减少的程度.</p>
</li>
<li><p>对于数据集 $D$ 而言，信息增益依赖于特征，<code>不同特征具有不同的信息增益，信息增益大的特征具有更强的分类能力</code>（也就是我们需要挑选的目标）</p>
</li>
</ul>
</blockquote>
<h3 id="3-2-信息增益比-Gain-ratio"><a href="#3-2-信息增益比-Gain-ratio" class="headerlink" title="3.2 信息增益比 Gain ratio"></a>3.2 信息增益比 Gain ratio</h3><blockquote>
<p>以信息增益作为划分训练数据集的特征，存在偏向于<strong>选择取值较多</strong>的特征的问题，使用信息增益比可以对这问题进行校正.</p>
</blockquote>
<p>特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：</p>
<p>$$<br>g_R(D,A)= \frac{g(D,A)}{H_A(D)}<br>$$</p>
<p>其中</p>
<p>$$<br>H_A(D)=- \sum_{i=1}^{n} \frac{|D_{i}|}{|D|}log_2 \frac{|D_{i}|}{|D|}<br>$$</p>
<blockquote>
<p>$n$ 为特征 $A$ 的取值个数，$D_i$ 表示据特征 $A$ 的取值将 $D$ 分成的子集</p>
</blockquote>
<h2 id="4-决策树的生成"><a href="#4-决策树的生成" class="headerlink" title="4. 决策树的生成"></a>4. 决策树的生成</h2><h3 id="4-1-ID3"><a href="#4-1-ID3" class="headerlink" title="4.1 ID3"></a>4.1 ID3</h3><p><strong>核心思想:</strong></p>
<blockquote>
<ul>
<li>在决策树的各个结点上应用信息增益准则选择特征，递归地构建决策树。</li>
<li>递归终止条件：所有特征的信息增益（设置信息增益的阈值来判断是否进一步划分）均很小或没特征可选为止.</li>
</ul>
<p>（每选一个<code>特征</code>则后期划分子树不再使用前面用过的<code>特征</code>，因为子树已经是在该特征下属于同一取值的实例集合）</p>
</blockquote>
<p><strong>决策树的生成:</strong></p>
<blockquote>
<p>输入：训练数据集 $D$ 和特征集 $A$，阈值 $ε$；<br>输出：决策树 $T$</p>
<p> (1) （叶子结点）若 $D$ 中所有实例属于同一类 $C_k$, 则 $T$ 为单节点树，并将类$C_k$ 作为该结点的类标记，返回 $T$<br> (2) （终止条件之没有特征可供选择） 若 $A=∅$, 则 $T$为单节点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的 class 标记（多数表决规则），返回 $T$</p>
<p> (3) （计算 $A$ 中各特征对 $D$ 的<strong>信息增益</strong>，选择信息增益最大的特征 $A_g$<br> (4) （终止条件之阈值) 若 $A_g$ 的信息增益小于阈值 $ε$，则置 $T$ 为单节点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$</p>
<p>(5) 否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g=a_i$ 将 $D$ 划分为若干非空子集 $D_i$，并将 $D_i$ 中实例数最大的类作为标记构建子节点，返回 $T_i$<br>(6) 对第 $i$ 个子节点，以 $D_i$ 为训练集，$A − {A_g}$ 为特征集，递归地调用(1)~(5)得到子树 $T_i$ 并返回.</p>
</blockquote>
<h3 id="4-2-C4-5"><a href="#4-2-C4-5" class="headerlink" title="4.2 C4.5"></a>4.2 C4.5</h3><p>C4.5算法与ID3算法类似，不同之处在于，C4.5在生成的过程中，用<code>Gain ratio</code>来选择特征。</p>
<blockquote>
<p>Notes: 上述决策树的生成算法只有树的生成，而且是针对训练集构造的树，容易产生过拟合。</p>
</blockquote>
<h2 id="5-CART-classification-and-regression-tree"><a href="#5-CART-classification-and-regression-tree" class="headerlink" title="5. CART(classification and regression tree)"></a>5. CART(classification and regression tree)</h2><ul>
<li>CART 假设决策树是二叉树，而 ID3,C4.5 生成的过程中并无此假设，这也导致了两者的根本不同，ID3,C4.5 每次选择出最佳特征之后，是按照该特征的每一个取值划分子树；</li>
<li>CART 则是对每一个特征、每一个特征的每一个取值计算基尼指数（分类树）然后从所有特征对应的取值计算所得的基尼指数中最小的特征及特征值作为<code>切分点</code>来划分子树，并在这些子空间上确定预测的概率分布，也就是在输入给定的条件下输出对应的条件概率分布.</li>
</ul>
<h3 id="5-1-CART-纯度度量"><a href="#5-1-CART-纯度度量" class="headerlink" title="5.1 CART 纯度度量"></a>5.1 CART 纯度度量</h3><p>CART 中用于选择变量的不纯性度量是 <strong>Gini index</strong>；如果目标变量是标称的，并且是具有两个以上的类别，则 CART 可能考虑将目标类别合并成两个超类别（双化）；如果目标变量是连续的，则 CART 找出一组基于树的回归方程来预测目标变量。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Algorithm</th>
<th style="text-align:center">Feature Selection</th>
<th style="text-align:center">Author</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">CART</td>
<td style="text-align:center">回归树： 最小二乘<br>分类树： 基尼指数 Gini index</td>
<td style="text-align:center">Breiman. 1984<br>(Classification and Regression Tree 分类与回归树)</td>
</tr>
</tbody>
</table>
<h3 id="5-2-CART-步骤"><a href="#5-2-CART-步骤" class="headerlink" title="5.2 CART 步骤"></a>5.2 CART 步骤</h3><p><code>build decision tree</code>时通常采用自上而下的方法，在每一步选择一个最好的属性来分裂。 “最好” 的定义是使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义”最好”。</p>
<p><strong>CART</strong> 是在给定输入随机变量 $X$ 条件下求得输出随机变量 $Y$ 的条件概率分布的学习方法。</p>
<blockquote>
<p>可以看出CART算法在叶节点表示上不同于ID3、C4.5方法，后二者叶节点对应数据子集通过“多数表决”的方式来确定一个类别（固定一个值）；而CART算法的叶节点对应类别的概率分布。如此看来，我们可以很容易地用 CART 来学习一个 <code>multi-label</code> 的分类任务。</p>
</blockquote>
<p><strong>CART</strong> 算法也主要由两步组成:</p>
<ul>
<li>决策树的生成：基于训练数据集生成一棵二分决策树；</li>
<li>决策树的剪枝：用验证集对已生成的二叉决策树进行剪枝，剪枝的标准为损失函数最小化。</li>
</ul>
<p>由于分类树与回归树在递归地构建二叉决策树的过程中，选择特征划分的准则不同。</p>
<ul>
<li>二叉分类树构建过程中采用<code>基尼指数（Gini Index）</code>为特征选择标准；</li>
<li>二叉回归树采用<code>平方误差最小化</code>作为特征选择标准。</li>
</ul>
<h3 id="5-3-树的构建"><a href="#5-3-树的构建" class="headerlink" title="5.3 树的构建"></a>5.3 树的构建</h3><h4 id="5-3-1-二叉回归树"><a href="#5-3-1-二叉回归树" class="headerlink" title="5.3.1 二叉回归树"></a>5.3.1 二叉回归树</h4><p>设 $X$, $Y$ 分别为输入和输出变量，其中 $Y$ 为连续变量，给定训练数据集 $D= \lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \rbrace$</p>
<blockquote>
<p>决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树</p>
</blockquote>
<p>一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值，所以我们的主要目的是要构建回归树，也就是<strong>如何划分输入空间</strong>，因为一旦划分好输入空间，如将输入空间划分为 $M$ 个单元, $R_1, R_2, … , R_M$ , 并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$，那么回归树的模型就可以表示为</p>
<p>$$<br>f(x)=\sum_{m=1}^Mc_mI(x \in R_m)<br>$$</p>
<p><strong>概念强调</strong></p>
<p><code>首先要强调的是 CART假设决策树是二叉树，内部结点特征的取值只有“是”和“否”，左分支是取值为“是”的分支，有分支则相反。这样的决策树等价于递归地二分每个特征</code>.</p>
<p><strong>最小二叉回归树生成算法</strong></p>
<blockquote>
<p>输入：训练数据集$D$；<br>输出：回归树 $f(x)$</p>
<p>算法：在训练集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上输出值，构建二叉决策树</p>
</blockquote>
<p>(1). 择最优切分变量$j$与切分点$s$，求解</p>
<p>$$<br>\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]<br>$$</p>
<blockquote>
<p>遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式最小值的对$(j,s)$。其中$R_m$是被划分的输入空间，$c_m$是空间$R_m$对应的固定输出值。</p>
</blockquote>
<p>(2). 用选定的对$(j,s)$划分区域并决定相应的输出值：</p>
<p>$$<br>R_1(j,s)=\lbrace x\mid x^{(j)} \le s \rbrace , \quad R_2(j,s)=\lbrace x \mid x^{(j)} \gt s \rbrace<br>$$</p>
<p>$$<br> \hat c_m = {1\over N_m}\sum_{x_i\in R_m(j,s)}y_i , \quad x\in R_m , m=1,2<br>$$</p>
<p>(3). 继续对两个子区域调用步骤（1），（2），直至满足停止条件。</p>
<p>(4). 将输入空间划分为$M$个区域$R_1$,$R_2$ , … , $R_M$，生成决策树：</p>
<p>$$<br>f(x) = \sum_{m=1}^M\hat c_m I(x\in R)<br>$$</p>
<p>举个🌰🌰🌰:</p>
<p>训练数据见下表，$x$ 的取值范围为区间 $[0.5,10.5]$, $y$ 的取值范围为区间$[5.0,10.0]$ ,学习这个回归问题的最小二叉回归树。</p>
<table>
<thead>
<tr>
<th>$x_i$</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>$y_i$</td>
<td>5.56</td>
<td>5.70</td>
<td>5.91</td>
<td>6.40</td>
<td>6.80</td>
<td>7.05</td>
<td>8.90</td>
<td>8.70</td>
<td>9.00</td>
<td>9.05</td>
</tr>
</tbody>
</table>
<p><strong>首先来看这个优化问题</strong></p>
<p>$$<br>\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]<br>$$</p>
<p>求解训练数据的切分点s:</p>
<p>$$<br>R_1(j,s)=\lbrace x\mid x^{(j)} \le s \rbrace , \quad R_2(j,s)=\lbrace x \mid x^{(j)} \gt s \rbrace<br>$$</p>
<p>容易求得在 $R_1$,$R_2$ 内部使得平方损失误差达到最小值的 $c_1$,$c_2$ 为：</p>
<p>$$<br>c_1={1\over N_1}\sum_{x_i \in R_1}y_i , \quad c_2={1\over N_2}\sum_{x_i \in R_2}y_i<br>$$</p>
<p>这里 $N_1$,$N_2$ 是 $R_1$,$R_2$的样本点数。</p>
<p>求训练数据的切分点，根据所给数据，考虑如下切分点：</p>
<p>$$1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5$$</p>
<p>对各切分点，不难求出相应的 $R_1$ , $R_2$ , $c_1$ , $c_2$ 及</p>
<p>$$<br>m(s)=\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]<br>$$</p>
<p>例如，当 $s=1.5$ 时，$R_1 = \lbrace 1\rbrace$, $R_2 = \lbrace 2, 3 , \ldots , 10\rbrace$, $c_1 = 5.56$, $c_2 = 7.50$, </p>
<p>$$<br>m(s)=\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2] = 0+15.72 = 15.72<br>$$</p>
<p>现将$s$及$m(s)$的计算结果列表如下：</p>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
<th>6.5</th>
<th>7.5</th>
<th>8.5</th>
<th>9.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>m(s)</td>
<td>15.72</td>
<td>12.07</td>
<td>8.36</td>
<td>5.78</td>
<td>3.91</td>
<td>1.93</td>
<td>8.01</td>
<td>11.73</td>
<td>15.74</td>
</tr>
</tbody>
</table>
<p>由上表可知，当$x=6.5$的时候达到最小值，此时 $R_1 = \lbrace 1 ,2 , \ldots , 6\rbrace$, $R_2 ={7 ,8 ,9 , 10}$, $c_1 = 6.24$, $c_2 = 8.9$</p>
<p>所以回归树 $T_1(x)$ 为：</p>
<p>$$<br>T_1(x) =<br>\begin{cases}<br>6.24, &amp; x\lt 6.5 \\<br>8.91, &amp; x \ge 6.5 \\<br>\end{cases}<br>$$</p>
<p>$$<br>f_1(x) = T_1(x)<br>$$</p>
<p>用 $f_1(x)$ 拟合训练数据的残差见下表，表中 $r_{2i} = y_i - f_1(x_i),i=1,2,\ldots , 10$</p>
<table>
<thead>
<tr>
<th>$x_i$</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>$y_i$</td>
<td>-0.68</td>
<td>-0.54</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.56</td>
<td>0.81</td>
<td>-0.01</td>
<td>-0.21</td>
<td>0.09</td>
<td>0.14</td>
</tr>
</tbody>
</table>
<p>用$f_1(x)$拟合训练数据的平方误差：</p>
<p>$$<br>L(y,f_1(x)) = \sum_{i=1}^{10}(y_i-f_1(x_i))^2 = 1.93<br>$$</p>
<p>第2步求 $T_2(x)$.方法与求 $T_1(x)$ 一样，只是拟合的数据是上表的残差，可以得到：</p>
<p>$$<br>T_2(x) =<br>\begin{cases}<br>-0.52, &amp; x\lt 3.5 \\<br>0.22, &amp; x \ge 3.5 \\<br>\end{cases}<br>$$</p>
<p>$$<br>f_2(x) = f_1(x) + T_2(x)=<br>\begin{cases}<br>5.72, &amp; x\lt 3.5 \\<br>6.46, &amp; 3.5\le x \lt 6.5 \\<br>9.13, &amp; x\ge 6.5 \\<br>\end{cases}<br>$$</p>
<p>用$f_2(x)$拟合训练数据的平方误差是：</p>
<p>$$<br>L(y,f_2(x)) = \sum_{i=1}^{10}(y_i-f_2(x_i))^2 = 0.79<br>$$</p>
<p>继续求得</p>
<p>$$<br>T_3(x) =<br>\begin{cases}<br>0.15, &amp; x\lt 6.5 \\<br>-0.22, &amp; x \ge 6.5 \\<br>\end{cases}<br>\quad L(y,f_3(x)) = 0.47 ,<br>$$</p>
<p>$$<br>T_4(x) =<br>\begin{cases}<br>-0.16, &amp; x\lt 4.5 \\<br>0.11, &amp; x \ge 4.5 \\<br>\end{cases}<br>\quad L(y,f_3(x)) = 0.30 ,<br>$$</p>
<p>$$<br>T_5(x) =<br>\begin{cases}<br>0.07, &amp; x\lt 6.5 \\<br>-0.11, &amp; x \ge 6.5 \\<br>\end{cases}<br>\quad L(y,f_3(x)) = 0.23 ,<br>$$</p>
<p>$$<br>T_6(x) =<br>\begin{cases}<br>-0.15, &amp; x\lt 2.5 \<br>0.04, &amp; x \ge 2.5 \<br>\end{cases}<br>$$</p>
<p>$$<br>f_6(x) = f_5(x)+T_6(x) =T_1(x)+ \ldots + T_5(x) + T_6(x)=<br>\begin{cases}<br>5.63, &amp; x\lt 2.5 \\<br>5.82, &amp; 2.5 \le x\lt 3.5 \\<br>6.56, &amp; 3.5 \le x\lt 4.5 \\<br>6.83, &amp; 4.5 \le x\lt 6.5 \\<br>8.95, &amp; x\ge 6.5 \\<br>\end{cases}<br>$$</p>
<p>用$f_6(x)$拟合训练数据的平方损失误差是</p>
<p>$$<br>L(y,f_6(x)) = \sum_{i=1}^{10}(y_i-f_6(x_i))^2 = 0.71<br>$$</p>
<p>假设此时已经满足误差要求，那么 $f(x) = f_6(x)$ 即为所求的回归树。</p>
<h4 id="5-3-2-二叉分类树"><a href="#5-3-2-二叉分类树" class="headerlink" title="5.3.2 二叉分类树"></a>5.3.2 二叉分类树</h4><p>二叉分类树中用基尼指数（Gini Index）作为最优特征选择的度量标准。基尼指数定义如下：</p>
<p><strong>Gini Index :</strong></p>
<ol>
<li>是一种不等性度量；</li>
<li>通常用来度量收入不平衡，可以用来度量任何不均匀分布；</li>
<li>是介于0~1之间的数，0-完全相等，1-完全不相等；</li>
<li>总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似）</li>
</ol>
<p><strong>Gini Index :</strong></p>
<p>同样以分类系统为例，数据集 $D$ 中类别 $C$ 可能的取值为$c_1, c_2, \cdots, c_k$ （$k$是类别数），一个样本属于类别 $c_i$ 的概率为$p(i)$。那么概率分布的 Gini index 公式表示为：</p>
<p>$$<br>Gini(D) = 1 - \sum_{i=1}^{k} {p_i}^2    \qquad(fmt.2.1.1)<br>$$</p>
<blockquote>
<p>其中$p_i = \frac{类别属于c_i的样本数}{总样本数}$。如果所有的样本 Category 相同，则 $p_1 = 1, p_2 = p_3 = \cdots = p_k = 0$，则有$p_1 = 1, p_2 = p_3 = \cdots = p_k = 0$，此时数据不纯度最低。$Gini(D)$ 的物理含义是表示数据集 $D$ 的不确定性。数值越大，表明其不确定性越大（这一点与 <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)" target="_blank" rel="external">Info Entropy</a> 相似）。<br>如果 $k=2$（二分类问题，类别命名为正类和负类），若样本属于正类的概率是 $p$，那么对应基尼指数为：</p>
<p>$$<br>\begin{align} Gini(D) &amp; = 1 - [p^2 + {(1-p)}^2] \\ &amp; = \underline {2p (1-p)} \qquad\qquad (fmt.2.1.2)<br>\end{align}<br>$$</p>
</blockquote>
<p>如果数据集 $D$ 根据特征 $f$ 是否取某一可能值 $f_∗$，将 $D$ 划分为 $D_1={(x, y) \in D | f(x) = f_{\ast}}, D_2=D-D_1$。那么特征 $f$ 在数据集 $D$ 上的 Gini index 定义为：</p>
<p>$$<br>Gini(D, f=f_{\ast}) = \frac{\vert D_1 \vert}{\vert D \vert} Gini(D_1) + \frac{\vert D_2 \vert}{\vert D \vert} Gini(D_2) \qquad\qquad (fmt.2.1.3)<br>$$</p>
<p>代表性的例子说明 :</p>
<table>
<thead>
<tr>
<th style="text-align:center">ID</th>
<th style="text-align:center">阴晴(F)</th>
<th style="text-align:center">温度(F)</th>
<th style="text-align:center">湿度(F)</th>
<th style="text-align:center">刮风(F)</th>
<th style="text-align:center">是否玩（C）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">hot</td>
<td style="text-align:center">high</td>
<td style="text-align:center">false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">hot</td>
<td style="text-align:center">high</td>
<td style="text-align:center">true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">overcast</td>
<td style="text-align:center">hot</td>
<td style="text-align:center">high</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">high</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">cool</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">cool</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">overcast</td>
<td style="text-align:center">cool</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">high</td>
<td style="text-align:center">false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">cool</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">overcast</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">high</td>
<td style="text-align:center">true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center">overcast</td>
<td style="text-align:center">hot</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">high</td>
<td style="text-align:center">true</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
<p><strong>下图是 IG 的决策树，并不是 二分分类树 ${Gini}$ ${index}$ 决策树, 放这里仅仅是为了感知一下</strong></p>
<p><img src="/images/ml/decision-tree/decision-tree-2.png" width="530" img=""></p>
<p>在实际操作中，通过遍历所有特征（如果是连续值，需做离散化）及其取值，选择 $Min_{gini-index}$ 所对应的特征和特征值。</p>
<p>这里仍然以天气数据为例，给出特征<strong>阴晴</strong>的 Gini index 计算过程。</p>
<blockquote>
<p>(1). 当特征“阴晴”取值为”sunny”时，$D_1 = {1,2,8,9,11}, |D_1|=5$；$D_2={3,4,5,6,7,10,12,13,14}, |D_2|=9$. 数据自己对应的类别数分别为 $(+2,-3)、(+7,-2)$。因此 $Gini(D_1) = 2 \cdot \frac{3}{5} \cdot \frac{2}{5} = \frac{12}{25}$；$Gini(D_2) = 2 \cdot \frac{7}{9} \cdot \frac{2}{9} = \frac{28}{81}$. 对应的基尼指数为：<br>$$<br>Gini(C, “阴晴”=”sunny”) = \frac{5}{14} Gini(D_1) + \frac{9}{14} Gini(D_2) = \frac{5}{14} \frac{12}{25} + \frac{9}{14} \frac{28}{81} = 0.394 \quad(exp.2.2.1)<br>$$<br>(2). 当特征“阴晴”取值为”overcast”时，$D_1 = {2,7,12,13}, |D_1|=4$；$D_2={1,2,4,5,6,8,9,10,11,14}, |D_2|=10$。$D_1$、$D_2$ 数据自己对应的类别数分别为 $(+4,-0)、(+5,-5)$。因此 $Gini(D_1) = 2 \cdot 1 \cdot 0 = 0；Gini(D_2) = 2 \cdot \frac{5}{10} \cdot \frac{5}{10} = \frac{1}{2}$ 对应的基尼指数为：<br>$$<br>Gini(C, “阴晴”=”overcast”) = \frac{4}{14} Gini(D_1) + \frac{10}{14} Gini(D_2) = 0 + \frac{10}{14} \cdot \frac{1}{2} = \frac{5}{14} = 0.357 \quad(exp.2.2.2)<br>$$</p>
<p>(3). 当特征“阴晴”取值为”rainy”时，$D_1 = {4,5,6,10,14}, |D_1|=5$; $D_2={1,2,3,7,8,9,11,12,13}, |D_2|=9$。 $D_1$、$D_2$ 数据自己对应的类别数分别为 $(+3,−2)、(+6,−3)$。因此 $Gini(D_1) = 2 \cdot \frac{3}{5} \cdot \frac{2}{5} = \frac{12}{25}$；$Gini(D_2) = 2 \cdot \frac{6}{9} \cdot \frac{3}{9} = \frac{4}{9}$。 对应的基尼指数为：<br>$$<br>Gini(C, “阴晴”=”rainy”) = \frac{5}{14} Gini(D_1) + \frac{9}{14} Gini(D_2) = \frac{5}{14} \frac{12}{25} + \frac{9}{14} \frac{4}{9} = \frac{4}{7} = 0.457 \quad(exp.2.2.3)<br>$$</p>
</blockquote>
<p>如果特征”阴晴”是最优特征的话，那么特征取值为”overcast”应作为划分节点。</p>
<h2 id="6-决策树模型的优缺点"><a href="#6-决策树模型的优缺点" class="headerlink" title="6. 决策树模型的优缺点"></a>6. 决策树模型的优缺点</h2><h3 id="6-1-优点"><a href="#6-1-优点" class="headerlink" title="6.1 优点"></a>6.1 优点</h3><ul>
<li>可解释性–模拟人类决策过程</li>
<li>训练、预测效率较高–关于其切分方式，每次是在一个条件下的局部空间划分样本，而类似Adaboost则是每次在整个空间划分样本，所以就决策树而言相对高效</li>
<li>适用于类别类型数据–decision set(穷举类别特征值然后按照特征值的子集集合来划分样本)</li>
<li>能够很方便的由二分类模型转换为多分类模型–主要修改不纯度计算以及返回值的设置</li>
<li>能够处理缺失特征值–用其他的特征值来替代进行划分(一般要求替代特征划分结果接近缺失特征值)</li>
<li>易于实现</li>
</ul>
<h3 id="6-2-缺点"><a href="#6-2-缺点" class="headerlink" title="6.2 缺点"></a>6.2 缺点</h3><ul>
<li>经验多于理论，大多数决策树模型是根据经验来判断的，效果好坏尚无较好的理论支撑</li>
</ul>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="http://www.cnblogs.com/fengfenggirl/p/classsify_decision_tree.html" target="_blank" rel="external">逗比算法工程师</a>、<a href="http://www.52caml.com/" target="_blank" rel="external">算法杂货铺</a></li>
<li><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="external">52caml</a>、<a href="http://izhaoyi.top/2017/06/19/Decision-Tree" target="_blank" rel="external">懒死骆驼</a></li>
<li><a href="http://blog.csdn.net/ljp812184246/article/details/47402639" target="_blank" rel="external">决策树ID3、C4.5、CART算法：信息熵，区别，剪枝理论总结</a></li>
<li><a href="https://www.zybuluo.com/mdeditor" target="_blank" rel="external">Markdown 学习好教材</a>、<a href="https://cethik.vip/2016/09/21/machineCAST/" target="_blank" rel="external">CART之回归树构建</a></li>
<li>《机器学习导论》《统计学习方法》</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[LR 与 SVM 的异同]]></title>
      <url>http://sggo.me/2018/06/23/ml/model-lr-vs-svm/</url>
      <content type="html"><![CDATA[<p>LR 和 SVM 之间的相同点和不同点, 查询了网络上的一些资料，特整理汇总如下 : 😄</p>
<a id="more"></a>
<ol>
<li>LR 和 SVM 放在一起来进行比较, why？</li>
<li>LR 和 SVM 的 相同点和不同点?</li>
</ol>
<!-- more -->
<h2 id="1-为什么将LR和SVM放在一起来进行比较？"><a href="#1-为什么将LR和SVM放在一起来进行比较？" class="headerlink" title="1. 为什么将LR和SVM放在一起来进行比较？"></a>1. 为什么将LR和SVM放在一起来进行比较？</h2><p>回答这个问题其实就是回答LR和SVM有什么相同点</p>
<h3 id="LR-和-SVM-都是分类算法"><a href="#LR-和-SVM-都是分类算法" class="headerlink" title="LR 和 SVM 都是分类算法"></a>LR 和 SVM 都是分类算法</h3><blockquote>
<p>判断一个算法是分类还是回归算法的唯一标准就是样本label的类型，如果label是离散的，就是分类算法，如果label是连续的，就是回归算法。很明显，LR的训练数据的label是“0或者1”，当然是分类算法.</p>
</blockquote>
<h3 id="LR-和-SVM-都是线性分类算法"><a href="#LR-和-SVM-都是线性分类算法" class="headerlink" title="LR 和 SVM 都是线性分类算法"></a>LR 和 SVM 都是线性分类算法</h3><blockquote>
<p>如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性</p>
<p>这里要先说明一点，那就是LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？</p>
</blockquote>
<h3 id="LR-和-SVM-都是判别模型"><a href="#LR-和-SVM-都是判别模型" class="headerlink" title="LR 和 SVM 都是判别模型"></a>LR 和 SVM 都是判别模型</h3><table>
<thead>
<tr>
<th>-</th>
<th style="text-align:center">generative approach</th>
<th style="text-align:center">discriminative approach</th>
</tr>
</thead>
<tbody>
<tr>
<td>定义</td>
<td style="text-align:center">由数据学习联合概率分布$P(X,Y)$ 然后,<br>求出在$X$情况下，$P(Y)$作为预测的模型</td>
<td style="text-align:center">决策函数$f(x)$或条件概率分布$P(X)$作为预测模型</td>
</tr>
<tr>
<td>特点</td>
<td style="text-align:center">1. 可还原出$P(X,Y)$；<br> 2. 学习收敛速度更快；<br> 3. 存在隐变量时仍可用</td>
<td style="text-align:center">1. 直接面对预测，准确率更高些; <br> 2. 便于数据抽象，特征定义使用;</td>
</tr>
<tr>
<td>模型</td>
<td style="text-align:center">native bayes、hidden markov</td>
<td style="text-align:center">Logistic Regression、SVM、Gradient Boosting、CRF.. </td>
</tr>
<tr>
<td>Note</td>
<td style="text-align:center">给定输入$X$产生输出$Y$的生成关系</td>
<td style="text-align:center">对给定的输入$X$，应预测什么样的输出$Y$</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>discriminative model</strong> 判别模型不关心数据是怎么生成的，关心信号之间的差别，后用差别来对给定的一个信号进行分类<br>常见的判别模型有：KNN、SVM、LR</p>
</blockquote>
<h2 id="2-LR-和-SVM-的-不同"><a href="#2-LR-和-SVM-的-不同" class="headerlink" title="2. LR 和 SVM 的 不同"></a>2. LR 和 SVM 的 不同</h2><ol>
<li>Linear SVM 和 LR 都是线性分类器</li>
<li>Linear SVM 不直接依赖数据分布，分类平面不受一部分样本点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance一般需要先对数据做balancing。</li>
<li>Linear SVM 依赖数据表达的距离测度，所以需要对数据先做normalization；LR不受其影响</li>
<li>Linear SVM 依赖penalty的系数，实验中需要做validation</li>
<li>Linear SVM 和 LR 的 performance 都会收到outlier的影响，其敏感程度而言，谁更好很难下明确结论.</li>
</ol>
<h2 id="3-SVM-和-LR-的比较"><a href="#3-SVM-和-LR-的比较" class="headerlink" title="3. SVM 和 LR 的比较"></a>3. SVM 和 LR 的比较</h2><p>两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是<strong>增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重</strong>。</p>
<p>SVM 的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而LR通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。</p>
<p>LR 相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。还有很重要的一点，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。</p>
<h3 id="LR-amp-SVM-不同点"><a href="#LR-amp-SVM-不同点" class="headerlink" title="LR &amp; SVM 不同点"></a>LR &amp; SVM 不同点</h3><blockquote>
<p>要说有什么本质区别，那就是两个模型对数据和参数的敏感程度不同</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">model</th>
<th style="text-align:center">desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Linear SVM</strong></td>
<td style="text-align:center">比较依赖 penalty的系数 和 数据表达空间的测度</td>
</tr>
<tr>
<td style="text-align:center"><strong>LR(自带正则项)</strong></td>
<td style="text-align:center">比较依赖对参数做 L1 regularization 的系数</td>
</tr>
</tbody>
</table>
<blockquote>
<p>但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？</p>
<p>因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。</p>
<p>所以使用<strong>Linear SVM</strong>都需要先对数据做<strong>normalization</strong>，而求解LR（without regularization）时则不需要或者结果不敏感。</p>
<p>注：不带正则化的LR，其做normalization的目的是为了方便选择优化过程的起始值，不代表最后的解的performance会跟normalization相关，而其线性约束是可以被放缩的（等式两边可同时乘以一个系数），所以做normalization只是为了求解优化模型过程中更容易选择初始值。</p>
</blockquote>
<h3 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h3><blockquote>
<p>具体到 loss function 上来看， LR 用的是 <code>log-loss</code>, SVM 用的是 <code>hinge-loss</code>, 两者的相似之处在于 loss 在错误分类的时候都很大，但是对于正确分类的点，hinge-loss 就不管了，而 log-loss 还要考虑进去。此外因为 log-loss 在 mis-classified 的点上是指数级增长的，而 hinge-loss 是线性增长，所以 <strong>LR 在偶尔出现 mis-label 的情况下的表现会比较糟糕</strong>。</p>
<p>另外 regularization 在这里没有区别，L1/L2 两个都能用，效果也差不多。Class imbalance 的话 SVM 一般用 weight 解决，LR 因为可以预测概率，所以也可以直接对最后的结果进行调整，取不同的阈值来达到理想的效果。</p>
<p>实践中 LR 的速度明显更快，维度小的时候 bias 小 也不容易 overfit. 相反 Kernel SVM 在大规模数据集的情况下基本不实用，但是如果数据集本身比较小而且维度高的的话一般 SVM 表现更好。</p>
</blockquote>
<h2 id="4-LR-amp-SVM-选型"><a href="#4-LR-amp-SVM-选型" class="headerlink" title="4. LR &amp; SVM 选型"></a>4. LR &amp; SVM 选型</h2><p>在Andrew NG的课里讲到过：</p>
<ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ol>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="https://www.cnblogs.com/zhizhan/p/5038747.html" target="_blank" rel="external">LR 与 SVM的异同</a></li>
<li><a href="http://izhaoyi.top/2017/06/02/Note-StatisticalML/" target="_blank" rel="external">懒死骆驼 - 统计学习方法笔记(一)</a></li>
<li><a href="https://www.zhihu.com/question/24900876" target="_blank" rel="external">知乎 : 最小二乘、极大似然、梯度下降有何区别？</a></li>
<li><a href="https://www.zhihu.com/question/26768865/answer/139613835" target="_blank" rel="external">知乎 : Linear SVM 和 LR 有什么异同？</a></li>
<li><a href="http://www.cnblogs.com/peizhe123/p/5674730.html" target="_blank" rel="external">白开水加糖 SVM与LR的比较</a></li>
<li><a href="http://izhaoyi.top/2017/09/03/model-pre/" target="_blank" rel="external">懒死骆驼 - 口述模型整理</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Support Vecor Machine (六部曲)]]></title>
      <url>http://sggo.me/2018/06/20/ml/svm-1/</url>
      <content type="html"><![CDATA[<p>Support Vecor Machine, 自一诞生便由于它良好的分类性能席卷了机器学习领域，并牢牢压制了神经网络领域好多年。 如果不考虑集成学习的算法，不考虑特定的训练数据集，在分类算法中的表现SVM说是排第一估计是没有什么异议的.</p>
<a id="more"></a>
<h2 id="1-SVM-间隔-Margin"><a href="#1-SVM-间隔-Margin" class="headerlink" title="1. SVM 间隔 Margin"></a>1. SVM 间隔 Margin</h2><ol>
<li>支持向量机（SVM）的目标是什么？</li>
<li>什么是分离超平面， Margin</li>
</ol>
<p>详情可参 : <a href="https://blog.csdn.net/han_xiaoyang/article/details/52678373" target="_blank" rel="external">机器学习系列(13)_SVM碎碎念part1：间隔</a></p>
<blockquote>
<p>认识一下SVM中很重要的一个概念：Margin，也就是间隔。</p>
</blockquote>
<h2 id="2-SVM-向量与空间距离"><a href="#2-SVM-向量与空间距离" class="headerlink" title="2. SVM 向量与空间距离"></a>2. SVM 向量与空间距离</h2><ol>
<li>从向量到距离计算 (向量定义、计算方向向量、向量的和与差、向量内积、向量正交投影)</li>
<li>SVM的超平面 (1 计算点到超平面距离、2 计算超平面的间隔)</li>
</ol>
<p>详情可参 : <a href="https://blog.csdn.net/han_xiaoyang/article/details/52679559" target="_blank" rel="external">机器学习系列(14)_SVM碎碎念part2：SVM中的向量与空间距离</a></p>
<blockquote>
<p>回顾了一下向量中的一些概念，依用向量的知识，怎么帮助我们去计算超平面间隔，有兴趣的同学请接着看part3</p>
<p>$ w^{T}X = 0 $， w</p>
</blockquote>
<h2 id="3-SVM-如何找到最优分离超平面"><a href="#3-SVM-如何找到最优分离超平面" class="headerlink" title="3. SVM 如何找到最优分离超平面"></a>3. SVM 如何找到最优分离超平面</h2><ol>
<li>如何找到最优超平面</li>
<li>如何计算两超平面间的距离</li>
<li>SVM的最优化问题是什么</li>
</ol>
<blockquote>
<p>找到两个平行超平面，可以划分数据并且两平面之间没有数据点<br>两个超平面之间的距离最大化</p>
</blockquote>
<p>详情可参 : <a href="https://blog.csdn.net/han_xiaoyang/article/details/52683653" target="_blank" rel="external">机器学习系列(15)_SVM碎碎念part3：如何找到最优分离超平面</a></p>
<h2 id="4-SVM-无约束最小化问题"><a href="#4-SVM-无约束最小化问题" class="headerlink" title="4. SVM 无约束最小化问题"></a>4. SVM 无约束最小化问题</h2><p>详情可参 : <a href="https://blog.csdn.net/han_xiaoyang/article/details/79079540" target="_blank" rel="external">机器学习系列(21)_SVM碎碎念part4：无约束最小化问题</a></p>
<h2 id="5-SVM-凸函数与优化"><a href="#5-SVM-凸函数与优化" class="headerlink" title="5. SVM 凸函数与优化"></a>5. SVM 凸函数与优化</h2><p>详情可参 : <a href="https://blog.csdn.net/yaoqiang2011/article/details/79080100" target="_blank" rel="external">机器学习系列(22)_SVM碎碎念part5：凸函数与优化</a></p>
<h2 id="6-SVM-对偶和拉格朗日乘子"><a href="#6-SVM-对偶和拉格朗日乘子" class="headerlink" title="6. SVM 对偶和拉格朗日乘子"></a>6. SVM 对偶和拉格朗日乘子</h2><p>详情可参 : <a href="https://blog.csdn.net/yaoqiang2011/article/details/79080123" target="_blank" rel="external">机器学习系列(23)_SVM碎碎念part6：对偶和拉格朗日乘子</a></p>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[阿滴英文｜會讓你墜入愛河的電影! 浪漫愛情電影台詞分享!]]></title>
      <url>http://sggo.me/2018/05/06/English/english-RD-moives-1/</url>
      <content type="html"><![CDATA[<p><img src="/images/english/RD-moive-1-0.jpg" width="400"></p>
<a id="more"></a>
<h2 id="1-The-Lucky-One"><a href="#1-The-Lucky-One" class="headerlink" title="1. The Lucky One"></a>1. The Lucky One</h2><p><img src="/images/english/RD-moive-1-2.jpg" width="650"></p>
<h3 id="classical-lines"><a href="#classical-lines" class="headerlink" title="classical lines"></a>classical lines</h3><p><img src="/images/english/RD-moive-1-3.jpg" width="650"></p>
<h2 id="2-Safe-Haven"><a href="#2-Safe-Haven" class="headerlink" title="2. Safe Haven"></a>2. Safe Haven</h2><p><img src="/images/english/RD-moive-1-4.jpg" width="650"></p>
<h3 id="classical-lines-1"><a href="#classical-lines-1" class="headerlink" title="classical lines"></a>classical lines</h3><p><img src="/images/english/RD-moive-1-5.jpg" width="650"></p>
<h2 id="3-The-Choice"><a href="#3-The-Choice" class="headerlink" title="3. The Choice"></a>3. The Choice</h2><p><img src="/images/english/RD-moive-1-6.jpg" width="650"></p>
<h3 id="classical-lines-2"><a href="#classical-lines-2" class="headerlink" title="classical lines"></a>classical lines</h3><p><img src="/images/english/RD-moive-1-7.jpg" width="650"></p>
<p><img src="/images/english/RD-moive-1-8.jpg" width="650"></p>
<h2 id="4-The-Notebook"><a href="#4-The-Notebook" class="headerlink" title="4. The Notebook"></a>4. The Notebook</h2><p><img src="/images/english/RD-moive-1-9.jpg" width="650"></p>
<h3 id="leading-actor-and-actress"><a href="#leading-actor-and-actress" class="headerlink" title="leading actor and actress"></a>leading actor and actress</h3><p><img src="/images/english/RD-moive-1-10.jpg" width="650"></p>
<h3 id="classical-lines-3"><a href="#classical-lines-3" class="headerlink" title="classical lines"></a>classical lines</h3><p><img src="/images/english/RD-moive-1-12.jpg" width="650"></p>
<h3 id="the-wedding"><a href="#the-wedding" class="headerlink" title="the wedding"></a>the wedding</h3><p><img src="/images/english/RD-moive-1-11.jpg" width="650"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://nicholassparks.com/" target="_blank" rel="external">Nicholas Sparks</a></li>
</ul>
<div class="video-container"><iframe src="//www.youtube.com/embed/rpizH_ZNIy8" frameborder="0" allowfullscreen></iframe></div>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[互联网金融风控中的数据科学 (part3) ： Lending Club 的数据试验]]></title>
      <url>http://sggo.me/2018/04/23/data-science/internet-finance-3/</url>
      <content type="html"><![CDATA[<p>用户全流程欺诈⻛险评分体系</p>
<a id="more"></a>
<h2 id="反欺诈是一种机器学习过程"><a href="#反欺诈是一种机器学习过程" class="headerlink" title="反欺诈是一种机器学习过程"></a>反欺诈是一种机器学习过程</h2><blockquote>
<p>对于做互联网金融一般情况是 正负样本 是极度不平衡的(最高可以达到 100 : 1), 这样的情况对于 SVM 这种分类器是不合适的，所以在做金融的<strong>评分卡模型</strong> 或 <strong>欺诈模型</strong> 也好，这样对特征的处理 和 样本的非平衡处理是比较高的.</p>
<p>好坏用户的定义，一般是根据用户的贷后表现，来定义好坏用户的.</p>
</blockquote>
<p>举个栗子🌰 :</p>
<blockquote>
<p>用户借款 5W 元，可能是分期还款 12个月，这样每个月都会还一笔固定的额度.</p>
<p><strong>信用风险</strong> : 在挺长的时间可以按时还款.</p>
<p><strong>欺诈风险</strong> : </p>
<ol>
<li>用户可能 第 1、2 期 是还的，之后是不还的.<br> (因为中介也越来越聪明，给他自己留出时间，躲避催收的手段，也躲避追踪等等)</li>
<li>贷前审核 （触碰到拒贷规则）</li>
<li>造假行为 （信息资料造假）</li>
<li>调查员 调查出来是 <strong>中介</strong> 或者 <strong>有欺诈风向的</strong>，进入黑名单的.</li>
<li>…</li>
</ol>
<p>所以我们在定义模型负样本的话，我们可能定义为 m1+ 信用风险、m3+ (90天以上不还款的话)，我们可以定义为欺诈风险</p>
<p><code>坏用户</code> ： 欺诈风险用户<br><code>好用户</code> ： 一天都不逾期还款<br><code>灰用户</code> ： m1+ 未还款，但是90天之内可以还款的 (不放在训练中，否则会给模型带来很多额外的信息，影响效果)</p>
<p>灰用户不放在模型中，这样训练出的模型对好坏用户的区分程度也越高.</p>
</blockquote>
<p>金融模型 和 CTR预估 相比是 有一个周期性质的</p>
<blockquote>
<ol>
<li>广告点击的话，用户点击，立马有一个样本出现</li>
<li>做长期现金贷，选择样本是选择半年之前的用户，作为样本</li>
</ol>
</blockquote>
<h2 id="正负样本"><a href="#正负样本" class="headerlink" title="正负样本"></a>正负样本</h2><p>真实场景正负样本比例 <code>(10~30) ：1</code> （成熟平台的风险是越来越小的，所以我们拿到的 <code>正负样本比例</code>是逐步增加的）.</p>
<p>数据的不平衡处理 ： <strong>降采样</strong>、<strong>过采样</strong>、<strong>SMOTE</strong></p>
<ul>
<li><strong>降采样</strong> ： 正负样本成 3:1， 5:1 来做一个模型,  坏样本是全部取的, (一般这种情况 做评分卡的时候是需要做的)</li>
<li><strong>过采样</strong> ： 实际用的不多，如果负样本实在是过少 都 &lt;100 个， 那么可以考虑 减低我们的观察周期， 或者 欺诈定义的并不是一个很严格 来放进来多一些的 负样本过来来做训练，或者在拒贷里面找一些人过来.</li>
<li><strong>SMOTE</strong> ： 在分布上模拟一些数据，模拟完的数据可做训练，比较经典拿真实的数据做训练是更贴近真实的情况.<blockquote>
<p>做模型 如 GBRT、RF 等，他们对不平衡的数据是有容忍的，这种直接用真实数据进行训练，也能得到很好的效果.</p>
</blockquote>
</li>
</ul>
<h2 id="模型选型"><a href="#模型选型" class="headerlink" title="模型选型"></a>模型选型</h2><p>对于做评分卡的模型 或者 LR 的话，样本的平衡要在 10:1 范围内， LR 对变量相关性的筛选 和 数据平衡 有要求</p>
<p>做模型，至少要用 RF 来做模型， 或者 GBDT、GBRT，这种 Boosting 的模型，对于样本的不平衡容忍度更高一些，他们对于学习 更小而细微 的特征和变量 可以学习的更深一些.</p>
<h2 id="Lending-Club"><a href="#Lending-Club" class="headerlink" title="Lending Club"></a>Lending Club</h2><p>Lending Club 创立于 2016年， 主要做一个提供 P2P 贷款的平台中介服务，2016年底在 纽交所上市，后来爆出来很多丑闻，创始人离职，股价下跌. 但是不管怎么样，它的数据在我们做反欺诈等是非常有重要的.</p>
<blockquote>
<p>Lending Club 2016 的借贷数据，Q3，Q4 可以一起做一下，半年的数据做训练是更好的.</p>
</blockquote>
<h3 id="1-Data"><a href="#1-Data" class="headerlink" title="1. Data"></a>1. Data</h3><p>Lending Club 2016年Q3数据：<a href="https://www.lendingclub.com/info/download-data.action" target="_blank" rel="external">https://www.lendingclub.com/info/download-data.action</a></p>
<p>参考：<a href="http://kldavenport.com/lending-club-data-analysis-revisted-with-python/" target="_blank" rel="external">http://kldavenport.com/lending-club-data-analysis-revisted-with-python/</a></p>
<p><img src="/images/datascience/finance-LC-18.jpg" width="900"></p>
<p>看下数据，其实我也不能完全了解这些所有字段的含义</p>
<blockquote>
<ol>
<li>int_rate 利率</li>
<li>term 待多少期</li>
<li>grade 等级 C、B、D，7个等级吧</li>
<li>sub_grade 会分为更细的等级.</li>
<li>后面这些是从 FICO 获取的数据吧…</li>
</ol>
</blockquote>
<p>我们的目的是判断，来了一个用户，之后输入该用户的这些特征，我们判断他是不是一个欺诈用户</p>
<p><strong>如果用户填写假资料</strong></p>
<blockquote>
<p>用户贷款之后的表现，如果填的真假我们不了解，填写的是假资料，但是之后还款表现好那么它还是一个好用户.</p>
</blockquote>
<p><strong>数据上</strong> </p>
<blockquote>
<p>我只取了2016年Q3数据，9W+ 的数据，列数 122 列。数据有 99124行， 去掉表头，有 99123 行</p>
</blockquote>
<h2 id="2-Keep-what-we-need"><a href="#2-Keep-what-we-need" class="headerlink" title="2. Keep what we need"></a>2. Keep what we need</h2><blockquote>
<p>我们初步做特征筛选…, 我们在看的时候，可以分片分片的看这 122 个列…</p>
</blockquote>
<h3 id="2-1-特征分析-part1"><a href="#2-1-特征分析-part1" class="headerlink" title="2.1 特征分析 part1"></a>2.1 特征分析 part1</h3><p><img src="/images/datascience/finance-LC-19.jpg" width="850"></p>
<blockquote>
<p>id 和 member_id 不作为特征，可以直接去掉, int_rate 带 % 的可以直接去掉 %， 变为 float 的</p>
</blockquote>
<p><strong>Loan Amount Requested Verus the Funded Amount</strong></p>
<p><img src="/images/datascience/finance-LC-20.png" width="850"></p>
<h3 id="2-2-特征分析-part2"><a href="#2-2-特征分析-part2" class="headerlink" title="2.2 特征分析 part2"></a>2.2 特征分析 part2</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.ix[:<span class="number">5</span>,<span class="number">8</span>:<span class="number">15</span>]</span><br><span class="line"></span><br><span class="line">print(df.emp_title.value_counts().head())</span><br><span class="line">print(df.emp_title.value_counts().tail())</span><br><span class="line">df.emp_title.unique().shape </span><br><span class="line"><span class="comment"># 37421 emp_title， 太多了，可信度不高，我们也无法做 emp_title 非数值型变量的 one-hot enconding</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df.drop([&apos;emp_title&apos;],1, inplace=True)</span><br></pre></td></tr></table></figure>
<p>employment title</p>
<hr>
<ul>
<li><a href="https://github.com/blair101/machine-learning-action/blob/master/LC/LC.ipynb" target="_blank" rel="external">LC code</a></li>
</ul>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>删不删除变量，需要看模型，LR 需要删除，GBRT 不用删除也可以.</p>
<p>LR 做评分卡模型，变量 一定是强变量，20个左右，不会几百个，有个变量可以训练处系数</p>
<p>出来 0 ~ 1 概率，拉一下橡皮筋， 分数映射，拉到 300 ~ 900 分，可以做一个评分卡，600分 可能是好用户:坏用户，可能是 50:1</p>
<p>评分卡的阶梯可能是增加。</p>
<p>模型不同 0 ~ 1 的概率可能是不同的，那么我增加50分，风险水平会降低一半</p>
<p>550 25:1</p>
<p>评分卡分的映射和模型是没有关系的，是样本里人群的好坏是有关系的，所以模型的参数做映射，是不需要重新训练的。</p>
<p>lending club 要求 FICO 是个特定的评分方法，是一个固定的评分方法。</p>
<p>比如 芝麻信用都是用自己的模型，自己算出来的</p>
<p>如果人群变了</p>
<p>模型的稳定性非常重要，当前要评估的人群已经和去年下半年的用户已经不一样的，所以训练的时候要尽可能提升模型的稳定性，如果训练时模型稳定性非常差，那么一上线就崩溃了。</p>
<p>如何提高模型稳定性，2种方法</p>
<ol>
<li><p>特征筛选的时候，我会去把特征从样本的时间开始，2016.06 开始每个月我一直在看它的均值和方差的变化是否在容忍的范围内，比如我去年这个月这个特征是30，当前 2017.06 这个特征变为了 100，那么这个特征变化太大，是不能用的，超过50%， 太不稳定了，其实这个变量，或者做评分卡，反欺诈等是不合适的，直接扔掉。</p>
</li>
<li><p>尽量做模型融合，单模型的模型稳定性是不好的，随着月份的变化，你的预测是有变化的，波动的范围是有点低，ensemble 集成学习，</p>
<p>三种方法 (bagging、boosting，Stacking）</p>
</li>
</ol>
<p>2.1 Bagging 比如 RF，每个模型取一样的权重，进行评估<br>2.2 Boosting 根据模型训练出不同权重，给予不同的权重<br>2.3 Stacking 我在用一个分类器，去处理我要集成的这3，4种模型，训练出一个参数</p>
<p>这三种方法，都能提高模型的稳定性</p>
<p>让你在线上运行 3~6 个月，信贷产品比较长的话，2个月更新一次比较好，贷款周期短的话，周更新都可以</p>
<blockquote>
<p>有做 KS 比较高的话，会送大家小礼品，</p>
</blockquote>
<p>我们线上有用 spark streaming 也有处理实时特征，但是目前体量，一般单机和离线处理就够了。</p>
<p>9W 个用户，100多个变量，那根本不需要用分布式来计算了。</p>
<p>半年的样本数据，把数据取出来之后，你要定义你的<code>好坏</code>样本,会把一些灰色地段的用户给他摘除掉，只留下最好或者最坏的用户，这些用户提特征之后，在做训练，<code>样本内的验证</code>和<code>跨时间的验证</code> ，就是说我的时间段是完全不一样的，那么能够验证模型的稳定性，那么最好就要拿 2017年，1和2月的数据，在做一个跨时间的验证，跨时间的验证才是你真正上线之后的效果，因为你在时间窗口内训练或者test的话，它的 ks 可能 30 多，如果跨时间验证的话，你的人群可能会偏移，那么ks可能会下降，ks就变为20，如果差别控制在 15%，差别大稳定性就很不好，是不能上线的。</p>
<p>模型的话，你现在开始做模型，你一定取的是 去年 下半年的是数据，做验证的话是拿去年1月份的数据，一个月的数据还有5，6，7个还款表现，基本上等你做完模型，你做跨时间验证的话，刚刚好，你花2个礼拜做一个模型，上线的时候，你就不需要重新训练了。除非你到9月份上线，那么时间久了，就需要重新训练，一般是不需要重新训练的。</p>
<p>欺诈模型的稳定性评价指标： 1. 对比训练集 与 跨时间验证集 的 KS 偏差，一般偏差大不大的话，觉得这个模型是可以在时间维度上hold住的，那么可以模型上线。另外指标金融上比较常用的指标是 psi，这个是验证不同人群的偏移程度，以后可以自己查查资料。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://myslide.cn/slides/3199" target="_blank" rel="external">金融反欺诈场景下的Spark实践</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[互联网金融风控中的数据科学 (part2)  ： 模型策略]]></title>
      <url>http://sggo.me/2018/04/21/data-science/internet-finance-2/</url>
      <content type="html"><![CDATA[<p>反欺诈也是一种机器学习过程， 反欺诈建模中的数据科学</p>
<a id="more"></a>
<p><img src="/images/datascience/finance-8.jpg" width="850"></p>
<h2 id="反欺诈也是一种机器学习过程"><a href="#反欺诈也是一种机器学习过程" class="headerlink" title="反欺诈也是一种机器学习过程"></a>反欺诈也是一种机器学习过程</h2><p><img src="/images/datascience/finance-9.jpg" width="850"></p>
<blockquote>
<p>对于做互联网金融一般情况是 正负样本 是极度不平衡的(最高可以达到 100 : 1), 这样的情况对于 SVM 这种分类器是不合适的，所以在做金融的<strong>评分卡模型</strong> 或 <strong>欺诈模型</strong> 也好，这样对特征的处理 和 样本的非平衡处理是比较高的.</p>
<p>好坏用户的定义，一般是根据用户的贷后表现，来定义好坏用户的.</p>
</blockquote>
<p>举个栗子🌰 :</p>
<blockquote>
<p>用户借款 5W 元，可能是分期还款 12个月，这样每个月都会还一笔固定的额度.</p>
<p><strong>信用风险</strong> : 在挺长的时间可以按时还款.</p>
<p><strong>欺诈风险</strong> : </p>
<ol>
<li>用户可能 第 1、2 期 是还的，之后是不还的.<br> (因为中介也越来越聪明，给他自己留出时间，躲避催收的手段，也躲避追踪等等)</li>
<li>贷前审核 （触碰到拒贷规则）</li>
<li>造假行为 （信息资料造假）</li>
<li>调查员 调查出来是 <strong>中介</strong> 或者 <strong>有欺诈风向的</strong>，进入黑名单的.</li>
<li>…</li>
</ol>
<p>所以我们在定义模型负样本的话，我们可能定义为 m1+ 信用风险、m3+ (90天以上不还款的话)，我们可以定义为欺诈风险</p>
<p><code>坏用户</code> ： 欺诈风险用户<br><code>好用户</code> ： 一天都不逾期还款<br><code>灰用户</code> ： m1+ 未还款，但是90天之内可以还款的 (不放在训练中，否则会给模型带来很多额外的信息，影响效果)</p>
</blockquote>
<p>金融模型 和 CTR 预估的相比是 有一个周期性质的</p>
<blockquote>
<ol>
<li>广告点击的话，用户点击，立马有一个样本出现</li>
<li>做长期现金贷，选择样本是选择半年之前的用户，作为样本</li>
</ol>
</blockquote>
<h2 id="模型策略"><a href="#模型策略" class="headerlink" title="模型策略"></a>模型策略</h2><p><img src="/images/datascience/finance-10.jpg" width="800"></p>
<h3 id="1-Linear-Regression"><a href="#1-Linear-Regression" class="headerlink" title="1. Linear Regression"></a>1. Linear Regression</h3><p><img src="/images/datascience/finance-11.jpg" width="800"></p>
<h3 id="2-Logistic-Regression"><a href="#2-Logistic-Regression" class="headerlink" title="2 Logistic Regression"></a>2 Logistic Regression</h3><p><img src="/images/datascience/finance-12.jpg" width="800"></p>
<h3 id="3-Decision-Tree"><a href="#3-Decision-Tree" class="headerlink" title="3. Decision Tree"></a>3. Decision Tree</h3><p><img src="/images/datascience/finance-13.jpg" width="850"></p>
<h3 id="4-Random-Forest"><a href="#4-Random-Forest" class="headerlink" title="4. Random Forest"></a>4. Random Forest</h3><p><img src="/images/datascience/finance-14.png" width="850"></p>
<h3 id="5-Gradient-Boosting-RT"><a href="#5-Gradient-Boosting-RT" class="headerlink" title="5. Gradient Boosting RT"></a>5. Gradient Boosting RT</h3><p><img src="/images/datascience/finance-15.png" width="850"></p>
<blockquote>
<hr>
</blockquote>
<p><img src="/images/datascience/finance-16.png" width="850"></p>
<h2 id="结果评估-混淆矩阵"><a href="#结果评估-混淆矩阵" class="headerlink" title="结果评估-混淆矩阵"></a>结果评估-混淆矩阵</h2><ul>
<li>Precision: 评估认定坏用户的精确度</li>
<li>Recall: 评估坏用户的召回率</li>
<li>F-Measure: 组合判断</li>
</ul>
<p><img src="/images/datascience/finance-17.png" width="830"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://myslide.cn/slides/3199" target="_blank" rel="external">金融反欺诈场景下的Spark实践</a></li>
<li><a href="http://www.itdks.com/dakalive/detail/442" target="_blank" rel="external">大咖说 王婷</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[互联网金融风控中的数据科学 (part1) ： 金融科技企业面临的欺诈⻛险]]></title>
      <url>http://sggo.me/2018/04/20/data-science/internet-finance-1/</url>
      <content type="html"><![CDATA[<p>金融科技企业面临的欺诈⻛险介绍 , 互联网金融 主要是通过互联网平台，连接 出借方 和 借款方</p>
<a id="more"></a>
<h2 id="1-金融与科技的结晶"><a href="#1-金融与科技的结晶" class="headerlink" title="1. 金融与科技的结晶"></a>1. 金融与科技的结晶</h2><ul>
<li>金融的本质 : 资源的最合理化应用</li>
<li>互联网技术 : 交易的边界成本趋向“零”</li>
<li>金融科技 : 用大数据、云计算等技术实现的资金融通、支付、投资和信息中介服务</li>
</ul>
<p><img src="/images/datascience/finance-1.jpg" width="520"></p>
<blockquote>
<p>我们国家没有覆盖度很全的所有人的征信，虽然有央行，但是还远远不够…</p>
<p>授权抓取的用户的数据，(百融、同盾、芝麻信用 等，工作中可能都会对接到这些平台)，这些平台会通过很多渠道收集用户，然后给用户打一些风险标签和欺诈的评分.</p>
</blockquote>
<h2 id="2-中国信用贷款行业"><a href="#2-中国信用贷款行业" class="headerlink" title="2. 中国信用贷款行业"></a>2. 中国信用贷款行业</h2><p>&nbsp;&nbsp;&nbsp; 网贷之家 收集P2P网贷平台的运营数据，并为行业排行, 以下为 2018年3月数据做的行业排行 :</p>
<p><img src="/images/datascience/finance-3.jpg" width="800"></p>
<blockquote>
<p>网贷之家 : <a href="http://www.wdzj.com/pingji.html" target="_blank" rel="external">http://www.wdzj.com/pingji.html</a></p>
<p>现在消亡的 P2P 公司逐渐消亡的也很多，之前 3000 多家，行业大洗牌之后，现在 2000 多家…</p>
<p>在整个行业的体量上，陆金所 一定是体量非常大的，整个网贷的市场大概有万亿级别.</p>
</blockquote>
<h2 id="3-中国信用贷款行业分层"><a href="#3-中国信用贷款行业分层" class="headerlink" title="3. 中国信用贷款行业分层"></a>3. 中国信用贷款行业分层</h2><p><img src="/images/datascience/finance-4.jpg" width="800"></p>
<blockquote>
<p>APR (Annual Percentage Rate) 年利率, 对银行信用卡来说，一般 APR 在 16% ~ 18%, 那么日利率为 0.04%~0.05% 之间</p>
</blockquote>
<p>举个栗子🌰 :</p>
<blockquote>
<p>银行是按日收利息的。简单介绍一下利息是如何计算的 :</p>
<p>假设用信用卡提现1000刀，20天后还清，这张卡的Cash Advance APR是25.49%.<br>一年有365天（部分银行按360天算（不要问我闰年怎么算Orz））日利率应该是25.49%/365=0.07%.<br>20天后产生的利息为1000 * 20 * 0.07% = 14刀</p>
</blockquote>
<p>APR 可以划分人群，APR 不同级别，贷前贷后的审核，催收的制度 也是不同的</p>
<blockquote>
<p>APR越低，人群是越好的. 在 APR 低的人群，是基本不需要催收的，在 APR 高的人群，是要催收的.</p>
</blockquote>
<p>高 APR 人群:</p>
<blockquote>
<p>对于 APR 在 40% ~ 80% 的，比如 拍拍贷，这种小额的现金贷，5000 以下，7天~1个月，贷款的时间也短，多还的利息用户是不感知的，但是如果变成年化，APR 就会非常高</p>
<p>在 APR &gt; 80%, 是 现金巴士，用钱宝，这些存在也是有人们的需求存在的<br>APR 低的用户，就是信用好，APR高的话，就是信用没有那么好，或者还款能力没有那么好</p>
</blockquote>
<p><strong>做金融最大的本质就是在控制风险，在风险可控的情况下获得最大的利润</strong>.</p>
<h2 id="4-个人对个人的信用贷款"><a href="#4-个人对个人的信用贷款" class="headerlink" title="4. 个人对个人的信用贷款"></a>4. 个人对个人的信用贷款</h2><p><img src="/images/datascience/finance-5.jpg" width="800"></p>
<h2 id="5-急速信任-自动化信用评估"><a href="#5-急速信任-自动化信用评估" class="headerlink" title="5. 急速信任-自动化信用评估"></a>5. 急速信任-自动化信用评估</h2><p><img src="/images/datascience/finance-6.jpg" width="800"></p>
<p>国内外 P2P 网贷的比较 :</p>
<blockquote>
<p>国家金融环境存在较大区别,在信用体系建设等方面也都存在很大的差异，国外拥有较为完善的信用评估体制,中国在这方面却非常缺失，所以国内的借贷平台在用户信用评估方面都做出自己的努力，构建了不同形式的评价方法.</p>
<p>国外有完善的信用评估体质，有 <a href="https://xueqiu.com/k?q=FICO#/" target="_blank" rel="external">FICO</a>官方的评分. 国外80%都是信用风险，20%是欺诈风险. <code>中国更多的是欺诈风险</code>.</p>
<p><a href="https://xueqiu.com/k?q=FICO#/" target="_blank" rel="external">FICO</a> 成立于1956年，为纽交所上市公司，市值52亿美金，提供跨多个行业的分析软件和工具.</p>
</blockquote>
<p>国内黑产业链 :</p>
<blockquote>
<p>国内不还钱的话惩罚的措施跟不上，国内有些中介我不还钱的话，你找不到我的话，这个钱就是我空手套白狼的利润，这样催生了越来越多贷款的欺诈的情况，他们有一个黑产业链，从账号的获取到恶意的注册，再到互联网金融公司的平台申请贷款，有的中介会有一些现象. </p>
</blockquote>
<p>举个栗子🌰: </p>
<blockquote>
<p>他们会到燕郊找一批老人妇女，然后说我给你3000元钱，你跟着我走一趟。还有一些客户对自己的资质没有信心，然后找中介包装一些材料。有好中介，有坏中介，如果坏中介带你贷了5W元钱，然后给你2.5W告诉你爱还不还，然后还可以带你的信息再去其他家平台再贷款，这样用户在不知情的情况下会背负很多债务信息.</p>
<p>中介做的事，就是不停的去试各个P2P平台产品，发现其中漏洞，这些中介比产品经理还要了解这个产品，然后他帮助他的客户去做包装，这样比如一下子可能进来 100 个欺诈用户，每个用户5W，这样一下子就是500W，对企业来说损失很大，然后在这种高额收入的诱惑下，这些中介会升级不断自己的伪造技术.</p>
</blockquote>
<h2 id="6-金融科技企业面临的欺诈风险"><a href="#6-金融科技企业面临的欺诈风险" class="headerlink" title="6. 金融科技企业面临的欺诈风险"></a>6. 金融科技企业面临的欺诈风险</h2><p><img src="/images/datascience/finance-7.jpg" width="800"></p>
<blockquote>
<p>对于线上反欺诈来说，你看不见用户，只面对数据，要发现数据之间的异常、用户与用户之间有没有异常相似度联系等.</p>
</blockquote>
<h2 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h2><ul>
<li><a href="https://myslide.cn/slides/3199" target="_blank" rel="external">宜人贷数据科学家王婷: 金融反欺诈场景下的Spark实践</a></li>
<li><a href="http://www.itdks.com/dakalive/detail/442" target="_blank" rel="external">大咖说 王婷</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Ensemble Learning (part2)]]></title>
      <url>http://sggo.me/2018/04/11/ml/9-ensumble-boosting-2/</url>
      <content type="html"><![CDATA[<p>提升（boosting）方法是一类应用广泛且非常有效的统计学习方法。</p>
<ul>
<li>Boosting 概念</li>
<li>代表性 Boosting 算法 AbaBoost 介绍</li>
</ul>
<a id="more"></a>
<blockquote>
<p>《An Empirical Comparison of Supervised Learning Algorithms》ICML2006.</p>
</blockquote>
<p>Adaboost 在处理二类分类问题时，随着弱分类器的个数增加，训练误差与测试误差的曲线图。</p>
<div class="limg1"><br><img src="/images/ml/ensumble/ml_boosting_adaboost_binary_classification.png" width="400"><br></div>

<p>从图中可以看出，Adaboost算法随着模型复杂度的增加，测试误差（红色点线）基本保持稳定，并没有出现过拟合的现象。</p>
<p>其实不仅是Adaboost算法有这种表现，Boosting方法的学习思想和模型结构上可以保证其不容易产生过拟合（除非Weak Learner本身出现过拟合）。</p>
<p>下面我们主要是从损失函数的差异，来介绍Boosting的家族成员；然后我们针对每个具体的家族成员，详细介绍其学习过程和核心公式；最后从算法应用场景和工具方法给出简单的介绍。</p>
<p><strong>Boosting</strong></p>
<p>Boosting方法基于这样一种思想：</p>
<blockquote>
<p>对于一个复杂任务来说，将多个专家的判定进行适当的综合得出的判断，要比其中任何一个专家单独的判断好。</p>
<p>就是 “三个臭皮匠顶个诸葛亮” …😄😄😄</p>
</blockquote>
<h2 id="1-概率可学习性-PAC"><a href="#1-概率可学习性-PAC" class="headerlink" title="1. 概率可学习性 (PAC)"></a>1. 概率可学习性 (PAC)</h2><p>PAC理论是由2010年图灵奖的得主Valiant和Kearns提出的一套理论体系，主要讨论什么时候，一个问题是可以被学习的。</p>
<blockquote>
<p>PAC体系定义了学习算法的强弱：</p>
<p>(1) 弱学习算法 : 如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，<br>(2) 强学习算法 : 存在一个多项式的学习算法能够学习它，并且正确率很高</p>
</blockquote>
<p>在概率近似正确（probably approximately correct，PAC）学习框架中：</p>
<blockquote>
<p>①. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；</p>
<p>②. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。</p>
</blockquote>
<p>Valiant和 Kearns提出PAC学习模型中弱学习算法和强学习算法的等价性猜想：</p>
<blockquote>
<p>该猜想最重要的含义是，如果二者等价 ,那么只需找到一个比随机猜测略好的弱学习算法就可以将其提升为强学习算法，而不必寻找很难获得的强学习算法。</p>
<p>该问题的重要性随即引起方法论大师的追捧，大家都在试图设计算法来验证PAC理论的正确性。</p>
<p>1996，Schapire提出一种新的名叫AdaBoost的算法证明了上述猜想。AdaBoost把多个不同的决策树用一种<strong>非随机的方式组合</strong>起来，表现出惊人的性能。同时，Schapire证明强可学习与弱可学习是等价的，也就是说，<strong>在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的</strong>。</p>
</blockquote>
<p>Summary : <code>强可学习⇔弱可学习</code></p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>对于一个学习问题来说（以分类问题为例），给定训练数据集，求一个弱学习算法要比求一个强学习算法要容易的多。Boosting方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后<strong>组合弱分类器，得到一个强分类器</strong>。Boosting方法在学习过程中通过<strong>改变训练数据的权值分布</strong>，针对不同的数据分布调用弱学习算法得到一系列弱分类器。</p>
<blockquote>
<p>还有就是，Boosting算法更加关注错分的样本，这点和Active Learning的寻找最有价值的训练样本有点遥相呼应的感觉</p>
<p>很抽象对不对，但是过一会儿我们通过Adaboost来理解这个核心思想</p>
</blockquote>
<p><strong>回答两个问题</strong> ：</p>
<ol>
<li>在每一轮学习之前，如何改变训练数据的权值分布？</li>
<li>如何将一组弱分类器组合成一个强分类器？</li>
</ol>
<blockquote>
<p>具体不同的boosting实现，主要区别在弱学习算法本身和上面两个问题的回答上。</p>
</blockquote>
<p>问题1，Adaboost算法的做法是 ：</p>
<blockquote>
<p>提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。</p>
<p>如此，那些没有得到正确分类的样本，由于其权值加大而受到后一轮的弱分类器的更大关注。</p>
</blockquote>
<p>问题2，AdaBoost采取加权多数表决的方法 ：</p>
<blockquote>
<p>(1). 加大 分类误差率小 的弱分类器的权值，使其在表决中起较大的作用；<br>(2). 减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</p>
</blockquote>
<p>AdaBoost算法的巧妙之处就在于它将这些学习思路自然并且有效地在一个算法里面实现。</p>
<h3 id="Boosting算法代表-：Adaboost-Adaptive-Boosting"><a href="#Boosting算法代表-：Adaboost-Adaptive-Boosting" class="headerlink" title="Boosting算法代表 ：Adaboost(Adaptive Boosting)"></a>Boosting算法代表 ：Adaboost(Adaptive Boosting)</h3><blockquote>
<p>核心思想：一种迭代算法，针对同一个训练集训练不同的分类器(弱分类器)，然后进行分类，对于分类正确的样本权值低，分类错误的样本权值高（通常是边界附近的样本），最后的分类器是很多弱分类器的线性叠加（加权组合），分类器相当简单。实际上就是一个简单的弱分类算法提升(boost)的过程。</p>
</blockquote>
<p><strong>看图形来过一遍Adaboost算法</strong></p>
<p><img src="/images/ml/ensumble/ml-ensumble-4-adaboost.jpeg" width="600"></p>
<blockquote>
<p>算法开始前，需要将每个样本的权重初始化为 1/m, 这样一开始每个样本都是等概率的分布，每个分类器都会公正对待.</p>
</blockquote>
<p><img src="/images/ml/ensumble/ml-ensumble-5-adaboost.jpeg" width="600"></p>
<blockquote>
<p>Round1，因为样本权重都一样，所以分类器开始划分，根据自己分类器的情况，只和分类器有关。划分之后发现分错了三个”+”号，那么这些分错的样本，在给下一个分类器的时候权重就得到提高,也就是会影响到下次取训练样本的分布，就是提醒下一个分类器，“诶！你注意点这几个小子，我上次栽在他们手里了！”</p>
</blockquote>
<p><img src="/images/ml/ensumble/ml-ensumble-6-adaboost.jpeg" width="600"></p>
<blockquote>
<p>Round2,第二代分类器信誓旦旦的对上一代分类器说”我知道了，大哥！我一定睁大眼睛好好分着三个玩意！”ok，这次三个上次分错的都被分出来了，但是并不是全部正确，这次又栽倒在左下角三个”-“上了，然后临死前，第二代分类器对下一代分类器说”这次我和上一代分类器已经把他们摸得差不多了，你再稍微注意下左下角那三个小子，也别忘了上面那三个(一代错分的那三个”+”)！”</p>
</blockquote>
<p><img src="/images/ml/ensumble/ml-ensumble-7-adaboost.jpeg" width="600"></p>
<blockquote>
<p>Round3:有了上面两位大哥的提醒，第三代分类器表示，我差不多都知道上次大哥们都错哪了，我只要小心这几个，应该没什么问题！只要把他们弄错的我给整对了，然后把我们收集的信息一对，这不就行了么！ok，第三代分类器不负众望，成功分对上面两代分类器重点关注的对象，至于分错的那几个小的，以前大哥们都分对了，我们坐下来核对一下就行了！</p>
</blockquote>
<p><img src="/images/ml/ensumble/ml-ensumble-8-adaboost.jpeg" width="600"></p>
<blockquote>
<p>最后，三个分类器坐下来，各自谈了谈心得，分配了下权重，然后一个诸葛亮就诞生啦！这也就是 “三个臭皮匠顶个诸葛亮的故事” …😄😄😄, 是不是道理很简单！至于权重如何计算，暂不在本文讨论.</p>
</blockquote>
<p><strong>Adaboost 优点</strong></p>
<blockquote>
<ol>
<li>可以使用各种方法构造子分类器，Adaboost算法提供的是框架</li>
<li>简单，不用做特征筛选</li>
<li>相比较于RF，更不用担心过拟合问题</li>
</ol>
</blockquote>
<p><strong>Adaboost 缺点</strong></p>
<blockquote>
<ol>
<li>Adaboost对于<strong>噪声是十分敏感</strong>的。Boosting方法本身对噪声点异常点很敏感，因此在每次迭代时候会给噪声点较大的权重，这不是我们系统所期望的。</li>
<li>运行速度慢，凡是涉及迭代的基本上都无法采用并行计算，Adaboost是一种”串行”算法.所以GBDT(Gradient Boosting Decision Tree)也非常慢。</li>
</ol>
</blockquote>
<p><strong>Pay Attention</strong></p>
<blockquote>
<ol>
<li>Bagging 树”并行”生成,如 Random Forest ; Boosting：树”串行”生成,如Adaboost</li>
<li>Boosting 中基模型为弱模型，而 Random Forest 中的基树是强模型(大多数情况)</li>
<li><p>Boosting 重采样的不是样本，而是样本的分布，每次迭代之后，样本的分布会发生变化，也就是被分错的样本会更多的出现在下一次训练集中</p>
</li>
<li><p>明确一点，我们迭代也好(Adaboost), 并行(RF)也好，只和训练集有关，和测试集真的一毛钱关系都没有好么！我们先把原始数据分类测试集和训练集，然后测试集放一边，训练集里面再挑子集作为迭代算法用的训练集！这个和<a href="http://statweb.stanford.edu/~tibs/sta306bfiles/cvwrong.pdf" target="_blank" rel="external">K-Fold Cross-Validation</a>思想类似.</p>
</li>
</ol>
</blockquote>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="https://www.jianshu.com/p/708dff71df3a" target="_blank" rel="external">总结：Bootstrap(自助法)，Bagging，Boosting(提升)</a></li>
<li><a href="https://blog.csdn.net/xlinsist/article/details/51475345" target="_blank" rel="external">Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost</a></li>
<li><a href="http://bbs.quanttech.cn/article/524" target="_blank" rel="external">机器学习选讲：AdaBoost方法详解</a></li>
<li><a href="http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/" target="_blank" rel="external">52caml</a></li>
<li><a href="https://www.zhihu.com/question/49386395" target="_blank" rel="external">统计学习方法</a></li>
<li><a href="https://blog.csdn.net/u010859707/article/details/78677989" target="_blank" rel="external">Scikit-Learn 中文文档 概率校准 - 监督学习</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Ensemble Learning (part1)]]></title>
      <url>http://sggo.me/2018/04/07/ml/9-ensumble-boosting-1/</url>
      <content type="html"><![CDATA[<p>Ensemble learning（集成学习）：是目前机器学习的一大热门方向，所谓集成学习简单理解就是指采用多个分类器对数据集进行预测，从而提高整体分类器的泛化能力。</p>
<a id="more"></a>
<ol>
<li>Bootstraping</li>
<li>Bagging、Boosting、Stacking</li>
</ol>
<h2 id="Bootstraping"><a href="#Bootstraping" class="headerlink" title="Bootstraping"></a>Bootstraping</h2><p>Bootstraping的名称来自成语 “pull up by your own bootstraps”，意思是依靠你自己的资源，它是一种有放回的抽样方法.</p>
<blockquote>
<p>注:Bootstrap本义是指高靴子口后面的悬挂物、小环、带子，是穿靴子时用手向上拉的工具。“pull up by your own bootstraps”即“通过拉靴子让自己上 升”，意思是“不可能发生的事情”。后来意思发 生了转变，隐喻“不需要外界帮助，仅依靠自身力 量让自己变得更好”</p>
</blockquote>
<p>bootstraping 的思想和步骤如下：</p>
<p>举个🌰：我要统计鱼塘里面的鱼的条数，怎么统计呢？假设鱼塘总共有鱼1000条，我是开了上帝视角的，但是你是不知道里面有多少。</p>
<p>步骤：</p>
<ol>
<li>承包鱼塘，不让别人捞鱼(规定总体分布不变)。</li>
<li>自己捞鱼，捞100条，都打上标签(构造样本)</li>
<li>把鱼放回鱼塘，休息一晚(使之混入整个鱼群，确保之后抽样随机)</li>
<li>开始捞鱼，每次捞100条，数一下，自己昨天标记的鱼有多少条，占比多少(一次重采样取分布)。</li>
<li>重复3，4步骤n次。建立分布。</li>
</ol>
<blockquote>
<p>假设一下，第一次重新捕鱼100条，发现里面有标记的鱼12条，记下为12%，放回去，再捕鱼100条，发现标记的为9条，记下9%，重复重复好多次之后，假设取置信区间95%，你会发现，每次捕鱼平均在10条左右有标记，所以，我们可以大致推测出鱼塘有1000条左右。其实是一个很简单的类似于一个比例问题。这也是因为提出者Efron给统计学顶级期刊投稿的时候被拒绝的理由–”太简单”。这也就解释了，为什么在小样本的时候，bootstrap效果较好，你这样想，如果我想统计大海里有多少鱼，你标记100000条也没用啊，因为实际数量太过庞大，你取的样本相比于太过渺小，最实际的就是，你下次再捕100000的时候，发现一条都没有标记，，，这TM就尴尬了。。</p>
</blockquote>
<p><strong>Bootstrap 经典语录</strong></p>
<blockquote>
<p>Bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。<br>就是一个在自身样本重采样的方法来估计真实分布的问题</p>
<p>当我们不知道样本分布的时候，bootstrap方法最有用。</p>
</blockquote>
<h2 id="Ensemble-learning"><a href="#Ensemble-learning" class="headerlink" title="Ensemble learning"></a>Ensemble learning</h2><p>了解 boosting 和 bagging 之前，先了解一下什么是 ensemble，一句话，三个臭皮匠顶个诸葛亮，一箭易折十箭难折，千里之堤溃于蚁穴 …😄😄😄 ，在分类的表现上就是，多个弱分类器组合变成强分类器。</p>
<p><img src="/images/ml/ensumble/ml-ensemble-1.png" width="500"></p>
<blockquote>
<p>假设各弱分类器间具有一定差异性（如不同的算法，或相同算法不同参数配置），这会导致生成的分类决策边界不同，也就是说它们在决策时会犯不同的错误。将它们结合后能得到更合理的边界，减少整体错误，实现更好的分类效果。</p>
</blockquote>
<h3 id="Bagging-bootstrap-aggregation"><a href="#Bagging-bootstrap-aggregation" class="headerlink" title="Bagging (bootstrap aggregation)"></a>Bagging (bootstrap aggregation)</h3><blockquote>
<p>bagging：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果,至于为什么叫bootstrap aggregation，因为它抽取训练样本的时候采用的就是bootstrap的方法！</p>
</blockquote>
<p>Bagging 策略过程 😄 :</p>
<ol>
<li>从样本集中重采样(有重复的)选出n个样本</li>
<li>在所有属性上，对这n个样本建立分类器 (ID3、C4.5、CART、SVM、Logistic回归等)</li>
<li>重复以上两步m次，即获得了m个分类器</li>
<li>将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类.</li>
<li>投票机制 (多数服从少数, 民主政治) 看到底分到哪一类(分类问题)</li>
</ol>
<p><img src="/images/ml/ensumble/ml-ensemble-2-bagging.jpeg" width="600"></p>
<p><strong>Bagging 代表算法 - Random Forest</strong></p>
<p>随机森林，它不仅可以用来做分类，也可用来做回归即预测，一般随机森林机由多个决策树构成，相比于单个决策树算法，它分类、预测效果更好，不容易出现过度拟合的情况。</p>
<p>1.训练样本选择方面的Random：</p>
<blockquote>
<p>Bootstrap方法随机选择子样本</p>
</blockquote>
<p>2.特征选择方面的Random：</p>
<blockquote>
<p>属性集中随机选择k个属性，每个树节点分裂时，从这随机的k个属性，选择最优的(如何选择最优又有各种最大增益的方法，不在本文讨论范围内)。</p>
</blockquote>
<p><strong>决策树</strong></p>
<p>决策树对训练样本有良好的分类能力，只要我们的层数不加限制，我们甚至可以把它分的没有任何误差，这样可能导致你的泛化能力很弱。 缓解的方法就是 1. 剪枝 2. 随机森林</p>
<p>剪枝 我还没用过，所以我们看常用的 随机森林 Romdom Forest</p>
<p>决策树 ： 特征选择</p>
<ol>
<li>ID3 仅具有教学价值</li>
<li>gini 系数，作为指标比较多，在实践当中</li>
</ol>
<p><strong>Random Forest 构造流程</strong></p>
<p><img src="/images/ml/ensumble/ml-ensemble-3-bagging.jpeg" width="600"></p>
<blockquote>
<ol>
<li>用 Random(训练样本用Bootstrap方法，选择分离叶子节点用上面的2)的方式构造一棵决策树(CART)</li>
<li>用1的方法构造很多决策树, 不剪枝, 许多决策树构成一片森林，决策树之间没有联系</li>
<li>测试数据进入每一棵决策树，每棵树做出自己的判断，然后投票选出最终所属类别(默认每棵树权重一致)</li>
</ol>
</blockquote>
<p><strong>Random Forest 优点</strong></p>
<blockquote>
<ol>
<li>不容易出现过拟合，因为选择训练样本的时候就不是全部样本。</li>
<li>既可处理属性为离散值的量，比如ID3算法来构造树，也可以处理属性为连续值的量，比如C4.5算法来构造树</li>
<li>对于高维数据集的处理能力令人兴奋，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。该模型能够输出变量的重要性程度，这是一个非常便利的功能。</li>
<li>分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法</li>
</ol>
</blockquote>
<p><strong>Random Forest 缺点</strong></p>
<blockquote>
<ol>
<li>随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。</li>
<li>对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。</li>
</ol>
<p>优缺点，需要与杰神商讨再?</p>
</blockquote>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="https://www.jianshu.com/p/708dff71df3a" target="_blank" rel="external">Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost</a></li>
<li><a href="https://blog.csdn.net/xlinsist/article/details/51475345" target="_blank" rel="external">Bootstrap(自助法)，Bagging，Boosting(提升)</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[one-hot encoding 与 dummy encoding（not finish）]]></title>
      <url>http://sggo.me/2018/04/05/ml/3-feature-one-hot-dummy/</url>
      <content type="html"><![CDATA[<p>One-Hot编码和哑变量应该怎么用考虑一个具有三个类别的离散型特征，采用 One-Hot 编码后：</p>
<a id="more"></a>
<p>其中</p>
<p>因此有</p>
<p>从上面的公式可以看出，参数(θ0, θ1, θ2, θ3) 与参数(θ0 + αθ3, θ1 − αθ3, θ2 − αθ3, (1 − α)θ3) 等价，α 可以取任意值。此时模型很难学到靠谱的参数，此问题被称为虚拟陷阱（dummy variable trap）。产生这种问题的原因是因为偏置项θ0 与其它变量之间有线性相关关系，解决这个问题有以下三种方法：</p>
<blockquote>
<p>（1）去掉偏置项θ0 。此时上面公式中θ0=0，此时模型只有唯一解（思考下为什么）。<br>（2）引入正则项。既然有很多等价的参数，那我们可以考虑从这些等价的参数中选择我们最想要的，常用的做法就是使用正则项，控制参数的取值范围。<br>（3）使用哑变量替代One-Hot编码。此时上面公式中x3=0，模型只有唯一解.</p>
</blockquote>
<p>此问题思考的关键点在于，对于特征变量的引入，要保证参数之间不存在线性相关关系，如果存在线性相关关系，则容易出现虚拟陷阱的问题。上面考虑的是一个离散型特征的情况，如果有很多种不同类型的离散型特征，例如三个离散型特征，分别是M类、N类和P类，那么最终我们要用多少个变量来表示他们并且不出问题呢？</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.jianshu.com/p/08a5396ca2ed" target="_blank" rel="external">One-Hot编码和哑变量应该怎么用</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[一般英文聽力練習方法]]></title>
      <url>http://sggo.me/2018/01/28/English/english-cindy-listening-duoyi/</url>
      <content type="html"><![CDATA[<p>多益 以及 一般英文聽力加強 | 和Cindy學英文 笔记</p>
<a id="more"></a>
<h2 id="練習聽力"><a href="#練習聽力" class="headerlink" title="練習聽力"></a>練習聽力</h2><p>Useful Websites</p>
<ul>
<li><p><a href="http://www.newsinlevels.com/" target="_blank" rel="external">News In Levels</a></p>
</li>
<li><p><a href="https://umano.me/" target="_blank" rel="external">Umano</a></p>
</li>
<li><p><a href="http://lyricstraining.com/" target="_blank" rel="external">Lyrics Training</a></p>
</li>
</ul>
<p>Listen to Music</p>
<ul>
<li><p><a href="http://lyricstraining.com/" target="_blank" rel="external">Lyrics Training</a></p>
</li>
<li><p><a href="https://www.youtube.com/results?search_query=Taylor+Swift" target="_blank" rel="external">Taylor Swift、Miley Cyrus、Blake Shelton…</a></p>
</li>
</ul>
<p>Sitcom and Movies</p>
<ul>
<li><p><a href="http://www.livesinabox.com/friends/scripts.shtml" target="_blank" rel="external">Friends</a></p>
</li>
<li><p><a href="https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=how-i-met-your-mother" target="_blank" rel="external">How I met your Mother</a></p>
</li>
<li><p><a href="http://www.imsdb.com/scripts/Titanic.html" target="_blank" rel="external">Titanic</a></p>
</li>
<li><p><a href="https://www.springfieldspringfield.co.uk/movie_script.php?movie=the-hobbit-an-unexpected-journey" target="_blank" rel="external">Hobbit</a></p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.youtube.com/watch?v=D8gdg1zdM7U" target="_blank" rel="external">多益 英文聽力加強 和Cindy學英文</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python copy & deepcopy]]></title>
      <url>http://sggo.me/2018/01/24/python/py-language-13-copy/</url>
      <content type="html"><![CDATA[<p>对象的赋值，拷贝（深/浅拷贝）之间是有差异的，如果使用不当，可能产生意外的结果.</p>
<a id="more"></a>
<h2 id="id"><a href="#id" class="headerlink" title="id"></a>id</h2><p>什么是<code>id</code>？一个对象的<code>id</code>值在<code>CPython</code>解释器里就代表它在内存中的`地址</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=a</span><br><span class="line"></span><br><span class="line">print(id(a))</span><br><span class="line"></span><br><span class="line">print(id(b))</span><br><span class="line"></span><br><span class="line">print(id(a)==id(b))    <span class="comment">#附值后，两者的id相同，为true。</span></span><br><span class="line"></span><br><span class="line">b[<span class="number">0</span>]=<span class="number">222222</span>  <span class="comment"># 此时，改变b的第一个值，也会导致a值改变。</span></span><br><span class="line">print(a,b)</span><br></pre></td></tr></table></figure>
<pre><code>4449594888
4449594888
True
[222222, 2, 3] [222222, 2, 3]
</code></pre><h2 id="浅拷贝"><a href="#浅拷贝" class="headerlink" title="浅拷贝"></a>浅拷贝</h2><p>当使用浅拷贝时，python 只是拷贝了最外围的对象本身，内部的元素都只是拷贝了一个引用而已</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">c=copy.copy(a)  <span class="comment">#拷贝了a的外围对象本身,</span></span><br><span class="line">print(id(c))</span><br><span class="line"></span><br><span class="line">print(id(a)==id(c))  <span class="comment">#id 改变 为false</span></span><br><span class="line"></span><br><span class="line">c[<span class="number">1</span>]=<span class="number">22222</span>   <span class="comment">#此时，我去改变c的第二个值时，a不会被改变。</span></span><br><span class="line">print(a,c)</span><br><span class="line"><span class="comment"># [1, 2, 3] [1, 22222, 3] #a值不变,c的第二个值变了，这就是copy和‘==’的不同</span></span><br></pre></td></tr></table></figure>
<pre><code>4449594440
False
[1, 2, 3] [1, 22222, 3]
</code></pre><h2 id="深拷贝"><a href="#深拷贝" class="headerlink" title="深拷贝"></a>深拷贝</h2><p><code>deepcopy</code> 对外围和内部元素都进行了拷贝对象本身，而不是对象的引用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#copy.copy()</span></span><br><span class="line"></span><br><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,[<span class="number">3</span>,<span class="number">4</span>]]  <span class="comment">#第三个值为列表[3,4],即内部元素</span></span><br><span class="line">d=copy.copy(a) <span class="comment">#浅拷贝a中的[3，4]内部元素的引用，非内部元素对象的本身</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>id(a)==id(d)</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>id(a[<span class="number">2</span>])==id(d[<span class="number">2</span>])</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">2</span>][<span class="number">0</span>]=<span class="number">3333</span>  <span class="comment">#改变a中内部原属列表中的第一个值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d             <span class="comment">#这时d中的列表元素也会被改变</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3333</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#copy.deepcopy()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e=copy.deepcopy(a) <span class="comment">#e为深拷贝了a</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">2</span>][<span class="number">0</span>]=<span class="number">333</span> <span class="comment">#改变a中内部元素列表第一个的值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3333</span>, <span class="number">4</span>]] <span class="comment">#因为时深拷贝，这时e中内部元素[]列表的值不会因为a中的值改变而改变</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.python.org/" target="_blank" rel="external">docs.python.org</a></li>
<li><a href="https://morvanzhou.github.io/" target="_blank" rel="external">python morvanzhou</a></li>
<li><a href="https://www.liaoxuefeng.com/" target="_blank" rel="external">python liaoxuefeng</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python try … except … as …]]></title>
      <url>http://sggo.me/2018/01/24/python/py-language-12-try-exception/</url>
      <content type="html"><![CDATA[<p>try:, except … as …:</p>
<a id="more"></a>
<h2 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h2><p>输出错误：<code>try:, except ... as ...</code>: 看如下代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file=open(<span class="string">'eeee.txt'</span>,<span class="string">'r'</span>)  <span class="comment">#会报错的代码</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:  <span class="comment"># 将报错存储在 e 中</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<pre><code>[Errno 2] No such file or directory: &apos;eeee.txt&apos;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    file=open(<span class="string">'eeee.txt'</span>,<span class="string">'r+'</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line">    response = input(<span class="string">'do you want to create a new file:'</span>)</span><br><span class="line">    <span class="keyword">if</span> response==<span class="string">'y'</span>:</span><br><span class="line">        file=open(<span class="string">'eeee.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    file.write(<span class="string">'ssss'</span>)</span><br><span class="line">    file.close()</span><br></pre></td></tr></table></figure>
<pre><code>[Errno 2] No such file or directory: &apos;eeee.txt&apos;
do you want to create a new file:y
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.python.org/" target="_blank" rel="external">docs.python.org</a></li>
<li><a href="https://morvanzhou.github.io/" target="_blank" rel="external">python morvanzhou</a></li>
<li><a href="https://www.liaoxuefeng.com/" target="_blank" rel="external">python liaoxuefeng</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python 函数式编程 zip、lambda、map...]]></title>
      <url>http://sggo.me/2018/01/24/python/py-language-11-zip-lambda-map/</url>
      <content type="html"><![CDATA[<p>zip、lambda、map…</p>
<a id="more"></a>
<h2 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h2><p><code>zip</code> 函数接受任意多个（包括0个和1个）序列作为参数，合并后返回一个tuple列表</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">ab=zip(a,b)</span><br><span class="line">print(list(ab))  <span class="comment">#需要加list来可视化这个功能</span></span><br></pre></td></tr></table></figure>
<pre><code>[(1, 4), (2, 5), (3, 6)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">ab=zip(a,b)</span><br><span class="line"></span><br><span class="line">print(list(ab))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,j <span class="keyword">in</span> zip(a,b):</span><br><span class="line">     print(i/<span class="number">2</span>,j*<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(1, 4), (2, 5), (3, 6)]
0.5 8
1.0 10
1.5 12
</code></pre><h2 id="lambda"><a href="#lambda" class="headerlink" title="lambda"></a>lambda</h2><p><code>lambda</code> 定义一个简单的函数，实现简化代码的功能，看代码会更好理解。</p>
<p><code>fun = lambda x,y : x+y</code>, 冒号前的<code>x</code>,<code>y</code>为自变量，冒号后<code>x+y</code>为具体运算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fun= <span class="keyword">lambda</span> x,y:x+y</span><br><span class="line">x=int(input(<span class="string">'x='</span>))    <span class="comment">#这里要定义int整数，否则会默认为字符串</span></span><br><span class="line">y=int(input(<span class="string">'y='</span>))</span><br><span class="line">print(fun(x,y))</span><br></pre></td></tr></table></figure>
<pre><code>x=4
y=6
10
</code></pre><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p><code>map</code> 是把 <code>函数</code> 和 <code>参数</code> 绑定在一起.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(x,y)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> (x+y)</span><br><span class="line"></span><br><span class="line">print(list(map(fun,[<span class="number">1</span>],[<span class="number">2</span>])))</span><br><span class="line"></span><br><span class="line">list(map(fun,[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>[3]
[4, 6]
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.python.org/" target="_blank" rel="external">docs.python.org</a></li>
<li><a href="https://morvanzhou.github.io/" target="_blank" rel="external">python morvanzhou</a></li>
<li><a href="https://www.liaoxuefeng.com/" target="_blank" rel="external">python liaoxuefeng</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Bar]]></title>
      <url>http://sggo.me/2018/01/24/python/py-matplotlib-9-bar/</url>
      <content type="html"><![CDATA[<p>上篇学习了如何 plot Scatter，今天我们讲述如何 plot <code>Bar</code></p>
<a id="more"></a>
<p>今日目标 : 柱状图分成上下两部分，每一个柱体上都有相应的数值标注，并且取消坐标轴的显示.</p>
<h2 id="生成基本图形"><a href="#生成基本图形" class="headerlink" title="生成基本图形"></a>生成基本图形</h2><p>向上向下生成<code>12个数据</code>，<code>X</code> 为 [0,11] 的整数 ，<code>Y</code>是均匀分布的随机数据。 使用的函数是<code>plt.bar</code>，参数为<code>X</code>和<code>Y</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">n = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">X = np.arange(n)</span><br><span class="line"></span><br><span class="line">Y1 = (<span class="number">1</span> - X / float(n)) * np.random.uniform(<span class="number">0.5</span>, <span class="number">1.0</span>, n)</span><br><span class="line">Y2 = (<span class="number">1</span> - X / float(n)) * np.random.uniform(<span class="number">0.5</span>, <span class="number">1.0</span>, n)</span><br><span class="line"></span><br><span class="line">plt.bar(X, +Y1)</span><br><span class="line">plt.bar(X, -Y2)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">-.5</span>, n)</span><br><span class="line">plt.xticks(())</span><br><span class="line"></span><br><span class="line">plt.ylim(<span class="number">-1.25</span>, <span class="number">1.25</span>)</span><br><span class="line">plt.yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-9-bar-1-output_1_0.png" height="100" width="550"><br></div>

<h2 id="加颜色和数据"><a href="#加颜色和数据" class="headerlink" title="加颜色和数据"></a>加颜色和数据</h2><p>用<code>facecolor</code>设置主体颜色，<code>edgecolor</code>设置边框颜色为白色，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.bar(X, +Y1, facecolor=<span class="string">'#9999ff'</span>, edgecolor=<span class="string">'white'</span>)</span><br><span class="line">plt.bar(X, -Y2, facecolor=<span class="string">'#ff9999'</span>, edgecolor=<span class="string">'white'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-9-bar-2-output_3_0.png" height="100" width="550"><br></div>

<p>接下来我们用函数<code>plt.text</code>分别在柱体上方（下方）加上数值，用<code>%.2f</code>保留两位小数，横向居中对齐<code>ha=&#39;center&#39;</code>，纵向底部（顶部）对齐<code>va=&#39;bottom&#39;</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.bar(X, +Y1, facecolor=<span class="string">'#9999ff'</span>, edgecolor=<span class="string">'white'</span>)</span><br><span class="line">plt.bar(X, -Y2, facecolor=<span class="string">'#ff9999'</span>, edgecolor=<span class="string">'white'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X, Y1):</span><br><span class="line">    <span class="comment"># ha: horizontal alignment</span></span><br><span class="line">    <span class="comment"># va: vertical alignment</span></span><br><span class="line">    plt.text(x + <span class="number">0.4</span>, y + <span class="number">0.05</span>, <span class="string">'%.2f'</span> % y, ha=<span class="string">'center'</span>, va=<span class="string">'bottom'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X, Y2):</span><br><span class="line">    <span class="comment"># ha: horizontal alignment</span></span><br><span class="line">    <span class="comment"># va: vertical alignment</span></span><br><span class="line">    plt.text(x + <span class="number">0.4</span>, -y - <span class="number">0.05</span>, <span class="string">'%.2f'</span> % y, ha=<span class="string">'center'</span>, va=<span class="string">'top'</span>)</span><br><span class="line">    </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-9-bar-3-output_5_0.png" height="100" width="550"><br></div>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Scatter]]></title>
      <url>http://sggo.me/2018/01/24/python/py-matplotlib-8-scatter/</url>
      <content type="html"><![CDATA[<p>上篇学习了如何 plot 线，今天学习如何 plot <code>Scatter</code> 散点图</p>
<a id="more"></a>
<p>引入模块<code>numpy</code>用来产生一些随机数据。生成<code>1024</code>个呈标准正态分布的二维数据组 (平均数是<code>0</code>，方差为<code>1</code>) 作为一个数据集，并图像化这个数据集。每一个点的颜色值用<code>T</code>来表示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">n = <span class="number">1024</span>    <span class="comment"># data size</span></span><br><span class="line"></span><br><span class="line">X = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, n) <span class="comment"># 每一个点的X值</span></span><br><span class="line">Y = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, n) <span class="comment"># 每一个点的Y值</span></span><br><span class="line"></span><br><span class="line">T = np.arctan2(Y,X) <span class="comment"># for color value</span></span><br></pre></td></tr></table></figure>
<p>数据集生成完毕，现在来用 <code>scatter</code> <code>plot</code> 这个点集</p>
<p>输入<code>X</code>和<code>Y</code>作为location，<code>size=75</code>，颜色为<code>T</code>，<code>color map</code> 用默认值，透明度<code>alpha</code> 为 50%。 x轴显示范围定位(-1.5，1.5)，并用<code>xtick()</code> 函数来隐藏<code>x</code>坐标轴，<code>y</code>轴同理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X, Y, s=<span class="number">75</span>, c=T, alpha=<span class="number">.5</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">-1.5</span>, <span class="number">1.5</span>)</span><br><span class="line">plt.xticks(())  <span class="comment"># ignore xticks</span></span><br><span class="line">plt.ylim(<span class="number">-1.5</span>, <span class="number">1.5</span>)</span><br><span class="line">plt.yticks(())  <span class="comment"># ignore yticks</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-8-scatter-1.png" height="100" width="700"><br></div>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Tick bbox]]></title>
      <url>http://sggo.me/2018/01/23/python/py-matplotlib-7-tick/</url>
      <content type="html"><![CDATA[<p>图中的内容较多，可通过设置相关内容的<code>透明度</code>来使图片更易于观察，也即是本节中的<code>bbox</code>参数设置来调节图像信息.</p>
<a id="more"></a>
<h2 id="生成图形"><a href="#生成图形" class="headerlink" title="生成图形"></a>生成图形</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">0.1</span>*x</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"><span class="comment"># 在 plt 2.0.2 或更高的版本中, 设置 zorder 给 plot 在 z 轴方向排序</span></span><br><span class="line">plt.plot(x, y, linewidth=<span class="number">10</span>, zorder=<span class="number">1</span>)</span><br><span class="line">plt.ylim(<span class="number">-2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">ax = plt.gca()</span><br><span class="line"></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-7-tick-output_1_0.png" height="100" width="700"><br></div>

<h2 id="调整坐标-bbox"><a href="#调整坐标-bbox" class="headerlink" title="调整坐标 bbox"></a>调整坐标 bbox</h2><p>然后对被遮挡的图像调节相关透明度，本例中设置 x轴 和 y轴 的刻度数字进行透明度设置</p>
<p>其中<code>label.set_fontsize(12)</code>重新调节字体大小，<code>bbox</code>设置目的内容的透明度相关参数，<code>facecolor</code>调节 <code>box</code> 前景色，<code>edgecolor</code> 设置边框， 本处设置边框为无，<code>alpha</code>设置透明度. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> ax.get_xticklabels() + ax.get_yticklabels():</span><br><span class="line">    label.set_fontsize(<span class="number">12</span>)</span><br><span class="line">    <span class="comment"># 在 plt 2.0.2 或更高的版本中, 设置 zorder 给 plot 在 z 轴方向排序</span></span><br><span class="line">    label.set_bbox(dict(facecolor=<span class="string">'white'</span>, edgecolor=<span class="string">'None'</span>, alpha=<span class="number">0.7</span>, zorder=<span class="number">2</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-7-tick-output_3_0.png" height="100" width="700"><br></div>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Annotation]]></title>
      <url>http://sggo.me/2018/01/23/python/py-matplotlib-6-Annotation/</url>
      <content type="html"><![CDATA[<p>当图线中某些特殊地方需要标注时，我们可以使用 <code>annotation</code>.  </p>
<p>matplotlib 中的 <code>annotation</code> 有两种方法， 一种是用 plt 里面的 <code>annotate</code>，一种是直接用 plt 里面的 <code>text</code> 来写标注.</p>
<a id="more"></a>
<h2 id="画出基本图"><a href="#画出基本图" class="headerlink" title="画出基本图"></a>画出基本图</h2><p>首先，我们在坐标轴中绘制一条直线.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.figure(num=<span class="number">1</span>, figsize=(<span class="number">8</span>, <span class="number">5</span>),)</span><br><span class="line">plt.plot(x, y,)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-6-annotation-output_1_0.png" height="100" width="500"><br></div>

<h2 id="移动坐标"><a href="#移动坐标" class="headerlink" title="移动坐标"></a>移动坐标</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.figure(num=<span class="number">1</span>, figsize=(<span class="number">8</span>, <span class="number">5</span>),)</span><br><span class="line">plt.plot(x, y,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动坐标</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-6-annotation-output_3_0.png" height="100" width="550"><br></div>

<p>然后标注出点<code>(x0, y0)</code>的位置信息. 用 <code>plt.plot([x0, x0,], [0, y0,], &#39;k--&#39;, linewidth=2.5)</code> 画出一条垂直于x轴的虚线.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.figure(num=<span class="number">1</span>, figsize=(<span class="number">8</span>, <span class="number">5</span>),)</span><br><span class="line">plt.plot(x, y,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动坐标</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">x0 = <span class="number">1</span></span><br><span class="line">y0 = <span class="number">2</span>*x0 + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###   plt.plot([x0, x0,], [0, y0,], 'k--', linewidth=2.5) 画出一条垂直于x轴的虚线.  ###</span></span><br><span class="line"></span><br><span class="line">plt.plot([x0, x0,], [<span class="number">0</span>, y0,], <span class="string">'k--'</span>, linewidth=<span class="number">2.5</span>)</span><br><span class="line"><span class="comment"># set dot styles</span></span><br><span class="line">plt.scatter([x0, ], [y0, ], s=<span class="number">50</span>, color=<span class="string">'b'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-6-annotation-output_5_0.png" height="100" width="550"><br></div>

<h2 id="加注释-annotate"><a href="#加注释-annotate" class="headerlink" title="加注释 annotate"></a>加注释 annotate</h2><p>接下来我们就对<code>(x0, y0)</code>这个点进行标注.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-4</span>, <span class="number">4</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.figure(num=<span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">6</span>),)</span><br><span class="line">plt.plot(x, y,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动坐标</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">x0 = <span class="number">1</span></span><br><span class="line">y0 = <span class="number">2</span>*x0 + <span class="number">1</span></span><br><span class="line">plt.plot([x0, x0,], [<span class="number">0</span>, y0,], <span class="string">'k--'</span>, linewidth=<span class="number">2.5</span>)</span><br><span class="line"><span class="comment"># set dot styles</span></span><br><span class="line">plt.scatter([x0, ], [y0, ], s=<span class="number">50</span>, color=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">############## 添加注释 annotate ###############</span></span><br><span class="line"></span><br><span class="line">plt.annotate(<span class="string">r'$2x+1=%s$'</span> % y0, xy=(x0, y0), xycoords=<span class="string">'data'</span>, xytext=(+<span class="number">30</span>, <span class="number">-30</span>),</span><br><span class="line">             textcoords=<span class="string">'offset points'</span>, fontsize=<span class="number">16</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-6-annotation-output_7_0.png" height="100" width="550"><br></div>

<p>其中参数 <code>xycoords=&#39;data&#39;</code> 是说基于数据的值来选位置, <code>xytext=(+30, -30)</code> 和 <code>textcoords=&#39;offset points&#39;</code> 对于标注位置的描述 和 <code>xy</code> 偏差值, <code>arrowprops</code>是对图中箭头类型的一些设置.</p>
<h2 id="加注释-text"><a href="#加注释-text" class="headerlink" title="加注释 text"></a>加注释 text</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.text(<span class="number">-3.7</span>, <span class="number">3</span>, <span class="string">r'$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$'</span>,</span><br><span class="line">         fontdict=&#123;<span class="string">'size'</span>: <span class="number">16</span>, <span class="string">'color'</span>: <span class="string">'r'</span>&#125;)</span><br></pre></td></tr></table></figure>
<pre><code>Text(-3.7,3,&apos;$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$&apos;)
</code></pre><p>其中<code>-3.7, 3,</code>是选取text的位置, 空格需要用到转字符<code>\</code> ,<code>fontdict</code>设置文本字体.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-4</span>, <span class="number">4</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.figure(num=<span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">6</span>),)</span><br><span class="line">plt.plot(x, y,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动坐标</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">x0 = <span class="number">1</span></span><br><span class="line">y0 = <span class="number">2</span>*x0 + <span class="number">1</span></span><br><span class="line">plt.plot([x0, x0,], [<span class="number">0</span>, y0,], <span class="string">'k--'</span>, linewidth=<span class="number">2.5</span>)</span><br><span class="line"><span class="comment"># set dot styles</span></span><br><span class="line">plt.scatter([x0, ], [y0, ], s=<span class="number">50</span>, color=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">############## 添加注释 annotate ###############</span></span><br><span class="line"></span><br><span class="line">plt.annotate(<span class="string">r'$2x+1=%s$'</span> % y0, xy=(x0, y0), xycoords=<span class="string">'data'</span>, xytext=(+<span class="number">30</span>, <span class="number">-30</span>),</span><br><span class="line">             textcoords=<span class="string">'offset points'</span>, fontsize=<span class="number">16</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############# 添加注释 text #################</span></span><br><span class="line"></span><br><span class="line">plt.text(<span class="number">-3.7</span>, <span class="number">3</span>, <span class="string">r'$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$'</span>,</span><br><span class="line">         fontdict=&#123;<span class="string">'size'</span>: <span class="number">16</span>, <span class="string">'color'</span>: <span class="string">'r'</span>&#125;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-6-annotation-output_11_0.png" height="100" width="500"><br></div>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Legend]]></title>
      <url>http://sggo.me/2018/01/23/python/py-matplotlib-5-legend/</url>
      <content type="html"><![CDATA[<p>matplotlib 中的 <code>legend</code> 图例就是为了展示出每个数据对应的<code>图像名称</code>,可读性更好.</p>
<a id="more"></a>
<h2 id="添加图例-legend"><a href="#添加图例-legend" class="headerlink" title="添加图例 legend"></a>添加图例 legend</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">y1 = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line">y2 = x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"><span class="comment">#set x limits</span></span><br><span class="line">plt.xlim((<span class="number">-1</span>, <span class="number">2</span>))</span><br><span class="line">plt.ylim((<span class="number">-2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># set new sticks</span></span><br><span class="line">new_sticks = np.linspace(<span class="number">-1</span>, <span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">plt.xticks(new_sticks)</span><br><span class="line"><span class="comment"># set tick labels</span></span><br><span class="line">plt.yticks([<span class="number">-2</span>, <span class="number">-1.8</span>, <span class="number">-1</span>, <span class="number">1.22</span>, <span class="number">3</span>],</span><br><span class="line">           [<span class="string">r'$really\ bad$'</span>, <span class="string">r'$bad$'</span>, <span class="string">r'$normal$'</span>, <span class="string">r'$good$'</span>, <span class="string">r'$really\ good$'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>([&lt;matplotlib.axis.YTick at 0x1195c3358&gt;,
  &lt;matplotlib.axis.YTick at 0x112681080&gt;,
  &lt;matplotlib.axis.YTick at 0x1195ce710&gt;,
  &lt;matplotlib.axis.YTick at 0x1195f5240&gt;,
  &lt;matplotlib.axis.YTick at 0x1195fc550&gt;],
 &lt;a list of 5 Text yticklabel objects&gt;)
</code></pre><p>对图中的两条线绘制图例，首先我们设置两条线的类型等信息（蓝色实线与红色虚线).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set line syles</span></span><br><span class="line">l1, = plt.plot(x, y1, label=<span class="string">'linear line'</span>)</span><br><span class="line">l2, = plt.plot(x, y2, color=<span class="string">'red'</span>, linewidth=<span class="number">1.0</span>, linestyle=<span class="string">'--'</span>, label=<span class="string">'square line'</span>)</span><br></pre></td></tr></table></figure>
<p>需要注意的是 <code>l1,</code> <code>l2,</code> 要以<code>逗号</code>结尾, 因为 <code>plt.plot()</code> 返回的是一个list.</p>
<p><code>legend</code> 将要显示的信息来自于上面代码中的 <code>label</code>. 所以我们只需要简单写一下代码, <code>plt</code> 就能自动的为我们添加图例.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-5-legend-1.png" height="100" width="650"><br></div>

<p>参数 <code>loc=&#39;upper right&#39;</code> 表示图例将添加在图中的右上角.</p>
<h2 id="调整位置和名称"><a href="#调整位置和名称" class="headerlink" title="调整位置和名称"></a>调整位置和名称</h2><p>如果我们想单独修改之前的 <code>label</code> 信息, 给不同类型的线条设置图例信息. 我们可以在 <code>plt.legend</code> 输入更多参数. 如果以下面这种形式添加 <code>legend</code>, 我们需要确保, 在上面的代码 <code>plt.plot(x, y2, label=&#39;linear line&#39;)</code> 和 <code>plt.plot(x, y1, label=&#39;square line&#39;)</code> 中有用变量 l1 和 l2 分别存储起来. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.legend(handles=[l1, l2], labels=[<span class="string">'up'</span>, <span class="string">'down'</span>],  loc=<span class="string">'best'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-5-legend-2.png" height="100" width="650"><br></div>

<p>这样我们就能分别重新设置线条对应的 <code>label</code> 了.</p>
<p>其中’loc’参数有多种，’best’表示自动分配最佳位置，其余的如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'best'</span> : <span class="number">0</span>,          </span><br><span class="line"><span class="string">'upper right'</span>  : <span class="number">1</span>,</span><br><span class="line"><span class="string">'upper left'</span>   : <span class="number">2</span>,</span><br><span class="line"><span class="string">'lower left'</span>   : <span class="number">3</span>,</span><br><span class="line"><span class="string">'lower right'</span>  : <span class="number">4</span>,</span><br><span class="line"><span class="string">'right'</span>        : <span class="number">5</span>,</span><br><span class="line"><span class="string">'center left'</span>  : <span class="number">6</span>,</span><br><span class="line"><span class="string">'center right'</span> : <span class="number">7</span>,</span><br><span class="line"><span class="string">'lower center'</span> : <span class="number">8</span>,</span><br><span class="line"><span class="string">'upper center'</span> : <span class="number">9</span>,</span><br><span class="line"><span class="string">'center'</span>       : <span class="number">10</span>,</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Coordinate axis]]></title>
      <url>http://sggo.me/2018/01/23/python/py-matplotlib-4-coordinate_axis/</url>
      <content type="html"><![CDATA[<p>如何移动 matplotlib 中 axis 坐标轴的位置.</p>
<a id="more"></a>
<h2 id="设置名字和位置"><a href="#设置名字和位置" class="headerlink" title="设置名字和位置"></a>设置名字和位置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p>使用 <code>np.linspace</code> 定义 <code>x</code> ：范围是(-3,3);个数是50.<br>仿真一维数据组(<code>x</code> ,<code>y1</code>)表示曲线1.  仿真一维数据组(<code>x</code> ,<code>y2</code>)表示曲线2.  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">y1 = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line">y2 = x**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>使用<code>plt.figure</code>定义一个图像窗口. </p>
<p>使用<code>plt.plot</code>画(<code>x</code> ,<code>y2</code>)曲线. 使用<code>plt.plot</code>画(<code>x</code> ,<code>y1</code>)曲线，曲线的颜色属性(<code>color</code>)为红色; 曲线的宽度(<code>linewidth</code>) 为 1.0; 曲线的类型(<code>linestyle</code>)为虚线.   </p>
<p>使用<code>plt.xlim</code>设置<code>x</code>坐标轴范围: (-1, 2); 使用<code>plt.ylim</code>设置<code>y</code>坐标轴范围: (-2, 3);   </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=<span class="string">'red'</span>, linewidth=<span class="number">1.0</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.xlim((<span class="number">-1</span>, <span class="number">2</span>))</span><br><span class="line">plt.ylim((<span class="number">-2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<pre><code>(-2, 3)
</code></pre><p>使用<code>np.linspace</code>定义范围以及个数：范围是(-1,2);个数是5. </p>
<p>使用<code>plt.xticks</code>设置<code>x</code>轴刻度：范围是(-1,2);个数是5.<br>使用<code>plt.yticks</code>设置<code>y</code>轴刻度以及名称: 刻度为[-2, -1.8, -1, 1.22, 3]; 对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’].</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_ticks = np.linspace(<span class="number">-1</span>, <span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">plt.xticks(new_ticks)</span><br><span class="line">plt.yticks([<span class="number">-2</span>, <span class="number">-1.8</span>, <span class="number">-1</span>, <span class="number">1.22</span>, <span class="number">3</span>],[<span class="string">'$really\ bad$'</span>, <span class="string">'$bad$'</span>, <span class="string">'$normal$'</span>, <span class="string">'$good$'</span>, <span class="string">'$really\ good$'</span>])</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>function</th>
<th>desc</th>
<th>设置效果</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>plt.gca</code></td>
<td>获取当前坐标轴信息</td>
<td>-</td>
</tr>
<tr>
<td><code>.spines</code></td>
<td>设置边框</td>
<td>右侧边框 &amp; 上边框</td>
</tr>
<tr>
<td><code>.set_color</code></td>
<td>设置边框颜色</td>
<td>默认白色</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-4-ax-4_1.png" height="100" width="700"><br></div>

<h2 id="调整坐标轴"><a href="#调整坐标轴" class="headerlink" title="调整坐标轴"></a>调整坐标轴</h2><p>使用 <code>.xaxis.set_ticks_position</code>设置<code>x</code>坐标刻度数字或名称的位置：<code>bottom</code>.（所有位置：<code>top</code>，<code>bottom</code>，<code>both</code>，<code>default</code>，<code>none</code>）<br>使用 <code>.spines</code> 设置边框：<code>x</code>轴；<br>使用 <code>.set_position</code> 设置边框位置：<code>y=0</code> 的位置；（位置所有属性：<code>outward</code>，<code>axes</code>，<code>data</code>）  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line"></span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-4-ax-4_2.png" height="100" width="700"><br></div>

<p>使用<code>.yaxis.set_ticks_position</code>设置<code>y</code>坐标刻度数字或名称的位置：<code>left</code>.（所有位置：<code>left</code>，<code>right</code>，<code>both</code>，<code>default</code>，<code>none</code>）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure>
<p>使用<code>.spines</code>设置边框：<code>y</code>轴;<br>使用<code>.set_position</code>设置边框位置：<code>x=0</code>的位置；（位置所有属性：<code>outward</code>，<code>axes</code>，<code>data</code>）<br>使用<code>plt.show</code>显示图像.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>,<span class="number">0</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-4-ax-4_3.png" height="100" width="700"><br></div>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[探索世界財富自由之路]]></title>
      <url>http://sggo.me/2018/01/22/tools/finance-blockchain-coin/</url>
      <content type="html"><![CDATA[<p>財富自由: 指的是某个人再也不用为了满足生活必需而出售自己的时间了。我们要的自由，最根本不是财富，财富只是工具. <a id="more"></a> 我们要的自由，本质是时间的自主权。在你获得财富自由之后，还是会不断做的事情，比如: 专注成长而不是专注成功，很多人没有意识到的事，不管你获得了怎样的成功，你依然需要成长，财富自由不是终点，那只是通往终点过程中的一个里程碑而已。</p>
<!-- more -->
<p>对于投机者，“炒币”需谨慎，应将风险控制在自己能够承受的范围内。PS：其实我真的不关心这个问题。</p>
<h2 id="Blockchain"><a href="#Blockchain" class="headerlink" title="Blockchain"></a>Blockchain</h2><p>&nbsp;&nbsp;区块链本质上是一个去中心化的分布式账本数据库.  </p>
<ul>
<li><p><a href="https://www.youtube.com/watch?v=frLy-3_p1qA" target="_blank" rel="external">1. 区块链介绍</a></p>
</li>
<li><p><a href="https://www.youtube.com/watch?v=uKr-rKEALiE" target="_blank" rel="external">2. 怪奇專欄-秒懂区块链</a></p>
</li>
</ul>
<h2 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a>Exchange</h2><ul>
<li><p><a href="https://www.huobi.pro" target="_blank" rel="external">1. 火币</a></p>
</li>
<li><p><a href="https://www.bitfinex.com/" target="_blank" rel="external">2. Bitfinex</a></p>
</li>
<li><p><a href="https://www.feixiaohao.com/exchange/" target="_blank" rel="external">…</a></p>
</li>
</ul>
<h2 id="Information"><a href="#Information" class="headerlink" title="Information"></a>Information</h2><ul>
<li><p><a href="https://www.feixiaohao.com/" target="_blank" rel="external">1. 非小号</a></p>
</li>
<li><p><a href="/finance">…</a></p>
</li>
</ul>
<h2 id="Commentator"><a href="#Commentator" class="headerlink" title="Commentator"></a>Commentator</h2><ul>
<li><p><a href="https://www.youtube.com/channel/UCWcgX3JEIAT0EElHM1xF2kA/featured" target="_blank" rel="external">1. 希多说币</a></p>
</li>
<li><p><a href="https://www.youtube.com/channel/UCU5_xFE6j0WxpnQFNXqN3LQ" target="_blank" rel="external">2. 比特大书</a></p>
</li>
<li><p><a href="/finance">…</a></p>
</li>
</ul>
<h2 id="other"><a href="#other" class="headerlink" title="other"></a>other</h2><ul>
<li><p><a href="https://www.zhihu.com/question/38138675" target="_blank" rel="external">1. 未来哪些领域可能用区块链技术 ?</a></p>
</li>
<li><p><a href="https://www.zhihu.com/question/38138675" target="_blank" rel="external">2. 数字货币与区块链原理 廖雪峰</a></p>
</li>
<li><p><a href="https://www.feixiaohao.com/currencies/tether/" target="_blank" rel="external">3. What USDT ?</a>, <a href="https://zh.wikipedia.org/wiki/%E4%BB%A5%E5%A4%AA%E5%9D%8A" target="_blank" rel="external">What ETH ?</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/iTgBLYUOj4t7Kk-gPpPe-w" target="_blank" rel="external">4. 我的区块链世界观</a></p>
</li>
</ul>
<h2 id="next-⋯⋯"><a href="#next-⋯⋯" class="headerlink" title="next ⋯⋯"></a>next ⋯⋯</h2><blockquote>
<p>notes：next …</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[pickle]]></title>
      <url>http://sggo.me/2018/01/19/python/py-language-14-pickle/</url>
      <content type="html"><![CDATA[<p>Python 语言特定的序列化模块是pickle，但如果要把序列化搞得更通用、更符合Web标准，可以使用json模块</p>
<a id="more"></a>
<p>pickle 是一个 python 中, 压缩/保存/提取 文件的模块. 最一般的使用方式非常简单. </p>
<h2 id="pickle-保存"><a href="#pickle-保存" class="headerlink" title="pickle 保存"></a>pickle 保存</h2><p>字典和列表都是能被保存的.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">a_dict = &#123;<span class="string">'da'</span>: <span class="number">111</span>, <span class="number">2</span>: [<span class="number">23</span>,<span class="number">1</span>,<span class="number">4</span>], <span class="string">'23'</span>: &#123;<span class="number">1</span>:<span class="number">2</span>,<span class="string">'d'</span>:<span class="string">'sad'</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># pickle a variable to a file</span></span><br><span class="line">file = open(<span class="string">'pickle_example.pickle'</span>, <span class="string">'wb'</span>)</span><br><span class="line">pickle.dump(a_dict, file)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure>
<p><code>pickle.dump</code> 你要保存的东西去这个打开的 <code>file</code>. </p>
<p>最后关闭 <code>file</code> 你就会发现你的文件目录里多了一个 <code>pickle_example.pickle</code> 文件, 这就是那个字典了.</p>
<h2 id="pickle-提取"><a href="#pickle-提取" class="headerlink" title="pickle 提取"></a>pickle 提取</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reload a file to a variable</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'pickle_example.pickle'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    a_dict1 =pickle.load(file)</span><br><span class="line"></span><br><span class="line">print(a_dict1)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;da&apos;: 111, 2: [23, 1, 4], &apos;23&apos;: {1: 2, &apos;d&apos;: &apos;sad&apos;}}
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.python.org/" target="_blank" rel="external">docs.python.org</a></li>
<li><a href="https://morvanzhou.github.io/" target="_blank" rel="external">python morvanzhou</a></li>
<li><a href="https://www.liaoxuefeng.com/" target="_blank" rel="external">python liaoxuefeng</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn Save Model]]></title>
      <url>http://sggo.me/2018/01/10/python/py-sklearn-7-save-model/</url>
      <content type="html"><![CDATA[<p>我们训练好了一个 <code>Model</code> 以后总需要保存和再次预测, 所以保存和读取我们的sklearn model也是同样重要的一步。<br>这次主要介绍两种保存Model的模块 <code>pickle</code> 与 <code>joblib</code></p>
<a id="more"></a>
<h2 id="pickle-保存"><a href="#pickle-保存" class="headerlink" title="pickle 保存"></a>pickle 保存</h2><p>首先简单建立与训练一个 <code>SVC</code> Model</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">clf = svm.SVC()</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">clf.fit(X,y)</span><br></pre></td></tr></table></figure>
<pre><code>SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&apos;ovr&apos;, degree=3, gamma=&apos;auto&apos;, kernel=&apos;rbf&apos;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre><p>使用 <code>pickle</code> 来保存与读取训练好的 <code>Model</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle <span class="comment">#pickle模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存Model(注:save文件夹要预先建立，否则会报错)</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'save/clf.pickle'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(clf, f)</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取Model</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'save/clf.pickle'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    clf2 = pickle.load(f)</span><br><span class="line">    <span class="comment">#测试读取后的Model</span></span><br><span class="line">    print(clf2.predict(X[<span class="number">0</span>:<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [0]</span></span><br></pre></td></tr></table></figure>
<pre><code>[0]
</code></pre><h2 id="joblib-保存"><a href="#joblib-保存" class="headerlink" title="joblib 保存"></a>joblib 保存</h2><p><code>joblib</code> 是 sklearn的外部模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib <span class="comment">#jbolib模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存Model(注:save文件夹要预先建立，否则会报错)</span></span><br><span class="line">joblib.dump(clf, <span class="string">'save/clf.pkl'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取Model</span></span><br><span class="line">clf3 = joblib.load(<span class="string">'save/clf.pkl'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试读取后的Model</span></span><br><span class="line">print(clf3.predict(X[<span class="number">0</span>:<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [0]</span></span><br></pre></td></tr></table></figure>
<pre><code>[0]
</code></pre><p><code>joblib</code> 在使用上比较容易，读取速度也相对<code>pickle</code>快</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">scikit-learn morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn Cross-validation 3]]></title>
      <url>http://sggo.me/2018/01/09/python/py-sklearn-6-cross-validation-3/</url>
      <content type="html"><![CDATA[<p>交叉验证(cross validation)让我们知道在机器学习中验证是有多么的重要, 这一次的 sklearn 中我们用到了<code>sklearn.learning_curve</code>当中的另外一种, 叫做<code>validation_curve</code>,用这一种曲线我们就能更加直观看出改变模型中的参数的时候有没有过拟合(overfitting)的问题了. 这也是可以让我们更好的选择参数的方法.</p>
<a id="more"></a>
<h2 id="validation-curve-检视过拟合"><a href="#validation-curve-检视过拟合" class="headerlink" title="validation_curve 检视过拟合"></a>validation_curve 检视过拟合</h2><p>验证<code>SVC</code>中的一个参数 <code>gamma</code> 在什么范围内能使 <code>model</code> 产生好的结果. 以及过拟合和 <code>gamma</code> 取值的关系.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.learning_curve <span class="keyword">import</span> validation_curve <span class="comment">#validation_curve模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits </span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#digits数据集</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment">#建立参数测试集</span></span><br><span class="line">param_range = np.logspace(<span class="number">-6</span>, <span class="number">-2.3</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用validation_curve快速找出参数对模型的影响</span></span><br><span class="line">train_loss, test_loss = validation_curve(</span><br><span class="line">    SVC(), X, y, param_name=<span class="string">'gamma'</span>, param_range=param_range, cv=<span class="number">10</span>, scoring=<span class="string">'mean_squared_error'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#平均每一轮的平均方差</span></span><br><span class="line">train_loss_mean = -np.mean(train_loss, axis=<span class="number">1</span>)</span><br><span class="line">test_loss_mean = -np.mean(test_loss, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化图形</span></span><br><span class="line">plt.plot(param_range, train_loss_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>,</span><br><span class="line">         label=<span class="string">"Training"</span>)</span><br><span class="line">plt.plot(param_range, test_loss_mean, <span class="string">'o-'</span>, color=<span class="string">"g"</span>,</span><br><span class="line">        label=<span class="string">"Cross-validation"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">"gamma"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/sklearn-6-cross-validation-3-output_1_1.png" width="400"><br></div>

<p>由图中可以明显看到 <code>gamma</code> 值大于 <code>0.001</code>，模型就会有过拟合(<code>Overfitting</code>)的问题。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">scikit-learn morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn Cross-validation 2]]></title>
      <url>http://sggo.me/2018/01/09/python/py-sklearn-6-cross-validation-2/</url>
      <content type="html"><![CDATA[<p>Sklearn 中的 <code>learning curve</code> 可以很直观的看出我们的 <code>model</code> 学习的进度, 对比发现有没有 <code>overfitting</code> 的问题. 然后我们可以对我们的 <code>model</code> 进行调整, 克服 <code>overfitting</code> 的问题.</p>
<a id="more"></a>
<h2 id="Learning-curve-检视过拟合"><a href="#Learning-curve-检视过拟合" class="headerlink" title="Learning curve 检视过拟合"></a>Learning curve 检视过拟合</h2><p>加载对应模块:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.learning_curve <span class="keyword">import</span> learning_curve <span class="comment">#学习曲线模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits <span class="comment">#digits数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC <span class="comment">#Support Vector Classifier</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#可视化模块</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p>加载digits数据集，其包含的是手写体的数字，从0到9。<br>数据集总共有1797个样本，每个样本由64个特征组成， 分别为其手写体对应的8×8像素表示，每个特征取值0~16。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment">#print(len(X[0]))</span></span><br></pre></td></tr></table></figure>
<p>观察样本由小到大的学习曲线变化, 采用K折交叉验证 <code>cv=10</code>, 选择平均方差检视模型效能 <code>scoring=&#39;mean_squared_error&#39;</code>, 样本由小到大分成5轮检视学习曲线(<code>10%</code>, <code>25%</code>, <code>50%</code>, <code>75%</code>, <code>100%</code>):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_sizes, train_loss, test_loss = learning_curve(</span><br><span class="line">    SVC(gamma=<span class="number">0.001</span>), X, y, cv=<span class="number">10</span>, scoring=<span class="string">'mean_squared_error'</span>,</span><br><span class="line">    train_sizes=[<span class="number">0.1</span>, <span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">0.75</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%)</span></span><br><span class="line">train_loss_mean = -np.mean(train_loss, axis=<span class="number">1</span>)</span><br><span class="line">test_loss_mean = -np.mean(test_loss, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>可视化图形:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(train_sizes, train_loss_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>,</span><br><span class="line">         label=<span class="string">"Training"</span>)</span><br><span class="line">plt.plot(train_sizes, test_loss_mean, <span class="string">'o-'</span>, color=<span class="string">"g"</span>,</span><br><span class="line">        label=<span class="string">"Cross-validation"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">"Training examples"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/sklearn-6-cross-validation-2-output_7_0.png" width="400"><br></div>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">scikit-learn morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn Cross-validation 1]]></title>
      <url>http://sggo.me/2018/01/08/python/py-sklearn-6-cross-validation-1/</url>
      <content type="html"><![CDATA[<p>Sklearn 中的 <code>Cross-validation</code> 对于我们选择正确的 <code>Model</code> 和 <code>Model 的参数</code>是非常有用， 有了它我们能直观的看出不同 Model 或者参数对结构准确度的影响。</p>
<a id="more"></a>
<h2 id="Model-基础验证法"><a href="#Model-基础验证法" class="headerlink" title="Model 基础验证法"></a>Model 基础验证法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris <span class="comment"># iris数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment"># 分割数据模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier <span class="comment"># K最近邻(kNN，k-NearestNeighbor)分类算法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#加载iris数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment">#分割数据并</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#建立模型</span></span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将准确率打印出</span></span><br><span class="line">print(knn.score(X_test, y_test))</span><br></pre></td></tr></table></figure>
<pre><code>0.973684210526
</code></pre><p>可以看到基础验证的准确率为 <code>0.973684210526</code></p>
<h2 id="Model-Cross-Validation"><a href="#Model-Cross-Validation" class="headerlink" title="Model Cross Validation"></a>Model Cross Validation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> cross_val_score <span class="comment"># K折交叉验证模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用K折交叉验证模块</span></span><br><span class="line">scores = cross_val_score(knn, X, y, cv=<span class="number">5</span>, scoring=<span class="string">'accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将5次的预测准确率打印出</span></span><br><span class="line">print(scores)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将5次的预测准确平均率打印出</span></span><br><span class="line">print(scores.mean())</span><br></pre></td></tr></table></figure>
<pre><code>[ 0.96666667  1.          0.93333333  0.96666667  1.        ]
0.973333333333
</code></pre><p>可以看到交叉验证的准确平均率为 <code>0.973333333333</code></p>
<h2 id="Aaccuracy-准确率判断"><a href="#Aaccuracy-准确率判断" class="headerlink" title="Aaccuracy 准确率判断"></a>Aaccuracy 准确率判断</h2><p>一般来说 <code>准确率(accuracy)</code> 会用于判断分类(Classification)模型的好坏</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#可视化模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#建立测试参数集</span></span><br><span class="line">k_range = range(<span class="number">1</span>, <span class="number">31</span>)</span><br><span class="line"></span><br><span class="line">k_scores = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#藉由迭代的方式来计算不同参数对模型的影响，并返回交叉验证后的平均准确率</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    scores = cross_val_score(knn, X, y, cv=<span class="number">10</span>, scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    k_scores.append(scores.mean())</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化数据</span></span><br><span class="line">plt.plot(k_range, k_scores)</span><br><span class="line">plt.xlabel(<span class="string">'Value of K for KNN'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cross-Validated Accuracy'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/sklearn-6-cross-validation-1-output_6_0.png" width="400"><br></div>

<p>从图中得知，选择 <code>12~18</code> 的 <code>k</code> 值最好。高过 <code>18</code> 之后，准确率开始下降则是因为过拟合(<code>Over fitting</code>)的问题。</p>
<h2 id="Mean-squared-error"><a href="#Mean-squared-error" class="headerlink" title="Mean squared error"></a>Mean squared error</h2><p>一般来说平均方差(<code>Mean squared error</code>)会用于判断回归(<code>Regression</code>)模型的好坏</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">k_range = range(<span class="number">1</span>, <span class="number">31</span>)</span><br><span class="line">k_scores = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    loss = -cross_val_score(knn, X, y, cv=<span class="number">10</span>, scoring=<span class="string">'mean_squared_error'</span>)</span><br><span class="line">    k_scores.append(loss.mean())</span><br><span class="line"></span><br><span class="line">plt.plot(k_range, k_scores)</span><br><span class="line">plt.xlabel(<span class="string">'Value of K for KNN'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cross-Validated MSE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/sklearn-6-cross-validation-1-output_8_1.png" width="400"><br></div>

<p>由图可以得知，平均方差越低越好，因此选择<code>13~18</code>左右的<code>K</code>值会最好</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">scikit-learn morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[杨帅完美发音班 10.1]]></title>
      <url>http://sggo.me/2018/01/07/English/english-ielts-speaking-yangshuai-10.1/</url>
      <content type="html"><![CDATA[<p>一定要不断重复，一定要超级夸张，一个音一个音读</p>
<a id="more"></a>
<h2 id="易错辅音"><a href="#易错辅音" class="headerlink" title="易错辅音"></a>易错辅音</h2><p><img src="/images/english/ielts-yangshuai-10.1-1.jpg" width="700"></p>
<div class="limg1"><br><img src="/images/english/ielts-yangshuai-10.1-2.jpg" width="400"><br></div>

<p><img src="/images/english/ielts-yangshuai-10.1-3.jpg" width="700"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn Normalization]]></title>
      <url>http://sggo.me/2018/01/06/python/py-sklearn-5-normalization/</url>
      <content type="html"><![CDATA[<p>Data Normalization 可以提升机器学习的成效</p>
<a id="more"></a>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing <span class="comment">#标准化数据模块</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立Array</span></span><br><span class="line">a = np.array([[<span class="number">10</span>, <span class="number">2.7</span>, <span class="number">3.6</span>],</span><br><span class="line">              [<span class="number">-100</span>, <span class="number">5</span>, <span class="number">-2</span>],</span><br><span class="line">              [<span class="number">120</span>, <span class="number">20</span>, <span class="number">40</span>]], dtype=np.float64)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将normalized后的a打印出</span></span><br><span class="line">print(preprocessing.scale(a))</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.         -0.85170713 -0.55138018]
 [-1.22474487 -0.55187146 -0.852133  ]
 [ 1.22474487  1.40357859  1.40351318]]
</code></pre><h2 id="Normalization-对结果的影响"><a href="#Normalization-对结果的影响" class="headerlink" title="Normalization 对结果的影响"></a>Normalization 对结果的影响</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 标准化数据模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将资料分割成train与test的模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成适合做classification资料的模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_classification </span><br><span class="line"></span><br><span class="line"><span class="comment"># Support Vector Machine中的Support Vector Classifier</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC </span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化数据的模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h3 id="生成适合做-Classification-数据"><a href="#生成适合做-Classification-数据" class="headerlink" title="生成适合做 Classification 数据"></a>生成适合做 Classification 数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成具有2种属性的300笔数据</span></span><br><span class="line">X, y = make_classification(</span><br><span class="line">    n_samples=<span class="number">300</span>, n_features=<span class="number">2</span>,</span><br><span class="line">    n_redundant=<span class="number">0</span>, n_informative=<span class="number">2</span>, </span><br><span class="line">    random_state=<span class="number">22</span>, n_clusters_per_class=<span class="number">1</span>, </span><br><span class="line">    scale=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># n_features 特征个数 = n_informative（） + n_redundant + n_repeated</span></span><br><span class="line"><span class="comment"># n_informative 多信息特征的个数</span></span><br><span class="line"><span class="comment"># n_redundant 冗余信息，informative 特征的随机线性组合</span></span><br><span class="line"><span class="comment"># n_classes 分类类别</span></span><br><span class="line"><span class="comment"># n_clusters_per_class 某一个类别是由几个 cluster 构成的</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化数据</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/sklearn-5-normalization-output_5_0.png" width="400"><br></div>

<h3 id="data-normalization-before"><a href="#data-normalization-before" class="headerlink" title="data normalization before"></a>data normalization before</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br><span class="line">clf = SVC()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>
<pre><code>0.477777777778
</code></pre><h3 id="data-normalization-after"><a href="#data-normalization-after" class="headerlink" title="data normalization after"></a>data normalization after</h3><p>数据的单位发生了变化, <code>X</code> 数据也被压缩到差不多大小范围.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = preprocessing.scale(X)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br><span class="line">clf = SVC()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(clf.score(X_test, y_test))</span><br><span class="line"><span class="comment"># 0.9</span></span><br></pre></td></tr></table></figure>
<pre><code>0.933333333333
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">scikit-learn morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn General Learning Model]]></title>
      <url>http://sggo.me/2018/01/05/python/py-sklearn-2-general-learning-model/</url>
      <content type="html"><![CDATA[<p>Sklearn 把所有机器学习的模式整合统一起来了，学会了一个模式就可以通吃其他不同类型的学习模式</p>
<a id="more"></a>
<h2 id="使用分类器"><a href="#使用分类器" class="headerlink" title="使用分类器"></a>使用分类器</h2><p>Sklearn 本身就有很多数据库，可以用来练习。 以 Iris 的数据为例，这种花有四个属性，花瓣的长宽，茎的长宽，根据这些属性把花分为三类。</p>
<p>我们要用 分类器 去把四种类型的花分开。</p>
<p><img src="/images/python/sklearn-2-general-learning-model.png" alt=""></p>
<p>今天用 <code>KNN classifier</code>，就是选择几个临近点，综合它们做个平均来作为预测值</p>
<h2 id="导入模块"><a href="#导入模块" class="headerlink" title="导入模块"></a>导入模块</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br></pre></td></tr></table></figure>
<h2 id="创建数据"><a href="#创建数据" class="headerlink" title="创建数据"></a>创建数据</h2><p>加载 <code>iris</code> 的数据，把属性存在 <code>X</code>，类别标签存在 <code>y</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">iris_X = iris.data</span><br><span class="line">iris_y = iris.target</span><br></pre></td></tr></table></figure>
<p>观察一下数据集，<code>X</code> 有四个属性，<code>y</code> 有 <code>0</code>，<code>1</code>，<code>2</code> 三类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(iris_X[:<span class="number">2</span>, :])</span><br><span class="line">print(iris_y)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 5.1  3.5  1.4  0.2]
 [ 4.9  3.   1.4  0.2]]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
</code></pre><p>把数据集分为训练集和测试集，其中 <code>test_size=0.3</code>，即测试集占总数据的 30%：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到分开后的数据集，顺序也被打乱，这样更有利于学习模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(y_train)</span><br></pre></td></tr></table></figure>
<pre><code>[0 0 2 2 1 0 2 0 1 1 0 2 1 2 2 0 0 1 0 1 0 2 1 1 1 2 2 1 0 0 2 2 2 2 2 1 0
 0 0 0 1 2 1 2 1 0 2 1 2 2 2 1 0 1 2 1 0 0 2 1 1 0 2 2 0 2 1 0 0 2 0 0 0 1
 2 0 1 1 2 2 0 1 0 2 2 0 1 0 0 1 2 1 1 2 2 1 1 0 0 2 0 0 1 1 0]
</code></pre><h2 id="建立模型－训练－预测"><a href="#建立模型－训练－预测" class="headerlink" title="建立模型－训练－预测"></a>建立模型－训练－预测</h2><p>定义模块方式 <code>KNeighborsClassifier()</code>， 用 <code>fit</code> 来训练 <code>training data</code>，这一步就完成了训练的所有步骤， 后面的 <code>knn</code> 就已经是训练好的模型，可以直接用来 <code>predict</code> 测试集的数据， 对比用模型预测的值与真实的值，可以看到大概模拟出了数据，但是有误差，是不会完完全全预测正确的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn = KNeighborsClassifier()</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(knn.predict(X_test))</span><br><span class="line">print(y_test)</span><br></pre></td></tr></table></figure>
<pre><code>[1 0 1 0 1 2 0 1 0 1 2 1 1 1 2 2 1 2 0 1 2 0 0 2 1 2 1 1 0 1 1 0 2 2 2 0 1
 0 2 0 2 0 1 2 1]
[1 0 1 0 1 2 0 1 0 1 2 1 1 1 2 2 1 2 0 1 2 0 0 2 1 2 1 1 0 1 1 0 2 2 2 0 1
 0 2 0 2 0 2 2 1]
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">scikit-learn morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn Common Attributes and Functions]]></title>
      <url>http://sggo.me/2018/01/05/python/py-sklearn-4-common-attributes/</url>
      <content type="html"><![CDATA[<p>今天来看 <code>Model</code> 的属性和功能, 这里以 <code>LinearRegressor</code> 为例，所以先导入包，数据，还有模型</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">loaded_data = datasets.load_boston()</span><br><span class="line"></span><br><span class="line">data_X = loaded_data.data</span><br><span class="line">data_y = loaded_data.target</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure>
<h2 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h2><p>接下来 <code>model.fit</code> 和 <code>model.predict</code> 就属于 <code>Model</code> 的功能，用来训练模型，用训练好的模型预测</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(data_X, data_y)</span><br><span class="line"></span><br><span class="line">print(model.predict(data_X[:<span class="number">4</span>, :]))</span><br></pre></td></tr></table></figure>
<pre><code>[ 30.00821269  25.0298606   30.5702317   28.60814055]
</code></pre><h2 id="参数和分数"><a href="#参数和分数" class="headerlink" title="参数和分数"></a>参数和分数</h2><p><code>model.coef_</code> 和 <code>model.intercept_</code> 属于 <code>Model</code> 的属性， 例如对于 <code>LinearRegressor</code> 这个模型，这两个属性分别输出模型的斜率和截距（与y轴的交点）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(model.coef_)</span><br><span class="line">print(model.intercept_)</span><br></pre></td></tr></table></figure>
<pre><code>[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00
  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00
   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03
  -5.25466633e-01]
36.4911032804
</code></pre><p><code>model.get_params()</code> 也是功能，它可以取出之前定义的参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(model.get_params())</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;copy_X&apos;: True, &apos;fit_intercept&apos;: True, &apos;n_jobs&apos;: 1, &apos;normalize&apos;: False}
</code></pre><p><code>model.score(data_X, data_y)</code> 它可以对 <code>Model</code> 用 <code>R^2</code> 的方式进行打分，输出精确度。<br>关于 <code>R^2 coefficient of determination</code> 可以查看 <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" target="_blank" rel="external">Coefficient_of_determination</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(model.score(data_X, data_y)) <span class="comment"># R^2 coefficient of determination</span></span><br></pre></td></tr></table></figure>
<pre><code>0.740607742865
</code></pre><blockquote>
<p>按标准的来说, 是要将数据分成训练数据和测试数据, 这里不是一个完整的测试, 只是展示 model 里面的一些属性. 正确率很少能真正100%, 取决于拟合度怎么样. 拟合度好, 正确率高</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">scikit-learn morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn DataSets]]></title>
      <url>http://sggo.me/2018/01/03/python/py-sklearn-3-database/</url>
      <content type="html"><![CDATA[<p>Sklearn 中的 <a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets" target="_blank" rel="external">data sets</a>，很多而且有用，可以用来学习算法模型</p>
<a id="more"></a>
<h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><p>eg: boston 房价, 糖尿病, 数字, Iris 花。</p>
<p>也可以生成虚拟的数据，例如用来训练线性回归模型的数据，可以用函数来生成</p>
<div class="limg1"><br><img src="/images/python/sklearn-3-datasets-1.png" width="400"><br></div><br><!--![][img-1]--><br><br>例如，点击进入 boston 房价的数据，可以看到 <code>sample</code> 的总数，属性，以及 <code>label</code> 等信息<br><br><div class="limg1"><br><img src="/images/python/sklearn-3-datasets-2.png" width="650"><br></div>

<p>如果是自己生成数据，按照函数的形式，输入 <code>sample</code>，<code>feature</code>，<code>target</code> 的个数等等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sklearn.datasets.make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">100</span>, n_informative=<span class="number">10</span>, n_targets=<span class="number">1</span>, bias=<span class="number">0.0</span>, effective_rank=<span class="keyword">None</span>, tail_strength=<span class="number">0.5</span>, noise=<span class="number">0.0</span>, shuffle=<span class="keyword">True</span>, coef=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>)[source]</span><br></pre></td></tr></table></figure>
<p>接下来用代码练习…</p>
<h2 id="导入模块"><a href="#导入模块" class="headerlink" title="导入模块"></a>导入模块</h2><p>导入 <code>datasets</code> 包，以 Linear Regression 为例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="导入数据－训练模型"><a href="#导入数据－训练模型" class="headerlink" title="导入数据－训练模型"></a>导入数据－训练模型</h2><p>用 <code>datasets.load_boston()</code> 的形式加载数据，并给 <code>X</code> 和 <code>y</code> 赋值，这种形式在 Sklearn 中都是高度统一的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loaded_data = datasets.load_boston()</span><br><span class="line"></span><br><span class="line">data_X = loaded_data.data</span><br><span class="line">data_y = loaded_data.target</span><br><span class="line"></span><br><span class="line">print(data_X[:<span class="number">4</span>, <span class="number">0</span>]) <span class="comment"># == print(data_X[:4][0])</span></span><br><span class="line">print(data_y[:<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[ 0.00632  0.02731  0.02729  0.03237]
[ 24.   21.6  34.7  33.4]
</code></pre><p>定义模型</p>
<p>可以直接用默认值去建立 <code>model</code>，默认值也不错，也可以自己改变参数使模型更好。 然后用 <code>training data</code> 训练模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(data_X, data_y)</span><br></pre></td></tr></table></figure>
<pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre><p>再打印出预测值，这里用 <code>X</code> 的前 4 个来预测，同时打印真实值，作为对比，可以看到是有些误差的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(model.predict(data_X[:<span class="number">4</span>, :]))</span><br><span class="line">print(data_y[:<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[ 30.00821269  25.0298606   30.5702317   28.60814055]
[ 24.   21.6  34.7  33.4]
</code></pre><p>为了提高准确度，可以通过尝试不同的 <code>model</code>，不同的参数，不同的预处理等方法，入门的话可以直接用默认值</p>
<h2 id="创建虚拟数据－可视化"><a href="#创建虚拟数据－可视化" class="headerlink" title="创建虚拟数据－可视化"></a>创建虚拟数据－可视化</h2><p>下面是创造数据的例子。</p>
<p>用函数来建立 100 个 <code>sample</code>，有一个 <code>feature</code>，和一个 <code>target</code>，这样比较方便可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = datasets.make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">1</span>, n_targets=<span class="number">1</span>, noise=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>用 scatter 的形式来输出结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/sklearn-3-datasets-output_11_0.png"><br></div>

<p>可以看到用函数生成的 <code>Linear Regression</code> 用的数据。</p>
<p><code>noise</code> 越大的话，点就会越来越离散，例如 <code>noise</code> 由 10 变为 50.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = datasets.make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">1</span>, n_targets=<span class="number">1</span>, noise=<span class="number">50</span>)</span><br><span class="line">plt.scatter(X, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/sklearn-3-datasets-output_13_0.png"><br></div>

<!--![output_13_0][img-4]-->
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets" target="_blank" rel="external">scikit-learn machine_learning_map</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn Choosing The Right Estimator]]></title>
      <url>http://sggo.me/2018/01/03/python/py-sklearn-1-choosing-estimator/</url>
      <content type="html"><![CDATA[<p>Sciki-learn 选择学习方法，选择模型 <a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" target="_blank" rel="external">流程图</a></p>
<a id="more"></a>
<p>Sklearn 官网提供了一个流程图，蓝色圆圈内是判断条件，绿色方框内是可以选择的算法：<a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" target="_blank" rel="external">详情</a></p>
<p><img src="/images/python/sklearn-1-model-choosing.png" alt="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html]"></p>
<p>从 START 开始，首先看数据的样本是否 <code>&gt;50</code>，小于则需要收集更多的数据。</p>
<p>由图中，可以看到算法有四类，<code>分类</code>，<code>回归</code>，<code>聚类</code>，<code>降维</code>。</p>
<table>
<thead>
<tr>
<th>algorithm</th>
<th>desc</th>
</tr>
</thead>
<tbody>
<tr>
<td>分类</td>
<td>监督式学习，即每个数据对应一个 label </td>
</tr>
<tr>
<td>回归</td>
<td>监督式学习，即每个数据对应一个 label </td>
</tr>
<tr>
<td>聚类</td>
<td>非监督式学习，即没有 label。 </td>
</tr>
<tr>
<td>降维</td>
<td>当数据集有很多很多属性的时候，可以通过 降维 算法把属性归纳起来。<br><br> 例如 20 个属性只变成 2 个，注意，这不是挑出 2 个，而是压缩成为 2 个，<br>它们集合了 20 个属性的所有特征，相当于把重要的信息提取的更好，不重要的信息就不要了</td>
</tr>
</tbody>
</table>
<p>然后看问题属于哪一类问题，是分类还是回归，还是聚类，就选择相应的算法。 当然还要考虑数据的大小，例如 <code>100K</code> 是一个阈值。</p>
<p>可以发现有些方法是既可以作为分类，也可以作为回归，例如 <code>SGD</code>。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" target="_blank" rel="external">scikit-learn machine_learning_map</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sklearn Why ?]]></title>
      <url>http://sggo.me/2018/01/03/python/py-sklearn-0-why/</url>
      <content type="html"><![CDATA[<p>Scikit learn 也简称 sklearn, 是机器学习领域当中最知名的 python 模块之一.</p>
<a id="more"></a>
<p>Sklearn 包含了很多种机器学习的方式:</p>
<ul>
<li>Classification 分类</li>
<li>Regression 回归</li>
<li>Clustering 非监督分类</li>
<li>Dimensionality reduction 数据降维</li>
<li>Model Selection 模型选择</li>
<li>Preprocessing 数据预处理</li>
</ul>
<p>我们总能够从这些方法中挑选出一个适合于自己问题的, 然后解决自己的问题.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn.org</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="external">scikit-learn docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Lucy地道美语 英语国际音标]]></title>
      <url>http://sggo.me/2018/01/03/English/english-ielts-lucy-speaking/</url>
      <content type="html"><![CDATA[<ul>
<li>一定要不断重复，不要相信自己的耳朵，要相信音标</li>
<li>最开始练习，一定要超级夸张，一个音一个音读</li>
</ul>
<a id="more"></a>
<h2 id="1-分组元音"><a href="#1-分组元音" class="headerlink" title="1. 分组元音"></a>1. 分组元音</h2><table>
<thead>
<tr>
<th>元音 1</th>
<th>desc</th>
<th>sample</th>
<th>元音 2</th>
<th>desc</th>
<th>sample</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><em>i:</em></strong></td>
<td>微笑音，嘴巴拉长</td>
<td>please、sleep</td>
<td><strong><em>i</em></strong></td>
<td>军训音</td>
<td>big、pig</td>
</tr>
<tr>
<td><strong><em>e</em></strong></td>
<td>一个手指音</td>
<td>egg、bed</td>
<td><strong><em>æ</em></strong></td>
<td>大嘴巴音 /ɛ/+/a:/</td>
<td>bad、dad</td>
</tr>
<tr>
<td><strong><em>a:</em></strong></td>
<td>尾部儿话~</td>
<td>car, star</td>
<td><strong><em>ɒ</em></strong></td>
<td>嘴张开、无变化</td>
<td>not、lost、job </td>
</tr>
<tr>
<td><strong><em>ɔ:</em></strong></td>
<td>有点像 ‘沃’,多儿话</td>
<td>horse、short</td>
<td><strong><em>ʊ</em></strong></td>
<td>发音靠后，脖子下</td>
<td>good、should、would、could</td>
</tr>
<tr>
<td><strong><em>u:</em></strong></td>
<td>悟空的‘悟’拉长</td>
<td>sch<code>oo</code>l、r<code>oo</code>m</td>
<td><strong><em>ʌ</em></strong></td>
<td>短元音</td>
<td>l<code>o</code>ve、c<code>o</code>me</td>
</tr>
<tr>
<td><strong><em>ɜ:</em></strong></td>
<td>-</td>
<td>b<code>ir</code>d、f<code>ir</code>st</td>
<td><strong><em>ə</em></strong></td>
<td>-</td>
<td><code>a</code>bout、<code>a</code>sleep、Chin<code>a</code></td>
</tr>
<tr>
<td><strong><em>eɪ</em></strong></td>
<td>-</td>
<td>b<code>a</code>by、f<code>a</code>ce</td>
<td><strong><em>aɪ</em></strong></td>
<td>-</td>
<td>h<code>i</code></td>
</tr>
<tr>
<td><strong><em>aʊ</em></strong></td>
<td>-</td>
<td>c<code>ow</code>、h<code>ow</code></td>
<td><strong><em>ɪə(r)</em></strong></td>
<td>-</td>
<td>ear、dear</td>
</tr>
<tr>
<td><strong><em>eə</em></strong></td>
<td>-</td>
<td><code>air</code>、h<code>air</code></td>
<td><strong><em>əʊ</em></strong></td>
<td>-</td>
<td>b<code>oa</code>t、c<code>oa</code>t</td>
</tr>
<tr>
<td><strong><em>ɔɪ</em></strong></td>
<td>-</td>
<td>b<code>oy</code>、j<code>oy</code></td>
<td><strong><em>ʊə</em></strong></td>
<td>-</td>
<td>sure /[ʃʊə(r)]/</td>
</tr>
</tbody>
</table>
<blockquote>
<p>China <strong><em>ə</em></strong> 词尾变音</p>
</blockquote>
<h2 id="2-清浊辅音"><a href="#2-清浊辅音" class="headerlink" title="2. 清浊辅音"></a>2. 清浊辅音</h2><table>
<thead>
<tr>
<th>清辅音</th>
<th>desc</th>
<th>sample</th>
<th>浊辅音</th>
<th>desc</th>
<th>sample</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><em>p</em></strong></td>
<td></td>
<td>pen、map</td>
<td><strong><em>b</em></strong></td>
<td></td>
<td>big、job</td>
</tr>
<tr>
<td><strong><em>k</em></strong></td>
<td></td>
<td>book、week</td>
<td><strong><em>g</em></strong></td>
<td></td>
<td>egg、dog</td>
</tr>
<tr>
<td><strong><em>f</em></strong></td>
<td></td>
<td>wife、knife</td>
<td><strong><em>v</em></strong></td>
<td></td>
<td>love、give</td>
</tr>
<tr>
<td><strong><em>s</em></strong></td>
<td>舌头放嘴里</td>
<td>sink、sick、month、nice、rice</td>
<td><strong><em>z</em></strong></td>
<td></td>
<td>his、size</td>
</tr>
<tr>
<td><strong><em>ʃ</em></strong></td>
<td></td>
<td>wish、dish</td>
<td><strong><em>ʒ</em></strong></td>
<td></td>
<td>usually、vision</td>
</tr>
<tr>
<td><strong><em>θ</em></strong></td>
<td>用力吐舌头</td>
<td>think、thick、mouth、month、bath</td>
<td><strong><em>ð</em></strong></td>
<td></td>
<td>with、smooth</td>
</tr>
<tr>
<td><strong><em>r</em></strong></td>
<td></td>
<td>sorry、story</td>
<td><strong><em>h</em></strong></td>
<td></td>
<td>hand、home</td>
</tr>
<tr>
<td><strong><em>tʃ</em></strong></td>
<td></td>
<td>teach、lunch</td>
<td><strong><em>dʒ</em></strong></td>
<td></td>
<td>page、large</td>
</tr>
<tr>
<td><strong><em>tr</em></strong></td>
<td></td>
<td>tree、trip</td>
<td><strong><em>dr</em></strong></td>
<td></td>
<td>dress、dry</td>
</tr>
<tr>
<td><strong><em>ts</em></strong></td>
<td></td>
<td>cats、hats</td>
<td><strong><em>dz</em></strong></td>
<td></td>
<td>beds、cards</td>
</tr>
<tr>
<td><strong><em>m</em></strong></td>
<td></td>
<td>mom、some</td>
<td><strong><em>n</em></strong></td>
<td></td>
<td>pen、son、sun</td>
</tr>
<tr>
<td><strong><em>ŋ</em></strong></td>
<td></td>
<td>drink</td>
<td><strong><em>l</em></strong></td>
<td></td>
<td>let、ball</td>
</tr>
<tr>
<td><strong><em>w</em></strong></td>
<td></td>
<td>why、water</td>
<td><strong><em>j</em></strong></td>
<td></td>
<td>year、yellow</td>
</tr>
</tbody>
</table>
<blockquote>
<p>clothes [kləʊðz] </p>
</blockquote>
<div class="limg1"><br><img src="/images/english/ielts-phonetic-symbol.jpg" height="100" width="650"><br></div>

<h2 id="3-Part-of-Speech"><a href="#3-Part-of-Speech" class="headerlink" title="3. Part of Speech"></a>3. Part of Speech</h2><p><img src="/images/english/ielts-lucy-8-part-of-speech.png" height="100" width="680"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Figure]]></title>
      <url>http://sggo.me/2018/01/01/python/py-matplotlib-3-figure-image/</url>
      <content type="html"><![CDATA[<p>matplotlib 的 figure 就是一个 单独的 figure 小窗口, 小窗口里面还可以有更多的小图片</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p>使用<code>np.linspace</code>定义<code>x</code>：范围是(-3,3);个数是50. 仿真一维数据组(<code>x</code> ,<code>y1</code>)表示曲线1. 仿真一维数据组(<code>x</code> ,<code>y2</code>)表示曲线2.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">y1 = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line">y2 = x**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>使用<code>plt.figure</code>定义一个图像窗口. 使用<code>plt.plot</code>画(<code>x</code> ,<code>y1</code>)曲线.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-2-figure-1.png" height="100" width="450"><br></div>

<p>使用<code>plt.figure</code>定义一个图像窗口：编号为3；大小为(8, 5). </p>
<p>使用<code>plt.plot</code>画(<code>x</code> ,<code>y2</code>)曲线.<br>使用<code>plt.plot</code>画(<code>x</code> ,<code>y1</code>)曲线，曲线的颜色属性(<code>color</code>)为红色;曲线的宽度(<code>linewidth</code>)为1.0；曲线的类型(<code>linestyle</code>)为虚线. 使用<code>plt.show</code>显示图像.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(num=<span class="number">3</span>, figsize=(<span class="number">8</span>, <span class="number">5</span>),)</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=<span class="string">'red'</span>, linewidth=<span class="number">1.0</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-2-figure-2.png" height="100" width="450"><br></div>

<h2 id="Set-Coordinate-axis"><a href="#Set-Coordinate-axis" class="headerlink" title="Set Coordinate axis"></a>Set Coordinate axis</h2><p>使用<code>plt.xlim</code>设置x坐标轴范围：(-1, 2)； 使用<code>plt.ylim</code>设置y坐标轴范围：(-2, 3)；<br>使用<code>plt.xlabel</code>设置x坐标轴名称：’I am x’； 使用<code>plt.ylabel</code>设置y坐标轴名称：’I am y’；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(num=<span class="number">3</span>, figsize=(<span class="number">8</span>, <span class="number">5</span>),)</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=<span class="string">'red'</span>, linewidth=<span class="number">1.0</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim((<span class="number">-1</span>, <span class="number">2</span>))</span><br><span class="line">plt.ylim((<span class="number">-2</span>, <span class="number">3</span>))</span><br><span class="line">plt.xlabel(<span class="string">'I am x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'I am y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/python/matplotlib-2-figure-3-output_9_0.png" alt="output_9_0.png"></p>
<p>使用 <code>np.linspace</code> 定义范围以及个数：范围是(-1,2);个数是5.<br>使用 <code>print</code> 打印出新定义的范围.<br>使用 <code>plt.xticks</code> 设置x轴刻度：范围是(-1,2);个数是5.   </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(num=<span class="number">3</span>, figsize=(<span class="number">8</span>, <span class="number">5</span>),)</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y1, color=<span class="string">'red'</span>, linewidth=<span class="number">1.0</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim((<span class="number">-1</span>, <span class="number">2</span>))</span><br><span class="line">plt.ylim((<span class="number">-2</span>, <span class="number">3</span>))</span><br><span class="line">plt.xlabel(<span class="string">'I am x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'I am y'</span>)</span><br><span class="line"></span><br><span class="line">new_ticks = np.linspace(<span class="number">-1</span>, <span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">print(new_ticks)</span><br><span class="line">plt.xticks(new_ticks)</span><br></pre></td></tr></table></figure>
<pre><code>[-1.   -0.25  0.5   1.25  2.  ]

([&lt;matplotlib.axis.XTick at 0x11dd68ba8&gt;,
  &lt;matplotlib.axis.XTick at 0x11dd68a58&gt;,
  &lt;matplotlib.axis.XTick at 0x11dd7d5c0&gt;,
  &lt;matplotlib.axis.XTick at 0x11e04af28&gt;,
  &lt;matplotlib.axis.XTick at 0x11e0515c0&gt;],
 &lt;a list of 5 Text xticklabel objects&gt;)
</code></pre><p>使用<code>plt.yticks</code>设置y轴刻度以及名称：刻度为[-2, -1.8, -1, 1.22, 3]；对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 使用plt.show显示图像.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.yticks([<span class="number">-2</span>, <span class="number">-1.8</span>, <span class="number">-1</span>, <span class="number">1.22</span>, <span class="number">3</span>],[<span class="string">r'$really\ bad$'</span>, <span class="string">r'$bad$'</span>, <span class="string">r'$normal$'</span>, <span class="string">r'$good$'</span>, <span class="string">r'$really\ good$'</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/python/matplotlib-2-figure-4-output_13_0.png" alt="output_13_0.png"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Basic Use]]></title>
      <url>http://sggo.me/2018/01/01/python/py-matplotlib-2-basic-use/</url>
      <content type="html"><![CDATA[<p>Matplotlib 最基本的使用介绍</p>
<a id="more"></a>
<h2 id="基础应用"><a href="#基础应用" class="headerlink" title="基础应用"></a>基础应用</h2><p>使用<code>import</code>导入模块<code>matplotlib.pyplot</code>，并简写成<code>plt</code> 使用<code>import</code>导入模块<code>numpy</code>，并简写成<code>np</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([-1.        , -0.95918367, -0.91836735, -0.87755102, -0.83673469,
       -0.79591837, -0.75510204, -0.71428571, -0.67346939, -0.63265306,
       -0.59183673, -0.55102041, -0.51020408, -0.46938776, -0.42857143,
       -0.3877551 , -0.34693878, -0.30612245, -0.26530612, -0.2244898 ,
       -0.18367347, -0.14285714, -0.10204082, -0.06122449, -0.02040816,
        0.02040816,  0.06122449,  0.10204082,  0.14285714,  0.18367347,
        0.2244898 ,  0.26530612,  0.30612245,  0.34693878,  0.3877551 ,
        0.42857143,  0.46938776,  0.51020408,  0.55102041,  0.59183673,
        0.63265306,  0.67346939,  0.71428571,  0.75510204,  0.79591837,
        0.83673469,  0.87755102,  0.91836735,  0.95918367,  1.        ])
</code></pre><p>使用 <a href="https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.linspace.html" target="_blank" rel="external">np.linspace</a> 定义<code>x</code>：范围是(-1,1); 个数是50. 仿真一维数据组(<code>x</code> ,<code>y</code>)表示曲线1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">2</span>*x + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>使用<code>plt.figure</code>定义一个图像窗口. 使用<code>plt.plot</code>画(<code>x</code> ,<code>y</code>)曲线. 使用<code>plt.show</code>显示图像.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div class="limg1"><br><img src="/images/python/matplotlib-1-basic-use-1.png" height="100" width="450"><br></div>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Matplotlib Why ?]]></title>
      <url>http://sggo.me/2018/01/01/python/py-matplotlib-1-why/</url>
      <content type="html"><![CDATA[<p>Matplotlib 是一个非常强大的 Python 画图工具</p>
<a id="more"></a>
<p>Matplotlib 它能帮你画出美丽的:</p>
<ul>
<li>线图;</li>
<li>散点图;</li>
<li>等高线图;</li>
<li>条形图;</li>
<li>柱状图;</li>
<li>3D 图形,</li>
<li>甚至是图形动画等等.</li>
</ul>
<p>下面是一些例图:</p>
<div class="limg1"><br><img src="/images/python/matplotlib-0-why-1.png" height="100" width="650"><br></div>

<div class="limg1"><br><img src="/images/python/matplotlib-0-why-3.png" height="100" width="650"><br></div>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/" target="_blank" rel="external">matplotlib.org</a></li>
<li><a href="https://matplotlib.org/contents.html" target="_blank" rel="external">matplotlib docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pandas Matplotlib Intro]]></title>
      <url>http://sggo.me/2017/12/31/python/py-pandas-8-matplotlib/</url>
      <content type="html"><![CDATA[<p>matplotlib 将数据可视化. 仅仅是用来 show 图片的, 即 plt.show()</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="创建一个Series"><a href="#创建一个Series" class="headerlink" title="创建一个Series"></a>创建一个Series</h2><p>这是一个线性的数据，我们随机生成1000个数据，<code>Series</code> 默认的 <code>index</code> 就是从0开始的整数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机生成1000个数据</span></span><br><span class="line">data = pd.Series(np.random.randn(<span class="number">1000</span>),index=np.arange(<span class="number">1000</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># pandas 数据可以直接观看其可视化形式</span></span><br><span class="line">data.plot()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/python/pandas-output_3_0.png" alt="png"></p>
<p>可以使用 <code>plt.plot(x=, y=)</code>，把<code>x</code>,<code>y</code>的数据作为参数存进去，但是<code>data</code>本来就是一个数据，所以我们可以直接<code>plot</code></p>
<h2 id="Dataframe-可视化"><a href="#Dataframe-可视化" class="headerlink" title="Dataframe 可视化"></a>Dataframe 可视化</h2><p>我们生成一个 1000*4 的 <code>DataFrame</code>，并对他们累加</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.DataFrame(</span><br><span class="line">    np.random.randn(<span class="number">1000</span>,<span class="number">4</span>),</span><br><span class="line">    index=np.arange(<span class="number">1000</span>),</span><br><span class="line">    columns=list(<span class="string">"ABCD"</span>)</span><br><span class="line">    )</span><br><span class="line"><span class="comment">#data.cumsum()</span></span><br><span class="line">print(data)</span><br><span class="line">data.plot()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>            A         B         C         D
0    1.163604 -0.689103  1.958018  0.241444
1    0.595765  0.816026  1.573164 -0.443003
2   -0.101446  0.768321 -0.203069 -0.638841
3   -0.439233 -0.161273  0.398774  1.309622
4   -0.524647 -0.180073 -1.499978  0.628436
5   -0.305683  0.668840  0.243668 -1.386839
..        ...       ...       ...       ...
998 -0.243955 -0.190122 -0.299633  3.350200
999 -0.055184  0.936187  0.146156  0.604271

[1000 rows x 4 columns]
</code></pre><p><img src="/images/python/pandas-output_5_1.png" alt="png"></p>
<p>这个就是我们刚刚生成的4个<code>column</code>的数据，因为有4组数据，所以4组数据会分别<code>plot</code>出来。</p>
<p>plot 可以指定很多参数，具体参见<a href="http://pandas.pydata.org/pandas-docs/version/0.18.1/visualization.html" target="_blank" rel="external">官方文档</a></p>
<p>除了plot，我经常会用到还有scatter，这个会显示散点图，首先说一下在 pandas 中有多少种方法</p>
<ul>
<li>bar</li>
<li>hist</li>
<li>box</li>
<li>kde</li>
<li>area</li>
<li>scatter</li>
<li>hexbin</li>
</ul>
<p>主要说一下 <code>plot</code> 和 <code>scatter</code>. 因为 <code>scatter</code> 只有 <code>x</code>，<code>y</code> 两个属性，可以分别给 <code>x</code>, <code>y</code> 指定数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = data.plot.scatter(x=<span class="string">'A'</span>,y=<span class="string">'B'</span>,color=<span class="string">'DarkBlue'</span>,label=<span class="string">'Class1'</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们在可以再画一个在同一个<code>ax</code>上面，选择不一样的数据列，不同的 <code>color</code> 和 <code>label</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将之下这个 data 画在上一个 ax 上面</span></span><br><span class="line">data.plot.scatter(x=<span class="string">'A'</span>,y=<span class="string">'C'</span>,color=<span class="string">'LightGreen'</span>,label=<span class="string">'Class2'</span>,ax=ax)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/python/pandas-output_9_0.png" alt="png"></p>
<p>两种呈现方式，一种是<strong>线性的方式</strong>，一种是<strong>散点图</strong></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="external">pandas.pydata.org</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/version/0.21/" target="_blank" rel="external">pandas docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pandas Merge]]></title>
      <url>http://sggo.me/2017/12/31/python/py-pandas-7-merge/</url>
      <content type="html"><![CDATA[<p><code>pandas</code>中的<code>merge</code>和<code>concat</code>类似,但主要是用于两组有<strong>key column</strong>的数据,统一索引的数据.</p>
<a id="more"></a>
<h2 id="依据一组key合并"><a href="#依据一组key合并" class="headerlink" title="依据一组key合并"></a>依据一组key合并</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集并打印出</span></span><br><span class="line">left = pd.DataFrame(&#123;<span class="string">'key'</span>: [<span class="string">'K0'</span>, <span class="string">'K1'</span>, <span class="string">'K2'</span>, <span class="string">'K3'</span>],</span><br><span class="line">                             <span class="string">'A'</span>: [<span class="string">'A0'</span>, <span class="string">'A1'</span>, <span class="string">'A2'</span>, <span class="string">'A3'</span>],</span><br><span class="line">                             <span class="string">'B'</span>: [<span class="string">'B0'</span>, <span class="string">'B1'</span>, <span class="string">'B2'</span>, <span class="string">'B3'</span>]&#125;)</span><br><span class="line">right = pd.DataFrame(&#123;<span class="string">'key'</span>: [<span class="string">'K0'</span>, <span class="string">'K1'</span>, <span class="string">'K2'</span>, <span class="string">'K3'</span>],</span><br><span class="line">                              <span class="string">'C'</span>: [<span class="string">'C0'</span>, <span class="string">'C1'</span>, <span class="string">'C2'</span>, <span class="string">'C3'</span>],</span><br><span class="line">                              <span class="string">'D'</span>: [<span class="string">'D0'</span>, <span class="string">'D1'</span>, <span class="string">'D2'</span>, <span class="string">'D3'</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(left)</span></span><br><span class="line"><span class="comment">#    A   B key</span></span><br><span class="line"><span class="comment"># 0  A0  B0  K0</span></span><br><span class="line"><span class="comment"># 1  A1  B1  K1</span></span><br><span class="line"><span class="comment"># 2  A2  B2  K2</span></span><br><span class="line"><span class="comment"># 3  A3  B3  K3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(right)</span></span><br><span class="line"><span class="comment">#    C   D key</span></span><br><span class="line"><span class="comment"># 0  C0  D0  K0</span></span><br><span class="line"><span class="comment"># 1  C1  D1  K1</span></span><br><span class="line"><span class="comment"># 2  C2  D2  K2</span></span><br><span class="line"><span class="comment"># 3  C3  D3  K3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#依据key column合并，并打印出</span></span><br><span class="line">res = pd.merge(left, right, on=<span class="string">'key'</span>)</span><br><span class="line"></span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<pre><code>    A   B key   C   D
0  A0  B0  K0  C0  D0
1  A1  B1  K1  C1  D1
2  A2  B2  K2  C2  D2
3  A3  B3  K3  C3  D3
</code></pre><h2 id="依据两组key合并"><a href="#依据两组key合并" class="headerlink" title="依据两组key合并"></a>依据两组key合并</h2><p>合并时有4种方法<code>how = [&#39;left&#39;, &#39;right&#39;, &#39;outer&#39;, &#39;inner&#39;]</code>，预设值<code>how=&#39;inner&#39;</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集并打印出</span></span><br><span class="line">left = pd.DataFrame(&#123;<span class="string">'key1'</span>: [<span class="string">'K0'</span>, <span class="string">'K0'</span>, <span class="string">'K1'</span>, <span class="string">'K2'</span>],</span><br><span class="line">                      <span class="string">'key2'</span>: [<span class="string">'K0'</span>, <span class="string">'K1'</span>, <span class="string">'K0'</span>, <span class="string">'K1'</span>],</span><br><span class="line">                      <span class="string">'A'</span>: [<span class="string">'A0'</span>, <span class="string">'A1'</span>, <span class="string">'A2'</span>, <span class="string">'A3'</span>],</span><br><span class="line">                      <span class="string">'B'</span>: [<span class="string">'B0'</span>, <span class="string">'B1'</span>, <span class="string">'B2'</span>, <span class="string">'B3'</span>]&#125;)</span><br><span class="line">right = pd.DataFrame(&#123;<span class="string">'key1'</span>: [<span class="string">'K0'</span>, <span class="string">'K1'</span>, <span class="string">'K1'</span>, <span class="string">'K2'</span>],</span><br><span class="line">                       <span class="string">'key2'</span>: [<span class="string">'K0'</span>, <span class="string">'K0'</span>, <span class="string">'K0'</span>, <span class="string">'K0'</span>],</span><br><span class="line">                       <span class="string">'C'</span>: [<span class="string">'C0'</span>, <span class="string">'C1'</span>, <span class="string">'C2'</span>, <span class="string">'C3'</span>],</span><br><span class="line">                       <span class="string">'D'</span>: [<span class="string">'D0'</span>, <span class="string">'D1'</span>, <span class="string">'D2'</span>, <span class="string">'D3'</span>]&#125;)</span><br><span class="line"></span><br><span class="line">print(left)</span><br><span class="line"><span class="comment">#    A   B key1 key2</span></span><br><span class="line"><span class="comment"># 0  A0  B0   K0   K0</span></span><br><span class="line"><span class="comment"># 1  A1  B1   K0   K1</span></span><br><span class="line"><span class="comment"># 2  A2  B2   K1   K0</span></span><br><span class="line"><span class="comment"># 3  A3  B3   K2   K1</span></span><br><span class="line"></span><br><span class="line">print(right)</span><br><span class="line"><span class="comment">#    C   D key1 key2</span></span><br><span class="line"><span class="comment"># 0  C0  D0   K0   K0</span></span><br><span class="line"><span class="comment"># 1  C1  D1   K1   K0</span></span><br><span class="line"><span class="comment"># 2  C2  D2   K1   K0</span></span><br><span class="line"><span class="comment"># 3  C3  D3   K2   K0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#依据key1与key2 columns进行合并，并打印出四种结果['left', 'right', 'outer', 'inner']</span></span><br><span class="line">res = pd.merge(left, right, on=[<span class="string">'key1'</span>, <span class="string">'key2'</span>], how=<span class="string">'inner'</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#    A   B key1 key2   C   D</span></span><br><span class="line"><span class="comment"># 0  A0  B0   K0   K0  C0  D0</span></span><br><span class="line"><span class="comment"># 1  A2  B2   K1   K0  C1  D1</span></span><br><span class="line"><span class="comment"># 2  A2  B2   K1   K0  C2  D2</span></span><br><span class="line"></span><br><span class="line">res = pd.merge(left, right, on=[<span class="string">'key1'</span>, <span class="string">'key2'</span>], how=<span class="string">'outer'</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#     A    B key1 key2    C    D</span></span><br><span class="line"><span class="comment"># 0   A0   B0   K0   K0   C0   D0</span></span><br><span class="line"><span class="comment"># 1   A1   B1   K0   K1  NaN  NaN</span></span><br><span class="line"><span class="comment"># 2   A2   B2   K1   K0   C1   D1</span></span><br><span class="line"><span class="comment"># 3   A2   B2   K1   K0   C2   D2</span></span><br><span class="line"><span class="comment"># 4   A3   B3   K2   K1  NaN  NaN</span></span><br><span class="line"><span class="comment"># 5  NaN  NaN   K2   K0   C3   D3</span></span><br><span class="line"></span><br><span class="line">res = pd.merge(left, right, on=[<span class="string">'key1'</span>, <span class="string">'key2'</span>], how=<span class="string">'left'</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#    A   B key1 key2    C    D</span></span><br><span class="line"><span class="comment"># 0  A0  B0   K0   K0   C0   D0</span></span><br><span class="line"><span class="comment"># 1  A1  B1   K0   K1  NaN  NaN</span></span><br><span class="line"><span class="comment"># 2  A2  B2   K1   K0   C1   D1</span></span><br><span class="line"><span class="comment"># 3  A2  B2   K1   K0   C2   D2</span></span><br><span class="line"><span class="comment"># 4  A3  B3   K2   K1  NaN  NaN</span></span><br><span class="line"></span><br><span class="line">res = pd.merge(left, right, on=[<span class="string">'key1'</span>, <span class="string">'key2'</span>], how=<span class="string">'right'</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#     A    B key1 key2   C   D</span></span><br><span class="line"><span class="comment"># 0   A0   B0   K0   K0  C0  D0</span></span><br><span class="line"><span class="comment"># 1   A2   B2   K1   K0  C1  D1</span></span><br><span class="line"><span class="comment"># 2   A2   B2   K1   K0  C2  D2</span></span><br><span class="line"><span class="comment"># 3  NaN  NaN   K2   K0  C3  D3</span></span><br></pre></td></tr></table></figure>
<h2 id="Indicator"><a href="#Indicator" class="headerlink" title="Indicator"></a>Indicator</h2><p><code>indicator=True</code>会将合并的记录放在新的一列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集并打印出</span></span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">'col1'</span>:[<span class="number">0</span>,<span class="number">1</span>], <span class="string">'col_left'</span>:[<span class="string">'a'</span>,<span class="string">'b'</span>]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'col1'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],<span class="string">'col_right'</span>:[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]&#125;)</span><br><span class="line"></span><br><span class="line">print(df1)</span><br><span class="line"><span class="comment">#   col1 col_left</span></span><br><span class="line"><span class="comment"># 0     0        a</span></span><br><span class="line"><span class="comment"># 1     1        b</span></span><br><span class="line"></span><br><span class="line">print(df2)</span><br><span class="line"><span class="comment">#   col1  col_right</span></span><br><span class="line"><span class="comment"># 0     1          2</span></span><br><span class="line"><span class="comment"># 1     2          2</span></span><br><span class="line"><span class="comment"># 2     2          2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 依据col1进行合并，并启用indicator=True，最后打印出</span></span><br><span class="line">res = pd.merge(df1, df2, on=<span class="string">'col1'</span>, how=<span class="string">'outer'</span>, indicator=<span class="keyword">True</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#   col1 col_left  col_right      _merge</span></span><br><span class="line"><span class="comment"># 0   0.0        a        NaN   left_only</span></span><br><span class="line"><span class="comment"># 1   1.0        b        2.0        both</span></span><br><span class="line"><span class="comment"># 2   2.0      NaN        2.0  right_only</span></span><br><span class="line"><span class="comment"># 3   2.0      NaN        2.0  right_only</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定indicator column的名称，并打印出</span></span><br><span class="line">res = pd.merge(df1, df2, on=<span class="string">'col1'</span>, how=<span class="string">'outer'</span>, indicator=<span class="string">'indicator_column'</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#   col1 col_left  col_right indicator_column</span></span><br><span class="line"><span class="comment"># 0   0.0        a        NaN        left_only</span></span><br><span class="line"><span class="comment"># 1   1.0        b        2.0             both</span></span><br><span class="line"><span class="comment"># 2   2.0      NaN        2.0       right_only</span></span><br><span class="line"><span class="comment"># 3   2.0      NaN        2.0       right_only</span></span><br></pre></td></tr></table></figure>
<pre><code>   col1 col_left
0     0        a
1     1        b
   col1  col_right
0     1          2
1     2          2
2     2          2
   col1 col_left  col_right      _merge
0     0        a        NaN   left_only
1     1        b        2.0        both
2     2      NaN        2.0  right_only
3     2      NaN        2.0  right_only
   col1 col_left  col_right indicator_column
0     0        a        NaN        left_only
1     1        b        2.0             both
2     2      NaN        2.0       right_only
3     2      NaN        2.0       right_only
</code></pre><h2 id="依据index合并"><a href="#依据index合并" class="headerlink" title="依据index合并"></a>依据index合并</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集并打印出</span></span><br><span class="line">left = pd.DataFrame(&#123;<span class="string">'A'</span>: [<span class="string">'A0'</span>, <span class="string">'A1'</span>, <span class="string">'A2'</span>],</span><br><span class="line">                     <span class="string">'B'</span>: [<span class="string">'B0'</span>, <span class="string">'B1'</span>, <span class="string">'B2'</span>]&#125;,</span><br><span class="line">                     index=[<span class="string">'K0'</span>, <span class="string">'K1'</span>, <span class="string">'K2'</span>])</span><br><span class="line">right = pd.DataFrame(&#123;<span class="string">'C'</span>: [<span class="string">'C0'</span>, <span class="string">'C2'</span>, <span class="string">'C3'</span>],</span><br><span class="line">                      <span class="string">'D'</span>: [<span class="string">'D0'</span>, <span class="string">'D2'</span>, <span class="string">'D3'</span>]&#125;,</span><br><span class="line">                     index=[<span class="string">'K0'</span>, <span class="string">'K2'</span>, <span class="string">'K3'</span>])</span><br><span class="line"></span><br><span class="line">print(left)</span><br><span class="line"><span class="comment">#     A   B</span></span><br><span class="line"><span class="comment"># K0  A0  B0</span></span><br><span class="line"><span class="comment"># K1  A1  B1</span></span><br><span class="line"><span class="comment"># K2  A2  B2</span></span><br><span class="line"></span><br><span class="line">print(right)</span><br><span class="line"><span class="comment">#     C   D</span></span><br><span class="line"><span class="comment"># K0  C0  D0</span></span><br><span class="line"><span class="comment"># K2  C2  D2</span></span><br><span class="line"><span class="comment"># K3  C3  D3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#依据左右资料集的index进行合并，how='outer',并打印出</span></span><br><span class="line">res = pd.merge(left, right, left_index=<span class="keyword">True</span>, right_index=<span class="keyword">True</span>, how=<span class="string">'outer'</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#      A    B    C    D</span></span><br><span class="line"><span class="comment"># K0   A0   B0   C0   D0</span></span><br><span class="line"><span class="comment"># K1   A1   B1  NaN  NaN</span></span><br><span class="line"><span class="comment"># K2   A2   B2   C2   D2</span></span><br><span class="line"><span class="comment"># K3  NaN  NaN   C3   D3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#依据左右资料集的index进行合并，how='inner',并打印出</span></span><br><span class="line">res = pd.merge(left, right, left_index=<span class="keyword">True</span>, right_index=<span class="keyword">True</span>, how=<span class="string">'inner'</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#     A   B   C   D</span></span><br><span class="line"><span class="comment"># K0  A0  B0  C0  D0</span></span><br><span class="line"><span class="comment"># K2  A2  B2  C2  D2</span></span><br></pre></td></tr></table></figure>
<pre><code>     A   B
K0  A0  B0
K1  A1  B1
K2  A2  B2
     C   D
K0  C0  D0
K2  C2  D2
K3  C3  D3
      A    B    C    D
K0   A0   B0   C0   D0
K1   A1   B1  NaN  NaN
K2   A2   B2   C2   D2
K3  NaN  NaN   C3   D3
     A   B   C   D
K0  A0  B0  C0  D0
K2  A2  B2  C2  D2
</code></pre><h2 id="解决overlapping的问题"><a href="#解决overlapping的问题" class="headerlink" title="解决overlapping的问题"></a>解决overlapping的问题</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集</span></span><br><span class="line">boys = pd.DataFrame(&#123;<span class="string">'k'</span>: [<span class="string">'K0'</span>, <span class="string">'K1'</span>, <span class="string">'K2'</span>], <span class="string">'age'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</span><br><span class="line">girls = pd.DataFrame(&#123;<span class="string">'k'</span>: [<span class="string">'K0'</span>, <span class="string">'K0'</span>, <span class="string">'K3'</span>], <span class="string">'age'</span>: [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]&#125;)</span><br><span class="line"></span><br><span class="line">print(boys)</span><br><span class="line"></span><br><span class="line">print(girls)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用suffixes解决overlapping的问题</span></span><br><span class="line">res = pd.merge(boys, girls, on=<span class="string">'k'</span>, suffixes=[<span class="string">'_boy'</span>, <span class="string">'_girl'</span>], how=<span class="string">'inner'</span>)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<pre><code>   age   k
0    1  K0
1    2  K1
2    3  K2
   age   k
0    4  K0
1    5  K0
2    6  K3
   age_boy   k  age_girl
0        1  K0         4
1        1  K0         5
</code></pre><p>pandas 也有 <code>join</code> 和 <code>merge</code> 是类似的，如需要使用，请参考官方文档</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="external">pandas.pydata.org</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/version/0.21/" target="_blank" rel="external">pandas docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pandas Concat、Join、join_axes、append]]></title>
      <url>http://sggo.me/2017/12/31/python/py-pandas-6-concat-join-append/</url>
      <content type="html"><![CDATA[<p><code>pandas</code> 处理多组数据的时候往往会要用到数据的合并处理,使用 <code>concat</code> 是一种基本的合并方式.而且 <code>concat</code> 中有很多参数可以调整,合并成你想要的数据形式.</p>
<a id="more"></a>
<h2 id="axis-合并方向"><a href="#axis-合并方向" class="headerlink" title="axis (合并方向)"></a>axis (合并方向)</h2><p><code>axis=0</code> 是预设值，因此未设定任何参数时，默认axis=0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集</span></span><br><span class="line">df1 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>, columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">df2 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>, columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">df3 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">2</span>, columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#concat纵向合并</span></span><br><span class="line">res = pd.concat([df1, df2, df3], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印结果</span></span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<pre><code>     a    b    c    d
0  0.0  0.0  0.0  0.0
1  0.0  0.0  0.0  0.0
2  0.0  0.0  0.0  0.0
0  1.0  1.0  1.0  1.0
1  1.0  1.0  1.0  1.0
2  1.0  1.0  1.0  1.0
0  2.0  2.0  2.0  2.0
1  2.0  2.0  2.0  2.0
2  2.0  2.0  2.0  2.0
</code></pre><p>仔细观察会发现结果的<code>index</code>是0, 1, 2, 0, 1, 2, 0, 1, 2，若要将<code>index</code>重置，请看例子二。</p>
<h2 id="ignore-index-重置-index"><a href="#ignore-index-重置-index" class="headerlink" title="ignore_index (重置 index)"></a>ignore_index (重置 index)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#承上一个例子，并将index_ignore设定为True</span></span><br><span class="line">res = pd.concat([df1, df2, df3], axis=<span class="number">0</span>, ignore_index=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印结果</span></span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<pre><code>     a    b    c    d
0  0.0  0.0  0.0  0.0
1  0.0  0.0  0.0  0.0
2  0.0  0.0  0.0  0.0
3  1.0  1.0  1.0  1.0
4  1.0  1.0  1.0  1.0
5  1.0  1.0  1.0  1.0
6  2.0  2.0  2.0  2.0
7  2.0  2.0  2.0  2.0
8  2.0  2.0  2.0  2.0
</code></pre><p>结果的<code>index</code>变 0, 1, 2, 3, 4, 5, 6, 7, 8</p>
<h2 id="join-合并方式"><a href="#join-合并方式" class="headerlink" title="join (合并方式)"></a>join (合并方式)</h2><p><code>join=&#39;outer&#39;</code> 为预设值，未设定任何参数时，函数默认 <code>join=&#39;outer&#39;</code>。此方式是依照<code>column</code>来做纵向合并，有相同的<code>column</code>上下合并在一起，其他独自的<code>column</code>个自成列，原本没有值的位置皆以<code>NaN</code>填充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集</span></span><br><span class="line">df1 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>, columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>], index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">df2 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>, columns=[<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>], index=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#纵向"外"合并df1与df2</span></span><br><span class="line">res = pd.concat([df1, df2], axis=<span class="number">0</span>, join=<span class="string">'outer'</span>)</span><br><span class="line"></span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<pre><code>     a    b    c    d    e
1  0.0  0.0  0.0  0.0  NaN
2  0.0  0.0  0.0  0.0  NaN
3  0.0  0.0  0.0  0.0  NaN
2  NaN  1.0  1.0  1.0  1.0
3  NaN  1.0  1.0  1.0  1.0
4  NaN  1.0  1.0  1.0  1.0
</code></pre><p>只有相同的<code>column</code>合并在一起，其他的会被抛弃</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#承上一个例子</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#纵向"内"合并df1与df2</span></span><br><span class="line">res = pd.concat([df1, df2], axis=<span class="number">0</span>, join=<span class="string">'inner'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#重置index并打印结果</span></span><br><span class="line">res = pd.concat([df1, df2], axis=<span class="number">0</span>, join=<span class="string">'inner'</span>, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<pre><code>     b    c    d
0  0.0  0.0  0.0
1  0.0  0.0  0.0
2  0.0  0.0  0.0
3  1.0  1.0  1.0
4  1.0  1.0  1.0
5  1.0  1.0  1.0
</code></pre><h2 id="join-axes-依照-axes-合并"><a href="#join-axes-依照-axes-合并" class="headerlink" title="join_axes (依照 axes 合并)"></a>join_axes (依照 axes 合并)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集</span></span><br><span class="line">df1 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>, columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>], index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">df2 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>, columns=[<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>], index=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#依照`df1.index`进行横向合并</span></span><br><span class="line">res = pd.concat([df1, df2], axis=<span class="number">1</span>, join_axes=[df1.index])</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印结果</span></span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#     a    b    c    d    b    c    d    e</span></span><br><span class="line"><span class="comment"># 1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN</span></span><br><span class="line"><span class="comment"># 2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0</span></span><br><span class="line"><span class="comment"># 3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#移除join_axes，并打印结果</span></span><br><span class="line">res = pd.concat([df1, df2], axis=<span class="number">1</span>)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<pre><code>     a    b    c    d    b    c    d    e
1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN
2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0
3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0
     a    b    c    d    b    c    d    e
1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN
2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0
3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0
4  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0
</code></pre><h2 id="append-添加数据"><a href="#append-添加数据" class="headerlink" title="append (添加数据)"></a>append (添加数据)</h2><p><code>append</code>只有纵向合并，没有横向合并</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义资料集</span></span><br><span class="line">df1 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>, columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">df2 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>, columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">df3 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>, columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">s1 = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], index=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将df2合并到df1的下面，以及重置index，并打印出结果</span></span><br><span class="line">res = df1.append(df2, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#     a    b    c    d</span></span><br><span class="line"><span class="comment"># 0  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 1  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 2  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 3  1.0  1.0  1.0  1.0</span></span><br><span class="line"><span class="comment"># 4  1.0  1.0  1.0  1.0</span></span><br><span class="line"><span class="comment"># 5  1.0  1.0  1.0  1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#合并多个df，将df2与df3合并至df1的下面，以及重置index，并打印出结果</span></span><br><span class="line">res = df1.append([df2, df3], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#     a    b    c    d</span></span><br><span class="line"><span class="comment"># 0  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 1  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 2  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 3  1.0  1.0  1.0  1.0</span></span><br><span class="line"><span class="comment"># 4  1.0  1.0  1.0  1.0</span></span><br><span class="line"><span class="comment"># 5  1.0  1.0  1.0  1.0</span></span><br><span class="line"><span class="comment"># 6  1.0  1.0  1.0  1.0</span></span><br><span class="line"><span class="comment"># 7  1.0  1.0  1.0  1.0</span></span><br><span class="line"><span class="comment"># 8  1.0  1.0  1.0  1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#合并series，将s1合并至df1，以及重置index，并打印出结果</span></span><br><span class="line">res = df1.append(s1, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#     a    b    c    d</span></span><br><span class="line"><span class="comment"># 0  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 1  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 2  0.0  0.0  0.0  0.0</span></span><br><span class="line"><span class="comment"># 3  1.0  2.0  3.0  4.0</span></span><br></pre></td></tr></table></figure>
<pre><code>     a    b    c    d
0  0.0  0.0  0.0  0.0
1  0.0  0.0  0.0  0.0
2  0.0  0.0  0.0  0.0
3  1.0  1.0  1.0  1.0
4  1.0  1.0  1.0  1.0
5  1.0  1.0  1.0  1.0
     a    b    c    d
0  0.0  0.0  0.0  0.0
1  0.0  0.0  0.0  0.0
2  0.0  0.0  0.0  0.0
3  1.0  1.0  1.0  1.0
4  1.0  1.0  1.0  1.0
5  1.0  1.0  1.0  1.0
6  1.0  1.0  1.0  1.0
7  1.0  1.0  1.0  1.0
8  1.0  1.0  1.0  1.0
     a    b    c    d
0  0.0  0.0  0.0  0.0
1  0.0  0.0  0.0  0.0
2  0.0  0.0  0.0  0.0
3  1.0  2.0  3.0  4.0
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="external">pandas.pydata.org</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/version/0.21/" target="_blank" rel="external">pandas docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pandas IO]]></title>
      <url>http://sggo.me/2017/12/30/python/py-pandas-5-import-output/</url>
      <content type="html"><![CDATA[<p><code>pandas</code> 可以读取与存取的资料格式有很多种，像 <code>csv</code>、<code>excel</code>、<code>json</code>、<code>html</code> 与 <code>pickle</code> 等…</p>
<p>详细请看<a href="http://pandas.pydata.org/pandas-docs/stable/io.html" target="_blank" rel="external">官方文档</a></p>
<a id="more"></a>
<h2 id="读取csv"><a href="#读取csv" class="headerlink" title="读取csv"></a>读取csv</h2><p>示范档案下载 - student.csv</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#加载模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取csv</span></span><br><span class="line">data = pd.read_csv(<span class="string">'students.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印出data</span></span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>
<pre><code>    Student ID  name   age  gender
0         1100  Kelly   22  Female
1         1101    Clo   21  Female
2         1102  Tilly   22  Female
3         1103   Tony   24    Male
4         1104  David   20    Male
5         1105  Catty   22  Female
6         1106      M    3  Female
7         1107      N   43    Male
8         1108      A   13    Male
9         1109      S   12    Male
10        1110  David   33    Male
11        1111     Dw    3  Female
12        1112      Q   23    Male
13        1113      W   21  Female
</code></pre><h2 id="将资料存取成pickle"><a href="#将资料存取成pickle" class="headerlink" title="将资料存取成pickle"></a>将资料存取成pickle</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.to_pickle(<span class="string">'student.pickle'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="external">pandas.pydata.org</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/version/0.21/" target="_blank" rel="external">pandas docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/io.html" target="_blank" rel="external">pandas IO Tools</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pandas Deal NaN Value]]></title>
      <url>http://sggo.me/2017/12/30/python/py-pandas-4-deal-NaN-value/</url>
      <content type="html"><![CDATA[<p>处理 <code>NaN</code> 数据, 一些 空 或者 <code>NaN</code> 数据, 如何删除或者填补这些 <code>NaN</code> 数据.</p>
<a id="more"></a>
<h2 id="创建含-NaN-的矩阵"><a href="#创建含-NaN-的矩阵" class="headerlink" title="创建含 NaN 的矩阵"></a>创建含 NaN 的矩阵</h2><p>建立了一个6X4的矩阵数据并且把两个位置置为空.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>, periods=<span class="number">6</span>)</span><br><span class="line">df = pd.DataFrame(np.arange(<span class="number">24</span>).reshape((<span class="number">6</span>,<span class="number">4</span>)),index=dates, columns=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>])</span><br><span class="line">df.iloc[<span class="number">0</span>,<span class="number">1</span>] = np.nan</span><br><span class="line">df.iloc[<span class="number">1</span>,<span class="number">2</span>] = np.nan</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<pre><code>             A     B     C   D
2013-01-01   0   NaN   2.0   3
2013-01-02   4   5.0   NaN   7
2013-01-03   8   9.0  10.0  11
2013-01-04  12  13.0  14.0  15
2013-01-05  16  17.0  18.0  19
2013-01-06  20  21.0  22.0  23
</code></pre><h2 id="pd-dropna"><a href="#pd-dropna" class="headerlink" title="pd.dropna()"></a>pd.dropna()</h2><p>如果想直接去掉有 <code>NaN</code> 的行或列, 可以使用 <code>dropna</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = df.dropna(</span><br><span class="line">    axis=<span class="number">0</span>,     <span class="comment"># 0: 对行进行操作; 1: 对列进行操作</span></span><br><span class="line">    how=<span class="string">'any'</span>   <span class="comment"># 'any': 只要存在 NaN 就 drop 掉; 'all': 必须全部是 NaN 才 drop </span></span><br><span class="line">    ) </span><br><span class="line">print(df1)</span><br></pre></td></tr></table></figure>
<pre><code>             A     B     C   D
2013-01-03   8   9.0  10.0  11
2013-01-04  12  13.0  14.0  15
2013-01-05  16  17.0  18.0  19
2013-01-06  20  21.0  22.0  23
</code></pre><h2 id="pd-fillna"><a href="#pd-fillna" class="headerlink" title="pd.fillna()"></a>pd.fillna()</h2><p>如果是将 <code>NaN</code> 的值用其他值代替, 比如代替成 <code>0</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.fillna(value=<span class="number">0</span>)</span><br><span class="line">print(df2)</span><br></pre></td></tr></table></figure>
<pre><code>             A     B     C   D
2013-01-01   0   0.0   2.0   3
2013-01-02   4   5.0   0.0   7
2013-01-03   8   9.0  10.0  11
2013-01-04  12  13.0  14.0  15
2013-01-05  16  17.0  18.0  19
2013-01-06  20  21.0  22.0  23
</code></pre><h2 id="pd-isnull"><a href="#pd-isnull" class="headerlink" title="pd.isnull()"></a>pd.isnull()</h2><p>判断是否有缺失数据 <code>NaN</code>, 为 <code>True</code> 表示缺失数据:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df.isnull())</span><br></pre></td></tr></table></figure>
<pre><code>                A      B      C      D
2013-01-01  False   True  False  False
2013-01-02  False  False   True  False
2013-01-03  False  False  False  False
2013-01-04  False  False  False  False
2013-01-05  False  False  False  False
2013-01-06  False  False  False  False
</code></pre><p>检测在数据中是否存在 <code>NaN</code>, 如果存在就返回 <code>True</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.any(df.isnull()) == <span class="keyword">True</span>  </span><br><span class="line"><span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="external">pandas.pydata.org</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/version/0.21/" target="_blank" rel="external">pandas docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pandas Set Value]]></title>
      <url>http://sggo.me/2017/12/30/python/py-pandas-3-set-value/</url>
      <content type="html"><![CDATA[<p>我们可以根据自己的需求, 用 pandas 进行更改数据里面的值, 或者加上一些空的,或者有数值的列.</p>
<a id="more"></a>
<h2 id="创建数据"><a href="#创建数据" class="headerlink" title="创建数据"></a>创建数据</h2><p>首先建立了一个 6X4 的矩阵数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>, periods=<span class="number">6</span>) <span class="comment"># 2013-01-01 结果一样</span></span><br><span class="line">df = pd.DataFrame(np.arange(<span class="number">24</span>).reshape((<span class="number">6</span>,<span class="number">4</span>)),index=dates, columns=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>])</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<pre><code>             A   B   C   D
2013-01-01   0   1   2   3
2013-01-02   4   5   6   7
2013-01-03   8   9  10  11
2013-01-04  12  13  14  15
2013-01-05  16  17  18  19
2013-01-06  20  21  22  23
</code></pre><h2 id="设置-loc-和-iloc"><a href="#设置-loc-和-iloc" class="headerlink" title="设置 loc 和 iloc"></a>设置 loc 和 iloc</h2><p>我们可以利用索引或者标签确定需要修改值的位置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.iloc[<span class="number">2</span>,<span class="number">2</span>] = <span class="number">1111</span></span><br><span class="line">df.loc[<span class="string">'2013-01-01'</span>,<span class="string">'B'</span>] = <span class="number">2222</span></span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<pre><code>             A     B     C   D
2013-01-01   0  2222     2   3
2013-01-02   4     5     6   7
2013-01-03   8     0  1111  11
2013-01-04  12     0    14  15
2013-01-05  16     0    18  19
2013-01-06  20     0    22  23
</code></pre><h2 id="根据条件设置"><a href="#根据条件设置" class="headerlink" title="根据条件设置"></a>根据条件设置</h2><p>如果现在的判断条件是这样, 我们想要更改B中的数, 而更改的位置是取决于 A 的. 对于A大于4的位置. 更改B在相应位置上的数为0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.B[df.A&gt;<span class="number">4</span>] = <span class="number">0</span></span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<pre><code>             A     B     C   D
2013-01-01   0  2222     2   3
2013-01-02   4     5     6   7
2013-01-03   8     0  1111  11
2013-01-04  12     0    14  15
2013-01-05  16     0    18  19
2013-01-06  20     0    22  23
</code></pre><h2 id="按行或列设置"><a href="#按行或列设置" class="headerlink" title="按行或列设置"></a>按行或列设置</h2><p>如果对整列做批处理, 加上一列 ‘F’, 并将 F 列全改为 NaN, 如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'F'</span>] = np.nan</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<pre><code>             A     B     C   D   F
2013-01-01   0  2222     2   3 NaN
2013-01-02   4     5     6   7 NaN
2013-01-03   8     0  1111  11 NaN
2013-01-04  12     0    14  15 NaN
2013-01-05  16     0    18  19 NaN
2013-01-06  20     0    22  23 NaN
</code></pre><h2 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h2><p>用上面的方法也可以加上 Series 序列（但是长度必须对齐）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'E'</span>] = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], index=pd.date_range(<span class="string">'20130101'</span>,periods=<span class="number">6</span>)) </span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<pre><code>             A     B     C   D   F  E
2013-01-01   0  2222     2   3 NaN  1
2013-01-02   4     5     6   7 NaN  2
2013-01-03   8     0  1111  11 NaN  3
2013-01-04  12     0    14  15 NaN  4
2013-01-05  16     0    18  19 NaN  5
2013-01-06  20     0    22  23 NaN  6
</code></pre><p>这样我们大概学会了如何对 <code>DataFrame</code> 中在自己想要的地方赋值或者增加数据。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="external">pandas.pydata.org</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/version/0.21/" target="_blank" rel="external">pandas docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pandas Select Data]]></title>
      <url>http://sggo.me/2017/12/28/python/py-pandas-2-select-data/</url>
      <content type="html"><![CDATA[<p>pandas 选择数据, 首先我们建立了一个 6X4 的矩阵数据</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>, periods=<span class="number">6</span>)</span><br><span class="line">df = pd.DataFrame(np.arange(<span class="number">24</span>).reshape((<span class="number">6</span>,<span class="number">4</span>)),index=dates, columns=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>])</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<pre><code>             A   B   C   D
2013-01-01   0   1   2   3
2013-01-02   4   5   6   7
2013-01-03   8   9  10  11
2013-01-04  12  13  14  15
2013-01-05  16  17  18  19
2013-01-06  20  21  22  23
</code></pre><h2 id="简单的筛选"><a href="#简单的筛选" class="headerlink" title="简单的筛选"></a>简单的筛选</h2><p>如果我们想选取 DataFrame 中的数据，下面描述了两种途径, 他们都能达到同一个目的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df[<span class="string">'A'</span>])</span><br><span class="line">print(df.A)</span><br></pre></td></tr></table></figure>
<pre><code>2013-01-01     0
2013-01-02     4
2013-01-03     8
2013-01-04    12
2013-01-05    16
2013-01-06    20
Freq: D, Name: A, dtype: int64

2013-01-01     0
2013-01-02     4
2013-01-03     8
2013-01-04    12
2013-01-05    16
2013-01-06    20
Freq: D, Name: A, dtype: int64
</code></pre><p>让选择跨越多行或多列:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df[<span class="number">0</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>            A  B   C   D
2013-01-01  0  1   2   3
2013-01-02  4  5   6   7
2013-01-03  8  9  10  11
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df[<span class="number">3</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Empty DataFrame
Columns: [A, B, C, D]
Index: []
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df[<span class="string">'20130102'</span>:<span class="string">'20130104'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>             A   B   C   D
2013-01-02   4   5   6   7
2013-01-03   8   9  10  11
2013-01-04  12  13  14  15
</code></pre><p>如果 <code>df[3:3]</code> 将会是一个空对象。后者选择 <code>20130102</code> 到 <code>20130104</code> 标签之间的数据，并且包括这两个标签</p>
<h2 id="根据标签-loc"><a href="#根据标签-loc" class="headerlink" title="根据标签 loc"></a>根据标签 loc</h2><p>可以使用标签来选择数据 <code>loc</code>, 本例子主要通过标签名字选择某一行数据， 或者通过选择某行或者所有行（<code>:</code>代表所有行）然后选其中某一列或几列数据 :</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df.loc[<span class="string">'20130102'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>A    4
B    5
C    6
D    7
Name: 2013-01-02 00:00:00, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df.loc[:,[<span class="string">'A'</span>,<span class="string">'B'</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>             A   B
2013-01-01   0   1
2013-01-02   4   5
2013-01-03   8   9
2013-01-04  12  13
2013-01-05  16  17
2013-01-06  20  21
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df.loc[<span class="string">'20130102'</span>,[<span class="string">'A'</span>,<span class="string">'B'</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>A    4
B    5
Name: 2013-01-02 00:00:00, dtype: int64
</code></pre><h2 id="根据序列-iloc"><a href="#根据序列-iloc" class="headerlink" title="根据序列 iloc"></a>根据序列 iloc</h2><p>可以采用位置进行选择 <code>iloc</code>, 在这里我们可以通过位置选择在不同情况下所需要的数据例如选某一个，连续选或者跨行选等操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df.iloc[<span class="number">3</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>13
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df.iloc[<span class="number">3</span>:<span class="number">5</span>,<span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>             B   C
2013-01-04  13  14
2013-01-05  17  18
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df.iloc[[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>],<span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>             B   C
2013-01-02   5   6
2013-01-04  13  14
2013-01-06  21  22
</code></pre><p>在这里我们可以通过位置选择在不同情况下所需要的数据, 例如选某一个，连续选或者跨行选等操作。</p>
<h2 id="根据混合两种-ix"><a href="#根据混合两种-ix" class="headerlink" title="根据混合两种 ix"></a>根据混合两种 ix</h2><p>当然我们可以采用混合选择 <code>ix</code>, 其中选择’A’和’C’的两列，并选择前三行的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df.ix[:<span class="number">3</span>,[<span class="string">'A'</span>,<span class="string">'C'</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>            A   C
2013-01-01  0   2
2013-01-02  4   6
2013-01-03  8  10


/Users/blair/.pyenv/versions/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: 
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing

See the documentation here:
http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated
  &quot;&quot;&quot;Entry point for launching an IPython kernel.
</code></pre><h2 id="通过判断的筛选"><a href="#通过判断的筛选" class="headerlink" title="通过判断的筛选"></a>通过判断的筛选</h2><p>最后我们可以采用判断指令 (Boolean indexing) 进行选择. 我们可以约束某项条件然后选择出当前所有数据.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df[df.A&gt;<span class="number">8</span>])</span><br><span class="line">df.A&gt;<span class="number">8</span></span><br></pre></td></tr></table></figure>
<pre><code>             A   B   C   D
2013-01-04  12  13  14  15
2013-01-05  16  17  18  19
2013-01-06  20  21  22  23

2013-01-01    False
2013-01-02    False
2013-01-03    False
2013-01-04     True
2013-01-05     True
2013-01-06     True
Freq: D, Name: A, dtype: bool
</code></pre><p>下节我们将会讲到Pandas中如何设置值。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="external">pandas.pydata.org</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/version/0.21/" target="_blank" rel="external">pandas docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pandas Basic Intro]]></title>
      <url>http://sggo.me/2017/12/27/python/py-pandas-1-intro/</url>
      <content type="html"><![CDATA[<p>如果用列表和字典来作比较, 那么可以说 Numpy 是列表形式的，没有数值标签，而 Pandas 就是字典形式</p>
<a id="more"></a>
<p>Pandas是基于Numpy构建的，让Numpy为中心的应用变得更加简单。</p>
<p>要使用pandas，首先需要了解他主要两个数据结构：Series 和 DataFrame。</p>
<h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">s = pd.Series([<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>,np.nan,<span class="number">44</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">print(s)</span><br></pre></td></tr></table></figure>
<pre><code>0     1.0
1     3.0
2     6.0
3     NaN
4    44.0
5     1.0
dtype: float64
</code></pre><p><code>Series</code> 的字符串表现形式为：索引在左边，值在右边。<br>由于我们没有为数据指定索引。于是会自动创建一个0到N-1（N为长度）的整数型索引。</p>
<h2 id="DataFrame-矩阵创建"><a href="#DataFrame-矩阵创建" class="headerlink" title="DataFrame 矩阵创建"></a>DataFrame 矩阵创建</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dates = pd.date_range(<span class="string">'20160101'</span>,periods=<span class="number">6</span>)</span><br><span class="line">df = pd.DataFrame(np.random.randn(<span class="number">6</span>,<span class="number">4</span>),index=dates,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line"></span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
<pre><code>                   a         b         c         d
2016-01-01 -0.186992  0.228857  0.572464 -0.842974
2016-01-02 -0.689623 -1.491299  0.647805  0.819846
2016-01-03 -1.294425  0.138935  1.729793 -1.270880
2016-01-04  0.088744  0.745256  0.380425  0.048070
2016-01-05  0.003135 -2.240388  0.188038 -0.069044
2016-01-06 -1.358217 -0.820133  1.606467 -1.622589
</code></pre><p><code>DataFrame</code> 是一个表格型的数据结构，它包含有一组有序的列，每列可以是不同的值类型（数值，字符串，布尔值等）。<br><code>DataFrame</code> 既有行索引也有列索引， 它可以被看做由 <code>Series</code> 组成的大字典。</p>
<p>我们可以根据每一个不同的索引来挑选数据, 比如挑选 <code>b</code> 的元素:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df[<span class="string">'b'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>2016-01-01    0.228857
2016-01-02   -1.491299
2016-01-03    0.138935
2016-01-04    0.745256
2016-01-05   -2.240388
2016-01-06   -0.820133
Freq: D, Name: b, dtype: float64
</code></pre><p>我们在创建一组没有给定行标签和列标签的数据 <code>df1</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = pd.DataFrame(np.arange(<span class="number">12</span>).reshape((<span class="number">3</span>,<span class="number">4</span>)))</span><br><span class="line">print(df1)</span><br></pre></td></tr></table></figure>
<pre><code>   0  1   2   3
0  0  1   2   3
1  4  5   6   7
2  8  9  10  11
</code></pre><p>这样,他就会采取默认的从0开始 index.</p>
<h2 id="DataFrame-字典创建"><a href="#DataFrame-字典创建" class="headerlink" title="DataFrame 字典创建"></a>DataFrame 字典创建</h2><p>还有一种生成 <code>df</code> 的方法, 如下 <code>df2</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'A'</span> : [<span class="number">1</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>],</span><br><span class="line">                    <span class="string">'B'</span> : pd.Timestamp(<span class="string">'20130102'</span>),</span><br><span class="line">                    <span class="string">'C'</span> : pd.Series(<span class="number">1</span>,index=list(range(<span class="number">4</span>)),dtype=<span class="string">'float32'</span>),</span><br><span class="line">                    <span class="string">'D'</span> : np.array([<span class="number">3</span>] * <span class="number">4</span>,dtype=<span class="string">'int32'</span>),</span><br><span class="line">                    <span class="string">'E'</span> : pd.Categorical([<span class="string">"test"</span>,<span class="string">"train"</span>,<span class="string">"test"</span>,<span class="string">"train"</span>]),</span><br><span class="line">                    <span class="string">'F'</span> : <span class="string">'foo'</span>&#125;)</span><br><span class="line">                    </span><br><span class="line">print(df2)</span><br></pre></td></tr></table></figure>
<pre><code>   A          B    C  D      E    F
0  1 2013-01-02  1.0  3   test  foo
1  3 2013-01-02  1.0  3  train  foo
2  7 2013-01-02  1.0  3   test  foo
3  5 2013-01-02  1.0  3  train  foo
</code></pre><p>这种方法能对每一列的数据进行特殊对待. 如果想要查看数据中的类型, 我们可以用 <code>dtype</code> 这个属性:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df2.dtypes)</span><br></pre></td></tr></table></figure>
<pre><code>A             int64
B    datetime64[ns]
C           float32
D             int32
E          category
F            object
dtype: object
</code></pre><p>如果想看对列的序号:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df2.index)</span><br></pre></td></tr></table></figure>
<pre><code>Int64Index([0, 1, 2, 3], dtype=&apos;int64&apos;)
</code></pre><p>同样, 每种数据的名称也能看到:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df2.columns)</span><br></pre></td></tr></table></figure>
<pre><code>Index([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;, &apos;E&apos;, &apos;F&apos;], dtype=&apos;object&apos;)
</code></pre><p>如果只想看所有 <code>df2</code> 的值:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df2.values)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 Timestamp(&apos;2013-01-02 00:00:00&apos;) 1.0 3 &apos;test&apos; &apos;foo&apos;]
 [3 Timestamp(&apos;2013-01-02 00:00:00&apos;) 1.0 3 &apos;train&apos; &apos;foo&apos;]
 [7 Timestamp(&apos;2013-01-02 00:00:00&apos;) 1.0 3 &apos;test&apos; &apos;foo&apos;]
 [5 Timestamp(&apos;2013-01-02 00:00:00&apos;) 1.0 3 &apos;train&apos; &apos;foo&apos;]]
</code></pre><p>想知道数据的总结, 可以用 <code>describe()</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2.describe()</span><br></pre></td></tr></table></figure>
<pre><code>    A    C    D
count    4.000000    4.0    4.0
mean    4.000000    1.0    3.0
std    2.581989    0.0    0.0
min    1.000000    1.0    3.0
25%    2.500000    1.0    3.0
50%    4.000000    1.0    3.0
75%    5.500000    1.0    3.0
max    7.000000    1.0    3.0
</code></pre><p>如果想翻转数据, <code>transpose</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df2.T)</span><br></pre></td></tr></table></figure>
<pre><code>                     0                    1                    2  \
A                    1                    3                    7   
B  2013-01-02 00:00:00  2013-01-02 00:00:00  2013-01-02 00:00:00   
C                    1                    1                    1   
D                    3                    3                    3   
E                 test                train                 test   
F                  foo                  foo                  foo   

                     3  
A                    5  
B  2013-01-02 00:00:00  
C                    1  
D                    3  
E                train  
F                  foo  
</code></pre><p>如果想对数据的 <code>index</code> 进行排序并输出:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df2.sort_index(axis=<span class="number">1</span>, ascending=<span class="keyword">False</span>)) <span class="comment"># 对列名称进行排序，索引名，倒排序</span></span><br><span class="line"></span><br><span class="line">print(df2.sort_index(axis=<span class="number">0</span>, ascending=<span class="keyword">False</span>)) <span class="comment"># 对列名称进行排序，索引名，倒排序</span></span><br></pre></td></tr></table></figure>
<pre><code>     F      E  D    C          B  A
0  foo   test  3  1.0 2013-01-02  1
1  foo  train  3  1.0 2013-01-02  3
2  foo   test  3  1.0 2013-01-02  7
3  foo  train  3  1.0 2013-01-02  5
   A          B    C  D      E    F
3  5 2013-01-02  1.0  3  train  foo
2  7 2013-01-02  1.0  3   test  foo
1  3 2013-01-02  1.0  3  train  foo
0  1 2013-01-02  1.0  3   test  foo
</code></pre><p>如果是对数据 值 排序输出:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(df2.sort_values(by=<span class="string">'E'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>   A          B    C  D      E    F
0  1 2013-01-02  1.0  3   test  foo
2  7 2013-01-02  1.0  3   test  foo
1  3 2013-01-02  1.0  3  train  foo
3  5 2013-01-02  1.0  3  train  foo
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://pandas.pydata.org/" target="_blank" rel="external">pandas.pydata.org</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/version/0.21/" target="_blank" rel="external">pandas docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Numpy copy & deep copy]]></title>
      <url>http://sggo.me/2017/12/27/python/py-numpy-8-copy-deep-copy/</url>
      <content type="html"><![CDATA[<p>numpy copy &amp; deep copy</p>
<a id="more"></a>
<h2 id="的赋值方式会带有关联性"><a href="#的赋值方式会带有关联性" class="headerlink" title="= 的赋值方式会带有关联性"></a>= 的赋值方式会带有关联性</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.arange(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># array([0, 1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">b = a</span><br><span class="line">c = a</span><br><span class="line">d = b</span><br></pre></td></tr></table></figure>
<p>改变<code>a</code>的第一个值，<code>b</code>、<code>c</code>、<code>d</code>的第一个值也会同时改变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a[<span class="number">0</span>] = <span class="number">11</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># array([11,  1,  2,  3])</span></span><br></pre></td></tr></table></figure>
<pre><code>[11  1  2  3]
</code></pre><p>确认<code>b</code>、<code>c</code>、<code>d</code>是否与<code>a</code>相同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b <span class="keyword">is</span> a  <span class="comment"># True</span></span><br><span class="line">c <span class="keyword">is</span> a  <span class="comment"># True</span></span><br><span class="line">d <span class="keyword">is</span> a  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<h2 id="copy-的赋值方式没有关联性"><a href="#copy-的赋值方式没有关联性" class="headerlink" title="copy() 的赋值方式没有关联性"></a>copy() 的赋值方式没有关联性</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = a.copy()    <span class="comment"># deep copy</span></span><br><span class="line">print(b)        <span class="comment"># array([11, 22, 33,  3])</span></span><br><span class="line">a[<span class="number">3</span>] = <span class="number">44</span></span><br><span class="line">print(a)        <span class="comment"># array([11, 22, 33, 44])</span></span><br><span class="line">print(b)        <span class="comment"># array([11, 22, 33,  3])</span></span><br></pre></td></tr></table></figure>
<pre><code>[11  1  2  3]
[11  1  2 44]
[11  1  2  3]
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external">numpy.org</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">numpy docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Numpy Array Split]]></title>
      <url>http://sggo.me/2017/12/27/python/py-numpy-7-Split/</url>
      <content type="html"><![CDATA[<p>Numpy array 横向分割、纵向分割、等量分割、非等量分割</p>
<a id="more"></a>
<h2 id="创建数据"><a href="#创建数据" class="headerlink" title="创建数据"></a>创建数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.arange(<span class="number">12</span>).reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span><br><span class="line">array([[ 0,  1,  2,  3],</span><br><span class="line">    [ 4,  5,  6,  7],</span><br><span class="line">    [ 8,  9, 10, 11]])</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line">print(A)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]]
</code></pre><h2 id="纵向分割"><a href="#纵向分割" class="headerlink" title="纵向分割"></a>纵向分割</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.split(A, <span class="number">2</span>, axis=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span><br><span class="line">[array([[0, 1],</span><br><span class="line">        [4, 5],</span><br><span class="line">        [8, 9]]), array([[ 2,  3],</span><br><span class="line">        [ 6,  7],</span><br><span class="line">        [10, 11]])]</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="横向分割"><a href="#横向分割" class="headerlink" title="横向分割"></a>横向分割</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.split(A, <span class="number">3</span>, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]</span></span><br></pre></td></tr></table></figure>
<h2 id="不等量的分割"><a href="#不等量的分割" class="headerlink" title="不等量的分割"></a>不等量的分割</h2><p>在机器学习时经常会需要将数据做不等量的分割，因此解决办法为<code>np.array_split()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.array_split(A, <span class="number">3</span>, axis=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span><br><span class="line">[array([[0, 1],</span><br><span class="line">        [4, 5],</span><br><span class="line">        [8, 9]]), array([[ 2],</span><br><span class="line">        [ 6],</span><br><span class="line">        [10]]), array([[ 3],</span><br><span class="line">        [ 7],</span><br><span class="line">        [11]])]</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<p>成功将Array不等量分割!</p>
<h2 id="其他的分割方式"><a href="#其他的分割方式" class="headerlink" title="其他的分割方式"></a>其他的分割方式</h2><p>在Numpy里还有<code>np.vsplit()</code>与横<code>np.hsplit()</code>方式可用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.vsplit(A, <span class="number">3</span>)) <span class="comment">#等于 print(np.split(A, 3, axis=0))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(np.hsplit(A, <span class="number">2</span>)) <span class="comment">#等于 print(np.split(A, 2, axis=1))</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">[array([[0, 1],</span><br><span class="line">       [4, 5],</span><br><span class="line">       [8, 9]]), array([[ 2,  3],</span><br><span class="line">        [ 6,  7],</span><br><span class="line">        [10, 11]])]</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external">numpy.org</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">numpy docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Numpy Array Merge]]></title>
      <url>http://sggo.me/2017/12/26/python/py-numpy-6-Array-Merge/</url>
      <content type="html"><![CDATA[<p>对于一个<code>array</code>的合并，我们可以想到按行、按列等多种方式进行合并</p>
<a id="more"></a>
<h2 id="np-vstack"><a href="#np-vstack" class="headerlink" title="np.vstack()"></a>np.vstack()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">B = np.array([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">         </span><br><span class="line">print(np.vstack((A,B)))    <span class="comment"># vertical stack</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span><br><span class="line">[[1,1,1]</span><br><span class="line"> [2,2,2]]</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line">np.vstack((A,B))</span><br></pre></td></tr></table></figure>
<pre><code>[[1 1 1]
 [2 2 2]]

array([[1, 1, 1],
       [2, 2, 2]])
</code></pre><p>vertical stack 本身属于一种上下合并，即对括号中的两个整体进行对应操作。此时我们对组合而成的矩阵进行属性探究：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C = np.vstack((A,B))      </span><br><span class="line">print(A.shape,C.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (3,) (2,3) # A 是序列, 序列合并后 C 为矩阵</span></span><br></pre></td></tr></table></figure>
<pre><code>(3,) (2, 3)
</code></pre><p>利用shape函数可以让我们很容易地知道A和C的属性，从打印出的结果来看，A仅仅是一个拥有3项元素的数组（数列），而合并后得到的C是一个2行3列的矩阵。</p>
<h2 id="np-hstack-左右合并："><a href="#np-hstack-左右合并：" class="headerlink" title="np.hstack() 左右合并："></a>np.hstack() 左右合并：</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D = np.hstack((A,B))       <span class="comment"># horizontal stack</span></span><br><span class="line"></span><br><span class="line">print(D)</span><br><span class="line"><span class="comment"># [1,1,1,2,2,2]</span></span><br><span class="line"></span><br><span class="line">print(A.shape,D.shape)</span><br><span class="line"><span class="comment"># (3,) (6,)</span></span><br></pre></td></tr></table></figure>
<pre><code>[1 1 1 2 2 2]
(3,) (6,)
</code></pre><p>不能用 A.T 这样的方法，将一个序列变为矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(A.T)</span><br><span class="line">print(A.T.shape)</span><br></pre></td></tr></table></figure>
<pre><code>[1 1 1]
(3,)
</code></pre><p>通过打印出的结果可以看出：D本身来源于A，B两个数列的左右合并，而且新生成的D本身也是一个含有6项元素的序列。</p>
<h2 id="np-newaxis"><a href="#np-newaxis" class="headerlink" title="np.newaxis()"></a>np.newaxis()</h2><p>如果面对如同前文所述的A序列， 转置操作便很有可能无法对其进行转置（因为A并不是矩阵的属性），此时就需要我们借助其他的函数操作进行转置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(A[np.newaxis,:])</span><br><span class="line"><span class="comment"># [[1 1 1]]</span></span><br><span class="line"></span><br><span class="line">print(A[np.newaxis,:].shape)</span><br><span class="line"><span class="comment"># (1,3)</span></span><br><span class="line"></span><br><span class="line">print(A[:,np.newaxis])</span><br><span class="line"><span class="string">"""</span><br><span class="line">[[1]</span><br><span class="line">[1]</span><br><span class="line">[1]]</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line">print(A[:,np.newaxis].shape)</span><br><span class="line"><span class="comment"># (3,1)</span></span><br></pre></td></tr></table></figure>
<pre><code>[[1 1 1]]
(1, 3)
[[1]
 [1]
 [1]]
(3, 1)
</code></pre><p>此时我们便将具有3个元素的<code>array</code>转换为了1行3列以及3行1列的矩阵了。</p>
<h2 id="综合总结"><a href="#综合总结" class="headerlink" title="综合总结"></a>综合总结</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])[:,np.newaxis]</span><br><span class="line">B = np.array([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])[:,np.newaxis]</span><br><span class="line">         </span><br><span class="line">C = np.vstack((A,B))   <span class="comment"># vertical stack</span></span><br><span class="line">D = np.hstack((A,B))   <span class="comment"># horizontal stack</span></span><br><span class="line"></span><br><span class="line">print(D)</span><br><span class="line"><span class="string">"""</span><br><span class="line">[[1 2]</span><br><span class="line">[1 2]</span><br><span class="line">[1 2]]</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line">print(A.shape,D.shape)</span><br><span class="line"><span class="comment"># (3,1) (3,2)</span></span><br></pre></td></tr></table></figure>
<pre><code>[[1 2]
 [1 2]
 [1 2]]
(3, 1) (3, 2)
</code></pre><h2 id="np-concatenate"><a href="#np-concatenate" class="headerlink" title="np.concatenate()"></a>np.concatenate()</h2><p>当你的合并操作需要针对多个矩阵或序列时，借助concatenate函数可能会让你使用起来比前述的函数更加方便：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C = np.concatenate((A,B,B,A),axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(C)</span><br><span class="line"><span class="string">"""</span><br><span class="line">array([[1],</span><br><span class="line">       [1],</span><br><span class="line">       [1],</span><br><span class="line">       [2],</span><br><span class="line">       [2],</span><br><span class="line">       [2],</span><br><span class="line">       [2],</span><br><span class="line">       [2],</span><br><span class="line">       [2],</span><br><span class="line">       [1],</span><br><span class="line">       [1],</span><br><span class="line">       [1]])</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line">D = np.concatenate((A,B,B,A),axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(D)</span><br><span class="line"><span class="string">"""</span><br><span class="line">array([[1, 2, 2, 1],</span><br><span class="line">       [1, 2, 2, 1],</span><br><span class="line">       [1, 2, 2, 1]])</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<pre><code>[[1 2 2 1]
 [1 2 2 1]
 [1 2 2 1]]
</code></pre><p><code>axis</code> 参数很好的控制了矩阵的纵向或是横向打印，相比较 <code>vstack</code> 和 <code>hstack</code> 函数显得更加方便。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external">numpy.org</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">numpy docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Numpy Index]]></title>
      <url>http://sggo.me/2017/12/26/python/py-numpy-5-Index/</url>
      <content type="html"><![CDATA[<p>在元素列表或者数组中，我们可以用如同 <code>a[2]</code> 一样的表示方法，同样的，Numpy中也有相应的表示方法</p>
<a id="more"></a>
<h2 id="一维索引"><a href="#一维索引" class="headerlink" title="一维索引"></a>一维索引</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">3</span>,<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])</span></span><br><span class="line">         </span><br><span class="line">print(A[<span class="number">3</span>])    <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<pre><code>6
</code></pre><p>让我们将矩阵转换为二维的，此时进行同样的操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.arange(<span class="number">3</span>,<span class="number">15</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="string">"""</span><br><span class="line">array([[ 3,  4,  5,  6]</span><br><span class="line">       [ 7,  8,  9, 10]</span><br><span class="line">       [11, 12, 13, 14]])</span><br><span class="line">"""</span></span><br><span class="line">         </span><br><span class="line">print(A[<span class="number">2</span>])         </span><br><span class="line"><span class="comment"># [11 12 13 14]</span></span><br></pre></td></tr></table></figure>
<pre><code>[11 12 13 14]
</code></pre><p>实际上这时的 <code>A[2]</code> 对应的就是 矩阵A 中第三行(从0开始算第一行)的所有元素。</p>
<h2 id="二维索引"><a href="#二维索引" class="headerlink" title="二维索引"></a>二维索引</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(A[<span class="number">1</span>][<span class="number">1</span>])      <span class="comment"># 8</span></span><br><span class="line">print(A[<span class="number">1</span>, <span class="number">1</span>])      <span class="comment"># 8</span></span><br></pre></td></tr></table></figure>
<pre><code>8
8
</code></pre><p>在Python的 list 中，我们可以利用:对一定范围内的元素进行切片操作，在Numpy中我们依然可以给出相应的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(A[<span class="number">1</span>, <span class="number">1</span>:<span class="number">3</span>])    <span class="comment"># [8 9]</span></span><br></pre></td></tr></table></figure>
<pre><code>[8 9]
</code></pre><p>这一表示形式即针对第二行中第2到第4列元素进行切片输出（不包含第4列）。 此时我们适当的利用for函数进行打印：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> A:</span><br><span class="line">    print(row)</span><br><span class="line"><span class="string">"""    </span><br><span class="line">[ 3,  4,  5, 6]</span><br><span class="line">[ 7,  8,  9, 10]</span><br><span class="line">[11, 12, 13, 14]</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<p>此时它会逐行进行打印操作。如果想进行逐列打印，就需要稍稍变化一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> A.T:</span><br><span class="line">    print(column)</span><br><span class="line"><span class="string">"""  </span><br><span class="line">[ 3,  7,  11]</span><br><span class="line">[ 4,  8,  12]</span><br><span class="line">[ 5,  9,  13]</span><br><span class="line">[ 6, 10,  14]</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<p>说一些关于迭代输出的问题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">3</span>,<span class="number">15</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">         </span><br><span class="line">print(A.flatten())   </span><br><span class="line"><span class="comment"># array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> A.flat:</span><br><span class="line">    print(item)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"><span class="comment"># ……</span></span><br><span class="line"><span class="comment"># 14</span></span><br></pre></td></tr></table></figure>
<pre><code>[ 3  4  5  6  7  8  9 10 11 12 13 14]
3
4
5
6
7
8
9
10
11
12
13
14
</code></pre><p>这一脚本中的flatten是一个展开性质的函数，将多维的矩阵进行展开成1行的数列。而flat是一个迭代器，本身是一个object属性。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external">numpy.org</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">numpy docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Numpy Basic Operation 2]]></title>
      <url>http://sggo.me/2017/12/25/python/py-numpy-4-basic-operation-2/</url>
      <content type="html"><![CDATA[<p>numpy 矩阵的基本操作，argmin/argmax、mean/average、cumsum、sort、transpose/A.T、clip</p>
<a id="more"></a>
<h2 id="argmin-amp-argmax"><a href="#argmin-amp-argmax" class="headerlink" title="argmin &amp; argmax"></a>argmin &amp; argmax</h2><p>矩阵对应元素的索引也是非常重要的</p>
<p>其中的 <code>argmin()</code> 和 <code>argmax()</code> 两个函数分别对应着求矩阵中最小元素和最大元素的索引。相应的，在矩阵的12个元素中，最小值即2，对应索引0，最大值为13，对应索引为11。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">2</span>,<span class="number">14</span>).reshape((<span class="number">3</span>,<span class="number">4</span>)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[ 2, 3, 4, 5]</span></span><br><span class="line"><span class="comment">#        [ 6, 7, 8, 9]</span></span><br><span class="line"><span class="comment">#        [10,11,12,13]])</span></span><br><span class="line">         </span><br><span class="line">print(np.argmin(A))    <span class="comment"># 0</span></span><br><span class="line">print(np.argmax(A))    <span class="comment"># 11</span></span><br></pre></td></tr></table></figure>
<pre><code>0
11
</code></pre><h2 id="mean-amp-average"><a href="#mean-amp-average" class="headerlink" title="mean &amp; average"></a>mean &amp; average</h2><p>统计中的均值，可以利用下面的方式，将整个矩阵的均值求出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.mean(A))        <span class="comment"># 7.5</span></span><br><span class="line">print(np.average(A))     <span class="comment"># 7.5</span></span><br></pre></td></tr></table></figure>
<pre><code>7.5
7.5
</code></pre><p>仿照着前一节中dot() 的使用法则，mean()函数还有另外一种写法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(A.mean())          <span class="comment"># 7.5</span></span><br></pre></td></tr></table></figure>
<pre><code>7.5
</code></pre><h2 id="cumsum"><a href="#cumsum" class="headerlink" title="cumsum"></a>cumsum</h2><p>和matlab中的cumsum()累加函数类似，Numpy中也具有cumsum()函数，其用法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.cumsum(A)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># [2 5 9 14 20 27 35 44 54 65 77 90]</span></span><br></pre></td></tr></table></figure>
<pre><code>[ 2  5  9 14 20 27 35 44 54 65 77 90]
</code></pre><p>在cumsum()函数中：生成的每一项矩阵元素均是从原矩阵首项累加到对应项的元素之和。比如元素9，在cumsum()生成的矩阵中序号为3，即原矩阵中2，3，4三个元素的和。</p>
<h2 id="diff"><a href="#diff" class="headerlink" title="diff"></a>diff</h2><p>相应的有累差运算函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.diff(A))    </span><br><span class="line"></span><br><span class="line"><span class="comment"># [[1 1 1]</span></span><br><span class="line"><span class="comment">#  [1 1 1]</span></span><br><span class="line"><span class="comment">#  [1 1 1]]</span></span><br></pre></td></tr></table></figure>
<pre><code>[[1 1 1]
 [1 1 1]
 [1 1 1]]
</code></pre><p>该函数计算的便是每一行中后一项与前一项之差。故一个3行4列矩阵通过函数计算得到的矩阵便是3行3列的矩阵。</p>
<blockquote>
<p><code>nonzero()</code> 函数, 觉得用处不大未学。</p>
</blockquote>
<h2 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">14</span>,<span class="number">2</span>, <span class="number">-1</span>).reshape((<span class="number">3</span>,<span class="number">4</span>)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[14, 13, 12, 11],</span></span><br><span class="line"><span class="comment">#       [10,  9,  8,  7],</span></span><br><span class="line"><span class="comment">#       [ 6,  5,  4,  3]])</span></span><br><span class="line"></span><br><span class="line">print(np.sort(A))    </span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[11,12,13,14]</span></span><br><span class="line"><span class="comment">#        [ 7, 8, 9,10]</span></span><br><span class="line"><span class="comment">#        [ 3, 4, 5, 6]])</span></span><br></pre></td></tr></table></figure>
<pre><code>[[11 12 13 14]
 [ 7  8  9 10]
 [ 3  4  5  6]]
</code></pre><h2 id="transpose-amp-A-T"><a href="#transpose-amp-A-T" class="headerlink" title="transpose &amp; A.T"></a>transpose &amp; A.T</h2><p>矩阵的转置有两种表示方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(np.transpose(A))    </span><br><span class="line">print(A.T)</span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[14,10, 6]</span></span><br><span class="line"><span class="comment">#        [13, 9, 5]</span></span><br><span class="line"><span class="comment">#        [12, 8, 4]</span></span><br><span class="line"><span class="comment">#        [11, 7, 3]])</span></span><br><span class="line"><span class="comment"># array([[14,10, 6]</span></span><br><span class="line"><span class="comment">#        [13, 9, 5]</span></span><br><span class="line"><span class="comment">#        [12, 8, 4]</span></span><br><span class="line"><span class="comment">#        [11, 7, 3]])</span></span><br></pre></td></tr></table></figure>
<pre><code>[[14 10  6]
 [13  9  5]
 [12  8  4]
 [11  7  3]]
[[14 10  6]
 [13  9  5]
 [12  8  4]
 [11  7  3]]
</code></pre><h2 id="clip"><a href="#clip" class="headerlink" title="clip"></a>clip</h2><p>特别的，在Numpy中具有clip()函数，例子如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(A)</span><br><span class="line"><span class="comment"># array([[14,13,12,11]</span></span><br><span class="line"><span class="comment">#        [10, 9, 8, 7]</span></span><br><span class="line"><span class="comment">#        [ 6, 5, 4, 3]])</span></span><br><span class="line"></span><br><span class="line">print(np.clip(A,<span class="number">5</span>,<span class="number">9</span>))    </span><br><span class="line"><span class="comment"># array([[ 9, 9, 9, 9]</span></span><br><span class="line"><span class="comment">#        [ 9, 9, 8, 7]</span></span><br><span class="line"><span class="comment">#        [ 6, 5, 5, 5]])</span></span><br></pre></td></tr></table></figure>
<pre><code>[[14 13 12 11]
 [10  9  8  7]
 [ 6  5  4  3]]
[[9 9 9 9]
 [9 9 8 7]
 [6 5 5 5]]
</code></pre><p>这个函数的格式是clip(Array,Array_min,Array_max)，顾名思义，Array指的是将要被执行用的矩阵，而后面的最小值最大值则用于让函数判断矩阵中元素是否有比最小值小的或者比最大值大的元素，并将这些指定的元素转换为最小值或者最大值。</p>
<p>实际上每一个Numpy中大多数函数均具有很多变量可以操作，你可以指定行、列甚至某一范围中的元素。更多具体的使用细节请记得查阅Numpy官方文档。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external">numpy.org</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">numpy docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Numpy Basic Operation 1]]></title>
      <url>http://sggo.me/2017/12/25/python/py-numpy-3-basic-operation-1/</url>
      <content type="html"><![CDATA[<p>numpy 矩阵的基本运算，加减乘除、数学函数、最大最小值、axis 查找 等</p>
<a id="more"></a>
<h2 id="numpy-生成矩阵"><a href="#numpy-生成矩阵" class="headerlink" title="numpy 生成矩阵"></a>numpy 生成矩阵</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a=np.array([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>])   <span class="comment"># array([10, 20, 30, 40])</span></span><br><span class="line">b=np.arange(<span class="number">4</span>)              <span class="comment"># array([0, 1, 2, 3])</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<pre><code>array([0, 1, 2, 3])
</code></pre><h2 id="numpy-加减乘除"><a href="#numpy-加减乘除" class="headerlink" title="numpy 加减乘除"></a>numpy 加减乘除</h2><p>上述代码中的 <code>a</code> 和 <code>b</code> 是两个属性为 array 也就是矩阵的变量，而且二者都是1行4列的矩阵， 其中b矩阵中的元素分别是从0到3。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c=a-b  <span class="comment"># array([10, 19, 28, 37])</span></span><br><span class="line">print(c)</span><br><span class="line">c=a+b   <span class="comment"># array([10, 21, 32, 43])</span></span><br><span class="line">print(c)</span><br><span class="line">c=a*b   <span class="comment"># array([  0,  20,  60, 120])</span></span><br><span class="line">print(c)</span><br><span class="line"></span><br><span class="line">c=b**<span class="number">2</span>  <span class="comment"># array([0, 1, 4, 9])</span></span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<pre><code>[10 19 28 37]
[10 21 32 43]
[  0  20  60 120]
[0 1 4 9]
</code></pre><h2 id="numpy-数学函数"><a href="#numpy-数学函数" class="headerlink" title="numpy 数学函数"></a>numpy 数学函数</h2><p>numpy 三角函数等，当我们需要对矩阵中每一项元素进行函数运算时，可以很简便的调用它们（以sin函数为例）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c=<span class="number">10</span>*np.sin(a)  </span><br><span class="line"><span class="comment"># array([-5.44021111,  9.12945251, -9.88031624,  7.4511316 ])</span></span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<pre><code>[-5.44021111  9.12945251 -9.88031624  7.4511316 ]
</code></pre><h2 id="print-函数进行逻辑判断"><a href="#print-函数进行逻辑判断" class="headerlink" title="print 函数进行逻辑判断"></a>print 函数进行逻辑判断</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(b&lt;<span class="number">3</span>)  </span><br><span class="line"><span class="comment"># array([ True,  True,  True, False], dtype=bool)</span></span><br></pre></td></tr></table></figure>
<pre><code>[ True  True  True False]
</code></pre><p>上述运算均是建立在一维矩阵，即只有一行的矩阵上面的计算，如果我们想要对多行多维度的矩阵进行操作，需要对开始的脚本进行一些修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=np.array([[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">b=np.arange(<span class="number">4</span>).reshape((<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># array([[1, 1],</span></span><br><span class="line"><span class="comment">#       [0, 1]])</span></span><br><span class="line"></span><br><span class="line">print(b)</span><br><span class="line"><span class="comment"># array([[0, 1],</span></span><br><span class="line"><span class="comment">#       [2, 3]])</span></span><br><span class="line"></span><br><span class="line">print(b &gt; <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 1]
 [0 1]]
[[0 1]
 [2 3]]
[[False False]
 [False  True]]
</code></pre><h2 id="numpy-两种矩阵乘法"><a href="#numpy-两种矩阵乘法" class="headerlink" title="numpy 两种矩阵乘法"></a>numpy 两种矩阵乘法</h2><p>此时构造出来的矩阵a和b便是2行2列的，其中 reshape 操作是对矩阵的形状进行重构， 其重构的形状便是括号中给出的数字。 稍显不同的是，Numpy 中的矩阵乘法分为两种， 其一是前文中的对应元素相乘，其二是标准的矩阵乘法运算，即对应行乘对应列得到相应元素：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">c_dot = np.dot(a,b)</span><br><span class="line"><span class="comment"># array([[2, 4],</span></span><br><span class="line"><span class="comment">#       [2, 3]])</span></span><br><span class="line">print(<span class="string">"========"</span>)</span><br><span class="line">print(c_dot)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 1]
 [0 1]]
[[0 1]
 [2 3]]
========
[[2 4]
 [2 3]]
</code></pre><p>除此之外还有另外的一种关于<code>dot</code>的表示方法，即：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c_dot_2 = a.dot(b)</span><br><span class="line"><span class="comment"># array([[2, 4],</span></span><br><span class="line"><span class="comment">#       [2, 3]])</span></span><br></pre></td></tr></table></figure>
<h2 id="sum-min-max"><a href="#sum-min-max" class="headerlink" title="sum(), min(), max()"></a>sum(), min(), max()</h2><p>下面我们将重新定义一个脚本, 来看看关于 <code>sum()</code>, <code>min()</code>, <code>max()</code> 的使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a=np.random.random((<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># array([[ 0.94692159,  0.20821798,  0.35339414,  0.2805278 ],</span></span><br><span class="line"><span class="comment">#       [ 0.04836775,  0.04023552,  0.44091941,  0.21665268]])</span></span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.38281924  0.40654978  0.69744113  0.90707595]
 [ 0.40572074  0.652105    0.24226191  0.95931459]]
</code></pre><p>因为是随机生成数字, 所以你的结果可能会不一样. 在第二行中对a的操作是令a中生成一个2行4列的矩阵，且每一元素均是来自从0到1的随机数。 在这个随机生成的矩阵中，我们可以对元素进行求和以及寻找极值的操作，具体如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.sum(a)   <span class="comment"># 4.6532883360785817</span></span><br><span class="line">np.min(a)   <span class="comment"># 0.24226191007863129</span></span><br><span class="line">np.max(a)   <span class="comment"># 0.95931458707579575</span></span><br></pre></td></tr></table></figure>
<pre><code>0.95931458707579575
</code></pre><h2 id="axis-进行赋值"><a href="#axis-进行赋值" class="headerlink" title="axis 进行赋值"></a>axis 进行赋值</h2><p>对应的便是对矩阵中所有元素进行求和，寻找最小值，寻找最大值的操作。 可以通过print()函数对相应值进行打印检验。</p>
<p>如果你需要对行或者列进行查找运算，就需要在上述代码中为 axis 进行赋值。 </p>
<p>当axis的值为0的时候，将会以列作为查找单元， 当axis的值为1的时候，将会以行作为查找单元。</p>
<p>为了更加清晰，在刚才的例子中我们继续进行查找：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"a ="</span>,a)</span><br><span class="line"><span class="comment"># a = [[ 0.23651224  0.41900661  0.84869417  0.46456022]</span></span><br><span class="line"><span class="comment"># [ 0.60771087  0.9043845   0.36603285  0.55746074]]</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"sum ="</span>,np.sum(a,axis=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># sum = [ 1.96877324  2.43558896]</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"min ="</span>,np.min(a,axis=<span class="number">0</span>))</span><br><span class="line"><span class="comment"># min = [ 0.23651224  0.41900661  0.36603285  0.46456022]</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"max ="</span>,np.max(a,axis=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># max = [ 0.84869417  0.9043845 ]</span></span><br></pre></td></tr></table></figure>
<pre><code>a = [[ 0.38281924  0.40654978  0.69744113  0.90707595]
 [ 0.40572074  0.652105    0.24226191  0.95931459]]
sum = [ 2.3938861   2.25940224]
min = [ 0.38281924  0.40654978  0.24226191  0.90707595]
max = [ 0.90707595  0.95931459]
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external">numpy.org</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">numpy docs</a></li>
<li><a href="https://morvanzhou.github.io" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Numpy Array]]></title>
      <url>http://sggo.me/2017/12/22/python/py-numpy-2-array/</url>
      <content type="html"><![CDATA[<p>创建 array 有很多 形式</p>
<a id="more"></a>
<h2 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h2><ul>
<li><code>array</code>：创建数组</li>
<li><code>dtype</code>：指定数据类型</li>
<li><code>zeros</code>：数据全为0</li>
<li><code>ones</code>：数据全为1</li>
<li><code>arrange</code>：按指定范围创建数据</li>
<li><code>linspace</code>：创建线段</li>
</ul>
<blockquote>
<p>与 List 区别之一 : 没有逗号分隔</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([<span class="number">2</span>,<span class="number">23</span>,<span class="number">4</span>])</span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># [ 2 23  4]</span></span><br></pre></td></tr></table></figure>
<pre><code>[ 2 23  4]
int64
</code></pre><h2 id="指定数据"><a href="#指定数据" class="headerlink" title="指定数据"></a>指定数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([<span class="number">2</span>,<span class="number">23</span>,<span class="number">4</span>], dtype=np.int) <span class="comment"># 默认 int 为 int64</span></span><br><span class="line">print(a.dtype) </span><br><span class="line"><span class="comment">#int64</span></span><br><span class="line"></span><br><span class="line">a = np.array([<span class="number">2</span>,<span class="number">23</span>,<span class="number">4</span>],dtype=np.int32)</span><br><span class="line">print(a.dtype)</span><br><span class="line"><span class="comment"># int32</span></span><br><span class="line"></span><br><span class="line">a = np.array([<span class="number">2</span>,<span class="number">23.1</span>,<span class="number">4.0</span>], dtype=np.float) <span class="comment"># 默认 float 为 float64</span></span><br><span class="line">print(a)</span><br><span class="line">print(a.dtype)</span><br><span class="line"><span class="comment"># float64</span></span><br><span class="line"></span><br><span class="line">a = np.array([<span class="number">2</span>,<span class="number">23</span>,<span class="number">4</span>],dtype=np.float32)</span><br><span class="line">print(a.dtype)</span><br><span class="line"><span class="comment"># float32</span></span><br></pre></td></tr></table></figure>
<pre><code>int64
int32
[  2.   23.1   4. ]
float64
float32
</code></pre><h2 id="创建特定数据"><a href="#创建特定数据" class="headerlink" title="创建特定数据"></a>创建特定数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">2</span>,<span class="number">23</span>,<span class="number">4</span>],</span><br><span class="line">        [<span class="number">2</span>,<span class="number">23</span>,<span class="number">4</span>]</span><br><span class="line">    ]</span><br><span class="line">)  <span class="comment"># 2d 矩阵 2行3列</span></span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span><br><span class="line">[[ 2 23  4]</span><br><span class="line"> [ 2 32  4]]</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="全零数组"><a href="#全零数组" class="headerlink" title="全零数组"></a>全零数组</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建全零数组</span></span><br><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>))  <span class="comment"># 数据全为0，3行4列</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">array([[ 0.,  0.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.,  0.]])</span><br><span class="line">"""</span></span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<h3 id="全一数组"><a href="#全一数组" class="headerlink" title="全一数组"></a>全一数组</h3><blockquote>
<p>同时也能指定这些特定数据的 <code>dtype</code>:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.ones((<span class="number">3</span>,<span class="number">4</span>),dtype = np.int)   <span class="comment"># 数据为1，3行4列</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">array([[1, 1, 1, 1],</span><br><span class="line">       [1, 1, 1, 1],</span><br><span class="line">       [1, 1, 1, 1]])</span><br><span class="line">"""</span></span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<h3 id="全空数组"><a href="#全空数组" class="headerlink" title="全空数组"></a>全空数组</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.empty( (<span class="number">2</span>,<span class="number">3</span>) ) <span class="comment"># 这个方法最大的好处就是速度快，因为少了初始化空间的操作</span></span><br></pre></td></tr></table></figure>
<pre><code>array([[  9.88131292e-324,   1.13635099e-322,   1.97626258e-323],
       [  9.88131292e-324,   1.13635099e-322,   1.97626258e-323]])
</code></pre><h3 id="连续数组arange"><a href="#连续数组arange" class="headerlink" title="连续数组arange"></a>连续数组<code>arange</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">10</span>,<span class="number">20</span>,<span class="number">2</span>) <span class="comment"># 10-19 的数据，2步长</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">array([10, 12, 14, 16, 18])</span><br><span class="line">"""</span></span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = np.arange(<span class="number">12</span>)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<pre><code>[10 12 14 16 18]
array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
</code></pre><h3 id="改变数据的形状reshape"><a href="#改变数据的形状reshape" class="headerlink" title="改变数据的形状reshape"></a>改变数据的形状<code>reshape</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">12</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))    <span class="comment"># 3行4列，0到11</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">array([[ 0,  1,  2,  3],</span><br><span class="line">       [ 4,  5,  6,  7],</span><br><span class="line">       [ 8,  9, 10, 11]])</span><br><span class="line">"""</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<h3 id="线段型数据linspace"><a href="#线段型数据linspace" class="headerlink" title="线段型数据linspace"></a>线段型数据<code>linspace</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">20</span>)    <span class="comment"># 开始端1，结束端10，且分割成20个数据，生成线段</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">array([  1.        ,   1.47368421,   1.94736842,   2.42105263,</span><br><span class="line">         2.89473684,   3.36842105,   3.84210526,   4.31578947,</span><br><span class="line">         4.78947368,   5.26315789,   5.73684211,   6.21052632,</span><br><span class="line">         6.68421053,   7.15789474,   7.63157895,   8.10526316,</span><br><span class="line">         8.57894737,   9.05263158,   9.52631579,  10.        ])</span><br><span class="line">"""</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<p>也能进行 <code>reshape</code> 工作:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">20</span>).reshape((<span class="number">5</span>,<span class="number">4</span>)) <span class="comment"># 更改shape</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">array([[  1.        ,   1.47368421,   1.94736842,   2.42105263],</span><br><span class="line">       [  2.89473684,   3.36842105,   3.84210526,   4.31578947],</span><br><span class="line">       [  4.78947368,   5.26315789,   5.73684211,   6.21052632],</span><br><span class="line">       [  6.68421053,   7.15789474,   7.63157895,   8.10526316],</span><br><span class="line">       [  8.57894737,   9.05263158,   9.52631579,  10.        ]])</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external">numpy.org</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">numpy docs</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/data-manipulation/np-pd/2-2-np-array/" target="_blank" rel="external">morvanzhou</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Numpy Attribute]]></title>
      <url>http://sggo.me/2017/12/21/python/py-numpy-1-attribute/</url>
      <content type="html"><![CDATA[<p>numpy 的几种属性 维度、行列个数、元素个数</p>
<a id="more"></a>
<ul>
<li><code>ndim</code>：维度</li>
<li><code>shape</code>：行数和列数</li>
<li><code>size</code>：元素个数</li>
</ul>
<p>列表转化为矩阵：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">array1 = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">print(array1)</span><br><span class="line">print(<span class="string">'number of dim:'</span>, array1.ndim)</span><br><span class="line">print(<span class="string">'shape:'</span>, array1.shape)</span><br><span class="line">print(<span class="string">'size:'</span>, array1.size)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span><br><span class="line">    [[1 2 3]</span><br><span class="line">     [2 3 4]]</span><br><span class="line">    number of dim: 2</span><br><span class="line">    shape: (2, 3)</span><br><span class="line">    size: 6</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external">numpy.org</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">numpy docs</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[杨帅完美发音班(2)]]></title>
      <url>http://sggo.me/2017/12/10/English/english-ielts-speaking-yangshuai-2/</url>
      <content type="html"><![CDATA[<ul>
<li>一定要不断重复，不要相信自己的耳朵，要相信音标</li>
<li>最开始练习，一定要超级夸张，一个音一个音读</li>
</ul>
<a id="more"></a>
<table>
<thead>
<tr>
<th>音标</th>
<th>读法</th>
<th>常见组合</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>/i:/</strong></td>
<td>长元紧张音</td>
<td>ee, ea, e, ie, ei </td>
</tr>
<tr>
<td><strong>/i/</strong></td>
<td>短元放松音</td>
<td>i, y, e</td>
</tr>
<tr>
<td><strong>/ɛ/</strong></td>
<td>/i/ 嘴型稍微大些</td>
<td>e、ea、a、ai </td>
</tr>
<tr>
<td><strong>/æ/</strong></td>
<td>/ɛ/+/a:/</td>
<td>a</td>
</tr>
<tr>
<td><strong>/ɒ/</strong></td>
<td>嘴张开、无变化</td>
<td>o、a</td>
</tr>
<tr>
<td><strong>/ɔ:/</strong></td>
<td>有点像 ‘沃’</td>
<td>or、our、ar、al、au、ou</td>
</tr>
<tr>
<td><strong>/aʊ/</strong></td>
<td>/æ/+/u/</td>
<td>ou、ow</td>
</tr>
</tbody>
</table>
<!--more-->
<table>
<thead>
<tr>
<th>&emsp;&emsp;&emsp;</th>
<th>i:、i、u:、u、ɛ、æ、ɒ、ɔ:、aʊ</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>i:</code></strong></td>
<td><em>seat、deed、least、feet、colleague [ˈkɒli:g]、eat、beat、leave、steel、steal</em></td>
</tr>
<tr>
<td><strong><code>i</code></strong></td>
<td><em>sit、did、list、fit、college、it、bit、live、still</em></td>
</tr>
<tr>
<td><strong><code>u:</code></strong></td>
<td><em>food、pool、fool、mood、moon、room、school、lose、true、blue、glue</em></td>
</tr>
<tr>
<td><strong><code>ʊ</code></strong></td>
<td><em>look、good、foot、book、cook、full、pull、push、put、should、would、could</em></td>
</tr>
<tr>
<td><strong><code>ɛ</code></strong></td>
<td><strong><em>bed、better、get、internet、lesson、desk、sell、slept、then、help、very、never、head、bread、pleasure、treasure、health、breath、said、there、their、where、everywhere、any、many、guess</em></strong></td>
</tr>
<tr>
<td><strong><code>æ</code></strong></td>
<td><strong><em>flag、cat、back、that、have、activity、natural、travel、relax、attractive、casual、magazine、fancy</em></strong></td>
</tr>
<tr>
<td><strong><code>ɒ</code></strong></td>
<td><strong><em>not、lost、loss、box、mop、collar、shop、job、top、soccer、biography、psychology、novel、want</em></strong></td>
</tr>
<tr>
<td><strong><code>ɔ:</code></strong></td>
<td><em>horse、more、short、store、court、course、mourn、warm、quarter、small、wall、ball、talk、walk、autumn、caught、taught、bought、thought、brought</em></td>
</tr>
<tr>
<td><strong><code>aʊ</code></strong></td>
<td><em>house、out、about、loud、down、town、cow、however、now、how、mouse、mouth、around、ground、found、accounting、sound</em></td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pyenv Install Anaconda3]]></title>
      <url>http://sggo.me/2017/12/07/ops/ops-pyenv-Anaconda3-note/</url>
      <content type="html"><![CDATA[<p>pyenv install anaconda3-5.0.0 无法下载包的情况下使用的，因为网速实在太慢了T T</p>
<a id="more"></a>
<h2 id="Anaconda3"><a href="#Anaconda3" class="headerlink" title="Anaconda3"></a>Anaconda3</h2><p><a href="https://repo.continuum.io/archive/Anaconda3-5.0.1-MacOSX-x86_64.sh" target="_blank" rel="external">官网</a> 下载了.sh文件，在该.sh文件目录使用下面的命令安装该文件</p>
<p>Anaconda3-5.0.1-MacOSX-x86_64.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  mv Anaconda3-5.0.1-MacOSX-x86_64.sh ~/.pyenv/cache/</span><br><span class="line">➜  cache git:(master) sh Anaconda3-5.0.1-MacOSX-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>安装过程需要操作的地方：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Please, press ENTER to continue</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">...</span><br><span class="line">Do you accept the license terms? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Anaconda3 will now be installed into this location:</span><br><span class="line">/Users/blair/anaconda3	</span><br><span class="line">- Press ENTER to confirm the location</span><br><span class="line">- Press CTRL-C to abort the installation</span><br><span class="line">- Or specify a different location below</span><br></pre></td></tr></table></figure>
<blockquote>
<p>[/Users/blair/anaconda3] 改为 /Users/blair/.pyenv/versions/anaconda3</p>
<p>输入需要安装的目录，因为要添加到pyenv管理器中，所以与其他以安装的Python版本放在同一目录下。</p>
</blockquote>
<p>开始输出安装信息，会安装许多包：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[/Users/blair/anaconda3] &gt;&gt;&gt; /Users/blair/.pyenv/versions/anaconda3</span><br><span class="line">PREFIX=/Users/blair/.pyenv/versions/anaconda3</span><br><span class="line">installing: python-3.6.3-h6804ab2_0 ...</span><br><span class="line">Python 3.6.3 :: Anaconda, Inc.</span><br><span class="line">installing: bzip2-1.0.6-h92991f9_1 ...</span><br><span class="line">installing: ca-certificates-2017.08.26-ha1e5d58_0 ...</span><br><span class="line">installing: conda-env-2.6.0-h36134e3_0 ...</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>然后出现提示，输入yes即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">installation finished.</span><br><span class="line">Do you wish the installer to prepend the Anaconda3 install location</span><br><span class="line">to PATH in your /Users/blair/.bash_profile ? [yes|no]</span><br><span class="line">[yes] &gt;&gt;&gt; yes</span><br></pre></td></tr></table></figure>
<p>安装成功！</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜ pyenv versions</span><br><span class="line">  system</span><br><span class="line">  2.7.14</span><br><span class="line">  2.7.14/envs/vpy2</span><br><span class="line">  3.5.4</span><br><span class="line">  3.5.4/envs/vpy3.5</span><br><span class="line">* anaconda3 (<span class="built_in">set</span> by /Users/blair/.pyenv/version)</span><br><span class="line">  anaconda3/envs/vconda3</span><br><span class="line">  vconda3</span><br><span class="line">  vpy2</span><br><span class="line">  vpy3.5</span><br><span class="line">(anaconda3)</span><br><span class="line"><span class="comment"># ~/ghome [10:34:54]</span></span><br></pre></td></tr></table></figure>
<p>为anaconda3创建虚拟环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  cache git:(master) pyenv virtualenv vconda3</span><br></pre></td></tr></table></figure>
<p>可能会装一些更新包，会有提示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#</span><br><span class="line"># To activate this environment, use:</span><br><span class="line"># &gt; source activate vconda3</span><br><span class="line">#</span><br><span class="line"># To deactivate an active environment, use:</span><br><span class="line"># &gt; source deactivate</span><br></pre></td></tr></table></figure>
<p>根据提示激活：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  cache git:(master) source activate vconda3</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[杨帅完美发音班(1)]]></title>
      <url>http://sggo.me/2017/12/03/English/english-ielts-speaking-yangshuai-1/</url>
      <content type="html"><![CDATA[<p>把发音练好的方法就是重复, 语言学习的本质就是模仿</p>
<p>but you’re old…</p>
<p>0 ~ 6 years 可以通过耳朵识别 140 个不同的音, 也可以马上模仿</p>
<p>6 岁之后，这种能力逐渐下降…， 所以你天天听剑桥英语，或者你去外国10几年，还是中式发音</p>
<a id="more"></a>
<h2 id="1-理论-模仿"><a href="#1-理论-模仿" class="headerlink" title="1. 理论 + 模仿"></a>1. 理论 + 模仿</h2><p>Less is more. </p>
<p>Practice makes progress</p>
<blockquote>
<p>少而精，融入血液，融入骨髓</p>
<p>理论 - 不用天天 (让模仿更加有效果)<br>模仿 - 是天天需要的</p>
</blockquote>
<h2 id="2-发音学习的几大注意"><a href="#2-发音学习的几大注意" class="headerlink" title="2. 发音学习的几大注意:"></a>2. 发音学习的几大注意:</h2><ul>
<li>懂音标 (自学)</li>
<li>一定要不断重复</li>
<li>不要相信自己的耳朵，要相信音标</li>
<li>最开始练习，一定要超级夸张</li>
<li>一个音一个音读</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">experience [ɪkˈspɪriəns]</span><br><span class="line">serious [ˈsɪriəs]</span><br><span class="line">series [ˈsɪri:z]</span><br><span class="line">down [daʊn]</span><br></pre></td></tr></table></figure>
<h3 id="推荐字典"><a href="#推荐字典" class="headerlink" title="推荐字典"></a>推荐字典</h3><p><a href="http://www.ox.ac.uk/" target="_blank" rel="external">Oxford</a> (英音)<br><a href="https://www.merriam-webster.com/" target="_blank" rel="external">Merriam Webster</a> (美音)</p>
<ol>
<li>最易错元音 – 保6</li>
<li>辅音 + 语调 (重读、连读) – 争7</li>
<li>略读 + 英美音主要区别 – 带你装带你飞</li>
</ol>
<h3 id="从英到美"><a href="#从英到美" class="headerlink" title="从英到美"></a>从英到美</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flat [flæt]、cat [kæt]</span><br><span class="line">kite [kaɪt]、 dog [dɒg] [dɔ:g] </span><br><span class="line">bed [bed]、[bɛd]</span><br><span class="line">bought [bɔ:t]、[bɔt]</span><br><span class="line">usually [ˈju:ʒuəli]、 [ˈjuːʒuəli] </span><br><span class="line">garage [ˈgærɑ:ʒ]、[gəˈrɑ:ʒ] </span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<h3 id="中国人问题最大的三个元音"><a href="#中国人问题最大的三个元音" class="headerlink" title="中国人问题最大的三个元音"></a>中国人问题最大的三个元音</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/ai/ /æ/ /ɛ/</span><br></pre></td></tr></table></figure>
<p><strong>/ai/ = /a:/-/i/</strong></p>
<ul>
<li>字母组合: 主要 <strong><em>i，ie，y，igh，uy</em></strong></li>
</ul>
<blockquote>
<p><strong><em>t<font color="#c7254e">i</font>me，wh<font color="#c7254e">i</font>te，d<font color="#c7254e">ie</font>，t<font color="#c7254e">ie</font>d<br>dry，why，July<br>h<font color="blue">igh</font>t，n<font color="blue">igh</font>t、r<font color="blue">igh</font>t<br>bu<font color="#c7254e">y</font>，gu<font color="#c7254e">y</font><br>k<font color="#c7254e">i</font>nd，m<font color="#c7254e">i</font>nd，w<font color="#c7254e">i</font>nd<br>all kinds of，spring to mind，wind down，unwind</em></strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>word</th>
<th>phonetic symbol</th>
<th>sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td>die</td>
<td>[dai]</td>
<td>I don’t want to die.</td>
</tr>
<tr>
<td>died</td>
<td>[daid]</td>
<td>My dog died yesterday.</td>
</tr>
<tr>
<td>dying</td>
<td>[ˈdaɪɪŋ]</td>
<td>The man is dying.</td>
</tr>
<tr>
<td>dead</td>
<td>[ded]，[dɛd]</td>
<td>Her cat is dead.</td>
</tr>
</tbody>
</table>
<h3 id="长短音的区分"><a href="#长短音的区分" class="headerlink" title="长短音的区分"></a>长短音的区分</h3><ul>
<li>长音: <code>tense</code>(紧张音) – 长元紧张音 - beach</li>
<li>短音: <code>lax</code> (放松音) – 短元放松音 - bitch</li>
</ul>
<p><strong>/i:/</strong> 不放松音 , 字母组合: 主要 <strong><em>ee，ea，e，ie，ei</em></strong></p>
<blockquote>
<p><strong><em>w<font color="#c7254e">ee</font>k，w<font color="#c7254e">ea</font>k，tr<font color="#c7254e">ee</font>，m<font color="#c7254e">ee</font>t，thr<font color="#c7254e">ee</font>，s<font color="#c7254e">ee</font>，s<font color="#c7254e">ee</font>k，m<font color="#c7254e">ea</font>t，l<font color="#c7254e">ea</font>ve，<font color="#c7254e">ea</font>t，b<font color="#c7254e">ea</font>t<br>l<font color="#c7254e">ea</font>d，<font color="#c7254e">ea</font>ch，coll<font color="#c7254e">ea</font>gue，t<font color="#c7254e">ea</font>m，r<font color="#c7254e">ea</font>d，sp<font color="#c7254e">ea</font>k，pl<font color="#c7254e">ea</font>se，p<font color="#c7254e">ie</font>ce，rec<font color="#c7254e">ei</font>ve<br>extr<font color="#c7254e">e</font>mely，m<font color="#c7254e">e</font>dia</em></strong></p>
</blockquote>
<p><strong>/ei/</strong> 一定要饱满 , 字母组合: 主要 <strong><em>ay，a</em></strong></p>
<blockquote>
<p><strong><em>tree. tray <font color="#c7254e">[trei]</font><br>see. say <font color="#c7254e">[sei]</font><br>week. wake <font color="#c7254e">[weik]</font><br>piece. pace <font color="#c7254e">[peis]</font></em></strong></p>
</blockquote>
<p><strong>/i/ 是 /i:/</strong> 的放松不用力音 , 字母组合: 主要 <strong><em>i，y</em></strong></p>
<blockquote>
<p><strong><em>sit，it，is，live，did，this，kid，list，six<br>fix，fit，click，if，big，give，gift，trip<br>still，miss，finish，habit，m<font color="#c7254e">y</font>stery [ˈmɪstəri]，coll<font color="#c7254e">e</font>ge [ˈkɑ:lɪdʒ]</em></strong></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[mac 安装 mysql 与 常用命令]]></title>
      <url>http://sggo.me/2017/12/03/ops/ops-mac-mysql/</url>
      <content type="html"><![CDATA[<p>介绍 Mac 安装 Mysql 与 mysql 在 mac 在的命令</p>
<a id="more"></a>
<h2 id="1-brew-install"><a href="#1-brew-install" class="headerlink" title="1. brew install"></a>1. brew install</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew install mysql</span><br></pre></td></tr></table></figure>
<p><strong>在mac下使用 brew 安装 mysql，之前没有使用过，今天启动的时候发现启动不了</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># /usr/local/bin [9:31:54]</span></span><br><span class="line">➜ mysql</span><br><span class="line">ERROR 2002 (HY000): Can<span class="string">'t connect to local MySQL server through socket '</span>/tmp/mysql.sock<span class="string">' (2)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜ brew info mysql</span><br><span class="line">mysql: stable 8.0.12 (bottled)</span><br></pre></td></tr></table></figure>
<h2 id="2-启动mysql"><a href="#2-启动mysql" class="headerlink" title="2. 启动mysql"></a>2. 启动mysql</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜ mysql.server start</span><br></pre></td></tr></table></figure>
<h2 id="3-设置密码"><a href="#3-设置密码" class="headerlink" title="3. 设置密码"></a>3. 设置密码</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql_secure_installation</span><br></pre></td></tr></table></figure>
<h2 id="4-进入mysql"><a href="#4-进入mysql" class="headerlink" title="4. 进入mysql"></a>4. 进入mysql</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br><span class="line">4 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure>
<h2 id="5-常用命令"><a href="#5-常用命令" class="headerlink" title="5. 常用命令"></a>5. 常用命令</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mysql.server start</span><br><span class="line">$ mysql.server restart</span><br><span class="line">$ mysql.server stop</span><br><span class="line">$ mysql.server status</span><br><span class="line">$ mysql -u root -p</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/plpldog/article/details/80761646" target="_blank" rel="external">mac用brew安装mysql,设置初始密码</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Neural Networks and Deep Learning (week1) - Introduction to Deep Learning]]></title>
      <url>http://sggo.me/2017/12/01/deeplearning/Neural-Networks-and-Deep-Learning-week1/</url>
      <content type="html"><![CDATA[<ol>
<li>Introduction to Deep Learning</li>
<li>What is a Neural Network ?</li>
<li>Why is Deep Learning taking off ?</li>
</ol>
<a id="more"></a>
<h2 id="1-Introduction-to-Deep-Learning"><a href="#1-Introduction-to-Deep-Learning" class="headerlink" title="1. Introduction to Deep Learning"></a>1. Introduction to Deep Learning</h2><p><img src="/images/deeplearning/light.jpg" width="420"></p>
<h3 id="What’s-you’ll-learn"><a href="#What’s-you’ll-learn" class="headerlink" title="What’s you’ll learn"></a>What’s you’ll learn</h3><div class="limg0"><br><img src="/images/deeplearning/deeplearning-ai-w1-6.jpg" width="720"><br></div>

<h2 id="2-What-is-a-Neural-Network"><a href="#2-What-is-a-Neural-Network" class="headerlink" title="2. What is a Neural Network ?"></a>2. What is a Neural Network ?</h2><h3 id="Housing-Price-Prediction"><a href="#Housing-Price-Prediction" class="headerlink" title="Housing Price Prediction"></a>Housing Price Prediction</h3><p><img src="/images/deeplearning/deeplearning-ai-w1-3-housing.jpg" width="720"></p>
<h2 id="3-Why-is-Deep-Learning-taking-off"><a href="#3-Why-is-Deep-Learning-taking-off" class="headerlink" title="3. Why is Deep Learning taking off ?"></a>3. Why is Deep Learning taking off ?</h2><p><img src="/images/deeplearning/deeplearning-ai-w1-4.jpg" width="720"></p>
<ul>
<li><strong>Data</strong> 、 <strong>Computation</strong> 、 <strong>Algorithms</strong></li>
</ul>
<p><img src="/images/deeplearning/deeplearning-ai-w1-5.jpg" width="720"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.deeplearning.ai/" target="_blank" rel="external">Andrew Ng - deeplearning.ai</a></li>
<li><a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029#/learn/content?type=detail&amp;id=2001702004&amp;cid=2001694006" target="_blank" rel="external">网易云课堂 - 第一周深度学习概论</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 11 - Chatbot 的第二个版本 (新API实现)]]></title>
      <url>http://sggo.me/2017/11/29/chatbot/chatbot-research11/</url>
      <content type="html"><![CDATA[<!-- 2018 -->
<p>上期我们使用 tf.contrib.legacy_seq2seq 下的 API 构建了一个简单的 chatbot 对话系统. 代码是1.0之前旧版.</p>
<p>这期我们学习新版本灵活的的API，这里先来说一下二者的不同：</p>
<a id="more"></a>
<p><strong>新版本API：</strong></p>
<blockquote>
<ul>
<li><p>用 dynamic_rnn 来构造 RNN模型，这样就避免了数据长度不同所带来的困扰，不需要再使用 model_with_buckets 这种方法来构建模型，使得我们数据处理和模型代码都简洁很多。</p>
</li>
<li><p>新版本将 Attention、 Decoder 等几个主要的功能都分别进行封装，直接调用相应的 Wapper函数 进行封装即可，更加灵活方便，而且只需要写几个简单的函数既可以自定义的各个模块以满足我们个性化的需求。</p>
</li>
</ul>
<ul>
<li>实现了beam_search功能，可直接调用。</li>
</ul>
</blockquote>
<h2 id="1-数据处理"><a href="#1-数据处理" class="headerlink" title="1. 数据处理"></a>1. 数据处理</h2><blockquote>
<p> word2id is :  { ‘decorations’: 12002, ‘scraps’: 4599, …}</p>
<p>id2word is :  { 0: ‘<pad>‘, 1: ‘<go>‘, 2: ‘<eos>‘, 3: ‘<unknown>‘, 4: ‘can’, 5: ‘we’, 6: ‘make’, … }</unknown></eos></go></pad></p>
<p>trainingSamples is :<br> [<br>    [ [793, 138, 65], [35, 209, 110, 9016, 208, 382, 35, 22] ],<br>    [ [35, 209, 110, 9016, 208, 382, 35, 22], [26, 92, 1906, 47, 254, 65] ],<br>    …<br> ]</p>
</blockquote>
<h2 id="2-模型构建"><a href="#2-模型构建" class="headerlink" title="2. 模型构建"></a>2. 模型构建</h2><p>代码主要是从 tensorflow官网 给出的nmt例子的代码简化而来，实现了最基本的 <strong>attention</strong> 和 <strong>beam_search</strong> 等功能，同时有将nmt代码中繁杂的代码逻辑进行简化。这里参考nmt中所提到的构建<code>train</code>、<code>eval</code>、<code>inference</code> , 三个图进行模型构建，好处在于 <a href="https://github.com/tensorflow/nmt#building-training-eval-and-inference-graphs" target="_blank" rel="external">nmt官方文档 Building Training, Eval, and Inference Graphs </a></p>
<blockquote>
<ul>
<li>inference图 往往与 train 和 eval结构 存在较大差异，所以往往需要单独进行构建<br>.   （没有decoder输入和目标，需要使用 greedy 或 beam_search 进行 decode，batch_size 也不同等等）</li>
<li>eval图，不需要进行反向传播，只需要得到一个 <strong>loss</strong> 和 <strong>acc</strong>值</li>
<li>数据进行 <code>feed</code>，简化数据操作</li>
<li>变量重用变得简单，因为 train、eval 存在一些公用变量和代码块，就不需要我们重复定义</li>
<li>只需要在 train 时不断保存模型参数，然后在 eval 和 infer 的时候 restore参数 即可</li>
</ul>
</blockquote>
<p>以上，所以我们构建了 train、eval、infer 三个函数来实现上面的功能。在看代码之前我们先来简单说一下新版API几个主要的模块以及相互之间的调用关系。tf.contrib.seq2seq文件夹下面主要有下面6个文件，除了loss文件和之前的sequence_loss函数没有很大区别，这里不介绍之外，其他几个文件都会简单的说一下，这里主要介绍函数和类的功能，源码会放在下篇文章中介绍。</p>
<blockquote>
<ul>
<li>decoder</li>
<li>basic_decoder</li>
<li>helper</li>
<li>attention_wrapper</li>
<li>beam_search_decoder</li>
<li>loss</li>
</ul>
</blockquote>
<h3 id="2-1-BasicDecoder-类和-dynamic-decode"><a href="#2-1-BasicDecoder-类和-dynamic-decode" class="headerlink" title="2.1 BasicDecoder 类和 dynamic_decode"></a>2.1 BasicDecoder 类和 dynamic_decode</h3><p>decoder文件中定义了</p>
<ul>
<li>Decoder抽象类</li>
<li>dynamic_decode函数</li>
</ul>
<blockquote>
<p>dynamic_decode 可以视为整个解码过程的入口，需要传入的参数就是 Decoder 的一个实例，他会动态的调用 Decoder 的 <code>step</code>函数 按步执行 decode，可以理解为Decoder类定义了单步解码（根据输入求出输出，并将该输出当做下一时刻输入），而dynamic_decode则会调用control_flow_ops.while_loop这个函数来循环执行直到输出<eos>结束编码过程.</eos></p>
</blockquote>
<h3 id="2-2-cell类型（Attention类型）"><a href="#2-2-cell类型（Attention类型）" class="headerlink" title="2.2 cell类型（Attention类型）"></a>2.2 cell类型（Attention类型）</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分为3步，</span></span><br><span class="line"><span class="comment">#.  1. 定义attention机制</span></span><br><span class="line"><span class="comment">#.  2. 定义要是用的基础的RNNCell</span></span><br><span class="line"><span class="comment">#.  3. 使用AttentionWrapper进行封装</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义要使用的attention机制。</span></span><br><span class="line">attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=self.rnn_size, memory=encoder_outputs, memory_sequence_length=encoder_inputs_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义decoder阶段要是用的LSTMCell，然后为其封装attention wrapper</span></span><br><span class="line">decoder_cell = self._create_rnn_cell()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 使用AttentionWrapper进行封装</span></span><br><span class="line">decoder_cell = tf.contrib.seq2seq.AttentionWrapper(cell=decoder_cell, attention_mechanism=attention_mechanism, attention_layer_size=self.rnn_size, name=<span class="string">'Attention_Wrapper'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-helper类型"><a href="#2-3-helper类型" class="headerlink" title="2.3 helper类型"></a>2.3 helper类型</h3><p>helper其实就是decode阶段如何根据预测结果得到下一时刻的输入，比如train训练过程中应该直接使用上一时刻的真实值作为下一时刻输入，预测过程中可以使用贪婪的方法选择概率最大的那个值作为下一时刻等等。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/thriving_fcl/article/details/74165062" target="_blank" rel="external">Tensorflow新版Seq2Seq接口使用</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq" target="_blank" rel="external">tensorflow官网API指导</a></li>
<li><a href="https://github.com/Conchylicultor/DeepQA#chatbot" target="_blank" rel="external">DeepQA</a></li>
<li><a href="https://github.com/pbhatia243/Neural_Conversation_Models" target="_blank" rel="external">Neural_Conversation_Models</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 10 - Chatbot 的第一个版本 (简单实现)]]></title>
      <url>http://sggo.me/2017/11/26/chatbot/chatbot-research10/</url>
      <content type="html"><![CDATA[<!-- 2018 -->
<p>本篇主要讲述如何调用 tf 提供的 seq2seq 的 API，实现一个chatbot对话系统.</p>
<p>网上很多参考代码都是基于tf的旧版本实现，导致这些代码在新版本的tf中无法正常运行。</p>
<a id="more"></a>
<h2 id="1-版本兼容"><a href="#1-版本兼容" class="headerlink" title="1. 版本兼容"></a>1. 版本兼容</h2><p><strong>常见的几个问题主要是</strong>：</p>
<ul>
<li>seq2seq API 旧版 tf.contrib.legacy_seq2seq, 新的接口 tf.contrib.seq2seq</li>
<li>rnn 目前也大都使用 tf.contrib.rnn 下面的 RNNCell；</li>
<li>embedding_attention_seq2seq 函数中调用deepcopy(cell)这个函数报异常<blockquote>
<p>deepcopy(cell)这个函数经常会爆出（TypeError: can’t pickle _thread.lock objects）的错误</p>
</blockquote>
</li>
</ul>
<p><strong>解决方案</strong>：</p>
<ol>
<li>切换TF版本 1.4 问题解决。</li>
<li>不切换版本：一种解决方案就是将embedding_attention_seq2seq的传入参数中的cell改成两个，分别是encoder_cell和decoder_cell，然后这两个cell分别使用下面代码进行初始化：</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">encoCell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)],)</span><br><span class="line">decoCell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)],)</span><br></pre></td></tr></table></figure>
<p>这样做不需要调用deepcopy函数对cell进行复制了，问题解决了，但在模型构建的时候速度会比较慢，猜测是因为需要构造两份RNN模型，但是最后训练的时候发现速度也很慢，无奈只能放弃这种做法。</p>
<p>然后分析代码，发现问题并不是单纯的出现在 embedding_attention_seq2seq 这个函数，而是在调用module_with_buckets的时候会构建很多个不同bucket的seq2seq模型，这就导致了embedding_attention_seq2seq会被重复调用很多次，后来发现确实是这里出现的问题。</p>
<p>解决方案的话就是，<code>不适用buckets构建模型</code>，而是简单的将所有序列都padding到统一长度，然后直接调用一次embedding_attention_seq2seq 函数构建模型即可，这样是不会抱错的。</p>
<h2 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2. 数据处理"></a>2. 数据处理</h2><p>用<a href="https://github.com/Conchylicultor/DeepQA#chatbot" target="_blank" rel="external">DeepQA</a>里数据处理的代码，省去从原始本文文件构造对话的过程直接使用其生成的 dataset-cornell-….pkl文件</p>
<blockquote>
<p>dataset-cornell-length10-filter1-vocabSize40000.pkl</p>
</blockquote>
<p>主要包括：</p>
<ol>
<li>读取数据的函数loadDataset()</li>
<li>根据数据创建batches的函数getBatches()和createBatch()</li>
<li>预测时将用户输入的句子转化成batch的函数sentence2enco()</li>
</ol>
<h2 id="3-模型构建"><a href="#3-模型构建" class="headerlink" title="3. 模型构建"></a>3. 模型构建</h2><ol>
<li>一些变量的传入和定义</li>
<li>OutputProjection层和sampled_softmax_loss函数的定义</li>
<li>RNNCell的定义和创建</li>
<li>根据训练或者测试调用相应的embedding_attention_seq2seq函数构建模型</li>
<li>step函数定义，主要用于给定一个batch的数据，构造相应的 feed_dict 和 run_opt</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> seq2seq <span class="keyword">import</span> embedding_attention_seq2seq</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqModel</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, source_vocab_size, target_vocab_size, en_de_seq_len, hidden_size, num_layers,</span><br><span class="line">                 batch_size, learning_rate, num_samples=<span class="number">1024</span>,</span><br><span class="line">                 forward_only=False, beam_search=True, beam_size=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span><br><span class="line">        初始化并创建模型</span><br><span class="line">        :param source_vocab_size:encoder输入的vocab size</span><br><span class="line">        :param target_vocab_size: decoder输入的vocab size，这里跟上面一样</span><br><span class="line">        :param en_de_seq_len: 源和目的序列最大长度</span><br><span class="line">        :param hidden_size: RNN模型的隐藏层单元个数</span><br><span class="line">        :param num_layers: RNN堆叠的层数</span><br><span class="line">        :param batch_size: batch大小</span><br><span class="line">        :param learning_rate: 学习率</span><br><span class="line">        :param num_samples: 计算loss时做sampled softmax时的采样数</span><br><span class="line">        :param forward_only: 预测时指定为真</span><br><span class="line">        :param beam_search: 预测时是采用greedy search还是beam search</span><br><span class="line">        :param beam_size: beam search的大小</span><br><span class="line">        '''</span></span><br><span class="line">        self.source_vocab_size = source_vocab_size</span><br><span class="line">        self.target_vocab_size = target_vocab_size</span><br><span class="line">        self.en_de_seq_len = en_de_seq_len</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.learning_rate = tf.Variable(float(learning_rate), trainable=<span class="keyword">False</span>)</span><br><span class="line">        self.num_samples = num_samples</span><br><span class="line">        self.forward_only = forward_only</span><br><span class="line">        self.beam_search = beam_search</span><br><span class="line">        self.beam_size = beam_size</span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">        output_projection = <span class="keyword">None</span></span><br><span class="line">        softmax_loss_function = <span class="keyword">None</span></span><br><span class="line">        <span class="comment"># 定义采样loss函数，传入后面的sequence_loss_by_example函数</span></span><br><span class="line">        <span class="keyword">if</span> num_samples &gt; <span class="number">0</span> <span class="keyword">and</span> num_samples &lt; self.target_vocab_size:</span><br><span class="line">            w = tf.get_variable(<span class="string">'proj_w'</span>, [hidden_size, self.target_vocab_size])</span><br><span class="line">            w_t = tf.transpose(w)</span><br><span class="line">            b = tf.get_variable(<span class="string">'proj_b'</span>, [self.target_vocab_size])</span><br><span class="line">            output_projection = (w, b)</span><br><span class="line">            <span class="comment">#调用sampled_softmax_loss函数计算sample loss，这样可以节省计算时间</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">sample_loss</span><span class="params">(logits, labels)</span>:</span></span><br><span class="line">                labels = tf.reshape(labels, [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">                <span class="keyword">return</span> tf.nn.sampled_softmax_loss(w_t, b, labels=labels, inputs=logits, num_sampled=num_samples, num_classes=self.target_vocab_size)</span><br><span class="line">            softmax_loss_function = sample_loss</span><br><span class="line"></span><br><span class="line">        self.keep_drop = tf.placeholder(tf.float32)</span><br><span class="line">        <span class="comment"># 定义encoder和decoder阶段的多层dropout RNNCell</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">create_rnn_cell</span><span class="params">()</span>:</span></span><br><span class="line">            encoDecoCell = tf.contrib.rnn.BasicLSTMCell(hidden_size)</span><br><span class="line">            encoDecoCell = tf.contrib.rnn.DropoutWrapper(encoDecoCell, input_keep_prob=<span class="number">1.0</span>, output_keep_prob=self.keep_drop)</span><br><span class="line">            <span class="keyword">return</span> encoDecoCell</span><br><span class="line">        encoCell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义输入的placeholder，采用了列表的形式</span></span><br><span class="line">        self.encoder_inputs = []</span><br><span class="line">        self.decoder_inputs = []</span><br><span class="line">        self.decoder_targets = []</span><br><span class="line">        self.target_weights = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(en_de_seq_len[<span class="number">0</span>]):</span><br><span class="line">            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[<span class="keyword">None</span>, ], name=<span class="string">"encoder&#123;0&#125;"</span>.format(i)))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(en_de_seq_len[<span class="number">1</span>]):</span><br><span class="line">            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[<span class="keyword">None</span>, ], name=<span class="string">"decoder&#123;0&#125;"</span>.format(i)))</span><br><span class="line">            self.decoder_targets.append(tf.placeholder(tf.int32, shape=[<span class="keyword">None</span>, ], name=<span class="string">"target&#123;0&#125;"</span>.format(i)))</span><br><span class="line">            self.target_weights.append(tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, ], name=<span class="string">"weight&#123;0&#125;"</span>.format(i)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># test模式，将上一时刻输出当做下一时刻输入传入</span></span><br><span class="line">        <span class="keyword">if</span> forward_only:</span><br><span class="line">            <span class="keyword">if</span> beam_search:<span class="comment">#如果是beam_search的话，则调用自己写的embedding_attention_seq2seq函数，而不是legacy_seq2seq下面的</span></span><br><span class="line">                self.beam_outputs, _, self.beam_path, self.beam_symbol = embedding_attention_seq2seq(</span><br><span class="line">                    self.encoder_inputs, self.decoder_inputs, encoCell, num_encoder_symbols=source_vocab_size,</span><br><span class="line">                    num_decoder_symbols=target_vocab_size, embedding_size=hidden_size,</span><br><span class="line">                    output_projection=output_projection, feed_previous=<span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                decoder_outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(</span><br><span class="line">                    self.encoder_inputs, self.decoder_inputs, encoCell, num_encoder_symbols=source_vocab_size,</span><br><span class="line">                    num_decoder_symbols=target_vocab_size, embedding_size=hidden_size,</span><br><span class="line">                    output_projection=output_projection, feed_previous=<span class="keyword">True</span>)</span><br><span class="line">                <span class="comment"># 因为seq2seq模型中未指定output_projection，所以需要在输出之后自己进行output_projection</span></span><br><span class="line">                <span class="keyword">if</span> output_projection <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    self.outputs = tf.matmul(decoder_outputs, output_projection[<span class="number">0</span>]) + output_projection[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 因为不需要将output作为下一时刻的输入，所以不用output_projection</span></span><br><span class="line">            decoder_outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(</span><br><span class="line">                self.encoder_inputs, self.decoder_inputs, encoCell, num_encoder_symbols=source_vocab_size,</span><br><span class="line">                num_decoder_symbols=target_vocab_size, embedding_size=hidden_size, output_projection=output_projection,</span><br><span class="line">                feed_previous=<span class="keyword">False</span>)</span><br><span class="line">            self.loss = tf.contrib.legacy_seq2seq.sequence_loss(</span><br><span class="line">                decoder_outputs, self.decoder_targets, self.target_weights, softmax_loss_function=softmax_loss_function)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the optimizer</span></span><br><span class="line">            opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-08</span>)</span><br><span class="line">            self.optOp = opt.minimize(self.loss)</span><br><span class="line"></span><br><span class="line">        self.saver = tf.train.Saver(tf.all_variables())</span><br></pre></td></tr></table></figure>
<p>step 函数定义，主要用于给定一个batch的数据，构造相应的 feed_dict 和 run_opt</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, session, encoder_inputs, decoder_inputs, decoder_targets, target_weights, go_token_id)</span>:</span></span><br><span class="line">    <span class="comment"># 传入一个batch的数据，并训练性对应的模型</span></span><br><span class="line">    <span class="comment"># 构建sess.run时的feed_inpits</span></span><br><span class="line">    feed_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.forward_only:</span><br><span class="line">        feed_dict[self.keep_drop] = <span class="number">0.5</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.en_de_seq_len[<span class="number">0</span>]):</span><br><span class="line">            feed_dict[self.encoder_inputs[i].name] = encoder_inputs[i]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.en_de_seq_len[<span class="number">1</span>]):</span><br><span class="line">            feed_dict[self.decoder_inputs[i].name] = decoder_inputs[i]</span><br><span class="line">            feed_dict[self.decoder_targets[i].name] = decoder_targets[i]</span><br><span class="line">            feed_dict[self.target_weights[i].name] = target_weights[i]</span><br><span class="line">        run_ops = [self.optOp, self.loss]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        feed_dict[self.keep_drop] = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.en_de_seq_len[<span class="number">0</span>]):</span><br><span class="line">            feed_dict[self.encoder_inputs[i].name] = encoder_inputs[i]</span><br><span class="line">        feed_dict[self.decoder_inputs[<span class="number">0</span>].name] = [go_token_id]</span><br><span class="line">        <span class="keyword">if</span> self.beam_search:</span><br><span class="line">            run_ops = [self.beam_path, self.beam_symbol]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            run_ops = [self.outputs]</span><br><span class="line"></span><br><span class="line">    outputs = session.run(run_ops, feed_dict)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.forward_only:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span>, outputs[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> self.beam_search:</span><br><span class="line">            <span class="keyword">return</span> outputs[<span class="number">0</span>], outputs[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/thriving_fcl/article/details/74165062" target="_blank" rel="external">Tensorflow新版Seq2Seq接口使用</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq" target="_blank" rel="external">tensorflow官网API指导</a></li>
<li><a href="https://github.com/Conchylicultor/DeepQA#chatbot" target="_blank" rel="external">DeepQA</a></li>
<li><a href="https://github.com/pbhatia243/Neural_Conversation_Models" target="_blank" rel="external">Neural_Conversation_Models</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 9 - 旧版 tf.contrib.legacy_seq2seq API 介绍]]></title>
      <url>http://sggo.me/2017/11/19/chatbot/chatbot-research9/</url>
      <content type="html"><![CDATA[<p>有了对代码的深层次理解，我们之后构建 Chatbot 系统的时候有很大的帮助。</p>
<a id="more"></a>
<blockquote>
<p>旧的seq2seq接口也就是tf.contrib.legacy_seq2seq下的那部分，新的接口在tf.contrib.seq2seq下。</p>
<p>新seq2seq接口与旧的相比最主要的区别是它是动态展开的，而旧的是静态展开的。</p>
<p>静态展开(static unrolling) ：指的是定义模型创建graph的时候，序列的长度是固定的，之后传入的所有序列都得是定义时指定的长度。这样所有的句子都要padding到指定的长度，很浪费存储空间，计算效率也不高。但想处理变长序列，也是有办法的，需要预先指定一系列的buckets，如</p>
</blockquote>
<h2 id="函数部分"><a href="#函数部分" class="headerlink" title="函数部分"></a>函数部分</h2><p><a href="https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py" target="_blank" rel="external">旧版legacy_seq2seq代码</a></p>
<p>首先看一下这个文件的组成，主要包含下面几个函数：</p>
<blockquote>
<ul>
<li>def _extract_argmax_and_embed(embedding, …</li>
<li>def rnn_decoder(decoder_inputs, initial_state, …</li>
<li>def basic_rnn_seq2seq(encoder_inputs, … </li>
<li>def tied_rnn_seq2seq(encoder_inputs, …</li>
<li>def embedding_rnn_seq2seq(encoder_inputs, …</li>
<li>def embedding_tied_rnn_seq2seq(encoder_inputs, …</li>
<li>def attention_decoder(decoder_inputs, …</li>
<li>def embedding_attention_decoder(decoder_inputs, …</li>
<li>def embedding_attention_seq2seq(encoder_inputs, …</li>
<li>def one2many_rnn_seq2seq(encoder_inputs, …</li>
<li>def sequence_loss_by_example(logits, …</li>
<li>def sequence_loss(logits, …</li>
<li>def model_with_buckets(encoder_inputs, …</li>
</ul>
</blockquote>
<p><strong>可以看到按照调用关系和功能不同可以分成下面的结构</strong>：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model_with_buckets</span><br><span class="line">│</span><br><span class="line">├── seq2seq函数</span><br><span class="line">│   </span><br><span class="line">│   ├── basic_rnn_seq2seq</span><br><span class="line">│   │   ├── rnn_decoder</span><br><span class="line">│   └── tied_rnn_seq2seq</span><br><span class="line">│   ├── embedding_tied_rnn_seq2seq</span><br><span class="line">│   └── embedding_rnn_seq2seq</span><br><span class="line">│   │   ├── embedding_rnn_decoder</span><br><span class="line">│   ├── embedding_attention_seq2seq</span><br><span class="line">│   │   ├── embedding_attention_decoder</span><br><span class="line">│   │   │   ├── attention_decoder</span><br><span class="line">│   │   │   ├── attention</span><br><span class="line">│   └── one2many_rnn_seq2seq</span><br><span class="line">│   </span><br><span class="line">└── loss函数</span><br><span class="line">    ├── sequence_loss_by_example</span><br><span class="line">    ├── sequence_loss</span><br></pre></td></tr></table></figure>
<h3 id="model-with-buckets-函数"><a href="#model-with-buckets-函数" class="headerlink" title="model_with_buckets()函数"></a>model_with_buckets()函数</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_with_buckets</span><span class="params">(encoder_inputs,</span><br><span class="line">                      decoder_inputs,</span><br><span class="line">                      targets,</span><br><span class="line">                      weights,</span><br><span class="line">                      buckets,</span><br><span class="line">                      seq2seq,</span><br><span class="line">                      softmax_loss_function=None,</span><br><span class="line">                      per_example_loss=False,</span><br><span class="line">                      name=None)</span>:</span></span><br></pre></td></tr></table></figure>
<p>这个函数，目的是为了减少计算量和加快模型计算速度，然后由于这部分代码比较古老，你会发现有些地方还在使用static_rnn()这种函数，其实新版的tf中引入dynamic_rnn之后就不需要这么做了。</p>
<p>分析一下，其实思路很简单，就是将输入长度分成不同的间隔，这样数据的在填充时只需要填充到相应的bucket长度即可，不需要都填充到最大长度。</p>
<p>比如 buckets 取 <code>[(5，10), (10，20),(20，30)...]</code> 每个 bucket 的</p>
<ol>
<li>第一个数字表示 source 填充的长度</li>
<li>第二个数字表示 target 填充的长度</li>
</ol>
<p>举个🌰 eg：<strong>‘我爱你’–&gt;‘I love you’</strong>， 应该会被分配到第一个bucket中</p>
<p>然后‘我爱你’会被pad成长度为5的序列，‘I love you’会被pad成长度为10的序列。其实就是每个bucket表示一个模型的参数配置。这样对每个bucket都构造一个模型，然后训练时取相应长度的序列进行，而这些模型将会共享参数。其实这一部分可以参考现在的dynamic_rnn来进行理解，dynamic_rnn是对每个batch的数据将其pad至本batch中长度最大的样本，而bucket则是在数据预处理环节先对数据长度进行聚类操作。</p>
<p>我们再看一下该函数的参数和内部实现：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">encoder_inputs: encoder的输入，一个tensor的列表。列表中每一项都是encoder时的一个词（batch）。</span><br><span class="line">decoder_inputs: decoder的输入，同上</span><br><span class="line">targets:        目标值，与decoder_input只相差一个&lt;EOS&gt;符号，int32型</span><br><span class="line">weights:        目标序列长度值的mask标志，如果是padding则weight=<span class="number">0</span>，否则weight=<span class="number">1</span></span><br><span class="line">buckets:        就是定义的bucket值，是一个列表：[(<span class="number">5</span>，<span class="number">10</span>), (<span class="number">10</span>，<span class="number">20</span>),(<span class="number">20</span>，<span class="number">30</span>)...]</span><br><span class="line">seq2seq:        定义好的seq2seq模型，可以使用后面介绍的embedding_attention_seq2seq，embedding_rnn_seq2seq，basic_rnn_seq2seq等</span><br><span class="line">softmax_loss_function: 计算误差的函数，(labels, logits)，默认为sparse_softmax_cross_entropy_with_logits</span><br><span class="line">per_example_loss: 如果为真，则调用sequence_loss_by_example，返回一个列表，其每个元素就是一个样本的loss值。如果为假，则调用sequence_loss函数，对一个batch的样本只返回一个求和的loss值，具体见后面的分析</span><br><span class="line">name: Optional name <span class="keyword">for</span> this operation, defaults to <span class="string">"model_with_buckets"</span>.</span><br></pre></td></tr></table></figure>
<p>内部代码这里不会全部贴上来，捡关键的说一下：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#保存每个bucket对应的loss和output    </span></span><br><span class="line">losses = []</span><br><span class="line">outputs = []</span><br><span class="line"><span class="keyword">with</span> ops.name_scope(name, <span class="string">"model_with_buckets"</span>, all_inputs):</span><br><span class="line"><span class="comment">#对每个bucket都要选择数据进行构建模型</span></span><br><span class="line"><span class="keyword">for</span> j, bucket <span class="keyword">in</span> enumerate(buckets):</span><br><span class="line"> <span class="comment">#buckets之间的参数要进行复用</span></span><br><span class="line"> <span class="keyword">with</span> variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=<span class="keyword">True</span> <span class="keyword">if</span> j &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="keyword">None</span>):</span><br><span class="line"></span><br><span class="line">   <span class="comment">#调用seq2seq进行解码得到输出，这里需要注意的是，encoder_inputs和decoder_inputs是定义好的placeholder，</span></span><br><span class="line">   <span class="comment">#都是长度为序列最大长度的列表（也就是最大的那个buckets的长度），按上面的例子，这两个placeholder分别是长度为20和30的列表。</span></span><br><span class="line">   <span class="comment">#在构建模型时，对于每个bucket，只取其对应的长度个placeholder即可，如对于（5,10）这个bucket，就取前5/10个placeholder进行构建模型</span></span><br><span class="line">   bucket_outputs, _ = seq2seq(encoder_inputs[:bucket[<span class="number">0</span>]], decoder_inputs[:bucket[<span class="number">1</span>]])</span><br><span class="line">   outputs.append(bucket_outputs)</span><br><span class="line">   <span class="comment">#如果指定per_example_loss则调用sequence_loss_by_example，losses添加的是一个batch_size大小的列表</span></span><br><span class="line">   <span class="keyword">if</span> per_example_loss:</span><br><span class="line">     losses.append(</span><br><span class="line">         sequence_loss_by_example(</span><br><span class="line">             outputs[<span class="number">-1</span>],</span><br><span class="line">             targets[:bucket[<span class="number">1</span>]],</span><br><span class="line">             weights[:bucket[<span class="number">1</span>]],</span><br><span class="line">             softmax_loss_function=softmax_loss_function))</span><br><span class="line">   <span class="comment">#否则调用sequence_loss，对上面的结果进行求和，losses添加的是一个值</span></span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">     losses.append(</span><br><span class="line">         sequence_loss(</span><br><span class="line">             outputs[<span class="number">-1</span>],</span><br><span class="line">             targets[:bucket[<span class="number">1</span>]],</span><br><span class="line">             weights[:bucket[<span class="number">1</span>]],</span><br><span class="line">             softmax_loss_function=softmax_loss_function))</span><br></pre></td></tr></table></figure>
<p>函数的输出为outputs和losses，其tensor的shape见上面解释。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py" target="_blank" rel="external">官网代码</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27769667" target="_blank" rel="external">Tensorflow源码解读（一）：Attention Seq2Seq模型</a></li>
<li><a href="http://complx.me/2016-06-28-easy-seq2seq/" target="_blank" rel="external">Chatbots with Seq2Seq</a></li>
<li><a href="https://lan2720.github.io/2017/03/10/tensorflow%E7%9A%84legacy-seq2seq/" target="_blank" rel="external">tensorflow的legacy_seq2seq</a></li>
<li><a href="https://github.com/tensorflow/nmt#tips--tricks" target="_blank" rel="external">Neural Machine Translation (seq2seq) Tutorial</a></li>
<li><a href="https://blog.csdn.net/thriving_fcl/article/details/74165062" target="_blank" rel="external">Tensorflow新版Seq2Seq接口使用</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 8 - 理论 seq2seq+Attention 机制模型详解]]></title>
      <url>http://sggo.me/2017/11/17/chatbot/chatbot-research8/</url>
      <content type="html"><![CDATA[<p>从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。</p>
<a id="more"></a>
<ul>
<li>Seq-to-Seq 框架1</li>
<li>Seq-to-Seq 框架2（teacher forcing）</li>
<li>Seq-to-Seq with Attention（NMT）</li>
<li>Seq-to-Seq with Attention 各种变形</li>
<li>Seq-to-Seq with Beam-Search</li>
</ul>
<p>当输入输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder）<a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">1</a> 或者 seq2seq 模型 <a href="https://zh.gluon.ai/chapter_recurrent-neural-networks/gru.html" target="_blank" rel="external">2</a>。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。</p>
<h2 id="1-Seq2Seq-框架1"><a href="#1-Seq2Seq-框架1" class="headerlink" title="1. Seq2Seq 框架1"></a>1. Seq2Seq 框架1</h2><p><img src="/images/chatbot/seq2seq-1.jpg" width="300"></p>
<p><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p>
<h3 id="1-1-encode-编码器"><a href="#1-1-encode-编码器" class="headerlink" title="1.1 encode 编码器"></a>1.1 encode 编码器</h3><p>编码器的作用是把一个不定长的输入序列变换成一个定长的背景变量 $c$，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。</p>
<p>让我们考虑批量大小为 1 的时序数据样本。假设输入序列是 $x_1,\ldots,x_T$, 例如 $x_i$ 是输入句子中的第 $i$ 个词。在时间步 $t$，循环神经网络将输入 $x_t$ 的特征向量 $x_t$ 和上个时间步的隐藏状态 $\boldsymbol{h}_{t-1}$ 变换为当前时间步的隐藏状态 $h_t$。我们可以用函数 $f$ 表达循环神经网络隐藏层的变换：</p>
<p>$$<br>\boldsymbol{h}_t = f(\boldsymbol{x}_t, \boldsymbol{h}_{t-1}).<br>$$</p>
<p>接下来编码器通过自定义函数 $q$ 将各个时间步的隐藏状态变换为背景变量</p>
<p>$$<br>\boldsymbol{c} =  q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T).<br>$$</p>
<p>例如，当选择 $q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) = \boldsymbol{h}_T$ 时，背景变量是输入序列最终时间步的隐藏状态 $\boldsymbol{h}_T$。</p>
<p>以上描述的编码器是一个单向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用 Bi-RNN 构造编码器。这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。</p>
<p><img src="/images/chatbot/seq2seq-5.jpeg" width="800"></p>
<h3 id="1-2-decode-解码器"><a href="#1-2-decode-解码器" class="headerlink" title="1.2 decode 解码器"></a>1.2 decode 解码器</h3><p>Encode 编码器输出的背景变量 $c$ 编码了整个输入序列 $x_1, \ldots, x_T$ 的信息。给定训练样本中的输出序列 $y_1, y_2, \ldots, y_{T’}$，对每个时间步 $t’$（符号与输入序列或编码器的时间步 $t$ 有区别）， 解码器输出 $y_{t’}$ 的条件概率将基于之前的输出序列 $y_1,\ldots,y_{t’-1}$ 和背景变量 $c$，<strong>即</strong> $\mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c})$。</p>
<p>为此，我们可以使用<code>另一个RNN</code>作为解码器。 在输出序列的时间步 $t^\prime$，解码器将上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $c$ 作为输入，并将它们与上一时间步的隐藏状态 $\boldsymbol{h}_{t^\prime-1}$ 变换为当前时间步的隐藏状态 $\boldsymbol{h}_{t^\prime}$。因此，我们可以用函数 $g$ 表达解码器隐藏层的变换：</p>
<p>$$<br>\boldsymbol{h}_{t^\prime} = g(y_{t^\prime-1}, \boldsymbol{c}, \boldsymbol{h}_{t^\prime-1}).<br>$$</p>
<p>有了decode的隐藏状态后，我们可以使用自定义的输出层和 softmax 运算来计算 $\mathbb{P}(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})$，例如基于当前时间步的解码器隐藏状态 $\boldsymbol{h}_{t^\prime}$、上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $c$ 来计算当前时间步输出 $y_{t^\prime}$ 的概率分布。</p>
<h3 id="1-3-train-模型训练"><a href="#1-3-train-模型训练" class="headerlink" title="1.3 train 模型训练"></a>1.3 train 模型训练</h3><p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>
<p>$$<br>\begin{split}\begin{aligned}<br>\mathbb{P}(y_1, \ldots, y_{T’} \mid x_1, \ldots, x_T)<br>&amp;= \prod_{t’=1}^{T’} \mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, x_1, \ldots, x_T)\\<br>&amp;= \prod_{t’=1}^{T’} \mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c}),<br>\end{aligned}\end{split}<br>$$</p>
<p>并得到该输出序列的损失</p>
<p>$$ - \log\mathbb{P}(y_1, \ldots, y_{T’} \mid x_1, \ldots, x_T) = -\sum_{t’=1}^{T’} \log \mathbb{P}(y_{t’} \mid y_1,  \ldots, y_{t’-1}, \boldsymbol{c}),<br>$$</p>
<p><img src="/images/chatbot/seq2seq-6.png" width="800"></p>
<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图中所描述的模型预测中，我们需要将decode在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列在上一个时间步的标签作为decode在当前时间步的输入。这叫做强制教学（teacher forcing）。</p>
<h3 id="1-4-小结"><a href="#1-4-小结" class="headerlink" title="1.4 小结"></a>1.4 小结</h3><ul>
<li>编码器 - 解码器（seq2seq）可以输入并输出不定长的序列。</li>
<li>编码器—解码器使用了两个循环神经网络。</li>
<li>在编码器—解码器的训练中，我们可以采用强制教学。 （这也是 Seq2Seq 2 的内容）</li>
</ul>
<h2 id="2-Seq2Seq-框架2"><a href="#2-Seq2Seq-框架2" class="headerlink" title="2. Seq2Seq 框架2"></a>2. Seq2Seq 框架2</h2><p>第二个要讲的Seq-to-Seq模型来自于 “<a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a>”，其模型结构图如下所示：</p>
<p><img src="/images/chatbot/seq2seq-2.jpg" width="600"></p>
<p>与上面模型最大的区别在于其source编码后的 向量$C$ 直接作为Decoder阶段RNN的初始化state，而不是在每次decode时都作为<code>RNN cell</code>的输入。此外，decode时RNN的输入是目标值，而不是前一时刻的输出。首先看一下编码阶段：</p>
<p><img src="/images/chatbot/seq2seq-3.jpg" width="500"></p>
<p>就是简单的RNN模型，每个词经过RNN之后都会编码为hidden state（e0,e1,e2），并且source序列的编码向量e就是最终的hidden state e2。接下来再看一下解码阶段：</p>
<p><img src="/images/chatbot/seq2seq-4.jpg" width="500"></p>
<p>e向量仅作为RNN的初始化状态传入decode模型。接下来就是标准的循环神经网络，每一时刻输入都是前一时刻的正确label。直到最终输入<eos>符号截止滚动。</eos></p>
<h2 id="3-Seq2Seq-Attention"><a href="#3-Seq2Seq-Attention" class="headerlink" title="3. Seq2Seq Attention"></a>3. Seq2Seq Attention</h2><p><strong>decode</strong> 在各个时间步依赖相同的 <strong>背景变量 $c$</strong> 来获取输入序列信息。当 <strong>encode</strong> 为 RNN 时，<strong>背景变量$c$</strong> 来自它最终时间步的隐藏状态。</p>
<blockquote>
<p>英语输入：“They”、“are”、“watching”、“.”<br>法语输出：“Ils”、“regardent”、“.”</p>
<p>翻译例子：输入为英语序列“They”、“are”、“watching”、“.”，输出为法语序列“Ils”、“regardent”、“.”。，<strong>decode</strong> 在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步 1，解码器可以主要依赖“They”、“are”的信息来生成“Ils”，在时间步 2 则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步 3 则直接映射句号“.”。这看上去就像是在 <strong>decode</strong> 的每一时间步对输入序列中不同时间步的编码信息分配不同的注意力一样。这也是注意力机制的由来 <a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">1</a>。</p>
<p>仍以 RNN 为例，Attention 通过对 Encode 所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量$c$。Decode 在每一时间步调整这些权重，即 Attention weight，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量$c$。本节我们将讨论 Attention机制 是怎么工作的。</p>
</blockquote>
<p>在“编码器—解码器（seq2seq）”, 解码器在时间步 $t’$ 的隐藏状态</p>
<p>$$<br>\boldsymbol{s}_{t’} = g(\boldsymbol{y}_{t’-1}, \boldsymbol{c}, \boldsymbol{s}_{t’-1})<br>$$</p>
<p>在 Attention机制 中, 解码器的每一时间步将使用可变的背景变量$c$</p>
<p>$$<br>\boldsymbol{s}_{t’} = g(\boldsymbol{y}_{t’-1}, \boldsymbol{c}_{t’}, \boldsymbol{s}_{t’-1}).<br>$$</p>
<p>关键是如何计算背景变量 $\boldsymbol{c}_{t’}$ 和如何利用它来更新隐藏状态 $\boldsymbol{s}_{t’}$。以下将分别描述这两个关键点。</p>
<h3 id="3-1-计算背景变量-c"><a href="#3-1-计算背景变量-c" class="headerlink" title="3.1 计算背景变量 c"></a>3.1 计算背景变量 c</h3><p>$$<br>\boldsymbol{c}_{t’} = \sum_{t=1}^T \alpha_{t’ t} \boldsymbol{h}_t,<br>$$</p>
<p>其中给定 $t’$ 时，权重 $\alpha_{t’ t}$ 在 $t=1,\ldots,T$ 的值是一个概率分布。为了得到概率分布，可以使用 softmax 运算:</p>
<p>$$<br>\alpha_{t’ t} = \frac{\exp(e_{t’ t})}{ \sum_{k=1}^T \exp(e_{t’ k}) },\quad t=1,\ldots,T.<br>$$</p>
<p>现在，我们需要定义如何计算上式中 softmax 运算的输入 $e_{t’ t}$。由于 $e_{t’ t}$ 同时取决于decode的时间步 $t’$ 和encode的时间步 $t$，我们不妨以解码器在时间步 $t’−1$ 的隐藏状态 $\boldsymbol{s}_{t’ - 1}$ 与编码器在时间步 $t$ 的隐藏状态 $h_t$ 为输入，并通过函数 $a$ 计算 $e_{t’ t}$：</p>
<p>$$<br>e_{t’ t} = a(\boldsymbol{s}_{t’ - 1}, \boldsymbol{h}_t).<br>$$</p>
<p>这里函数 a 有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积 $a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出Attention机制的论文则将输入连结后通过含单隐藏层的多层感知机MLP 变换 </p>
<p>$$<br>a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),<br>$$</p>
<p>其中 $v、W_s、W_h$ 都是可以学习的模型参数。</p>
<h3 id="3-2-更新隐藏状态"><a href="#3-2-更新隐藏状态" class="headerlink" title="3.2 更新隐藏状态"></a>3.2 更新隐藏状态</h3><p>以门控循环单元为例，在解码器中我们可以对门控循环单元的设计稍作修改。解码器在时间步 $t’$ 的隐藏状态为</p>
<p>$$<br>\boldsymbol{s}_{t’} = \boldsymbol{z}_{t’} \odot \boldsymbol{s}_{t’-1}  + (1 - \boldsymbol{z}_{t’}) \odot \tilde{\boldsymbol{s}}_{t’},<br>$$</p>
<p>其中的重置门、更新门和候选隐含状态分别为 :</p>
<p>$$<br>\begin{split}\begin{aligned}<br>\boldsymbol{r}_{t’} &amp;= \sigma(\boldsymbol{W}_{yr} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{sr} \boldsymbol{s}_{t’ - 1} + \boldsymbol{W}_{cr} \boldsymbol{c}_{t’} + \boldsymbol{b}_r),\\<br>\boldsymbol{z}_{t’} &amp;= \sigma(\boldsymbol{W}_{yz} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{sz} \boldsymbol{s}_{t’ - 1} + \boldsymbol{W}_{cz} \boldsymbol{c}_{t’} + \boldsymbol{b}_z),\\<br>\tilde{\boldsymbol{s}}_{t’} &amp;= \text{tanh}(\boldsymbol{W}_{ys} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{ss} (\boldsymbol{s}_{t’ - 1} \odot \boldsymbol{r}_{t’}) + \boldsymbol{W}_{cs} \boldsymbol{c}_{t’} + \boldsymbol{b}_s),<br>\end{aligned}\end{split}<br>$$</p>
<p>其中含下标的 W 和 b 分别为门控循环单元的权重参数和偏差参数。</p>
<p><img src="/images/chatbot/seq2seq-7.jpeg" width="800"></p>
<h3 id="3-3-小结"><a href="#3-3-小结" class="headerlink" title="3.3 小结"></a>3.3 小结</h3><ul>
<li>可以在decode的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。</li>
<li>Attention机制可以采用更为高效的矢量化计算。</li>
</ul>
<h2 id="4-Seq2Seq-Attention各种变形"><a href="#4-Seq2Seq-Attention各种变形" class="headerlink" title="4. Seq2Seq Attention各种变形"></a>4. Seq2Seq Attention各种变形</h2><p>第四个Seq-to-Seq模型，来自于论文 <a href="http://link.zhihu.com/?target=http%3A//aclweb.org/anthology/D15-1166" target="_blank" rel="external">Effective Approaches to Attention-based Neural Machine Translation</a> 这篇论文提出了两种 Seq2Seq模型 分别是global Attention 和 local Attention。</p>
<h2 id="5-Seq2Seq-with-Beam-Search"><a href="#5-Seq2Seq-with-Beam-Search" class="headerlink" title="5. Seq2Seq with Beam-Search"></a>5. Seq2Seq with Beam-Search</h2><p>上面讲的几种Seq2Seq模型都是从模型结构上进行的改进，也就说为了从训练的层面上改善模型的效果，但这里要介绍的beam-search是在测试的时候才用到的技术。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制</a></li>
<li><a href="https://zh.gluon.ai/chapter_recurrent-neural-networks/gru.html" target="_blank" rel="external">门控循环单元（GRU）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32092871" target="_blank" rel="external">seq2seq+Attention机制模型详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/38064637" target="_blank" rel="external">三分钟带你对 Softmax 划重点</a></li>
<li><a href="https://zh.gluon.ai/chapter_deep-learning-basics/softmax-regression.html" target="_blank" rel="external">Softmax 回归</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[理论 seq2seq+Attention 机制模型详解]]></title>
      <url>http://sggo.me/2017/11/17/nlp/seq2seq+Attention/</url>
      <content type="html"><![CDATA[<p>从具体的模型细节、公式推导、结构图以及变形等几个方向详细介绍一下 Seq-to-Seq 模型。</p>
<a id="more"></a>
<ul>
<li>Seq-to-Seq 框架1</li>
<li>Seq-to-Seq 框架2（teacher forcing）</li>
<li>Seq-to-Seq with Attention（NMT）</li>
<li>Seq-to-Seq with Attention 各种变形</li>
<li>Seq-to-Seq with Beam-Search</li>
</ul>
<p>当输入输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder）<a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">1</a> 或者 seq2seq 模型 <a href="https://zh.gluon.ai/chapter_recurrent-neural-networks/gru.html" target="_blank" rel="external">2</a>。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。</p>
<h2 id="1-Seq2Seq-框架1"><a href="#1-Seq2Seq-框架1" class="headerlink" title="1. Seq2Seq 框架1"></a>1. Seq2Seq 框架1</h2><p><img src="/images/chatbot/seq2seq-1.jpg" width="300"></p>
<p><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p>
<h3 id="1-1-encode-编码器"><a href="#1-1-encode-编码器" class="headerlink" title="1.1 encode 编码器"></a>1.1 encode 编码器</h3><p>编码器的作用是把一个不定长的输入序列变换成一个定长的背景变量 $c$，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。</p>
<p>让我们考虑批量大小为 1 的时序数据样本。假设输入序列是 $x_1,\ldots,x_T$, 例如 $x_i$ 是输入句子中的第 $i$ 个词。在时间步 $t$，循环神经网络将输入 $x_t$ 的特征向量 $x_t$ 和上个时间步的隐藏状态 $\boldsymbol{h}_{t-1}$ 变换为当前时间步的隐藏状态 $h_t$。我们可以用函数 $f$ 表达循环神经网络隐藏层的变换：</p>
<p>$$<br>\boldsymbol{h}_t = f(\boldsymbol{x}_t, \boldsymbol{h}_{t-1}).<br>$$</p>
<p>接下来编码器通过自定义函数 $q$ 将各个时间步的隐藏状态变换为背景变量</p>
<p>$$<br>\boldsymbol{c} =  q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T).<br>$$</p>
<p>例如，当选择 $q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) = \boldsymbol{h}_T$ 时，背景变量是输入序列最终时间步的隐藏状态 $\boldsymbol{h}_T$。</p>
<p>以上描述的编码器是一个单向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用 Bi-RNN 构造编码器。这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。</p>
<p><img src="/images/chatbot/seq2seq-5.jpeg" width="800"></p>
<h3 id="1-2-decode-解码器"><a href="#1-2-decode-解码器" class="headerlink" title="1.2 decode 解码器"></a>1.2 decode 解码器</h3><p>Encode 编码器输出的背景变量 $c$ 编码了整个输入序列 $x_1, \ldots, x_T$ 的信息。给定训练样本中的输出序列 $y_1, y_2, \ldots, y_{T’}$，对每个时间步 $t’$（符号与输入序列或编码器的时间步 $t$ 有区别）， 解码器输出 $y_{t’}$ 的条件概率将基于之前的输出序列 $y_1,\ldots,y_{t’-1}$ 和背景变量 $c$，<strong>即</strong> $\mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c})$。</p>
<p>为此，我们可以使用<code>另一个RNN</code>作为解码器。 在输出序列的时间步 $t^\prime$，解码器将上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $c$ 作为输入，并将它们与上一时间步的隐藏状态 $\boldsymbol{h}_{t^\prime-1}$ 变换为当前时间步的隐藏状态 $\boldsymbol{h}_{t^\prime}$。因此，我们可以用函数 $g$ 表达解码器隐藏层的变换：</p>
<p>$$<br>\boldsymbol{h}_{t^\prime} = g(y_{t^\prime-1}, \boldsymbol{c}, \boldsymbol{h}_{t^\prime-1}).<br>$$</p>
<p>有了decode的隐藏状态后，我们可以使用自定义的输出层和 softmax 运算来计算 $\mathbb{P}(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})$，例如基于当前时间步的解码器隐藏状态 $\boldsymbol{h}_{t^\prime}$、上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $c$ 来计算当前时间步输出 $y_{t^\prime}$ 的概率分布。</p>
<h3 id="1-3-train-模型训练"><a href="#1-3-train-模型训练" class="headerlink" title="1.3 train 模型训练"></a>1.3 train 模型训练</h3><p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>
<p>$$<br>\begin{split}\begin{aligned}<br>\mathbb{P}(y_1, \ldots, y_{T’} \mid x_1, \ldots, x_T)<br>&amp;= \prod_{t’=1}^{T’} \mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, x_1, \ldots, x_T)\\<br>&amp;= \prod_{t’=1}^{T’} \mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c}),<br>\end{aligned}\end{split}<br>$$</p>
<p>并得到该输出序列的损失</p>
<p>$$ - \log\mathbb{P}(y_1, \ldots, y_{T’} \mid x_1, \ldots, x_T) = -\sum_{t’=1}^{T’} \log \mathbb{P}(y_{t’} \mid y_1,  \ldots, y_{t’-1}, \boldsymbol{c}),<br>$$</p>
<p><img src="/images/chatbot/seq2seq-6.png" width="800"></p>
<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图中所描述的模型预测中，我们需要将decode在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列在上一个时间步的标签作为decode在当前时间步的输入。这叫做强制教学（teacher forcing）。</p>
<h3 id="1-4-小结"><a href="#1-4-小结" class="headerlink" title="1.4 小结"></a>1.4 小结</h3><ul>
<li>编码器 - 解码器（seq2seq）可以输入并输出不定长的序列。</li>
<li>编码器—解码器使用了两个循环神经网络。</li>
<li>在编码器—解码器的训练中，我们可以采用强制教学。 （这也是 Seq2Seq 2 的内容）</li>
</ul>
<h2 id="2-Seq2Seq-框架2"><a href="#2-Seq2Seq-框架2" class="headerlink" title="2. Seq2Seq 框架2"></a>2. Seq2Seq 框架2</h2><p>第二个要讲的Seq-to-Seq模型来自于 “<a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a>”，其模型结构图如下所示：</p>
<p><img src="/images/chatbot/seq2seq-2.jpg" width="600"></p>
<p>与上面模型最大的区别在于其source编码后的 向量$C$ 直接作为Decoder阶段RNN的初始化state，而不是在每次decode时都作为<code>RNN cell</code>的输入。此外，decode时RNN的输入是目标值，而不是前一时刻的输出。首先看一下编码阶段：</p>
<p><img src="/images/chatbot/seq2seq-3.jpg" width="500"></p>
<p>就是简单的RNN模型，每个词经过RNN之后都会编码为hidden state（e0,e1,e2），并且source序列的编码向量e就是最终的hidden state e2。接下来再看一下解码阶段：</p>
<p><img src="/images/chatbot/seq2seq-4.jpg" width="500"></p>
<p>e向量仅作为RNN的初始化状态传入decode模型。接下来就是标准的循环神经网络，每一时刻输入都是前一时刻的正确label。直到最终输入<eos>符号截止滚动。</eos></p>
<h2 id="3-Seq2Seq-Attention"><a href="#3-Seq2Seq-Attention" class="headerlink" title="3. Seq2Seq Attention"></a>3. Seq2Seq Attention</h2><p><strong>decode</strong> 在各个时间步依赖相同的 <strong>背景变量 $c$</strong> 来获取输入序列信息。当 <strong>encode</strong> 为 RNN 时，<strong>背景变量$c$</strong> 来自它最终时间步的隐藏状态。</p>
<blockquote>
<p>英语输入：“They”、“are”、“watching”、“.”<br>法语输出：“Ils”、“regardent”、“.”</p>
<p>翻译例子：输入为英语序列“They”、“are”、“watching”、“.”，输出为法语序列“Ils”、“regardent”、“.”。，<strong>decode</strong> 在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步 1，解码器可以主要依赖“They”、“are”的信息来生成“Ils”，在时间步 2 则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步 3 则直接映射句号“.”。这看上去就像是在 <strong>decode</strong> 的每一时间步对输入序列中不同时间步的编码信息分配不同的注意力一样。这也是注意力机制的由来 <a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">1</a>。</p>
<p>仍以 RNN 为例，Attention 通过对 Encode 所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量$c$。Decode 在每一时间步调整这些权重，即 Attention weight，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量$c$。本节我们将讨论 Attention机制 是怎么工作的。</p>
</blockquote>
<p>在“编码器—解码器（seq2seq）”, 解码器在时间步 $t’$ 的隐藏状态</p>
<p>$$<br>\boldsymbol{s}_{t’} = g(\boldsymbol{y}_{t’-1}, \boldsymbol{c}, \boldsymbol{s}_{t’-1})<br>$$</p>
<p>在 Attention机制 中, 解码器的每一时间步将使用可变的背景变量$c$</p>
<p>$$<br>\boldsymbol{s}_{t’} = g(\boldsymbol{y}_{t’-1}, \boldsymbol{c}_{t’}, \boldsymbol{s}_{t’-1}).<br>$$</p>
<p>关键是如何计算背景变量 $\boldsymbol{c}_{t’}$ 和如何利用它来更新隐藏状态 $\boldsymbol{s}_{t’}$。以下将分别描述这两个关键点。</p>
<h3 id="3-1-计算背景变量-c"><a href="#3-1-计算背景变量-c" class="headerlink" title="3.1 计算背景变量 c"></a>3.1 计算背景变量 c</h3><p>$$<br>\boldsymbol{c}_{t’} = \sum_{t=1}^T \alpha_{t’ t} \boldsymbol{h}_t,<br>$$</p>
<p>其中给定 $t’$ 时，权重 $\alpha_{t’ t}$ 在 $t=1,\ldots,T$ 的值是一个概率分布。为了得到概率分布，可以使用 softmax 运算:</p>
<p>$$<br>\alpha_{t’ t} = \frac{\exp(e_{t’ t})}{ \sum_{k=1}^T \exp(e_{t’ k}) },\quad t=1,\ldots,T.<br>$$</p>
<p>现在，我们需要定义如何计算上式中 softmax 运算的输入 $e_{t’ t}$。由于 $e_{t’ t}$ 同时取决于decode的时间步 $t’$ 和encode的时间步 $t$，我们不妨以解码器在时间步 $t’−1$ 的隐藏状态 $\boldsymbol{s}_{t’ - 1}$ 与编码器在时间步 $t$ 的隐藏状态 $h_t$ 为输入，并通过函数 $a$ 计算 $e_{t’ t}$：</p>
<p>$$<br>e_{t’ t} = a(\boldsymbol{s}_{t’ - 1}, \boldsymbol{h}_t).<br>$$</p>
<p>这里函数 a 有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积 $a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出Attention机制的论文则将输入连结后通过含单隐藏层的多层感知机MLP 变换 </p>
<p>$$<br>a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),<br>$$</p>
<p>其中 $v、W_s、W_h$ 都是可以学习的模型参数。</p>
<h3 id="3-2-更新隐藏状态"><a href="#3-2-更新隐藏状态" class="headerlink" title="3.2 更新隐藏状态"></a>3.2 更新隐藏状态</h3><p>以门控循环单元为例，在解码器中我们可以对门控循环单元的设计稍作修改。解码器在时间步 $t’$ 的隐藏状态为</p>
<p>$$<br>\boldsymbol{s}_{t’} = \boldsymbol{z}_{t’} \odot \boldsymbol{s}_{t’-1}  + (1 - \boldsymbol{z}_{t’}) \odot \tilde{\boldsymbol{s}}_{t’},<br>$$</p>
<p>其中的重置门、更新门和候选隐含状态分别为 :</p>
<p>$$<br>\begin{split}\begin{aligned}<br>\boldsymbol{r}_{t’} &amp;= \sigma(\boldsymbol{W}_{yr} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{sr} \boldsymbol{s}_{t’ - 1} + \boldsymbol{W}_{cr} \boldsymbol{c}_{t’} + \boldsymbol{b}_r),\\<br>\boldsymbol{z}_{t’} &amp;= \sigma(\boldsymbol{W}_{yz} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{sz} \boldsymbol{s}_{t’ - 1} + \boldsymbol{W}_{cz} \boldsymbol{c}_{t’} + \boldsymbol{b}_z),\\<br>\tilde{\boldsymbol{s}}_{t’} &amp;= \text{tanh}(\boldsymbol{W}_{ys} \boldsymbol{y}_{t’-1} + \boldsymbol{W}_{ss} (\boldsymbol{s}_{t’ - 1} \odot \boldsymbol{r}_{t’}) + \boldsymbol{W}_{cs} \boldsymbol{c}_{t’} + \boldsymbol{b}_s),<br>\end{aligned}\end{split}<br>$$</p>
<p>其中含下标的 W 和 b 分别为门控循环单元的权重参数和偏差参数。</p>
<p><img src="/images/chatbot/seq2seq-7.jpeg" width="800"></p>
<h3 id="3-3-小结"><a href="#3-3-小结" class="headerlink" title="3.3 小结"></a>3.3 小结</h3><ul>
<li>可以在decode的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。</li>
<li>Attention机制可以采用更为高效的矢量化计算。</li>
</ul>
<h2 id="4-Seq2Seq-Attention各种变形"><a href="#4-Seq2Seq-Attention各种变形" class="headerlink" title="4. Seq2Seq Attention各种变形"></a>4. Seq2Seq Attention各种变形</h2><p>第四个Seq-to-Seq模型，来自于论文 <a href="http://link.zhihu.com/?target=http%3A//aclweb.org/anthology/D15-1166" target="_blank" rel="external">Effective Approaches to Attention-based Neural Machine Translation</a> 这篇论文提出了两种 Seq2Seq模型 分别是global Attention 和 local Attention。</p>
<h2 id="5-Seq2Seq-with-Beam-Search"><a href="#5-Seq2Seq-with-Beam-Search" class="headerlink" title="5. Seq2Seq with Beam-Search"></a>5. Seq2Seq with Beam-Search</h2><p>上面讲的几种Seq2Seq模型都是从模型结构上进行的改进，也就说为了从训练的层面上改善模型的效果，但这里要介绍的beam-search是在测试的时候才用到的技术。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="external">动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制</a></li>
<li><a href="https://zh.gluon.ai/chapter_recurrent-neural-networks/gru.html" target="_blank" rel="external">门控循环单元（GRU）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32092871" target="_blank" rel="external">seq2seq+Attention机制模型详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/38064637" target="_blank" rel="external">三分钟带你对 Softmax 划重点</a></li>
<li><a href="https://zh.gluon.ai/chapter_deep-learning-basics/softmax-regression.html" target="_blank" rel="external">Softmax 回归</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hidden Markov Model]]></title>
      <url>http://sggo.me/2017/11/14/nlp/hidden-markov-model/</url>
      <content type="html"><![CDATA[<p>Hidden Markov Model (HMM) model is the most rapid and effective method to solve most Natural Language Processing problems. It successfully solves the problems of speech recognition, Machine Translation.</p>
<a id="more"></a>
<p>HMM is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states.</p>
<h2 id="Communication-Model"><a href="#Communication-Model" class="headerlink" title="Communication Model"></a>Communication Model</h2><p><img src="/images/nlp/nlp-communication-model.png" alt=""></p>
<p>如何根据接收端的观测信号 $o_1,o_2,o_3,…$来推测信号源发送的信息 $s_1,s_2,s_3,…$ 呢?只需要从所有的源信息中找到最可能产生出观测信号的那一个信息。</p>
<blockquote>
<p>很多NLP应用可以这样理解。从汉语到英语的翻译中，说话者讲的是汉语，但是信道传播编码的方式是英语，如何利用计算机，根据接收到的英语信息，推测说话者汉语的意思，这就是机器翻译。同样，如果要根据带有拼写错误的语句推测说话者想表达的正确意思，那就是自动纠错，这样，几乎所有的NLP问题都可以等价成通信的解码问题。</p>
</blockquote>
<p>用概率论语言描述为 :</p>
<p>已知 $o_1,o_2,o_3$, 求得令条件概率$P(s_1,s_2,s_3,…|o_1,o_2,o_3,…)$达到最大值的信息串 $o_1,o_2,o_3,…$</p>
<p>$$<br>s_1,s_2,s_3,… = Argument.Max.P(s_1,s_2,s_3,…|o_1,o_2,o_3,…)<br>$$</p>
<p>贝叶斯转换 :</p>
<p>$$<br>\frac {P(o_1,o_2,o_3,…|s_1,s_2,s_3,…) \cdot P(s_1,s_2,s_3,…)} {P(o_1,o_2,o_3,…)}<br>$$</p>
<blockquote>
<p>$P(o_1,o_2,o_3,…)一个可以忽略的常数$</p>
</blockquote>
<p>$$<br>P(o_1,o_2,o_3,…|s_1,s_2,s_3,…) \cdot P(s_1,s_2,s_3,…)<br>$$</p>
<h2 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model"></a>Hidden Markov Model</h2><p>马尔科夫假设:</p>
<p>$$<br>P(s_{t}|s_1,s_2,s_3,…,s_{t-1}) = P(s_t|s_{t-1})<br>$$</p>
<blockquote>
<p>$s_1,s_2,s_3,…,s_{t}$ 看成是北京每天的最高气温，这里面的每个状态 $s_t$ 都是随机的， 假设随机过程的各个状态 $s_t$ 的概率分布只与它的前一个状态 $s_{t-1}$有关，即 $P(s_{t}|s_1,s_2,s_3,…,s_{t-1}) = P(s_t|s_{t-1})$。</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>《数学之美》 读书笔记 </li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[NLP 简介 & 统计语言模型]]></title>
      <url>http://sggo.me/2017/11/13/nlp/statistics-language-model/</url>
      <content type="html"><![CDATA[<p>1946年 计算机出现之后，计算机很多事情比人类做得好，那么机器是否能懂<strong>自然语言</strong>?</p>
<a id="more"></a>
<h2 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h2><p>NLP是一门集计算机科学，人工智能，语言学三者于一身的交叉性学科。她的终极研究目标是让计算机能够处理甚至是“理解”人类的自然语言，进而帮助人类解决一些现实生活中遇到的实际问题。这里的语言“理解”是一个很抽象也很哲学的概念。在 NLP 中，我们将对语言的“理解”定义为是学习一个能够解决具体问题的复杂函数的过程。</p>
<p>一些NLP技术的应用:</p>
<ul>
<li>简单的任务：拼写检查，关键词检索，同义词检索等</li>
<li>复杂的任务：信息提取、情感分析、文本分类等</li>
<li>更复杂任务：机器翻译、人机对话、QA系统</li>
</ul>
<h2 id="From-rules-to-statistics"><a href="#From-rules-to-statistics" class="headerlink" title="From rules to statistics"></a>From rules to statistics</h2><blockquote>
<p><strong>为什么要把NLP从机器学习的任务列表中单独抽取出来做为一门研究的对象？</strong></p>
<p>根本原因在于语言用于表达客观世界的内在复杂性和多义性。举一个简单的例子：”Jane hit June and then she [fell/ran]”。当she所伴随的动作不同（fell or ran），其所指代的对象也发生了变化（June or Jane）。这样的例子太多，显然我们无法通过枚举所有的规则来解决语言内在的复杂性。另一个多义性的例子是：”I made her duck”。我们可以理解为：”I cooked her a duck”，或是”I curved her a wooden duck”，也可以理解为：”I transformed her into a duck with some magic”。</p>
</blockquote>
<h2 id="Statistics-Language-Model"><a href="#Statistics-Language-Model" class="headerlink" title="Statistics Language Model"></a>Statistics Language Model</h2><p>自然语言逐渐演变成一种上下文信息表达和传递的方式，让计算机处理自然语言，一个基本的问题就是为自然语言这种上下文相关的特性建立数学模型。 这个数学模型也就是NLP说的 Statistics Language Model.</p>
<blockquote>
<ol>
<li>美联储主席昨天告诉媒体 7000 亿美金的救助资金将借给上百家银行、汽车公司。</li>
<li>美联储主席昨天 7000 亿美金的救助资金告诉媒体将借给上百家银行、汽车公司。</li>
<li>美联储主席昨天 告媒诉体 70 亿00美金的救助资金上百家银行将借给、汽车公司。</li>
</ol>
<p>上世纪70年代科学家们试图用规则文法判断句子是否合理。贾里尼克用统计模型解决方法更有效。</p>
</blockquote>
<p>如果 S 表示一连串特定顺序排列的词 $w_1$， $w_2$，…， $w_n$ ，换句话说，S 表示的是一个有意义的句子。机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为：</p>
<p>$$<br>P(S) = P(w_1)P(w_2|w_1)P(w_3| w_1 w_2)…P(w_n|w_1 w_2…w_{n-1})<br>$$</p>
<p>马尔可夫假设</p>
<p>$$<br>P(S) = P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_i|w_{i-1})…<br>$$</p>
<p>接下来如何估计 $P (w_i|w_{i-1})$。只要机器数一数这对词 $(w_i{-1}, w_i)$ 在统计的文本中出现了多少次，以及 $w_{i-1}$ 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,</p>
<p>$$<br>P(w_i|w_{i-1}) = \frac {P(w_{i-1}, w_i)} {P(w_{i-1})}<br>$$</p>
<blockquote>
<p>如何计算 ： </p>
<p>$$<br> \frac {P(w_{i-1}, w_i)} {P(w_{i-1})}<br>$$</p>
<p>根据大数定理，只要统计量足够，相对频度就等于概率</p>
</blockquote>
<h3 id="higher-order-language-model"><a href="#higher-order-language-model" class="headerlink" title="higher order language model"></a>higher order language model</h3><p>假定文本中的每个词 $w_i$ 和 前面N-1个词有关，而和更前面的词无关，这样当前词 $w_i$ 的概率值取决于前面 N-1个词 $P(w_{i-N+1}, w_{i-N+2}, …, w_{i-1})$</p>
<p>因此，</p>
<p>$$<br>P(w_{i}|w_{1}, w_{2}, …, w_{i-1}) = P(w_i | w_{i-N+1}, w_{i-N+2}, …, w_{i-1})<br>$$</p>
<blockquote>
<p>N元模型， N=2 时，为二元模型。 在实际中应用最多的是 N=3 的三元模型</p>
</blockquote>
<h3 id="zero-probability-smoothing-method"><a href="#zero-probability-smoothing-method" class="headerlink" title="zero probability, smoothing method"></a>zero probability, smoothing method</h3><p>解决统计样本不足的概率估计问题， 不可信的统计数据打折扣的一种概率估计方法。</p>
<blockquote>
<p>1953年有古德（I.J.Good）图灵（Turing）的方法而提出来的。其基本思想是：对于没有看见的事件，我们不能认为它发生的概率就是零，因此我们从概率的总量（Probability Mass）中，分配一个很小的比例给这些没有看见的事件。这样一来看的见概率总和就要小于1了，因此，需要将所有看见的事件概率调小一点。至于小多少，要根据“越是不可信的统计折扣越多”的方法进行。</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>《数学之美》 读书笔记 </li>
<li><a href="https://whiskytina.github.io/word2vec.html" target="_blank" rel="external">word2vec前世今生</a></li>
<li><a href="https://whiskytina.github.io/14947653164873.html" target="_blank" rel="external">CS224N NLP with Deep Learning: Lecture 1 课程笔记</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[搬瓦工VPS 配置 SSH 与 OpenVpn]]></title>
      <url>http://sggo.me/2017/11/12/ops/ops-bandwagonhost-ssh-openvpn/</url>
      <content type="html"><![CDATA[<p>搬瓦工VPS 配置 ssh 登录 与 iphone 配置使用 openvpn</p>
<a id="more"></a>
<h2 id="SSH-配置"><a href="#SSH-配置" class="headerlink" title="SSH 配置"></a>SSH 配置</h2><blockquote>
<p>ssh 登录搬瓦工机器</p>
<ol>
<li>stop server @Main controls</li>
<li>Root password modification</li>
<li>start Server</li>
<li>Root shell - interactive</li>
<li><p>vi /etc/ssh/sshd_config, add </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PermitRootLogin yes</span><br><span class="line">Port 22</span><br></pre></td></tr></table></figure>
</li>
<li><p>/etc/init.d/sshd restart</p>
</li>
<li>ssh root@ip</li>
</ol>
</blockquote>
<h2 id="OpenVpn-配置"><a href="#OpenVpn-配置" class="headerlink" title="OpenVpn 配置"></a>OpenVpn 配置</h2><blockquote>
<ol>
<li>在iOS设备上打开app store，搜索openvpn，install</li>
<li>打开 <a href="https://bandwagonhost.com/clientarea.php?action=products" target="_blank" rel="external">bandwagon my service pandel</a><br><code>My Service Pandel</code> -&gt; <code>KiwiVM Control Pandel</code> -&gt; <code>OpenVPN Server</code> -&gt; <code>install OpenVPN</code> -&gt; <code>Download Key Files</code></li>
<li><p>把 ca.crt、client1.crt、client1.key 证书放入 .ovpn 配置文件</p>
<p>在 .ovpn 文件尾部中新增 标签 <code>&lt;ca&gt;、&lt;cert&gt;、&lt;key&gt;</code> 标签</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ca.crt 文件内容复制到 &lt;ca&gt;和&lt;/ca&gt; 的中间，</span><br><span class="line">client1.crt 文件内容复制到 &lt;cert&gt;和&lt;/cert&gt; 的中间，</span><br><span class="line">client1.key 文件内容复制到 &lt;key&gt;和&lt;/key&gt; 的中间，</span><br><span class="line"></span><br><span class="line">修改完成后删除 .ovpn 配置文件中类似</span><br><span class="line"></span><br><span class="line">ca ca.crt</span><br><span class="line">cert client1.crt</span><br><span class="line">key client1.key</span><br></pre></td></tr></table></figure>
</li>
<li><p>.ovpn 通过 Airdrop 传到 iphone 手机里</p>
</li>
<li>打开OpenVPN，点那些绿色加号，将配置文件导入</li>
<li>向右滑动最下面那个白色滑块至蓝色，连接服务器。</li>
<li>连接成功。如果显示 Connected，表示连接成功了。</li>
</ol>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.yuntionly.com/" target="_blank" rel="external">www.yuntionly.com</a></li>
<li><a href="https://www.wisevpn.net/" target="_blank" rel="external">www.wisevpn.net</a></li>
<li><a href="https://www.banwago.com/797.html" target="_blank" rel="external">www.banwago.com</a></li>
<li><a href="https://www.godaddy.com/" target="_blank" rel="external">www.godaddy.com</a></li>
<li><a href="https://www.cnbanwagong.com/4.html" target="_blank" rel="external">搬瓦工中文网</a></li>
<li><a href="https://bwh1.net/" target="_blank" rel="external">搬瓦工购买页面</a></li>
<li><a href="http://ulis.me/archives/5909" target="_blank" rel="external">搬瓦工VPS续费的那些事</a></li>
<li><a href="https://www.igfw.net/archives/13042" target="_blank" rel="external">OpenVPN支持iOS啦</a></li>
<li><a href="https://www.igfw.net/archives/1974" target="_blank" rel="external">绿色便携汉化可保存密码的OpenVPN客户端</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[文字和语言 vs 数字和信息]]></title>
      <url>http://sggo.me/2017/11/08/nlp/word-language-number-info-history/</url>
      <content type="html"><![CDATA[<p>数字、文字、自然语言 一样，都是信息的载体。<a id="more"></a><br>语言和数字的产生为的是同一个目的 ：记录和传播信息。</p>
<blockquote>
<p>1948年，<a href="https://zh.wikipedia.org/wiki/克劳德·香农" target="_blank" rel="external">香农</a>提出信息论，人们才把 数学和语言 联系起来。</p>
</blockquote>
<h2 id="1-信息"><a href="#1-信息" class="headerlink" title="1. 信息"></a>1. 信息</h2><p><img src="/images/nlp/history-info-1.jpg" width="480" height="400" align="middle" img=""></p>
<blockquote>
<p>人类最早利用 voice 进行 通信</p>
</blockquote>
<p><img src="/images/nlp/history-info-2.png" width="650" height="100" align="middle" img=""></p>
<blockquote>
<p>人类文明的进步，需要表达的信息量越来越多，人类发明的自然语言</p>
<p>语言的出现是为了人类之间的通信。字母、文字、数字 是信息编码的不同单位。任何一种语言都是一种编码方式，语言的语法规则是编解码的算法。</p>
</blockquote>
<h2 id="2-文字和数字"><a href="#2-文字和数字" class="headerlink" title="2. 文字和数字"></a>2. 文字和数字</h2><p>当语言和词汇多到一定程度，人类大脑记不住所有词汇，高效记录信息的形式出现，人类便发明了 <strong>文字</strong></p>
<h3 id="文字"><a href="#文字" class="headerlink" title="文字"></a>文字</h3><p>古埃及，读音相同的词用同一个符号来记录。</p>
<p>文字按照<strong>聚类</strong>会带来歧义性，弄不清多义字在特定环境的含义，就要依靠<strong>上下文</strong>。</p>
<p>不同的文明，由于地域原因，历史上互相隔绝，便会有不同的文字。文明的融合与冲突，不同文明下的人们进行交流(通信)，那么<strong>翻译</strong>的需求便产生了。不同的文字系统在记录信息上的能力是等价的。</p>
<p>罗塞塔 Rosetta Stone 石碑 [古埃及象形文字、埃及拼音文字、古希腊文] 的破译对于 NLP 学者的两点指导意义 :</p>
<ol>
<li>信息的冗余是信息安全的保障</li>
<li>语言的数据，我们称之为 语料</li>
</ol>
<blockquote>
<p>Rosetta Stone, Google推出的翻译软件</p>
</blockquote>
<h3 id="数字"><a href="#数字" class="headerlink" title="数字"></a>数字</h3><p>祖先需要记录物件的个数越来越多，所以开始发明了计数系统，也就有了 数字。因为 10 个手指头，所以发明了 10 进制。</p>
<p>描述数字最有效的是 古印度人，他们发明了 10个 阿拉伯数字。数字的革命性在于它的简单有效，而且 标志着数字和文字的分离。这在客观上让 自然语言的研究 和 数学的研究 在几千年里没有重合的轨迹。</p>
<h2 id="3-文字和语言背后的数学"><a href="#3-文字和语言背后的数学" class="headerlink" title="3. 文字和语言背后的数学"></a>3. 文字和语言背后的数学</h2><p>从 <code>象形文字</code> 到 <code>拼音文字</code> 是一个飞跃，描述物体方式上，从外表进化到了抽象的概念，同时不自觉的采用了对 信息的编码。同时祖先对文字的编码还非常合理，<strong>常用字短，生僻字长</strong>。这完全符合信息论中的 最短编码理论。  </p>
<p>这种文字设计(其实是一种编码方法)带来的好处是写起来省时间、省材料。</p>
<blockquote>
<p>公元前26世纪,约4700年前，出现了楔形文字(一种拼音文字)</p>
</blockquote>
<p>在古代，在造纸术发明之前，人们说话还是类似白话文，文字书写要刻在 龟壳、石碑、竹简 等上，很费时间和材料，所以惜墨如金，使得古文非常简洁。 </p>
<p>类比信息科学 ：</p>
<ul>
<li>通信时: 如果信道较宽，信息不必压缩，可直接传递;    </li>
<li>通信时: 如果信道较窄，信息在传递前需要尽可能压缩，然后在接收端进行解压缩。</li>
</ul>
<p><code>词语</code> ： 是有限和封闭的集合 (其实可设置完备的编码规则)<br><code>语言</code> ： 是无限和开放的集合 (不可以设置完备的编码规则)</p>
<p>任何语言的都有语法规则覆盖不到的地方，这些不精确性，也造就了语言的丰富多彩。</p>
<h2 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h2><p>了解 文字、数字、语言 的历史</p>
<ul>
<li>通信的原理和信息传播的模型</li>
<li>(信源) 编码 和 最短编码</li>
<li>解码的规则 : 语法</li>
<li>聚类的概念</li>
<li>双语对照文本，语料库 和 机器翻译</li>
<li>多义性和利用上下文消除歧义性</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>《数学之美》 读书笔记 </li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[python 结构化您的工程]]></title>
      <url>http://sggo.me/2017/11/04/python/py-language-structure-project/</url>
      <content type="html"><![CDATA[<p>如何利用Python的特性来创造简洁、高效的代码。 “<strong>结构化</strong>” 意味着通过编写简洁的代码，使逻辑和依赖清晰.</p>
<a id="more"></a>
<h2 id="仓库的结构"><a href="#仓库的结构" class="headerlink" title="仓库的结构"></a>仓库的结构</h2><p>在一个健康的开发周期中，代码风格，API设计和自动化是非常关键的。同样的，对于工程的 架构 ,仓库的结构也是关键的一部分。</p>
<p>当一个潜在的用户和贡献者登录到您的仓库页面时，他们会看到这些:</p>
<ul>
<li>工程的名字</li>
<li>工程的描述</li>
<li>一系列的文件</li>
</ul>
<blockquote>
<p>拥有良好的布局，事半功倍。</p>
</blockquote>
<h2 id="仓库样例"><a href="#仓库样例" class="headerlink" title="仓库样例"></a>仓库样例</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">README.rst</span><br><span class="line">LICENSE</span><br><span class="line">setup.py</span><br><span class="line">requirements.txt</span><br><span class="line">sample/__init__.py</span><br><span class="line">sample/core.py</span><br><span class="line">sample/helpers.py</span><br><span class="line">docs/conf.py</span><br><span class="line">docs/index.rst</span><br><span class="line">tests/<span class="built_in">test</span>_basic.py</span><br><span class="line">tests/<span class="built_in">test</span>_advanced.py</span><br></pre></td></tr></table></figure>
<h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p>除了源代码本身以外，这个毫无疑问是您仓库最重要的一部分。在这个文件中要有完整的许可说明和授权。</p>
<p>如果您不太清楚您应该使用哪种许可方式，请查看 choosealicense.com.</p>
<h2 id="Setup-py"><a href="#Setup-py" class="headerlink" title="Setup.py"></a>Setup.py</h2><p>…</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://pythonguidecn.readthedocs.io/zh/latest/writing/structure.html" target="_blank" rel="external">结构化你的工程</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Taking care of your pet DOG!]]></title>
      <url>http://sggo.me/2017/11/02/English/engvid-Taking-care-of-your-pet/</url>
      <content type="html"><![CDATA[<p>Do you have a dog? Do you want a dog? </p>
<a id="more"></a>
<p>Here in North America, we have specific rules, laws, and cultural customs that every dog owner must know. And if you’re not a dog owner, you need to know this stuff, too, because you’ll be around a lot of dogs here! In this video, I’ll teach you English vocabulary for the dog-related people you’ll meet: breeders, vets, and groomers; as well as the doggie things you must have, such as a leash, dish, and treats. The best part? Dog owners love to talk about dogs. So you can practice your English by talking to other dog owners. Want more? Check out my friend Emma’s video about pets and animals here!</p>
<div class="video-container"><iframe src="//www.youtube.com/embed/CgGicK1q1eg" frameborder="0" allowfullscreen></iframe></div>
<blockquote>
<p>&amp;t=171s</p>
</blockquote>
<p>Ruff. Hi. I’m Ronnie. I’m not a dog, but I have a dog, and I like dogs. Not: I like dog. I don’t want to eat a dog. No, actually I would eat a dog if you gave it to me. If you barbequed it, I’d eat it. But, oh, we’re not talking about eating dogs. We’re going to talk about how to actually not eat your dog, how to care for your dog. In English and in our society we call our dog our best friends, although it really has no choice. Even if your dog doesn’t like you, it has to hang out with you a lot because you have to care for your dog. So, I’m going to teach you English vocabulary, English verbs for caring for your dog. Hopefully you can learn some stuff. You probably know it in your language, but now you’re going to learn English with me because that’s what I do.</p>
<p>So, the first thing that we should talk about is someone called a “breeder”. Okay? A breeder is a person who breeds dogs. So, I’ve written the word “breeds” here as a verb. Now, to breed a dog means that you get the boy dog who’s called a stud and you get the girl dog who’s called a bitch, and you make them have sex. I don’t know how they do this, maybe they give them some doggy wine, play some nice doggy music, but anyways, the boy dog and the girl dog have dog sex, doggy style - and they have puppies. Woo-hoo. So the breeder is the person who cares for the dogs and makes more dogs. They’re magical.</p>
<p>It is a really, really bad idea, almost illegal in Canada to buy a dog from the “pet shop”. We have really tried to cut down on dogs who are for sale in pet shops. Just the treatment of the dogs, there’s been a lot of controversy. Also, you can find a dog on the internet, you can go to Kijiji or you can go to many websites where you can buy a dog. There’s also dog rescue sites where maybe a dog doesn’t have a house, a dog is homeless, and you can rescue the dog and make him your… Or her your best friend. So there’s many options. I do not recommend the pet shop option, but make sure that your dog is going to be… You’re going to be able to take care of a dog before you get one.</p>
<p>So, the place where the dogs or the puppies live and where they have all their doggy sex is called a “kennel”. So the kennel is the place, and the breeder is the person. You can use both of these words interchangeably, but a breeder is always a person. It’s important that you go and look at the kennel when you’re buying a dog to make sure that it’s clean and the dogs are well taken care of. There’s some terrible-looking kennels I imagine.</p>
<p>The next person that you’re not going to want to go to and your dog is going to hate more than the mailman is the “veterinarian”. Now, in English we don’t need to bother saying: “Veterinarian”, which is an animal doctor. What we need to say is: “Vet”. So you’re going to say: “Uh-oh, I have to take my dog to the vet”, because maybe your dog is sick. Vets in Canada and probably around the world, America as well, are very, very expensive. So we don’t want our dogs to be sick because it costs us a lot of money.</p>
<p>But there is one thing that people or breeders really encourage you to do if you have gotten a puppy and you’re not going to breed the dog, you’re not going to have… Make more baby dogs with this. That’s a whole other business. So, for this you’re going to do what we call “get your dog fixed”. Your dog is not broken, don’t worry, it’s not a toy. So: “Get your dog fixed”, you’re going to go to the vet, you’re going to go to the animal doctor and what’s happened… What’s going to happen is you’re going to get your animal neutered. “Neutered” means… We have two different meaning… Two different words we use for this. For the boys or for the studs, neutered means “castrated”. Oh, gentlemen, gentlemen, this means they’re going to remove the dog’s testicles so the dog cannot produce sperm to have little puppies. Don’t worry, I’m sure the dog doesn’t feel anything, and when he wakes up he is very happy because he has less balls to lick. So, “castrated” is only for boys and they remove the testicles. The word that we use for women, or sorry, girls or bitches, mm-hmm, is “spayed”.</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux Search Cmd]]></title>
      <url>http://sggo.me/2017/10/24/ops/ops-linux-cmd-file-search/</url>
      <content type="html"><![CDATA[<p>which，whereis，locate，find，find exec，find xargs，find…</p>
<a id="more"></a>
<p>which   查看可执行文件的位置。<br>whereis 查看文件的位置。<br>locate  配合数据库查看文件位置。<br>find    实际搜寻硬盘查询文件名称。  </p>
<h2 id="1-which"><a href="#1-which" class="headerlink" title="1. which"></a>1. which</h2><p>which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># which pwd</span></span><br><span class="line">/bin/<span class="built_in">pwd</span></span><br><span class="line">[root@localhost ~]<span class="comment">#  which adduser</span></span><br><span class="line">/usr/sbin/adduser</span><br><span class="line">[root@localhost ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>cd 是bash 内建的命令！</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜ <span class="built_in">which</span> <span class="built_in">cd</span></span><br><span class="line"><span class="built_in">cd</span>: shell built-in <span class="built_in">command</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>which 默认是找 PATH 内所规范的目录，所以当然一定找不到的！</p>
</blockquote>
<h2 id="2-whereis"><a href="#2-whereis" class="headerlink" title="2. whereis"></a>2. whereis</h2><p>whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。</p>
<p>和find相比，whereis查找的速度非常快，这是因为linux系统会将 系统内的所有文件都记录在一个数据库文件中，当使用whereis和下面即将介绍的locate时，会从数据库中查找数据，而不是像find命令那样，通 过遍历硬盘来查找，效率自然会很高。 </p>
<p>但是该数据库文件并不是实时更新，默认情况下时一星期更新一次，因此，我们在用whereis和locate 查找文件时，有时会找到已经被删除的数据，或者刚刚建立文件，却无法查找到，原因就是因为数据库文件没有被更新。</p>
<h2 id="3-locate"><a href="#3-locate" class="headerlink" title="3. locate"></a>3. locate</h2><p>locate命令可以在搜寻数据库时快速找到档案，数据库由updatedb程序来更新，updatedb是由cron daemon周期性建立的，locate命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是locate所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb每天会跑一次，可以由修改crontab来更新设定值。(etc/crontab)</p>
<p>locate指定用在搜寻符合条件的档案，它会去储存档案与目录名称的数据库内，寻找合乎范本样式条件的档案或目录录，可以使用特殊字元（如”<em>” 或”?”等）来指定范本样式，如指定范本为kcpa</em>ner, locate会找出所有起始字串为kcpa且结尾为ner的档案或目录，如名称为kcpartner若目录录名称为kcpa_ner则会列出该目录下包括 子目录在内的所有档案。</p>
<p>locate指令和find找寻档案的功能类似，但locate是透过update程序将硬盘中的所有档案和目录资料先建立一个索引数据库，在 执行loacte时直接找该索引，查询速度会较快，索引数据库一般是由操作系统管理，但也可以直接下达update强迫系统立即修改索引数据库。</p>
<blockquote>
<p>mac 默认没启动 locate</p>
</blockquote>
<h2 id="4-find"><a href="#4-find" class="headerlink" title="4. find"></a>4. find</h2><h3 id="4-1-查找指定时间内修改过的文件"><a href="#4-1-查找指定时间内修改过的文件" class="headerlink" title="4.1 查找指定时间内修改过的文件"></a>4.1 查找指定时间内修改过的文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@peidachang ~]<span class="comment"># find -atime -2</span></span><br><span class="line">.</span><br><span class="line">./logs/monitor</span><br><span class="line">./.bashrc</span><br><span class="line">./.bash_profile</span><br><span class="line">./.bash_<span class="built_in">history</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>说明：超找48小时内修改过的文件 </p>
</blockquote>
<h3 id="4-2-根据关键字查找"><a href="#4-2-根据关键字查找" class="headerlink" title="4.2 根据关键字查找"></a>4.2 根据关键字查找</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -name <span class="string">"*.log"</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3-查找当前所有目录并排序"><a href="#4-3-查找当前所有目录并排序" class="headerlink" title="4.3 查找当前所有目录并排序"></a>4.3 查找当前所有目录并排序</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -type d | sort</span><br></pre></td></tr></table></figure>
<h3 id="4-4-按类型查找"><a href="#4-4-按类型查找" class="headerlink" title="4.4 按类型查找"></a>4.4 按类型查找</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -type f -name <span class="string">"*.log"</span></span><br></pre></td></tr></table></figure>
<h3 id="4-5-查找当前所有目录并排序"><a href="#4-5-查找当前所有目录并排序" class="headerlink" title="4.5 查找当前所有目录并排序"></a>4.5 查找当前所有目录并排序</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -type d | sort</span><br></pre></td></tr></table></figure>
<h3 id="4-6-按大小查找文件"><a href="#4-6-按大小查找文件" class="headerlink" title="4.6 按大小查找文件"></a>4.6 按大小查找文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -size +1000c -print</span><br></pre></td></tr></table></figure>
<h2 id="5-find命令之exec"><a href="#5-find命令之exec" class="headerlink" title="5. find命令之exec"></a>5. find命令之exec</h2><p>find是我们很常用的一个Linux命令，但是我们一般查找出来的并不仅仅是看看而已，还会有进一步的操作，这个时候exec的作用就显现出来了。 </p>
<p><strong>exec解释：</strong></p>
<p>-exec  参数后面跟的是command命令，它的终止是以;为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。  </p>
<p>{}   花括号代表前面find查找出来的文件名。  </p>
<p>使用find时，只要把想要的操作写在一个文件里，就可以用exec来配合find查找，很方便的。在有些操作系统中只允许-exec选项执行诸如l s或ls -l这样的命令。大多数用户使用这一选项是为了查找旧文件并删除它们。建议在真正执行rm命令删除文件之前，最好先用ls命令看一下，确认它们是所要删除的文件。 exec选项后面跟随着所要执行的命令或脚本，然后是一对儿{ }，一个空格和一个\，最后是一个分号。为了使用exec选项，必须要同时使用print选项。如果验证一下find命令，会发现该命令只输出从当前路径起的相对路径及文件名。</p>
<h3 id="5-1-ls-l命令放find命令的-exec"><a href="#5-1-ls-l命令放find命令的-exec" class="headerlink" title="5.1 ls -l命令放find命令的-exec"></a>5.1 ls -l命令放find命令的-exec</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ~/ghome/github/test [15:45:03]</span></span><br><span class="line">➜ ll</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:44 <span class="built_in">log</span>1</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:45 <span class="built_in">log</span>2</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:45 <span class="built_in">log</span>3</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:45 <span class="built_in">log</span>4</span><br><span class="line">(vpy2)</span><br><span class="line"><span class="comment"># ~/ghome/github/test [15:45:03]</span></span><br><span class="line">➜ find . -type f -exec ls <span class="_">-l</span> &#123;&#125; \;</span><br><span class="line">-rw-r--r--  1 blair  staff  0 Oct 28 15:45 ./<span class="built_in">log</span>4</span><br><span class="line">-rw-r--r--  1 blair  staff  0 Oct 28 15:45 ./<span class="built_in">log</span>3</span><br><span class="line">-rw-r--r--  1 blair  staff  0 Oct 28 15:45 ./<span class="built_in">log</span>2</span><br><span class="line">-rw-r--r--  1 blair  staff  0 Oct 28 15:44 ./<span class="built_in">log</span>1</span><br><span class="line">(vpy2)</span><br><span class="line"><span class="comment"># ~/ghome/github/test [15:45:04]</span></span><br><span class="line">➜</span><br></pre></td></tr></table></figure>
<h3 id="5-2-更改时间在n日以前的文件并删除"><a href="#5-2-更改时间在n日以前的文件并删除" class="headerlink" title="5.2 更改时间在n日以前的文件并删除"></a>5.2 更改时间在n日以前的文件并删除</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -type f -mtime +14 -exec rm &#123;&#125; \;</span><br></pre></td></tr></table></figure>
<h3 id="5-3-更改时间在n日前文件并提示删除"><a href="#5-3-更改时间在n日前文件并提示删除" class="headerlink" title="5.3 更改时间在n日前文件并提示删除"></a>5.3 更改时间在n日前文件并提示删除</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ~/ghome/github/test [15:47:35]</span><br><span class="line">➜ ll</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:44 log1</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:45 log2</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:45 log3</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:45 log4</span><br><span class="line">(vpy2)</span><br><span class="line"># ~/ghome/github/test [15:47:38]</span><br><span class="line">➜ find . -name &quot;*log*&quot; -mtime -1 -ok rm &#123;&#125; \;</span><br><span class="line">&quot;rm ./log4&quot;?</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在目录中查找更改时间在n日以前的文件并删除它们，在删除之前先给出提示</p>
</blockquote>
<h3 id="5-4-exec中使用grep命令"><a href="#5-4-exec中使用grep命令" class="headerlink" title="5.4 -exec中使用grep命令"></a>5.4 -exec中使用grep命令</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find /etc -name <span class="string">"passwd*"</span> -exec grep <span class="string">"root"</span> &#123;&#125; \;</span><br></pre></td></tr></table></figure>
<h3 id="5-5-查找文件移动到指定目录"><a href="#5-5-查找文件移动到指定目录" class="headerlink" title="5.5 查找文件移动到指定目录"></a>5.5 查找文件移动到指定目录</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -name <span class="string">"*.log"</span> -exec mv &#123;&#125; .. \;</span><br></pre></td></tr></table></figure>
<h3 id="5-6-exec选项执行cp命令"><a href="#5-6-exec选项执行cp命令" class="headerlink" title="5.6 exec选项执行cp命令"></a>5.6 exec选项执行cp命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -name &quot;*.log&quot; -exec cp &#123;&#125; test3 \;</span><br></pre></td></tr></table></figure>
<h2 id="6-find命令之xargs"><a href="#6-find命令之xargs" class="headerlink" title="6. find命令之xargs"></a>6. find命令之xargs</h2><p>在使用 find命令的-exec选项处理匹配到的文件时， find命令将所有匹配到的文件一起传递给exec执行。但有些系统对能够传递给exec的命令长度有限制，这样在find命令运行几分钟之后，就会出现溢出错误。错误信息通常是“参数列太长”或“参数列溢出”。这就是xargs命令的用处所在，特别是与find命令一起使用。  </p>
<p>find命令把匹配到的文件传递给xargs命令，而xargs命令每次只获取一部分文件而不是全部，不像-exec选项那样。这样它可以先处理最先获取的一部分文件，然后是下一批，并如此继续下去。  </p>
<p>在有些系统中，使用-exec选项会为处理每一个匹配到的文件而发起一个相应的进程，并非将匹配到的文件全部作为参数一次执行；这样在有些情况下就会出现进程过多，系统性能下降的问题，因而效率不高； 而使用xargs命令则只有一个进程。另外，在使用xargs命令时，究竟是一次获取所有的参数，还是分批取得参数，以及每一次获取参数的数目都会根据该命令的选项及系统内核中相应的可调参数来确定。</p>
<h3 id="6-1-find-type-f-print-xargs-file"><a href="#6-1-find-type-f-print-xargs-file" class="headerlink" title="6.1 find . -type f -print | xargs file"></a>6.1 find . -type f -print | xargs file</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ~/ghome/github/test [7:41:40]</span></span><br><span class="line">➜ find . -type f -print | xargs file</span><br><span class="line">./<span class="built_in">log</span>4: empty</span><br><span class="line">./<span class="built_in">log</span>3: empty</span><br><span class="line">./<span class="built_in">log</span>2: empty</span><br><span class="line">./<span class="built_in">log</span>1: ASCII text</span><br><span class="line">(vpy2)</span><br></pre></td></tr></table></figure>
<h3 id="6-2-用户具有读、写和执行权限的文件，并收回相应的写权限"><a href="#6-2-用户具有读、写和执行权限的文件，并收回相应的写权限" class="headerlink" title="6.2 用户具有读、写和执行权限的文件，并收回相应的写权限"></a>6.2 用户具有读、写和执行权限的文件，并收回相应的写权限</h3><p>find . -perm -7 -print | xargs chmod o-w</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ~/ghome/github/test [7:44:26]</span></span><br><span class="line">➜ find . -perm -7 -print</span><br><span class="line">./<span class="built_in">log</span>3</span><br><span class="line">(vpy2)</span><br><span class="line"><span class="comment"># ~/ghome/github/test [7:44:27]</span></span><br><span class="line">➜ find . -perm -7 -print | xargs chmod o-w</span><br><span class="line">(vpy2)</span><br><span class="line"><span class="comment"># ~/ghome/github/test [7:44:49]</span></span><br><span class="line">➜ ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r--  1 blair  staff    32B Oct 28 15:54 <span class="built_in">log</span>1</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:45 <span class="built_in">log</span>2</span><br><span class="line">-rwxrwxr-x  1 blair  staff     0B Oct 28 15:45 <span class="built_in">log</span>3</span><br><span class="line">-rw-r--r--  1 blair  staff     0B Oct 28 15:45 <span class="built_in">log</span>4</span><br><span class="line">(vpy2)</span><br><span class="line"><span class="comment"># ~/ghome/github/test [7:44:50]</span></span><br></pre></td></tr></table></figure>
<h3 id="6-3-grep命令在所有的普通文件中搜索hostname这个词"><a href="#6-3-grep命令在所有的普通文件中搜索hostname这个词" class="headerlink" title="6.3 grep命令在所有的普通文件中搜索hostname这个词"></a>6.3 grep命令在所有的普通文件中搜索hostname这个词</h3><p>find . -type f -print | xargs grep “hostname”</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost <span class="built_in">test</span>]<span class="comment"># find . -type f -print | xargs grep "hostname"</span></span><br><span class="line">./<span class="built_in">log</span>2013.log:hostnamebaidu=baidu.com</span><br><span class="line">./<span class="built_in">log</span>2013.log:hostnamesina=sina.com</span><br><span class="line">./<span class="built_in">log</span>2013.log:hostnames=<span class="literal">true</span>[root@localhost <span class="built_in">test</span>]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<h3 id="6-4-使用xargs执行mv"><a href="#6-4-使用xargs执行mv" class="headerlink" title="6.4 使用xargs执行mv"></a>6.4 使用xargs执行mv</h3><p>find . -name “*.log” | xargs -i mv {} test4</p>
<blockquote>
<p>使用-i参数默认的前面输出用{}代替，-I参数可以指定其他代替字符</p>
</blockquote>
<h3 id="6-5-xargs的-p参数的使用"><a href="#6-5-xargs的-p参数的使用" class="headerlink" title="6.5 xargs的-p参数的使用"></a>6.5 xargs的-p参数的使用</h3><p>find . -name “*.log” | xargs -p -i mv {} ..</p>
<blockquote>
<p>-p参数会提示让你确认是否执行后面的命令,y执行，n不执行。</p>
</blockquote>
<h2 id="7-find-命令的参数详解"><a href="#7-find-命令的参数详解" class="headerlink" title="7. find 命令的参数详解"></a>7. <a href="http://www.cnblogs.com/peida/archive/2012/11/16/2773289.html" target="_blank" rel="external">find 命令的参数详解</a></h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.cnblogs.com/peida/archive/2012/12/05/2803591.html" target="_blank" rel="external">每天一个linux命令目录</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux File Basic Cmd]]></title>
      <url>http://sggo.me/2017/10/23/ops/ops-linux-cmd-1-file-directory-operation/</url>
      <content type="html"><![CDATA[<p>ls, cd, pwd, mkdir, rm, rmdir, mv, cp, touch, cat, nl, more, less, head, tail</p>
<a id="more"></a>
<h2 id="1-ls"><a href="#1-ls" class="headerlink" title="1. ls"></a>1. ls</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ls -l -R /home/blair/ghome</span><br><span class="line">ls -ctrl s*</span><br></pre></td></tr></table></figure>
<p>列出 /opt/soft 文件下面的子目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls -F /opt/soft |grep /$</span><br></pre></td></tr></table></figure>
<p>目录于名称后加”/“, 可执行档于名称后加”*” </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls -AFl</span><br></pre></td></tr></table></figure>
<p>列出当前目录下的所有文件（包括隐藏文件）的绝对路径， 对目录不做递归</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find <span class="variable">$PWD</span> -maxdepth 1 | xargs ls -ld</span><br></pre></td></tr></table></figure>
<p>递归列出当前目录下的所有文件（包括隐藏文件）的绝对路径</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find <span class="variable">$PWD</span> | xargs ls -ld</span><br></pre></td></tr></table></figure>
<h2 id="2-cd"><a href="#2-cd" class="headerlink" title="2. cd"></a>2. cd</h2><blockquote>
<p><code>cd -</code> or <code>cd</code> or <code>cd ~</code></p>
</blockquote>
<h2 id="3-pwd"><a href="#3-pwd" class="headerlink" title="3. pwd"></a>3. pwd</h2><p><code>pwd -P</code> 显示出实际路径，而非使用连接（link）路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># /usr/local/xsoft/software/scala [22:17:59]</span><br><span class="line">➜ pwd</span><br><span class="line">/usr/local/xsoft/software/scala</span><br><span class="line">(vpy3)</span><br><span class="line"># /usr/local/xsoft/software/scala [22:17:59]</span><br><span class="line">➜ pwd -P</span><br><span class="line">/usr/local/xsoft/deploy/scala-2.11.7</span><br><span class="line">(vpy3)</span><br><span class="line"># /usr/local/xsoft/software/scala [22:18:02]</span><br><span class="line">➜</span><br></pre></td></tr></table></figure>
<h2 id="4-mkdir"><a href="#4-mkdir" class="headerlink" title="4. mkdir"></a>4. mkdir</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span>1</span><br><span class="line">mkdir -p <span class="built_in">test</span>2/<span class="built_in">test</span>22</span><br><span class="line">mkdir -m 777 <span class="built_in">test</span>3</span><br><span class="line">mkdir -v <span class="built_in">test</span>4</span><br></pre></td></tr></table></figure>
<p>一个命令创建项目的目录结构</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ~/ghome/seek/test2 [22:23:06]</span></span><br><span class="line">➜ mkdir -vp scf/&#123;lib/,bin/,doc/&#123;info,product&#125;,logs/&#123;info,product&#125;,service/deploy/&#123;info,product&#125;&#125;</span><br><span class="line">(vpy3)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ~/ghome/seek/test2 [22:23:08]</span><br><span class="line">➜ tree</span><br><span class="line">.</span><br><span class="line">└── scf</span><br><span class="line">    ├── bin</span><br><span class="line">    ├── doc</span><br><span class="line">    │   ├── info</span><br><span class="line">    │   └── product</span><br><span class="line">    ├── lib</span><br><span class="line">    ├── logs</span><br><span class="line">    │   ├── info</span><br><span class="line">    │   └── product</span><br><span class="line">    └── service</span><br><span class="line">        └── deploy</span><br><span class="line">            ├── info</span><br><span class="line">            └── product</span><br><span class="line"></span><br><span class="line">13 directories, 0 files</span><br><span class="line">(vpy3)</span><br><span class="line"># ~/ghome/seek/test2 [22:23:10]</span><br></pre></td></tr></table></figure>
<h2 id="5-rm"><a href="#5-rm" class="headerlink" title="5. rm"></a>5. rm</h2><p>-f, –force    忽略不存在的文件，从不给出提示。<br>-i, –interactive 进行交互式删除<br>-r, -R, –recursive   指示rm将参数中列出的全部目录和子目录均递归地删除。<br>-v, –verbose    详细显示进行的步骤</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ~/ghome/seek/test2 [22:29:28]</span></span><br><span class="line">➜ rm -i f1</span><br><span class="line">remove f1?</span><br></pre></td></tr></table></figure>
<p><strong>自定义回收站功能</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">myrm</span></span>()&#123; D=/tmp/$(date +%Y%m%d%H%M%S); mkdir -p <span class="variable">$D</span>; mv <span class="string">"<span class="variable">$@</span>"</span> <span class="variable">$D</span> &amp;&amp; <span class="built_in">echo</span> <span class="string">"moved to <span class="variable">$D</span> ok"</span>; &#125;</span><br></pre></td></tr></table></figure>
<p><strong>for example</strong>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost test]# myrm()&#123; D=/tmp/$(date +%Y%m%d%H%M%S); mkdir -p $D; mv &quot;$@&quot; $D &amp;&amp; echo &quot;moved to $D ok&quot;; &#125;</span><br><span class="line">[root@localhost test]# alias rm=&apos;myrm&apos;</span><br><span class="line">[root@localhost test]# touch 1.log 2.log 3.log</span><br><span class="line">[root@localhost test]# ll</span><br><span class="line">总计 16</span><br><span class="line">-rw-r--r-- 1 root root    0 10-26 15:08 1.log</span><br><span class="line">-rw-r--r-- 1 root root    0 10-26 15:08 2.log</span><br><span class="line">-rw-r--r-- 1 root root    0 10-26 15:08 3.log</span><br><span class="line">drwxr-xr-x 7 root root 4096 10-25 18:07 scf</span><br><span class="line">drwxrwxrwx 2 root root 4096 10-25 17:46 test3</span><br><span class="line">drwxr-xr-x 2 root root 4096 10-25 17:56 test4</span><br><span class="line">drwxr-xr-x 3 root root 4096 10-25 17:56 test5</span><br><span class="line">[root@localhost test]# rm [123].log</span><br><span class="line">moved to /tmp/20121026150901 ok</span><br><span class="line">[root@localhost test]# ll</span><br><span class="line">总计 16drwxr-xr-x 7 root root 4096 10-25 18:07 scf</span><br><span class="line">drwxrwxrwx 2 root root 4096 10-25 17:46 test3</span><br><span class="line">drwxr-xr-x 2 root root 4096 10-25 17:56 test4</span><br><span class="line">drwxr-xr-x 3 root root 4096 10-25 17:56 test5</span><br><span class="line">[root@localhost test]# ls /tmp/20121026150901/</span><br><span class="line">1.log  2.log  3.log</span><br><span class="line">[root@localhost test]#</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>Reference: </strong> <a href="http://www.cnblogs.com/peida/archive/2012/10/26/2740521.html" target="_blank" rel="external">每天一个linux命令（5）：rm 命令</a></p>
</blockquote>
<h2 id="6-rmdir"><a href="#6-rmdir" class="headerlink" title="6. rmdir"></a>6. rmdir</h2><blockquote>
<p>删除空目录</p>
<p>rmdir -p 当子目录被删除后使它也成为空目录的话，则顺便一并删除 </p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost scf]<span class="comment"># rmdir -p logs</span></span><br><span class="line">rmdir: logs: 目录非空</span><br><span class="line"></span><br><span class="line">[root@localhost scf]<span class="comment"># tree</span></span><br><span class="line">.</span><br><span class="line">|-- bin</span><br><span class="line">|-- doc</span><br><span class="line">|-- lib</span><br><span class="line">|-- logs</span><br><span class="line">|   `-- product</span><br><span class="line">`-- service</span><br><span class="line">    `-- deploy</span><br><span class="line">        |-- info</span><br><span class="line">        `-- product</span><br><span class="line"> </span><br><span class="line">9 directories, 0 files</span><br></pre></td></tr></table></figure>
<p>[root@localhost scf]# rmdir -p logs/product</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost scf]<span class="comment"># tree</span></span><br><span class="line">.</span><br><span class="line">|-- bin</span><br><span class="line">|-- doc</span><br><span class="line">|-- lib</span><br><span class="line">`-- service</span><br><span class="line">`-- deploy</span><br><span class="line">        |-- info</span><br><span class="line">        `-- product</span><br></pre></td></tr></table></figure>
<h2 id="7-mv"><a href="#7-mv" class="headerlink" title="7. mv"></a>7. mv</h2><p>-i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！<br>-t  ： –target-directory=DIRECTORY move all SOURCE arguments into DIRECTORY，即指定mv的目标目录，该选项适用于移动多个源文件到一个目录的情况，此时目标目录在前，源文件在后。</p>
<h2 id="8-cp"><a href="#8-cp" class="headerlink" title="8. cp"></a>8. cp</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -a test3 test4</span><br><span class="line">cp -s log.log log_link.log</span><br></pre></td></tr></table></figure>
<h2 id="9-touch"><a href="#9-touch" class="headerlink" title="9. touch"></a>9. touch</h2><p>设定文件的时间戳</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch -t 201211142234.50 log.log</span><br></pre></td></tr></table></figure>
<p>更新 log2012.log 的时间和 log.log 时间戳相同</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch -r log.log log2012.log</span><br></pre></td></tr></table></figure>
<h2 id="10-cat"><a href="#10-cat" class="headerlink" title="10. cat"></a>10. cat</h2><ol>
<li>一次显示整个文件:cat filename</li>
<li>从键盘创建一个文件:cat &gt; filename 只能创建新文件,不能编辑已有文件.</li>
<li>将几个文件合并为一个文件:cat file1 file2 &gt; file</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜ cat -n log.log <span class="built_in">log</span>2012.log</span><br><span class="line">     1	asd</span><br><span class="line">     2</span><br><span class="line">     3	asd</span><br><span class="line">     1	k1</span><br><span class="line">     2	k2</span><br><span class="line">     3</span><br><span class="line">     4	k3</span><br><span class="line">(vpy3)</span><br><span class="line"><span class="comment"># ~/ghome/seek/test2 [23:02:19]</span></span><br><span class="line">➜</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tac (反向列示).<br>tac log.txt</p>
</blockquote>
<h2 id="11-nl"><a href="#11-nl" class="headerlink" title="11. nl"></a>11. nl</h2><p>nl命令在linux系统中用来计算文件中行号。nl 可以将输出的文件内容自动的加上行号！</p>
<p>命令参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-b  ：指定行号指定的方式，主要有两种：</span><br><span class="line">-b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)；</span><br><span class="line">-b t ：如果有空行，空的那一行不要列出行号(默认值)；</span><br><span class="line">-n  ：列出行号表示的方法，主要有三种：</span><br><span class="line">-n ln ：行号在萤幕的最左方显示；</span><br><span class="line">-n rn ：行号在自己栏位的最右方显示，且不加 0 ；</span><br><span class="line">-n rz ：行号在自己栏位的最右方显示，且加 0 ；</span><br><span class="line">-w  ：行号栏位的占用的位数。</span><br><span class="line">-p 在逻辑定界符处不重新开始计算。</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost <span class="built_in">test</span>]<span class="comment"># nl log2012.log </span></span><br><span class="line">     1  2012-01</span><br><span class="line">     2  2012-02</span><br><span class="line">       </span><br><span class="line">       </span><br><span class="line">     3  ======[root@localhost <span class="built_in">test</span>]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost test]# nl -b a log2012.log </span><br><span class="line">     1  2012-01</span><br><span class="line">     2  2012-02</span><br><span class="line">     3</span><br><span class="line">     4</span><br><span class="line">     5  ======[root@localhost test]#</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost <span class="built_in">test</span>]<span class="comment"># nl -b a -n rz log2014.log </span></span><br><span class="line">000001  2014-01</span><br><span class="line">000002  2014-02</span><br><span class="line">000003  2014-03</span><br><span class="line">000004  2014-04</span><br><span class="line">000005  2014-05</span><br><span class="line">000006  2014-06</span><br><span class="line">000007  2014-07</span><br><span class="line">000008  2014-08</span><br><span class="line">000009  2014-09</span><br><span class="line">000010  2014-10</span><br><span class="line">000011  2014-11</span><br><span class="line">000012  2014-12</span><br><span class="line">000013  =======</span><br><span class="line">[root@localhost <span class="built_in">test</span>]<span class="comment"># nl -b a -n rz -w 3 log2014.log </span></span><br><span class="line">001     2014-01</span><br><span class="line">002     2014-02</span><br><span class="line">003     2014-03</span><br><span class="line">004     2014-04</span><br><span class="line">005     2014-05</span><br><span class="line">006     2014-06</span><br><span class="line">007     2014-07</span><br><span class="line">008     2014-08</span><br><span class="line">009     2014-09</span><br><span class="line">010     2014-10</span><br><span class="line">011     2014-11</span><br><span class="line">012     2014-12</span><br><span class="line">013     =======</span><br></pre></td></tr></table></figure>
<blockquote>
<p>说明： nl -b a -n rz 命令行号默认为六位，要调整位数可以加上参数 -w 3 调整为3位。</p>
</blockquote>
<h2 id="12-more"><a href="#12-more" class="headerlink" title="12. more"></a>12. more</h2><p>命令参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+n       从笫n行开始显示</span><br><span class="line">-n       定义屏幕大小为n行</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost test]# cat log2012.log </span><br><span class="line">2012-01</span><br><span class="line">2012-02</span><br><span class="line">2012-03</span><br><span class="line">2012-04-day1</span><br><span class="line">2012-04-day2</span><br><span class="line">2012-04-day3</span><br><span class="line">======[root@localhost test]# more +3 log2012.log </span><br><span class="line">2012-03</span><br><span class="line">2012-04-day1</span><br><span class="line">2012-04-day2</span><br><span class="line">2012-04-day3</span><br><span class="line">======[root@localhost test]#</span><br></pre></td></tr></table></figure>
<p>设定每屏显示行数   more -5 log2012.log. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost <span class="built_in">test</span>]<span class="comment"># more -5 log2012.log </span></span><br><span class="line">2012-01</span><br><span class="line">2012-02</span><br><span class="line">2012-03</span><br><span class="line">2012-04-day1</span><br><span class="line">2012-04-day2</span><br></pre></td></tr></table></figure>
<p>列一个目录下的文件，由于内容太多，我们应该学会用more来分页显示。这得和管道 | 结合起来</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls <span class="_">-l</span>  | more -5</span><br></pre></td></tr></table></figure>
<h2 id="13-less"><a href="#13-less" class="headerlink" title="13. less"></a>13. less</h2><p>less 工具也是对文件或其它输出进行分页显示的工具，应该说是linux正统查看文件内容的工具，功能极其强大。</p>
<p>less 的用法比起 more 更加的有弹性。在 more 的时候，我们并没有办法向前面翻， 只能往后面看，但若使用了 less 时，就可以使用 [pageup] [pagedown] 等按键的功能来往前往后翻看文件，更容易用来查看一个文件的内容！</p>
<p>在 less 里头可以拥有更多的搜索功能，不止可以向下搜，也可以向上搜。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">less <span class="built_in">log</span>2013.log</span><br><span class="line"></span><br><span class="line">ps -ef |less</span><br><span class="line"></span><br><span class="line"><span class="built_in">history</span> | less</span><br></pre></td></tr></table></figure>
<p>输出文件除了最后n行的全部内容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">head -n -6 <span class="built_in">log</span>2014.log</span><br></pre></td></tr></table></figure>
<h2 id="14-head"><a href="#14-head" class="headerlink" title="14. head"></a>14. head</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost test]# head -n 5 log2014.log </span><br><span class="line">2014-01</span><br><span class="line">2014-02</span><br><span class="line">2014-03</span><br><span class="line">2014-04</span><br><span class="line">2014-05[root@localhost test]#</span><br></pre></td></tr></table></figure>
<p>显示文件前n个字节  head -c 20 log2014.log</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost <span class="built_in">test</span>]<span class="comment"># head -c 20 log2014.log</span></span><br><span class="line">2014-01</span><br><span class="line">2014-02</span><br><span class="line">2014</span><br><span class="line">[root@localhost <span class="built_in">test</span>]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>输出文件除了最后n行的全部内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">head -n -6 log2014.log</span><br></pre></td></tr></table></figure>
<h2 id="15-tail"><a href="#15-tail" class="headerlink" title="15. tail"></a>15. tail</h2><p>显示文件末尾内容. tail -n 5 log2014.log</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost <span class="built_in">test</span>]<span class="comment"># tail -n 5 log2014.log </span></span><br><span class="line">2014-09</span><br><span class="line">2014-10</span><br><span class="line">2014-11</span><br><span class="line">2014-12</span><br><span class="line">==============================[root@localhost <span class="built_in">test</span>]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>循环查看文件内容 tail -f test.log</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.cnblogs.com/peida/tag/每日一linux命令/" target="_blank" rel="external">每日一linux命令</a></li>
<li><a href="http://www.cnblogs.com/peida/archive/2012/12/05/2803591.html" target="_blank" rel="external">每天一个linux命令目录</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow 快速学习 & 文档]]></title>
      <url>http://sggo.me/2017/10/23/tensorflow/tf-doc/</url>
      <content type="html"><![CDATA[<p>介绍 Tensorflow 的学习流程 与 <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-bm7y28si.html" target="_blank" rel="external">W3C shchool Tensorflow</a> 的中文文档。</p>
<a id="more"></a>
<h2 id="1-结构目录"><a href="#1-结构目录" class="headerlink" title="1. 结构目录"></a>1. 结构目录</h2><h3 id="1-1-Tensorflow-入门介绍"><a href="#1-1-Tensorflow-入门介绍" class="headerlink" title="1.1 Tensorflow 入门介绍"></a>1.1 Tensorflow 入门介绍</h3><h3 id="1-2-Tensorflow-使用指南"><a href="#1-2-Tensorflow-使用指南" class="headerlink" title="1.2 Tensorflow 使用指南"></a>1.2 Tensorflow 使用指南</h3><p>TensorFlow <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-85v22c69.html" target="_blank" rel="external">张量变换</a></p>
<p>TensorFlow <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-y6p82c6e.html" target="_blank" rel="external">Training函数</a> ： tf.train 提供了一组帮助训练模型的类和函数。</p>
<h3 id="1-3-Tensorflow-函数介绍"><a href="#1-3-Tensorflow-函数介绍" class="headerlink" title="1.3 Tensorflow 函数介绍"></a>1.3 Tensorflow 函数介绍</h3><p>函数模块 : tf</p>
<p>TensorFlow <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-58lx2coj.html" target="_blank" rel="external">app模块 : 定义通用入口点脚本</a></p>
<p>TensorFlow contrib模块</p>
<p>TensorFlow 的 errors模块</p>
<p>estimator 模块 <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-xp3r2dl5.html" target="_blank" rel="external">估算器（Estimator）</a> ： 用于处理模型的高级工具。</p>
<blockquote>
<ul>
<li>TensorFlow的estimator类函数：<a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-hd9a2oyb.html" target="_blank" rel="external">tf.estimator.Estimator</a></li>
</ul>
</blockquote>
<p>TensorFlow 的 image模块</p>
<p>TensorFlow 使用之 <strong>tf.initializers</strong></p>
<p>TensorFlow 使用之 tf.keras</p>
<p>TensorFlow <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-59ay2s9i.html" target="_blank" rel="external">使用之 tf.layers</a></p>
<p>TensorFlow 使用之 <strong>tf.losses</strong></p>
<p>TensorFlow <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-ke8y2yhg.html" target="_blank" rel="external">使用之 tf.metrics</a></p>
<h3 id="1-4-TensorFlow-功能函数"><a href="#1-4-TensorFlow-功能函数" class="headerlink" title="1.4 TensorFlow 功能函数"></a>1.4 TensorFlow 功能函数</h3><p><strong>tf.get_variable</strong> 函数</p>
<p>TensorFlow范数：<code>tf.norm</code>函数</p>
<p>TensorFlow函数：<code>tf.ones</code></p>
<p>tf.one_hot函数：返回one-hot张量</p>
<p>TensorFlow随机值：tf.random_normal函数</p>
<p>tf.random_normal_initializer：TensorFlow初始化器</p>
<p>TensorFlow函数：tf.ones_initializer</p>
<p>TensorFlow占位符：<code>tf.placeholder</code></p>
<p>TensorFlow函数： <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-hckq2htb.html" target="_blank" rel="external">tf.reduce_mean</a></p>
<p>TensorFlow函数：<code>tf.reduce_sum</code></p>
<blockquote>
<p>axis = 0， 为纵向<br>axis = 1， 为横向</p>
</blockquote>
<h3 id="1-5-TensorFlow-手写数字分类问题"><a href="#1-5-TensorFlow-手写数字分类问题" class="headerlink" title="1.5 TensorFlow 手写数字分类问题"></a>1.5 TensorFlow 手写数字分类问题</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-bm7y28si.html" target="_blank" rel="external">TensorFlow 入门基础 W3C school</a></li>
<li><a href="https://blog.csdn.net/jerr__y/article/category/6747409" target="_blank" rel="external">大学之道，在明明德 - 我的Tensorflow学习之路</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[My Zsh Config Files]]></title>
      <url>http://sggo.me/2017/10/21/ops/ops-zsh-config/</url>
      <content type="html"><![CDATA[<p>我的配置文件 <strong>~/.zprofile</strong> 和 <strong>~/.zshrc</strong></p>
<a id="more"></a>
<h2 id="zprofile"><a href="#zprofile" class="headerlink" title=".zprofile"></a>.zprofile</h2><figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment">###################################################</span></span><br><span class="line"><span class="comment">###       blair custom config @2017.10.21       ###</span></span><br><span class="line"><span class="comment">###################################################</span></span><br><span class="line"></span><br><span class="line">MS=/usr/<span class="built_in">local</span>/xsoft/software</span><br><span class="line"></span><br><span class="line"><span class="comment">### JAVA ###</span></span><br><span class="line">JAVA_HOME=/Library/Java/JavaVirtualMachines/Contents/Home</span><br><span class="line">JAVA_BIN=<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line">PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/jre/lib/rt.jar:<span class="variable">$JAVA_HOME</span>/jre/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/jre/lib/tools.jar</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME JAVA_BIN PATH CLASSPATH</span><br><span class="line"></span><br><span class="line"><span class="comment">### Maven ###</span></span><br><span class="line">M2_HOME=/usr/<span class="built_in">local</span>/xsoft/software/apache-maven</span><br><span class="line">MAVEN_HOME=<span class="variable">$M2_HOME</span></span><br><span class="line">M3_HOME=<span class="variable">$M2_HOME</span></span><br><span class="line">PATH=<span class="variable">$M3_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="comment">#MAVEN_OPTS="-Xms128m -Xmx512m"</span></span><br><span class="line"><span class="built_in">export</span> MAVEN_HOME M2_HOME PATH</span><br><span class="line"></span><br><span class="line"><span class="comment">### Tomcat ###</span></span><br><span class="line">CATALINA_HOME=/usr/<span class="built_in">local</span>/xsoft/software/apache-tomcat</span><br><span class="line">PATH=<span class="variable">$CATALINA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> CATALINA_HOME PATH</span><br><span class="line"></span><br><span class="line"><span class="comment">### Scala ###</span></span><br><span class="line"><span class="comment">#export SCALA_HOME=/usr/local/xsoft/software/scala</span></span><br><span class="line"><span class="comment">#export SCALA_HOME=/usr/local/Cellar/scala/2.11.5</span></span><br><span class="line"><span class="comment">#export PATH=$&#123;SCALA_HOME&#125;/bin:$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Spark ###</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/xsoft/software/spark</span><br><span class="line"></span><br><span class="line"><span class="comment">### IPython ###</span></span><br><span class="line"><span class="built_in">alias</span> ipython=<span class="string">'python -m IPython'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Pyenv ###</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"/Users/blair/.pyenv/bin:<span class="variable">$PATH</span>"</span></span><br><span class="line"><span class="built_in">alias</span> pyenv_init=<span class="string">'eval "$(pyenv init -)" &amp;&amp; eval "$(pyenv virtualenv-init -)"'</span></span><br><span class="line">pyenv_init</span><br><span class="line"><span class="comment">#export PYENV_VIRTUALENV_DISABLE_PROMPT=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Node.js ###</span></span><br><span class="line"><span class="built_in">export</span> NVM_DIR=<span class="string">"<span class="variable">$HOME</span>/.nvm"</span></span><br><span class="line"><span class="comment">#[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; \. "$NVM_DIR/nvm.sh"  # This loads nvm</span></span><br><span class="line"><span class="comment">#[ -s "$NVM_DIR/bash_completion" ] &amp;&amp; \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### LANG ###</span></span><br><span class="line"><span class="built_in">export</span> LC_ALL=en_US.UTF-8</span><br><span class="line"><span class="built_in">export</span> LANG=en_US.UTF-8</span><br><span class="line"></span><br><span class="line"><span class="comment">### Fast Function ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># show ip</span></span><br><span class="line"><span class="built_in">alias</span> ip=<span class="string">'ipconfig getifaddr en0'</span></span><br><span class="line"><span class="built_in">alias</span> ip0=<span class="string">'ipconfig getifaddr en0'</span></span><br><span class="line"><span class="built_in">alias</span> ip1=<span class="string">'ipconfig getifaddr en1'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># shutdown</span></span><br><span class="line"><span class="built_in">alias</span> shuth=<span class="string">'sudo shutdown -h now'</span></span><br><span class="line"><span class="built_in">alias</span> shutr=<span class="string">'sudo shutdown -r now'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pip list</span></span><br><span class="line"><span class="built_in">alias</span> pipl=<span class="string">'pip list --format=columns'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># lsof -i:port</span></span><br><span class="line"><span class="built_in">alias</span> lsofi=<span class="string">'lsof -i:'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># http_proxy</span></span><br><span class="line"><span class="built_in">alias</span> hp=<span class="string">"http_proxy=http://localhost:8123"</span></span><br><span class="line"><span class="built_in">alias</span> hps=<span class="string">"https_proxy=http://localhost:8123"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hexo</span></span><br><span class="line"><span class="built_in">alias</span> hx=<span class="string">'source ~/.nvm/nvm.sh'</span></span><br><span class="line"><span class="built_in">alias</span> hs=<span class="string">'hx &amp;&amp; cd ~/ghome/blog &amp;&amp; hexo s'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># deploy blog</span></span><br><span class="line"><span class="built_in">alias</span> db=<span class="string">'hx &amp;&amp; cd ~/ghome/blog &amp;&amp; hexo clean &amp;&amp; sh dp.sh'</span></span><br><span class="line"><span class="built_in">alias</span> dg=<span class="string">'cd ~/ghome/blog &amp;&amp; hexo clean &amp;&amp; sh dp.sh'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pyenv</span></span><br><span class="line"><span class="built_in">alias</span> pyenv_install_python_pre=<span class="string">'source $MS/custom-machine/pyenv-install.sh'</span></span><br><span class="line"><span class="built_in">alias</span> py2=<span class="string">'pyenv_init &amp;&amp; pyenv activate vpy2 &amp;&amp; clear &amp;&amp; python -V'</span></span><br><span class="line"><span class="built_in">alias</span> py3=<span class="string">'pyenv_init &amp;&amp; pyenv activate vpy3 &amp;&amp; clear &amp;&amp; python -V'</span></span><br><span class="line"><span class="built_in">alias</span> pyde=<span class="string">'pyenv deactivate'</span></span><br><span class="line"><span class="built_in">alias</span> pys=<span class="string">'pyenv activate vpy3 &amp;&amp; pyenv global system &amp;&amp; pyde &amp;&amp; python -V'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># zshrc</span></span><br><span class="line"><span class="built_in">alias</span> vzp=<span class="string">'vim ~/.zprofile'</span></span><br><span class="line"><span class="built_in">alias</span> szp=<span class="string">'source ~/.zprofile'</span></span><br></pre></td></tr></table></figure>
<h2 id="zshrc"><a href="#zshrc" class="headerlink" title=".zshrc"></a>.zshrc</h2><figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># If you come from bash you might have to change your $PATH.</span></span><br><span class="line"><span class="comment"># export PATH=$HOME/bin:/usr/local/bin:$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Path to your oh-my-zsh installation.</span></span><br><span class="line"><span class="built_in">export</span> ZSH=/Users/blair/.oh-my-zsh</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set name of the theme to load. Optionally, if you set this to "random"</span></span><br><span class="line"><span class="comment"># it'll load a random theme each time that oh-my-zsh is loaded.</span></span><br><span class="line"><span class="comment"># See https://github.com/robbyrussell/oh-my-zsh/wiki/Themes</span></span><br><span class="line">ZSH_THEME=<span class="string">"robbyrussell"</span></span><br><span class="line"><span class="comment"># by blair add @2017-10-10</span></span><br><span class="line"><span class="comment">#ZSH_THEME="ys" </span></span><br><span class="line">ZSH_THEME=<span class="string">"astro"</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to use case-sensitive completion.</span></span><br><span class="line"><span class="comment"># CASE_SENSITIVE="true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to use hyphen-insensitive completion. Case</span></span><br><span class="line"><span class="comment"># sensitive completion must be off. _ and - will be interchangeable.</span></span><br><span class="line"><span class="comment"># HYPHEN_INSENSITIVE="true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to disable bi-weekly auto-update checks.</span></span><br><span class="line"><span class="comment"># DISABLE_AUTO_UPDATE="true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to change how often to auto-update (in days).</span></span><br><span class="line"><span class="comment"># export UPDATE_ZSH_DAYS=13</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to disable colors in ls.</span></span><br><span class="line"><span class="comment"># DISABLE_LS_COLORS="true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to disable auto-setting terminal title.</span></span><br><span class="line"><span class="comment"># DISABLE_AUTO_TITLE="true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to enable command auto-correction.</span></span><br><span class="line"><span class="comment"># ENABLE_CORRECTION="true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to display red dots whilst waiting for completion.</span></span><br><span class="line"><span class="comment"># COMPLETION_WAITING_DOTS="true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line if you want to disable marking untracked files</span></span><br><span class="line"><span class="comment"># under VCS as dirty. This makes repository status check for large repositories</span></span><br><span class="line"><span class="comment"># much, much faster.</span></span><br><span class="line"><span class="comment"># DISABLE_UNTRACKED_FILES_DIRTY="true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line if you want to change the command execution time</span></span><br><span class="line"><span class="comment"># stamp shown in the history command output.</span></span><br><span class="line"><span class="comment"># The optional three formats: "mm/dd/yyyy"|"dd.mm.yyyy"|"yyyy-mm-dd"</span></span><br><span class="line"><span class="comment"># HIST_STAMPS="mm/dd/yyyy"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Would you like to use another custom folder than $ZSH/custom?</span></span><br><span class="line"><span class="comment"># ZSH_CUSTOM=/path/to/new-custom-folder</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Which plugins would you like to load? (plugins can be found in ~/.oh-my-zsh/plugins/*)</span></span><br><span class="line"><span class="comment"># Custom plugins may be added to ~/.oh-my-zsh/custom/plugins/</span></span><br><span class="line"><span class="comment"># Example format: plugins=(rails git textmate ruby lighthouse)</span></span><br><span class="line"><span class="comment"># Add wisely, as too many plugins slow down shell startup.</span></span><br><span class="line">plugins=(git)</span><br><span class="line">plugins=(git autojump) <span class="comment"># by blair add @2017-10-10</span></span><br><span class="line"><span class="comment">#[[ -s $(brew --prefix)/etc/profile.d/autojump.sh ]] &amp;&amp; . $(brew --prefix)/etc/profile.d/autojump.sh</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> <span class="variable">$ZSH</span>/oh-my-zsh.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># User configuration</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># export MANPATH="/usr/local/man:$MANPATH"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You may need to manually set your language environment</span></span><br><span class="line"><span class="comment"># export LANG=en_US.UTF-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Preferred editor for local and remote sessions</span></span><br><span class="line"><span class="comment"># if [[ -n $SSH_CONNECTION ]]; then</span></span><br><span class="line"><span class="comment">#   export EDITOR='vim'</span></span><br><span class="line"><span class="comment"># else</span></span><br><span class="line"><span class="comment">#   export EDITOR='mvim'</span></span><br><span class="line"><span class="comment"># fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compilation flags</span></span><br><span class="line"><span class="comment"># export ARCHFLAGS="-arch x86_64"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh</span></span><br><span class="line"><span class="comment"># export SSH_KEY_PATH="~/.ssh/rsa_id"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set personal aliases, overriding those provided by oh-my-zsh libs,</span></span><br><span class="line"><span class="comment"># plugins, and themes. Aliases can be placed here, though oh-my-zsh</span></span><br><span class="line"><span class="comment"># users are encouraged to define aliases within the ZSH_CUSTOM folder.</span></span><br><span class="line"><span class="comment"># For a full list of active aliases, run `alias`.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Example aliases</span></span><br><span class="line"><span class="comment"># alias zshconfig="mate ~/.zshrc"</span></span><br><span class="line"><span class="comment"># alias ohmyzsh="mate ~/.oh-my-zsh"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###################################################</span></span><br><span class="line"><span class="comment">###       blair custom config @2017.10.21       ###</span></span><br><span class="line"><span class="comment">###################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Fast Login Machine Function ###</span></span><br><span class="line"><span class="comment"># ssh add</span></span><br><span class="line"><span class="built_in">alias</span> sd=<span class="string">'ssh-add'</span></span><br><span class="line"><span class="built_in">alias</span> x2=<span class="string">'ssh loguser@192.168.***.*'</span></span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Macos Terminal Set Shadowsocks]]></title>
      <url>http://sggo.me/2017/10/19/ops/ops-mac-for-shadowsocks/</url>
      <content type="html"><![CDATA[<ol>
<li>first, you need to have vps</li>
<li>second, you need to have shadowsocks app</li>
</ol>
<a id="more"></a>
<p><img src="/images/ops/ops-vpn-shadowsocks.png" width="520" height="300" align="middle" img="">  </p>
<h2 id="install"><a href="#install" class="headerlink" title="install"></a>install</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew install polipo</span><br></pre></td></tr></table></figure>
<h2 id="config"><a href="#config" class="headerlink" title="config"></a>config</h2><p>设置每次登陆启动polipo</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ln -sfv /usr/local/opt/polipo/*.plist ~/Library/LaunchAgents</span><br></pre></td></tr></table></figure>
<p>修改文件 <code>/usr/local/opt/polipo/homebrew.mxcl.polipo.plist</code> 设置parentProxy</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;</span><br><span class="line">&lt;plist version=&quot;1.0&quot;&gt;</span><br><span class="line">  &lt;dict&gt;</span><br><span class="line">    &lt;key&gt;Label&lt;/key&gt;</span><br><span class="line">    &lt;string&gt;homebrew.mxcl.polipo&lt;/string&gt;</span><br><span class="line">    &lt;key&gt;RunAtLoad&lt;/key&gt;</span><br><span class="line">    &lt;true/&gt;</span><br><span class="line">    &lt;key&gt;KeepAlive&lt;/key&gt;</span><br><span class="line">    &lt;true/&gt;</span><br><span class="line">    &lt;key&gt;ProgramArguments&lt;/key&gt;</span><br><span class="line">    &lt;array&gt;</span><br><span class="line">        &lt;string&gt;/usr/local/opt/polipo/bin/polipo&lt;/string&gt;</span><br><span class="line">        &lt;string&gt;socksParentProxy=localhost:1080&lt;/string&gt;</span><br><span class="line">    &lt;/array&gt;</span><br><span class="line">    &lt;!-- Set `ulimit -n 20480`. The default OS X limit is 256, that&apos;s</span><br><span class="line">         not enough for Polipo (displays &apos;too many files open&apos; errors).</span><br><span class="line">         It seems like you have no reason to lower this limit</span><br><span class="line">         (and unlikely will want to raise it). --&gt;</span><br><span class="line">    &lt;key&gt;SoftResourceLimits&lt;/key&gt;</span><br><span class="line">    &lt;dict&gt;</span><br><span class="line">      &lt;key&gt;NumberOfFiles&lt;/key&gt;</span><br><span class="line">      &lt;integer&gt;20480&lt;/integer&gt;</span><br><span class="line">    &lt;/dict&gt;</span><br><span class="line">  &lt;/dict&gt;</span><br><span class="line">&lt;/plist&gt;</span><br></pre></td></tr></table></figure>
<p>修改的地方是增加了 <code>&lt;string&gt;socksParentProxy=localhost:1080&lt;/string&gt;</code></p>
<h2 id="start-stop"><a href="#start-stop" class="headerlink" title="start / stop"></a>start / stop</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.polipo.plist</span><br><span class="line">launchctl load ~/Library/LaunchAgents/homebrew.mxcl.polipo.plist</span><br></pre></td></tr></table></figure>
<h2 id="setting-profile"><a href="#setting-profile" class="headerlink" title="setting profile"></a>setting profile</h2><p>vim ~/.zshrc</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#export http_proxy=http://localhost:8123</span><br><span class="line">alias hp=&quot;http_proxy=http://localhost:8123&quot;</span><br></pre></td></tr></table></figure>
<p>source ~/.zshrc</p>
<h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜ hp curl ip.cn</span><br><span class="line">当前 IP：97.64.**.** 来自：美国</span><br></pre></td></tr></table></figure>
<h2 id="reference-article"><a href="#reference-article" class="headerlink" title="reference article"></a>reference article</h2><ul>
<li><a href="http://droidyue.com/blog/2016/04/04/set-shadowsocks-proxy-for-terminal/" target="_blank" rel="external">技术小黑屋</a></li>
<li><a href="https://segmentfault.com/a/1190000008449046" target="_blank" rel="external">Mac+shadowsocks+polipo快捷实现终端科学上网</a></li>
<li><a href="http://blog.liujiangbei.com/14532622130324.html" target="_blank" rel="external">VPS-install-shadowsocks-proxy</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Macos Sierra Uninstall App & Solve Allow apps Anywhere]]></title>
      <url>http://sggo.me/2017/10/19/ops/ops-mac-sierra-some-tips/</url>
      <content type="html"><![CDATA[<p>Here are some tips to help you fun macos sierra</p>
<a id="more"></a>
<h2 id="MacOS-Sierra-amp-High-Sierra-Complete-Uninstall-App"><a href="#MacOS-Sierra-amp-High-Sierra-Complete-Uninstall-App" class="headerlink" title="MacOS Sierra &amp; High Sierra Complete Uninstall App"></a>MacOS Sierra &amp; High Sierra Complete Uninstall App</h2><blockquote>
<p>About This Mac -&gt; Storage -&gt; Manage -&gt; Applications -&gt; Delete</p>
</blockquote>
<h2 id="How-to-Allow-Apps-from-Anywhere-in-Gatekeeper-for-macOS-High-Sierra"><a href="#How-to-Allow-Apps-from-Anywhere-in-Gatekeeper-for-macOS-High-Sierra" class="headerlink" title="How to Allow Apps from Anywhere in Gatekeeper for macOS High Sierra"></a>How to Allow Apps from Anywhere in Gatekeeper for macOS High Sierra</h2><p>1). Open the Terminal app from the /Applications/Utilities/ folder and then enter the following command syntax:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo spctl --master-disable</span><br></pre></td></tr></table></figure>
<p>Hit return and authenticate with an admin password</p>
<p>2). Relaunch System Preferences and go to “Security &amp; Privacy” and the “General” tab</p>
<p>3). You will now see the “Anywhere” option under ‘Allow apps downloaded from:’ Gatekeeper options</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pyenv Install For Virtual Multi Python Version Switch]]></title>
      <url>http://sggo.me/2017/10/18/ops/ops-pyenv-install/</url>
      <content type="html"><![CDATA[<p>It needs to be used in both python2 and python3 environments, or different packages need to be installed in different projects.</p>
<a id="more"></a>
<p>we hope that the packages installed between different projects do not interfere with each other, and then you can configure the virtual environment of Python using pyenv.</p>
<h2 id="installation-pyenv"><a href="#installation-pyenv" class="headerlink" title="installation pyenv"></a>installation pyenv</h2><p><a href="https://github.com/pyenv/pyenv-installer" target="_blank" rel="external">Official Pyenv Install</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">"~/.pyenv/bin:<span class="variable">$PATH</span>"</span></span><br><span class="line"><span class="built_in">eval</span> <span class="string">"<span class="variable">$(pyenv init -)</span>"</span></span><br><span class="line"><span class="built_in">eval</span> <span class="string">"<span class="variable">$(pyenv virtualenv-init -)</span>"</span></span><br><span class="line"><span class="comment">#export PYENV_VIRTUALENV_DISABLE_PROMPT=1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>按照官方文档配置即可，mac zsh 用户，将 以上三句放入到 .zshrc 即可。</p>
</blockquote>
<h2 id="see-available-versions"><a href="#see-available-versions" class="headerlink" title="see available versions"></a>see available versions</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pyenv install -l</span><br></pre></td></tr></table></figure>
<h2 id="install-python-in-virtual-env"><a href="#install-python-in-virtual-env" class="headerlink" title="install python in virtual env"></a>install python in virtual env</h2><p>install python 2.7.14</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pyenv install 2.7.14</span><br></pre></td></tr></table></figure>
<p>install python 3.6.3</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pyenv install 3.6.3</span><br></pre></td></tr></table></figure>
<blockquote>
<p> <a href="https://github.com/pyenv/pyenv/issues/993" target="_blank" rel="external">solve macOS High Sierra: ERROR: The Python ssl extension was not compiled. Missing the OpenSSL lib?</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># about zlib</span><br><span class="line">export CFLAGS=&quot;-I$(xcrun --show-sdk-path)/usr/include&quot;</span><br><span class="line"># about readline</span><br><span class="line">export CFLAGS=&quot;-I$(brew --prefix readline)/include $CFLAGS&quot;</span><br><span class="line">export LDFLAGS=&quot;-L$(brew --prefix readline)/lib $LDFLAGS&quot;</span><br><span class="line"># about openssl</span><br><span class="line">export CFLAGS=&quot;-I$(brew --prefix openssl)/include $CFLAGS&quot;</span><br><span class="line">export LDFLAGS=&quot;-L$(brew --prefix openssl)/lib $LDFLAGS&quot;</span><br><span class="line"># about SQLite (maybe not necessary)</span><br><span class="line">export CFLAGS=&quot;-I$(brew --prefix sqlite)/include $CFLAGS&quot;</span><br><span class="line">export LDFLAGS=&quot;-L$(brew --prefix sqlite)/lib $LDFLAGS&quot;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>Set or show the global Python version</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pyenv global 3.6.3 or system</span><br></pre></td></tr></table></figure>
<blockquote>
<p>system stands for this mac</p>
</blockquote>
<p>show list all Python versions available to pyenv</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pyenv versions</span><br></pre></td></tr></table></figure>
<h2 id="create-virtual-env"><a href="#create-virtual-env" class="headerlink" title="create virtual env"></a>create virtual env</h2><p>create current <code>3.6.3 version python</code> virtual env</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pyenv virtualenv vpy3</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>vpy3</strong> is this virtual env alias</p>
</blockquote>
<h2 id="pyenv-activate-amp-deactivate"><a href="#pyenv-activate-amp-deactivate" class="headerlink" title="pyenv activate &amp; deactivate"></a>pyenv activate &amp; deactivate</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pyenv activate vpy3</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这是时候就可以开始pip安装依赖包了</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜ pyenv activate vpy3</span><br><span class="line">pyenv-virtualenv: prompt changing will be removed from future release. configure `export PYENV_VIRTUALENV_DISABLE_PROMPT=1&apos; to simulate the behavior.</span><br><span class="line">(vpy3)</span><br><span class="line"># ~ [12:25:44]</span><br><span class="line">➜ python -V</span><br><span class="line">Python 3.6.3</span><br><span class="line">(vpy3)</span><br><span class="line"># ~ [12:25:49]</span><br><span class="line">➜ pyenv deactivate vpy3</span><br><span class="line"># ~ [12:25:57]</span><br><span class="line">➜ python -V</span><br><span class="line">Python 2.7.10</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Common Useful Links]]></title>
      <url>http://sggo.me/2017/10/14/ops/ops-common-links/</url>
      <content type="html"><![CDATA[<p>Here are some useful links</p>
<a id="more"></a>
<h2 id="Deep-Learning-Coursera"><a href="#Deep-Learning-Coursera" class="headerlink" title="Deep Learning Coursera"></a>Deep Learning Coursera</h2><ul>
<li><a href="http://daniellaah.github.io/" target="_blank" rel="external">daniellaah.github.io</a></li>
<li><a href="https://www.ctolib.com/Yukong-Deeplearning-ai-Solutions.html" target="_blank" rel="external">deeplearning.ai深度学习课程字幕翻译项目</a></li>
<li><a href="https://www.cnblogs.com/marsggbo/" target="_blank" rel="external">互道晚安，王者峡谷见 机器学习 &amp; 深度学习 NG 笔记</a></li>
<li><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="external">deeplearning.ai（吴恩达老师的深度学习课程笔记及资源）</a></li>
<li><a href="https://github.com/theBigDataDigest/Andrew-Ng-deeplearning-part-5-Course-notes-in-Chinese/blob/master/Andrew-Ng-deeplearning.ai-part-5-Course%20notes.pdf" target="_blank" rel="external">大数据文摘 deeplearning.ai Sequence Models 中文笔记</a></li>
<li><a href="https://kulbear.github.io/pdf/sequence-models.pdf" target="_blank" rel="external">Sequence Models 英文版笔记</a></li>
<li><a href="https://blog.csdn.net/column/details/dl-nlp.html" target="_blank" rel="external">寒小阳 - 深度学习与自然语言处理</a></li>
<li><a href="https://blog.csdn.net/han_xiaoyang/article/category/5877239" target="_blank" rel="external">寒小阳主页</a></li>
<li><a href="http://people.csail.mit.edu/bzhou/" target="_blank" rel="external">Bolei Zhou @Assistant Professor CUHK</a></li>
<li><a href="https://www.zhihu.com/question/49432647/answer/144958145" target="_blank" rel="external">计算机视觉和自然语言处理，哪个更具有发展前景呢，还是各有千秋呢？</a></li>
</ul>
<h2 id="Recommend"><a href="#Recommend" class="headerlink" title="Recommend"></a>Recommend</h2><ul>
<li><a href="https://blog.csdn.net/bvl10101111/article/details/78822739" target="_blank" rel="external">推荐系统经典论文文献及业界应用</a></li>
</ul>
<h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><ul>
<li><a href="https://github.com/explosion/sense2vec" target="_blank" rel="external">sense2vec</a></li>
<li><a href="https://www.zybuluo.com/hanxiaoyang/note/472184" target="_blank" rel="external">zybuluo hanxiaoyang</a></li>
<li><a href="https://radimrehurek.com/gensim/" target="_blank" rel="external">gensim</a></li>
<li><a href="https://yq.aliyun.com/articles/158691" target="_blank" rel="external">自然语言理解-从规则到深度学习</a></li>
<li><a href="https://plushunter.github.io/" target="_blank" rel="external">Free Will</a></li>
<li><a href="http://www.wildml.com/" target="_blank" rel="external">www.wildml.com</a></li>
</ul>
<h2 id="Friends"><a href="#Friends" class="headerlink" title="Friends"></a>Friends</h2><ul>
<li><a href="https://blog.fazero.me/" target="_blank" rel="external">fazero</a></li>
<li><a href="http://wuchong.me/" target="_blank" rel="external">ali wuchong</a></li>
<li><a href="http://www.cnblogs.com/maybe2030/" target="_blank" rel="external">Poll的笔记</a></li>
<li><a href="http://www.ruanyifeng.com/blog/" target="_blank" rel="external">阮一峰的网络日志</a></li>
<li><a href="https://david-abel.github.io/" target="_blank" rel="external">ICML 学霸 David Abel</a></li>
<li><a href="https://coolshell.cn" target="_blank" rel="external">酷壳</a></li>
</ul>
<h2 id="vps"><a href="#vps" class="headerlink" title="vps"></a>vps</h2><ul>
<li><a href="https://www.yuntionly.com/" target="_blank" rel="external">www.yuntionly.com</a></li>
<li><a href="https://www.wisevpn.net/" target="_blank" rel="external">www.wisevpn.net</a></li>
<li><a href="https://www.banwago.com/797.html" target="_blank" rel="external">www.banwago.com</a></li>
<li><a href="https://www.godaddy.com/" target="_blank" rel="external">www.godaddy.com</a></li>
<li><a href="https://www.cnbanwagong.com/4.html" target="_blank" rel="external">搬瓦工中文网</a></li>
<li><a href="https://bwh1.net/" target="_blank" rel="external">搬瓦工购买页面</a></li>
<li><a href="http://ulis.me/archives/5909" target="_blank" rel="external">搬瓦工VPS续费的那些事</a></li>
<li><a href="https://bwhgw.wordpress.com/2018/03/30/ban_wa_gong_qu_xiao_yi_jian_ss_gong_neng_hou_jiao_nin_san_zhong_fang_fa_qing_song_da_jian_ss/" target="_blank" rel="external">搬瓦工取消一键SS功能后，教您三种方法轻松搭建SS！</a></li>
<li><a href="https://kiwivm.64clouds.com/preloader.php?load=/main-exec.php?mode=extras_shadowsocks" target="_blank" rel="external">登陆到搬瓦工后台, 一键安装SS，类似方法一（小白适用</a></li>
<li><a href="https://blog.csdn.net/qq_31897023/article/details/82533887" target="_blank" rel="external">Centos6.8搭建SS</a></li>
</ul>
<blockquote>
<p>ssh 登录搬瓦工机器</p>
<ol>
<li>stop server @Main controls</li>
<li>Root password modification</li>
<li>start Server</li>
<li>Root shell - interactive</li>
<li><p>vi /etc/ssh/sshd_config, add </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PermitRootLogin yes</span><br><span class="line">Port 22</span><br></pre></td></tr></table></figure>
</li>
<li><p>/etc/init.d/sshd restart</p>
</li>
<li>ssh root@ip</li>
</ol>
</blockquote>
<h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><ul>
<li><a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000" target="_blank" rel="external">廖雪峰 Python 3 </a></li>
<li><a href="https://github.com/elastic/elasticsearch-dsl-py" target="_blank" rel="external">elasticsearch-dsl-py</a></li>
<li><a href="https://pypi.python.org/pypi" target="_blank" rel="external">pypi.python.org/pypi</a></li>
<li><a href="https://github.com/pyenv/pyenv-installer" target="_blank" rel="external">install pyenv</a></li>
<li><a href="http://flask.pocoo.org/" target="_blank" rel="external">flask microframework</a></li>
</ul>
<blockquote>
<p>pip install elasticsearch_dsl==0.0.11</p>
</blockquote>
<h2 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h2><ul>
<li><a href="http://www.runoob.com/linux/linux-command-manual.html" target="_blank" rel="external">runoob linux</a></li>
<li><a href="https://shazi.info/mac-osx-%E6%B2%92%E6%9C%89%E7%9A%84-rename%EF%BC%8C%E7%94%A8-brew-%E6%8A%93%E5%9B%9E%E4%BE%86%EF%BD%9E/" target="_blank" rel="external">Mac OSX 沒有的 rename，用 brew 抓回來～</a></li>
</ul>
<h2 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h2><ul>
<li><a href="http://spark.apache.org/" target="_blank" rel="external">spark.apache.org</a></li>
<li><a href="http://spark.apache.org/docs/1.6.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="external">python spark 1.6.3</a></li>
<li><a href="https://github.com/apache/spark" target="_blank" rel="external">github spark</a></li>
</ul>
<p>在 pycharm 上配置 pyspark</p>
<ul>
<li><a href="https://blog.csdn.net/rifengxxc/article/details/74503119" target="_blank" rel="external">pycharm 上配置 pyspark</a></li>
<li><a href="https://blog.csdn.net/suzyu12345/article/details/53885092" target="_blank" rel="external">Pycharm开发spark程序</a></li>
</ul>
<h2 id="devops"><a href="#devops" class="headerlink" title="devops"></a>devops</h2><ul>
<li>免登陆设置 ssh-copy-id -i id_rsa.pub hdfs@192.192.0.27</li>
</ul>
<h2 id="mac"><a href="#mac" class="headerlink" title="mac"></a>mac</h2><ul>
<li><a href="https://www.zhihu.com/question/20021861" target="_blank" rel="external">macOS (OS X) 有哪些常用的快捷键？</a></li>
</ul>
<h2 id="比特币"><a href="#比特币" class="headerlink" title="比特币"></a>比特币</h2><ul>
<li><a href="http://www.btcranks.com/hk/" target="_blank" rel="external">港台数字货币交易平台排名</a></li>
<li><a href="https://www.bitfinex.com/" target="_blank" rel="external">bitfinex</a></li>
<li><a href="https://www.binance.com/en" target="_blank" rel="external">binance</a></li>
<li><a href="https://www.zhihu.com/question/269003572" target="_blank" rel="external">如何把火币网的比特币移到OKEX？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36776300" target="_blank" rel="external">币安(Binance) | 注册和使用教程</a></li>
<li><a href="https://www.bitoex.com/dashboard/statistics?locale=zh-CN" target="_blank" rel="external">台湾币托</a></li>
<li><a href="https://otcbtc.com/" target="_blank" rel="external">OTCBTC - 支持场外交易，支付宝</a></li>
</ul>
<h2 id="blog"><a href="#blog" class="headerlink" title="blog"></a>blog</h2><ul>
<li><a href="https://dotblogs.com.tw/hatelove/2017/03/26/why-engineers-should-keep-blogging" target="_blank" rel="external">鼓励工程师写blog</a></li>
<li><a href="https://coolshell.cn/articles/17583.html" target="_blank" rel="external">技术人员的发展之路</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera 7 - Support Vector Machines]]></title>
      <url>http://sggo.me/2017/10/13/ml/coursera-ng-w7-svm/</url>
      <content type="html"><![CDATA[<p>From <code>Logistic Regression</code> to <code>Support Vector Machines</code></p>
<a id="more"></a>
<h2 id="1-Large-Margin-Classification"><a href="#1-Large-Margin-Classification" class="headerlink" title="1. Large Margin Classification"></a>1. Large Margin Classification</h2><p><strong>Alternation view of logistic regression</strong></p>
<p>$ \begin{align} h_\theta (x) = g({\theta^T x}) = \dfrac{1}{1 + e^{-\theta^T x}} \end{align}  \; , \; h_\theta (x) \in [0, 1] $ </p>
<p><img src="/images/ml/coursera/ml-ng-w3-02.png" width="820" height="500" align="middle" img=""></p>
<blockquote>
<p>$ y = 1 \; when \; h_\theta(x) = g(\theta^T x) \geq 0.5 \; when \; \theta^T x \geq 0 $.   </p>
<p>$ y = 0 \; when \; h_\theta(x) = g(\theta^T x) \le 0.5 \; when \; \theta^T x \le 0 $ </p>
</blockquote>
<p>We can compress our cost function’s two conditional cases into one case:</p>
<p>$ \mathrm{Cost}(h_\theta(x),y) = - y \cdot \log(h_\theta(x)) - (1 - y) \cdot \log(1 - h_\theta(x))$</p>
<p>We can fully write out our entire cost function as follows:</p>
<p>$<br>J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]<br>$</p>
<p>$<br>J(\theta) = \mathop{min}\limits_{_\theta} \frac{1}{m} \left[ \displaystyle \sum_{i=1}^m y^{(i)}\ \left(-\log h_\theta (x^{(i)}) \right) + (1 - y^{(i)}) \left( - \log (1 - h_\theta(x^{(i)})) \right) \right]+ \frac{\lambda}{2m} \displaystyle \sum_{j=1}^n \theta_j^2<br>$</p>
<blockquote>
<p>$cost_1(\theta^T x^{i}) = -\log h_\theta (x^{(i)})$<br>$cost_0(\theta^T x^{i}) = - \log (1 - h_\theta(x^{(i)}))$</p>
</blockquote>
<p>$<br>J(\theta) = \mathop{min}\limits_{_\theta} \frac{1}{m} \left[ \displaystyle \sum_{i=1}^m y^{(i)}\ \left(cost_1(\theta^T x^{i}) \right) + (1 - y^{(i)}) \left( cost_0(\theta^T x^{i}) \right) \right]+ \frac{\lambda}{2m} \displaystyle \sum_{j=1}^n \theta_j^2<br>$</p>
<h3 id="1-1-Optimization-Objective"><a href="#1-1-Optimization-Objective" class="headerlink" title="1.1 Optimization Objective"></a>1.1 Optimization Objective</h3><p>$<br>J(\theta) = \mathop{min}\limits_{_\theta} \frac{1}{m} \left[ \displaystyle \sum_{i=1}^m y^{(i)}\ \left(cost_1(\theta^T x^{i}) \right) + (1 - y^{(i)}) \left( cost_0(\theta^T x^{i}) \right) \right]+ \frac{\lambda}{2m} \displaystyle \sum_{j=1}^n \theta_j^2<br>$</p>
<blockquote>
<p>令 $C = \frac{1}{\theta}$ </p>
</blockquote>
<p>$<br>J(\theta) = \mathop{min}\limits_{_\theta} C \displaystyle \sum_{i=1}^m \left[  y^{(i)}\ cost_1(\theta^T x^{i}) + (1 - y^{(i)}) cost_0(\theta^T x^{i}) \right]+ \frac{1}{2m} \displaystyle \sum_{j=1}^n \theta_j^2<br>$</p>
<h3 id="1-2-Large-Margin-Intuition"><a href="#1-2-Large-Margin-Intuition" class="headerlink" title="1.2 Large Margin Intuition"></a>1.2 Large Margin Intuition</h3><p><img src="/images/ml/coursera/ml-ng-w7-svm-1.png" width="620" height="400" align="middle" img=""></p>
<p><img src="/images/ml/coursera/ml-ng-w7-svm-2.png" width="620" height="400" align="middle" img=""></p>
<p><img src="/images/ml/coursera/ml-ng-w7-svm-3.png" width="620" height="400" align="middle" img=""></p>
<h3 id="1-3-Mathematics-Behind-Large-Margin-Classification"><a href="#1-3-Mathematics-Behind-Large-Margin-Classification" class="headerlink" title="1.3 Mathematics Behind Large Margin Classification"></a>1.3 Mathematics Behind Large Margin Classification</h3><p><img src="/images/ml/coursera/ml-ng-w7-svm-4.png" width="620" height="400" align="middle" img=""></p>
<p><img src="/images/ml/coursera/ml-ng-w7-svm-5.png" width="620" height="400" align="middle" img=""></p>
<p><img src="/images/ml/coursera/ml-ng-w7-svm-6.png" width="620" height="400" align="middle" img=""></p>
<h2 id="2-Kernels"><a href="#2-Kernels" class="headerlink" title="2. Kernels"></a>2. Kernels</h2><h3 id="2-1-Kernels-I"><a href="#2-1-Kernels-I" class="headerlink" title="2.1 Kernels I"></a>2.1 Kernels I</h3><h3 id="2-2-Kernels-II"><a href="#2-2-Kernels-II" class="headerlink" title="2.2 Kernels II"></a>2.2 Kernels II</h3><h2 id="3-SVMs-in-Practice"><a href="#3-SVMs-in-Practice" class="headerlink" title="3. SVMs in Practice"></a>3. SVMs in Practice</h2><h3 id="3-1-Using-An-SVM"><a href="#3-1-Using-An-SVM" class="headerlink" title="3.1 Using An SVM"></a>3.1 Using An SVM</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/24638007" target="_blank" rel="external">零基础学SVM—Support Vector Machine(一)</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[My Blog Config.yml]]></title>
      <url>http://sggo.me/2017/10/08/ops/ops-hexo-blog_config.yml/</url>
      <content type="html"><![CDATA[<p>hexo blog config.yml file</p>
<a id="more"></a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Hexo Configuration</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/configuration.html</span></span><br><span class="line"><span class="comment">## Source: https://github.com/hexojs/hexo/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Site</span></span><br><span class="line">title: Home</span><br><span class="line">subtitle: 春有百花秋有月，夏有涼風冬有雪 .</span><br><span class="line">description: Everyone should not forget his dream</span><br><span class="line">author: Blair Chan</span><br><span class="line"><span class="comment">#avatar: /images/avatar.jpeg</span></span><br><span class="line"></span><br><span class="line">language: </span><br><span class="line">- en</span><br><span class="line">- zh-Hans</span><br><span class="line">- zh-tw</span><br><span class="line">timezone:</span><br><span class="line"></span><br><span class="line"><span class="comment">#leancloud_visitors:</span></span><br><span class="line"><span class="comment">#  enable: true</span></span><br><span class="line"><span class="comment">#  app_id: #&lt;AppID&gt;</span></span><br><span class="line"><span class="comment">#  app_key: #&lt;AppKEY&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#comments</span></span><br><span class="line">disqus_shortname: blairos-sn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'</span></span><br><span class="line">url: http://iequa.com/</span><br><span class="line">root: /</span><br><span class="line">permalink: :year/:month/:day/:title/</span><br><span class="line">permalink_defaults:</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory</span></span><br><span class="line"><span class="built_in">source</span>_dir: <span class="built_in">source</span></span><br><span class="line">public_dir: public</span><br><span class="line">tag_dir: tags</span><br><span class="line">archive_dir: archives</span><br><span class="line">category_dir: categories</span><br><span class="line">code_dir: downloads/code</span><br><span class="line">i18n_dir: :lang</span><br><span class="line">skip_render:</span><br><span class="line"></span><br><span class="line"><span class="comment"># Writing</span></span><br><span class="line">new_post_name: :title.md <span class="comment"># File name of new posts</span></span><br><span class="line">default_layout: post</span><br><span class="line">titlecase: <span class="literal">false</span> <span class="comment"># Transform title into titlecase</span></span><br><span class="line">external_link: <span class="literal">true</span> <span class="comment"># Open external links in new tab</span></span><br><span class="line">filename_<span class="keyword">case</span>: 0</span><br><span class="line">render_drafts: <span class="literal">false</span></span><br><span class="line">post_asset_folder: <span class="literal">false</span></span><br><span class="line">relative_link: <span class="literal">false</span></span><br><span class="line">future: <span class="literal">true</span></span><br><span class="line">highlight:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  line_number: <span class="literal">false</span></span><br><span class="line">  auto_detect: <span class="literal">false</span></span><br><span class="line">  tab_replace:</span><br><span class="line"></span><br><span class="line"><span class="comment"># Category &amp; Tag</span></span><br><span class="line">default_category: uncategorized</span><br><span class="line">category_map:</span><br><span class="line">tag_map:</span><br><span class="line"></span><br><span class="line"><span class="comment"># Date / Time format</span></span><br><span class="line"><span class="comment">## Hexo uses Moment.js to parse and display date</span></span><br><span class="line"><span class="comment">## You can customize the date format as defined in</span></span><br><span class="line"><span class="comment">## http://momentjs.com/docs/#/displaying/format/</span></span><br><span class="line">date_format: YYYY-MM-DD</span><br><span class="line">time_format: HH:mm:ss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pagination</span></span><br><span class="line"><span class="comment">## Set per_page to 0 to disable pagination</span></span><br><span class="line">per_page: 10</span><br><span class="line">pagination_dir: page</span><br><span class="line"></span><br><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="comment">## theme: hexo-theme-next</span></span><br><span class="line"><span class="comment">## theme: yinwang</span></span><br><span class="line"><span class="comment">## theme: minos</span></span><br><span class="line">theme: blairos</span><br><span class="line"><span class="comment">## theme: jacman</span></span><br><span class="line"><span class="comment">## theme: landscape-plus</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy:<span class="built_in">type</span>: git</span><br><span class="line">repository: https://github.com/52binge/52binge.github.io.git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[多层 LSTM 通俗版]]></title>
      <url>http://sggo.me/2017/10/07/tensorflow/tf-simple-lstms/</url>
      <content type="html"><![CDATA[<p>官方给出的例子，用多层 LSTM 来实现 PTBModel 语言模型，比如： <a href="https://blog.csdn.net/u014595019/article/details/52759104" target="_blank" rel="external">tensorflow笔记：多层LSTM代码分析</a> 感觉这些例子还是太复杂了，所以这里写了个比较简单的版本</p>
<a id="more"></a>
<p>声明： 本文部分内容转自 <a href="https://blog.csdn.net/Jerr__y/article/details/61195257" target="_blank" rel="external">永永夜 Tensorflow学习之路</a></p>
<p>自己做了一个示意图，希望帮助初学者更好地理解 多层RNN. </p>
<p><img src="/images/tensorflow/tf-4.4_1-simple-lstms.png" width="800"></p>
<p>通过本例，你可以了解到单层 LSTM 的实现，多层 LSTM 的实现。输入输出数据的格式。 RNN 的 dropout layer 的实现。</p>
<h2 id="MNIST-背景"><a href="#MNIST-背景" class="headerlink" title="MNIST 背景"></a>MNIST 背景</h2><p><strong>准备数据</strong></p>
<p>MNIST 是在机器学习领域中的一个经典问题。该问题解决的是把 28x28像素 的灰度手写数字图片识别为相应的数字，其中数字的范围从 0到9.</p>
<p><img src="/images/tensorflow/tf-4.4_5-mnist_digits.png" width="400"></p>
<blockquote>
<p>MNIST 数据集 包含了 60000 张图片来作为训练数据，10000 张图片作为测试数据。每张图片都代表了 0~9 中的一个数字。图片大小都为 28*28，处理后的每张图片是一个长度为 784 的一维数组，这个数组中的元素对应图片像素矩阵提供给神经网络的输入层，像素矩阵中元素的取值范围 [0, 1]， 它代表了颜色的深浅。其中 0 表示白色背景(background)，1 表示黑色前景(foreground)。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 GPU 按需增长</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先导入数据，看一下数据的形式</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">print(mnist.train.images.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Extracting MNIST_data/train-images-idx3-ubyte.gz</span><br><span class="line">Extracting MNIST_data/train-labels-idx1-ubyte.gz</span><br><span class="line">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</span><br><span class="line">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</span><br><span class="line">(55000, 784) # 训练集图片 - 55000 张 * 784维一维数组</span><br></pre></td></tr></table></figure>
<p>执行 input_data.read_data_sets 后自动创建一个目录 MNIST_data，并开始下载数据</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(anaconda3)</span><br><span class="line"><span class="comment"># ~/ghome/github/TensorFlowExamples [master ✗ (98591d9)] [16:15:03]</span></span><br><span class="line">➜ ll</span><br><span class="line">total 24</span><br><span class="line">drwxr-xr-x   6 blair  staff   192B Oct  9 16:14 MNIST_data</span><br><span class="line">-rw-r--r--   1 blair  staff   2.3K Oct  9 16:13 simple-lstms.ipynb</span><br><span class="line">(anaconda3)</span><br><span class="line"><span class="comment"># ~/ghome/github/TensorFlowExamples [master ✗ (98591d9)] [16:15:06]</span></span><br><span class="line">➜ ll MNIST_data</span><br><span class="line">total 22672</span><br><span class="line">-rw-r--r--  1 blair  staff   1.6M Oct  9 16:14 t10k-images-idx3-ubyte.gz</span><br><span class="line">-rw-r--r--  1 blair  staff   4.4K Oct  9 16:14 t10k-labels-idx1-ubyte.gz</span><br><span class="line">-rw-r--r--  1 blair  staff   9.5M Oct  9 16:14 train-images-idx3-ubyte.gz</span><br><span class="line">-rw-r--r--  1 blair  staff    28K Oct  9 16:14 train-labels-idx1-ubyte.gz</span><br><span class="line">(anaconda3)</span><br><span class="line"><span class="comment"># ~/ghome/github/TensorFlowExamples [master ✗ (98591d9)] [16:15:12]</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:center">文件</th>
<th style="text-align:center">内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">train-images-idx3-ubyte.gz</td>
<td style="text-align:center">训练集图片 - 55000 张 训练图片, 5000 张 验证图片</td>
</tr>
<tr>
<td style="text-align:center">train-labels-idx1-ubyte.gz</td>
<td style="text-align:center">训练集图片对应的数字标签</td>
</tr>
<tr>
<td style="text-align:center">t10k-images-idx3-ubyte.gz</td>
<td style="text-align:center">测试集图片 - 10000 张 图片</td>
</tr>
<tr>
<td style="text-align:center">t10k-labels-idx1-ubyte.gz</td>
<td style="text-align:center">测试集图片对应的数字标签</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'training data shape '</span>, mnist.train.images.shape)</span><br><span class="line">print(<span class="string">'training label shape '</span>, mnist.train.labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training data shape  (55000, 784)</span></span><br><span class="line"><span class="comment"># training label shape  (55000, 10)</span></span><br></pre></td></tr></table></figure>
<h2 id="1-首先设置好模型用到的各个超参数"><a href="#1-首先设置好模型用到的各个超参数" class="headerlink" title="1. 首先设置好模型用到的各个超参数"></a>1. 首先设置好模型用到的各个超参数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">1e-3</span> <span class="comment"># 0.001</span></span><br><span class="line"><span class="comment"># 在训练和测试的时候，我们想用不同的 batch_size.所以采用占位符的方式</span></span><br><span class="line">batch_size = tf.placeholder(tf.int32, [])  <span class="comment"># 注意类型必须为 tf.int32</span></span><br><span class="line"></span><br><span class="line">keep_prob = tf.placeholder(tf.float32, [])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个时刻的输入特征是28维的，就是每个时刻输入一行，一行有 28 个像素</span></span><br><span class="line">input_size = <span class="number">28</span></span><br><span class="line"><span class="comment"># 时序持续长度为28，即每做一次预测，需要先输入28行</span></span><br><span class="line">timestep_size = <span class="number">28</span></span><br><span class="line"><span class="comment"># 每个隐含层的节点数</span></span><br><span class="line">hidden_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># LSTM layer 的层数</span></span><br><span class="line">layer_num = <span class="number">2</span></span><br><span class="line"><span class="comment"># 最后输出分类类别数量，如果是回归预测的话应该是 1</span></span><br><span class="line">class_num = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">_X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, class_num])</span><br></pre></td></tr></table></figure>
<h2 id="2-开始搭建-LSTM-模型，其实普通-RNNs-模型也一样"><a href="#2-开始搭建-LSTM-模型，其实普通-RNNs-模型也一样" class="headerlink" title="2. 开始搭建 LSTM 模型，其实普通 RNNs 模型也一样"></a>2. 开始搭建 LSTM 模型，其实普通 RNNs 模型也一样</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 把784个点的字符信息还原成 28 * 28 的图片</span></span><br><span class="line"><span class="comment"># 下面几个步骤是实现 RNN / LSTM 的关键</span></span><br><span class="line"><span class="comment">###################################################################</span></span><br><span class="line"><span class="comment"># tf.reshape(tensor, shape, name=None)  函数的作用是将 tensor 变换为参数shape的形式</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># **步骤1：RNN 的输入shape = (batch_size, timestep_size, input_size) </span></span><br><span class="line">X = tf.reshape(_X, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># **步骤2：定义一层 LSTM_cell，只需要说明 hidden_size, 它会自动匹配输入的 X 的维度</span></span><br><span class="line">lstm_cell = rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># **步骤3：添加 dropout layer, 一般只设置 output_keep_prob</span></span><br><span class="line">lstm_cell = rnn.DropoutWrapper(cell=lstm_cell, input_keep_prob=<span class="number">1.0</span>, output_keep_prob=keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># **步骤4：调用 MultiRNNCell 来实现多层 LSTM</span></span><br><span class="line">mlstm_cell = rnn.MultiRNNCell([lstm_cell] * layer_num, state_is_tuple=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># **步骤5：用全零来初始化state</span></span><br><span class="line">init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># **步骤6：方法一，调用 dynamic_rnn() 来让我们构建好的网络运行起来</span></span><br><span class="line"><span class="comment"># ** 当 time_major==False 时， outputs.shape = [batch_size, timestep_size, hidden_size] </span></span><br><span class="line"><span class="comment"># ** 所以，可以取 h_state = outputs[:, -1, :] 作为最后输出</span></span><br><span class="line"><span class="comment"># ** state.shape = [layer_num, 2, batch_size, hidden_size], </span></span><br><span class="line"><span class="comment"># ** 或者，可以取 h_state = state[-1][1] 作为最后输出</span></span><br><span class="line"><span class="comment"># ** 最后输出维度是 [batch_size, hidden_size]</span></span><br><span class="line"><span class="comment"># outputs, state = tf.nn.dynamic_rnn(mlstm_cell, inputs=X, initial_state=init_state, time_major=False)</span></span><br><span class="line"><span class="comment"># h_state = outputs[:, -1, :]  # 或者 h_state = state[-1][1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># *************** 为了更好的理解 LSTM 工作原理，我们把上面 步骤6 中的函数自己来实现 ***************</span></span><br><span class="line"><span class="comment"># 通过查看文档你会发现， RNNCell 都提供了一个 __call__()函数（见最后附），我们可以用它来展开实现LSTM按时间步迭代。</span></span><br><span class="line"><span class="comment"># **步骤6：方法二，按时间步展开计算 (暂时没有运行通过)</span></span><br><span class="line">outputs = list()</span><br><span class="line">state = init_state</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'RNN'</span>):</span><br><span class="line">    <span class="keyword">for</span> timestep <span class="keyword">in</span> range(timestep_size):</span><br><span class="line">        <span class="keyword">if</span> timestep &gt; <span class="number">0</span>:</span><br><span class="line">            tf.get_variable_scope().reuse_variables()</span><br><span class="line">        <span class="comment"># 这里的state保存了每一层 LSTM 的状态</span></span><br><span class="line">        (cell_output, state) = mlstm_cell(X[:, timestep, :], state)</span><br><span class="line">        outputs.append(cell_output)</span><br><span class="line">h_state = outputs[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#  X[:, timestep, :] 就是取第timestep个时刻的特征 x_t 输入 mlstm_cell 中计算，因为每次用 batch_size 个样本来训练，所以相当于（并行）输入 batch_size 个 x_t 到 mlstm_cell 中计算。</span></span><br></pre></td></tr></table></figure>
<h2 id="3-设置-loss-function-和-优化器，展开训练并完成测试"><a href="#3-设置-loss-function-和-优化器，展开训练并完成测试" class="headerlink" title="3. 设置 loss function 和 优化器，展开训练并完成测试"></a>3. 设置 loss function 和 优化器，展开训练并完成测试</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上面 LSTM 部分的输出会是一个 [hidden_size] 的 tensor，我们要分类的话，还需要接一个 softmax 层</span></span><br><span class="line"><span class="comment"># 首先定义 softmax 的连接权重矩阵和偏置</span></span><br><span class="line"><span class="comment"># out_W = tf.placeholder(tf.float32, [hidden_size, class_num], name='out_Weights')</span></span><br><span class="line"><span class="comment"># out_bias = tf.placeholder(tf.float32, [class_num], name='out_bias')</span></span><br><span class="line"><span class="comment"># 开始训练和测试</span></span><br><span class="line">W = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=<span class="number">0.1</span>), dtype=tf.float32)</span><br><span class="line">bias = tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[class_num]), dtype=tf.float32)</span><br><span class="line">y_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失和评估函数</span></span><br><span class="line">cross_entropy = -tf.reduce_mean(y * tf.log(y_pre))</span><br><span class="line">train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_pre,<span class="number">1</span>), tf.argmax(y,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2000</span>):</span><br><span class="line">    _batch_size = <span class="number">128</span></span><br><span class="line">    batch = mnist.train.next_batch(_batch_size)</span><br><span class="line">    sess.run(train_op, feed_dict=&#123;_X: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>, batch_size: _batch_size&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">        train_accuracy = sess.run(accuracy, feed_dict=&#123;</span><br><span class="line">            _X:batch[<span class="number">0</span>], y: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>, batch_size: _batch_size&#125;)</span><br><span class="line">        <span class="comment"># 已经迭代完成的 epoch 数: mnist.train.epochs_completed</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Iter%d, step %d, training accuracy %g"</span> % ( mnist.train.epochs_completed, (i+<span class="number">1</span>), train_accuracy)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 计算测试数据的准确率</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"test accuracy %g"</span>% sess.run(accuracy, feed_dict=&#123;</span><br><span class="line">    _X: mnist.test.images, y: mnist.test.labels, keep_prob: <span class="number">1.0</span>, batch_size:mnist.test.images.shape[<span class="number">0</span>]&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Iter0, step 200, training accuracy 0.851562</span><br><span class="line">Iter0, step 400, training accuracy 0.960938</span><br><span class="line">Iter1, step 600, training accuracy 0.984375</span><br><span class="line">Iter1, step 800, training accuracy 0.960938</span><br><span class="line">Iter2, step 1000, training accuracy 0.984375</span><br><span class="line">Iter2, step 1200, training accuracy 0.9375</span><br><span class="line">Iter3, step 1400, training accuracy 0.96875</span><br><span class="line">Iter3, step 1600, training accuracy 0.984375</span><br><span class="line">Iter4, step 1800, training accuracy 0.992188</span><br><span class="line">Iter4, step 2000, training accuracy 0.984375</span><br><span class="line">test accuracy 0.9858</span><br></pre></td></tr></table></figure>
<p>我们一共只迭代不到 5 个 epoch，在测试集上就已经达到了 0.98 的准确率，可以看出来 LSTM 在做这个字符分类的任务上还是比较有效的，而且我们最后一次性对 10000 张测试图片进行预测，才占了 725 MiB 的显存。而我们在之前的两层 CNNs 网络中，预测 10000 张图片一共用了 8721 MiB 的显存，差了整整 12 倍呀！！ 这主要是因为 RNN/LSTM 网络中，每个时间步所用的权值矩阵都是共享的，可以通过前面介绍的 LSTM 的网络结构分析一下，整个网络的参数非常少。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/jerr__y/article/category/6747409" target="_blank" rel="external">大学之道，在明明德 永永夜 Tensorflow学习之路</a></li>
<li><a href="https://blog.csdn.net/u014595019/article/details/52759104" target="_blank" rel="external">tensorflow笔记：多层LSTM代码分析 </a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_download.html" target="_blank" rel="external">极客学院 MNIST 数据下载</a></li>
<li><a href="https://www.zhihu.com/question/41949741" target="_blank" rel="external">隔壁小王 LSTM 神经网络输入输出究竟是怎样的？</a></li>
<li><a href="https://colab.research.google.com" target="_blank" rel="external">colab.research.google</a></li>
<li><a href="https://zh.gluon.ai/" target="_blank" rel="external">zh.gluon.ai 动手学深度学习</a></li>
<li><a href="http://discuss.gluon.ai/" target="_blank" rel="external">discuss.gluon.ai 论坛</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">roll_jj： 博主你好， outputs, state = tf.nn.dynamic_rnn(mlstm_cell, inputs=X, initial_state=init_state, time_major=False) h_state = outputs[:, -1, :] 这两句话里，outputs的三个维度是什么意思，为什么把中间那个维度去掉就是我们要的输出结果了？(1年前#6楼)收起回复举报回复</span><br><span class="line">Jerr__y</span><br><span class="line">CQU_HYX回复 roll_jj： 是的(1年前)</span><br><span class="line">roll_jj</span><br><span class="line">roll_jj回复 CQU_HYX： 谢谢博主解答。我看官方的那个PTB例子里，没有取[-1]的这个操作，而是用了output = tf.reshape(tf.concat(1, outputs), [-1, size])操作，这是因为预测目标的不同么？(1年前)</span><br><span class="line">Jerr__y</span><br><span class="line">CQU_HYX回复 roll_jj： 原文注释上面有说了，outputs.shape = [batch_size, timestep_size, hidden_size]。 因为是分类问题，所有只需要在看完最后一行像素后才输出分类结果。-1 表示取最后一个 timestep 的结果， 而不是说把中间维度去掉</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[(转) 理解 LSTM 网络 （Understanding LSTM Networks by colah）]]></title>
      <url>http://sggo.me/2017/10/06/tensorflow/tf-Understanding-LSTMs/</url>
      <content type="html"><![CDATA[<p>原文链接1： <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a><br>原文链接2： <a href="https://blog.csdn.net/jerr__y/article/details/58598296" target="_blank" rel="external">理解 LSTM 网络</a> @翻译：huangyongye</p>
<a id="more"></a> 
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/jerr__y/article/category/6747409" target="_blank" rel="external">大学之道，在明明德 永永夜 Tensorflow学习之路</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a></li>
<li><a href="https://blog.csdn.net/jerr__y/article/details/58598296" target="_blank" rel="external">大学之道，在明明德 永永夜 理解 LSTM 网络 （Understanding LSTM Networks by colah）</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 命名方法 name_scope / variable_scope]]></title>
      <url>http://sggo.me/2017/10/05/tensorflow/tf-4.3-name-variable_scope/</url>
      <content type="html"><![CDATA[<p>本例主要介绍 name_scope 和 variable_scope 的正确使用方式，学习并理解本例之后，你就能够真正读懂 TensorFlow 的很多代码并能够清晰地理解模型结构了.</p>
<a id="more"></a> 
<p>scope 能让你命名变量的时候轻松很多. 同时也会在 reusing variable 代码中常常见到. 所以今天我们会来讨论下 tensorflow 当中的两种定义 scope 的方式. 最后并附加一个 RNN 运用 reuse variable 的例子.</p>
<p>在 TensorFlow 中，经常看到 name_scope 和 variable_scope 两个东东，这到底是什么鬼，到底系做咩噶!!! 在做 LSTM 的时候遇到了下面的错误： <code>ValueError: Variable rnn/basic_lstm_cell/weights already exists, disallowed.</code></p>
<h2 id="1-先说结论"><a href="#1-先说结论" class="headerlink" title="1. 先说结论"></a>1. 先说结论</h2><p>要理解  name_scope 和 variable_scope， 首先必须明确二者的使用目的。我们都知道，和普通模型相比，神经网络的节点非常多，节点节点之间的连接（权值矩阵）也非常多。所以我们费尽心思，准备搭建一个网络，然后有了图1的网络，WTF! 因为变量太多，我们构造完网络之后，一看，什么鬼，这个变量到底是哪层的？？</p>
<p><img src="/images/tensorflow/tf-4.3_1.jpg" width="750"></p>
<p>为了解决这个问题，我们引入了 <strong>name_scope</strong> 和 <strong>variable_scope</strong>， 二者又分别承担着不同的责任：</p>
<ul>
<li>name_scope: 为了更好地管理变量的命名空间而提出的。比如在 tensorboard 中，因为引入了 name_scope， 我们的 Graph 看起来才井然有序。</li>
<li>variable_scope: 大大大部分情况下，跟 <code>tf.get_variable()</code> 配合使用，实现变量共享的功能。</li>
</ul>
<p>下面通过两组实验来探索 TensorFlow 的命名机制。</p>
<h2 id="2-name-scope-variable-scope-实验-🌰"><a href="#2-name-scope-variable-scope-实验-🌰" class="headerlink" title="2. name_scope/variable_scope 实验 🌰"></a>2. name_scope/variable_scope 实验 🌰</h2><p><strong>实验 1 三种方式创建变量：</strong></p>
<p>三种方式创建变量： <strong>tf.placeholder</strong>, <strong>tf.Variable</strong>, <strong>tf.get_variable</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 设置GPU按需增长</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">sess = tf.Session(config=config)</span><br></pre></td></tr></table></figure>
<h3 id="2-1-placeholder"><a href="#2-1-placeholder" class="headerlink" title="2.1 placeholder"></a>2.1 placeholder</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.placeholder </span></span><br><span class="line">v1 = tf.placeholder(tf.float32, shape=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(v1.name)</span><br><span class="line">v1 = tf.placeholder(tf.float32, shape=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], name=<span class="string">'ph'</span>)</span><br><span class="line">print(v1.name)</span><br><span class="line">v1 = tf.placeholder(tf.float32, shape=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], name=<span class="string">'ph'</span>)</span><br><span class="line">print(v1.name)</span><br><span class="line">print(type(v1))</span><br><span class="line">print(v1)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Placeholder:0</span><br><span class="line">ph:0</span><br><span class="line">ph_1:0</span><br><span class="line">&lt;class &apos;tensorflow.python.framework.ops.Tensor&apos;&gt;</span><br><span class="line">Tensor(&quot;ph_1:0&quot;, shape=(2, 3, 4), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-tf-Variable"><a href="#2-2-tf-Variable" class="headerlink" title="2.2 tf.Variable()"></a>2.2 tf.Variable()</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2. tf.Variable()</span></span><br><span class="line">v2 = tf.Variable([<span class="number">1</span>,<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">print(v2.name)</span><br><span class="line">v2 = tf.Variable([<span class="number">1</span>,<span class="number">2</span>], dtype=tf.float32, name=<span class="string">'V'</span>)</span><br><span class="line">print(v2.name)</span><br><span class="line">v2 = tf.Variable([<span class="number">1</span>,<span class="number">2</span>], dtype=tf.float32, name=<span class="string">'V'</span>)</span><br><span class="line">print(v2.name)</span><br><span class="line">print(type(v2))</span><br><span class="line">print(v2)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Variable:0</span><br><span class="line">V:0</span><br><span class="line">V_1:0</span><br><span class="line">&lt;class &apos;tensorflow.python.ops.variables.Variable&apos;&gt;</span><br><span class="line">Tensor(&quot;V_1/read:0&quot;, shape=(2,), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-tf-get-variable"><a href="#2-3-tf-get-variable" class="headerlink" title="2.3 tf.get_variable()"></a>2.3 tf.get_variable()</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.tf.get_variable() 创建变量的时候必须要提供 name</span></span><br><span class="line">v3 = tf.get_variable(name=<span class="string">'gv'</span>, shape=[])  </span><br><span class="line">print(v3.name)</span><br><span class="line">v4 = tf.get_variable(name=<span class="string">'gv'</span>, shape=[<span class="number">2</span>])</span><br><span class="line">print(v4.name)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gv:0</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">ValueError                                Traceback (most recent call last)</span><br><span class="line"></span><br><span class="line">&lt;ipython-input-7-29efaac2d76c&gt; in &lt;module&gt;()</span><br><span class="line">      2 v3 = tf.get_variable(name=&apos;gv&apos;, shape=[])</span><br><span class="line">      3 print(v3.name)</span><br><span class="line">----&gt; 4 v4 = tf.get_variable(name=&apos;gv&apos;, shape=[2])</span><br><span class="line">      5 print(v4.name)</span><br><span class="line">此处还有一堆错误信息。。。</span><br><span class="line">ValueError: Variable gv already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(type(v3))</span><br><span class="line">print(v3)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;class &apos;tensorflow.python.ops.variables.Variable&apos;&gt;</span><br><span class="line">Tensor(&quot;gv/read:0&quot;, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>还记得有这么个函数吗？ <strong>tf.trainable_variables(</strong>), 它能够将我们定义的所有的 trainable=True 的所有变量以一个 list 的形式返回。 very good, 现在要派上用场了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vs = tf.trainable_variables()</span><br><span class="line">print(len(vs))</span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> vs:</span><br><span class="line">    print(v)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4</span><br><span class="line">Tensor(&quot;Variable/read:0&quot;, shape=(2,), dtype=float32)</span><br><span class="line">Tensor(&quot;V/read:0&quot;, shape=(2,), dtype=float32)</span><br><span class="line">Tensor(&quot;V_1/read:0&quot;, shape=(2,), dtype=float32)</span><br><span class="line">Tensor(&quot;gv/read:0&quot;, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>实验1 结论:</strong></p>
<p>从上面的实验结果来看，这三种方式所定义的变量具有相同的类型。</p>
<p>只有 <code>tf.get_variable()</code> 创建的变量之间会发生 <strong>命名冲突</strong>。在实际使用中，三种创建变量方式的用途分工非常明确。</p>
<ul>
<li>tf.placeholder() 占位符。 trainable==False</li>
<li>tf.Variable() 一般变量用这种方式定义。 可以选择 trainable 类型</li>
<li>tf.get_variable() 一般都是和 tf.variable_scope() 配合使用，从而实现变量共享的功能。  可以选择 trainable 类型</li>
</ul>
</blockquote>
<h2 id="3-探索-name-scope-和-variable-scope"><a href="#3-探索-name-scope-和-variable-scope" class="headerlink" title="3. 探索 name_scope 和 variable_scope"></a>3. 探索 name_scope 和 variable_scope</h2><p>实验目的： 熟悉两种命名空间的应用情景</p>
<h3 id="3-1-tf-name-scope"><a href="#3-1-tf-name-scope" class="headerlink" title="3.1 tf.name_scope()"></a>3.1 tf.name_scope()</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'nsc1'</span>):</span><br><span class="line">    v1 = tf.Variable([<span class="number">1</span>], name=<span class="string">'v1'</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'vsc1'</span>):</span><br><span class="line">        v2 = tf.Variable([<span class="number">1</span>], name=<span class="string">'v2'</span>)</span><br><span class="line">        v3 = tf.get_variable(name=<span class="string">'v3'</span>, shape=[])</span><br><span class="line">print(<span class="string">'v1.name: '</span>, v1.name)</span><br><span class="line">print(<span class="string">'v2.name: '</span>, v2.name)</span><br><span class="line">print(<span class="string">'v3.name: '</span>, v3.name)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">v1.name:  nsc1/v1:0</span><br><span class="line">v2.name:  nsc1/vsc1/v2:0</span><br><span class="line">v3.name:  vsc1/v3:0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'nsc1'</span>):</span><br><span class="line">    v4 = tf.Variable([<span class="number">1</span>], name=<span class="string">'v4'</span>)</span><br><span class="line">print(<span class="string">'v4.name: '</span>, v4.name) <span class="comment"># v4.name:  nsc1_1/v4:0</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>tf.name_scope() 并不会对 tf.get_variable() 创建的变量有任何影响。 </li>
<li><p>tf.name_scope() 主要是用来管理命名空间的，这样子让我们的整个模型更加有条理。</p>
</li>
<li><p>tf.variable_scope() 的作用是为了实现<strong>变量共享</strong>，它和 tf.get_variable() 来完成变量共享的功能。</p>
</li>
</ul>
</blockquote>
<h3 id="3-2-tf-variable-scope"><a href="#3-2-tf-variable-scope" class="headerlink" title="3.2 tf.variable_scope()"></a>3.2 tf.variable_scope()</h3><p>如果想要达到重复利用变量的效果, 我们就要使用 <code>tf.variable_scope()</code>, 并搭配 <code>tf.get_variable()</code> 这种方式产生和提取变量. 不像 <code>tf.Variable()</code> 每次都会产生新的变量, <code>tf.get_variable()</code> 如果遇到了同样名字的变量时, 它会单纯的提取这个同样名字的变量(避免产生新变量). 而在重复使用的时候, 一定要在代码中强调 <code>scope.reuse_variables()</code>, 否则系统将会报错, 以为你只是单纯的不小心重复使用到了一个变量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"a_variable_scope"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    initializer = tf.constant_initializer(value=<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    var3 = tf.get_variable(name=<span class="string">'var3'</span>, shape=[<span class="number">1</span>], dtype=tf.float32, initializer=initializer)</span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">    var3_reuse = tf.get_variable(name=<span class="string">'var3'</span>,)</span><br><span class="line">    </span><br><span class="line">    var4 = tf.Variable(name=<span class="string">'var4'</span>, initial_value=[<span class="number">4</span>], dtype=tf.float32)</span><br><span class="line">    var4_reuse = tf.Variable(name=<span class="string">'var4'</span>, initial_value=[<span class="number">4</span>], dtype=tf.float32)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    </span><br><span class="line">    print(var3.name)            <span class="comment"># a_variable_scope/var3:0</span></span><br><span class="line">    print(sess.run(var3))       <span class="comment"># [ 3.]</span></span><br><span class="line">    print(var3_reuse.name)      <span class="comment"># a_variable_scope/var3:0</span></span><br><span class="line">    print(sess.run(var3_reuse)) <span class="comment"># [ 3.]</span></span><br><span class="line">    </span><br><span class="line">    print(var4.name)            <span class="comment"># a_variable_scope/var4:0</span></span><br><span class="line">    print(sess.run(var4))       <span class="comment"># [ 4.]</span></span><br><span class="line">    print(var4_reuse.name)      <span class="comment"># a_variable_scope/var4_1:0</span></span><br><span class="line">    print(sess.run(var4_reuse)) <span class="comment"># [ 4.]</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>首先我们要确立一种 Graph 的思想。在 TensorFlow 中，我们定义一个变量，相当于往 Graph 中添加了一个节点。和普通的 python 函数不一样，在一般的函数中，我们对输入进行处理，然后返回一个结果，而函数里边定义的一些局部变量我们就不管了。但是在 TensorFlow 中，我们在函数里边创建了一个变量，就是往 Graph 中添加了一个节点。出了这个函数后，这个节点还是存在于 Graph 中的。</p>
</blockquote>
<h2 id="4-RNN-应用例子"><a href="#4-RNN-应用例子" class="headerlink" title="4. RNN 应用例子"></a>4. RNN 应用例子</h2><p>整个 RNN 的结构已经在这里定义好了. 在 training RNN 和 test RNN 的时候, RNN 的 <code>time_steps</code> 会有不同的取值, 这将会影响到整个 RNN 的结构, 所以导致在 test 的时候, 不能单纯地使用 training 时建立的那个 RNN. 但是 training RNN 和 test RNN 又必须是有同样的 weights biases 的参数. 所以, 这时, 就是使用 reuse variable 的好时机.</p>
<p>首先定义 training 和 test 的不同参数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainConfig</span>:</span></span><br><span class="line">    batch_size = <span class="number">20</span></span><br><span class="line">    time_steps = <span class="number">20</span></span><br><span class="line">    input_size = <span class="number">10</span></span><br><span class="line">    output_size = <span class="number">2</span></span><br><span class="line">    cell_size = <span class="number">11</span></span><br><span class="line">    learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestConfig</span><span class="params">(TrainConfig)</span>:</span></span><br><span class="line">    time_steps = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">train_config = TrainConfig()</span><br><span class="line">test_config = TestConfig()</span><br></pre></td></tr></table></figure>
<p>然后让 <code>train_rnn</code> 和 <code>test_rnn</code> 在同一个 <code>tf.variable_scope(&#39;rnn&#39;)</code> 之下. 并且定义 <strong>scope.reuse_variables()</strong>, 使我们能把 <code>train_rnn</code> 的所有 weights, biases 参数全部绑定到 <code>test_rnn</code> 中. </p>
<p>这样, 不管两者的 <code>time_steps</code> 有多不同, 结构有多不同, <code>train_rnn</code> W, b 参数更新成什么样, <code>test_rnn</code> 的参数也更新成什么样.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    train_rnn = RNN(train_config)</span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">    test_rnn = RNN(test_config)</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/jerr__y/article/category/6747409" target="_blank" rel="external">大学之道，在明明德 永永夜 Tensorflow学习之路</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-12-scope/" target="_blank" rel="external">morvanzhou, scope 命名方法</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[My Vim Config File]]></title>
      <url>http://sggo.me/2017/10/04/ops/ops-vimrc/</url>
      <content type="html"><![CDATA[<p>my vim custom config .vimrc file</p>
<a id="more"></a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> nocompatible <span class="string">" 关闭 vi 兼容模式  </span><br><span class="line">syntax on        "</span> 自动语法高亮  </span><br><span class="line"><span class="built_in">set</span> number     <span class="string">" 显示行号         "</span> <span class="built_in">set</span> cursorline    突出显示当前行  </span><br><span class="line"><span class="built_in">set</span> ruler        <span class="string">" 打开状态栏标尺    (不错)  </span><br><span class="line">set shiftwidth=4 "</span> 设定 &lt;&lt; 和 &gt;&gt; 命令移动时的宽度为 4  </span><br><span class="line"><span class="built_in">set</span> smartindent  </span><br><span class="line"><span class="built_in">set</span> tabstop=4         <span class="string">" tabstop=4 设定 tab 长度为4  </span><br><span class="line">set softtabstop=4   </span><br><span class="line">set expandtab  </span><br><span class="line">set encoding=utf-8 fileencodings=ucs-bom,utf-8,cp936</span><br><span class="line">set nobackup     "</span> 覆盖文件时不备份  </span><br><span class="line"><span class="built_in">set</span> ignorecase smartcase    <span class="string">" 搜索时忽略大小写，但在有一个或以上大写字母时仍大小写敏感  </span><br><span class="line">"</span><span class="built_in">set</span> nowrapscan              <span class="string">" 禁止在搜索到文件两端时重新搜索  </span><br><span class="line">set nowrapscan              "</span> 禁止在搜索到文件两端时重新搜索  </span><br><span class="line"><span class="built_in">set</span> incsearch               <span class="string">" 输入搜索内容时就显示搜索结果  </span><br><span class="line">set hlsearch                "</span> 搜索时高亮显示被找到的文本  </span><br><span class="line"><span class="built_in">set</span> noerrorbells            <span class="string">" 关闭错误信息响铃  </span><br><span class="line">set novisualbell            "</span> 关闭使用可视响铃代替呼叫  </span><br><span class="line"><span class="built_in">set</span> t_vb=                   <span class="string">" 置空错误铃声的终端代码  </span><br><span class="line">set showmatch               "</span> 插入括号时，短暂地跳转到匹配的对应括号  </span><br><span class="line"><span class="string">" set matchtime=2             "</span> 短暂跳转到匹配括号的时间  </span><br><span class="line"><span class="built_in">set</span> magic                   <span class="string">" 设置魔术  </span><br><span class="line">set hidden                  "</span> 允许在有未保存的修改时切换缓冲区，此时的修改由 vim 负责保存  </span><br><span class="line"><span class="string">"set guioptions-=T           "</span> 隐藏工具栏  </span><br><span class="line"><span class="string">"set guioptions-=m           "</span> 隐藏菜单栏  </span><br><span class="line"><span class="built_in">set</span> smartindent             <span class="string">" 开启新行时使用智能自动缩进  </span><br><span class="line">set backspace=indent,eol,start  </span><br><span class="line">                            "</span> 不设定在插入状态无法用退格键和 Delete 键删除回车符  </span><br><span class="line"><span class="built_in">set</span> cmdheight=1             <span class="string">" 设定命令行的行数为 1  </span><br><span class="line">set laststatus=2            "</span> 显示状态栏 (默认值为 1, 无法显示状态栏)  </span><br><span class="line"><span class="built_in">set</span> statusline=\ %&lt;%F[%1*%M%*%n%R%H]%=\ %y\ %0(%&#123;&amp;fileformat&#125;\ %&#123;&amp;encoding&#125;\ %c:%l/%L%)\  </span><br><span class="line"><span class="string">" 设置在状态行显示的信息  </span><br><span class="line">"</span><span class="built_in">set</span> foldenable              <span class="string">" 开始折叠  </span><br><span class="line">"</span><span class="built_in">set</span> foldmethod=syntax       <span class="string">" 设置语法折叠  </span><br><span class="line">"</span><span class="built_in">set</span> foldcolumn=0            <span class="string">" 设置折叠区域的宽度  </span><br><span class="line">"</span>setlocal foldlevel=1        <span class="string">" 设置折叠层数为  </span><br><span class="line">"</span> <span class="built_in">set</span> foldclose=all           <span class="string">" 设置为自动关闭折叠                             </span><br><span class="line">"</span> nnoremap &lt;space&gt; @=((foldclosed(line(<span class="string">'.'</span>)) &lt; 0) ? <span class="string">'zc'</span> : <span class="string">'zo'</span>)&lt;CR&gt;  </span><br><span class="line"><span class="string">" 用空格键来开关折叠  </span><br><span class="line"></span><br><span class="line">set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936</span><br><span class="line">set termencoding=utf-8</span><br><span class="line">set encoding=utf-8  </span><br><span class="line">  </span><br><span class="line">"</span> <span class="built_in">return</span> OS <span class="built_in">type</span>, eg: windows, or linux, mac, et.st..  </span><br><span class="line"><span class="keyword">function</span>! MySys()  </span><br><span class="line">    <span class="keyword">if</span> has(<span class="string">"win16"</span>) || has(<span class="string">"win32"</span>) || has(<span class="string">"win64"</span>) || has(<span class="string">"win95"</span>)  </span><br><span class="line">       <span class="built_in">return</span> <span class="string">"windows"</span>  </span><br><span class="line">    elseif has(<span class="string">"unix"</span>)  </span><br><span class="line">       <span class="built_in">return</span> <span class="string">"linux"</span>  </span><br><span class="line">    endif  </span><br><span class="line">endfunction  </span><br><span class="line">  </span><br><span class="line"><span class="string">" 用户目录变量<span class="variable">$VIMFILES</span>  </span><br><span class="line">if MySys() == "</span>windows<span class="string">"  </span><br><span class="line">    let <span class="variable">$VIMFILES</span> = <span class="variable">$VIM</span>.'/vimfiles'  </span><br><span class="line">elseif MySys() == "</span>linux<span class="string">"  </span><br><span class="line">    let <span class="variable">$VIMFILES</span> = <span class="variable">$HOME</span>.'/.vim'  </span><br><span class="line">endif  </span><br><span class="line">  </span><br><span class="line">"</span> 设定doc文档目录  </span><br><span class="line"><span class="built_in">let</span> helptags=<span class="variable">$VIMFILES</span>.<span class="string">'/doc'</span>  </span><br><span class="line">  </span><br><span class="line"><span class="string">" 设置字体 以及中文支持  </span><br><span class="line">if has("</span>win32<span class="string">")  </span><br><span class="line">    set guifont=Inconsolata:h12:cANSI  </span><br><span class="line">endif  </span><br><span class="line">  </span><br><span class="line">"</span> 配置多语言环境  </span><br><span class="line"><span class="keyword">if</span> has(<span class="string">"multi_byte"</span>)  </span><br><span class="line">   <span class="string">" UTF-8 编码  </span><br><span class="line">    set encoding=utf-8  </span><br><span class="line">    set termencoding=utf-8  </span><br><span class="line">    set formatoptions+=mM  </span><br><span class="line">    set fencs=utf-8,gbk  </span><br><span class="line">  </span><br><span class="line">    if v:lang =~? '^\(zh\)\|\(ja\)\|\(ko\)'  </span><br><span class="line">          set ambiwidth=double  </span><br><span class="line">    endif  </span><br><span class="line">  </span><br><span class="line">    if has("</span>win32<span class="string">")  </span><br><span class="line">        source <span class="variable">$VIMRUNTIME</span>/delmenu.vim  </span><br><span class="line">           source <span class="variable">$VIMRUNTIME</span>/menu.vim  </span><br><span class="line">           language messages zh_CN.utf-8  </span><br><span class="line">    endif  </span><br><span class="line">    else  </span><br><span class="line">           echoerr "</span>Sorry, this version of (g)vim was not compiled with +multi_byte<span class="string">"  </span><br><span class="line">endif  </span><br><span class="line">  </span><br><span class="line">"</span> Buffers操作快捷方式!  </span><br><span class="line">nnoremap &lt;C-RETURN&gt; :bnext&lt;CR&gt;  </span><br><span class="line">nnoremap &lt;C-S-RETURN&gt; :bprevious&lt;CR&gt;  </span><br><span class="line"><span class="string">" Tab操作快捷方式!  </span><br><span class="line">nnoremap &lt;C-TAB&gt; :tabnext&lt;CR&gt;  </span><br><span class="line">nnoremap &lt;C-S-TAB&gt; :tabprev&lt;CR&gt;  </span><br><span class="line">"</span>关于tab的快捷键  </span><br><span class="line"><span class="string">" map tn :tabnext&lt;cr&gt;  </span><br><span class="line">"</span> map tp :tabprevious&lt;cr&gt;  </span><br><span class="line"><span class="string">" map td :tabnew .&lt;cr&gt;  </span><br><span class="line">"</span> map te :tabedit  </span><br><span class="line"><span class="string">" map tc :tabclose&lt;cr&gt;  </span><br><span class="line">"</span>窗口分割时,进行切换的按键热键需要连接两次,比如从下方窗口移动  </span><br><span class="line"><span class="string">"光标到上方窗口,需要&lt;c-w&gt;&lt;c-w&gt;k,非常麻烦,现在重映射为&lt;c-k&gt;,切换的  </span><br><span class="line">"</span>时候会变得非常方便.  </span><br><span class="line">nnoremap &lt;C-h&gt; &lt;C-w&gt;h  </span><br><span class="line">nnoremap &lt;C-j&gt; &lt;C-w&gt;j  </span><br><span class="line">nnoremap &lt;C-k&gt; &lt;C-w&gt;k  </span><br><span class="line">nnoremap &lt;C<span class="_">-l</span>&gt; &lt;C-w&gt;l  </span><br><span class="line"><span class="string">"一些不错的映射转换语法（如果在一个文件中混合了不同语言时有用）  </span><br><span class="line">nnoremap &lt;leader&gt;1 :set filetype=xhtml&lt;CR&gt;  </span><br><span class="line">nnoremap &lt;leader&gt;2 :set filetype=css&lt;CR&gt;  </span><br><span class="line">nnoremap &lt;leader&gt;3 :set filetype=javascript&lt;CR&gt;  </span><br><span class="line">nnoremap &lt;leader&gt;4 :set filetype=php&lt;CR&gt;  </span><br><span class="line">"</span> <span class="built_in">set</span> fileformats=unix,dos,mac  </span><br><span class="line"><span class="string">" nmap &lt;leader&gt;fd :se fileformat=dos&lt;CR&gt;  </span><br><span class="line">"</span> nmap &lt;leader&gt;fu :se fileformat=unix&lt;CR&gt;  </span><br><span class="line">  </span><br><span class="line"><span class="string">" use Ctrl+[l|n|p|cc] to list|next|previous|jump to count the result  </span><br><span class="line">"</span> map &lt;C-x&gt;l &lt;ESC&gt;:cl&lt;CR&gt;  </span><br><span class="line"><span class="string">" map &lt;C-x&gt;n &lt;ESC&gt;:cn&lt;CR&gt;  </span><br><span class="line">"</span> map &lt;C-x&gt;p &lt;ESC&gt;:cp&lt;CR&gt;  </span><br><span class="line"><span class="string">" map &lt;C-x&gt;c &lt;ESC&gt;:cc&lt;CR&gt;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">"</span> 让 Tohtml 产生有 CSS 语法的 html  </span><br><span class="line"><span class="string">" syntax/2html.vim，可以用:runtime! syntax/2html.vim  </span><br><span class="line">let html_use_css=1  </span><br><span class="line">"</span> Python 文件的一般设置，比如不要 tab 等  </span><br><span class="line">autocmd FileType python <span class="built_in">set</span> tabstop=4 shiftwidth=4 expandtab  </span><br><span class="line">autocmd FileType python map &lt;F12&gt; :!python %&lt;CR&gt;  </span><br><span class="line"><span class="string">" 选中状态下 Ctrl+c 复制  </span><br><span class="line">vmap &lt;C-c&gt; "</span>+y  </span><br><span class="line"><span class="string">" 打开javascript折叠  </span><br><span class="line">let b:javascript_fold=1  </span><br><span class="line">"</span> 打开javascript对dom、html和css的支持  </span><br><span class="line"><span class="built_in">let</span> javascript_<span class="built_in">enable</span>_domhtmlcss=1  </span><br><span class="line"><span class="string">" 设置字典 ~/.vim/dict/文件的路径  </span><br><span class="line">autocmd filetype javascript set dictionary=<span class="variable">$VIMFILES</span>/dict/javascript.dict  </span><br><span class="line">autocmd filetype css set dictionary=<span class="variable">$VIMFILES</span>/dict/css.dict  </span><br><span class="line">autocmd filetype php set dictionary=<span class="variable">$VIMFILES</span>/dict/php.dict  </span><br><span class="line">  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line">    <span class="string">" plugin - bufexplorer.vim Buffers切换  </span><br><span class="line">    "</span> \be 全屏方式查看全部打开的文件列表  </span><br><span class="line">    <span class="string">" \bv 左右方式查看   \bs 上下方式查看  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">    "</span> plugin - taglist.vim  查看函数列表，需要ctags程序  </span><br><span class="line">    <span class="string">" F4 打开隐藏taglist窗口  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line"><span class="keyword">if</span> MySys() == <span class="string">"windows"</span>                <span class="string">" 设定windows系统中ctags程序的位置  </span><br><span class="line">    let Tlist_Ctags_Cmd = '"</span><span class="string">'.$VIMRUNTIME.'</span>/ctags.exe<span class="string">"'  </span><br><span class="line">elseif MySys() == "</span>linux<span class="string">"              "</span> 设定windows系统中ctags程序的位置  </span><br><span class="line">    <span class="built_in">let</span> Tlist_Ctags_Cmd = <span class="string">'/usr/bin/ctags'</span>  </span><br><span class="line">endif  </span><br><span class="line">nnoremap &lt;silent&gt;&lt;F4&gt; :TlistToggle&lt;CR&gt;  </span><br><span class="line"><span class="built_in">let</span> Tlist_Show_One_File = 1            <span class="string">" 不同时显示多个文件的tag，只显示当前文件的  </span><br><span class="line">let Tlist_Exit_OnlyWindow = 1          "</span> 如果taglist窗口是最后一个窗口，则退出vim  </span><br><span class="line"><span class="built_in">let</span> Tlist_Use_Right_Window = 1         <span class="string">" 在右侧窗口中显示taglist窗口  </span><br><span class="line">let Tlist_File_Fold_Auto_Close=1       "</span> 自动折叠当前非编辑文件的方法列表  </span><br><span class="line"><span class="built_in">let</span> Tlist_Auto_Open = 0  </span><br><span class="line"><span class="built_in">let</span> Tlist_Auto_Update = 1  </span><br><span class="line"><span class="built_in">let</span> Tlist_Hightlight_Tag_On_BufEnter = 1  </span><br><span class="line"><span class="built_in">let</span> Tlist_Enable_Fold_Column = 0  </span><br><span class="line"><span class="built_in">let</span> Tlist_Process_File_Always = 1  </span><br><span class="line"><span class="built_in">let</span> Tlist_Display_Prototype = 0  </span><br><span class="line"><span class="built_in">let</span> Tlist_Compact_Format = 1  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">"</span> plugin - mark.vim 给各种tags标记不同的颜色，便于观看调式的插件。  </span><br><span class="line"><span class="string">" \m  mark or unmark the word under (or before) the cursor  </span><br><span class="line">"</span> \r  manually input a regular expression. 用于搜索.  </span><br><span class="line"><span class="string">" \n  clear this mark (i.e. the mark under the cursor), or clear all highlighted marks .  </span><br><span class="line">"</span> \*  当前MarkWord的下一个     \<span class="comment">#  当前MarkWord的上一个  </span></span><br><span class="line"><span class="string">" \/  所有MarkWords的下一个    \?  所有MarkWords的上一个  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">"</span> plugin - NERD_tree.vim 以树状方式浏览系统中的文件和目录  </span><br><span class="line"><span class="string">" :ERDtree 打开NERD_tree         :NERDtreeClose    关闭NERD_tree  </span><br><span class="line">"</span> o 打开关闭文件或者目录         t 在标签页中打开  </span><br><span class="line"><span class="string">" T 在后台标签页中打开           ! 执行此文件  </span><br><span class="line">"</span> p 到上层目录                   P 到根目录  </span><br><span class="line"><span class="string">" K 到第一个节点                 J 到最后一个节点  </span><br><span class="line">"</span> u 打开上层目录                 m 显示文件系统菜单（添加、删除、移动操作）  </span><br><span class="line"><span class="string">" r 递归刷新当前目录             R 递归刷新当前根目录  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line"><span class="string">" F3 NERDTree 切换  </span><br><span class="line">map &lt;F3&gt; :NERDTreeToggle&lt;CR&gt;  </span><br><span class="line">imap &lt;F3&gt; &lt;ESC&gt;:NERDTreeToggle&lt;CR&gt;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line"><span class="string">" plugin - NERD_commenter.vim   注释代码用的，  </span><br><span class="line">"</span> [count],cc 光标以下count行逐行添加注释(7,cc)  </span><br><span class="line"><span class="string">" [count],cu 光标以下count行逐行取消注释(7,cu)  </span><br><span class="line">"</span> [count],cm 光标以下count行尝试添加块注释(7,cm)  </span><br><span class="line"><span class="string">" ,cA 在行尾插入 /* */,并且进入插入模式。 这个命令方便写注释。  </span><br><span class="line">"</span> 注：count参数可选，无则默认为选中行或当前行  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">"</span><span class="built_in">let</span> NERDSpaceDelims=1       <span class="string">" 让注释符与语句之间留一个空格  </span><br><span class="line">"</span><span class="built_in">let</span> NERDCompact***yComs=1   <span class="string">" 多行注释时样子更好看  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line"><span class="string">" plugin - DoxygenToolkit.vim  由注释生成文档，并且能够快速生成函数标准注释  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line"><span class="built_in">let</span> g:DoxygenToolkit_authorName=<span class="string">"Asins - asinsimple AT gmail DOT com"</span>  </span><br><span class="line"><span class="built_in">let</span> g:DoxygenToolkit_briefTag_funcName=<span class="string">"yes"</span>  </span><br><span class="line">map &lt;leader&gt;da :DoxAuthor&lt;CR&gt;  </span><br><span class="line">map &lt;leader&gt;df :Dox&lt;CR&gt;  </span><br><span class="line">map &lt;leader&gt;db :DoxBlock&lt;CR&gt;  </span><br><span class="line">map &lt;leader&gt;dc a /*  */&lt;LEFT&gt;&lt;LEFT&gt;&lt;LEFT&gt;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">"</span> plugin – ZenCoding.vim 很酷的插件，HTML代码生成  </span><br><span class="line"><span class="string">" 插件最新版：http://github.com/mattn/zencoding-vim  </span><br><span class="line">"</span> 常用命令可看：http://nootn.com/blog/Tool/23/  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line"><span class="string">" plugin – checksyntax.vim    JavaScript常见语法错误检查  </span><br><span class="line">"</span> 默认快捷方式为 F5  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">let g:checksyntax_auto = 0 "</span> 不自动检查  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">"</span> plugin - NeoComplCache.vim    自动补全插件  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">let g:AutoComplPop_NotEnableAtStartup = 1  </span><br><span class="line">let g:NeoComplCache_EnableAtStartup = 1  </span><br><span class="line">let g:NeoComplCache_SmartCase = 1  </span><br><span class="line">let g:NeoComplCache_TagsAutoUpdate = 1  </span><br><span class="line">let g:NeoComplCache_EnableInfo = 1  </span><br><span class="line">let g:NeoComplCache_EnableCamelCaseCompletion = 1  </span><br><span class="line">let g:NeoComplCache_MinSyntaxLength = 3  </span><br><span class="line">let g:NeoComplCache_EnableSkipCompletion = 1  </span><br><span class="line">let g:NeoComplCache_SkipInputTime = '0.5'  </span><br><span class="line">let g:NeoComplCache_SnippetsDir = <span class="variable">$VIMFILES</span>.'/snippets'  </span><br><span class="line">"</span> &lt;TAB&gt; completion.  </span><br><span class="line">inoremap &lt;expr&gt;&lt;TAB&gt; pumvisible() ? <span class="string">"\&lt;C-n&gt;"</span> : <span class="string">"\&lt;TAB&gt;"</span>  </span><br><span class="line"><span class="string">" snippets expand key  </span><br><span class="line">imap &lt;silent&gt; &lt;C-e&gt; &lt;Plug&gt;(neocomplcache_snippets_expand)  </span><br><span class="line">smap &lt;silent&gt; &lt;C-e&gt; &lt;Plug&gt;(neocomplcache_snippets_expand)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line"><span class="string">" plugin - matchit.vim   对%命令进行扩展使得能在嵌套标签和语句之间跳转  </span><br><span class="line">"</span> % 正向匹配      g% 反向匹配  </span><br><span class="line"><span class="string">" [% 定位块首     ]% 定位块尾  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">"</span> plugin - vcscommand.vim   对%命令进行扩展使得能在嵌套标签和语句之间跳转  </span><br><span class="line"><span class="string">" SVN/git管理工具  </span><br><span class="line">"</span>-----------------------------------------------------------------  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">"</span> plugin – a.vim  </span><br><span class="line"><span class="string">"-----------------------------------------------------------------  </span><br><span class="line">  </span><br><span class="line">"</span>---------------------------------------------  </span><br><span class="line"><span class="string">"定义函数SetTitle，自动插入文件头  </span><br><span class="line">func SetTitle()  </span><br><span class="line">if &amp;filetype == 'c'  </span><br><span class="line">    call setline(1, "</span><span class="comment">#include &lt;stdio.h&gt;")  </span></span><br><span class="line">    call setline(2, <span class="string">"#include &lt;stdlib.h&gt;"</span>)  </span><br><span class="line">    call setline(3, <span class="string">""</span>)  </span><br><span class="line">    call setline(4, <span class="string">"int main() &#123;"</span>)  </span><br><span class="line">elseif &amp;filetype == <span class="string">'cpp'</span>  </span><br><span class="line">    call setline(1, <span class="string">"#include &lt;iostream&gt;"</span>)  </span><br><span class="line">    call setline(2, <span class="string">""</span>)  </span><br><span class="line">    call setline(3, <span class="string">"int main() &#123;"</span>)  </span><br><span class="line">elseif &amp;filetype == <span class="string">'sh'</span>  </span><br><span class="line">    call setline(1, <span class="string">"#!/bin/bash"</span>)  </span><br><span class="line">    call setline(2, <span class="string">""</span>)  </span><br><span class="line">elseif &amp;filetype == <span class="string">'py'</span>  </span><br><span class="line">    call setline(1, <span class="string">"#!coding:utf-8"</span>)  </span><br><span class="line">    call setline(2, <span class="string">""</span>)  </span><br><span class="line">endif  </span><br><span class="line">endfunc  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">set</span> completeopt=longest,menu  </span><br><span class="line">    <span class="string">"新建.c,.h,.sh,.java文件，自动插入文件头  </span><br><span class="line">    autocmd BufNewFile *.[ch],*.sh,*.cpp,*.java exec "</span>:call SetTitle()<span class="string">"  </span><br><span class="line">    "</span>---------------------------------------------  </span><br><span class="line">  </span><br><span class="line"><span class="string">"定义CompileRun函数，用来调用进行编译和运行  </span><br><span class="line">func! CompileRun()  </span><br><span class="line">    exec "</span>w<span class="string">"  </span><br><span class="line">    "</span>C程序  </span><br><span class="line">    <span class="keyword">if</span> &amp;filetype == <span class="string">'c'</span>  </span><br><span class="line">        <span class="built_in">exec</span> <span class="string">"!gcc % -g -o %&lt;.exe"</span>  </span><br><span class="line">        <span class="built_in">exec</span> <span class="string">"! ./%&lt;.exe"</span>  </span><br><span class="line">    elseif &amp;filetype == <span class="string">'cpp'</span>  </span><br><span class="line">        <span class="built_in">exec</span> <span class="string">"!g++ % -g -o %&lt;"</span>  </span><br><span class="line">        <span class="built_in">exec</span> <span class="string">"! ./%&lt;"</span>  </span><br><span class="line">        <span class="string">"Java程序  </span><br><span class="line">    elseif &amp;filetype == 'java'  </span><br><span class="line">        exec "</span>!javac %<span class="string">"  </span><br><span class="line">        exec "</span>!java %&lt;<span class="string">"  </span><br><span class="line">    endif  </span><br><span class="line">endfunc  </span><br><span class="line">"</span>结束定义CompileRun  </span><br><span class="line"><span class="string">"-------------------------  </span><br><span class="line">"</span> ======= 编译 &amp;&amp; 运行 ======= <span class="string">"  </span><br><span class="line">"</span> 编译源文件  </span><br><span class="line">func! CompileCode()  </span><br><span class="line">     <span class="built_in">exec</span> <span class="string">"w"</span>  </span><br><span class="line">     <span class="keyword">if</span> &amp;filetype == <span class="string">"c"</span>  </span><br><span class="line">         <span class="built_in">exec</span> <span class="string">"!gcc -Wall -std=c99 %&lt;.c -o %&lt;"</span>  </span><br><span class="line">     elseif &amp;filetype == <span class="string">"cpp"</span>  </span><br><span class="line">         <span class="built_in">exec</span> <span class="string">"!g++ -Wall -std=c++98 %&lt;.cpp -o %&lt;"</span>  </span><br><span class="line">     elseif &amp;filetype == <span class="string">"java"</span>  </span><br><span class="line">         <span class="built_in">exec</span> <span class="string">"!javac %&lt;.java"</span>  </span><br><span class="line">     endif  </span><br><span class="line">endfunc  </span><br><span class="line"><span class="string">" 运行可执行文件  </span><br><span class="line">func! RunCode()  </span><br><span class="line">    exec "</span>w<span class="string">"  </span><br><span class="line">    if &amp;filetype == "</span>c<span class="string">" || &amp;filetype == "</span>cpp<span class="string">" || &amp;filetype == "</span>haskell<span class="string">"  </span><br><span class="line">        exec "</span>! ./%&lt;<span class="string">"  </span><br><span class="line">    elseif &amp;filetype == "</span>java<span class="string">"  </span><br><span class="line">        exec "</span>!java %&lt;<span class="string">"  </span><br><span class="line">    endif  </span><br><span class="line">endfunc  </span><br><span class="line">  </span><br><span class="line">"</span>---------------------------------------------  </span><br><span class="line"><span class="string">" Ctrl + F9   一键保存, 编译  </span><br><span class="line">"</span> Ctrl + F10  一键保存，运行  </span><br><span class="line"><span class="string">" F9  编译 + 运行  </span><br><span class="line">"</span> F10 Debug  </span><br><span class="line">map&lt;C-F9&gt;:call CompileCode()&lt;CR&gt;  </span><br><span class="line">imap&lt;C-F9&gt; &lt;ESC&gt;:call CompileCode()&lt;CR&gt;  </span><br><span class="line">vmap&lt;C-F9&gt; &lt;ESC&gt;:call CompileCode()&lt;CR&gt;  </span><br><span class="line">map&lt;C-F10&gt;:call RunCode()&lt;CR&gt;  </span><br><span class="line">imap&lt;C-F10&gt; &lt;ESC&gt;:call RunCode()&lt;CR&gt;  </span><br><span class="line">vmap&lt;C-F10&gt; &lt;ESC&gt;:call RunCode()&lt;CR&gt;  </span><br><span class="line">map&lt;F9&gt;:call CompileRun()&lt;CR&gt;  </span><br><span class="line">imap&lt;F9&gt; &lt;ESC&gt;:call CompileRun()&lt;CR&gt;  </span><br><span class="line">vmap &lt;F9&gt; &lt;ESC&gt;:call CompileRun()&lt;CR&gt;  </span><br><span class="line">                                 <span class="string">"  </span><br><span class="line">"</span><span class="built_in">set</span> mouse=v <span class="string">" 鼠标支持</span></span><br></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Macos High Sierra 10.13 Work Environment Install]]></title>
      <url>http://sggo.me/2017/10/03/ops/ops-mac10.13-install-env/</url>
      <content type="html"><![CDATA[<p>Enable Dragging With Three Finger : </p>
<blockquote>
<p>System Preferences -&gt; Accessibility -&gt; Mouse &amp; Trackpad -&gt; Trackpad Options.</p>
</blockquote>
<a id="more"></a>
<h2 id="1-Common-Soft"><a href="#1-Common-Soft" class="headerlink" title="1. Common Soft"></a>1. Common Soft</h2><ol>
<li>Chrome</li>
<li>NeteaseMusic</li>
<li>Baiduyun &amp; Aira2GUI</li>
<li>Microsoft_Office_2016_15.38.17090200_Installer.pkg</li>
</ol>
<blockquote>
<p>Google Chrome is up to date<br>Version 61.0.3163.100 (Official Build) (64-bit)</p>
<p>百度云破解限速 (Aria2GUI + chrome plugin)</p>
<p>Evernote 国际版与国内版分开管理的.</p>
</blockquote>
<h2 id="2-Dev-Tools"><a href="#2-Dev-Tools" class="headerlink" title="2. Dev Tools"></a>2. Dev Tools</h2><h3 id="2-1-general-dev-tool"><a href="#2-1-general-dev-tool" class="headerlink" title="2.1 general dev tool"></a>2.1 general dev tool</h3><ol>
<li>Macdown</li>
<li>Alfred</li>
<li>Atom</li>
<li>SubLime Text</li>
<li><a href="https://brew.sh/" target="_blank" rel="external">Homebrew</a></li>
<li>Iterm2</li>
<li>Oh-my-zsh</li>
<li><a href="https://feiyang.li/2017/02/26/jetbrains/index.html" target="_blank" rel="external">PyCharm &amp; IDEA</a></li>
<li>GNU_Octave_3.8.0-6.dmg</li>
</ol>
<blockquote>
<p>brew (install 过程会自动需要 Xcode 被安装)<br>brew install wget tree </p>
<p>wget <a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="external">https://bootstrap.pypa.io/get-pip.py</a> <br><br>sudo python get-pip.py</p>
</blockquote>
<h3 id="2-2-iterm-amp-zsh"><a href="#2-2-iterm-amp-zsh" class="headerlink" title="2.2 iterm &amp; zsh"></a>2.2 iterm &amp; zsh</h3><p><strong>Iterm2 Change Font</strong></p>
<blockquote>
<p>Iterm2 -&gt; Preference -&gt; Profiles -&gt; Text -&gt; Change Font -&gt; 17pt Courier New Bold</p>
</blockquote>
<p><strong>Iterm2 Hide scrollbars And title bar</strong></p>
<blockquote>
<p>Preference -&gt; Appearance </p>
<p>取消 show per-pane title bar with split panes.<br>勾选 Hide scrollbars</p>
</blockquote>
<p><strong>Iterm2 Color Presets</strong></p>
<blockquote>
<p>Iterm2 -&gt; Preference -&gt; Profiles -&gt; Color -&gt; Color Presets -&gt; your_theme</p>
<p>maybe Atom, Brogrammer, Darkside </p>
<p><a href="https://github.com/iplaces/astro-zsh-theme" target="_blank" rel="external">Zsh astro theme</a></p>
</blockquote>
<p><strong>oh-my-zsh 自带 git 插件，里面的针对git 的别名设置见</strong>:</p>
<blockquote>
<p>➜ &gt;vim .oh-my-zsh/plugins/git/git.plugin.zsh</p>
</blockquote>
<p><strong>oh-my-zsh autojump plugin install</strong></p>
<ol>
<li>brew install autojump</li>
<li>vim .zshrc</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">plugins=(git autojump)</span><br><span class="line">[[ -s $(brew --prefix)/etc/profile.d/autojump.sh ]] &amp;&amp; . $(brew --prefix)/etc/profile.d/autojump.sh</span><br></pre></td></tr></table></figure>
<p>then, source ~/.zshrc</p>
<p><strong>Reference Article</strong></p>
<blockquote>
<p><a href="http://iterm2colorschemes.com/" target="_blank" rel="external">Iterm2-color-schemes</a><br><a href="https://github.com/bahlo/iterm-colors" target="_blank" rel="external">Iterm-colors</a><br><a href="https://github.com/iplaces/astro-zsh-theme" target="_blank" rel="external">Zsh astro theme</a><br><a href="http://blog.csdn.net/rapheler/article/details/51505003" target="_blank" rel="external">使用 Zsh 的十大优点</a>.<br><a href="http://yijiebuyi.com/blog/b9b5e1ebb719f22475c38c4819ab8151.html" target="_blank" rel="external">oh-my-zsh配置你的zsh提高shell逼格终极选择</a>.<br><a href="http://huang-jerryc.com/2016/08/11/%E6%89%93%E9%80%A0%E9%AB%98%E6%95%88%E4%B8%AA%E6%80%A7Terminal%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%20iTerm/" target="_blank" rel="external">打造高效个性Terminal（一）之 iTerm</a>.<br><a href="https://segmentfault.com/a/1190000006248107" target="_blank" rel="external">打造高效个性Terminal（二）之 zsh</a>.<br><a href="http://www.barretlee.com/blog/2015/03/30/autojump-in-mac/" target="_blank" rel="external">Mac下的效率工具autojump</a></p>
</blockquote>
<hr>
<h3 id="2-3-ssh-config"><a href="#2-3-ssh-config" class="headerlink" title="2.3 ssh config"></a>2.3 ssh config</h3><ol>
<li>ssh-keygen -t rsa -C “your-company-email-full-address”</li>
<li>~/.ssh/id_rsa.pub 粘贴到运维平台</li>
</ol>
<blockquote>
<p>mac iterm2 ssh 跳转失败, 解决办法 :</p>
<p>(1) 新建并编辑 .ssh/config, 并复制以下内容到 config文件中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Host * </span><br><span class="line">ForwardAgent yes </span><br><span class="line">PasswordAuthentication yes </span><br><span class="line">StrictHostKeyChecking no </span><br><span class="line">HashKnownHosts yes </span><br><span class="line">Compression yes</span><br></pre></td></tr></table></figure>
<p>(2) cd ～/.ssh, 并在 terminal 中执行 ssh-add</p>
</blockquote>
<h3 id="2-4-navicat-for-MySQL"><a href="#2-4-navicat-for-MySQL" class="headerlink" title="2.4 navicat for MySQL"></a>2.4 navicat for MySQL</h3><p><img src="/images/ops/ops-ssh-general.png" alt="rds-general"></p>
<blockquote>
<p>Encoding 设置为 utf-8 则，查询数据库，汉字乱码，改为 Auto 解决。</p>
</blockquote>
<p><img src="/images/ops/ops-ssh-rds.png" alt="ssh-rds"></p>
<h2 id="3-Java"><a href="#3-Java" class="headerlink" title="3. Java"></a>3. Java</h2><ol>
<li><a href="http://blog.tibame.com/?p=2068" target="_blank" rel="external">JDK</a></li>
<li>Maven</li>
<li>Tomcat</li>
<li>Scala</li>
<li>Spark</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  software pwd</span><br><span class="line">/usr/local/xsoft/software</span><br><span class="line">➜  software ll</span><br><span class="line">total 0</span><br><span class="line">lrwxr-xr-x  apache-maven -&gt; /usr/local/xsoft/deploy/apache-maven-3.3.9</span><br><span class="line">lrwxr-xr-x  apache-tomcat -&gt; /usr/local/xsoft/deploy/apache-tomcat-7.0.59</span><br><span class="line">lrwxr-xr-x  scala -&gt; /usr/local/xsoft/deploy/scala-2.11.7</span><br><span class="line">lrwxr-xr-x  spark -&gt; /usr/local/xsoft/deploy/spark-1.6.3-bin-hadoop2.6</span><br><span class="line">➜  software</span><br></pre></td></tr></table></figure>
<h2 id="4-Blog"><a href="#4-Blog" class="headerlink" title="4. Blog"></a>4. Blog</h2><ol>
<li><a href="https://hexo.io/docs/" target="_blank" rel="external">hexo</a></li>
<li>Install Node.js</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Once nvm is installed, restart the terminal and run the following command to install Node.js:</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nvm install v4.1.0</span><br><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<blockquote>
<p>v4.1.0 更合适 hexo</p>
</blockquote>
<h2 id="5-Python"><a href="#5-Python" class="headerlink" title="5. Python"></a>5. Python</h2><h3 id="5-1-this-mac-install-pip"><a href="#5-1-this-mac-install-pip" class="headerlink" title="5.1 this mac install pip"></a>5.1 this mac install pip</h3><p><a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="external">Python pip</a> , <code>sudo python get-pip.py</code></p>
<p>then, terminal input <code>pip list</code>.</p>
<blockquote>
<p>If exist warning:</p>
<p>DEPRECATION: The default format will switch to columns in the future. You can use –format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.</p>
<p>solve this warning:</p>
<p>~.vim ~/.pip/pip.conf<br>[list]<br>format=columns  </p>
</blockquote>
<h3 id="5-2-pyenv-install-package"><a href="#5-2-pyenv-install-package" class="headerlink" title="5.2 pyenv install package"></a>5.2 pyenv install package</h3><p><strong>First, you need to install python <code>pyenv</code> environment</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">Python <span class="number">3.6</span><span class="number">.3</span></span><br><span class="line">(vpy3)</span><br><span class="line">➜</span><br><span class="line"></span><br><span class="line">pip install numpy</span><br><span class="line">pip install scipy</span><br><span class="line">pip install matplotlib</span><br><span class="line">pip install pandas</span><br><span class="line"></span><br><span class="line">pip install xlrd</span><br><span class="line">pip install xlwt</span><br><span class="line">pip install StatsModels</span><br><span class="line">pip install scikit-learn</span><br><span class="line"></span><br><span class="line">pip install jieba</span><br><span class="line">pip install --upgrade gensim</span><br></pre></td></tr></table></figure>
<blockquote>
<p>pip install elasticsearch==1.9</p>
</blockquote>
<p><strong>ipython</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install ipython</span><br></pre></td></tr></table></figure>
<blockquote>
<p>然后, 如 terminall input <code>ipython</code> 不存在, 则 pip show ipython,  python -m IPython 试试. </p>
</blockquote>
<p><strong>notebook</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#pip install --ignore-installed six</span></span><br><span class="line"><span class="comment">#pip install target-gsheet tap-fixerio</span></span><br><span class="line">pip install notebook</span><br></pre></td></tr></table></figure>
<blockquote>
<p>pip install notebook, 如 macos High Sierra 10.13 报错，则<br>pip install –ignore-installed six<br>pip install target-gsheet tap-fixerio<br>then, pip install notebook</p>
</blockquote>
<h2 id="8-Reference"><a href="#8-Reference" class="headerlink" title="8. Reference"></a>8. Reference</h2><ul>
<li><a href="https://brew.sh/" target="_blank" rel="external">Homebrew</a></li>
<li><a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="external">Get-pip</a></li>
<li><a href="https://feiyang.li/2017/02/26/jetbrains/index.html" target="_blank" rel="external">IntelliJ、Pycharm激活</a></li>
<li><a href="http://blog.tibame.com/?p=2068" target="_blank" rel="external">Mac OSX 安裝JDK</a></li>
<li><a href="https://hexo.io/docs/" target="_blank" rel="external">Hexo Doc</a></li>
<li><a href="http://www.jianshu.com/p/3e0206dd23ac" target="_blank" rel="external">Mac 上完整卸载Node.js</a></li>
<li><a href="http://10176523.cn/archives/50" target="_blank" rel="external">Mac OSX 完整卸载Node.js</a></li>
<li><a href="https://segmentfault.com/a/1190000004404505" target="_blank" rel="external">node版本管理工具nvm-Mac下安装及使用</a></li>
<li><a href="http://xclient.info/" target="_blank" rel="external">XClient.info Mac App</a></li>
<li><a href="https://stackoverflow.com/questions/32856194/ipython-on-macos-10-10-command-not-found" target="_blank" rel="external">Stackoverflow python on MacOS 10.10 - command not found</a></li>
<li><a href="/2016/08/02/ml-python-env/">Blair python install data mining env</a></li>
</ul>
<p>Macos NSNavRecentPlaces 内部自动生成的配置，别乱改。</p>
<blockquote>
<p>defaults write -g NSNavRecentPlaces ‘(“~/Desktop”, “/usr/local/xsoft/software”)’;</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 7 - Dialog_Corpus 常用数据集]]></title>
      <url>http://sggo.me/2017/09/26/chatbot/chatbot-research7/</url>
      <content type="html"><![CDATA[<p>一些用于 Dialog 对话系统的数据集资料汇总</p>
<a id="more"></a>
<h2 id="对话系统常用数据集"><a href="#对话系统常用数据集" class="headerlink" title="对话系统常用数据集"></a>对话系统常用数据集</h2><blockquote>
<p>介绍一下公开的数据集 :</p>
<p>可以参考“<a href="https://arxiv.org/abs/1512.05742" target="_blank" rel="external">A Survey of Available Corpora for Building Data-Driven Dialogue Systems</a>”这篇论文，而且作者把所有的数据集按照不同类别进行分类总结，里面涵盖了很多数据集，详情请戳 <a href="https://docs.google.com/spreadsheets/d/1SJ4XV6NIEl_ReF1odYBRXs0q6mTkedoygY3kLMPjcP8/pubhtml" target="_blank" rel="external">Dialogue datasets</a></p>
</blockquote>
<h2 id="英文数据集"><a href="#英文数据集" class="headerlink" title="英文数据集"></a>英文数据集</h2><blockquote>
<ul>
<li><a href="http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" target="_blank" rel="external">Cornell Movie Dialogs</a></li>
<li><a href="https://arxiv.org/abs/1506.08909" target="_blank" rel="external">Ubuntu Dialogue Corpus</a></li>
<li><a href="http://opus.lingfil.uu.se/OpenSubtitles.php" target="_blank" rel="external">OpenSubtitles：电影字幕</a></li>
<li><a href="https://github.com/Marsan-Ma/twitter_scraper" target="_blank" rel="external">Twitter：twitter数据集</a></li>
<li><a href="https://github.com/bshao001/ChatLearner" target="_blank" rel="external">Papaya Conversational Data Set：基于Cornell、Reddit等数据集重新整理</a></li>
</ul>
</blockquote>
<p>相关数据集的处理代码可参见下面两个github项目：</p>
<ul>
<li><a href="https://github.com/Conchylicultor/DeepQA" target="_blank" rel="external">DeepQA</a></li>
<li><a href="https://github.com/Marsan-Ma/chat_corpus" target="_blank" rel="external">chat_corpus</a></li>
</ul>
<h2 id="中文数据集"><a href="#中文数据集" class="headerlink" title="中文数据集"></a>中文数据集</h2><blockquote>
<ul>
<li><a href="https://github.com/rustch3n/dgk_lost_conv" target="_blank" rel="external">dgk_shooter_min.conv：中文电影台词数据集</a></li>
<li><a href="https://github.com/Samurais/egret-wenda-corpus" target="_blank" rel="external">白鹭时代中文问答语料：白鹭时代论坛问答数据</a></li>
<li><a href="http://61.93.89.94/Noah_NRM_Data/" target="_blank" rel="external">微博数据集：华为李航实验室发布</a> 也是论文“Neural Responding Machine for Short-Text Conversation”使用的数据集</li>
<li><a href="http://lwc.daanvanesch.nl/openaccess.php" target="_blank" rel="external">新浪微博数据集，评论回复短句</a></li>
</ul>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><blockquote>
<ul>
<li><a href="https://github.com/candlewill/Dialog_Corpus" target="_blank" rel="external">Github 用于对话系统的中英文语料 汇总合集</a></li>
<li><a href="http://www.shareditor.com/blogshow/?blogId=112" target="_blank" rel="external">美剧 ： 近1GB的三千万聊天语料 - 10元</a></li>
<li><a href="https://github.com/gunthercox/chatterbot-corpus/blob/master/chatterbot_corpus/data/chinese/greetings.yml" target="_blank" rel="external">chatterbot-corpus chinese</a></li>
<li><a href="https://www.jianshu.com/u/73283aaafe29" target="_blank" rel="external">简书 ： 高质量的对话语料</a></li>
<li><a href="https://www.zhihu.com/question/44764422" target="_blank" rel="external">知乎 ： 现在有哪些中文的聊天语料库？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33088748" target="_blank" rel="external">深度学习对话系统理论篇–数据集和评价指标介绍</a></li>
</ul>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Singapore IT environment]]></title>
      <url>http://sggo.me/2017/09/25/tools/news-Singapore-for-it/</url>
      <content type="html"><![CDATA[<p>本文简短介绍了下 Singapore 的一些 IT 公司的情况</p>
<a id="more"></a>
<h2 id="1-Mozat"><a href="#1-Mozat" class="headerlink" title="1. Mozat"></a>1. Mozat</h2><blockquote>
<p>mozat致力于移动端社交应用和游戏应用，不断研发创新。</p>
</blockquote>
<h2 id="2-Garena"><a href="#2-Garena" class="headerlink" title="2. Garena"></a>2. Garena</h2><blockquote>
<p>游戏起家的综合互联网公司，估值达37.5亿美金</p>
</blockquote>
<h2 id="3-Carousell"><a href="#3-Carousell" class="headerlink" title="3. Carousell"></a>3. Carousell</h2><blockquote>
<p>闲鱼</p>
</blockquote>
<h2 id="4-Grabtaxi"><a href="#4-Grabtaxi" class="headerlink" title="4. Grabtaxi"></a>4. Grabtaxi</h2><blockquote>
<p>(获得滴滴、软银6亿美金投资)</p>
</blockquote>
<h2 id="5-iCarsClub"><a href="#5-iCarsClub" class="headerlink" title="5. iCarsClub"></a>5. iCarsClub</h2><blockquote>
<p>汽车借租iCarClub(7050万美元）</p>
</blockquote>
<p>鉴于新加坡的车主大都在自家车上花费了大把大把的钞票，对等私家车租赁这个想法也就应运而生了。</p>
<h2 id="6-Reebonz-4000万美元"><a href="#6-Reebonz-4000万美元" class="headerlink" title="6. Reebonz(4000万美元)"></a>6. Reebonz(4000万美元)</h2><p>当提到新加坡本地的电子商务网店时，除了上述提到的受Rocket Internet公司协助的两家电子商务零售商以外，最出名应该就是Reebonz。它和其他的电子商务网店不同的是它只关注奢侈品牌。Reebonz 在2014年3月收购了新加坡的在线奢侈品商店Clout Shoppe，扩大了自己的规模。</p>
<h2 id="7-MyRepublic（3750万美元）"><a href="#7-MyRepublic（3750万美元）" class="headerlink" title="7. MyRepublic（3750万美元）"></a>7. MyRepublic（3750万美元）</h2><blockquote>
<p>MyRepublic也许是这十家公司中最有野心的公司：它想成为新加坡第四大电信公司。它不满足只是一家本地的大公司，所以最近该公司为新西兰用户提供三个月免费的100兆位/秒的超速光纤宽带。我们还了解到，这家电信初创企业在一个月内就吸引了大约2000名新用户;不就的将来，它的年收入会是2014年三倍，达到1500万美元。2015年，MyRepublic将会打入亚洲的其它国家。</p>
</blockquote>
<h2 id="8-Bubbly-3900万美元"><a href="#8-Bubbly-3900万美元" class="headerlink" title="8. Bubbly(3900万美元)"></a>8. Bubbly(3900万美元)</h2><blockquote>
<p>对于Bubbly公司来说，也许它最想忘记的就是2014年的上半年。在那段时间，社交网络媒体初创企业Bubbly收到了各式各样的收购请求;并且，该公司解散了公司的管理团队，还达成一致协议对公司进行重组。公司的首席执行官Thomas Clayton开始把公司资产清算，几周之后，Bubbly公司就被Altruist Group收购，该集团在欧洲、非洲和亚洲提供电信服务，不过这次收购的条款没有对外披露。</p>
</blockquote>
<h2 id="9-Ninja-van"><a href="#9-Ninja-van" class="headerlink" title="9. Ninja van"></a>9. Ninja van</h2><p>成立于2014年的「最后一公里物流」公司 Ninja Van 两年内就拿到了两轮融资，这家技术驱动型公司使用算法来提供最好的物流路线并实时追踪订单，不仅解决了当地电商网站货品常常丢失的问题，还通过技术手段提高了物流配送效率。</p>
<h2 id="10-Lazada"><a href="#10-Lazada" class="headerlink" title="10. Lazada"></a>10. Lazada</h2><p>Lazada，东南亚地区最大的在线购物网站之一。获得德国创业孵化器RocketInternet桑威尔兄弟(SamwerBrothers)支持，Lazada的目标主要是印尼、马来西亚、菲律宾以及泰国用户。</p>
<h2 id="11-Zalora"><a href="#11-Zalora" class="headerlink" title="11. Zalora"></a>11. Zalora</h2><p>ZALORA是一个网上时装及美容产品购物平台,为男女顾客提供时装、饰物、鞋履及化妆护肤品。总部位于新加坡的ZALORA于不同地区设有分区网页,包括香港、新加坡、印尼、菲律宾、泰国、越南、马来西亚及文莱，到目前为止Zalora已经获得了拥有大量投资者的青睐。</p>
<p>Zalora的母公司于2014年上市，该公司的最新计划是和上述提到的项目Rocket Internet旗下另外四家流行的时尚品牌公司合并，这样一来便可成为全球的时尚团队。Rocket Intetrnet的首次公开募股便获得了82亿美元，这就意味着不久就会有越来越多的资金流入Zalora。</p>
<blockquote>
<p>2.38亿美元</p>
</blockquote>
<h2 id="12-ViSenze"><a href="#12-ViSenze" class="headerlink" title="12. ViSenze"></a>12. ViSenze</h2><p>视觉搜索和图像识别的公司 (产品主要服务Rakuten，Zalora等电商的推荐引擎和手机搜索应用)</p>
<p>ViSenze是一家非常重视技术发展的公司，技术团队内部会不定期举行技术分享会和深度训练。</p>
<h2 id="13-Wego（3450万美元）"><a href="#13-Wego（3450万美元）" class="headerlink" title="13. Wego（3450万美元）"></a>13. Wego（3450万美元）</h2><blockquote>
<p>2013年，旅游搜索公司Wego宣称，在其提供服务的52个市场上，每天起码有1000万个潜在预约客户。该公司于2014年中旬推出了自己的iOS应用和Android应用，很快这款应用的下载量就居印尼iOS应用商店下载量排行榜的榜首。</p>
</blockquote>
<h2 id="14-ViKi（2430万美元）"><a href="#14-ViKi（2430万美元）" class="headerlink" title="14. ViKi（2430万美元）"></a>14. ViKi（2430万美元）</h2><blockquote>
<p>视频聚合网<br>对于新加坡人而言，这个特别的众包视频字幕网站，是熟悉到不能再熟悉的日常浏览网站之一。Viki是2013年对新加坡公司进行大型收购中的璀璨之星，因为日本电商巨头Rakuten以2亿美元的价格收购了Viki。一年后，有关报道称Viki每个月活跃的用户有3500万，另外还有2500万移动用户;与该公司被收购之时相比，每月活跃用户增长了1300万，移动用户增长了1500万。</p>
</blockquote>
<h2 id="15-Neo-Innovation"><a href="#15-Neo-Innovation" class="headerlink" title="15. Neo Innovation"></a>15. Neo Innovation</h2><h2 id="16-Migme（3460万美元）"><a href="#16-Migme（3460万美元）" class="headerlink" title="16. Migme（3460万美元）"></a>16. Migme（3460万美元）</h2><blockquote>
<p>在过去的2014年里，社交娱乐平台Migme的发展是值得一提的。自去年的八月以来，它完成了几项重要的收购.12月，Migme拥有超过900万的MAU。该公司的目标是为了让客户体验更好玩的社交化电子商务。公司的首席执行官Steven Goh的愿望是建立一个像中国淘宝网那样的顾客对顾客的商业模式。</p>
</blockquote>
<h2 id="17-PropertyGuru"><a href="#17-PropertyGuru" class="headerlink" title="17. PropertyGuru"></a>17. PropertyGuru</h2><p>却有幸独占鳌头。最近，<a href="http://99.co公司加入新加坡房地产市场，成为了PropertyGuru最新的竞争对手。" target="_blank" rel="external">http://99.co公司加入新加坡房地产市场，成为了PropertyGuru最新的竞争对手。</a></p>
<h2 id="18-Redmart"><a href="#18-Redmart" class="headerlink" title="18. Redmart"></a>18. Redmart</h2><blockquote>
<p><a href="https://redmart.com/" target="_blank" rel="external">https://redmart.com/</a></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[LDA Topic Model]]></title>
      <url>http://sggo.me/2017/09/22/nlp/LDA/</url>
      <content type="html"><![CDATA[<p>Latent Dirichlet Allocation 主题模型理论:</p>
<ol>
<li>直观版</li>
<li>标准版</li>
<li>公式版</li>
</ol>
<a id="more"></a>
<blockquote>
<p>                                          据我推测，大部分人是可以撑过前两个版本的</p>
</blockquote>
<h2 id="1-什么是主题模型"><a href="#1-什么是主题模型" class="headerlink" title="1. 什么是主题模型"></a>1. 什么是主题模型</h2><p><img src="/images/nlp/lda-01.png" width="420" height="400" img=""></p>
<h3 id="1-1-直观版"><a href="#1-1-直观版" class="headerlink" title="1.1 直观版"></a>1.1 直观版</h3><p><code>场景</code> : 假设某企业想要招聘一个工程师，他们收到了一把简历，他们想直接通过简历来看谁是大牛，谁是彩笔 ?</p>
<p><code>特征</code> :</p>
<p>简历里通常包含这些个人特征 :</p>
<p><img src="/images/nlp/lda-02.png" width="490" height="400" img=""></p>
<p><code>判断基础</code> :</p>
<p><img src="/images/nlp/lda-04.png" width="400" height="200" img=""></p>
<p><code>总结公式</code> :</p>
<p><img src="/images/nlp/lda-05.png" width="700" height="400" img=""></p>
<blockquote>
<p>？</p>
</blockquote>
<h3 id="例子与理论的关系"><a href="#例子与理论的关系" class="headerlink" title="例子与理论的关系"></a>例子与理论的关系</h3><p><img src="/images/nlp/lda-04.png" width="400" height="200" img=""></p>
<p><img src="/images/nlp/lda-06.png" width="400" height="200" img=""></p>
<h2 id="2-什么是-LDA"><a href="#2-什么是-LDA" class="headerlink" title="2. 什么是 LDA ?"></a>2. 什么是 LDA ?</h2><p>Latent Dirichlet Allocation</p>
<p>            是一种无监督的贝叶斯模型</p>
<ol>
<li>是一种主题模型，它可以将文档集中每篇文档的主题按照概率分布的形式给出。</li>
<li>无监督学习算法，训练时需要手工标注训练集，需要的是文档集以及主题数量k即可。</li>
</ol>
<p>此外LDA的另一个优点则是: 每一个主题均可找出词语来描述它。                 </p>
<p>是一种词袋模型，即它认为一篇文档是由一组词构成的一个集合，一篇 Doc 可以包含多个Topic，文档中每个词都由其中一个主题生成。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorboard 可视化好帮手 1]]></title>
      <url>http://sggo.me/2017/09/12/tensorflow/tf-4.1-tensorboard1/</url>
      <content type="html"><![CDATA[<p>学会用 Tensorflow 自带的 tensorboard 去可视化我们所建造出来的神经网络是一个很好的学习理解方式. </p>
<p>用最直观的流程图告诉你, 你的神经网络是长怎样,有助于你发现编程中间的问题和疑问.</p>
<a id="more"></a>
<h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>这次我们会介绍如何可视化神经网络。因为很多时候我们都是做好了一个神经网络，但是没有一个图像可以展示给大家看。</p>
<p><strong>TensorFlow</strong> 的可视化工具 <strong>Tensorboard</strong> : 通过使用这个工具我们可以很直观的看到整个神经网络的结构、框架。 </p>
<p>今天要显示的神经网络差不多是这样子的</p>
<p><img src="/images/tensorflow/tf-4.1_1.png" width="600"></p>
<p>同时我们也可以展开看每个layer中的一些具体的结构：</p>
<p><img src="/images/tensorflow/tf-4.1_2.png" width="600"></p>
<p>好，通过阅读代码和之前的图片我们大概知道了此处是有:</p>
<blockquote>
<ul>
<li>一个输入层（<strong>inputs</strong>）</li>
<li>一个隐含层（<strong>layer</strong>）</li>
<li>一个输出层（<strong>output</strong>） </li>
</ul>
</blockquote>
<p>现在可以看看如何进行可视化.</p>
<h2 id="搭建图纸"><a href="#搭建图纸" class="headerlink" title="搭建图纸"></a>搭建图纸</h2><p>首先从 <code>Input</code> 开始：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>对于 input 我们进行如下修改： 首先，可以为<code>xs</code>指定名称为<code>x_in</code>, <code>ys</code> 同样。 :</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xs= tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>],name=<span class="string">'x_in'</span>)</span><br><span class="line">ys= tf.placeholder(tf.loat32, [<span class="keyword">None</span>, <span class="number">1</span>],name=<span class="string">'y_in'</span>)</span><br></pre></td></tr></table></figure>
<p>这里指定的名称将来会在可视化的图层 <code>inputs</code> 中显示出来</p>
<p>使用 <code>with tf.name_scope(&#39;inputs&#39;)</code> 可以将 <code>xs</code> 和 <code>ys</code> 包含进来，形成一个大的图层，图层的名字就是 <code>with tf.name_scope()</code> 方法里的参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</span><br><span class="line">    <span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">    xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">    ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>接下来开始编辑 <code>layer</code> ， 请看编辑前的程序片段 ：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    <span class="comment"># add one more layer and return the output of this layer</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b, )</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h3 id="2-1-编辑后-add-layer"><a href="#2-1-编辑后-add-layer" class="headerlink" title="2.1 编辑后 add_layer"></a>2.1 编辑后 add_layer</h3><p><font color="green"><strong>编辑后， 这里的名字应该叫 layer, 下面是编辑后的</strong></font>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    <span class="comment"># add one more layer and return the output of this layer</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer'</span>):</span><br><span class="line">        Weights= tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">        <span class="comment"># and so on...</span></span><br></pre></td></tr></table></figure>
<p>在定义完大的框架layer之后，同时也需要定义每一个’框架‘里面的小部件：(Weights biases 和 activation function): 现在现对 <code>Weights</code> 定义： 定义的方法同上，可以使用<code>tf.name.scope()</code> 方法，同时也可以在 <code>Weights</code> 中指定名称 <code>W</code> 。 即为：</p>
<p>接着继续定义 <code>biases</code> ， 定义方式同上。</p>
<p><code>activation_function</code> 的话，可以暂时忽略。因为当你自己选择用 tensorflow 中的激励函数（activation function）的时候，tensorflow会默认添加名称。 最终，layer形式如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    <span class="comment"># add one more layer and return the output of this layer</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer'</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">            Weights = tf.Variable(</span><br><span class="line">            tf.random_normal([in_size, out_size]), </span><br><span class="line">            name=<span class="string">'W'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">            biases = tf.Variable(</span><br><span class="line">            tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>, </span><br><span class="line">            name=<span class="string">'b'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">            Wx_plus_b = tf.add(</span><br><span class="line">            tf.matmul(inputs, Weights), </span><br><span class="line">            biases)</span><br><span class="line">        <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            outputs = Wx_plus_b</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = activation_function(Wx_plus_b, )</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>效果如下：（有没有看见刚才定义 layer 里面的“内部构件”呢？）</p>
<p><img src="/images/tensorflow/tf-4.1_4.png" width="600"></p>
<p>最后编辑 <code>loss</code> 部分：将 <code>with tf.name_scope()</code> 添加在 <code>loss</code> 上方，并为它起名为 <code>loss</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># the error between prediciton and real data</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>):</span><br><span class="line">    loss = tf.reduce_mean(</span><br><span class="line">    tf.reduce_sum(</span><br><span class="line">    tf.square(ys - prediction),</span><br><span class="line">    eduction_indices=[<span class="number">1</span>]</span><br><span class="line">    ))</span><br></pre></td></tr></table></figure>
<p>这句话就是“绘制” loss 了， 如下：</p>
<p><img src="/images/tensorflow/tf-4.1_5.png" width="600"></p>
<p>使用 <code>with tf.name_scope()</code> 再次对 <code>train_step</code> 部分进行编辑, 如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br></pre></td></tr></table></figure>
<p>我们需要使用 <code>tf.summary.FileWriter()</code> 将上面‘绘画’出的图保存到一个目录中，以方便后期在浏览器中可以浏览。 这个方法中的第二个参数需要使用 <code>sess.graph</code> ， 因此我们需要把这句话放在获取 <code>session</code> 的后面。 这里的 <code>graph</code> 是将前面定义的框架信息收集起来，然后放在 <code>logs/</code> 目录下面。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sess = tf.Session() <span class="comment"># get session</span></span><br><span class="line"><span class="comment"># tf.train.SummaryWriter soon be deprecated, use following</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"logs/"</span>, sess.graph)</span><br></pre></td></tr></table></figure>
<p>最后在你的terminal（终端）中 ，使用以下命令</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir logs</span><br></pre></td></tr></table></figure>
<p>同时将终端中输出的网址复制到浏览器中，便可以看到之前定义的视图框架了。</p>
<p>tensorboard 还有很多其他的参数，希望大家可以多多了解, 可以使用 tensorboard –help 查看tensorboard的详细参数 最终的全部代码在这里</p>
<h2 id="可能会遇到的问题"><a href="#可能会遇到的问题" class="headerlink" title="可能会遇到的问题"></a>可能会遇到的问题</h2><p>(1) 与 Tensorboard 兼容的浏览器是 “Google Chrome”. 使用其他的浏览器不保证所有内容都能正常显示.</p>
<p>(2) 请使用 <a href="http://localhost:6006" target="_blank" rel="external">http://localhost:6006</a>, 大多数朋友都是这个问题.</p>
<p>(3) 请确保你的 tensorboard 指令是在你的 logs 文件根目录执行的. 如果在其他目录下, 比如 <code>Desktop</code> 等, 可能不会成功看到图. 比如在下面这个目录, 你要 cd 到 <code>project</code> 这个地方执行 <code>/project &gt; tensorboard --logdir logs</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- project</span><br><span class="line">   - logs</span><br><span class="line">   model.py</span><br><span class="line">   env.py</span><br></pre></td></tr></table></figure>
<blockquote>
<p>有的朋友使用 anaconda 下的 python3.5 的虚拟环境, 如果你输入 tensorboard 的指令, 出现报错: <code>&quot;tensorboard&quot; is not recognized as an internal or external command...</code></p>
<p>解决方法的关键就是需要激活TensorFlow. 管理员模式打开 Anaconda Prompt, 输入 activate tensorflow, 接着按照上面的流程执行 tensorboard 指令.</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
<li><a href="https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf14_tensorboard/full_code.py" target="_blank" rel="external">莫烦代码 tf14_tensorboard/full_code.py</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow Speed Up Training]]></title>
      <url>http://sggo.me/2017/09/12/tensorflow/tf-3.3-A-speed-up-learning/</url>
      <content type="html"><![CDATA[<p>怎么样加速你的神经网络训练过程. Speed Up Training</p>
<a id="more"></a>
<p>学习资料:</p>
<ul>
<li>cs231n 各种 Optimizer 的对比 <a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">链接</a> (英文)</li>
<li>Tensorflow 的可用 Optimizer <a href="https://www.tensorflow.org/api_guides/python/train" target="_blank" rel="external">链接</a></li>
<li>Improving DNN (week2) Optimization Algorithm <a href="/2018/07/21/deeplearning-ai-Improving-Deep-Neural-Networks-week2/#3-Momentum">链接</a></li>
</ul>
<p>包括以下几种模式:</p>
<blockquote>
<ul>
<li>Stochastic Gradient Descent (SGD)</li>
<li>Momentum</li>
<li>AdaGrad</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
</blockquote>
<p><img src="/images/tensorflow/tf-3.4-speedup1.png" width="600"></p>
<p>越复杂的神经网络 , 越多的数据 , 我们需要在训练神经网络的过程上花费的时间也就越多. 原因很简单, 就是因为计算量太大了. 可是往往有时候为了解决复杂的问题, 复杂的结构和大数据又是不能避免的, 所以我们需要寻找一些方法, 让神经网络聪明起来, 快起来.</p>
<h2 id="1-Stochastic-Gradient-Descent-SGD"><a href="#1-Stochastic-Gradient-Descent-SGD" class="headerlink" title="1. Stochastic Gradient Descent (SGD)"></a>1. Stochastic Gradient Descent (SGD)</h2><p><img src="/images/tensorflow/tf-3.4-speedup2.png" width="600"></p>
<p>所以, 最基础的方法就是 SGD 啦, 想像红色方块是我们要训练的 data, 如果用普通的训练方法, 就需要重复不断的把整套数据放入神经网络 <strong>NN</strong> 训练, 这样消耗的计算资源会很大.</p>
<p>我们换一种思路, 如果把这些数据拆分成小批小批的, 然后再分批不断放入 NN 中计算, 这就是我们常说的 SGD 的正确打开方式了. 每次使用批数据, 虽然不能反映整体数据的情况, 不过却很大程度上加速了 NN 的训练过程, 而且也不会丢失太多准确率.如果运用上了 SGD, 你还是嫌训练速度慢, 那怎么办?</p>
<p><img src="/images/tensorflow/tf-3.4-speedup3.png" width="400"></p>
<p>没问题, 事实证明, SGD 并不是最快速的训练方法, 红色的线是 SGD, 但它到达学习目标的时间是在这些方法中最长的一种. 我们还有很多其他的途径来加速训练.</p>
<h2 id="2-Momentum-更新方法"><a href="#2-Momentum-更新方法" class="headerlink" title="2. Momentum 更新方法"></a>2. Momentum 更新方法</h2><p><img src="/images/tensorflow/tf-3.4-speedup4.png" width="650"></p>
<p>大多数其他途径是在更新神经网络参数那一步上动动手脚. 传统的参数 W 的更新是把原始的 W 累加上一个负的学习率(learning rate) 乘以校正值 (dx). 这种方法可能会让学习过程曲折无比, 看起来像 喝醉的人回家时, 摇摇晃晃走了很多弯路.</p>
<p><img src="/images/tensorflow/tf-3.4-speedup5.png" width="650"></p>
<p>所以我们把这个人从平地上放到了一个斜坡上, 只要他<strong><code>往下坡的方向走一点点, 由于向下的惯性</code></strong>, 他不自觉地就一直往下走, 走的弯路也变少了. 这就是 Momentum 参数更新. 另外一种加速方法叫 <strong>AdaGrad</strong>.</p>
<h2 id="3-AdaGrad-更新方法"><a href="#3-AdaGrad-更新方法" class="headerlink" title="3. AdaGrad 更新方法"></a>3. AdaGrad 更新方法</h2><p><img src="/images/tensorflow/tf-3.4-speedup6.png" width="650"></p>
<p>这种方法是在学习率上面动手脚, 使得每一个参数更新都会有自己与众不同的学习率, 他的作用和 momentum 类似, 不过不是给喝醉酒的人安排另一个下坡, 而是给他一双不好走路的鞋子, 使得他一摇晃着走路就脚疼, 鞋子成为了走弯路的阻力, 逼着他往前直着走. 他的数学形式是这样的. 接下来又有什么方法呢? 如果把下坡和不好走路的鞋子合并起来, 是不是更好呢? 没错, 这样我们就有了 RMSProp 更新方法.</p>
<h2 id="4-RMSProp-更新方法"><a href="#4-RMSProp-更新方法" class="headerlink" title="4. RMSProp 更新方法"></a>4. RMSProp 更新方法</h2><p><img src="/images/tensorflow/tf-3.4-speedup7.png" width="650"></p>
<p>有了 momentum 的惯性原则 , 加上 adagrad 的对错误方向的阻力, 我们就能合并成这样. 让 RMSProp同时具备他们两种方法的优势. 不过细心的同学们肯定看出来了, 似乎在 RMSProp 中少了些什么. 原来是我们还没把 Momentum合并完全, RMSProp 还缺少了 momentum 中的 这一部分. 所以, 我们在 Adam 方法中补上了这种想法.</p>
<h2 id="5-Adam-更新方法"><a href="#5-Adam-更新方法" class="headerlink" title="5. Adam 更新方法"></a>5. Adam 更新方法</h2><p><img src="/images/tensorflow/tf-3.4-speedup8.png" width="650"></p>
<p>计算m 时有 momentum 下坡的属性, 计算 v 时有 adagrad 阻力的属性, 然后再更新参数时 把 m 和 V 都考虑进去. 实验证明, 大多数时候, 使用 adam 都能又快又好的达到目标, 迅速收敛. 所以说, 在加速神经网络训练的时候, 一个下坡, 一双破鞋子, 功不可没.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow 例子3 ： 建造神经网络]]></title>
      <url>http://sggo.me/2017/09/11/tensorflow/tf-3.2-create-NN/</url>
      <content type="html"><![CDATA[<p>这次提到了怎样建造一个完整的神经网络, 包括添加 <strong>神经层</strong>, <strong>计算误差</strong>, <strong>训练步骤</strong>, 判断是否在学习.</p>
<a id="more"></a>
<h2 id="1-add-layer-功能"><a href="#1-add-layer-功能" class="headerlink" title="1. add_layer 功能"></a>1. add_layer 功能</h2><p>首先，我们导入本次所需的模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造添加一个神经层的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-n7hl2gmf.html" target="_blank" rel="external">TensorFlow随机值: tf.random_normal函数：</a> 将返回一个指定形状的张量，通过随机的正常值填充</p>
<p>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</p>
</blockquote>
<h2 id="2-导入数据"><a href="#2-导入数据" class="headerlink" title="2. 导入数据"></a>2. 导入数据</h2><p>构建所需的数据。 </p>
<p>这里的 <code>x_data</code> 和 <code>y_data</code> 并不是严格的一元二次函数的关系，因为我们多加了一个 <code>noise</code>, 这样看起来会更像真实情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>, dtype=np.float32)[:, np.newaxis]</span><br><span class="line"><span class="comment"># numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)</span></span><br><span class="line"><span class="comment"># 得到 300 个大小的一维数组， 通过 [:, np.newaxis] 行变列，变为 300 行，1 列 的二维 数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy.random.normal(loc=0.0 均值, scale=1.0 标准差, size=None 形状)</span></span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape).astype(np.float32)</span><br><span class="line"></span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用占位符定义我们所需的神经网络的输入。 `tf.placeholder()` 就是代表占位符</span></span><br><span class="line"><span class="comment"># 这里的 `None` 代表无论输入有多少都可以，因为输入只有一个特征，所以这里是 `1`。</span></span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>接下来，我们就可以开始定义神经层了。 通常神经层都包括 <strong>input</strong>输入层、<strong>hide</strong>隐藏层 和 <strong>output</strong>输出层。这里的输入层只有一个属性， 所以我们就只有一个输入；隐藏层我们可以自己假设，这里我们假设隐藏层有<strong>10</strong>个神经元； 输出层和输入层的结构是一样的，所以我们的输出层也是只有一层。 所以，我们构建的是——输入层1个、隐藏层10个、输出层1个的神经网络。</p>
<h2 id="3-搭建网络"><a href="#3-搭建网络" class="headerlink" title="3. 搭建网络"></a>3. 搭建网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义 hide隐藏层， 利用之前的 `add_layer()` 函数，这里使用 Tensorflow 自带的激励函数 `tf.nn.relu`。</span></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接着，定义输出层。此时的输入就是隐藏层的输出 —— `l1`，输入有 10 层（隐藏层的输出层），输出有 1 层。</span></span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测值 `prediction` 和 真实值的误差，对二者差的平方求和再取平均。</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),</span><br><span class="line">                     reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来，是很关键的一步，如何让机器学习提升它的准确率。</span></span><br><span class="line"><span class="comment"># `tf.train.GradientDescentOptimizer()` 中的值通常都小于 `1`，这里取的是 `0.1`，代表以 `0.1` 的效率来最小化误差 `loss`。</span></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用变量时，都要对它进行初始化，这是必不可少的。</span></span><br><span class="line">init = tf.global_variables_initializer()  <span class="comment"># 替换成这样就好</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 `Session`，并用 `Session` 来执行 `init` 初始化步骤。 </span></span><br><span class="line"><span class="comment">#（注意：在 `tensorflow` 中，只有session.run()才会执行我们定义的运算。）</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><p>下面，让机器开始学习。</p>
<p>比如这里，我们让机器学习1000次。机器学习的内容是 <code>train_step</code>, 用 <code>Session</code> 来 <code>run</code> 每一次 training 的数据，逐步提升神经网络的预测准确性。 (注意：当运算要用到 <code>placeholder</code> 时，就需要 <code>feed_dict</code> 这个字典来指定输入。)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每 50 步 我们输出一下机器学习的误差。</span></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span><br></pre></td></tr></table></figure>
<p>在电脑上运行本次代码的结果为：</p>
<p>0.0587868<br>0.00416427<br>0.00312624<br>0.00291327<br>0.00282026<br>0.0027577<br>0.00270546<br>0.00266943<br>0.00265278<br>0.00263559</p>
<p>通过上图可以看出，误差在逐渐减小，这说明机器学习是有积极的效果的</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/nn" target="_blank" rel="external">Tensorflow 提供的一些 激励函数</a></li>
<li><a href="http://www.ben-do.github.io/2016/09/15/change-shape-of-matrix-by-numpy/" target="_blank" rel="external">利用numpy的newaxis轉變矩陣的形狀</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow 例子3 ： 添加层 def add_layer()]]></title>
      <url>http://sggo.me/2017/09/09/tensorflow/tf-3.1-add-layer/</url>
      <content type="html"><![CDATA[<p>在 Tensorflow 里定义一个 添加层的函数， 可以很容易的 添加神经层, 为之后的添加省下不少时间.</p>
<a id="more"></a>
<h2 id="定义-add-layer"><a href="#定义-add-layer" class="headerlink" title="定义 add_layer()"></a>定义 add_layer()</h2><p>神经层里常见的参数通常有 <code>weights</code>、<code>biases</code> 和激励函数。</p>
<p>首先，我们需要导入 <code>tensorflow</code> 模块。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<p>然后定义添加神经层的函数 <code>def add_layer()</code>, 它有四个参数：输入值、输入的大小、输出的大小和激励函数，我们设定默认的激励函数是 <code>None</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br></pre></td></tr></table></figure>
<p>接下来，我们开始定义 <code>weights</code> 和 <code>biases</code>。</p>
<p>因为在生成初始参数时，随机变量(<strong>normal distribution</strong>)会比全部为0要好很多，所以我们这里的 <code>weights</code> 为一个 <code>in_size</code> 行, <code>out_size</code> 列的随机变量矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br></pre></td></tr></table></figure>
<p>在机器学习中，<code>biases</code> 的推荐值不为0，所以我们这里是在0向量的基础上又加了0.1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>下面，我们定义 Wx_plus_b, 即神经网络未激活的值。其中，tf.matmul() 是矩阵的乘法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br></pre></td></tr></table></figure>
<p>当 activation_function 为 <code>None</code> 时，输出就是当前的预测值 <code>Wx_plus_b</code>，不为 <code>None</code> 时，就把 <code>Wx_plus_b</code> 传到 <code>activation_function()</code> 函数中得到输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">    outputs = Wx_plus_b</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    outputs = activation_function(Wx_plus_b)</span><br></pre></td></tr></table></figure>
<p>最后，返回输出，添加一个神经层的函数 <code>def add_layer()</code> 就定义好了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/nn" target="_blank" rel="external">Tensorflow 提供的一些 激励函数</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 基本用法总结]]></title>
      <url>http://sggo.me/2017/09/08/tensorflow/tf-2.8-tensorflow-basic-summary/</url>
      <content type="html"><![CDATA[<p>tensorflow 中文文档学习 tensorflow 的基本用法。</p>
<a id="more"></a>
<p>按照文档说明，一点知识点小总结：</p>
<ol>
<li>就是 Session() 和 InteractiveSession() 的用法。后者用 Tensor.eval() 和 Operation.run() 来替代了 Session.run(). 其中更多的是用Tensor.eval()，所有的表达式都可以看作是 Tensor. </li>
<li>tf 表达式中所有的 <strong>var变量</strong> 或 <strong>constant常量</strong> 都应该是 <strong>tf</strong> 的类型。</li>
<li>只要是声明了 <strong>var变量</strong>，就得用 sess.run(tf.global_variables_initializer()) 方法来初始化才能用。</li>
</ol>
<h2 id="1-平面拟合"><a href="#1-平面拟合" class="headerlink" title="1. 平面拟合"></a>1. 平面拟合</h2><p>通过本例可以看到机器学习的一个通用过程：</p>
<ol>
<li>准备数据</li>
<li>构造模型（设置求解目标函数） </li>
<li>求解模型</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.准备数据：使用 NumPy 生成假数据(phony data), 总共 100 个点.</span></span><br><span class="line">x_data = np.float32(np.random.rand(<span class="number">2</span>, <span class="number">100</span>)) <span class="comment"># 随机输入</span></span><br><span class="line">y_data = np.dot([<span class="number">0.100</span>, <span class="number">0.200</span>], x_data) + <span class="number">0.300</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.构造一个线性模型</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">y = tf.matmul(W, x_data) + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.求解模型</span></span><br><span class="line"><span class="comment"># 设置损失函数：误差的均方差</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data))</span><br><span class="line"><span class="comment"># 选择梯度下降的方法</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 迭代的目标：最小化损失函数</span></span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############################################################</span></span><br><span class="line"><span class="comment"># 以下是用 tf 来解决上面的任务</span></span><br><span class="line"><span class="comment"># 1.初始化变量：tf 的必备步骤，主要声明了变量，就必须初始化才能用</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置tensorflow对GPU的使用按需分配</span></span><br><span class="line">config  = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line"><span class="comment"># 2.启动图 (graph)</span></span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.迭代，反复执行上面的最小化损失函数这一操作（train op）,拟合平面</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">0</span>, <span class="number">201</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> step, sess.run(W), sess.run(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到最佳拟合结果 W: [[0.100  0.200]], b: [0.300]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0 [[ 0.27467242  0.81889796]] [-0.13746099]</span><br><span class="line">20 [[ 0.1619305   0.39317462]] [ 0.18206716]</span><br><span class="line">40 [[ 0.11901411  0.25831661]] [ 0.2642329]</span><br><span class="line">60 [[ 0.10580806  0.21761954]] [ 0.28916073]</span><br><span class="line">80 [[ 0.10176832  0.20532639]] [ 0.29671678]</span><br><span class="line">100 [[ 0.10053726  0.20161074]] [ 0.29900584]</span><br><span class="line">120 [[ 0.100163    0.20048723]] [ 0.29969904]</span><br><span class="line">140 [[ 0.10004941  0.20014738]] [ 0.29990891]</span><br><span class="line">160 [[ 0.10001497  0.20004457]] [ 0.29997244]</span><br><span class="line">180 [[ 0.10000452  0.20001349]] [ 0.29999167]</span><br><span class="line">200 [[ 0.10000138  0.2000041 ]] [ 0.29999748]</span><br></pre></td></tr></table></figure>
<h2 id="2-两个数求和"><a href="#2-两个数求和" class="headerlink" title="2. 两个数求和"></a>2. 两个数求和</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input1 = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">input2 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">input3 = tf.constant(<span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">intermd = tf.add(input1, input2)</span><br><span class="line">mul = tf.multiply(input2, input3)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run([mul, intermd])  <span class="comment"># 一次执行多个op</span></span><br><span class="line">    print(result)</span><br><span class="line">    print(type(result))</span><br><span class="line">    print(type(result[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[15.0, 5.0]</span><br><span class="line">&lt;class &apos;list&apos;&gt;</span><br><span class="line">&lt;class &apos;numpy.float32&apos;&gt;</span><br></pre></td></tr></table></figure>
<h2 id="3-变量，常量"><a href="#3-变量，常量" class="headerlink" title="3. 变量，常量"></a>3. 变量，常量</h2><h3 id="3-1-tensorflow-实现计数器"><a href="#3-1-tensorflow-实现计数器" class="headerlink" title="3.1 tensorflow 实现计数器"></a>3.1 tensorflow 实现计数器</h3><p>主要是设计了在循环中调用加法实现计数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建变量，初始化为0</span></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">"counter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 op , 其作用是时 state 增加 1</span></span><br><span class="line">one = tf.constant(<span class="number">1</span>) <span class="comment"># 直接用 1 也就行了</span></span><br><span class="line">new_value = tf.add(state, <span class="number">1</span>)</span><br><span class="line">update = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图之后， 运行 update op</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 创建好图之后，变量必须经过‘初始化’ </span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># 查看state的初始化值</span></span><br><span class="line">    print(sess.run(state))</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)  <span class="comment"># 这样子每一次运行state 都还是1</span></span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<h3 id="3-2-用-tf-来实现对一组数求和，再计算平均"><a href="#3-2-用-tf-来实现对一组数求和，再计算平均" class="headerlink" title="3.2 用 tf 来实现对一组数求和，再计算平均"></a>3.2 用 tf 来实现对一组数求和，再计算平均</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">h_sum = tf.Variable(<span class="number">0.0</span>, dtype=tf.float32)</span><br><span class="line"><span class="comment"># h_vec = tf.random_normal(shape=([10]))</span></span><br><span class="line">h_vec = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line"><span class="comment"># 把 h_vec 的每个元素加到 h_sum 中，然后再除以 10 来计算平均值</span></span><br><span class="line"><span class="comment"># 待添加的数</span></span><br><span class="line">h_add = tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment"># 添加之后的值</span></span><br><span class="line">h_new = tf.add(h_sum, h_add)</span><br><span class="line"><span class="comment"># 更新 h_new 的 op</span></span><br><span class="line">update = tf.assign(h_sum, h_new)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># 查看原始值</span></span><br><span class="line">    print(<span class="string">'s_sum ='</span>, sess.run(h_sum))</span><br><span class="line">    print(<span class="string">"vec = "</span>, sess.run(h_vec))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 循环添加</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        sess.run(update, feed_dict=&#123;h_add: sess.run(h_vec[_])&#125;)</span><br><span class="line">        print(<span class="string">'h_sum ='</span>, sess.run(h_sum))</span><br><span class="line"></span><br><span class="line"><span class="comment">#     print 'the mean is ', sess.run(sess.run(h_sum) / 4)  # 这样写 4  是错误的， 必须转为 tf 变量或者常量</span></span><br><span class="line">    print(<span class="string">'the mean is '</span>, sess.run(sess.run(h_sum) / tf.constant(<span class="number">4.0</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="3-3-只用一个变量来实现计数器"><a href="#3-3-只用一个变量来实现计数器" class="headerlink" title="3.3 只用一个变量来实现计数器"></a>3.3 只用一个变量来实现计数器</h3><p>上面的计数器是 TensorFlow 官方文档的例子，但是觉得好臃肿，所以下面这个是更加简单的，只需要定义一个变量和一个 加 1 的操作（op）。通过for循环就能够实现了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果不是 assign() 重新赋值的话，每一次 sess.run()都会把 state再次初始化为 0.0</span></span><br><span class="line">state = tf.Variable(<span class="number">0.0</span>, tf.float32)</span><br><span class="line"><span class="comment"># 通过 assign 操作来改变state的值。</span></span><br><span class="line">add_op = tf.assign(state, state+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">print</span> <span class="string">'init state '</span>, sess.run(state)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> xrange(<span class="number">3</span>):</span><br><span class="line">    sess.run(add_op)</span><br><span class="line">    print(sess.run(state))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">init state  0.0</span><br><span class="line">1.0</span><br><span class="line">2.0</span><br><span class="line">3.0</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这样子和我们平时实现计数器的方法基本上就一致了。我们要重点理解的是， TensorFlow 中通过 tf.assign(ref, value) 的方式来把 value 值赋给 ref 变量。这样子，每一次循环的时候，ref 变量才不会再做定义时候的初始化操作。</p>
</blockquote>
<h2 id="4-InteractiveSession-的用法"><a href="#4-InteractiveSession-的用法" class="headerlink" title="4. InteractiveSession() 的用法"></a>4. InteractiveSession() 的用法</h2><p>InteractiveSession() 主要是避免 Session（会话）被一个变量持有</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面的两种情况是等价的</span></span><br><span class="line"><span class="keyword">with</span> tf.Session():  <span class="comment"># 不用 close()</span></span><br><span class="line">    print(c.eval())</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">print(c.eval())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3.0</span><br><span class="line">3.0</span><br></pre></td></tr></table></figure>
<h3 id="4-1-InteractiveSession-、eval、init"><a href="#4-1-InteractiveSession-、eval、init" class="headerlink" title="4.1 InteractiveSession()、eval、init"></a>4.1 InteractiveSession()、eval、init</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">c = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line">d = a + b</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="comment">###################</span></span><br><span class="line"><span class="comment"># 这样写是错误的</span></span><br><span class="line"><span class="comment"># print(a.run())</span></span><br><span class="line"><span class="comment"># print(d.run())</span></span><br><span class="line"></span><br><span class="line"><span class="comment">####################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样才是正确的</span></span><br><span class="line">print(a.eval())</span><br><span class="line">print(d.eval())</span><br><span class="line"></span><br><span class="line"><span class="comment"># run() 方法主要用来</span></span><br><span class="line">x = tf.Variable(<span class="number">1.2</span>)</span><br><span class="line"><span class="comment"># print(x.eval())  # 还没初始化，不能用</span></span><br><span class="line">x.initializer.run()  <span class="comment"># x.initializer 就是一个初始化的 op， op 才调用run() 方法</span></span><br><span class="line">print(x.eval())</span><br><span class="line"></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.0</span><br><span class="line">3.0</span><br><span class="line">1.2</span><br></pre></td></tr></table></figure>
<h3 id="4-2-使用-tf-InteractiveSession-来完成上面-求和、平均-的操作呢"><a href="#4-2-使用-tf-InteractiveSession-来完成上面-求和、平均-的操作呢" class="headerlink" title="4.2 使用 tf.InteractiveSession() 来完成上面 求和、平均 的操作呢?"></a>4.2 使用 tf.InteractiveSession() 来完成上面 求和、平均 的操作呢?</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">h_sum = tf.Variable(<span class="number">0.0</span>, dtype=tf.float32)</span><br><span class="line"><span class="comment"># h_vec = tf.random_normal(shape=([10]))</span></span><br><span class="line">h_vec = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line"><span class="comment"># 把 h_vec 的每个元素加到 h_sum 中，然后再除以 10 来计算平均值</span></span><br><span class="line"><span class="comment"># 待添加的数</span></span><br><span class="line">h_add = tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment"># 添加之后的值</span></span><br><span class="line">h_new = tf.add(h_sum, h_add)</span><br><span class="line"><span class="comment"># 更新 h_new 的 op</span></span><br><span class="line">update = tf.assign(h_sum, h_new)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">print(<span class="string">'s_sum ='</span>, h_sum.eval())</span><br><span class="line">print(<span class="string">"vec = "</span>, h_vec.eval())</span><br><span class="line">print(<span class="string">"vec = "</span>, h_vec[<span class="number">0</span>].eval())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    update.eval(feed_dict=&#123;h_add: h_vec[_].eval()&#125;)</span><br><span class="line">    print(<span class="string">'h_sum ='</span>, h_sum.eval())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">s_sum = 0.0</span><br><span class="line">vec =  [ 1.  2.  3.  4.]</span><br><span class="line">vec =  1.0</span><br><span class="line">h_sum = 1.0</span><br><span class="line">h_sum = 3.0</span><br><span class="line">h_sum = 6.0</span><br><span class="line">h_sum = 10.0</span><br></pre></td></tr></table></figure>
<h3 id="4-3-使用-feed-来对变量赋值"><a href="#4-3-使用-feed-来对变量赋值" class="headerlink" title="4.3 使用 feed 来对变量赋值"></a>4.3 使用 feed 来对变量赋值</h3><p>这些需要用到 feed 来赋值的操作可以通过 tf.placeholder() 说明，以创建占位符。</p>
<p>下面的例子中可以看出 session.run([output], …) 和 session.run(output, …) 的区别。前者输出了 output 的类型等详细信息，后者只输出简单结果。</p>
<p><strong>🌰🌰1：feed</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line">output = tf.multiply(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run([output], feed_dict=&#123;input1:[<span class="number">7.0</span>], input2:[<span class="number">2.0</span>]&#125;))</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[array([ 14.], dtype=<span class="built_in">float</span>32)]</span><br></pre></td></tr></table></figure>
<p><strong>🌰🌰2： input1:[7.0], input2:[2.0]</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(output, feed_dict=&#123;input1:[<span class="number">7.0</span>], input2:[<span class="number">2.0</span>]&#125;)</span><br><span class="line">    print(type(result))</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;type &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">[ 14.]</span><br></pre></td></tr></table></figure>
<p><strong>🌰🌰3： input1:7.0, input2:2.0</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(output, feed_dict=&#123;input1:<span class="number">7.0</span>, input2:<span class="number">2.0</span>&#125;)</span><br><span class="line">    print(type(result))</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;type &apos;numpy.float32&apos;&gt;</span><br><span class="line">14.0</span><br></pre></td></tr></table></figure>
<p><strong>🌰🌰4： [output], output</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run([output], feed_dict=&#123;input1:[<span class="number">7.0</span>, <span class="number">3.0</span>], input2:[<span class="number">2.0</span>, <span class="number">1.0</span>]&#125;))</span><br><span class="line">    print()</span><br><span class="line">    print(sess.run(output, feed_dict=&#123;input1:[<span class="number">7.0</span>, <span class="number">3.0</span>], input2:[<span class="number">2.0</span>, <span class="number">1.0</span>]&#125;))</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[array([ 14.,   3.], dtype=float32)]</span><br><span class="line"></span><br><span class="line">[ 14.   3.]</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://blog.csdn.net/jerr__y/article/details/57084008" target="_blank" rel="external">TensorFlow入门（一）基本用法</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Activation Function 激励函数]]></title>
      <url>http://sggo.me/2017/09/07/tensorflow/tf-2.6-B-activation-function/</url>
      <content type="html"><![CDATA[<p>Tensorflow 提供的一些 Activation Function <a href="https://www.tensorflow.org/api_guides/python/nn" target="_blank" rel="external">tensorflow/api_guides/python/nn</a></p>
<a id="more"></a>
<p>激励函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经系统。</p>
<p>激励函数的实质是非线性方程。 Tensorflow 的神经网络 里面处理较为复杂的问题时都会需要运用 activation function 。 </p>
<p>下面是一个 TensorFlow 搭建的 简单版神经网络 数据流图 :</p>
<p><img src="/images/tensorflow/tf-2.6-active6_1.png" width="550"></p>
<p>Layer2 展开部分，Layer1 出来的数据，再输入到 Layer2 中</p>
<p><img src="/images/tensorflow/tf-2.6-active7.jpg" width="550"></p>
<blockquote>
<p>详细介绍请前往 <a href="/2018/09/07/tensorflow-2-6-A-activation-function/">What’s Activation Function</a></p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/nn" target="_blank" rel="external">Tensorflow 提供的一些 激励函数</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[What's Activation Function]]></title>
      <url>http://sggo.me/2017/09/07/tensorflow/tf-2.6-A-activation-function/</url>
      <content type="html"><![CDATA[<p>现代神经网络中 必不可少的一个组成部分, 激励函数, activation function.</p>
<a id="more"></a>
<p>本文大部分内容转载自 <a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python /Tensorflow-Tutorial</a></p>
<h2 id="非线性方程"><a href="#非线性方程" class="headerlink" title="非线性方程"></a>非线性方程</h2><p>我们为什么要使用激励函数? 用简单的语句来概括. 就是因为, <span style="TEXT-DECORATION: underline"><strong>现实并没有我们想象的那么美好</strong></span>。激励函数也就是为了解决我们日常生活中 <strong>不能用线性方程所概括的问题</strong>. </p>
<p><img src="/images/tensorflow/tf-2.6-active1.png" alt="Linear function and Nonlinear function"></p>
<p>说到线性方程, 我们不得不提到另外一种方程, 非线性方程 (nonlinear function). 我们假设, 女生长得越漂亮, 越多男生爱. 这就可以被当做一个线性问题. 但是如果我们假设这个场景是发生在校园里. 校园里的男生数是有限的, 女生再漂亮, 也不可能会有无穷多的男生喜欢她. 所以这就变成了一个非线性问题.再说..女生也不可能是无穷漂亮的. 这个问题我们以后有时间私下讨论.</p>
<p><img src="/images/tensorflow/tf-2.6-active2.png" alt="Linear function -&gt; Activation Function"></p>
<p>然后我们就可以来讨论如何在神经网络中达成描述非线性的任务了. 我们可以把整个网络简化成这样一个式子. $Y = WX$, $W$ 就是我们要求的参数, $y$ 是预测值, $x$ 是输入值. 用这个式子, 我们很容易就能描述刚刚的那个线性问题, 因为 $W$ 求出来可以是一个固定的数. 不过这似乎并不能让这条直线变得扭起来 , 激励函数见状, 拔刀相助, 站出来说道: “让我来掰弯它!”.</p>
<h2 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h2><p><img src="/images/tensorflow/tf-2.6-active3.png" alt="Activation Function"></p>
<blockquote>
<p>这里的 AF 就是指激励函数. 激励函数拿出自己最擅长的”掰弯利器”, 套在原函数上用力一扭, 原来的 $WX$ 结果就被扭弯了.</p>
<p>其实这个 AF, 掰弯利器, 也不是什么触不可及的东西. 它其实就是另外一个非线性函数. 比如说<code>relu, sigmoid, tanh</code>. 将这些掰弯利器嵌套在原有的结果之上, 强行把原有的线性结果给扭曲了. 使得输出结果 $y$ 也有了非线性的特征. 举个例子, 比如我使用了 <code>relu</code> 这个掰弯利器, 如果此时 $Wx$ 的结果是 1, $y$ 还将是 1, 不过 $WX$ 为 -1 的时候, $y$ 不再是 -1, 而会是 0.</p>
</blockquote>
<p><img src="/images/tensorflow/tf-2.6-active5.jpg" width="550"></p>
<blockquote>
<p>你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在 backpropagation 误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去.</p>
</blockquote>
<h2 id="常用选择"><a href="#常用选择" class="headerlink" title="常用选择"></a>常用选择</h2><p><img src="/images/tensorflow/tf-2.6-active4.png" alt="Activation Function"></p>
<p>想要恰当使用这些激励函数, 还是有窍门的. 比如当你的神经网络层只有两三层, 不是很多的时候, 对于隐藏层, 使用任意的激励函数, 随便掰弯是可以的, 不会有特别大的影响. 不过, 当你使用特别多层的神经网络, 在掰弯的时候, 玩玩不得随意选择利器. 因为这会涉及到梯度爆炸, 梯度消失的问题. 因为时间的关系, 我们可能会在以后来具体谈谈这个问题.</p>
<p>最后我们说说, 在具体的例子中, 我们默认首选的激励函数是哪些. 在少量层结构中, 我们可以尝试很多种不同的激励函数. 在卷积神经网络 Convolutional neural networks 的卷积层中, 推荐的激励函数是 <code>relu</code>. 在循环神经网络中 Recurrent neural networks, 推荐的是 <code>tanh</code> 或者是 <code>relu</code> (这个具体怎么选, 我会在以后 循环神经网络 的介绍中在详细讲解).</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial" target="_blank" rel="external">Tensorflow 提供的一些 激励函数</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Placeholder 传入值]]></title>
      <url>http://sggo.me/2017/08/30/tensorflow/tf-2.5-placeholde/</url>
      <content type="html"><![CDATA[<p>placeholder 是 Tensorflow 中的占位符，暂时储存变量.</p>
<a id="more"></a>
<p>Tensorflow 如果想要从外部传入data, 那就需要用到 <code>tf.placeholder()</code>, 然后以这种形式传输数据 <code>sess.run(**, feed_dict={input: **})</code>.</p>
<p>举个🌰:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式</span></span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mul = multiply 是将input1和input2 做乘法运算，并输出为 output </span></span><br><span class="line">ouput = tf.multiply(input1, input2)</span><br></pre></td></tr></table></figure>
<p>接下来, 传值的工作交给了 <code>sess.run()</code> , 需要传入的值放在了 <code>feed_dict={}</code> 并一一对应每一个 <code>input</code>. <code>placeholder</code> 与 <code>feed_dict={}</code> 是绑定在一起出现的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(ouput, feed_dict=&#123;input1: [<span class="number">7.</span>], input2: [<span class="number">2.</span>]&#125;))</span><br><span class="line"><span class="comment"># [ 14.]</span></span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Variable 变量]]></title>
      <url>http://sggo.me/2017/08/29/tensorflow/tf-2.4-variable/</url>
      <content type="html"><![CDATA[<p>在 Tensorflow 中使用 Variable。 在 Tensorflow 中，定义了某字符串是变量，它才是变量，这一点是与 Python 所不同的。</p>
<a id="more"></a>
<p>定义语法： <code>state = tf.Variable()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">'counter'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义常量 one</span></span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义加法步骤 (注: 此步并没有直接计算)</span></span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 State 更新成 new_value</span></span><br><span class="line">update = tf.assign(state, new_value)</span><br></pre></td></tr></table></figure>
<p>如果你在 Tensorflow 中设定了变量，那么初始化变量是最重要的！！所以定义了变量以后, 一定要定义 <code>init = tf.initialize_all_variables()</code>.</p>
<p>到这里变量还是没有被激活，需要再在 <code>sess</code> 里, <code>sess.run(init)</code> , 激活 <code>init</code> 这一步.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果定义 Variable, 就一定要 initialize</span></span><br><span class="line"><span class="comment"># init = tf.initialize_all_variables() # tf 马上就要废弃这种写法</span></span><br><span class="line">init = tf.global_variables_initializer()  <span class="comment"># 替换成这样就好</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 使用 Session</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(new_value))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update) <span class="comment"># 相当于运行了一遍 tf.assign(state, new_value+100)， 因为这是 update</span></span><br><span class="line">        print(sess.run(state))</span><br><span class="line">    print(<span class="string">"Sess Hello !"</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：直接 print(state) 不起作用！！</p>
<p>一定要把 <code>sess</code> 的指针指向 <code>state</code> 再进行 <code>print</code> 才能得到想要的结果！</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial" target="_blank" rel="external">新版可视化教学代码</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Session 会话控制]]></title>
      <url>http://sggo.me/2017/08/28/tensorflow/tf-2.3-session/</url>
      <content type="html"><![CDATA[<p>Session 是 Tensorflow 为了控制,和输出文件的执行语句. 运行 session.run() 可以获得你要得知的运算结果.</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">"a"</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">"b"</span>) <span class="comment"># a, b 定义为 2 个常量 向量</span></span><br><span class="line"></span><br><span class="line">result = a + b</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">sess.run(result) <span class="comment"># array([ 3.,  5.], dtype=float32)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>要输出相加得到的结果，不能简单地直接输出 result，而需要先生成一个 Session，并通过这个 Session 来计算结果。</p>
</blockquote>
<p>🌰🌰🌰</p>
<p>  这次需要加载 Tensorflow ，然后建立两个 <code>matrix</code> , 输出两个 <code>matrix</code> 矩阵相乘的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># create two matrixes</span></span><br><span class="line"></span><br><span class="line">matrix1 = tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">matrix2 = tf.constant([[<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">2</span>]])</span><br><span class="line">product = tf.matmul(matrix1,matrix2)</span><br></pre></td></tr></table></figure>
<p>我们会要使用 <code>Session</code> 来激活 <code>product</code> 并得到计算结果. 有两种形式使用会话控制 <code>Session</code> 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># method 1</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(product)</span><br><span class="line">print(result) <span class="comment"># [[12]]</span></span><br><span class="line">sess.close()</span><br><span class="line"><span class="comment"># [[12]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result2 = sess.run(product)</span><br><span class="line">    print(result2)</span><br><span class="line"><span class="comment"># [[12]]</span></span><br></pre></td></tr></table></figure>
<p>以上就是我们今天所学的两种 <code>Session</code> 打开模式。</p>
<p>让我们学习下一节 — Tensorflow 中的 Variable。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial" target="_blank" rel="external">新版可视化教学代码</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow 入门例子]]></title>
      <url>http://sggo.me/2017/08/27/tensorflow/tf-2.2-example/</url>
      <content type="html"><![CDATA[<p>Tensorflow 是非常重视结构的, 我们建立好了神经网络的结构, 才能将数字放进去, 运行这个结构.</p>
<a id="more"></a>
<p>这个例子简单的阐述了 tensorflow 当中如何用代码来运行我们搭建的结构.</p>
<h2 id="1-创建数据"><a href="#1-创建数据" class="headerlink" title="1. 创建数据"></a>1. 创建数据</h2><p>首先, 我们这次需要加载 tensorflow 和 numpy 两个模块, 并且使用 numpy 来创建我们的数据.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># create data</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>).astype(np.float32)</span><br><span class="line">y_data = x_data*<span class="number">0.1</span> + <span class="number">0.3</span></span><br></pre></td></tr></table></figure>
<p>接着, 我们用 <code>tf.Variable</code> 来创建描述 <code>y</code> 的参数. 我们可以把 <code>y_data = x_data*0.1 + 0.3</code> 想象成 <code>y=Weights * x + biases</code>, 然后神经网络也就是学着把 Weights 变成 0.1, biases 变成 0.3.</p>
<h2 id="2-搭建模型"><a href="#2-搭建模型" class="headerlink" title="2. 搭建模型"></a>2. 搭建模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Weights = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">y = Weights*x_data + biases</span><br></pre></td></tr></table></figure>
<h2 id="3-计算误差"><a href="#3-计算误差" class="headerlink" title="3. 计算误差"></a>3. 计算误差</h2><p>接着就是计算 <code>y</code> 和 <code>y_data</code> 的误差:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.square(y-y_data))</span><br></pre></td></tr></table></figure>
<h2 id="4-传播误差"><a href="#4-传播误差" class="headerlink" title="4. 传播误差"></a>4. 传播误差</h2><p>反向传递误差的工作就教给 <code>optimizer</code> 了, 我们使用的误差传递方法是梯度下降法: Gradient Descent </p>
<p>然后我们使用 <code>optimizer</code> 来进行参数的更新.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure>
<h2 id="5-训练"><a href="#5-训练" class="headerlink" title="5. 训练"></a>5. 训练</h2><p>到目前为止, 我们只是建立了神经网络的结构, 还没有使用这个结构. </p>
<p>在使用这个结构之前, 我们必须先初始化所有之前定义的 <code>Variable</code>,  所以这一步是很重要的!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init = tf.global_variables_initializer()  <span class="comment"># 替换成这样就好</span></span><br></pre></td></tr></table></figure>
<p>接着,我们再创建会话 <code>Session</code>. 我们会在下一节中详细讲解 Session. 我们用 <code>Session</code> 来执行 <code>init</code> 初始化步骤. 并且, 用 <code>Session</code> 来 <code>run</code> 每一次 training 的数据. 逐步提升神经网络的预测准确性.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)          <span class="comment"># Very important</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">201</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(step, sess.run(Weights), sess.run(biases))</span><br></pre></td></tr></table></figure>
<h2 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6. Reference"></a>6. Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">莫烦Python</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow 处理结构]]></title>
      <url>http://sggo.me/2017/08/25/tensorflow/tf-2.1-structure/</url>
      <content type="html"><![CDATA[<p>Tensorflow 首先要定义神经网络的结构, 然后再把数据放入结构当中去运算 和 training.</p>
<a id="more"></a>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p><img src="/images/tensorflow/tf-1-why.gif" width="400"></p>
<p>因为 TensorFlow 是采用数据流图（<strong>data　flow　graphs</strong>）来计算, 所以首先我们得创建一个<strong>数据流图</strong>, 然后再将我们的数据（数据以张量(<strong>tensor</strong>)的形式存在）放在数据流图中计算. </p>
<blockquote>
<ul>
<li>Nodes 在图中表示数学操作</li>
<li>Edges 在图中则表示在节点间相互联系的多维数据数组，即张量（tensor）</li>
</ul>
</blockquote>
<p>训练模型时 <strong>tensor</strong> 会不断的从数据流图中的一个节点 <strong>flow</strong> 到另一节点, 这就是 TensorFlow 名字的由来.</p>
<h2 id="Tensor-张量意义"><a href="#Tensor-张量意义" class="headerlink" title="Tensor 张量意义"></a>Tensor 张量意义</h2><p><strong>张量（Tensor)</strong>: 张量有多种. </p>
<ul>
<li>零阶张量为 纯量或标量 (scalar) 也就是一个数值. 比如 <code>[1]</code></li>
<li>一阶张量为 向量 (vector), 比如 一维的 <code>[1, 2, 3]</code></li>
<li>二阶张量为 矩阵 (matrix), 比如 二维的 <code>[[1, 2, 3],[4, 5, 6],[7, 8, 9]]</code></li>
</ul>
<blockquote>
<p>以此类推, 还有 三阶 三维的 …</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Naive Bayes * 文本分类]]></title>
      <url>http://sggo.me/2017/08/23/ml/4-naive-bayes-2/</url>
      <content type="html"><![CDATA[<p>朴素贝叶斯，适用于新闻分类问题</p>
<p>$$<br>p(x)p(y|x) = p(y)p(x|y)<br>$$</p>
<a id="more"></a>
<h2 id="1-贝叶斯理论简单回顾"><a href="#1-贝叶斯理论简单回顾" class="headerlink" title="1. 贝叶斯理论简单回顾"></a>1. 贝叶斯理论简单回顾</h2><p>在我们有一大堆样本（包含特征和类别）的时候，我们非常容易通过统计得到  $p(特征|类别)$.<br>大家又都很熟悉下述公式：</p>
<p>$$<br>p(x)p(y|x) = p(y)p(x|y)<br>$$</p>
<p>所以做一个小小的变换</p>
<p>$$<br>p(特征)p(类别|特征) = p(类别)p(特征|类别)<br>$$</p>
<p>$$<br>p(类别|特征) = \frac{p(类别)p(特征|类别)}{p(特征)}<br>$$</p>
<h2 id="2-独立假设"><a href="#2-独立假设" class="headerlink" title="2. 独立假设"></a>2. 独立假设</h2><p>看起来很简单，但实际上，你的特征可能是很多维的</p>
<p>$$<br>p(features|class) = p({f_0, f_1, \ldots ,f_n}|c)<br>$$</p>
<p>就算是2个维度吧，可以简单写成</p>
<p>$$<br>p({f_0, f_1}|c) = p(f_1|c, f_0)p(f_0|c)<br>$$</p>
<p>加一个牛逼的假设：特征之间是独立的</p>
<p>$$<br>p({f_0, f_1}|c) = p(f_1|c)p(f_0|c)<br>$$</p>
<p>其实也就是：</p>
<p>$$<br>p({f_0, f_1, \ldots, f_n}|c) = \Pi^n_i p(f_i|c)<br>$$</p>
<h2 id="3-贝叶斯分类器"><a href="#3-贝叶斯分类器" class="headerlink" title="3. 贝叶斯分类器"></a>3. 贝叶斯分类器</h2><p>其实我们就是对每个类别计算一个概率 $p(ci)$ ，然后再计算所有特征的条件概率 $p(f_j|c_i)$ ，那么分类的时候我们就是依据贝叶斯找一个最可能的类别：</p>
<p>$$<br>p(class_i|{f_0, f_1, \ldots, f_n})= \frac{p(class_i)}{p({f_0, f_1, \ldots, f_n})} \Pi^n_j p(f_j|c_i)<br>$$</p>
<h2 id="4-文本分类问题"><a href="#4-文本分类问题" class="headerlink" title="4. 文本分类问题"></a>4. 文本分类问题</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow Why?]]></title>
      <url>http://sggo.me/2017/08/22/tensorflow/tf-1.1-why/</url>
      <content type="html"><![CDATA[<p>2015.10 开源的 TensorFlow 是由 Google brain 的工程师开发出来，用于机器学习和神经网络方面的研究</p>
<a id="more"></a>
<p>TensorFlow 是一款神经网络的 Python 外部的结构包, 也是一个采用数据流图来进行数值计算的开源软件库.</p>
<p><img src="/images/tensorflow/tf-1-why.gif" alt="TensorFlow 节点表示某种抽象的计算，边表示节点之间相互联系的张量"></p>
<h2 id="TensorFlow-Why"><a href="#TensorFlow-Why" class="headerlink" title="TensorFlow Why"></a>TensorFlow Why</h2><p>TensorFlow 擅长的任务就是训练深度神经网络. 使用它我们就可以大大降低深度学习的开发成本和开发难度;</p>
<p>TensorFlow 在众多深度学习框架中脱颖而出，在Github上获得了最多的Star量;</p>
<p>TensorFlow 支持各种异构的平台，支持多CPU/GPU，服务器，移动设备，具有良好的跨平台的特性;</p>
<p>TensorFlow 架构灵活，能够支持各种网络模型，具有良好的通用性;</p>
<p>TensorFlow 架构具有良好的可扩展性，对OP的扩展支持，Kernel特化方面表现出众.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow.org</a></li>
<li><a href="https://www.tensorflow.org/get_started/" target="_blank" rel="external">tensorflow.org get_started</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 6 - 更多论文 (感谢 PaperWeekly)]]></title>
      <url>http://sggo.me/2017/08/16/chatbot/chatbot-research6/</url>
      <content type="html"><![CDATA[<p>Chatbot 更多 Paper 论文和参考资料 (感谢 PaperWeekly)</p>
<a id="more"></a>
<h2 id="Paper-1"><a href="#Paper-1" class="headerlink" title="Paper 1"></a>Paper 1</h2><p>《A Neural Conversational Model》</p>
<p>代码: <a href="https://github.com/Conchylicultor/DeepQA" target="_blank" rel="external">https://github.com/Conchylicultor/DeepQA</a>  2128 star</p>
<ul>
<li>作者: 来自 Google Brain，毕业于 UC Berkeley 的 Oriol Vinyals博士</li>
<li>对比 cleverbot (第二代基于检索的聊天机器人)，部分回答 更智能。</li>
<li>如何客观地评价生成的效果? 有一些问题没有标准答案 来说，自动评价 VS 用户评价</li>
</ul>
<blockquote>
<p>作者对一些评估方法提出了一些自己的思考方式</p>
</blockquote>
<p>DeepQA</p>
<ol>
<li>chatbot 训练的部分</li>
<li>chatbot_website 服务的部分，网页端</li>
</ol>
<p><a href="https://github.com/Conchylicultor/DeepQA/tree/master/chatbot" target="_blank" rel="external">DeepQA/chatbot/</a> 这几个文件比较重要</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> chatbot.corpus.cornelldata <span class="keyword">import</span> CornellData</span><br><span class="line"><span class="keyword">from</span> chatbot.corpus.opensubsdata <span class="keyword">import</span> OpensubsData</span><br><span class="line"><span class="keyword">from</span> chatbot.corpus.scotusdata <span class="keyword">import</span> ScotusData</span><br><span class="line"><span class="keyword">from</span> chatbot.corpus.ubuntudata <span class="keyword">import</span> UbuntuData</span><br><span class="line"><span class="keyword">from</span> chatbot.corpus.lightweightdata <span class="keyword">import</span> LightweightData</span><br></pre></td></tr></table></figure>
<p>语料太大的情况下，做一些采样或者层次化的 softmax 可行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="number">0</span> &lt; self.args.softmaxSamples &lt; self.textData.getVocabularySize():</span><br><span class="line">    outputProjection = ProjectionOp(</span><br><span class="line">        (self.textData.getVocabularySize(), self.args.hiddenSize),</span><br><span class="line">        scope=<span class="string">'softmax_projection'</span>,</span><br><span class="line">        dtype=self.dtype</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sampledSoftmax</span><span class="params">(labels, inputs)</span>:</span></span><br></pre></td></tr></table></figure>
<h2 id="Paper-2"><a href="#Paper-2" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>《A Diversity-Promoting Objective Function for Neural Conversation Models》</p>
<blockquote>
<ul>
<li>作者： <a href="https://nlp.stanford.edu/~bdlijiwei/Myself.html" target="_blank" rel="external">Jiwei Li</a>， <a href="https://github.com/jiweil" target="_blank" rel="external">Jiwei Li’s Github</a></li>
<li><p>关于《A Persona-Based Neural Conversation Model》的pre-paper  Seq2seq 容易产出”呵呵”，”都可以”，”我不知道”这种 safe 但无意义的回答</p>
</li>
<li><p>NLG 问题，常使用 <strong>MLE</strong> 作为目标函数，产出的结果通畅，但 diversity 差，可考虑 decoder 产出 n-best, 再 rank </p>
</li>
<li>提出 Maximum Mutual Information(MMI) 作为目标函数， 有 MMI-antiLM 和MMI-bidi 2种</li>
</ul>
</blockquote>
<h2 id="Paper-3"><a href="#Paper-3" class="headerlink" title="Paper 3"></a>Paper 3</h2><p>《A Persona-Based Neural Conversation Model》</p>
<blockquote>
<ul>
<li>Speaker Model 和 Speaker-Addressee Model</li>
<li>作者： <a href="https://nlp.stanford.edu/~bdlijiwei/Myself.html" target="_blank" rel="external">Jiwei Li</a>， <a href="https://github.com/jiweil" target="_blank" rel="external">Jiwei Li’s Github</a></li>
<li>代码： <a href="https://github.com/jiweil/Neural-Dialogue-Generation" target="_blank" rel="external">https://github.com/jiweil/Neural-Dialogue-Generation</a>   </li>
<li><p>解决多轮对话回答不一致问题</p>
</li>
<li><p>Model 中融入 user identity (比如背景信息、用户画像，年龄等信息)，构建出个性化的 seq2seq 模型，为不同的 user， 以及 同一个user 对不同的对象对话生成不同风格的 response</p>
</li>
</ul>
</blockquote>
<p><img src="/images/chatbot/bot_A3.jpg" width="800"></p>
<h2 id="Paper-4"><a href="#Paper-4" class="headerlink" title="Paper 4"></a>Paper 4</h2><p>《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》</p>
<blockquote>
<ul>
<li>代码: <a href="https://github.com/julianser/hed-dlg-truncated" target="_blank" rel="external">https://github.com/julianser/hed-dlg-truncated</a></li>
<li>作者: 来自蒙特利尔大学和Maluuba公司</li>
<li>意在解决语言模型生成部分存在的问题</li>
<li>整个 seq2seq 框架中 decoder生成部分的问题，不仅是 bot领域对话生成的问题，都可以尝试用这个方式。</li>
</ul>
</blockquote>
<p><img src="/images/chatbot/bot_A4.jpg" width="700"></p>
<h2 id="Paper-5"><a href="#Paper-5" class="headerlink" title="Paper 5"></a>Paper 5</h2><p>更多论文和参考资料(感谢PaperWeekly)   </p>
<blockquote>
<ul>
<li><p>《End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning》<br><a href="http://rsarxiv.github.io/2016/07/17/End-to-end-LSTM-based-dialog-control-optimized-with-supervised-and-reinforcement-learning-PaperWeekly/" target="_blank" rel="external">http://rsarxiv.github.io/2016/07/17/End-to-end-LSTM-based-dialog-control-optimized-with-supervised-and-reinforcement-learning-PaperWeekly/</a></p>
</li>
<li><p>《A Network-based End-to-End Trainable Task-oriented Dialogue System》<br><a href="http://rsarxiv.github.io/2016/07/12/A-Network-based-End-to-End-Trainable-Task-oriented-Dialogue-System-PaperWeekly" target="_blank" rel="external">http://rsarxiv.github.io/2016/07/12/A-Network-based-End-to-End-Trainable-Task-oriented-Dialogue-System-PaperWeekly</a></p>
</li>
<li><p>《A Neural Network Approach to Context-Sensitive Generation of Conversational Responses》<br><a href="http://rsarxiv.github.io/2016/07/15/A-Neural-Network-Approach-to-Context-Sensitive-Generation-of-Conversational-Responses-PaperWeekly/" target="_blank" rel="external">http://rsarxiv.github.io/2016/07/15/A-Neural-Network-Approach-to-Context-Sensitive-Generation-of-Conversational-Responses-PaperWeekly/</a></p>
</li>
<li>Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation<br><a href="http://rsarxiv.github.io/2016/07/09/Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation-PaperWeekly/" target="_blank" rel="external">http://rsarxiv.github.io/2016/07/09/Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation-PaperWeekly/</a></li>
</ul>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/Conchylicultor/DeepQA" target="_blank" rel="external">2015 DeepQA</a></li>
<li><a href="https://www.jianshu.com/p/4fb194d143cf" target="_blank" rel="external">聊天机器人深度学习应用-part1：引言</a></li>
<li><a href="https://github.com/dennybritz/chatbot-retrieval/" target="_blank" rel="external">dennybritz/chatbot-retrieval</a></li>
<li>更多论文和参考资料(感谢PaperWeekly)</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 5 - 基于深度学习的检索聊天机器人]]></title>
      <url>http://sggo.me/2017/08/15/chatbot/chatbot-research5/</url>
      <content type="html"><![CDATA[<p>介绍基于检索式机器人。检索式架构有预定好的语料答复库。</p>
<p>检索式模型的输入是上下文潜在的答复。模型输出对这些答复的打分，可以选择最高分的答案作为回复。</p>
<a id="more"></a>
<blockquote>
<p>既然生成式的模型更弹性，也不需要预定义的语料，为何不选择它呢？</p>
<p>生成式模型的问题就是实际使用起来并不能好好工作，至少现在是。因为答复比较自由，容易犯语法错误和不相关、不合逻辑的答案，并且需要大量的数据且很难做优化。</p>
<p>大量的生产系统上还是采用 检索模型 或者 <code>检索模型</code> 和 <code>生成模型</code> 结合的方式。</p>
<ul>
<li>例如 google 的 <a href="https://arxiv.org/abs/1606.04870" target="_blank" rel="external">smart reply</a>。</li>
</ul>
<p>生成模型是研究的热门领域，但是我们还没到应用它的程度。如果你想要做一个聊天机器人，最好还是选用检索式模型</p>
</blockquote>
<p><strong>更聪明的聊天机器人 ：</strong></p>
<ol>
<li>生成式模型 VS 检索匹配模型 </li>
<li>Chatterbot的进化: 深度学习提高智能度</li>
</ol>
<p><strong>模型构建 ：</strong> </p>
<ol>
<li>问题的分析与转化</li>
<li>数据集与样本构造方法 </li>
<li>网络结构的构建 </li>
<li>模型的评估 </li>
<li>代码实现与解析</li>
</ol>
<h2 id="1-聊天机器人"><a href="#1-聊天机器人" class="headerlink" title="1. 聊天机器人"></a>1. 聊天机器人</h2><p><img src="/images/chatbot/chatbot-5_1.jpg" width="700"></p>
<h3 id="1-1-基于检索的-chatbot"><a href="#1-1-基于检索的-chatbot" class="headerlink" title="1.1 基于检索的 chatbot"></a>1.1 基于检索的 chatbot</h3><ul>
<li>根据 input 和 context，结合知识库的算法得到合适回复    </li>
<li>从一个固定的数据集中找到合适的内容作为回复</li>
<li>检索和匹配的方式有很多种</li>
<li>数据和匹配方法对质量有很大影响</li>
</ul>
<h3 id="1-2-基于生成模型的chatbot"><a href="#1-2-基于生成模型的chatbot" class="headerlink" title="1.2 基于生成模型的chatbot"></a>1.2 基于生成模型的chatbot</h3><ul>
<li>典型的是 seq2seq 的方法</li>
<li>生成的结果需要考虑通畅度和准确度</li>
</ul>
<blockquote>
<p>以前者为主(可控度高)，后者为辅</p>
</blockquote>
<h2 id="2-回顾-chatterbot"><a href="#2-回顾-chatterbot" class="headerlink" title="2. 回顾 chatterbot"></a>2. 回顾 chatterbot</h2><p><img src="/images/chatbot/chatbot-5_2.png" width="600"></p>
<h3 id="2-1-chatterbot-的问题"><a href="#2-1-chatterbot-的问题" class="headerlink" title="2.1 chatterbot 的问题"></a>2.1 chatterbot 的问题</h3><p><strong>应答模式的匹配方式太粗暴</strong></p>
<blockquote>
<ul>
<li>编辑距离无法捕获深层语义信息   </li>
<li>核心词 + word2vec 无法捕获整句话语义   </li>
<li>LSTM 等 RNN模型 能捕获序列信息<br>…<br>用深度学习来提高匹配阶段准确率!!</li>
</ul>
</blockquote>
<p>心得 :</p>
<blockquote>
<p>Open Domain 的 chatbot 很难做，话题太广，因为无法预知用户会问到什么问题. </p>
<p>你想吃什么 ： 随便<br>你感觉怎么样 : 还好</p>
<p>没问题其实</p>
<p>所以针对一个 Closed Domain + 检索 + 知识库，还应该可以做一个可以用的机器人.</p>
</blockquote>
<h3 id="2-2-应该怎么做"><a href="#2-2-应该怎么做" class="headerlink" title="2.2 应该怎么做"></a>2.2 应该怎么做</h3><p><strong>匹配本身是一个模糊的场景</strong></p>
<blockquote>
<p>转成排序问题</p>
</blockquote>
<p><strong>排序问题怎么处理?</strong>  </p>
<blockquote>
<p>转成能输出概率的01分类问题</p>
<p><strong>Q1 -&gt; { R1: 0.8, R2: 0.1, R3: 0.05, R4: 0.2 }</strong></p>
<p>Query &lt;&gt; Response</p>
</blockquote>
<p><strong>数据构建?</strong></p>
<blockquote>
<p>我们需要正样本(正确的回答) 和 负样本(不对的回答)</p>
<p><strong>{ 正样本 : Q1-R1 1 }, { 负样本 : Q1-R3 0 }</strong></p>
</blockquote>
<p><strong>Loss function?</strong></p>
<blockquote>
<p>回忆一下 logistic regression</p>
</blockquote>
<p><strong>心得 :</strong></p>
<blockquote>
<p>定义问题 和 解决问题 很重要</p>
<p>有一个问题，可以转换为 机器学习 或 深度学习 可以解决的问题，这非常重要。</p>
</blockquote>
<h2 id="3-用深度学习来完成"><a href="#3-用深度学习来完成" class="headerlink" title="3. 用深度学习来完成"></a>3. 用深度学习来完成</h2><p><img src="/images/chatbot/chatbot-5_3.png" width="700"></p>
<blockquote>
<p>不管网络结构如何，你抓住最好的 loss function</p>
<p>100 W 样本，50W+， 50W-， 这样的数据集</p>
<p>我们拿来做训练，这样的网络结构，不管如何搭建，都不要太担心，你就抓住 loss function，你的损失函数由 c 和 r 决定的。 c 和 r 是由于上面的结构产生的，所以我们就可以用 BPTT 做训练了.</p>
<p>Query 和 Respon 都是我们分词后用的 word embmming，灌入 RNN 中，我们把 LSTM 顺着捕捉下来，当做问题和回答，两个捕捉的信息来做匹配，我找了个参数 M，来做 c 和 r 的匹配。 M 是一定的，匹配程度和方式一致.</p>
<p>M 初始化的时候可以由 radom 是生成. M 之后是可以通过训练做更新的.</p>
<p><a href="http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/" target="_blank" rel="external">2016 Google Brain deep-learning-for-chatbots-2-retrieval-based-model-tensorflow, wildml blog</a></p>
</blockquote>
<h2 id="4-数据-Ubuntu-对话语料库"><a href="#4-数据-Ubuntu-对话语料库" class="headerlink" title="4. 数据 - Ubuntu 对话语料库"></a>4. 数据 - Ubuntu 对话语料库</h2><p>我们将使用Ubuntu对话数据集（<a href="https://arxiv.org/abs/1506.08909" target="_blank" rel="external">论文来源</a> <a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator" target="_blank" rel="external">github地址</a>）。这个数据集（Ubuntu Dialog Corpus, UDC）是目前最大的公开对话数据集之一，它是来自 Ubuntu 的 IRC网络 上的对话日志。<a href="https://arxiv.org/abs/1506.08909" target="_blank" rel="external">这篇论文</a>介绍了该数据集生成的具体细节。下面简单介绍一下数据的格式。</p>
<p>训练数据有 100W 条实例，其中一半是正例（label为1），一半是负例（label为0，负例为随机生成）。每条实例包括一段上下文信息（context），即Query；和一段可能的回复内容，即Response；Label为1表示该Response确实是Query的回复，Label为0则表示不是。下面是数据示例：</p>
<p>数据集生成脚本已用<a href="http://www.nltk.org/" target="_blank" rel="external">NLTK</a>做了一系列的语料处理包括（<a href="http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize" target="_blank" rel="external">分词</a>，<a href="http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball" target="_blank" rel="external">stemmed</a>，<a href="http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet" target="_blank" rel="external">lemmatized</a>）等文本预处理步骤；</p>
<blockquote>
<ul>
<li>使用了NER技术，将文本中的实体，如 姓名、地点、组织、URL 等替换成特殊字符。</li>
<li>这些预处理不是严格必要的，但是能改善一些系统的表现。</li>
<li>语料的上下文平均有86个词语，答复平均有17个词语长。有人做了语料的统计分析：<a href="https://github.com/dennybritz/chatbot-retrieval/blob/master/notebooks/Data%20Exploration.ipynb" target="_blank" rel="external">data analysis</a></li>
<li>数据集也包括了 <strong>Test / Validation sets</strong>，但这两部分的数据和训练数据在格式上不太一样。</li>
<li>在 <strong>Test / Validation sets</strong> 中，对于每一条实例，有一个正例和九个负例数据（也称为干扰数据）。</li>
<li>模型的目标在于给正例的得分尽可能的高，而给负例的得分尽可能的低。下面是数据示例：</li>
</ul>
</blockquote>
<h3 id="4-1-Train-sets"><a href="#4-1-Train-sets" class="headerlink" title="4.1 Train sets"></a>4.1 Train sets</h3><p><img src="/images/chatbot/chatbot-5_4.png" width="900"></p>
<h3 id="4-2-Test-Validation-sets"><a href="#4-2-Test-Validation-sets" class="headerlink" title="4.2 Test / Validation sets"></a>4.2 Test / Validation sets</h3><ul>
<li>每个样本，有一个正例和九个负例数据 (也称为干扰数据)。</li>
<li>建模的目标在于给正例的得分尽可能的高，而给负例的得分尽可能的低。(有点类似分类任务)</li>
<li>语料做过分词、stemmed、lemmatized 等文本预处理。</li>
</ul>
<p>NLTK stemmed</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> PorterStemmer</span><br><span class="line">p = PorterStemmer()</span><br><span class="line">p.stem(<span class="string">'wenting'</span>)</span><br></pre></td></tr></table></figure>
<p>NLTK Lemma</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line">wordnet_lemmatizer = WordNetLemmatizer()</span><br><span class="line">wordnet_lemmatizer.lemmatize(‘dogs’)</span><br><span class="line">u’dog’</span><br></pre></td></tr></table></figure>
<ul>
<li>用 NER(命名实体识别) 将文本中的 <strong>实体</strong>，如姓名、地点、组织、URL等 替换成特殊字符</li>
</ul>
<p><img src="/images/chatbot/chatbot-5_5.png" width="900"></p>
<h2 id="5-评估准则-BASELINE"><a href="#5-评估准则-BASELINE" class="headerlink" title="5. 评估准则 BASELINE"></a>5. 评估准则 BASELINE</h2><p><strong>Recall@K</strong></p>
<blockquote>
<ul>
<li>常见的 Kaggle 比赛评判准则</li>
<li>经模型对候选的 response 排序后，前 k 个候选中 存在正例数据(正确的那个)的占比。<br> 让 K=10，这就得到一个 100% 的召回率，因最多就 10 个备选。如果 K=1，模型只一次机会选中正确答案。</li>
<li>K 值 越大，指标值越高，对模型性能的要求越松。</li>
</ul>
</blockquote>
<p><strong>9个干扰项目怎么选出来</strong></p>
<blockquote>
<p>这个数据集里是随机的方法选择的。</p>
<p>但是现实世界里你可能数百万的可能答复，并且你并不知道答复是否合理正确。你没能力从数百万的可能的答复里去挑选一个得分最高的正确答复。成本太高了！ google 的 smart reply 用分布式集群技术计算一系列的可能答复去挑选,.</p>
<p>可能你只有百来个备选答案，可以去评估每一个。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_recall</span><span class="params">(y, y_test, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    num_examples = float(len(y))</span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> predictions, label <span class="keyword">in</span> zip(y, y_test):</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">in</span> predictions[:k]:</span><br><span class="line">            num_correct += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> num_correct/num_examples</span><br></pre></td></tr></table></figure>
<p>其中，y 是所预测的以降序排列的模型预测分值，y_test 是实际的 label 值。举个例子，假设 y 的值为 [0,3,1,2,5,6,4,7,8,9]，这说明 第0号 的候选的预测分值最高、作为回复的可能性最高，而9号则最低。这里的 第0号 同时也是正确的那个，即正例数据，标号为 1-9 的为随机生成的负例数据。</p>
<h3 id="5-1-基线模型-random-guess"><a href="#5-1-基线模型-random-guess" class="headerlink" title="5.1  基线模型:random guess"></a>5.1  基线模型:random guess</h3><p>理论上，最base的随机模型（Random Predictor）的recall@1的值为10%，recall@2的值为20%.</p>
<p>相应的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Random Predictor</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_random</span><span class="params">(context, utterances)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.random.choice(len(utterances), <span class="number">10</span>, replace=<span class="keyword">False</span>) <span class="comment">#np.random.choice(5, 3)  array([0, 3, 4])</span></span><br><span class="line">    <span class="comment"># 可以从一个 int数字 或 1维 array 里随机选取内容，并将选取结果放入 n维 array 中返回</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate Random predictor</span></span><br><span class="line">y_random = [predict_random(test_df.Context[x], test_df.iloc[x,<span class="number">1</span>:].values) <span class="keyword">for</span> x <span class="keyword">in</span> range(len(test_df))]</span><br><span class="line">y_test = np.zeros(len(y_random))</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>]:</span><br><span class="line">    print(<span class="string">"Recall @ (&#123;&#125;, 10): &#123;:g&#125;"</span>.format(n, evaluate_recall(y_random, y_test, n)))</span><br></pre></td></tr></table></figure>
<p>实际的模型结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Recall @ (<span class="number">1</span>, <span class="number">10</span>): <span class="number">0.0937632</span></span><br><span class="line">Recall @ (<span class="number">2</span>, <span class="number">10</span>): <span class="number">0.194503</span></span><br><span class="line">Recall @ (<span class="number">5</span>, <span class="number">10</span>): <span class="number">0.49297</span></span><br><span class="line">Recall @ (<span class="number">10</span>, <span class="number">10</span>): <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>这与理论预期相符，但这不是我们所追求的结果。</p>
<h3 id="5-2-基线模型-TF-IDF检索"><a href="#5-2-基线模型-TF-IDF检索" class="headerlink" title="5.2 基线模型:TF-IDF检索"></a>5.2 基线模型:TF-IDF检索</h3><p>另外一个 baseline 的模型为 <strong>tfidf predictor</strong>。直观上，两篇文档对应的 tfidf 向量 越接近，两篇文章的内容也越相似。同样的，对于一个 QR pair，它们语义上接近的词共现的越多，也将越可能是一个正确的 QR pair（这句话存疑，原因在于 Q R 之间也有可能不存在语义上的相似，一个Q对应的 R 是多样的。）。tfidf predictor 对应的代码如下（利用scikit-learn工具能够轻易实现）：</p>
<blockquote>
<p>tfidf表示词频（term frequency）和逆文档词频（inverse document frequency），它衡量了一个词在一篇文档中的重要程度（基于整个语料库）。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TFIDFPredictor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.vectorizer = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.vectorizer.fit(np.append(data.Context.values,data.Utterance.values))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, context, utterances)</span>:</span></span><br><span class="line">        <span class="comment"># Convert context and utterances into tfidf vector</span></span><br><span class="line">        vector_context = self.vectorizer.transform([context])</span><br><span class="line">        vector_doc = self.vectorizer.transform(utterances)</span><br><span class="line">        <span class="comment"># The dot product measures the similarity of the resulting vectors</span></span><br><span class="line">        result = np.dot(vector_doc, vector_context.T).todense()</span><br><span class="line">        result = np.asarray(result).flatten()</span><br><span class="line">        <span class="comment"># Sort by top results and return the indices in descending order</span></span><br><span class="line">        <span class="keyword">return</span> np.argsort(result, axis=<span class="number">0</span>)[::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate TFIDF predictor</span></span><br><span class="line">pred = TFIDFPredictor()</span><br><span class="line">pred.train(train_df)</span><br><span class="line">y = [pred.predict(test_df.Context[x], test_df.iloc[x,<span class="number">1</span>:].values) <span class="keyword">for</span> x <span class="keyword">in</span> range(len(test_df))]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>]:</span><br><span class="line">    print(<span class="string">"Recall @ (&#123;&#125;, 10): &#123;:g&#125;"</span>.format(n, evaluate_recall(y, y_test, n)))</span><br></pre></td></tr></table></figure>
<p>模型结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Recall @ (<span class="number">1</span>, <span class="number">10</span>): <span class="number">0.495032</span></span><br><span class="line">Recall @ (<span class="number">2</span>, <span class="number">10</span>): <span class="number">0.596882</span></span><br><span class="line">Recall @ (<span class="number">5</span>, <span class="number">10</span>): <span class="number">0.766121</span></span><br><span class="line">Recall @ (<span class="number">10</span>, <span class="number">10</span>): <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>显然这比 Random 的模型要好得多，但这还不够。之前的假设并不完美，首先 query 和 response 之间并不一定要是语义上的相近；其次 tfidf模型 忽略了词序这一重要的信息。使用 NN模型 我们能做得更好一些。</p>
<h2 id="6-LSTM"><a href="#6-LSTM" class="headerlink" title="6. LSTM"></a>6. LSTM</h2><p>建立的 NN模型 为两层 Encoder 的 LSTM模型（Dual Encoder LSTM Network），这种形式的网络被广泛应用 chatbot 中。</p>
<p><a href="https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html" target="_blank" rel="external">seq2seq模型</a> 常用于机器翻译领域，并取得了较大的效果。使用 Dual LSTM模型 的原因在于这个模型被证明在这个数据集有较好的效果（<a href="https://arxiv.org/abs/1510.03753" target="_blank" rel="external">详情见这里</a>）, 这可以作为我们后续模型效果的验证。</p>
<p>两层 Encoder 的 LSTM模型 的结构图如下（<a href="https://arxiv.org/abs/1506.08909" target="_blank" rel="external">论文来源</a>）：</p>
<p><strong>大致流程：</strong></p>
<blockquote>
<p>(1). Query 和 Response 都是经过分词的，分词后每个词 embedding 为向量形式。初始的词向量使用 GloVe / Word2vec，之后词向量随着模型的训练会进行 fine-tuned 。</p>
<p>(2). 分词且向量化的 Query 和 Response 经过相同的 RNN（word by word）。RNN 最终生成一个向量表示，捕捉了 Query 和 Response 之间的[语义联系]（图中的$c$和$r$）；这个向量的维度是可以指定的，这里指定为 256维。</p>
<p>(3). 将 向量c 与一个 矩阵M 相乘，来预测一个可能的 回复$r’$。如果 $c$ 为一个256维的向量，M维 256*256 的矩阵，两者相乘的结果为另一个256维的向量，我们可以将其解释为[一个生成式的回复向量]。矩阵M 是需要训练的参数。</p>
<p>(4). 通过点乘的方式来预测生成的 回复$r’$ 和 候选的 回复$r$ 之间的相似程度，点乘结果越大表示候选回复作为回复的可信度越高；之后通过 sigmoid 函数归一化，转成概率形式。</p>
<p> (sigmoid作为压缩函数经常使用) 图中把第(3)步和第(4)步结合在一起了。</p>
<ul>
<li><p>(5). 损失函数（loss function）。这里使用二元的交叉熵（binary cross-entropy）作为损失函数。我们已知实例的真实 label $y$， 值为 0 或 1； 通过上面的第(4)步可以得到一个概率值 $y’$；因此，交叉熵损失值为 $L = -y * ln(y’) - (1 - y) * ln(1 - y’)$。</p>
<p>这个公式意义是直观的，即当 $y=1$ 时，$L = -ln(y’)$，期望 $y’$ 尽量接近 1 使得损失函数的值越小；反之亦然。</p>
</li>
</ul>
<p>实现过程中使用了 numpy、pandas、TensorFlow 和 TF Learn 等工具。</p>
</blockquote>
<h3 id="6-1-数据预处理"><a href="#6-1-数据预处理" class="headerlink" title="6.1. 数据预处理"></a>6.1. 数据预处理</h3><p><a href="https://github.com/chatbot-tube/ubuntu-ranking-dataset-creator" target="_blank" rel="external">数据集</a>的原始格式为csv格式，我们需要先将其转为 TensorFlow 专有的格式，这种格式的好处在于能够直接从输入文件中 load tensors，并让 TensorFlow 来处理洗牌(shuffling)、批量(batching) 和 队列化(queuing) 等操作。预处理中还包括创建一个字典库，将词进行标号，TFRecord 文件将直接存储这些词的标号。</p>
<p>每个实例包括如下几个字段：</p>
<ul>
<li>Query：表示为一串词标号的序列，如 [231, 2190, 737, 0, 912]；</li>
<li>Query 的长度；</li>
<li>Response：同样是一串词标号的序列；</li>
<li>Response 的长度；</li>
<li>Label；</li>
<li>Distractor_[N]：表示负例干扰数据，仅在验证集和测试集中有，N 的取值为 0-8；</li>
<li>Distractor_[N]的长度；</li>
</ul>
<p>数据预处理的 <a href="https://github.com/dennybritz/chatbot-retrieval/blob/master/scripts/prepare_data.py" target="_blank" rel="external">Python脚本</a>，生成了3个文件：train.tfrecords, validation.tfrecords 和 test.tfrecords。你可以尝试自己运行程序，或者直接下载和使用预处理后的数据。</p>
<h3 id="6-2-创建输入函数"><a href="#6-2-创建输入函数" class="headerlink" title="6.2. 创建输入函数"></a>6.2. 创建输入函数</h3><p>为了使用 TensoFlow内置 的训练和评测模块，我们需要创建一个输入函数：这个函数返回输入数据的 batch。</p>
<blockquote>
<p>因为训练数据和测试数据的格式不同，我们需要创建不同的输入函数。</p>
<p>输入函数需要返回批量(<strong>batch</strong>)的特征和标签值(如果有的话)。类似于如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># TODO Load and preprocess data here</span></span><br><span class="line">  <span class="keyword">return</span> batched_features, labels</span><br></pre></td></tr></table></figure>
<p>因为我们需要在模型训练和评测过程中使用不同的输入函数，为了防止重复书写代码，我们创建一个包装器(wrapper)，名称为create_input_fn，针对不同的mode使用相应的code，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_input_fn</span><span class="params">(mode, input_files, batch_size, num_epochs=None)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">()</span>:</span></span><br><span class="line">	 <span class="comment"># TODO Load and preprocess data here</span></span><br><span class="line">        <span class="keyword">return</span> batched_features, labels</span><br><span class="line">    <span class="keyword">return</span> input_fn</span><br></pre></td></tr></table></figure>
<p>完整的code见<a href="https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_inputs.py" target="_blank" rel="external">udc_inputs.py</a>。整体上，这个函数做了如下的事情：</p>
<p>(1) 定义了示例文件中的 feature字段；<br>(2) 使用 tf.TFRecordReader 来读取 input_files 中的数据；<br>(3) 根据 feature字段 的定义对数据进行解析；<br>(4) 提取训练数据的标签 label；<br>(5) 产生批量化的训练数据 batch train_datasets；<br>(6) 返回批量的特征数据及对应标签 label；</p>
<h3 id="6-3-定义评测指标"><a href="#6-3-定义评测指标" class="headerlink" title="6.3. 定义评测指标"></a>6.3. 定义评测指标</h3><p>之前已经提到用 <strong>recall@k</strong> 这个指标来评测模型，TensorFlow 中已经实现了许多标准指标（包括 <strong>recall@k</strong>）。为了使用这些指标，需要创建一个字典，key 为指标名称，value 为对应的计算函数。如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_evaluation_metrics</span><span class="params">()</span>:</span></span><br><span class="line">    eval_metrics = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>]:</span><br><span class="line">        eval_metrics[<span class="string">"recall_at_%d"</span> % k] = functools.partial(</span><br><span class="line">            tf.contrib.metrics.streaming_sparse_recall_at_k,</span><br><span class="line">            k=k</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> eval_metrics</span><br></pre></td></tr></table></figure>
<p>如上，我们使用了 <a href="https://docs.python.org/2/library/functools.html#functools.partial" target="_blank" rel="external">functools.partial</a> 函数，这个函数的输入参数有两个。不要被 streaming_sparse_recall_at_k 所困惑，其中的 streaming 的含义是表示指标的计算是增量式的。</p>
<p>训练和测试所使用的评测方式是不一样的，训练过程中我们对 每个case 可能作为正确回复的概率进行预测，而测试过程中我们对每组数据（包含10个case，其中1个是正确的，另外9个是生成的负例/噪音数据）中的case进行逐条概率预测，得到例如 [0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11] 这样格式的输出，这些输出值的和并不要求为 1（因为是逐条预测的，有单独的预测概率值，在 0 到 1 之间）； 而对于这组数据而言，因为数据 index=0 对应的为正确答案，这里 recall@1 为 0，因为 0.34 是其中第二大的值，所以 recall@2 是 1（表示这组数据中预测概率值在前二的中有一个是正确的）。</p>
<h3 id="6-4-训练程序样例"><a href="#6-4-训练程序样例" class="headerlink" title="6.4. 训练程序样例"></a>6.4. 训练程序样例</h3><p>首先，给一个模型训练和测试的程序样例，这之后你可以参照程序中所用到的标准函数，来快速切换和使用其他的网络模型。假设我们有一个函数 <code>model_fn</code>，函数的输入参数有 <code>batched features</code>，<code>label</code> 和 <code>mode</code>(train/evaluation)，函数的输出为预测值。程序样例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator = tf.contrib.learn.Estimator(</span><br><span class="line">    model_fn=model_fn,</span><br><span class="line">    model_dir=MODEL_DIR,</span><br><span class="line">    config=tf.contrib.learn.RunConfig()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">input_fn_train = udc_inputs.create_input_fn(</span><br><span class="line">    mode=tf.contrib.learn.ModeKeys.TRAIN,</span><br><span class="line">    input_files=[TRAIN_FILE],</span><br><span class="line">    batch_size=hparams.batch_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">input_fn_eval = udc_inputs.create_input_fn(</span><br><span class="line">    mode=tf.contrib.learn.ModeKeys.EVAL,</span><br><span class="line">    input_files=[VALIDATION_FILE],</span><br><span class="line">    batch_size=hparams.eval_batch_size,</span><br><span class="line">    num_epochs=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">eval_metrics = udc_metrics.create_evaluation_metrics()</span><br><span class="line"></span><br><span class="line"><span class="comment"># We need to subclass theis manually for now. The next TF version will</span></span><br><span class="line"><span class="comment"># have support ValidationMonitors with metrics built-in.</span></span><br><span class="line"><span class="comment"># It's already on the master branch.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EvaluationMonitor</span><span class="params">(tf.contrib.learn.monitors.EveryN)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">every_n_step_end</span><span class="params">(self, step, outputs)</span>:</span></span><br><span class="line">        self._estimator.evaluate(</span><br><span class="line">        input_fn=input_fn_eval,</span><br><span class="line">        metrics=eval_metrics,</span><br><span class="line">        steps=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">eval_monitor = EvaluationMonitor(every_n_steps=FLAGS.eval_every)</span><br><span class="line"></span><br><span class="line">estimator.fit(input_fn=input_fn_train, steps=<span class="keyword">None</span>, monitors=[eval_monitor])</span><br></pre></td></tr></table></figure>
<p>这里创建了一个 <strong>model_fn</strong> 的 <strong>estimator</strong>(评估函数)；</p>
<p>两个输入函数，<strong>input_fn_train</strong> 和 <strong>input_fn_eval</strong>，以及计算评测指标的函数；</p>
<p>完整的code见<a href="https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_train.py" target="_blank" rel="external">udc_train.py</a>。</p>
<h3 id="6-5-创建模型"><a href="#6-5-创建模型" class="headerlink" title="6.5. 创建模型"></a>6.5. 创建模型</h3><p>到目前为止，我们创建了模型的 输入、解析、评测和训练 的样例程序。现在我们来写 LSTM 的程序，create_model_fn函数 用以处理不同格式的训练和测试数据；它的输入参数为 model_impl，这个函数表示实际作出预测的模型，这里就是用的LSTM，当然你可以替换成任意的其他模型。程序如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dual_encoder_model</span><span class="params">(</span><br><span class="line">    hparams,</span><br><span class="line">    mode,</span><br><span class="line">    context,</span><br><span class="line">    context_len,</span><br><span class="line">    utterance,</span><br><span class="line">    utterance_len,</span><br><span class="line">    targets)</span>:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Initialize embedidngs randomly or with pre-trained vectors if available</span></span><br><span class="line">  embeddings_W = get_embeddings(hparams)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Embed the context and the utterance</span></span><br><span class="line">  context_embedded = tf.nn.embedding_lookup(</span><br><span class="line">      embeddings_W, context, name=<span class="string">"embed_context"</span>)</span><br><span class="line">  utterance_embedded = tf.nn.embedding_lookup(</span><br><span class="line">      embeddings_W, utterance, name=<span class="string">"embed_utterance"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Build the RNN</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"rnn"</span>) <span class="keyword">as</span> vs:</span><br><span class="line">    <span class="comment"># We use an LSTM Cell</span></span><br><span class="line">    cell = tf.nn.rnn_cell.LSTMCell(</span><br><span class="line">        hparams.rnn_dim,</span><br><span class="line">        forget_bias=<span class="number">2.0</span>,</span><br><span class="line">        use_peepholes=<span class="keyword">True</span>,</span><br><span class="line">        state_is_tuple=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the utterance and context through the RNN</span></span><br><span class="line">    rnn_outputs, rnn_states = tf.nn.dynamic_rnn(</span><br><span class="line">        cell,</span><br><span class="line">        tf.concat(<span class="number">0</span>, [context_embedded, utterance_embedded]),</span><br><span class="line">        sequence_length=tf.concat(<span class="number">0</span>, [context_len, utterance_len]),</span><br><span class="line">        dtype=tf.float32)</span><br><span class="line">    encoding_context, encoding_utterance = tf.split(<span class="number">0</span>, <span class="number">2</span>, rnn_states.h)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"prediction"</span>) <span class="keyword">as</span> vs:</span><br><span class="line">    M = tf.get_variable(<span class="string">"M"</span>,</span><br><span class="line">      shape=[hparams.rnn_dim, hparams.rnn_dim],</span><br><span class="line">      initializer=tf.truncated_normal_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># "Predict" a  response: c * M</span></span><br><span class="line">    generated_response = tf.matmul(encoding_context, M)</span><br><span class="line">    generated_response = tf.expand_dims(generated_response, <span class="number">2</span>)</span><br><span class="line">    encoding_utterance = tf.expand_dims(encoding_utterance, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Dot product between generated response and actual response</span></span><br><span class="line">    <span class="comment"># (c * M) * r</span></span><br><span class="line">    logits = tf.batch_matmul(generated_response, encoding_utterance, <span class="keyword">True</span>)</span><br><span class="line">    logits = tf.squeeze(logits, [<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply sigmoid to convert logits to probabilities</span></span><br><span class="line">    probs = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the binary cross-entropy loss</span></span><br><span class="line">    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits, tf.to_float(targets))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Mean loss across the batch of examples</span></span><br><span class="line">  mean_loss = tf.reduce_mean(losses, name=<span class="string">"mean_loss"</span>)</span><br><span class="line">  <span class="keyword">return</span> probs, mean_loss</span><br></pre></td></tr></table></figure>
<p>完整的程序见 <a href="https://github.com/dennybritz/chatbot-retrieval/blob/master/models/dual_encoder.py" target="_blank" rel="external">dual_encoder.py</a>。基于这个，我们能够实例化 model函数 在我们之前定义的 <a href="https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_train.py" target="_blank" rel="external">udc_train.py</a>，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_fn = udc_model.create_model_fn(</span><br><span class="line">  hparams=hparams,</span><br><span class="line">  model_impl=dual_encoder_model)</span><br></pre></td></tr></table></figure>
<p>这样我们就可以直接运行 udc_train.py文件，来开始模型的训练和评测了，你可以设定–eval_every参数 来控制模型在验证集上的评测频率。更多的命令行参数信息可见 tf.flags 和 hparams，你也可以运行 python udc_train.py –help 来查看。</p>
<p>运行程序的效果如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">INFO:tensorflow:training step 20200, loss = 0.36895 (0.330 sec/batch).</span><br><span class="line">INFO:tensorflow:Step 20201: mean_loss:0 = 0.385877</span><br><span class="line">INFO:tensorflow:training step 20300, loss = 0.25251 (0.338 sec/batch).</span><br><span class="line">INFO:tensorflow:Step 20301: mean_loss:0 = 0.405653</span><br><span class="line">...</span><br><span class="line">INFO:tensorflow:Results after 270 steps (0.248 sec/batch): recall_at_1 = 0.507581018519, recall_at_2 = 0.689699074074, recall_at_5 = 0.913020833333, recall_at_10 = 1.0, loss = 0.5383</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="6-6-模型的评测"><a href="#6-6-模型的评测" class="headerlink" title="6.6. 模型的评测"></a>6.6. 模型的评测</h3><p>在训练完模型后，你可以将其应用在 <strong>测试集</strong> 上，使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python udc_test.py --model_dir=$MODEL_DIR_FROM_TRAINING</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python udc_test.py --model_dir=~/github/chatbot-retrieval/runs/<span class="number">1467389151</span></span><br></pre></td></tr></table></figure>
<p>这将得到模型在 <strong>测试集</strong> 上的 recall@k 的结果，注意在使用 udc_test.py文件 时，需要使用与训练时相同的参数。</p>
<p>在训练模型的次数大约 <strong>2w</strong> 次时(在GPU上大约花费1小时)，模型在测试集上得到如下的结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">recall_at_1 = 0.507581018519</span><br><span class="line">recall_at_2 = 0.689699074074</span><br><span class="line">recall_at_5 = 0.913020833333</span><br></pre></td></tr></table></figure>
<p>其中，recall@1的值与tfidf模型的差不多，但是recall@2和recall@5的值则比tfidf模型的结果好太多。原论文中的结果依次是0.55,0.72和0.92，可能通过模型调参或者预处理能够达到这个结果。</p>
<h3 id="6-7-使用模型进行预测"><a href="#6-7-使用模型进行预测" class="headerlink" title="6.7. 使用模型进行预测"></a>6.7. 使用模型进行预测</h3><p>对于新的数据，你可以使用 <a href="https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_predict.py" target="_blank" rel="external">udc_predict.py</a> 来进行预测；例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python udc_predict.py --model_dir=./runs/<span class="number">1467576365</span>/</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Context: Example context</span><br><span class="line">Response 1: 0.44806</span><br><span class="line">Response 2: 0.481638</span><br></pre></td></tr></table></figure>
<p>你可以从候选的回复中，选择预测分值最高的那个作为回复。</p>
<h3 id="6-8-总结"><a href="#6-8-总结" class="headerlink" title="6.8. 总结"></a>6.8. 总结</h3><p>这篇博文中，我们实现了一个基于检索的 NN模型，它能够对候选的回复进行预测和打分，通过输出分值最高（或者满足一定阈值）的候选回复已完成聊天的过程。后续可以尝试其他更好的模型，或者通过调参来取得更好的实验结果。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://chatterbot.readthedocs.io/en/stable/" target="_blank" rel="external">About ChatterBot</a></li>
<li><a href="http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/" target="_blank" rel="external">2016 Google Brain deep-learning-for-chatbots-part-1-introduction, wildml blog</a></li>
<li><a href="http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/" target="_blank" rel="external">2016 Google Brain deep-learning-for-chatbots-2-retrieval-based-model-tensorflow, wildml blog</a></li>
<li><a href="http://www.jeyzhang.com/deep-learning-for-chatbots-2.html" target="_blank" rel="external">聊天机器人中的深度学习技术之二：基于检索模型的实现</a></li>
<li><a href="http://www.jeyzhang.com/deep-learning-for-chatbots-1.html" target="_blank" rel="external">聊天机器人中的深度学习技术之一：导读</a></li>
<li><a href="http://www.cnblogs.com/LittleHann/p/6426610.html" target="_blank" rel="external">Tensorflow搞一个聊天机器人</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27285330" target="_blank" rel="external">Eric，基于多搜索引擎的自动问答机器人</a></li>
<li><a href="https://my.oschina.net/apdplat/blog/401622" target="_blank" rel="external">测试人机问答系统智能性的3760个问题</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25749274" target="_blank" rel="external">中国版的聊天机器人地图 Chatbots Landscape</a></li>
<li><a href="https://blog.csdn.net/SunJW_2017/article/details/82494360" target="_blank" rel="external">条件随机场简介</a></li>
<li><a href="https://blog.csdn.net/SunJW_2017/article/details/82460284" target="_blank" rel="external">知识图谱学习系列之二：命名实体识别1（技术及代码）</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 4 - 深度学习知识回顾]]></title>
      <url>http://sggo.me/2017/08/14/chatbot/chatbot-research4/</url>
      <content type="html"><![CDATA[<p>该篇主要介绍一些 RNN、LSTM 等深度学习知识。  <a href="/deeplearning/#5-Sequence-Models">详情请戳</a></p>
<a id="more"></a>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/Conchylicultor/DeepQA" target="_blank" rel="external">2015 DeepQA</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 3 - 机器学习构建 chatbot]]></title>
      <url>http://sggo.me/2017/08/13/chatbot/chatbot-research3/</url>
      <content type="html"><![CDATA[<p>Chatterbot 聊天机器人框架</p>
<p>检索与匹配 &amp; 分类与朴素贝叶斯</p>
<a id="more"></a>
<p><strong>关于聊天机器人的思考</strong> </p>
<ol>
<li>工程考量</li>
<li>机器学习角度考虑</li>
</ol>
<p><strong>预备知识</strong> </p>
<ol>
<li>检索与匹配</li>
<li>分类与朴素贝叶斯</li>
</ol>
<p><strong>chatterbot</strong> </p>
<ol>
<li>架构与使用方法 </li>
<li>源码分析</li>
</ol>
<h2 id="1-传统聊天机器人"><a href="#1-传统聊天机器人" class="headerlink" title="1. 传统聊天机器人"></a>1. 传统聊天机器人</h2><p>NLP 基础知识   </p>
<blockquote>
<ol>
<li>基本分词 (jieba)</li>
<li>关键词抽取 (tf-idf等)   </li>
<li>正则表达式模式匹配   </li>
<li>…</li>
</ol>
</blockquote>
<p>Machine Learning相关知识   </p>
<blockquote>
<ol>
<li>文本表示与匹配   </li>
<li>分类 (文本场景分析)   </li>
<li>数据驱动 (特征工程)</li>
<li>…</li>
</ol>
</blockquote>
<h2 id="2-聊天机器人的一些思考"><a href="#2-聊天机器人的一些思考" class="headerlink" title="2. 聊天机器人的一些思考"></a>2. 聊天机器人的一些思考</h2><p>工程考量</p>
<blockquote>
<ul>
<li>架构设计清晰、模块化</li>
<li>功能分拆，解耦，部件可插拔与扩展</li>
</ul>
</blockquote>
<p><img src="/images/chatbot/chatbot-3_1.jpg" width="600"></p>
<p>算法与机器学习角度考量</p>
<blockquote>
<ul>
<li>算法简单，数据(特征)驱动   </li>
<li>场景化与垂直领域</li>
</ul>
</blockquote>
<h2 id="3-预备知识"><a href="#3-预备知识" class="headerlink" title="3. 预备知识"></a>3. 预备知识</h2><p><strong>基于检索与匹配</strong>   </p>
<blockquote>
<ul>
<li>知识库 (存储了问题与回复内容)   </li>
<li>检索: 搜寻相关问题   </li>
<li>匹配: 对结果进行排序</li>
</ul>
</blockquote>
<p><strong>编辑距离</strong></p>
<blockquote>
<p>编辑距离/Levenshtein距离，是指两个字符串之间，由一个转成另一个所需要的最少编辑操作次数。</p>
<p>递归 &amp; 动态规划 DP</p>
</blockquote>
<p>TFIDF</p>
<blockquote>
<p>QA pair 的 TFIDF 的相近度</p>
<p>S1: “你喜欢什么书”<br>S2: “你喜欢什么电影”</p>
</blockquote>
<p>python 编辑距离</p>
<blockquote>
<p>Python在string 类型中，默认的 utf-8 编码下，一个中文字符是用三个字节来表示的。用unicode。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> Levenshtein</span><br><span class="line"></span><br><span class="line">texta = <span class="string">"u关键时刻"</span></span><br><span class="line">textb = <span class="string">"u关键·时刻"</span></span><br><span class="line"></span><br><span class="line">print(Levenshtein.distance(texta,textb)) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<h2 id="4-Chatterbot-聊天机器人"><a href="#4-Chatterbot-聊天机器人" class="headerlink" title="4. Chatterbot 聊天机器人"></a>4. Chatterbot 聊天机器人</h2><p><img src="/images/chatbot/chatbot-3_2.jpg" width="700"></p>
<p><strong>每个部分都设计了不同的 “适配器”(Adapter)</strong>  </p>
<p><strong>机器人应答逻辑 =&gt; Logic Adapters</strong></p>
<blockquote>
<ul>
<li>Closest Match Adapter<br> 字符串模糊匹配(编辑距离)</li>
<li>Closest Meaning Adapter<br> 借助 nltk 的 WordNet，近义词评估</li>
<li>Time Logic Adapter<br> 处理涉及时间的提问</li>
<li>Mathematical Evaluation Adapter    涉及数学运算</li>
</ul>
</blockquote>
<p><strong>存储器后端 =&gt; Storage Adapters</strong></p>
<blockquote>
<ul>
<li>Read Only Mode<br>只读模式，当有输入数据到 chatterbot 的时候，数据库并不会发生改变</li>
<li>Json Database Adapter<br>用以存储对话数据的接口，对话数据以 Json格式 进行存储. (载入数据特别慢，工业界不可行)</li>
<li>Mongo Database Adapter<br>以 MongoDB database 方式来存储对话数据</li>
</ul>
</blockquote>
<p><strong>输入形式 =&gt; Input Adapters</strong></p>
<blockquote>
<ul>
<li>Variable input type adapter<br> 允许 chatterbot 接收不同类型的输入的，如 strings, dictionaries 和Statements   </li>
<li>Terminal adapter<br> 使得 ChatterBot 可以通过终端进行对话   </li>
<li>Speech recognition<br> 语音识别输入，详见 chatterbot-voice</li>
</ul>
</blockquote>
<h2 id="5-Bayes-分类"><a href="#5-Bayes-分类" class="headerlink" title="5. Bayes 分类"></a>5. Bayes 分类</h2><p><strong>预备知识:场景分类与NB</strong></p>
<p><img src="/images/chatbot/chatbot-3_3.png" width="400"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/gunthercox/ChatterBot" target="_blank" rel="external">github ChatterBot</a></li>
<li><a href="https://chatterbot.readthedocs.io/en/stable/" target="_blank" rel="external">Docs » About ChatterBot</a></li>
<li><a href="https://blog.xiaoquankong.ai/使用chatterbot构建自己的中文chat(闲聊)机器人/" target="_blank" rel="external">使用chatterbot构建自己的中文chat(闲聊)机器人/</a></li>
<li><a href="https://www.cnblogs.com/LHWorldBlog/p/9292024.html" target="_blank" rel="external">自然语言处理 Chatterbot聊天机器人</a></li>
<li><a href="https://blog.csdn.net/qifengzou/article/details/77126933" target="_blank" rel="external">精品: 开源ChatterBot工作原理</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 2 - NLP基础知识回顾]]></title>
      <url>http://sggo.me/2017/08/12/chatbot/chatbot-research2/</url>
      <content type="html"><![CDATA[<p><a href="http://pypi.python.org/pypi/nltk" target="_blank" rel="external">NLTK</a> Python上著名的自然语言处理库。 </p>
<ul>
<li>自带语料库，词性分类库， 还有强大的社区支持。</li>
</ul>
<a id="more"></a>
<p><strong>文本处理流程</strong></p>
<ul>
<li>分词</li>
<li>归一化 </li>
<li>停止词</li>
</ul>
<p><strong>NLP经典三案例 </strong></p>
<ul>
<li>情感分析</li>
<li>文本相似度</li>
<li>文本分类</li>
</ul>
<blockquote>
<p>斯坦佛 CoreNLP (英文、中文、西班牙语)</p>
</blockquote>
<h2 id="1-NLTK"><a href="#1-NLTK" class="headerlink" title="1. NLTK"></a>1. NLTK</h2><ol>
<li>Python 著名的自然语言处理库</li>
<li>自带语料库、词性分类库</li>
<li>自带分类、分词 等功能</li>
<li>强大的社区支持</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo pip install -U nltk</span><br><span class="line">sudo pip install -U numpy</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download()</span><br></pre></td></tr></table></figure>
<!--<img src="/images/chatbot/chatbot-2_1.png" width="400" />
-->
<p>测试是否安装成功</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>python</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> nltk</span><br></pre></td></tr></table></figure>
<h2 id="2-功能一览表"><a href="#2-功能一览表" class="headerlink" title="2. 功能一览表"></a>2. 功能一览表</h2><p><img src="/images/chatbot/chatbot-2_2.png" width="800"></p>
<p><strong>NLTK 自带语料库</strong></p>
<p><img src="/images/chatbot/chatbot-2_3.png" width="600"></p>
<h2 id="3-Tokenize"><a href="#3-Tokenize" class="headerlink" title="3. Tokenize"></a>3. Tokenize</h2><p><strong>tokenize 把长句子拆成有“意义”的小部件</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> nltk<span class="meta">&gt;&gt;&gt; </span>sentence = “hello, world<span class="string">"&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)&gt;&gt;&gt; tokens['hello', ‘,', 'world']</span></span><br></pre></td></tr></table></figure>
<p><strong>中文分词 jieba</strong> (第三方开源库)</p>
<p><img src="/images/chatbot/chatbot-2_4.png" width="780"></p>
<p><strong>有时候tokenize没那么简单</strong></p>
<blockquote>
<p>比如社交网络上,这些乱七八糟的不合语法不合正常逻辑的语言很多:<br>拯救 @某人, 表情符号, URL, #话题符号</p>
</blockquote>
<p><img src="/images/chatbot/chatbot-2_5.png" width="650"></p>
<p><strong>社交网络语言的tokenize :</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenizetweet = <span class="string">'RT @angelababy: love you baby! :D http://ah.love #168cm'</span>print(word_tokenize(tweet))</span><br></pre></td></tr></table></figure>
<p><img src="/images/chatbot/chatbot-2_6.png" width="880"></p>
<p><a href="http://www.regexlab.com/zh/regref.htm" target="_blank" rel="external">正则表达式对照表</a></p>
<h2 id="4-词形归一化"><a href="#4-词形归一化" class="headerlink" title="4. 词形归一化"></a>4. 词形归一化</h2><p>Inflection变化: walk =&gt; walking =&gt; walked 不影响词性</p>
<p>derivation 引申: nation (noun) =&gt; national (adjective) =&gt; nationalize (verb) 影响词性</p>
<p>Stemming 词干提取:一般来说，就是把不影响词性的inflection的小尾巴砍掉</p>
<blockquote>
<p>walking 砍ing = walk<br>walked 砍ed = walk</p>
</blockquote>
<p>Lemmatization 词形归一:把各种类型的词的变形，都归为一个形式 </p>
<blockquote>
<p>went 归一 = go<br>are 归一 = be</p>
</blockquote>
<h3 id="4-1-Stemming"><a href="#4-1-Stemming" class="headerlink" title="4.1 Stemming"></a>4.1 Stemming</h3><ol>
<li>PorterStemmer</li>
<li>SnowballStemmer</li>
<li>LancasterStemmer</li>
<li>PorterStemmer</li>
</ol>
<p><strong>PorterStemmer</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> PorterStemmer<span class="meta">&gt;&gt;&gt; </span>porter_stemmer = PorterStemmer()<span class="meta">&gt;&gt;&gt; </span>porter_stemmer.stem(‘maximum’)u’maximum’<span class="meta">&gt;&gt;&gt; </span>porter_stemmer.stem(‘presumably’)u’presum’</span><br></pre></td></tr></table></figure>
<p><strong>LancasterStemmer</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> nltk.stem.lancaster <span class="keyword">import</span> LancasterStemmer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer = LancasterStemmer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer.stem(‘maximum’)</span><br><span class="line">‘maxim’</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer.stem(‘presumably’)</span><br><span class="line">‘presum’</span><br></pre></td></tr></table></figure>
<h3 id="4-2-Lemmatization"><a href="#4-2-Lemmatization" class="headerlink" title="4.2 Lemmatization"></a>4.2 Lemmatization</h3><p>词形归一： 把各种类型的词的变形,都归为一个形式 </p>
<p>went 归一 = go<br>are 归一 = be</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer<span class="meta">&gt;&gt;&gt; </span>wordnet_lemmatizer = WordNetLemmatizer()<span class="meta">&gt;&gt;&gt; </span>wordnet_lemmatizer.lemmatize(‘dogs’)u’dog’<span class="meta">&gt;&gt;&gt; </span>wordnet_lemmatizer.lemmatize(‘churches’)u’church’<span class="meta">&gt;&gt;&gt; </span>wordnet_lemmatizer.lemmatize(‘aardwolves’)u’aardwolf’</span><br></pre></td></tr></table></figure>
<p>NLTK更好地实现Lemma</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ⽊木有POS Tag,默认是NN 名词</span><span class="meta">&gt;&gt;&gt; </span>wordnet_lemmatizer.lemmatize(‘are’) <span class="comment"># ‘are’</span><span class="meta">&gt;&gt;&gt; </span>wordnet_lemmatizer.lemmatize(‘<span class="keyword">is</span>’)  <span class="comment"># ‘is’</span><span class="comment"># 加上POS Tag</span><span class="meta">&gt;&gt;&gt; </span>wordnet_lemmatizer.lemmatize(‘<span class="keyword">is</span>’, pos=’v’)  <span class="comment"># u’be’</span><span class="meta">&gt;&gt;&gt; </span>wordnet_lemmatizer.lemmatize(‘are’, pos=’v’) <span class="comment"># u’be’</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3-NLTK-标注-POS-Tag"><a href="#4-3-NLTK-标注-POS-Tag" class="headerlink" title="4.3 NLTK 标注 POS Tag"></a>4.3 NLTK 标注 POS Tag</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> nltk<span class="meta">&gt;&gt;&gt; </span>text = nltk.word_tokenize(<span class="string">'what does the fox say'</span>)<span class="meta">&gt;&gt;&gt; </span>text[<span class="string">'what'</span>, <span class="string">'does'</span>, <span class="string">'the'</span>, <span class="string">'fox'</span>, <span class="string">'say'</span>]<span class="meta">&gt;&gt;&gt; </span>nltk.pos_tag(text)[(<span class="string">'what'</span>, <span class="string">'WDT'</span>), (<span class="string">'does'</span>, <span class="string">'VBZ'</span>), (<span class="string">'the'</span>, <span class="string">'DT'</span>), (<span class="string">'fox'</span>, <span class="string">'NNS'</span>), (<span class="string">'say'</span>, <span class="string">'VBP'</span>)]</span><br></pre></td></tr></table></figure>
<h2 id="5-Stopwords"><a href="#5-Stopwords" class="headerlink" title="5. Stopwords"></a>5. Stopwords</h2><p>一千个 He 有一千种指代</p>
<p>一千个 The 有一千种指事 对于注重理解文本『意思』的应用场景来说</p>
<p>全体stopwords列表 <a href="http://www.ranks.nl/stopwords" target="_blank" rel="external">http://www.ranks.nl/stopwords</a></p>
<p><strong>去除 stopwords</strong></p>
<blockquote>
<p>首先记得在console里面下载一下词库， 或者 nltk.download(‘stopwords’)</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords </span><br><span class="line"></span><br><span class="line"><span class="comment"># 先token⼀一把,得到⼀一个word_list</span></span><br><span class="line"></span><br><span class="line">word_list = nltk.word_tokenize(<span class="string">'what does the fox say'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后filter⼀一把</span></span><br><span class="line"></span><br><span class="line">filtered_words = [word <span class="keyword">for</span> word <span class="keyword">in</span> word_list <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords.words(<span class="string">'english'</span>)]</span><br><span class="line"></span><br><span class="line">filtered_words</span><br></pre></td></tr></table></figure>
<h2 id="6-文本预处理流水线"><a href="#6-文本预处理流水线" class="headerlink" title="6. 文本预处理流水线"></a>6. 文本预处理流水线</h2><p>一条typical的文本预处理流水线</p>
<p><img src="/images/chatbot/chatbot-2_8.png" width="320"></p>
<p>文本预处理让我们得到了什么?</p>
<p><img src="/images/chatbot/chatbot-2_9.png" width="320"></p>
<h2 id="7-NLP上的经典应用"><a href="#7-NLP上的经典应用" class="headerlink" title="7. NLP上的经典应用"></a>7. NLP上的经典应用</h2><p>情感分析 、 文本相似度 、 文本分类</p>
<h3 id="7-1-情感分析"><a href="#7-1-情感分析" class="headerlink" title="7.1 情感分析"></a>7.1 情感分析</h3><p><img src="/images/chatbot/chatbot-2_10.png" width="720"></p>
<p>哪些是夸你？哪些是黑你？</p>
<p><strong>ML的情感分析</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> NaiveBayesClassifier<span class="comment"># 随⼿手造点训练集</span>s1 = <span class="string">'this is a good book'</span>s2 = <span class="string">'this is a awesome book'</span>s3 = <span class="string">'this is a bad book'</span>s4 = <span class="string">'this is a terrible book'</span><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(s)</span>:</span> <span class="comment"># Func: 句⼦处理</span><span class="comment"># 这⾥简单的⽤了了split(), 把句子中每个单词分开 # 显然 还有更多的processing method可以⽤ </span></span><br><span class="line">    <span class="keyword">return</span> &#123;word: <span class="keyword">True</span> <span class="keyword">for</span> word <span class="keyword">in</span> s.lower().split()&#125;<span class="comment"># return长这样:</span><span class="comment"># &#123;'this': True, 'is':True, 'a':True, 'good':True, 'book':True&#125; # 其中, 前⼀一个叫fname, 对应每个出现的文本单词;</span><span class="comment"># 后⼀一个叫fval, 指的是每个⽂文本单词对应的值。</span><span class="comment"># 这⾥里里我们⽤用最简单的True,来表示,这个词『出现在当前的句句⼦子中』的意义。</span><span class="comment"># 当然啦, 我们以后可以升级这个⽅方程, 让它带有更更加⽜牛逼的fval, 比如 word2vec</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把训练集给做成标准形式</span>training_data = [[preprocess(s1), <span class="string">'pos'</span>],                 [preprocess(s2), <span class="string">'pos'</span>],                 [preprocess(s3), <span class="string">'neg'</span>],                 [preprocess(s4), <span class="string">'neg'</span>]]<span class="comment"># 喂给model吃</span>model = NaiveBayesClassifier.train(training_data)<span class="comment"># 打出结果</span>print(model.classify(preprocess(<span class="string">'this is a good book'</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="7-2-文本相似度"><a href="#7-2-文本相似度" class="headerlink" title="7.2 文本相似度"></a>7.2 文本相似度</h3><p><strong>Frequency 频率统计</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> FreqDist</span><br><span class="line"><span class="comment"># 做个词库先</span></span><br><span class="line">corpus = <span class="string">'this is my sentence '</span> \</span><br><span class="line">           <span class="string">'this is my life '</span> \</span><br><span class="line">           <span class="string">'this is the day'</span></span><br><span class="line"><span class="comment"># 随便tokenize一下, 显然, 正如上文提到,</span></span><br><span class="line"><span class="comment"># 这里可以根据需要做任何的preprocessing:  stopwords, lemma, stemming, etc.</span></span><br><span class="line"><span class="comment"># 借⽤NLTK的FreqDist统计⼀下⽂字出现的频率 fdist = FreqDist(tokens)</span></span><br><span class="line"><span class="comment"># 它就类似于⼀个Dic,  带上某个单词, 可以看到它在整个文章中出现的次数</span></span><br><span class="line"></span><br><span class="line">tokens = nltk.word_tokenize(corpus) </span><br><span class="line">print(tokens)</span><br><span class="line"><span class="comment"># 得到 token 好的 word list</span></span><br><span class="line"><span class="comment"># ['this', 'is', 'my', 'sentence',</span></span><br><span class="line"><span class="comment"># 'this', 'is', 'my', 'life', 'this', # 'is', 'the', 'day']</span></span><br><span class="line"><span class="comment"># 借⽤ NLTK 的 FreqDist 统计⼀下文字出现的频率</span></span><br><span class="line">fdist = FreqDist(tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 它就类似于⼀一个Dict</span></span><br><span class="line"><span class="comment"># 带上某个单词, 可以看到它在整个文章中出现的次数</span></span><br><span class="line">print(fdist[<span class="string">'is'</span>]) <span class="comment">#3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 好, 此刻, 我们可以把最常⽤的50个单词拿出来 </span></span><br><span class="line">standard_freq_vector = fdist.most_common(<span class="number">50</span>) </span><br><span class="line">size = len(standard_freq_vector) </span><br><span class="line"><span class="keyword">print</span> <span class="string">"size: %s"</span> % (size)</span><br><span class="line">print(standard_freq_vector)</span><br><span class="line"><span class="comment"># [('is', 3), ('this', 3), ('my', 2),</span></span><br><span class="line"><span class="comment"># ('the', 1), ('day', 1), ('sentence', 1),</span></span><br><span class="line"><span class="comment"># ('life', 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Func: 按照出现频率⼤小, 记录下每⼀个单词的位置 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_lookup</span><span class="params">(v)</span>:</span></span><br><span class="line">    res = &#123;&#125;</span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v:</span><br><span class="line">        res[word[<span class="number">0</span>]] = counter</span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"><span class="comment"># 把标准的单词位置记录下来</span></span><br><span class="line">standard_position_dict = position_lookup(standard_freq_vector) </span><br><span class="line">print(standard_position_dict)</span><br><span class="line"><span class="comment"># 得到⼀个位置对照表</span></span><br><span class="line"><span class="comment"># &#123;'is': 0, 'the': 3, 'day': 4, 'this': 1, 'sentence': 5, 'my': 2, 'life': 6&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这时, 如果我们有个新句子:</span></span><br><span class="line">sentence = <span class="string">'this is cool'</span></span><br><span class="line"><span class="comment"># 先新建⼀个跟我们的标准vector同样⼤小的向量 </span></span><br><span class="line">freq_vector = [<span class="number">0</span>] * size</span><br><span class="line"><span class="comment"># 简单的Preprocessing</span></span><br><span class="line">tokens = nltk.word_tokenize(sentence) <span class="comment"># 对于这个新句⼦⾥的每一个单词</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> tokens:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 如果在我们的词库里出现过</span></span><br><span class="line">    <span class="comment"># 那么就在"标准位置"上+1 </span></span><br><span class="line">        freq_vector[standard_position_dict[word]] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">except</span> KeyError: <span class="comment"># 如果是个新词, 就 pass掉</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">print(freq_vector)   <span class="comment"># [1, 1, 0, 0, 0, 0, 0]</span></span><br><span class="line"><span class="comment"># [1, 1, 0, 0, 0, 0, 0]</span></span><br><span class="line"><span class="comment"># 第一个位置代表 is, 出现了一次</span></span><br><span class="line"><span class="comment"># 第二个位置代表 this, 出现了一次 </span></span><br><span class="line"><span class="comment"># 后面都没有</span></span><br></pre></td></tr></table></figure>
<h3 id="7-3-文本分类-tf-idf"><a href="#7-3-文本分类-tf-idf" class="headerlink" title="7.3 文本分类 tf-idf"></a>7.3 文本分类 tf-idf</h3><p>TF: Term Frequency, 衡量一个term在文档中出现得有多频繁。</p>
<p>$$<br>F(t) = (t出现在文档中的次数) / (文档中的term总数)<br>$$</p>
<p>IDF: Inverse Document Frequency, 衡量一个term有多重要。 有些词出现的很多,但是明显不是很有卵用。</p>
<p>$$<br>IDF(t) = log_e(文档总数 / 含有t的文档总数)<br>$$</p>
<blockquote>
<p><strong>TF-IDF = TF * IDF</strong></p>
<p>举个栗子🌰 :</p>
<p>一个文档有100个单词,其中单词baby出现了3次。 那么,TF(baby) = (3/100) = 0.03.</p>
<p>好,现在我们如果有10M的文档, baby出现在其中的1000个文档中。 那么,IDF(baby) = log(10,000,000 / 1,000) = 4</p>
<p>所以, TF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 = 0.12</p>
</blockquote>
<p><strong>nltk implement tf-idf</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.text <span class="keyword">import</span> TextCollection</span><br><span class="line"><span class="comment"># ⾸先, 把所有的⽂档放到TextCollection类中。</span></span><br><span class="line"><span class="comment"># 这个类会⾃动帮你断句, 做统计, 做计算</span></span><br><span class="line">corpus = TextCollection([<span class="string">'this is sentence one'</span>,</span><br><span class="line">                        <span class="string">'this is sentence two'</span>,</span><br><span class="line">                        <span class="string">'sentence three six'</span>,</span><br><span class="line">                        <span class="string">'this is sentence three'</span>])</span><br><span class="line"><span class="comment"># 直接就能算出tfidf</span></span><br><span class="line"><span class="comment"># (term: ⼀一句句话中的某个term, text: 这句句话)</span></span><br><span class="line">print(corpus.tf_idf(<span class="string">'this'</span>, <span class="string">'this is sentence four'</span>))</span><br><span class="line"><span class="comment"># 0.444342</span></span><br><span class="line"><span class="comment"># 同理, 怎么得到⼀一个标准⼤小的vector来表示所有的句子?</span></span><br><span class="line"><span class="comment"># 对于每个新句子</span></span><br><span class="line"><span class="comment">#new_sentence = 'this is sentence five' # 遍历⼀一遍所有的vocabulary中的词:</span></span><br><span class="line"><span class="comment">#for word in standard_vocab:</span></span><br><span class="line"><span class="comment">#    print(corpus.tf_idf(word, new_sentence)) # 我们会得到⼀个巨长(=所有vocab⻓度)的向量</span></span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Chatbot Research 1 - 聊天机器人的行业综述]]></title>
      <url>http://sggo.me/2017/08/11/chatbot/chatbot-research1/</url>
      <content type="html"><![CDATA[<p>英文原文： <a href="http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/" target="_blank" rel="external">《Deep Learning For Chatbots, Part 1 - Introduction》</a></p>
<p>中文翻译： <a href="http://www.jeyzhang.com/deep-learning-for-chatbots-1.html" target="_blank" rel="external">《聊天机器人中的深度学习导读》</a></p>
<a id="more"></a>
<p>这篇博文主要概述了目前聊天机器人主要用到的技术，从宏观上进行介绍，不涉及具体的技术细节。</p>
<p>聊天机器人 (Chatbot)，也被称为对话引擎或者对话系统，是目前的热点之一。微软公司在聊天机器人上的投入巨大（<a href="https://www.bloomberg.com/features/2016-microsoft-future-ai-chatbots/" target="_blank" rel="external">链接</a>），著名产品有小冰 (xiaoice)、bot framework等，其他公司也纷纷在这个领域发力，如Facebook的M，苹果公司的Siri等等。此外，一大批的创业公司发布了类似的产品，例如客户应用<a href="https://operator.com/" target="_blank" rel="external">Operator</a>，<a href="https://x.ai/" target="_blank" rel="external">x.ai</a>，bot framework <a href="http://chatfuel.com/" target="_blank" rel="external">Chatfuel</a>，bot开发工具 <a href="http://howdy.ai/botkit/" target="_blank" rel="external">Howdy’s Botkit</a>。微软也发布了供开发者使用的 <a href="https://dev.botframework.com/" target="_blank" rel="external">bot developer framework</a> </p>
<p>还有一些应用案例，列举如下：</p>
<ul>
<li>案例:语音助手 Siri</li>
<li>案例:餐饮 Pizza Hut</li>
<li>案例:健身 Fitness Tips</li>
<li>案例:旅游 <a href="https://viewfinder.expedia.com/features/introducing-expedia-bot-skype/" target="_blank" rel="external">expedia</a></li>
<li>案例:医疗 <a href="https://www.healthtap.com/" target="_blank" rel="external">healthtap</a></li>
<li>案例:新闻 <a href="https://qzprod.wordpress.com/" target="_blank" rel="external">wordpress</a></li>
<li>案例:财经 <a href="https://meetcleo.com/" target="_blank" rel="external">meetcleo</a></li>
</ul>
<p>许多公司希望能开发出与用户进行自然语言式对话的机器人，并且声称使用了NLP和深度学习相关技术使之成为可能，然而这并不容易实现。在这个博文系列中，我将会重温一些被用于聊天机器人中的深度学习技术，披露出目前技术能够解决或者可能解决的问题以及几乎难以解决的问题。这篇文章是个概述，在接下来的博文中将介绍具体的技术细节。</p>
<h2 id="1-模型分类"><a href="#1-模型分类" class="headerlink" title="1. 模型分类"></a>1. 模型分类</h2><h3 id="1-1-检索技术模型-VS-生成式模型"><a href="#1-1-检索技术模型-VS-生成式模型" class="headerlink" title="1.1 检索技术模型 VS 生成式模型"></a>1.1 检索技术模型 VS 生成式模型</h3><p><strong>基于检索技术的模型</strong>较为简单，主要是根据用户的输入和上下文内容，使用了知识库（存储了事先定义好的回复内容）和一些启发式方法来得到一个合适的回复。启发式方法简单的有基于规则的表达式匹配，复杂的有一些机器学习里的分类器。这些系统不能够生成任何新的内容，只是从一个固定的数据集中找到合适的内容作为回复。</p>
<p><strong>生成式模型</strong>则更加复杂，它不依赖于预定义好的回复内容，而是通过<strong>抓取(Scratch)</strong>的方法生成新的回复内容。生成式模型典型的有<strong>基于机器翻译模型</strong>的，与传统机器翻译模型不同的是，生成式模型的任务不是将一句话翻译成其他语言的一句话，而是将<strong>用户的输入[翻译]为一个回答(response)</strong></p>
<p><img src="/images/chatbot/chatbot-1_1.png" width="800"></p>
<h3 id="1-2-模型分类总结"><a href="#1-2-模型分类总结" class="headerlink" title="1.2 模型分类总结"></a>1.2 模型分类总结</h3><p>以上两种模型均有优缺点。对于基于检索技术的模型，由于使用了知识库且数据为预先定义好的，因此进行回复的内容语法上较为通顺，较少出现语法错误；但是基于检索技术的模型中没有会话概念，不能结合上下文给出更加[智能]的回复。而生成式模型则更加[智能]一些，它能够更加有效地利用上下文信息从而知道你在讨论的东西是什么；然而生成式模型比较难以训练，并且输出的内容经常存在一些语法错误（尤其对于长句子而言），以及模型训练需要大规模的数据。</p>
<p>深度学习技术都能够用于基于检索技术的模型和生成式模型中，但是目前的研究热点在生成式模型上。深度学习框架例如Sequence to Sequence非常适合用来生成文本，非常多的研究者希望能够在这个领域取得成功。然而目前这一块的研究还在初期阶段，工业界的产品更多的还是使用基于检索计算的模型。</p>
<h2 id="2-问题分类"><a href="#2-问题分类" class="headerlink" title="2. 问题分类"></a>2. 问题分类</h2><h3 id="2-1-短对话-VS-长对话"><a href="#2-1-短对话-VS-长对话" class="headerlink" title="2.1 短对话 VS 长对话"></a>2.1 短对话 VS 长对话</h3><p>直观上处理长对话内容将更加困难，这是因为你需要在当前对话的情境下知道之前的对话说过什么。如果是一问一答的形式，技术上这将简单的多。通常对于客服对话而言，长对话更加常见，一次对话中往往会伴随着多个关联问题。</p>
<h3 id="2-2-开放域-VS-特定领域"><a href="#2-2-开放域-VS-特定领域" class="headerlink" title="2.2 开放域 VS 特定领域"></a>2.2 开放域 VS 特定领域</h3><p>面向开放域的聊天机器人技术面临更多困难，这是因为会话可能涉及的面太广，没有一个清晰的目标和意图。在一些社交网站例如Twitter和Reddit上的会话是属于开放域的，会话涉及的主题多种多样，需要的知识量也将非常巨大。</p>
<p>面向特定领域的相关技术则相对简单一些，这是因为特定领域给会话的主题进行了限制，目标和意图也更加清晰，典型的例子有客服系统助手和购物助手。这些系统通常是为了完成某些特定任务，尽管用户在该系统中也能够问些其他方面的东西，但是系统并不会给出相应的回复。</p>
<h2 id="3-面临的挑战"><a href="#3-面临的挑战" class="headerlink" title="3. 面临的挑战"></a>3. 面临的挑战</h2><p>下面介绍一下聊天机器人技术所面临的挑战。</p>
<h3 id="3-1-如何结合上下文信息"><a href="#3-1-如何结合上下文信息" class="headerlink" title="3.1 如何结合上下文信息"></a>3.1 如何结合上下文信息</h3><p>为了产生质量更高的回复，聊天机器人系统通常需要利用一些上下文信息(Context)，这里的上下文信息包括了对话过程中的语言上下文信息和用户的身份信息等。在长对话中人们关注的是之前说了什么内容以及产生了什么内容的交换，这是语言上下文信息的典型。常见的方法是将一个会话转化为向量形式，但这对长会话而言是困难的。论文Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models和Attention with Intention for a Neural Network Conversation Model中的实验结果表明了这一点。另外，可以结合的上下文信息还包括会话进行时的日期地点信息、用户信息等。</p>
<h3 id="3-2-语义一致性"><a href="#3-2-语义一致性" class="headerlink" title="3.2 语义一致性"></a>3.2 语义一致性</h3><p>理论上来说，机器人面对相同语义而不同形式的问题应该给予一致的回复，例如这两个问题[How old are you?]和[What’s your age?]。这理解起来是简单的，但却是学术界目前的难题之一（如下图）。许多系统都试图对相同语义而不同形式的问题给予语义上合理的回复，但却没有考虑一致性，最大的原因在于训练模型的数据来源于大量不同的用户，这导致机器人失去了固定统一的人格。论文A Persona-Based Neural Conversation Model中提及的模型旨在创建具有固定统一人格的机器人。</p>
<p><img src="/images/chatbot/chatbot-1_2.png" width="400"></p>
<h3 id="3-3-对话模型的评测"><a href="#3-3-对话模型的评测" class="headerlink" title="3.3 对话模型的评测"></a>3.3 对话模型的评测</h3><p>评价一个对话模型的好坏在于它是否很好地完成了某项任务，例如在对话中解决了客户的问题。这样的训练数据需要人工标注和评测，所以获取上需要一定人力代价。有时在开放域中的对话系统也没有一个清晰的优化目标。用于机器翻译的评测指标BLEU不能适用于此，是因为它的计算基础是语言表面上的匹配程度，而对话中的回答可以是完全不同词型但语义通顺的语句。论文<a href="https://arxiv.org/abs/1603.08023" target="_blank" rel="external">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</a>中给出结论，目前常用的评测指标均与人工评测无关。</p>
<h3 id="3-4-意图和回复多样性"><a href="#3-4-意图和回复多样性" class="headerlink" title="3.4 意图和回复多样性"></a>3.4 意图和回复多样性</h3><p>生成式模型中的一个普遍问题是，它们都想要生成一些通用的回答，例如[That’s great!]和[I don’t know]这样的可以应付许多的用户询问。早期的Google智能回复基本上以[I love you]回复所有的东西链接，这是一些模型最终训练出来的结果，原因在于训练数据和训练的优化目标。因此，有些研究学者开始关注<a href="http://arxiv.org/abs/1510.03055" target="_blank" rel="external">如何提升机器人的回复的多样性</a>，然而人们在对话过程中的回复与询问有一定特定关系，是有一定意图的，而许多面向开放域的机器人不具备特定的意图。</p>
<h3 id="3-5-实际效果"><a href="#3-5-实际效果" class="headerlink" title="3.5 实际效果"></a>3.5 实际效果</h3><p>以目前的研究水平所制造的机器人能够取得的效果如何？使用基于检索技术的显然无法制作出面向开放域的机器人，这是因为你不能编写覆盖所有领域的语料；而生成式的面向开放域的机器人还属于通用人工智能(Artifical General Intelligence, AGI)水平，距离理想状态还相距甚远，但相关研究学者还在致力于此。</p>
<p>对于特定领域的机器人，基于检索的技术和生成式模型都能够利用。但是对于长对话的情境，也面临许多困难。</p>
<p>在最近对<a href="https://www.seattletimes.com/business/baidu-research-chief-andrew-ng-fixed-on-self-taught-computers-self-driving-cars/" target="_blank" rel="external">Andrew NG的采访</a>中，NG提到：</p>
<blockquote>
<p>目前深度学习的价值主要体现在能够获取大量数据的特定领域。</p>
<p>目前一个无法做的事情是产生一个有意义的对话。</p>
<p>详细： 当今深度学习的价值在你可以获得许多数据的狭窄领域内。有一件事它做不到：进行有意义的对话。存在一些演示，并且如果你仔细挑选这些对话，看起来就像它正在进行有意义的对话，但是如果你亲自尝试，它就会快速偏离轨道。</p>
</blockquote>
<p>许多创业公司声称只要有足够多的数据，就能够产生自动智能的对话系统。然而，目前的水平生产出面向一个特定的子领域的对话应用（如利用Uber打车），而对于一个稍微开放点的领域就难以实现了（如自动销售）。但是，帮助用户提供自动回复建议以及语法纠正还是可行的。</p>
<p>使用基于检索技术的对话系统更加可控和稳定，给出的回复出现语法错误的几率更低。而使用生成式模型的风险在于回复不可控，且容易出现一些风险，例如<a href="http://www.businessinsider.com/microsoft-deletes-racist-genocidal-tweets-from-ai-chatbot-tay-2016-3" target="_blank" rel="external">微软的Tay</a>。</p>
<h2 id="4-即将到来的事情和阅读列表"><a href="#4-即将到来的事情和阅读列表" class="headerlink" title="4. 即将到来的事情和阅读列表"></a>4. 即将到来的事情和阅读列表</h2><p>在之后的博文中将具体介绍深度学习方面的技术细节。提前阅读下面的文章将会对后面的学习更加有帮助。</p>
<ul>
<li><p><a href="http://arxiv.org/abs/1503.02364" target="_blank" rel="external">Neural Responding Machine for Short-Text Conversation (2015-03)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1506.05869" target="_blank" rel="external">A Neural Conversational Model (2015-06)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1506.06714" target="_blank" rel="external">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses (2015-06)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1506.08909" target="_blank" rel="external">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems (2015-06)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1507.04808" target="_blank" rel="external">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models (2015-07)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1510.03055" target="_blank" rel="external">A Diversity-Promoting Objective Function for Neural Conversation Models (2015-10)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1510.08565" target="_blank" rel="external">Attention with Intention for a Neural Network Conversation Model (2015-10)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1510.03753" target="_blank" rel="external">Improved Deep Learning Baselines for Ubuntu Corpus Dialogs (2015-10)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1512.05742" target="_blank" rel="external">A Survey of Available Corpora for Building Data-Driven Dialogue Systems (2015-12)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1603.06393" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning (2016-03)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1603.06155" target="_blank" rel="external">A Persona-Based Neural Conversation Model (2016-03)</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1603.08023" target="_blank" rel="external">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation (2016-03)</a></p>
</li>
</ul>
<p>数据驱动的意义是： </p>
<ul>
<li>算法越简单，解释性越好</li>
<li>数据量足够大，覆盖的真实世界的大部分场景</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Naive Bayes * 垃圾邮件分类]]></title>
      <url>http://sggo.me/2017/08/10/ml/4-naive-bayes-1/</url>
      <content type="html"><![CDATA[<p>贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效, 很多高级 NLP 模型也可以从它演化而来。因此，学习贝叶斯方法，是研究 NLP 问题的一个非常好的切入口。</p>
<a id="more"></a>
<h2 id="1-贝叶斯"><a href="#1-贝叶斯" class="headerlink" title="1. 贝叶斯"></a>1. 贝叶斯</h2><p>$$<br>P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}<br>$$</p>
<h2 id="2-贝叶斯公式"><a href="#2-贝叶斯公式" class="headerlink" title="2. 贝叶斯公式"></a>2. 贝叶斯公式</h2><p>贝叶斯公式就一行：</p>
<p>$$<br>P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}<br>$$</p>
<p>它其实是由以下的联合概率公式推导出来：</p>
<p>$$<br>P(Y,X)=P(Y|X)P(X)=P(X|Y)P(Y)<br>$$</p>
<p>其中 $P(Y)$ 叫做先验概率， $P(Y|X)$ 叫做后验概率， $P(Y,X)$ 叫做联合概率。</p>
<h2 id="3-机器学习视角理解贝叶斯公式"><a href="#3-机器学习视角理解贝叶斯公式" class="headerlink" title="3. 机器学习视角理解贝叶斯公式"></a>3. 机器学习视角理解贝叶斯公式</h2><p>把 $X$ 理解成 “$有某 feature$”<br>把 $Y$ 理解成 “$属于某类 label$”</p>
<blockquote>
<p>一般机器学习为题中都是 $X$ =&gt; 特征, $Y$ =&gt; 结果 对吧。</p>
<p>在最简单的二分类问题(是与否判定)下，我们将 $Y$ 理解成 $“属于某类”$ 的标签。<br>于是贝叶斯公式就变形成了下面的样子:</p>
</blockquote>
<p>$$<br>P(“属于某类”|“具有某特征”)=\frac{P(“具有某特征”|“属于某类”)P(“属于某类”)}{P(“具有某特征”)}<br>$$</p>
<p>而我们二分类问题的最终目的就是要判断 $P(“属于某类”|“具有某特征”)$ 是否大于1/2就够了。贝叶斯方法把计算 “$具有某特征的条件下属于某类$” 的概率转换成需要计算 “$属于某类的条件下具有某特征$” 的概率，而后者获取方法就简单多了，我们只需要找到一些包含已知特征标签的样本，即可进行训练。而样本的类别标签都是明确的，所以贝叶斯方法在机器学习里属于<code>有监督学习方法</code>。</p>
<h2 id="4-垃圾邮件识别"><a href="#4-垃圾邮件识别" class="headerlink" title="4. 垃圾邮件识别"></a>4. 垃圾邮件识别</h2><p>举个栗子 🌰</p>
<p>我们现在要对邮件进行分类，识别垃圾邮件和普通邮件，如果我们选择使用朴素贝叶斯分类器，那目标就是判断 $P(“垃圾邮件”|“具有某特征”)$ 是否大于1/2。</p>
<p>假设我们有垃圾邮件和正常邮件各1万封作为训练集。需要判断以下这个邮件是否属于垃圾邮件：</p>
<blockquote>
<p>“我司可办理正规发票（保真）17%增值税发票点数优惠！”</p>
</blockquote>
<p>也就是判断概率 $P(“垃圾邮件”|“我司可办理正规发票（保真）17\%增值税发票点数优惠！”)$ 是否大于1/2。</p>
<p>$$<br>P = \frac{垃圾邮件中出现这句话的次数}{垃圾邮件中出现这句话的次数+正常邮件中出现这句话的次数}<br>$$</p>
<blockquote>
<p>咳咳，有木有发现，转换成的这个概率，计算的方法：就是写个计数器，然后+1 +1 +1统计出所有垃圾邮件和正常邮件中出现这句话的次数啊！！！</p>
</blockquote>
<h2 id="5-分词"><a href="#5-分词" class="headerlink" title="5. 分词"></a>5. 分词</h2><p>一个很悲哀但是很现实的结论：<code>训练集是有限的，而句子的可能性则是无限的。所以覆盖所有句子可能性的训练集是不存在的</code>。</p>
<p>所以解决方法是？ $句子的可能性无限，但是词语就那么些$！！汉语常用字2500个，常用词语也就56000个(你终于明白小学语文老师的用心良苦了)。按人们的经验理解，两句话意思相近并不强求非得每个字、词语都一样。比如 $“我司可办理正规发票，17%增值税发票点数优惠！”$，这句话就比之前那句话少了“（保真）”这个词，但是意思基本一样。如果把这些情况也考虑进来，那样本数量就会增加，这就不方便我们计算了。</p>
<p>于是，我们可以不拿句子作为特征，而是拿句子里面的词语（组合）作为特征去考虑。比如 “$正规发票$” 可以作为一个单独的词语，“$增值税$” 也可以作为一个单独的词语等等。</p>
<blockquote>
<p>句子“我司可办理正规发票，17%增值税发票点数优惠！”就可以变成（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）。</p>
</blockquote>
<p>于是你接触到了中文NLP中，最最最重要的技术之一：<strong><code>分词</code></strong>！！！也就是把一整句话拆分成更细粒度的词语来进行表示。另外，分词之后去除标点符号、数字甚至无关成分(停用词)是特征预处理中的一项技术。  </p>
<p>中文分词是一个专门的技术领域(我不会告诉你某搜索引擎厂码砖工有专门做分词的！！！)  </p>
<p>我们观察$（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)$，这可以理解成一个向量：向量的每一维度都表示着该 $特征词$ 在文本中的特定位置存在。这种将特征拆分成更小的单元，依据这些更灵活、更细粒度的特征进行判断的思维方式，在自然语言处理与机器学习中都是非常常见又有效的。</p>
<p>因此贝叶斯公式就变成了：</p>
<p>$$<br>P(“垃圾邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）<br>$$</p>
<p>$$<br>=\frac{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”垃圾邮件”）P(“垃圾邮件”)}{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)) }<br>$$</p>
<hr>
<p>$$<br>P(“正常邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）<br>$$</p>
<p>$$<br>=\frac{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”正常邮件”）P(“正常邮件”)}{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)) }<br>$$</p>
<h2 id="6-条件独立假设"><a href="#6-条件独立假设" class="headerlink" title="6. 条件独立假设"></a>6. 条件独立假设</h2><p>下面我们马上会看到一个非常简单粗暴的假设。</p>
<p>$P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”垃圾邮件”）$ 依旧不够好求，我们引进一个很朴素的近似。为了让公式显得更加紧凑，我们令字母 <code>S</code> 表示“垃圾邮件”,令字母 <code>H</code> 表示“正常邮件”。近似公式如下：</p>
<p>$$<br>P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|S）<br>$$</p>
<p>$$<br>=P(“我”|S）×P(“司”|S）×P(“可”|S）×P(“办理”|S）×P(“正规发票”|S）<br>$$</p>
<p>$$<br>×P(“保真”|S）×P(“增值税”|S）×P(“发票”|S）×P(“点数”|S）×P(“优惠”|S)<br>$$</p>
<p>这就是传说中的条件独立假设。基于“正常邮件”的条件独立假设的式子与上式类似，此处省去。接着，将条件独立假设代入上面两个相反事件的贝叶斯公式。 </p>
<p>于是我们就只需要比较以下两个式子的大小：</p>
<p>$$<br>C = P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)<br>$$</p>
<p>$$<br>×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”)<br>$$</p>
<hr>
<p>$$<br>\overline{C}=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H)<br>$$</p>
<p>$$<br>×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”)<br>$$</p>
<p>厉(wo)害(cao)！酱紫处理后式子中的每一项都特别好求！只需要分别统计各类邮件中该关键词出现的概率就可以了！！！比如：</p>
<blockquote>
<p>$$<br>P(“发票”|S）=\frac{垃圾邮件中所有“发票”的次数}{垃圾邮件中所有词语的次数}<br>$$</p>
</blockquote>
<p>统计次数非常方便，而且样本数量足够大，算出来的概率比较接近真实。于是垃圾邮件识别的问题就可解了。</p>
<h2 id="7-Naive-Bayes，“Naive”在何处？"><a href="#7-Naive-Bayes，“Naive”在何处？" class="headerlink" title="7. Naive Bayes，“Naive”在何处？"></a>7. Naive Bayes，“Naive”在何处？</h2><p>加上条件独立假设的贝叶斯方法就是朴素贝叶斯方法（Naive Bayes）。 Naive的发音是“乃一污”，意思是“朴素的”、“幼稚的”、“蠢蠢的”。咳咳，也就是说，大神们取名说该方法是一种比较萌蠢的方法，为啥？</p>
<p>将句子（“我”,“司”,“可”,“办理”,“正规发票”) 中的 （“我”,“司”）与（“正规发票”）调换一下顺序，就变成了一个新的句子（“正规发票”,“可”,“办理”, “我”, “司”)。新句子与旧句子的意思完全不同。但由于乘法交换律，朴素贝叶斯方法中算出来二者的条件概率完全一样！计算过程如下：</p>
<p>$$<br>P(（“我”,“司”,“可”,“办理”,“正规发票”)|S) = P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)<br>$$</p>
<p>$$<br>=P(“正规发票”|S)P(“可”|S)P(“办理”|S)P(“我”|S)P(“司”|S） =P(（“正规发票”,“可”,“办理”, “我”, “司”)|S)<br>$$</p>
<p>也就是说，在朴素贝叶斯眼里，“<code>我司可办理正规发票</code>” 与 “<code>正规发票可办理我司</code>” 完全相同。朴素贝叶斯失去了词语之间的顺序信息。这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作词袋子模型(<code>bag of words</code>)。</p>
<p>词袋子模型与人们的日常经验完全不同。比如，在条件独立假设的情况下，“<code>武松打死了老虎</code>” 与 “<code>老虎打死了武松</code>” 被它认作一个意思了。恩，朴素贝叶斯就是这么单纯和直接，对比于其他分类器，好像是显得有那么点萌蠢</p>
<h2 id="8-简单高效，吊丝逆袭"><a href="#8-简单高效，吊丝逆袭" class="headerlink" title="8. 简单高效，吊丝逆袭"></a>8. 简单高效，吊丝逆袭</h2><p>虽然说朴素贝叶斯方法萌蠢萌蠢的，但实践证明在垃圾邮件识别的应用还令人诧异地好。Paul Graham先生自己简单做了一个朴素贝叶斯分类器，“1000封垃圾邮件能够被过滤掉995封，并且没有一个误判”。（Paul Graham《黑客与画家》）</p>
<p>那个…效果为啥好呢？</p>
<blockquote>
<p>“有人对此提出了一个理论解释，并且建立了什么时候朴素贝叶斯的效果能够等价于非朴素贝叶斯的充要条件，这个解释的核心就是：有些独立假设在各个分类之间的分布都是均匀的所以对于似然的相对大小不产生影响；即便不是如此，也有很大的可能性各个独立假设所产生的消极影响或积极影响互相抵消，最终导致结果受到的影响不大。具体的数学公式请参考[这篇 paper][2]。”（刘未鹏《：平凡而又神奇的贝叶斯方法》）</p>
</blockquote>
<p>恩，这个分类器中最简单直接看似萌蠢的小盆友『朴素贝叶斯』，实际上却是简单、实用、且强大的。</p>
<h2 id="9-处理重复词语的三种方式"><a href="#9-处理重复词语的三种方式" class="headerlink" title="9. 处理重复词语的三种方式"></a>9. 处理重复词语的三种方式</h2><p>我们之前的 <font color="blue"> 垃圾邮件向量（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”) </font>，其中每个词都不重复。而这在现实中其实很少见。因为如果文本长度增加，或者分词方法改变，必然会有许多词重复出现，因此需要对这种情况进行进一步探讨。比如以下这段邮件：</p>
<blockquote>
<p>“代开发票。增值税发票，正规发票。” 分词后为向量： （“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”）</p>
</blockquote>
<p>其中“发票”重复了三次。</p>
<h3 id="9-1-多项式模型"><a href="#9-1-多项式模型" class="headerlink" title="9.1 多项式模型"></a>9.1 多项式模型</h3><p>如果我们考虑重复词语的情况，<font color="blue">重复的词语我们视为其出现多次</font>，直接按条件独立假设的方式推导，则有</p>
<p>$$<br>P(（“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”)|S）<br>$$</p>
<p>$$<br>=P(“代开””|S)P(“发票”|S)P(“增值税”|S)P(“发票”|S)P(“正规”|S)P(“发票”|S）<br>$$</p>
<p>$$<br>=P(“代开””|S)P^3(“发票”|S)P(“增值税”|S)P(“正规”|S)<br>$$</p>
<blockquote>
<p>注意这项 $P^3(“发票”|S)$</p>
</blockquote>
<h3 id="9-2-伯努利模型"><a href="#9-2-伯努利模型" class="headerlink" title="9.2 伯努利模型"></a>9.2 伯努利模型</h3><p>另一种更加简化的方法是将重复的词语都视为其只出现1次，</p>
<p>$$<br>P(（“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”)|S）<br>$$</p>
<p>$$<br>=P(“发票”|S)P(“代开””|S)P(“增值税”|S)P(“正规”|S）<br>$$</p>
<p>统计计算 $P(“词语”|S)$ 时也是如此。</p>
<p>$$<br>P(“发票”|S）=\frac{出现“发票”的垃圾邮件的封数}{每封垃圾邮件中所有词出现次数（出现了只计算一次）的总和}<br>$$</p>
<p>这样的模型叫作 <font color="blue">伯努利模型</font>（又称为二项独立模型）。这种方式更加简化与方便。当然它丢失了词频的信息，因此效果可能会差一些。</p>
<h3 id="9-3-混合模型"><a href="#9-3-混合模型" class="headerlink" title="9.3 混合模型"></a>9.3 混合模型</h3><p>第三种方式是在计算句子概率时，不考虑重复词语出现的次数，但是在统计计算词语的概率$P(“词语”|S)$ 时，却考虑重复词语的出现次数，这样的模型可以叫作混合模型。</p>
<p><img src="/images/ml/bayes/bayes-03.jpg" alt=""></p>
<p>具体实践中采用那种模型，关键看具体的业务场景，一个简单经验是，对于垃圾邮件识别，混合模型更好些。</p>
<h2 id="10-去除停用词与选择关键词"><a href="#10-去除停用词与选择关键词" class="headerlink" title="10. 去除停用词与选择关键词"></a>10. 去除停用词与选择关键词</h2><p>我们继续观察<strong>（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)</strong> 这句话。其实，像<strong>“我”、“可”</strong>之类词其实非常中性，无论其是否出现在垃圾邮件中都无法帮助判断的有用信息。所以可以直接不考虑这些典型的词。这些无助于我们分类的词语叫作 “停用词”（<code>Stop Words</code>）。这样可以减少我们训练模型、判断分类的时间。 于是之前的句子就变成了<strong>（“司”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”) </strong>。</p>
<p>我们进一步分析。以人类的经验，其实 <strong>“正规发票”、“发票”</strong> 这类的词如果出现的话，邮件作为垃圾邮件的概率非常大，可以作为我们区分垃圾邮件的“关键词”。而像 <strong>“司”、“办理”、“优惠”</strong> 这类的词则有点鸡肋，可能有助于分类，但又不那么强烈。如果想省事做个简单的分类器的话，则可以直接采用“关键词”进行统计与判断，剩下的词就可以先不管了。于是之前的垃圾邮件句子就变成了<strong>（“正规发票”,“发票”)</strong> 。这样就更加减少了我们训练模型、判断分类的时间，速度非常快。</p>
<p><strong>“停用词”和“关键词”一般都可以提前靠人工经验指定</strong>。不同的“停用词”和“关键词”训练出来<strong>的分类器</strong>的效果也会有些差异。</p>
<h2 id="11-浅谈平滑技术"><a href="#11-浅谈平滑技术" class="headerlink" title="11. 浅谈平滑技术"></a>11. 浅谈平滑技术</h2><p>我们来说个问题(中文NLP里问题超级多)，比如在计算以下独立条件假设的概率：</p>
<p>$$<br>P(（“我”,“司”,“可”,“办理”,“正规发票”)|S)<br>$$</p>
<p>$$<br>=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S）<br>$$</p>
<p>我们扫描一下训练集，发现 <strong>“正规发票”这个词从出现过！！！，于是 $P(“正规发票”|S）=0$ …问题严重了，整个概率都变成0了！！！朴素贝叶斯方法面对一堆0，很凄惨地失效了…更残酷的是</strong> 这种情况其实很常见，<strong>因为哪怕训练集再大，也可能有覆盖不到的词语。本质上还是</strong>样本数量太少，不满足大数定律，计算出来的概率失真。为了解决这样的问题，一种分析思路就是直接不考虑这样的词语，但这种方法就相当于默认给 $P(“正规发票”|S）$ 赋值为1。其实效果不太好，大量的统计信息给浪费掉了。我们进一步分析，既然可以默认赋值为1，为什么不能默认赋值为一个很小的数？这就是平滑技术的基本思路，依旧保持着一贯的作风，朴实/土但是直接而有效。</p>
<p>对于伯努利模型，$P(“正规发票”|S)$ 的一种平滑算法是：</p>
<p>$$<br>P(“正规发票”|S）=\frac{出现“正规发票”的垃圾邮件的封数+1}{每封垃圾邮件中所有词出现次数（出现了只计算一次）的总和+2}<br>$$</p>
<p>对于多项式模型，$P(“正规发票”| S)$ 的一种平滑算法是：</p>
<p>$$<br>P(“发票”|S）=\frac{每封垃圾邮件中出现“发票”的次数的总和+1}{每封垃圾邮件中所有词出现次数（计算重复次数）的总和+被统计的词表的词语数量}<br>$$</p>
<p>说起来，平滑技术的种类其实非常多，有兴趣的话回头我们专门拉个专题讲讲好了。这里只提一点，就是所有的<strong>平滑技术都是给未出现在训练集中的词语一个估计的概率，而相应地调低其他已经出现的词语的概率</strong>。</p>
<p>平滑技术是因为数据集太小而产生的现实需求。<strong>如果数据集足够大，平滑技术对结果的影响将会变小</strong>。</p>
<h2 id="12-内容小结"><a href="#12-内容小结" class="headerlink" title="12. 内容小结"></a>12. 内容小结</h2><p>我们找了个最简单常见的例子：垃圾邮件识别，说明了一下朴素贝叶斯进行文本分类的思路过程。基本思路是先区分好训练集与测试集，对文本集合进行分词、去除标点符号等特征预处理的操作，然后使用条件独立假设，将原概率转换成词概率乘积，再进行后续的处理。</p>
<p>$$<br>贝叶斯公式 + 条件独立假设 = 朴素贝叶斯方法<br>$$</p>
<p>基于对重复词语在训练阶段与判断（测试）阶段的三种不同处理方式，我们相应的有伯努利模型、多项式模型和混合模型。在训练阶段，如果样本集合太小导致某些词语并未出现，我们可以采用平滑技术对其概率给一个估计值。而且并不是所有的词语都需要统计，我们可以按相应的“停用词”和“关键词”对模型进行进一步简化，提高训练和判断速度。</p>
<h2 id="13-匹配关键词识别spam？"><a href="#13-匹配关键词识别spam？" class="headerlink" title="13. 匹配关键词识别spam？"></a>13. 匹配关键词识别spam？</h2><p>有同学可能会问：“何必费这么大劲算那么多词的概率？直接看邮件中有没有‘代开发票’、‘转售发票’之类的关键词不就得了？如果关键词比较多就认为是垃圾邮件呗。”</p>
<p>其实关键词匹配的方法如果有效的话真不必用朴素贝叶斯。毕竟这种方法简单嘛，就是一个字符串匹配。从历史来看，之前没有贝叶斯方法的时候主要也是用关键词匹配。但是这种方法准确率太低。我们在工作项目中也尝试过用关键词匹配的方法去进行文本分类，发现大量误报。感觉就像扔到垃圾箱的邮件99%都是正常的！这样的效果不忍直视。而加一个朴素贝叶斯方法就可能把误报率拉低近一个数量级，体验好得不要不要的。</p>
<p>另一个原因是词语会随着时间不断变化。发垃圾邮件的人也不傻，当他们发现自己的邮件被大量屏蔽之后，也会考虑采用新的方式，如变换文字、词语、句式、颜色等方式来绕过反垃圾邮件系统。比如对于垃圾邮件“我司可办理正规发票，17%增值税发票点数优惠”,他们采用<strong>火星文：“涐司岢办理㊣規髮票，17%增値稅髮票嚸數優蕙”</strong>，那么字符串匹配的方法又要重新找出这些火星文，一个一个找出关键词，重新写一些匹配规则。更可怕的是，这些规则可能相互之间的耦合关系异常复杂，要把它们梳理清楚又是大一个数量级的工作量。等这些规则失效了又要手动更新新的规则……<strong>无穷无尽猫鼠游戏最终会把猫给累死</strong>。</p>
<p>而朴素贝叶斯方法却显示出无比的优势。因为它是基于统计方法的，只要训练样本中有更新的垃圾邮件的新词语，哪怕它们是火星文，都能自动地把哪些更敏感的词语（如“髮”、“㊣”等）给凸显出来，并根据统计意义上的敏感性给他们分配适当的权重 ，这样就不需要什么人工了，非常省事。你只需要时不时地拿一些最新的样本扔到训练集中，重新训练一次即可。</p>
<blockquote>
<p>小补充一下，对于火星文、同音字等替代语言，一般的分词技术可能会分得不准，最终可能只把一个一个字给分出来，成为“分字”。效果可能不会太好。也可以用过n-gram之类的语言模型，拿到最常见短语。当然，对于英文等天生自带空格来间隔单词的语言，分词则不是什么问题，使用朴素贝叶斯方法将会更加顺畅。</p>
</blockquote>
<h2 id="14-实际工程的tricks"><a href="#14-实际工程的tricks" class="headerlink" title="14. 实际工程的tricks"></a>14. 实际工程的tricks</h2><p>应用朴素贝叶斯方法的过程中，一些tricks能显著帮助工程解决问题。我们毕竟经验有限，无法将它们全都罗列出来，只能就所知的一点点经验与大家分享，欢迎批评指正。</p>
<h3 id="14-1-trick1：取对数"><a href="#14-1-trick1：取对数" class="headerlink" title="14.1 trick1：取对数"></a>14.1 trick1：取对数</h3><p>我们提到用来识别垃圾邮件的方法是比较以下两个概率的大小（字母S表示“垃圾邮件”,字母H表示“正常邮件”）：</p>
<p>$$<br>C = P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)<br>$$</p>
<p>$$<br>×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”)<br>$$</p>
<p>$$<br>\overline{C}=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H)<br>$$</p>
<p>$$<br>×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”)<br>$$</p>
<p>但这里进行了<strong>很多乘法运算，计算的时间开销比较大</strong>。尤其是对于篇幅比较长的邮件，几万个数相乘起来还是非常花时间的。如果能把<strong>这些乘法变成加法则方便得多</strong>。刚好数学中的对数函数log就可以实现这样的功能。两边同时取对数（本文统一取底数为2），则上面的公式变为：</p>
<p>$$<br>log{C} = log{P(“我”|S)}+log{P(“司”|S)}+log{P(“可”|S)}+log{P(“办理”|S)}+log{P(“正规发票”|S)}<br>$$</p>
<p>$$<br>+log{P(“保真”|S)}+log{P(“增值税”|S)}+log{P(“发票”|S)}+log{P(“点数”|S)}+log{P(“优惠”|S)}+log{P(“垃圾邮件”)}<br>$$</p>
<p>$$<br>log{\overline{C}}=log{P(“我”|H)}+log{P(“司”|H)}+log{P(“可”|H)}+log{P(“办理”|H)}+log{P(“正规发票”|H)}<br>$$</p>
<p>$$<br>+log{P(“保真”|H)}+log{P(“增值税”|H)}+log{P(“发票”|H)}+log{P(“点数”|H)}+log{P(“优惠”|H)}+log{P(“正常邮件”)}<br>$$</p>
<p>有同学可能要叫了：“做对数运算岂不会也很花时间？”的确如此，但是可以在训练阶段直接计算  $logP$  ，然后把他们存在一张大的hash表里。<strong>在判断的时候直接提取hash表中已经计算好的对数概率，然后相加即可。这样使得判断所需要的计算时间被转移到了训练阶段</strong>，实时运行的时候速度就比之前快得多，这可不止几个数量级的提升。</p>
<h3 id="14-2-trick2：转换为权重"><a href="#14-2-trick2：转换为权重" class="headerlink" title="14.2 trick2：转换为权重"></a>14.2 trick2：转换为权重</h3><p>对于二分类，我们还可以继续提高判断的速度。既然要比较 $log$ 和 $log{\overline{C}}$ 的大小，那就可以直接将上下两式相减，并继续化简：</p>
<p>$$<br>log{\frac{C}{\overline{C}}}=log{\frac{P(“我”|S)}{P(“我”|H)}}+log{\frac{P(“司”|S)}{P(“司”|H)}}+log{\frac{P(“可”|S)}{P(“可”|H)}}+log{\frac{P(“办理”|S)}{P(“办理”|H)}}+log{\frac{P(“正规发票”|S)}{P(“正规发票”|H)}}<br>$$</p>
<p>$$<br>+log{\frac{P(“保真”|S)}{P(“保真”|H)}}+log{\frac{P(“增值税”|S)}{P(“增值税”|H)}}+log{\frac{P(“发票”|S)}{P(“发票”|H)}}+log{\frac{P(“点数”|S)}{P(“点数”|H)}}+log{\frac{P(“优惠”|S)}{P(“优惠”|H)}}+log{\frac{P(“正常邮件”|S)}{P(“正常邮件”)}}<br>$$</p>
<p>$log{\frac{C}{\overline{C}}}$如果大于0则属于垃圾邮件。我们可以把其中每一项作为其对应词语的权重，比如 $log{\frac{P(“发票”|S)}{P(“发票”|H)}}$  就可以作为词语“发票”的权重，权重越大就越说明“发票”更可能是与“垃圾邮件”相关的特征。这样可以根据权重的大小来评估和筛选显著的特征，比如关键词。而这些权重值可以直接提前计算好而存在hash表中 。判断的时候直接将权重求和即可。</p>
<p>关键词hash表的样子如下，左列是权重，右列是其对应的词语，权重越高的说明越“关键”：</p>
<p><img src="/images/ml/bayes/bayes-04.jpg" alt=""></p>
<h3 id="14-3-trick3：选取topk的关键词"><a href="#14-3-trick3：选取topk的关键词" class="headerlink" title="14.3 trick3：选取topk的关键词"></a>14.3 trick3：选取topk的关键词</h3><p>前文说过可以通过提前选取关键词来提高判断的速度。有一种方法可以省略提前选取关键词的步骤，就是直接选取一段文本中权重最高的K个词语，将其权重进行加和。比如Paul Graham 在《黑客与画家》中是选取邮件中权重最高的15个词语计算的。</p>
<p>通过权重hash表可知，如果是所有词语的权重，则权重有正有负。如果只选择权重最高的 $K$ 个词语，则它们的权重基本都是正的。所以就不能像之前那样判断 $log{\frac{C}{\overline{C}}}$ 是否大于0来区分邮件了。而这需要依靠经验选定一个正数的阈值（门槛值） ，依据 $log{\frac{C}{\overline{C}}}$  与该门槛值的大小来识别垃圾邮件。</p>
<p>如下图所示，蓝色点代表垃圾邮件，绿色点代表正常邮件，横坐标为计算出来的 $log{\frac{C}{\overline{C}}}$  值，中间的红线代表阈值。</p>
<p><img src="/images/ml/bayes/bayes-07.jpg" style="width:500px; height:250px;"> </p>
<blockquote>
<p>k 的选取，需要你自己判断。可以通过交叉验证来判断。</p>
</blockquote>
<h3 id="14-4-trick4：分割样本"><a href="#14-4-trick4：分割样本" class="headerlink" title="14.4 trick4：分割样本"></a>14.4 trick4：分割样本</h3><p>选取topk个词语的方法对于篇幅变动不大的邮件样本比较有效。但是对篇幅过大或者过小的邮件则会有判断误差。</p>
<p>比如这个垃圾邮件的例子：（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)。分词出了10个词语，其中有“正规发票”、“发票”2个关键词。关键词的密度还是蛮大的，应该算是敏感邮件。但因为采用最高15个词语的权重求和，并且相应的阈值是基于15个词的情况有效，可能算出来的结果还小于之前的阈值，这就造成漏判了。  </p>
<p>类似的，如果一封税务主题的邮件有1000个词语，其中只有“正规发票”、“发票”、“避税方法”3个权重比较大的词语，它们只是在正文表述中顺带提到的内容。关键词的密度被较长的篇幅稀释了，应该算是正常邮件。但是却被阈值判断成敏感邮件，造成误判了。<br>这两种情况都说明topk关键词的方法需要考虑篇幅的影响。这里有许多种处理方式，它们的基本思想都是选取词语的个数及对应的阈值要与篇幅的大小成正比，本文只介绍其中一种方方法：  </p>
<p>对于长篇幅邮件，按一定的大小，比如每500字，将其分割成小的文本段落，再对小文本段落采用topk关键词的方法。只要其中有一个小文本段落超过阈值就判断整封邮件是垃圾邮件。  </p>
<p>对于超短篇幅邮件，比如50字，可以按篇幅与标准比较篇幅的比例来选取topk，以确定应该匹配关键词语的个数。比如选取  50500×15≈250500×15≈2  个词语进行匹配，相应的阈值可以是之前阈值的  215215  。以此来判断则更合理。</p>
<h3 id="14-5-trick5：位置权重"><a href="#14-5-trick5：位置权重" class="headerlink" title="14.5 trick5：位置权重"></a>14.5 trick5：位置权重</h3><h3 id="14-6-trick6：蜜罐"><a href="#14-6-trick6：蜜罐" class="headerlink" title="14.6 trick6：蜜罐"></a>14.6 trick6：蜜罐</h3><p>我们通过辛辛苦苦的统计与计算，好不容易得到了不同词语的权重。然而这并不是一劳永逸的。我们我们之前交代过，词语及其权重会随着时间不断变化，需要时不时地用最新的样本来训练以更新词语及其权重。</p>
<p>而搜集最新垃圾邮件有一个技巧，就是随便注册一些邮箱，然后将它们公布在各大论坛上。接下来就坐等一个月，到时候收到的邮件就绝大部分都是垃圾邮件了（好奸诈）。再找一些正常的邮件，基本就能够训练了。这些用于自动搜集垃圾邮件的邮箱叫做“蜜罐”。“蜜罐”是网络安全领域常用的手段，因其原理类似诱捕昆虫的装有蜜的罐子而得名。比如杀毒软件公司会利用蜜罐来监视或获得计算机网络中的病毒样本、攻击行为等。</p>
<h2 id="15-贝叶斯方法的思维方式"><a href="#15-贝叶斯方法的思维方式" class="headerlink" title="15. 贝叶斯方法的思维方式"></a>15. 贝叶斯方法的思维方式</h2><h3 id="15-1-逆概问题"><a href="#15-1-逆概问题" class="headerlink" title="15.1 逆概问题"></a>15.1 逆概问题</h3><p>$$<br>P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}<br>$$</p>
<p>先不考虑先验概率 $P(Y)$ 与 $P(X)$ ，观察两个后验概率 $P(Y|X)$ 与 $P(X|Y)$，可见贝叶斯公式能够揭示两个相反方向的条件概率之间的转换关系。</p>
<ul>
<li><p>从贝叶斯历史来看, 其就是为了处理所谓 “<code>逆概</code>” 问题而诞生的。比如 $P(Y|X)$  不能通过直接观测来得到结果，而 $P(X|Y)$ 却容易通过直接观测得到结果，就可以通过贝叶斯公式从间接地观测对象去推断不可直接观测的对象的情况。</p>
</li>
<li><p><code>人话版本</code>: 基于邮件的文本内容判断其属于垃圾邮件的概率不好求（不可通过直接观测、统计得到），但是基于已经搜集好的垃圾邮件样本，去统计（直接观测）其文本内部各个词语的概率却非常方便。这就可以用贝叶斯方法。</p>
</li>
<li><p>引申一步: 基于样本特征去判断其所属标签的概率不好求，但是基于已经搜集好的打上标签的样本（有监督），却可以直接统计属于同一标签的样本内部各个特征的概率分布。因此贝叶斯方法的理论视角适用于一切分类问题的求解。</p>
</li>
</ul>
<h3 id="15-2-多分类问题"><a href="#15-2-多分类问题" class="headerlink" title="15.2 多分类问题"></a>15.2 多分类问题</h3><ol>
<li>垃圾邮件</li>
<li>私人邮件 (正常)</li>
<li>工作邮件 (正常)</li>
</ol>
<p>现在有这 3 类邮件各 1 万封作为样本。需要训练出一个贝叶斯分类器。这里依次用 $Y_1,Y_2,Y_3$ 表示这三类邮件，用 $X$ 表示被判断的邮件。套用贝叶斯公式有：</p>
<p>$$<br>P(Y_1|X)=\frac{P(X|Y_1)P(Y_1)}{P(X)}<br>$$</p>
<p>$$<br>P(Y_2|X)=\frac{P(X|Y_2)P(Y_2)}{P(X)}<br>$$</p>
<p>$$<br>P(Y_3|X)=\frac{P(X|Y_3)P(Y_3)}{P(X)}<br>$$</p>
<p>通过比较 <strong>3</strong> 个概率值的大小即可得到 $X$ 所属的分类。发现三个式子的分母 $P(X)$ 一样，比较大小时可以忽略不计，于是就可以用下面这一个式子表达上面 <strong>3</strong> 式：</p>
<p>$$<br>P(Y_i|X)\propto P(X|Y_i)P(Y_i)  ； i=1,2,3<br>$$</p>
<p>其中 $\propto$ 表示“正比于”。而 $P(X|Y_i)$ 则有个特别高逼格的名字叫做 “<strong>似然函数</strong>”。其实把它直接理解成“ $P(Yi|X)$ 的逆反条件概率” 就方便了。</p>
<blockquote>
<p>对于任意多分类的问题都可以用这样的思路去理解。比如 <strong>新闻分类、情感喜怒哀乐分类</strong> 等等。</p>
</blockquote>
<h3 id="15-3-先验概率的问题"><a href="#15-3-先验概率的问题" class="headerlink" title="15.3 先验概率的问题"></a>15.3 先验概率的问题</h3><p>在垃圾邮件的例子中，先验概率都相等， $P(Y_1)=P(Y_2)=P(Y_3)=10000/30000=1/3$，所以上面是式子又可以进一步化简：</p>
<p>$$<br>P(Y_i|X)\propto P(X|Y_i)  ； i=1,2,3<br>$$</p>
<p>只需比较右边式子（也就是“似然函数”）的大小就可以了。这种方法就是传说中的<strong>最大似然法</strong>: 不考虑先验概率而直接比较似然函数。</p>
<p>关于选出最佳分类 $Y_i$ 是否要考虑先验概率 $P(Y_i)$ 的问题，曾经在频率学派和贝叶斯学派之间产生了激烈的教派冲突。统计学家（频率学派）说：我们让数据自己说话。言下之意就是要摒弃先验概率。而贝叶斯学派支持者则说：数据会有各种各样的偏差，而一个<strong>靠谱的先验概率</strong>则可以对这些随机噪音做到健壮.</p>
<p>比如我们在采集垃圾邮件样本的时候，不小心delete掉了一半的数据，就剩下5000封邮件。则计算出来的先验概率为:</p>
<p>$$<br>P(Y_1)=5000/25000=1/5<br>$$</p>
<p>$$<br>P(Y_2)=P(Y_3)=10000/25000=2/5<br>$$</p>
<p>如果还用贝叶斯方法,就要在似然函数后面乘上先验概率。比如之前用最大似然法算出 $Y_1$  垃圾邮件的概率大，但是因为 $P(Y_1)$ 特别小，用贝叶斯方法得出的结果是 $Y_2$  私人邮件的概率大。那相信哪个呢？其实，我们删掉了部分带标签的样本，从计算结果看 $P(Y1)$，$P(Y2)$，$P(Y3)$ 的概率分布变化了，但实际上<strong>这三个类别的真实分布应该是一个客观的状态，不应该因为我们的计算方法而发生变化</strong>。所以是我们计算出来的先验概率失真，应该放弃这样计算出来的先验概率，而用最大似然法。</p>
<p>但即便我们不删掉一半垃圾邮件，这三类邮件的分布就真的是 $1:1:1$ 那样平均吗？那也未必。<strong>我们只是按1:1:1这样的方式进行了抽样而已，真正在邮箱里收到的这三类邮件的分布可能并不是这样。也就是说，在我们对于先验概率一无所知时，只能假设每种猜测的先验概率是均等的（其实这也是人类经验的结果），这个时候就只有用最大似然了</strong>。在现实运用过程中如果发现最大似然法有偏差，可以考虑对不同的似然函数设定一些系数或者阈值，使其接近真实情况。</p>
<p>但是，<strong>如果我们有足够的自信，训练集中这三类的样本分布的确很接近真实的情况，这时就应该用贝叶斯方法</strong>。难怪前面的贝叶斯学派强调的是“靠谱的先验概率”。所以说<strong>贝叶斯学派的适用范围更广，关键要先验概率靠谱，而频率学派有效的前提也是他们的先验概率同样是经验统计的结果</strong>。</p>
<h2 id="16-朴素-贝叶斯方法的常见应用"><a href="#16-朴素-贝叶斯方法的常见应用" class="headerlink" title="16. (朴素)贝叶斯方法的常见应用"></a>16. (朴素)贝叶斯方法的常见应用</h2><p><strong>褒贬分析</strong></p>
<p>一个比较常见的应用场景是情感褒贬分析。比如你要统计微博上人们对一个新上映电影的褒贬程度评价：好片还是烂片。但是一条一条地看微博是根本看不过来，只能用自动化的方法。我们可以有一个很粗略的思路：</p>
<ul>
<li>首先是用爬虫将微博上提到这个电影名字的微博全都抓取下来，比如有10万条。</li>
<li>然后用训练好的朴素贝叶斯分类器分别判断这些微博对电影是好评还是差评。</li>
<li>最后统计出这些好评的影评占所有样本中的比例，就能形成微博网友对这个电影综合评价的大致估计。</li>
</ul>
<p>接下来的核心问题就是训练出一个靠谱的分类器。首先需要有打好标签的文本。这个好找，豆瓣影评上就有大量网友对之前电影的评价，并且对电影进行1星到5星的评价。我们可以认为3星以上的评论都是好评，3星以下的评论都是差评。这样就分别得到了好评差评两类的语料样本。剩下就可以用朴素贝叶斯方法进行训练了。基本思路如下：</p>
<ul>
<li>训练与测试样本：豆瓣影评的网友评论，用爬虫抓取下100万条。</li>
<li>标签：3星以上的是好评，3星以下的是差评。</li>
<li>特征：豆瓣评论分词后的词语。一个简单的方法是只选择其中的形容词，网上有大量的情绪词库可以为我们所用。</li>
<li>然后再用常规的朴素贝叶斯方法进行训练。</li>
</ul>
<p>但是由于自然语言的特点，在提取特征的过程当中，有一些tricks需要注意</p>
<ul>
<li><p><strong>对否定句进行特别的处理</strong>。比如这句话“我不是很喜欢部电影，因为它让我开心不起来。”其中两个形容词“喜欢”、“开心”都是褒义词，但是因为句子的否定句，所以整体是贬义的。有一种比较简单粗暴的处理方式，就是“对否定词（“不”、“非”、“没”等）与句尾标点之间的所有形容词都采用其否定形式” 。则这句话中提取出来的形容词就应该是“不喜欢”和“不开心”。</p>
</li>
<li><p>一般说来，<strong>最相关的情感词在一些文本片段中仅仅出现一次，词频模型起得作用有限</strong>，甚至是负作用，则使用<code>伯努利模型</code>代替多项式模型。这种情况在微博这样的小篇幅文本中似乎不太明显，但是在博客、空间、论坛之类允许长篇幅文本出现的平台中需要注意。</p>
</li>
<li><p>其实，副词对情感的评价有一定影响。“不很喜欢”与“很不喜欢”的程度就有很大差异。但如果是朴素贝叶斯方法的话比较难处理这样的情况。我们可以考虑用语言模型或者加入词性标注的信息进行综合判断。这些内容我们将在之后进行探讨。</p>
</li>
</ul>
<p>当然经过以上的处理，情感分析还是会有一部分误判。这里涉及到许多问题，都是情感分析的难点：</p>
<ul>
<li><strong>情绪表达的含蓄微妙</strong>：“导演你出来，我保证不打死你。”你让机器怎么判断是褒还是贬？</li>
<li><strong>转折性表达</strong>：“我非常喜欢这些大牌演员，非常崇拜这个导演，非常赞赏这个剧本，非常欣赏他们的预告片，我甚至为了这部影片整整期待了一年，最后进了电影院发现这是个噩梦。” 五个褒义的形容词、副词对一个不那么贬义的词。机器自然判断成褒义，但这句话是妥妥的贬义。</li>
</ul>
<h2 id="17-内容小结"><a href="#17-内容小结" class="headerlink" title="17. 内容小结"></a>17. 内容小结</h2><p>从前面大家基本可以看出，工程应用不同于学术理论，有许多tricks需要考虑，而理论本质就是翻来倒去折腾贝叶斯公式，都快玩出花来了。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python 字符串处理-正则表达式]]></title>
      <url>http://sggo.me/2017/07/30/nlp/string-operation-re/</url>
      <content type="html"><![CDATA[<p>Python 字符串处理 之 正则表达式 <a href="https://github.com/blair101/machine-learning-action/tree/master/string_operation" target="_blank" rel="external">Github-ipynb</a></p>
<h3 id="字符串操作"><a href="#字符串操作" class="headerlink" title="字符串操作"></a>字符串操作</h3><p>我们一起回归一下python字符串的相关操作，这是非常基础的知识，但却是使用频度非常高的一些功能。</p>
<a id="more"></a>
<h4 id="1-1-去空格及特殊符号"><a href="#1-1-去空格及特殊符号" class="headerlink" title="1.1 去空格及特殊符号"></a>1.1 去空格及特殊符号</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = <span class="string">' hello, world!'</span></span><br><span class="line"><span class="keyword">print</span> s.strip()</span><br><span class="line"><span class="keyword">print</span> s.lstrip(<span class="string">' hello, '</span>)</span><br><span class="line"><span class="keyword">print</span> s.rstrip(<span class="string">'!'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>hello, world!
world!
 hello, world
</code></pre><h4 id="1-2-连接字符串"><a href="#1-2-连接字符串" class="headerlink" title="1.2 连接字符串"></a>1.2 连接字符串</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sStr1 = <span class="string">'strcat'</span></span><br><span class="line">sStr2 = <span class="string">'append'</span></span><br><span class="line">sStr1 += sStr2</span><br><span class="line"><span class="keyword">print</span> sStr1</span><br></pre></td></tr></table></figure>
<pre><code>strcatappend
</code></pre><h4 id="1-3-查找字符"><a href="#1-3-查找字符" class="headerlink" title="1.3 查找字符"></a>1.3 查找字符</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># &lt; 0 为未找到</span></span><br><span class="line">sStr1 = <span class="string">'strchr'</span></span><br><span class="line">sStr2 = <span class="string">'r'</span></span><br><span class="line">nPos = sStr1.index(sStr2)</span><br><span class="line"><span class="keyword">print</span> nPos</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre><h4 id="1-4-较字符串"><a href="#1-4-较字符串" class="headerlink" title="1.4 较字符串"></a>1.4 较字符串</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sStr1 = <span class="string">'strchr'</span></span><br><span class="line">sStr2 = <span class="string">'strch'</span></span><br><span class="line"><span class="keyword">print</span> cmp(sStr2,sStr1)</span><br><span class="line"><span class="keyword">print</span> cmp(sStr1,sStr2)</span><br><span class="line"><span class="keyword">print</span> cmp(sStr1,sStr1)</span><br></pre></td></tr></table></figure>
<pre><code>-1
1
0
</code></pre><h4 id="1-5-大小写转换"><a href="#1-5-大小写转换" class="headerlink" title="1.5 大小写转换"></a>1.5 大小写转换</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sStr1 = <span class="string">'JCstrlwr'</span></span><br><span class="line">sStr1 = sStr1.upper()</span><br><span class="line"><span class="comment">#sStr1 = sStr1.lower()</span></span><br><span class="line"><span class="keyword">print</span> sStr1</span><br></pre></td></tr></table></figure>
<pre><code>JCSTRLWR
</code></pre><h4 id="1-6-翻转字符串"><a href="#1-6-翻转字符串" class="headerlink" title="1.6 翻转字符串"></a>1.6 翻转字符串</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sStr1 = <span class="string">'abcdefg'</span></span><br><span class="line">sStr1 = sStr1[::<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">print</span> sStr1</span><br></pre></td></tr></table></figure>
<pre><code>gfedcba
</code></pre><h4 id="1-7-查找字符串"><a href="#1-7-查找字符串" class="headerlink" title="1.7 查找字符串"></a>1.7 查找字符串</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sStr1 = <span class="string">'abcdefg'</span></span><br><span class="line">sStr2 = <span class="string">'cde'</span></span><br><span class="line"><span class="keyword">print</span> sStr1.find(sStr2)</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre><h4 id="1-8-分割字符串"><a href="#1-8-分割字符串" class="headerlink" title="1.8 分割字符串"></a>1.8 分割字符串</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sStr1 = <span class="string">'ab,cde,fgh,ijk'</span></span><br><span class="line">sStr2 = <span class="string">','</span></span><br><span class="line">sStr1 = sStr1[sStr1.find(sStr2) + <span class="number">1</span>:]</span><br><span class="line"><span class="keyword">print</span> sStr1</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">s = <span class="string">'ab,cde,fgh,ijk'</span></span><br><span class="line">print(s.split(<span class="string">','</span>))</span><br></pre></td></tr></table></figure>
<pre><code>cde,fgh,ijk
[&apos;ab&apos;, &apos;cde&apos;, &apos;fgh&apos;, &apos;ijk&apos;]
</code></pre><h4 id="1-9-频次最高的字母"><a href="#1-9-频次最高的字母" class="headerlink" title="1.9 频次最高的字母"></a>1.9 频次最高的字母</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#version 1</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_max_value_v1</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = text.lower()</span><br><span class="line">    result = re.findall(<span class="string">'[a-zA-Z]'</span>, text)  <span class="comment"># 去掉列表中的符号符</span></span><br><span class="line">    count = Counter(result)  <span class="comment"># Counter(&#123;'l': 3, 'o': 2, 'd': 1, 'h': 1, 'r': 1, 'e': 1, 'w': 1&#125;)</span></span><br><span class="line">    count_list = list(count.values())</span><br><span class="line">    max_value = max(count_list)</span><br><span class="line">    max_list = []</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> count.items():</span><br><span class="line">        <span class="keyword">if</span> v == max_value:</span><br><span class="line">            max_list.append(k)</span><br><span class="line">    max_list = sorted(max_list)</span><br><span class="line">    <span class="keyword">return</span> max_list[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#version 2</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_max_value</span><span class="params">(text)</span>:</span></span><br><span class="line">    count = Counter([x <span class="keyword">for</span> x <span class="keyword">in</span> text.lower() <span class="keyword">if</span> x.isalpha()])</span><br><span class="line">    m = max(count.values())</span><br><span class="line">    <span class="keyword">return</span> sorted([x <span class="keyword">for</span> (x, y) <span class="keyword">in</span> count.items() <span class="keyword">if</span> y == m])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#version 3</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_max_value</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = text.lower()</span><br><span class="line">    <span class="keyword">return</span> max(string.ascii_lowercase, key=text.count)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max(range(<span class="number">6</span>), key = <span class="keyword">lambda</span> x : x&gt;<span class="number">2</span>)</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; 3</span></span><br><span class="line"><span class="comment"># 带入key函数中，各个元素返回布尔值，相当于[False, False, False, True, True, True]</span></span><br><span class="line"><span class="comment"># key函数要求返回值为True，有多个符合的值，则挑选第一个。</span></span><br><span class="line"></span><br><span class="line">max([<span class="number">3</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">0</span>], key = <span class="keyword">lambda</span> x : x)</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; 5</span></span><br><span class="line"><span class="comment"># 带入key函数中，各个元素返回自身的值，最大的值为5，返回5.</span></span><br><span class="line"></span><br><span class="line">max(<span class="string">'ah'</span>, <span class="string">'bf'</span>, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; 'ah'</span></span><br><span class="line"><span class="comment"># 带入key函数，各个字符串返回最后一个字符，其中'ah'的h要大于'bf'中的f，因此返回'ah'</span></span><br><span class="line"></span><br><span class="line">max(<span class="string">'ah'</span>, <span class="string">'bf'</span>, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; 'bf'</span></span><br><span class="line"><span class="comment"># 带入key函数，各个字符串返回第一个字符，其中'bf'的b要大于'ah'中的a，因此返回'bf'</span></span><br><span class="line"></span><br><span class="line">text = <span class="string">'Hello World'</span></span><br><span class="line">max(<span class="string">'abcdefghijklmnopqrstuvwxyz'</span>, key=text.count)</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; 'l'</span></span><br><span class="line"><span class="comment"># 带入key函数，返回各个字符在'Hello World'中出现的次数，出现次数最多的字符为'l',因此输出'l'</span></span><br></pre></td></tr></table></figure>
<pre><code>&apos;l&apos;
</code></pre><h4 id="Count-occurrence-of-a-character-in-a-Python-string"><a href="#Count-occurrence-of-a-character-in-a-Python-string" class="headerlink" title="Count occurrence of a character in a Python string"></a>Count occurrence of a character in a Python string</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#T  h  e     M  i  s  s  i  s  s  i  p  p  i     R  i  v  e  r</span></span><br><span class="line"><span class="comment">#[1, 1, 2, 2, 1, 5, 4, 4, 5, 4, 4, 5, 2, 2, 5, 2, 1, 5, 1, 2, 1]</span></span><br><span class="line">sentence=<span class="string">'The Mississippi River'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_chars</span><span class="params">(s)</span>:</span></span><br><span class="line">        s=s.lower()</span><br><span class="line">        count=list(map(s.count,s))</span><br><span class="line">        <span class="keyword">return</span> (max(count))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> count_chars(sentence)</span><br></pre></td></tr></table></figure>
<pre><code>5
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Jieba 中文处理]]></title>
      <url>http://sggo.me/2017/07/29/nlp/jieba/</url>
      <content type="html"><![CDATA[<p>NLP 基础技能 ： Jieba 中文分词与处理</p>
<p><a href="https://github.com/blair101/machine-learning-action/blob/master/string_operation/jieba-learning-Notes.ipynb" target="_blank" rel="external">Github-ipynb</a></p>
<a id="more"></a>
<p>词汇是我们对句子和文章理解的基础，因此需要一个工具去把完整的文本中分解成粒度更细的词</p>
<p><strong>jieba</strong> 就是这样一个非常好用的中文工具，是以分词起家的，但是功能比分词要强大很多。</p>
<h2 id="1-基本分词函数与用法"><a href="#1-基本分词函数与用法" class="headerlink" title="1. 基本分词函数与用法"></a>1. 基本分词函数与用法</h2><ol>
<li>jieba.cut </li>
<li>jieba.cut_for_search </li>
</ol>
<p>返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)</p>
<p><strong>jieba.cut</strong> 方法接受三个输入参数:</p>
<ul>
<li>需要分词的字符串</li>
<li>cut_all 参数用来控制是否采用全模式</li>
<li>HMM 参数用来控制是否使用 HMM 模型</li>
</ul>
<p><strong>jieba.cut_for_search</strong> 方法接受两个参数</p>
<ul>
<li>需要分词的字符串</li>
<li>是否使用 HMM 模型。</li>
</ul>
<p>该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我在学习自然语言处理"</span>, cut_all=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span> seg_list</span><br><span class="line">print(<span class="string">"Full Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 全模式</span></span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我在学习自然语言处理"</span>, cut_all=<span class="keyword">False</span>)</span><br><span class="line">print(<span class="string">"Default Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 精确模式</span></span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"他毕业于上海交通大学，在百度深度学习研究院进行研究"</span>)  <span class="comment"># 默认是精确模式</span></span><br><span class="line">print(<span class="string">", "</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut_for_search(<span class="string">"小明硕士毕业于中国科学院计算所，后在哈佛大学深造"</span>)  <span class="comment"># 搜索引擎模式</span></span><br><span class="line">print(<span class="string">", "</span>.join(seg_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Building prefix dict from the default dictionary ...</span><br><span class="line">&lt;generator object cut at 0x110370460&gt;</span><br><span class="line">Dumping model to file cache /var/folders/mf/_jgd83rx0rgcmt42cp7fkkd00000gn/T/jieba.cache</span><br><span class="line">Loading model cost 2.184 seconds.</span><br><span class="line">Prefix dict has been built succesfully.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Full Mode: 我/ 在/ 学习/ 自然/ 自然语言/ 语言/ 处理</span><br><span class="line">Default Mode: 我/ 在/ 学习/ 自然语言/ 处理</span><br><span class="line">他, 毕业, 于, 上海交通大学, ，, 在, 百度, 深度, 学习, 研究院, 进行, 研究</span><br><span class="line">小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 哈佛, 大学, 哈佛大学, 深造</span><br></pre></td></tr></table></figure>
<p><strong>jieba.lcut</strong> 以及 <strong>jieba.lcut_for_search</strong> 直接返回 list</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result_lcut = jieba.lcut(<span class="string">"小明硕士毕业于中国科学院计算所，后在哈佛大学深造"</span>)</span><br><span class="line"><span class="keyword">print</span> result_lcut</span><br><span class="line"><span class="keyword">print</span> <span class="string">" "</span>.join(result_lcut)</span><br><span class="line"><span class="keyword">print</span> <span class="string">" "</span>.join(jieba.lcut_for_search(<span class="string">"小明硕士毕业于中国科学院计算所，后在哈佛大学深造"</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[u&apos;\u5c0f\u660e&apos;, u&apos;\u7855\u58eb&apos;, u&apos;\u6bd5\u4e1a&apos;, u&apos;\u4e8e&apos;, u&apos;\u4e2d\u56fd\u79d1\u5b66\u9662&apos;, u&apos;\u8ba1\u7b97\u6240&apos;, u&apos;\uff0c&apos;, u&apos;\u540e&apos;, u&apos;\u5728&apos;, u&apos;\u54c8\u4f5b\u5927\u5b66&apos;, u&apos;\u6df1\u9020&apos;]</span><br><span class="line">小明 硕士 毕业 于 中国科学院 计算所 ， 后 在 哈佛大学 深造</span><br><span class="line">小明 硕士 毕业 于 中国 科学 学院 科学院 中国科学院 计算 计算所 ， 后 在 哈佛 大学 哈佛大学 深造</span><br></pre></td></tr></table></figure>
<h3 id="1-1-添加用户自定义词典"><a href="#1-1-添加用户自定义词典" class="headerlink" title="1.1 添加用户自定义词典"></a>1.1 添加用户自定义词典</h3><p>很多时候我们需要针对自己的场景进行分词，会有一些领域内的专有词汇。</p>
<ol>
<li>可以用 <code>jieba.load_userdict(file_name)</code> 加载用户字典</li>
<li>少量的词汇可以自己用下面方法手动添加：</li>
</ol>
<p>用 <strong>add_word(word, freq=None, tag=None)</strong> 和 <strong>del_word(word)</strong> 在程序中动态修改词典<br>用 <strong>suggest_freq(segment, tune=True)</strong> 可调节单个词语的词频，使其能（或不能）被分出来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'/'</span>.join(jieba.cut(<span class="string">'如果放到旧字典中将出错。'</span>, HMM=<span class="keyword">False</span>)))</span><br><span class="line"><span class="keyword">print</span> jieba.suggest_freq((<span class="string">'中'</span>, <span class="string">'将'</span>), <span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">'/'</span>.join(jieba.cut(<span class="string">'如果放到旧字典中将出错。'</span>, HMM=<span class="keyword">False</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果/放到/旧/字典/中将/出错/。</span><br><span class="line">494</span><br><span class="line">如果/放到/旧/字典/中/将/出错/。</span><br></pre></td></tr></table></figure>
<h2 id="2-关键词提取"><a href="#2-关键词提取" class="headerlink" title="2. 关键词提取"></a>2. 关键词提取</h2><h3 id="2-1-TF-IDF-算法的关键词抽取"><a href="#2-1-TF-IDF-算法的关键词抽取" class="headerlink" title="2.1 TF-IDF 算法的关键词抽取"></a>2.1 TF-IDF 算法的关键词抽取</h3><p>import jieba.analyse<br>jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())</p>
<ul>
<li>sentence 为待提取的文本</li>
<li>topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20</li>
<li>withWeight 为是否一并返回关键词权重值，默认值为 False</li>
<li>allowPOS 仅包括指定词性的词，默认值为空，即不筛选</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> analyse</span><br><span class="line">lines = open(<span class="string">'NBA.txt'</span>).read()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"  "</span>.join(analyse.extract_tags(lines, topK=<span class="number">20</span>, withWeight=<span class="keyword">False</span>, allowPOS=()))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">韦少  杜兰特  全明星  全明星赛  MVP  威少  正赛  科尔  投篮  勇士  球员  </span><br><span class="line">斯布鲁克  更衣柜  张卫平  三连庄  NBA  西部  指导  雷霆  明星队</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines = open(<span class="string">u'西游记.txt'</span>).read()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"  "</span>.join(analyse.extract_tags(lines, topK=<span class="number">20</span>, withWeight=<span class="keyword">False</span>, allowPOS=()))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">行者  八戒  师父  三藏  唐僧  大圣  沙僧  妖精  菩萨  和尚  那怪  那里  </span><br><span class="line">长老  呆子  徒弟  怎么  不知  老孙  国王  一个</span><br></pre></td></tr></table></figure>
<h3 id="2-2-TF-IDF-关键词抽取补充"><a href="#2-2-TF-IDF-关键词抽取补充" class="headerlink" title="2.2 TF-IDF 关键词抽取补充"></a>2.2 TF-IDF 关键词抽取补充</h3><p>关键词提取所使用逆向文件频率（<strong>IDF</strong>）文本语料库可以切换成自定义语料库的路径</p>
<ul>
<li>用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径</li>
<li>自定义语料库示例见<a href="https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big" target="_blank" rel="external">这里</a></li>
<li>用法示例见<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py" target="_blank" rel="external">这里</a></li>
</ul>
<p>关键词提取所使用停止词（<strong>Stop Words</strong>）文本语料库可以切换成自定义语料库的路径</p>
<ul>
<li>用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径</li>
<li>自定义语料库示例见<a href="https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt" target="_blank" rel="external">这里</a></li>
<li>用法示例见<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py" target="_blank" rel="external">这里</a></li>
</ul>
<p>关键词一并返回关键词权重值示例</p>
<ul>
<li>用法示例见<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py" target="_blank" rel="external">这里</a></li>
</ul>
<h3 id="2-3-TextRank-的关键词抽取"><a href="#2-3-TextRank-的关键词抽取" class="headerlink" title="2.3 TextRank 的关键词抽取"></a>2.3 TextRank 的关键词抽取</h3><ul>
<li>jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’)) 直接使用，接口相同，注意默认过滤词性。</li>
<li>jieba.analyse.TextRank() 新建自定义 TextRank 实例</li>
</ul>
<p>算法论文： TextRank: <a href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" target="_blank" rel="external">Bringing Order into Texts</a></p>
<p>基本思想:</p>
<ul>
<li>将待抽取关键词的文本进行分词</li>
<li>以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图</li>
<li>计算图中节点的PageRank，注意是无向带权图</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> analyse</span><br><span class="line">lines = open(<span class="string">'NBA.txt'</span>).read()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"  "</span>.join(analyse.textrank(lines, topK=<span class="number">20</span>, withWeight=<span class="keyword">False</span>, allowPOS=(<span class="string">'ns'</span>, <span class="string">'n'</span>, <span class="string">'vn'</span>, <span class="string">'v'</span>)))</span><br><span class="line"><span class="keyword">print</span> <span class="string">"---------------------我是分割线----------------"</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"  "</span>.join(analyse.textrank(lines, topK=<span class="number">20</span>, withWeight=<span class="keyword">False</span>, allowPOS=(<span class="string">'ns'</span>, <span class="string">'n'</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Building prefix dict from the default dictionary ...</span><br><span class="line">Loading model from cache /var/folders/mf/_jgd83rx0rgcmt42cp7fkkd00000gn/T/jieba.cache</span><br><span class="line">Loading model cost 0.530 seconds.</span><br><span class="line">Prefix dict has been built succesfully.</span><br><span class="line"></span><br><span class="line">全明星赛  勇士  正赛  指导  对方  投篮  球员  没有  出现  时间  威少  </span><br><span class="line">认为  看来  结果  相隔  助攻  现场  三连庄  介绍  嘉宾</span><br><span class="line">---------------------我是分割线----------------</span><br><span class="line">勇士  正赛  全明星赛  指导  投篮  玩命  时间  对方  现场  结果  球员  </span><br><span class="line">嘉宾  时候  全队  主持人  特点  大伙  肥皂剧  全程  快船队</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines = open(<span class="string">u'西游记.txt'</span>).read()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"  "</span>.join(analyse.textrank(lines, topK=<span class="number">20</span>, withWeight=<span class="keyword">False</span>, allowPOS=(<span class="string">'ns'</span>, <span class="string">'n'</span>, <span class="string">'vn'</span>, <span class="string">'v'</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">行者  师父  八戒  三藏  大圣  不知  菩萨  妖精  只见  长老  国王  却说  </span><br><span class="line">呆子  徒弟  小妖  出来  不得  不见  不能  师徒</span><br></pre></td></tr></table></figure>
<h2 id="3-词性标注"><a href="#3-词性标注" class="headerlink" title="3. 词性标注"></a>3. 词性标注</h2><ul>
<li>jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。</li>
<li>jieba.posseg.dt 为默认词性标注分词器。  </li>
<li>标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。</li>
<li>具体的词性对照表参见计算所汉语词性标记集 <a href="http://ictclas.nlpir.org/nlpir/html/readme.htm" target="_blank" rel="external">计算所汉语词性标记集</a></li>
</ul>
<p><a href="http://ictclas.nlpir.org/" target="_blank" rel="external">ictclas</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line">words = pseg.cut(<span class="string">"我爱自然语言处理"</span>)</span><br><span class="line"><span class="keyword">for</span> word, flag <span class="keyword">in</span> words:</span><br><span class="line">    print(<span class="string">'%s %s'</span> % (word, flag))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我 r</span><br><span class="line">爱 v</span><br><span class="line">自然语言 l</span><br><span class="line">处理 v</span><br></pre></td></tr></table></figure>
<h2 id="4-并行分词"><a href="#4-并行分词" class="headerlink" title="4. 并行分词"></a>4. 并行分词</h2><p>原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows</p>
<p>用法：</p>
<ul>
<li>jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数</li>
<li>jieba.disable_parallel() # 关闭并行分词模式</li>
</ul>
<p>实验结果：在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。  </p>
<p>注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">jieba.enable_parallel()</span><br><span class="line">content = open(<span class="string">u'西游记.txt'</span>,<span class="string">"r"</span>).read()</span><br><span class="line">t1 = time.time()</span><br><span class="line">words = <span class="string">"/ "</span>.join(jieba.cut(content))</span><br><span class="line">t2 = time.time()</span><br><span class="line">tm_cost = t2-t1</span><br><span class="line">print(<span class="string">'并行分词速度为 %s bytes/second'</span> % (len(content)/tm_cost))</span><br><span class="line"></span><br><span class="line">jieba.disable_parallel()</span><br><span class="line">content = open(<span class="string">u'西游记.txt'</span>,<span class="string">"r"</span>).read()</span><br><span class="line">t1 = time.time()</span><br><span class="line">words = <span class="string">"/ "</span>.join(jieba.cut(content))</span><br><span class="line">t2 = time.time()</span><br><span class="line">tm_cost = t2-t1</span><br><span class="line">print(<span class="string">'非并行分词速度为 %s bytes/second'</span> % (len(content)/tm_cost))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">并行分词速度为 415863.760491 bytes/second</span><br><span class="line">非并行分词速度为 242471.700496 bytes/second</span><br></pre></td></tr></table></figure>
<h3 id="命令行分词"><a href="#命令行分词" class="headerlink" title="命令行分词"></a>命令行分词</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使用示例：python -m jieba news.txt &gt; cut_result.txt</span><br><span class="line">命令行选项（翻译）：</span><br><span class="line">使用: python -m jieba [options] filename</span><br><span class="line"></span><br><span class="line">结巴命令行界面。</span><br><span class="line"></span><br><span class="line">固定参数:</span><br><span class="line">  filename              输入文件</span><br><span class="line"></span><br><span class="line">可选参数:</span><br><span class="line">  -h, --help            显示此帮助信息并退出</span><br><span class="line">  -d [DELIM], --delimiter [DELIM]</span><br><span class="line">                        使用 DELIM 分隔词语，而不是用默认的&apos; / &apos;。</span><br><span class="line">                        若不指定 DELIM，则使用一个空格分隔。</span><br><span class="line">  -p [DELIM], --pos [DELIM]</span><br><span class="line">                        启用词性标注；如果指定 DELIM，词语和词性之间</span><br><span class="line">                        用它分隔，否则用 _ 分隔</span><br><span class="line">  -D DICT, --dict DICT  使用 DICT 代替默认词典</span><br><span class="line">  -u USER_DICT, --user-dict USER_DICT</span><br><span class="line">                        使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用</span><br><span class="line">  -a, --cut-all         全模式分词（不支持词性标注）</span><br><span class="line">  -n, --no-hmm          不使用隐含马尔可夫模型</span><br><span class="line">  -q, --quiet           不输出载入信息到 STDERR</span><br><span class="line">  -V, --version         显示版本信息并退出</span><br><span class="line"></span><br><span class="line">如果没有指定文件名，则使用标准输入。</span><br></pre></td></tr></table></figure>
<h3 id="Tokenize：返回词语在原文的起止位置"><a href="#Tokenize：返回词语在原文的起止位置" class="headerlink" title="Tokenize：返回词语在原文的起止位置"></a>Tokenize：返回词语在原文的起止位置</h3><p>注意，输入参数只接受 unicode</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"这是默认模式的tokenize"</span></span><br><span class="line">result = jieba.tokenize(<span class="string">u'自然语言处理非常有用'</span>)</span><br><span class="line"><span class="keyword">for</span> tk <span class="keyword">in</span> result:</span><br><span class="line">    print(<span class="string">"%s\t\t start: %d \t\t end:%d"</span> % (tk[<span class="number">0</span>],tk[<span class="number">1</span>],tk[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"\n-----------我是神奇的分割线------------\n"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"这是搜索模式的tokenize"</span></span><br><span class="line">result = jieba.tokenize(<span class="string">u'自然语言处理非常有用'</span>, mode=<span class="string">'search'</span>)</span><br><span class="line"><span class="keyword">for</span> tk <span class="keyword">in</span> result:</span><br><span class="line">    print(<span class="string">"%s\t\t start: %d \t\t end:%d"</span> % (tk[<span class="number">0</span>],tk[<span class="number">1</span>],tk[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这是默认模式的tokenize</span><br><span class="line">自然语言		 start: 0 		 end:4</span><br><span class="line">处理		 start: 4 		 end:6</span><br><span class="line">非常		 start: 6 		 end:8</span><br><span class="line">有用		 start: 8 		 end:10</span><br><span class="line">    </span><br><span class="line">-----------我是神奇的分割线------------</span><br><span class="line">    </span><br><span class="line">这是搜索模式的tokenize</span><br><span class="line">自然		 start: 0 		 end:2</span><br><span class="line">语言		 start: 2 		 end:4</span><br><span class="line">自然语言		 start: 0 		 end:4</span><br><span class="line">处理		 start: 4 		 end:6</span><br><span class="line">非常		 start: 6 		 end:8</span><br><span class="line">有用		 start: 8 		 end:10</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Word2vec 基础]]></title>
      <url>http://sggo.me/2017/07/12/nlp/word2vector-basic/</url>
      <content type="html"><![CDATA[<p>Natural Language Processing，计算机科学与语言学中关注于计算机与人类语言间转换的领域</p>
<a id="more"></a>
<h2 id="1-NLP-常见任务"><a href="#1-NLP-常见任务" class="headerlink" title="1. NLP 常见任务"></a>1. NLP 常见任务</h2><ol>
<li>自动摘要</li>
<li>指代消解   </li>
<li>机器翻译 </li>
<li>词性标注   </li>
<li>分词 (中文、日文等)</li>
<li>主题识别</li>
<li>文本分类.<br>……</li>
</ol>
<blockquote>
<p>自动摘要   -&gt;   搜索要索引，关键词等<br>指代消解   -&gt;   小明放学了, 妈妈去接<code>他</code><br>机器翻译   -&gt;   小心地滑、干货 =&gt; Slide carefully<br>词性标注   -&gt;   heat(v.) water(n.) in(p.) a(det.)   pot(n.)<br>分词 (中文、日文等)   -&gt;   大水沟/很/难/过  </p>
</blockquote>
<h2 id="2-NLP-处理方法"><a href="#2-NLP-处理方法" class="headerlink" title="2. NLP 处理方法"></a>2. NLP 处理方法</h2><h3 id="2-1-传统-基于规则"><a href="#2-1-传统-基于规则" class="headerlink" title="2.1 传统: 基于规则"></a>2.1 传统: 基于规则</h3><p>Dict…</p>
<blockquote>
<p>简单、粗暴、有用</p>
</blockquote>
<h3 id="2-2-现代-基于机器学习"><a href="#2-2-现代-基于机器学习" class="headerlink" title="2.2 现代: 基于机器学习"></a>2.2 现代: 基于机器学习</h3><blockquote>
<p>HMM, CRF, SVM, LDA, CNN…<br>“规则”隐含在模型参数里</p>
</blockquote>
<h2 id="3-词编码和词向量初步"><a href="#3-词编码和词向量初步" class="headerlink" title="3. 词编码和词向量初步"></a>3. 词编码和词向量初步</h2><blockquote>
<p>你需要一些 model，不管你是基于规则统计、机器学习、深度学习 的一些方法，第一步要做的，一定是对你的文本或者数据，进行表达, 词编码。</p>
</blockquote>
<p><font color="black">『词编码需要保证词的相似性』<font></font></font></p>
<p>Glove results</p>
<p>Nearest words to</p>
<ol>
<li>frog</li>
<li>toad</li>
<li>rana</li>
<li>…</li>
</ol>
<p><font color="black">『向量空间分布的相似性』<font></font></font></p>
<p><img src="/images/word2vector-2.png" alt=""></p>
<p><font color="black">『向量空间子结构』<font></font></font></p>
<blockquote>
<p>编码要尽量保证，相似的词的空间距离是相近的</p>
</blockquote>
<p>$V_{King}$ - $V_{Queen}$ + $V_{Women}$ = $V_{Man}$</p>
<p>$V_{Paris}$ - $V_{France}$ + $V_{German}$ = $V_{Berlin}$</p>
<blockquote>
<p>最终目标: 词向量表示作为机器学习、特别是深度学习的输入和表示空间</p>
</blockquote>
<hr>
<blockquote>
<p>你的 <code>数据</code> 决定了你的 <code>结果上限</code><br>你的 <code>算法</code> 只是以多大程度去 <code>逼近</code></p>
</blockquote>
<hr>
<p><strong>Linguists</strong></p>
<p><img src="/images/nlp/word2vector-3.png" width="520" height="400" align="middle" img=""></p>
<h3 id="3-1-离散表示-One-hot"><a href="#3-1-离散表示-One-hot" class="headerlink" title="3.1 离散表示 One-hot"></a>3.1 离散表示 One-hot</h3><ul>
<li><font color="blue">语料库<font></font></font></li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">John likes to watch movies. Mary likes too.John also likes to watch football games.</span><br></pre></td></tr></table></figure>
<ul>
<li><font color="blue">词典<font></font></font></li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"John"</span>: <span class="number">1</span>, <span class="string">"likes"</span>: <span class="number">2</span>, <span class="string">"to"</span>: <span class="number">3</span>, <span class="string">"watch"</span>: <span class="number">4</span>, <span class="string">"movies"</span>: <span class="number">5</span>, </span><br><span class="line"><span class="string">"also"</span>: <span class="number">6</span>, <span class="string">"football"</span>: <span class="number">7</span>, <span class="string">"games"</span>: <span class="number">8</span>, <span class="string">"Mary"</span>: <span class="number">9</span>, <span class="string">"too"</span>: <span class="number">10</span>&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><font color="blue">One-hot表示<font></font></font></li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">John: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]likes: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">...</span><br><span class="line">too : [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="3-2-离散表示-Bag-of-Words"><a href="#3-2-离散表示-Bag-of-Words" class="headerlink" title="3.2 离散表示 Bag of Words"></a>3.2 离散表示 Bag of Words</h3><blockquote>
<p>文档的向量表示可以直接将各词的词向量表示加和</p>
</blockquote>
<p>John likes to watch movies. Mary likes too. =&gt; [1, 2, 1, 1, 1, 0, 0, 0, 1, 1]<br>John also likes to watch football games. =&gt; [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]</p>
<blockquote>
<p>词权重  - (<code>词在文档中的顺序没有被考虑</code>)</p>
<p>tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度</p>
</blockquote>
<ul>
<li>TF-IDF (Term Frequency - Inverse Document Frequency)</li>
</ul>
<p>Term Frequency : F(term) = (该word词出现在当前文档中的次数) / (当前文档所有word的总数).</p>
<p>信息检索词 t 的 IDF </p>
<p>$$\log (1 + {\frac{N}{n^t}})$$</p>
<blockquote>
<p>N: 文档总数， n: 含有词 t 的文档数</p>
</blockquote>
<p>[0.693, 1.386, 0.693, 0.693, 1.099, 0, 0, 0, 0.693, 0.693]</p>
<p>Binary weighting</p>
<blockquote>
<p>不做计数的版本</p>
<p>短文本相似性, Bernoulli Naive Bayes [1, 1, 1, 1, 1, 0, 0, 0, 1, 1]</p>
<p>if so, I love you = you love I</p>
</blockquote>
<h3 id="3-3-离散表示-􏰜􏰝􏰞􏰟􏰒Bi-gram-􏰪N-gram"><a href="#3-3-离散表示-􏰜􏰝􏰞􏰟􏰒Bi-gram-􏰪N-gram" class="headerlink" title="3.3 离散表示 􏰜􏰝􏰞􏰟􏰒Bi-gram / 􏰪N-gram"></a>3.3 离散表示 􏰜􏰝􏰞􏰟􏰒Bi-gram / 􏰪N-gram</h3><p>John likes to watch movies. Mary likes too.<br>John also likes to watch football games.</p>
<p><img src="/images/nlp/word2vector-4.png" width="620" height="400" align="middle" img=""></p>
<h3 id="3-4-语言模型-词组合出现的概率"><a href="#3-4-语言模型-词组合出现的概率" class="headerlink" title="3.4 语言模型 词组合出现的概率"></a>3.4 语言模型 词组合出现的概率</h3><p>一句话(词组合)出现的概率</p>
<p><img src="/images/nlp/word2vector-5.png" width="620" height="400" align="middle" img=""></p>
<blockquote>
<p>简化计算 : $P(too | Mark, likes) \approx P(too | likes)$, 可参见 吴军 《数学之美》 解释</p>
</blockquote>
<h3 id="3-5-离散表示-的缺点"><a href="#3-5-离散表示-的缺点" class="headerlink" title="3.5 离散表示 的缺点"></a>3.5 离散表示 的缺点</h3><p><img src="/images/nlp/word2vector-6.png" width="620" height="400" align="middle" img=""></p>
<h3 id="3-6-分布式表示-提出"><a href="#3-6-分布式表示-提出" class="headerlink" title="3.6 分布式表示 提出"></a>3.6 分布式表示 提出</h3><p><img src="/images/nlp/word2vector-7.png" width="620" height="400" align="middle" img=""></p>
<blockquote>
<p>Distributed representation :  </p>
<p>用一个词附近的其他词来表示该词  - (不知道你的经济情况，就调查下你的狐朋狗友们)<br>􏰐􏱏􏲎􏰀􏳎􏳏􏰢􏳐􏳑􏰀􏳒􏰞You shall know a word by the company it keeps<br>  — J.R. Firth 1957<br>􏰐􏱏􏲎􏰀􏳎􏳏􏰢􏳐􏳑􏰀􏳒􏰞􏰟􏳓􏰀􏰑􏰐􏱏􏲎􏰀􏳎􏳏􏰢􏳐􏳑􏰀􏳒􏰞􏰟􏳓􏰀􏰑􏰐􏱏􏲎􏰀􏳎􏳏􏰢􏳐􏳑<br>􏱕􏳔􏳕􏳖􏳗&gt; 现代统计自然语言处理中最有创见的想法之一</p>
</blockquote>
<h2 id="4-共现矩阵-Cocurrence-matrix"><a href="#4-共现矩阵-Cocurrence-matrix" class="headerlink" title="4. 共现矩阵 (Cocurrence matrix)"></a>4. 共现矩阵 (Cocurrence matrix)</h2><p><img src="/images/nlp/word2vector-8.png" width="620" height="400" align="middle" img=""></p>
<blockquote>
<p>共现 : 共同出现</p>
</blockquote>
<h3 id="4-1-Word-Word"><a href="#4-1-Word-Word" class="headerlink" title="4.1 Word - Word"></a>4.1 Word - Word</h3><p><img src="/images/nlp/word2vector-9.png" width="620" height="400" align="middle" img=""></p>
<blockquote>
<p>左右窗 length 为 1， 得到的矩阵如上</p>
</blockquote>
<p>存在缺点</p>
<p><img src="/images/nlp/word2vector-10.png" width="620" height="400" align="middle" img=""></p>
<blockquote>
<p>模型欠稳定，可以考虑下 LR 的各个参数等，变化太大，对模型求解有影响</p>
</blockquote>
<p><strong>用SVD对共现矩阵向量做降维</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">la = np.linalg</span><br><span class="line"></span><br><span class="line">words = [<span class="string">"I"</span>, <span class="string">"like"</span>, <span class="string">"enjoy"</span>, <span class="string">"deep"</span>, <span class="string">"learning"</span>, <span class="string">"NLP"</span>, <span class="string">"flying"</span>, <span class="string">"."</span>]</span><br><span class="line"></span><br><span class="line">X = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">U, s, Vh = la.svd(X, full_matrices=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>SVD 降维存在的问题</strong></p>
<ul>
<li><p>计算量随语料库和词典增长膨胀太快，对X(n,n)维 的矩阵，计算量O(n^3)。而对大型的语料库， n~400k，语料库大小1~60B token</p>
</li>
<li><p>难以为词典中新加入的词分配词向量    </p>
</li>
<li>与其他深度学习模型框架差异大</li>
</ul>
<h3 id="4-2-NNLM"><a href="#4-2-NNLM" class="headerlink" title="4.2 NNLM"></a>4.2 NNLM</h3><h3 id="4-3-Word2Vec-CBOW"><a href="#4-3-Word2Vec-CBOW" class="headerlink" title="4.3 Word2Vec: CBOW"></a>4.3 Word2Vec: CBOW</h3><h3 id="4-4-Word2Vec-Skip-Gram"><a href="#4-4-Word2Vec-Skip-Gram" class="headerlink" title="4.4 Word2Vec: Skip-Gram"></a>4.4 Word2Vec: Skip-Gram</h3><h3 id="4-5-Word2Vec-缺点"><a href="#4-5-Word2Vec-缺点" class="headerlink" title="4.5 Word2Vec 缺点"></a>4.5 Word2Vec 缺点</h3><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><h3 id="5-1-离散表示"><a href="#5-1-离散表示" class="headerlink" title="5.1 离散表示"></a>5.1 离散表示</h3><ul>
<li>One-hot representation, Bag Of Words Unigram语言模型</li>
<li>N-gram词向量表示和语言模型</li>
<li>Co-currence矩阵的行(列)向量作为词向量</li>
</ul>
<h3 id="5-2-分布式连续表示"><a href="#5-2-分布式连续表示" class="headerlink" title="5.2 分布式连续表示"></a>5.2 分布式连续表示</h3><ul>
<li>Co-currence矩阵的SVD降维的低维词向量表示</li>
<li>Word2Vec: Continuous Bag of Words Model</li>
<li>Word2Vec: Skip-Gram Model</li>
</ul>
<blockquote>
<p>gensim 用 python 训练 word2vec 最好用的库<br>它的功能不至于 word2vec</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scrambled Egg with Ham]]></title>
      <url>http://sggo.me/2017/07/09/tools/my-cooking/</url>
      <content type="html"><![CDATA[<p>记录我学 Cooking 火腿炒蛋 的做法</p>
<a id="more"></a>
<ol>
<li>2个鸡蛋，打入小碗中，搅拌均匀</li>
<li>切一根火腿，切成片状</li>
<li>打开燃气阀门与起火，开小火便可</li>
<li>放入一些豆油，待油热一点</li>
<li>鸡蛋慢慢倒入油锅，鸡蛋成饼狀，期间可以颠勺</li>
<li>将火腿片，放入其中，进行一起炒，并可放入少量盐</li>
<li>炒得差不多后，关闭燃气阀门，并关闭起火开关</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[EF Conversation]]></title>
      <url>http://sggo.me/2017/06/30/English/ef-conversaion/</url>
      <content type="html"><![CDATA[<p>conversation</p>
<a id="more"></a>
<h2 id="2017-06-30"><a href="#2017-06-30" class="headerlink" title="2017-06-30"></a>2017-06-30</h2><p>According to statistics, online recruitment is becoming more and more common. What’s the best way to advertise a job in your country?</p>
<h2 id="2017-06-29"><a href="#2017-06-29" class="headerlink" title="2017-06-29"></a>2017-06-29</h2><p>Call it ‘gossip’, ‘small talk’ or ‘networking’, almost everyone participates in idle chat with their co-workers. Practicing sharing gossip with your classmates today.</p>
<p><code>What do you like to chat about with your friends?</code></p>
<p><code>Do you gossip with friends about these things?</code></p>
<p>politician，salary，celebrity</p>
<p>What does the media say about celebrities in your country?</p>
<p>People gossip about things that should be private</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python Basic Learning III]]></title>
      <url>http://sggo.me/2017/06/05/python/py-language-basic-learning-III/</url>
      <content type="html"><![CDATA[<p>廖雪峰的 Python 教程 - Functional Programming</p>
<a id="more"></a>
<h2 id="1-函数式编程"><a href="#1-函数式编程" class="headerlink" title="1. 函数式编程"></a>1. 函数式编程</h2><p>函数就是面向过程的程序设计的基本单元。</p>
<p>Functional Programming 其思想更接近数学计算。</p>
<p>在计算机的层次上，CPU执行的是加减乘除的指令代码，以及各种条件判断和跳转指令，所以，汇编语言是最贴近计算机的语言。</p>
<p>而计算则指数学意义上的计算，越是抽象的计算，离计算机硬件越远。</p>
<p>对应到编程语言，就是越低级的语言，越贴近计算机，抽象程度低，执行效率高，比如C语言；越高级的语言，越贴近计算，抽象程度高，执行效率低，比如Lisp语言。</p>
<p>函数式编程就是一种抽象程度很高的编程范式，<code>纯粹的函数式编程语言编写的函数没有变量</code>，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。</p>
<p>函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！</p>
<p>Python对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。</p>
<h3 id="1-1-高阶函数"><a href="#1-1-高阶函数" class="headerlink" title="1.1 高阶函数"></a>1.1 高阶函数</h3><p>Higher-order function</p>
<h2 id="2-模块"><a href="#2-模块" class="headerlink" title="2. 模块"></a>2. 模块</h2><blockquote>
<p>Python内置的模块 和 来自第三方的模块。</p>
<p>每一个包目录下面都会有一个<strong>init</strong>.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000" target="_blank" rel="external">廖雪峰的官方网站 liaoxuefeng</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python Class]]></title>
      <url>http://sggo.me/2017/06/05/python/py-language-8-class/</url>
      <content type="html"><![CDATA[<p>OO 最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类</p>
<a id="more"></a>
<h2 id="Class-def"><a href="#Class-def" class="headerlink" title="Class def"></a>Class def</h2><p>class 定义一个类,首字母大写，比如 Calculator. class可以先定义自己的属性，比如 name=’Good Calculator’. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Calculator</span>:</span>       <span class="comment">#首字母要大写，冒号不能缺</span></span><br><span class="line">    </span><br><span class="line">    name=<span class="string">'Good Calculator'</span>  <span class="comment">#该行为class的属性</span></span><br><span class="line">    price=<span class="number">18</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        print(self.name)</span><br><span class="line">        result = x + y</span><br><span class="line">        print(result)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minus</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        result=x-y</span><br><span class="line">        print(result)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">times</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        print(x*y)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">divide</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        print(x/y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cal=Calculator()</span><br><span class="line"></span><br><span class="line">print(cal.name)</span><br><span class="line">print(cal.price)</span><br></pre></td></tr></table></figure>
<pre><code>Good Calculator
18
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cal.add(<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line">cal.minus(<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line">cal.times(<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line">cal.divide(<span class="number">10</span>,<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Good Calculator
30
-10
200
0.5
</code></pre><h2 id="Class-init"><a href="#Class-init" class="headerlink" title="Class init"></a>Class init</h2><p>运行 <code>c=Calculator(&#39;bad calculator&#39;,18,17,16,15)</code>, 然后调出每个初始值的值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Calculator</span>:</span></span><br><span class="line">    name=<span class="string">'good calculator'</span></span><br><span class="line">    price=<span class="number">18</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name,price,height,width,weight)</span>:</span>   <span class="comment"># 注意，这里的下划线是双下划线</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.price=price</span><br><span class="line">        self.h=height</span><br><span class="line">        self.wi=width</span><br><span class="line">        self.we=weight</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c=Calculator(<span class="string">'bad calculator'</span>,<span class="number">18</span>,<span class="number">17</span>,<span class="number">16</span>,<span class="number">15</span>)</span><br><span class="line">print(c.name)</span><br><span class="line">print(c.price)</span><br><span class="line">print(c.h)</span><br><span class="line">print(c.wi)</span><br><span class="line">print(c.we)</span><br></pre></td></tr></table></figure>
<pre><code>bad calculator
18
17
16
15
</code></pre><h3 id="设置class属性默认值"><a href="#设置class属性默认值" class="headerlink" title="设置class属性默认值"></a>设置class属性默认值</h3><p>如何设置属性的默认值, 直接在def里输入即可，如下:</p>
<p><code>def __init__(self,name,price,height=10,width=14,weight=16):</code></p>
<p>查看运行结果， 三个有默认值的属性，可以直接输出默认值.</p>
<p>这些默认值可以在code中更改, 比如<code>c.wi=17</code>再输出<code>c.wi</code>就会把<code>wi</code>属性值更改为<code>17</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Calculator</span>:</span></span><br><span class="line">    name=<span class="string">'good calculator'</span></span><br><span class="line">    price=<span class="number">18</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name,price,hight=<span class="number">10</span>,width=<span class="number">14</span>,weight=<span class="number">16</span>)</span>:</span> <span class="comment">#后面三个属性设置默认值,查看运行</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.price=price</span><br><span class="line">        self.h=hight</span><br><span class="line">        self.wi=width</span><br><span class="line">        self.we=weight</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c=Calculator(<span class="string">'bad calculator'</span>,<span class="number">18</span>)</span><br><span class="line">print(c.h)</span><br><span class="line">print(<span class="string">"wi : "</span> + str(c.wi))</span><br><span class="line">c.wi = <span class="number">17</span></span><br><span class="line">print(<span class="string">"wi : "</span> + str(c.wi))</span><br><span class="line">print(c.we)</span><br></pre></td></tr></table></figure>
<pre><code>10
wi : 14
wi : 17
16
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.python.org/" target="_blank" rel="external">docs.python.org</a></li>
<li><a href="https://morvanzhou.github.io/" target="_blank" rel="external">python morvanzhou</a></li>
<li><a href="https://www.liaoxuefeng.com/" target="_blank" rel="external">python liaoxuefeng</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python Read File]]></title>
      <url>http://sggo.me/2017/06/04/python/py-language-6-read-file/</url>
      <content type="html"><![CDATA[<p>open()、append、file.read()、file.readline()、file.readlines()、file.close()、with open .. as f</p>
<a id="more"></a>
<p>读写文件前，必须了解，在磁盘上读写文件的功能都是由操作系统提供的，现代操作系统不允许普通的程序直接操作磁盘，所以，读写文件就是请求操作系统打开一个<code>文件对象</code>（通常称为文件描述符），然后，通过操作系统提供的接口从这个文件对象中读取数据（读文件），或者把数据写入这个文件对象（写文件）</p>
<h2 id="1-open"><a href="#1-open" class="headerlink" title="1. open"></a>1. open</h2><p>使用 <code>open</code> 能够打开一个文件, <code>open</code> 的第一个参数为文件名和路径 ‘my file.txt’, 第二个参数为将要以什么方式打开它, 比如 <code>w</code> 为可写方式. 如果计算机没有找到 ‘my file.txt’ 这个文件, <code>w</code> 方式能够创建一个新的文件, 并命名为 <code>my file.txt</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = <span class="string">'This is my first test.'</span></span><br><span class="line"></span><br><span class="line">my_file=open(<span class="string">'my file.txt'</span>,<span class="string">'w'</span>)   <span class="comment">#用法: open('文件名','形式'), 其中形式有'w':write;'r':read.</span></span><br><span class="line">my_file.write(text)               <span class="comment">#该语句会写入先前定义好的 text</span></span><br><span class="line">my_file.close()                   <span class="comment">#关闭文件</span></span><br></pre></td></tr></table></figure>
<h2 id="2-append"><a href="#2-append" class="headerlink" title="2. append"></a>2. append</h2><p>我们先保存一个已经有3行文字的 “my file.txt” 文件, 文件的内容如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">This is my first test. </span><br><span class="line">This is the second line.</span><br><span class="line">This the third</span><br></pre></td></tr></table></figure>
<p>使用添加文字的方式给这个文件添加一行 “This is appended file.”, 并将这行文字储存在 append_file 里，注意\n的适用性:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">append_text=<span class="string">'\nThis is appended file.'</span>  <span class="comment"># 为这行文字提前空行 "\n"</span></span><br><span class="line">my_file=open(<span class="string">'my file.txt'</span>,<span class="string">'a'</span>)   <span class="comment"># 'a'=append 以增加内容的形式打开</span></span><br><span class="line">my_file.write(append_text)</span><br><span class="line">my_file.close()</span><br></pre></td></tr></table></figure>
<pre><code>This is my first test.
This is the second line.
This the third line.
This is appended file.
</code></pre><h2 id="3-file-read"><a href="#3-file-read" class="headerlink" title="3. file.read()"></a>3. file.read()</h2><p>调用 <code>read()</code> 会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用<code>read(size)</code>方法，每次最多读取<code>size</code>个字节的内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file= open(<span class="string">'my file.txt'</span>,<span class="string">'r'</span>) </span><br><span class="line">content=file.read()  </span><br><span class="line">print(content)</span><br></pre></td></tr></table></figure>
<pre><code>This is my first test.
This is the second line.
This the third line.
This is appended file.    
</code></pre><h2 id="4-file-readline"><a href="#4-file-readline" class="headerlink" title="4. file.readline()"></a>4. file.readline()</h2><p>如果想在文本中一行行的读取文本, 可以使用 <code>file.readline()</code>, <code>file.readline()</code> 读取的内容和你使用的次数有关, 使用第二次的时候, 读取到的是文本的第二行, 并可以以此类推:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file= open(<span class="string">'my file.txt'</span>,<span class="string">'r'</span>) </span><br><span class="line">content=file.readline()  <span class="comment"># 读取第一行</span></span><br><span class="line">print(content)</span><br></pre></td></tr></table></figure>
<pre><code>This is my first test.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">second_read_time=file.readline()  <span class="comment"># 读取第二行</span></span><br><span class="line">print(second_read_time)</span><br></pre></td></tr></table></figure>
<pre><code>This is the second line.
</code></pre><h2 id="5-file-readlines"><a href="#5-file-readlines" class="headerlink" title="5. file.readlines()"></a>5. file.readlines()</h2><p>如果想要读取所有行, 并可以使用像 <code>for</code> 一样的迭代器迭代这些行结果, 我们可以使用 <code>file.readlines()</code>, 将每一行的结果存储在 <code>list</code> 中, 方便以后迭代.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file= open(<span class="string">'my file.txt'</span>,<span class="string">'r'</span>) </span><br><span class="line">content=file.readlines() <span class="comment"># python_list 形式</span></span><br><span class="line">print(content)</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;This is my first test.\n&apos;, &apos;This is the second line.\n&apos;, &apos;This the third line.\n&apos;, &apos;This is appended file.&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 之后如果使用 for 来迭代输出:</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> content:</span><br><span class="line">    print(item)</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    This <span class="keyword">is</span> my first test.</span><br><span class="line"></span><br><span class="line">    This <span class="keyword">is</span> the second line.</span><br><span class="line"></span><br><span class="line">    This the third line.</span><br><span class="line"></span><br><span class="line">    This <span class="keyword">is</span> appended file.</span><br><span class="line"></span><br><span class="line"><span class="comment">## 6. file.close()</span></span><br><span class="line"></span><br><span class="line">由于文件读写时都有可能产生`IOError`，一旦出错，后面的`f.close()`就不会调用。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用`<span class="keyword">try</span> ... <span class="keyword">finally</span>`来实现：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = open(<span class="string">'/path/to/file'</span>, <span class="string">'r'</span>)</span><br><span class="line">    <span class="keyword">print</span> f.read()</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="keyword">if</span> f:</span><br><span class="line">        f.close()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>每次都这么写实在太繁琐，所以，Python引入了<code>with</code>语句来自动帮我们调用<code>close()</code>方法：</p>
</blockquote>
<h2 id="7-with-open-as-f"><a href="#7-with-open-as-f" class="headerlink" title="7. with open .. as f"></a>7. with open .. as f</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'/path/to/file'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">print</span> f.read()</span><br></pre></td></tr></table></figure>
<p>这和前面的try … finally是一样的，但是代码更佳简洁，并且不必调用f.close()方法。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.python.org/" target="_blank" rel="external">docs.python.org</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/python-basic/basic/08-2-read-file2/" target="_blank" rel="external">python morvanzhou</a></li>
<li><a href="https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386820066616a77f826d876b46b9ac34cb5f34374f7a000" target="_blank" rel="external">python liaoxuefeng</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python Slice、Iteration、List generation、Generator]]></title>
      <url>http://sggo.me/2017/06/03/python/py-language-10-advanced/</url>
      <content type="html"><![CDATA[<p>Slice、Iteration、List generation、Generator</p>
<a id="more"></a>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">L = []</span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> n &lt;= <span class="number">99</span>:</span><br><span class="line">    L.append(n)</span><br><span class="line">    n = n + <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="1-Slice"><a href="#1-Slice" class="headerlink" title="1. Slice"></a>1. Slice</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = [<span class="string">'Michael'</span>, <span class="string">'Sarah'</span>, <span class="string">'Tracy'</span>, <span class="string">'Bob'</span>, <span class="string">'Jack'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">[<span class="string">'Sarah'</span>, <span class="string">'Tracy'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L[<span class="number">-2</span>:]</span><br><span class="line">[<span class="string">'Bob'</span>, <span class="string">'Jack'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = range(<span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">40</span>, <span class="number">41</span>, <span class="number">42</span>, <span class="number">43</span>, <span class="number">44</span>, <span class="number">45</span>, <span class="number">46</span>, <span class="number">47</span>, <span class="number">48</span>, <span class="number">49</span>, <span class="number">50</span>, <span class="number">51</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">54</span>, <span class="number">55</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">58</span>, <span class="number">59</span>, <span class="number">60</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">63</span>, <span class="number">64</span>, <span class="number">65</span>, <span class="number">66</span>, <span class="number">67</span>, <span class="number">68</span>, <span class="number">69</span>, <span class="number">70</span>, <span class="number">71</span>, <span class="number">72</span>, <span class="number">73</span>, <span class="number">74</span>, <span class="number">75</span>, <span class="number">76</span>, <span class="number">77</span>, <span class="number">78</span>, <span class="number">79</span>, <span class="number">80</span>, <span class="number">81</span>, <span class="number">82</span>, <span class="number">83</span>, <span class="number">84</span>, <span class="number">85</span>, <span class="number">86</span>, <span class="number">87</span>, <span class="number">88</span>, <span class="number">89</span>, <span class="number">90</span>, <span class="number">91</span>, <span class="number">92</span>, <span class="number">93</span>, <span class="number">94</span>, <span class="number">95</span>, <span class="number">96</span>, <span class="number">97</span>, <span class="number">98</span>, <span class="number">99</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L[:<span class="number">10</span>:<span class="number">2</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L[::<span class="number">5</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">65</span>, <span class="number">70</span>, <span class="number">75</span>, <span class="number">80</span>, <span class="number">85</span>, <span class="number">90</span>, <span class="number">95</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>tuple也是一种list，唯一区别是tuple不可变。因此，tuple也可以用切片操作，只是操作的结果仍是tuple</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)[:<span class="number">3</span>]</span><br><span class="line">(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>字符串<code>&#39;xxx&#39;</code>或Unicode字符串<code>u&#39;xxx&#39;</code>也可以看成是一种list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'ABCDEFG'</span>[:<span class="number">3</span>]</span><br><span class="line"><span class="string">'ABC'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'ABCDEFG'</span>[::<span class="number">2</span>]</span><br><span class="line"><span class="string">'ACEG'</span></span><br></pre></td></tr></table></figure>
<h2 id="2-Iteration"><a href="#2-Iteration" class="headerlink" title="2. Iteration"></a>2. Iteration</h2><p>只要是可迭代对象，无论有无下标，都可以迭代，比如dict就可以迭代</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;list.length; i++) &#123;</span><br><span class="line">    n = list[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">d = &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> d:</span><br><span class="line">    <span class="keyword">print</span> key</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> ch <span class="keyword">in</span> <span class="string">'ABC'</span>:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> ch</span><br><span class="line">...</span><br><span class="line">A</span><br><span class="line">B</span><br><span class="line">C</span><br></pre></td></tr></table></figure>
<p>如何判断一个对象是可迭代对象呢？方法是通过<code>collections模块</code>的<code>Iterable</code>类型判断：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> Iterable</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(<span class="string">'abc'</span>, Iterable) <span class="comment"># str是否可迭代</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], Iterable) <span class="comment"># list是否可迭代</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(<span class="number">123</span>, Iterable) <span class="comment"># 整数是否可迭代</span></span><br><span class="line"><span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<p>如果要对list实现类似Java那样的下标循环怎么办？Python内置的<code>enumerate</code>函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i, value <span class="keyword">in</span> enumerate([<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>]):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> i, value</span><br><span class="line">...</span><br><span class="line"><span class="number">0</span> A</span><br><span class="line"><span class="number">1</span> B</span><br><span class="line"><span class="number">2</span> C</span><br></pre></td></tr></table></figure>
<p><code>for</code>循环里，同时引用了两个变量，在Python里是很常见</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> x, y <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">4</span>), (<span class="number">3</span>, <span class="number">9</span>)]:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> x, y</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">2</span> <span class="number">4</span></span><br><span class="line"><span class="number">3</span> <span class="number">9</span></span><br></pre></td></tr></table></figure>
<h2 id="3-List-Generation"><a href="#3-List-Generation" class="headerlink" title="3. List Generation"></a>3. List Generation</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>) <span class="keyword">if</span> x % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line">[<span class="number">4</span>, <span class="number">16</span>, <span class="number">36</span>, <span class="number">64</span>, <span class="number">100</span>]</span><br></pre></td></tr></table></figure>
<p>还可以使用两层循环，可以生成全排列：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>[m + n <span class="keyword">for</span> m <span class="keyword">in</span> <span class="string">'ABC'</span> <span class="keyword">for</span> n <span class="keyword">in</span> <span class="string">'XYZ'</span>]</span><br><span class="line">[<span class="string">'AX'</span>, <span class="string">'AY'</span>, <span class="string">'AZ'</span>, <span class="string">'BX'</span>, <span class="string">'BY'</span>, <span class="string">'BZ'</span>, <span class="string">'CX'</span>, <span class="string">'CY'</span>, <span class="string">'CZ'</span>]</span><br></pre></td></tr></table></figure>
<p>运用列表生成式，可以写出非常简洁的代码。</p>
<p>例如，列出当前目录下的所有文件和目录名</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os <span class="comment"># 导入os模块，模块的概念后面讲到</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[d <span class="keyword">for</span> d <span class="keyword">in</span> os.listdir(<span class="string">'.'</span>)] <span class="comment"># os.listdir可以列出文件和目录</span></span><br><span class="line">[<span class="string">'.emacs.d'</span>, <span class="string">'.ssh'</span>, <span class="string">'.Trash'</span>, <span class="string">'Adlm'</span>, <span class="string">'Applications'</span>, <span class="string">'Desktop'</span>, <span class="string">'Documents'</span>, <span class="string">'Downloads'</span>, <span class="string">'Library'</span>, <span class="string">'Movies'</span>, <span class="string">'Music'</span>, <span class="string">'Pictures'</span>, <span class="string">'Public'</span>, <span class="string">'VirtualBox VMs'</span>, <span class="string">'Workspace'</span>, <span class="string">'XCode'</span>]</span><br></pre></td></tr></table></figure>
<p>for循环其实可以同时使用两个甚至多个变量，比如<code>dict</code>的<code>iteritems()</code>可以同时迭代key和value：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = &#123;<span class="string">'x'</span>: <span class="string">'A'</span>, <span class="string">'y'</span>: <span class="string">'B'</span>, <span class="string">'z'</span>: <span class="string">'C'</span> &#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> k, v <span class="keyword">in</span> d.iteritems():</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> k, <span class="string">'='</span>, v</span><br><span class="line"><span class="meta">... </span></span><br><span class="line">y = B</span><br><span class="line">x = A</span><br><span class="line">z = C</span><br></pre></td></tr></table></figure>
<p>列表生成式也可以使用两个变量来生成list：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = &#123;<span class="string">'x'</span>: <span class="string">'A'</span>, <span class="string">'y'</span>: <span class="string">'B'</span>, <span class="string">'z'</span>: <span class="string">'C'</span> &#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[k + <span class="string">'='</span> + v <span class="keyword">for</span> k, v <span class="keyword">in</span> d.iteritems()]</span><br><span class="line">[<span class="string">'y=B'</span>, <span class="string">'x=A'</span>, <span class="string">'z=C'</span>]</span><br></pre></td></tr></table></figure>
<p>最后把一个list中所有的字符串变成小写：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = [<span class="string">'Hello'</span>, <span class="string">'World'</span>, <span class="string">'IBM'</span>, <span class="string">'Apple'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> L]</span><br><span class="line">[<span class="string">'hello'</span>, <span class="string">'world'</span>, <span class="string">'ibm'</span>, <span class="string">'apple'</span>]</span><br></pre></td></tr></table></figure>
<p><strong>小结</strong></p>
<p>运用列表生成式，可以快速生成list，可以通过一个list推导出另一个list，而代码却十分简洁。</p>
<h2 id="4-Generator"><a href="#4-Generator" class="headerlink" title="4. Generator"></a>4. Generator</h2><p>如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器（<code>Generator</code>）</p>
<p>要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的<code>[]</code>改成<code>()</code>，就创建了一个generator：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = [x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>g = (x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>g</span><br><span class="line">&lt;generator object &lt;genexpr&gt; at <span class="number">0x104feab40</span>&gt;</span><br></pre></td></tr></table></figure>
<p>generator 是一个可迭代的对象</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>g = (x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> g:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="number">25</span></span><br><span class="line"><span class="number">36</span></span><br><span class="line"><span class="number">49</span></span><br><span class="line"><span class="number">64</span></span><br><span class="line"><span class="number">81</span></span><br></pre></td></tr></table></figure>
<p>斐波拉契数列</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(max)</span>:</span></span><br><span class="line">    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; max:</span><br><span class="line">        <span class="keyword">print</span> b</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">        n = n + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p><code>fib</code>函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。</p>
<p>把<code>fib</code>函数变成<code>generator</code>，只需要把 <code>print b</code> 改为 <code>yield b</code> 就可以了</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(max)</span>:</span></span><br><span class="line">    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; max:</span><br><span class="line">        <span class="keyword">yield</span> b</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">        n = n + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果一个函数定义中包含 <code>yield</code>关键字，那么这个函数就不再是一个普通函数，而是一个generator</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">6</span>)</span><br><span class="line">&lt;generator object fib at <span class="number">0x104feaaa0</span>&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>函数是顺序执行，遇到<code>return</code>语句或者最后一行函数语句就返回。而变成<code>generator</code>的函数，在每次调用<code>next()</code>的时候执行，遇到<code>yield</code>语句返回，再次执行时从上次返回的<code>yield</code> 语句处继续执行。</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">odd</span><span class="params">()</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> <span class="string">'step 1'</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> <span class="string">'step 2'</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">yield</span> <span class="number">3</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> <span class="string">'step 3'</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">yield</span> <span class="number">5</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o = odd()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o.next()</span><br><span class="line">step <span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o.next()</span><br><span class="line">step <span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o.next()</span><br><span class="line">step <span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o.next()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure>
<p><strong>小结</strong></p>
<p>generator是非常强大的工具，在Python中，可以简单地把列表生成式改成generator，也可以通过函数实现复杂逻辑的generator。</p>
<p>要理解generator的工作原理，它是在for循环的过程中不断计算出下一个元素，并在适当的条件结束<code>for</code>循环。对于函数改成的generator来说，遇到return语句或者执行到函数体最后一行语句，就是结束generator的指令，<code>for</code>循环随之结束。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/0013868196169906eb9ca5864384546bf3405ae6a172b3e000" target="_blank" rel="external">廖雪峰的官方网站</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python def函数、函数参数、函数默认参数]]></title>
      <url>http://sggo.me/2017/06/03/python/py-language-5-function/</url>
      <content type="html"><![CDATA[<p>定义函数、内置函数、函数参数、函数默认参数</p>
<a id="more"></a>
<h2 id="1-内置函数"><a href="#1-内置函数" class="headerlink" title="1. 内置函数"></a>1. 内置函数</h2><p>内置函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>abs(<span class="number">-12.34</span>)</span><br><span class="line"><span class="number">12.34</span></span><br></pre></td></tr></table></figure>
<p><a href="https://docs.python.org/2/library/functions.html#abs" target="_blank" rel="external">abs api</a></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cmp(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="number">-1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cmp(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>数据类型转换</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>int(<span class="string">'123'</span>)</span><br><span class="line"><span class="number">123</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>int(<span class="number">12.34</span>)</span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>float(<span class="string">'12.34'</span>)</span><br><span class="line"><span class="number">12.34</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>str(<span class="number">1.23</span>)</span><br><span class="line"><span class="string">'1.23'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>unicode(<span class="number">100</span>)</span><br><span class="line"><span class="string">u'100'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bool(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bool(<span class="string">''</span>)</span><br><span class="line"><span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<p>函数名其实就是指向一个函数对象的引用，完全可以把函数名赋给一个变量，相当于给这个函数起了一个“别名”：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = abs <span class="comment"># 变量a指向abs函数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a(<span class="number">-1</span>) <span class="comment"># 所以也可以通过a调用abs函数</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="2-def函数"><a href="#2-def函数" class="headerlink" title="2. def函数"></a>2. def函数</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_abs</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -x</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>return None</code> 可以简写为 <code>return</code>。</p>
</blockquote>
<h3 id="2-1-空函数"><a href="#2-1-空函数" class="headerlink" title="2.1 空函数"></a>2.1 空函数</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nop</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-函数参数类型检查"><a href="#2-2-函数参数类型检查" class="headerlink" title="2.2 函数参数类型检查"></a>2.2 函数参数类型检查</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_abs</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(x, (int, float)):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'bad operand type'</span>)</span><br><span class="line">    <span class="keyword">if</span> x &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -x</span><br></pre></td></tr></table></figure>
<h3 id="2-3-返回多个值-是一个tuple"><a href="#2-3-返回多个值-是一个tuple" class="headerlink" title="2.3 返回多个值 (是一个tuple)"></a>2.3 返回多个值 (是一个tuple)</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">move</span><span class="params">(x, y, step, angle=<span class="number">0</span>)</span>:</span></span><br><span class="line">    nx = x + step * math.cos(angle)</span><br><span class="line">    ny = y - step * math.sin(angle)</span><br><span class="line">    <span class="keyword">return</span> nx, ny</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x, y = move(<span class="number">100</span>, <span class="number">100</span>, <span class="number">60</span>, math.pi / <span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> x, y</span><br><span class="line"><span class="number">151.961524227</span> <span class="number">70.0</span></span><br></pre></td></tr></table></figure>
<h2 id="3-函数参数"><a href="#3-函数参数" class="headerlink" title="3. 函数参数"></a>3. 函数参数</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n=<span class="number">2</span>)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<blockquote>
<p>一是必选参数在前，默认参数在后，否则Python的解释器会报错</p>
<p>二是如何设置默认参数。</p>
<p>当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。</p>
<p>使用默认参数有什么好处？最大的好处是能降低调用函数的难度。</p>
<p>定义默认参数要牢记一点：<code>默认参数必须指向不变对象！</code></p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_end</span><span class="params">(L=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> L <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        L = []</span><br><span class="line">    L.append(<span class="string">'END'</span>)</span><br><span class="line">    <span class="keyword">return</span> L</span><br></pre></td></tr></table></figure>
<blockquote>
<p>为什么要设计str、None这样的不变对象呢？因为不变对象一旦创建，对象内部的数据就不能修改，这样就减少了由于修改数据导致的错误。此外，由于对象不变，多任务环境下同时读取对象不需要加锁，同时读一点问题都没有。我们在编写程序时，如果可以设计一个不变对象，那就尽量设计成不变对象。</p>
</blockquote>
<p><strong>可变参数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc</span><span class="params">(numbers)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> numbers:</span><br><span class="line">        sum = sum + n * n</span><br><span class="line">    <span class="keyword">return</span> sum</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>calc([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>calc((<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>))</span><br><span class="line"><span class="number">84</span></span><br></pre></td></tr></table></figure>
<p>如果利用可变参数，调用函数的方式可以简化成这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> numbers:</span><br><span class="line">        sum = sum + n * n</span><br><span class="line">    <span class="keyword">return</span> sum</span><br></pre></td></tr></table></figure>
<p>定义可变参数和定义list或tuple参数相比，仅仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此，函数代码完全不变。但是，调用该函数时，可以传入任意个参数，包括0个参数：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>calc(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>calc()</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这种写法相当有用，而且很常见，见如下 :</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>calc(*nums)</span><br><span class="line"><span class="number">14</span></span><br></pre></td></tr></table></figure>
<p><strong>关键字参数</strong></p>
<p><code>可变参数</code>允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。</p>
<p><code>关键字参数</code>允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。</p>
<p>请看示例：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person</span><span class="params">(name, age, **kw)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'name:'</span>, name, <span class="string">'age:'</span>, age, <span class="string">'other:'</span>, kw</span><br></pre></td></tr></table></figure>
<p>函数<code>person</code>除了必选参数<code>name</code>和<code>age</code>外，还接受关键字参数<code>kw</code>。在调用该函数时，可以只传入必选参数; 也可以传入任意个数的关键字参数;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>person(<span class="string">'Bob'</span>, <span class="number">35</span>, city=<span class="string">'Beijing'</span>)</span><br><span class="line">name: Bob age: <span class="number">35</span> other: &#123;<span class="string">'city'</span>: <span class="string">'Beijing'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>person(<span class="string">'Adam'</span>, <span class="number">45</span>, gender=<span class="string">'M'</span>, job=<span class="string">'Engineer'</span>)</span><br><span class="line">name: Adam age: <span class="number">45</span> other: &#123;<span class="string">'gender'</span>: <span class="string">'M'</span>, <span class="string">'job'</span>: <span class="string">'Engineer'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>kw = &#123;<span class="string">'city'</span>: <span class="string">'Beijing'</span>, <span class="string">'job'</span>: <span class="string">'Engineer'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>person(<span class="string">'Jack'</span>, <span class="number">24</span>, **kw)</span><br><span class="line">name: Jack age: <span class="number">24</span> other: &#123;<span class="string">'city'</span>: <span class="string">'Beijing'</span>, <span class="string">'job'</span>: <span class="string">'Engineer'</span>&#125;</span><br></pre></td></tr></table></figure>
<p><strong>参数组合</strong></p>
<p>在Python中定义函数，可以用必选参数、默认参数、可变参数和关键字参数，这4种参数都可以一起使用，或者只用其中某些，但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数和关键字参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(a, b, c=<span class="number">0</span>, *args, **kw)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'a ='</span>, a, <span class="string">'b ='</span>, b, <span class="string">'c ='</span>, c, <span class="string">'args ='</span>, args, <span class="string">'kw ='</span>, kw</span><br></pre></td></tr></table></figure>
<p>在函数调用的时候，Python解释器自动按照参数位置和参数名把对应的参数传进去。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a = <span class="number">1</span> b = <span class="number">2</span> c = <span class="number">0</span> args = () kw = &#123;&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">1</span>, <span class="number">2</span>, c=<span class="number">3</span>)</span><br><span class="line">a = <span class="number">1</span> b = <span class="number">2</span> c = <span class="number">3</span> args = () kw = &#123;&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line">a = <span class="number">1</span> b = <span class="number">2</span> c = <span class="number">3</span> args = (<span class="string">'a'</span>, <span class="string">'b'</span>) kw = &#123;&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, x=<span class="number">99</span>)</span><br><span class="line">a = <span class="number">1</span> b = <span class="number">2</span> c = <span class="number">3</span> args = (<span class="string">'a'</span>, <span class="string">'b'</span>) kw = &#123;<span class="string">'x'</span>: <span class="number">99</span>&#125;</span><br></pre></td></tr></table></figure>
<p>神奇的是通过一个tuple和dict，你也可以调用该函数：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>args = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kw = &#123;<span class="string">'x'</span>: <span class="number">99</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(*args, **kw)</span><br><span class="line">a = <span class="number">1</span> b = <span class="number">2</span> c = <span class="number">3</span> args = (<span class="number">4</span>,) kw = &#123;<span class="string">'x'</span>: <span class="number">99</span>&#125;</span><br></pre></td></tr></table></figure>
<p>所以，对于任意函数，都可以通过类似<code>func(*args, **kw)</code>的形式调用它，无论它的参数是如何定义的。</p>
<p><strong>小结</strong></p>
<p>Python的函数具有非常灵活的参数形态，既可以实现简单的调用，又可以传入非常复杂的参数。</p>
<p>默认参数一定要用不可变对象，如果是可变对象，运行会有逻辑错误！</p>
<p>要注意定义可变参数和关键字参数的语法：</p>
<p><code>*args</code>是可变参数，args接收的是一个tuple；</p>
<p><code>**kw</code>是关键字参数，kw接收的是一个dict。</p>
<p>以及调用函数时如何传入可变参数和关键字参数的语法：</p>
<p>可变参数既可直接传入：<code>func(1, 2, 3)</code>，又可先组装list或tuple，再通过<code>*args</code>传入：<code>func(*(1, 2, 3))</code>；</p>
<p>关键字参数既可直接传入：<code>func(a=1, b=2)</code>，又可先组装dict，再通过<code>**kw</code>传入：<code>func(**{&#39;a&#39;: 1, &#39;b&#39;: 2})</code>。</p>
<p>使用<code>*args</code>和<code>**kw</code>是Python的习惯写法，当然也可以用其他参数名，但最好使用习惯用法。</p>
<h3 id="递归函数"><a href="#递归函数" class="headerlink" title="递归函数"></a>递归函数</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fact</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> n * fact(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用递归函数需要注意防止栈溢出。在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出。</p>
<p>使用递归函数的优点是逻辑简单清晰，缺点是过深的调用会导致栈溢出。</p>
</blockquote>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/0013868196169906eb9ca5864384546bf3405ae6a172b3e000" target="_blank" rel="external">廖雪峰的官方网站</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python 集合 List、Tuple、Dict、 Set]]></title>
      <url>http://sggo.me/2017/05/31/python/py-language-4-collection-type/</url>
      <content type="html"><![CDATA[<p>Python 集合 List、Tuple、Dict、 Set</p>
<a id="more"></a>
<h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates = [<span class="string">'Michael'</span>, <span class="string">'Bob'</span>, <span class="string">'Tracy'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">'Michael'</span>, <span class="string">'Bob'</span>, <span class="string">'Tracy'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="append"><a href="#append" class="headerlink" title="append"></a>append</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates[<span class="number">-1</span>]</span><br><span class="line"><span class="string">'Tracy'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates.append(<span class="string">'Adam'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">'Michael'</span>, <span class="string">'Bob'</span>, <span class="string">'Tracy'</span>, <span class="string">'Adam'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="insert"><a href="#insert" class="headerlink" title="insert"></a>insert</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates.insert(<span class="number">1</span>, <span class="string">'Jack'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">'Michael'</span>, <span class="string">'Jack'</span>, <span class="string">'Bob'</span>, <span class="string">'Tracy'</span>, <span class="string">'Adam'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="pop"><a href="#pop" class="headerlink" title="pop"></a>pop</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates.pop()</span><br><span class="line"><span class="string">'Adam'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">'Michael'</span>, <span class="string">'Jack'</span>, <span class="string">'Bob'</span>, <span class="string">'Tracy'</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>要删除list末尾的元素，用<code>pop()</code>方法</p>
</blockquote>
<h3 id="pop-i"><a href="#pop-i" class="headerlink" title="pop(i)"></a>pop(i)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates.pop(<span class="number">1</span>)</span><br><span class="line"><span class="string">'Jack'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates</span><br><span class="line">[<span class="string">'Michael'</span>, <span class="string">'Bob'</span>, <span class="string">'Tracy'</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>要删除指定位置的元素，用<code>pop(i)</code>方法，其中<code>i</code>是索引位置：</p>
</blockquote>
<h3 id="list-元素类型不同"><a href="#list-元素类型不同" class="headerlink" title="list 元素类型不同"></a>list 元素类型不同</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = [<span class="string">'Apple'</span>, <span class="number">123</span>, <span class="keyword">True</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>list 里面的元素的数据类型也可以不同</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = [<span class="string">'python'</span>, <span class="string">'java'</span>, [<span class="string">'asp'</span>, <span class="string">'php'</span>], <span class="string">'scheme'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(s)</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>p = [<span class="string">'asp'</span>, <span class="string">'php'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = [<span class="string">'python'</span>, <span class="string">'java'</span>, p, <span class="string">'scheme'</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>list元素也可以是另一个list</p>
</blockquote>
<h2 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h2><p><code>tuple</code>一旦初始化就不能修改,元素指向不改变</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classmates = (<span class="string">'Michael'</span>, <span class="string">'Bob'</span>, <span class="string">'Tracy'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>如果要定义一个空的tuple，可以写成<code>()</code></p>
<blockquote>
<p>tuple 的陷阱</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; t = (1)</span><br><span class="line">&gt;&gt;&gt; t</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p>正确的方式如下 :</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; t = (1,)</span><br><span class="line">&gt;&gt;&gt; t</span><br><span class="line">(1,)</span><br></pre></td></tr></table></figure>
<p>最后来看一个“可变的”tuple：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; t = (&apos;a&apos;, &apos;b&apos;, [&apos;A&apos;, &apos;B&apos;])</span><br><span class="line">&gt;&gt;&gt; t[2][0] = &apos;X&apos;</span><br><span class="line">&gt;&gt;&gt; t[2][1] = &apos;Y&apos;</span><br><span class="line">&gt;&gt;&gt; t</span><br><span class="line">(&apos;a&apos;, &apos;b&apos;, [&apos;X&apos;, &apos;Y&apos;])</span><br></pre></td></tr></table></figure>
<p>其实变的不是tuple的元素，而是list的元素。tuple一开始指向的list并没有改成别的list</p>
<h2 id="Dict"><a href="#Dict" class="headerlink" title="Dict"></a>Dict</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = &#123;<span class="string">'Michael'</span>: <span class="number">95</span>, <span class="string">'Bob'</span>: <span class="number">75</span>, <span class="string">'Tracy'</span>: <span class="number">85</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d[<span class="string">'Michael'</span>]</span><br><span class="line"><span class="number">95</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d[<span class="string">'Adam'</span>] = <span class="number">67</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d[<span class="string">'Adam'</span>]</span><br><span class="line"><span class="number">67</span></span><br></pre></td></tr></table></figure>
<h3 id="判断-key-是否存在"><a href="#判断-key-是否存在" class="headerlink" title="判断 key 是否存在"></a>判断 key 是否存在</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'Thomas'</span> <span class="keyword">in</span> d</span><br><span class="line"><span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<p>二是通过dict提供的get方法，如果key不存在，可以返回None，或者自己指定的value：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.get(<span class="string">'Thomas'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.get(<span class="string">'Thomas'</span>, <span class="number">-1</span>)</span><br><span class="line"><span class="number">-1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：返回None的时候Python的交互式命令行不显示结果。</p>
</blockquote>
<h3 id="删除-key，pop-key"><a href="#删除-key，pop-key" class="headerlink" title="删除 key，pop(key)"></a>删除 key，pop(key)</h3><p>要删除一个key，用pop(key)方法，对应的value也会从dict中删除：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.pop(<span class="string">'Bob'</span>)</span><br><span class="line"><span class="number">75</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">&#123;<span class="string">'Michael'</span>: <span class="number">95</span>, <span class="string">'Tracy'</span>: <span class="number">85</span>&#125;</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>dict</th>
<th>list</th>
</tr>
</thead>
<tbody>
<tr>
<td>查找和插入的速度极快，不会随着key的增加而增加</td>
<td>查找和插入的时间随着元素的增加而增加</td>
</tr>
<tr>
<td>需要占用大量的内存，内存浪费多</td>
<td>占用空间小，浪费内存很少</td>
</tr>
</tbody>
</table>
<p><strong>所以，dict是用空间来换取时间的一种方法。</strong></p>
<blockquote>
<p>dict可以用在需要高速查找的很多地方，在Python代码中几乎无处不在，正确使用dict非常重要，需要牢记的第一条就是dict的key必须是不可变对象。</p>
<p>是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash）。</p>
<p>保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key：</p>
</blockquote>
<h2 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h2><p>set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。</p>
<h3 id="set-init"><a href="#set-init" class="headerlink" title="set init"></a>set init</h3><p>要创建一个set，需要提供一个list作为输入集合：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，传入的参数<code>[1, 2, 3]</code>是一个list，而显示的<code>set([1, 2, 3])</code>只是告诉你这个set内部有1，2，3这3个元素，显示的[]不表示这是一个list。</p>
</blockquote>
<h3 id="add、remove"><a href="#add、remove" class="headerlink" title="add、remove"></a>add、remove</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = set([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.add(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.add(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.remove(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h3 id="set-amp-and"><a href="#set-amp-and" class="headerlink" title="set &amp; and |"></a>set &amp; and |</h3><p>set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s1 = set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s2 = set([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s1 &amp; s2</span><br><span class="line">set([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s1 | s2</span><br><span class="line">set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>set和dict的唯一区别仅在于没有存储对应的value，set的原理和dict一样.</p>
</blockquote>
<h3 id="difference、intersection"><a href="#difference、intersection" class="headerlink" title="difference、intersection"></a>difference、intersection</h3><p>我们还能进行一些筛选操作, 比如对比另一个东西, 看看原来的 set 里有没有和他不同的 (difference). 或者对比另一个东西, 看看 set 里有没有相同的 (intersection).</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">print(unique_char)</span><br><span class="line"># &#123;'b', 'c', 'a'&#125;</span><br><span class="line"></span><br><span class="line">unique_char = set(char_list)</span><br><span class="line">print(unique_char.difference(&#123;'a', 'e', 'i'&#125;))</span><br><span class="line"># &#123;'b', 'd', 'c'&#125;</span><br><span class="line"></span><br><span class="line">print(unique_char.intersection(&#123;'a', 'e', 'i'&#125;))</span><br><span class="line"># &#123;'a'&#125;</span><br></pre></td></tr></table></figure>
<h2 id="议不可变对象"><a href="#议不可变对象" class="headerlink" title="议不可变对象"></a>议不可变对象</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.sort()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="string">'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.replace(<span class="string">'a'</span>, <span class="string">'A'</span>)</span><br><span class="line"><span class="string">'Abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line"><span class="string">'abc'</span></span><br></pre></td></tr></table></figure>
<p>小结 :</p>
<p>使用key-value存储结构的dict在Python中非常有用，选择不可变对象作为key很重要，最常用的key是字符串。</p>
<p><code>tuple</code> 虽然是不变对象，但试试把 <code>(1, 2, 3)</code> 和 <code>(1, [2, 3])</code> 放入dict或set中，并解释结果。</p>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000" target="_blank" rel="external">廖雪峰的官方网站</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python 字符编码 & 字符串]]></title>
      <url>http://sggo.me/2017/05/31/python/py-language-3-str-&-character-encoding/</url>
      <content type="html"><![CDATA[<p>Python 字符编码 大概原理 与 字符串简单操作</p>
<a id="more"></a>
<h2 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h2><p>因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。最早的计算机在设计时采用8个比特（bit）作为一个字节（byte），所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255），如果要表示更大的整数，就必须用更多的字节。比如两个字节可以表示的最大整数是65535，4个字节可以表示的最大整数是4294967295。</p>
<p>由于计算机是美国人发明的，因此，最早只有127个字母被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。</p>
<p>但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。</p>
<p><img src="/images/python/language-str-encoding.png" alt=""></p>
<p>因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。</p>
<p>Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。</p>
<p>字母<code>A</code>用ASCII编码是十进制的<code>65</code>，二进制的01000001；</p>
<p>字符<code>0</code>用ASCII编码是十进制的<code>48</code>，二进制的00110000，注意字符’0’和整数0是不同的；</p>
<p>你可以猜测，如果把ASCII编码的<code>A</code>用Unicode编码，只需要在前面补0就可以，因此，A的Unicode编码是00000000 01000001。</p>
<p>新的问题又出现了：如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。</p>
<p>所以，本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的<code>UTF-8</code>编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间：</p>
<blockquote>
<p>在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。</p>
</blockquote>
<h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><p>因为Python的诞生比Unicode标准发布的时间还要早，所以最早的Python只支持ASCII编码，普通的字符串<code>&#39;ABC&#39;</code>在Python内部都是ASCII编码的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ord(<span class="string">'A'</span>)</span><br><span class="line"><span class="number">65</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>chr(<span class="number">65</span>)</span><br><span class="line"><span class="string">'A'</span></span><br></pre></td></tr></table></figure>
<p>Python在后来添加了对Unicode的支持，以Unicode表示的字符串用<code>u&#39;...&#39;</code>表示，比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> <span class="string">u'中文'</span></span><br><span class="line">中文</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">u'中'</span></span><br><span class="line"><span class="string">u'\u4e2d'</span></span><br></pre></td></tr></table></figure>
<p>写<code>u&#39;中&#39;</code>和<code>u&#39;\u4e2d&#39;</code>是一样的，<code>\u</code>后面是十六进制的Unicode码。因此，<code>u&#39;A&#39;</code>和<code>u&#39;\u0041&#39;</code>也一样的。</p>
<p>两种字符串如何相互转换？字符串<code>&#39;xxx&#39;</code>虽然是ASCII编码，但也可以看成是UTF-8编码，而<code>u&#39;xxx&#39;</code>则只能是Unicode编码。</p>
<p>把<code>u&#39;xxx&#39;</code>转换为UTF-8编码的<code>&#39;xxx&#39;</code>用<code>encode(&#39;utf-8&#39;)</code>方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">u'ABC'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'ABC'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">u'中文'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'\xe4\xb8\xad\xe6\x96\x87'</span></span><br></pre></td></tr></table></figure>
<p>英文字符转换后表示的UTF-8的值和Unicode值相等（但占用的存储空间不同），而中文字符转换后1个Unicode字符将变为3个UTF-8字符，你看到的<code>\xe4</code>就是其中一个字节，因为它的值是<code>228</code>，没有对应的字母可以显示，所以以十六进制显示字节的数值。<code>len()</code>函数可以返回字符串的长度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(<span class="string">u'ABC'</span>)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(<span class="string">'ABC'</span>)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(<span class="string">u'中文'</span>)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(<span class="string">'\xe4\xb8\xad\xe6\x96\x87'</span>)</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>反过来，把UTF-8编码表示的字符串<code>&#39;xxx&#39;</code>转换为Unicode字符串<code>u&#39;xxx&#39;</code>用<code>decode(&#39;utf-8&#39;)</code>方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; &apos;abc&apos;.decode(&apos;utf-8&apos;)</span><br><span class="line">u&apos;abc&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;\xe4\xb8\xad\xe6\x96\x87&apos;.decode(&apos;utf-8&apos;)</span><br><span class="line">u&apos;\u4e2d\u6587&apos;</span><br><span class="line">&gt;&gt;&gt; print &apos;\xe4\xb8\xad\xe6\x96\x87&apos;.decode(&apos;utf-8&apos;)</span><br><span class="line">中文</span><br></pre></td></tr></table></figure>
<h3 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'Hello, %s'</span> % <span class="string">'world'</span></span><br><span class="line"><span class="string">'Hello, world'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'Hi, %s, you have $%d.'</span> % (<span class="string">'Michael'</span>, <span class="number">1000000</span>)</span><br><span class="line"><span class="string">'Hi, Michael, you have $1000000.'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'%2d-%02d'</span> % (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="string">' 3-01'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'%.2f'</span> % <span class="number">3.1415926</span></span><br><span class="line"><span class="string">'3.14'</span></span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000" target="_blank" rel="external">廖雪峰的官方网站</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python Output、Variable、dataType、If、While/For、Py Head]]></title>
      <url>http://sggo.me/2017/05/31/python/py-language-2-Output-Variable-dataType-If-While:For-PyHead/</url>
      <content type="html"><![CDATA[<p>Python 的 print 语句、Variable 变量定义、 数据类型、条件与循环</p>
<a id="more"></a>
<h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p><code>print</code>语句也可以跟上多个字符串，用逗号“,”隔开，就可以连成一串输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'The quick brown fox'</span>, <span class="string">'jumps over'</span>, <span class="string">'the lazy dog'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The quick brown fox jumps over the lazy dog
</code></pre><blockquote>
<p><code>print</code>会依次打印每个字符串，遇到逗号“,”会输出一个空格</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(int(<span class="string">'2'</span>)+<span class="number">3</span>) <span class="comment"># int 字符串会转为整数</span></span><br><span class="line">print(int(<span class="number">1.9</span>))  <span class="comment"># int会保留整数部分</span></span><br><span class="line">print(float(<span class="string">'1.2'</span>)+<span class="number">3</span>) <span class="comment">#float()是浮点型，可以把字符串转换成小数</span></span><br></pre></td></tr></table></figure>
<pre><code>5
1
4.2
</code></pre><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>name = input()</span><br><span class="line">Blair</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">'Hello,'</span>, name)</span><br><span class="line">Hello, Blair</span><br></pre></td></tr></table></figure>
<h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a,b,c=<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span></span><br><span class="line">print(a,b,c)</span><br></pre></td></tr></table></figure>
<pre><code>11 12 13
</code></pre><h3 id="global-var"><a href="#global-var" class="headerlink" title="global var"></a>global var</h3><p>那如何在外部也能调用一个在局部里修改了的全局变量呢. 首先我们在外部定义一个全局变量 <code>a=None</code>, 然后再 <code>fun()</code> 中声明 这个 <code>a</code> 是来自外部的 <code>a</code>. 声明方式就是 <code>global a</code>. 然后对这个外部的 <code>a</code> 修改后, 修改的效果会被施加到外部的 <code>a</code> 上. 所以我们将能看到运行完 <code>fun()</code>, <code>a</code> 的值从 <code>None</code> 变成了 <code>20</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">APPLY = <span class="number">100</span> <span class="comment"># 全局变量</span></span><br><span class="line">a = <span class="keyword">None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> a    <span class="comment"># 使用之前在全局里定义的 a</span></span><br><span class="line">    a = <span class="number">20</span>      <span class="comment"># 现在的 a 是全局变量了</span></span><br><span class="line">    <span class="keyword">return</span> a+<span class="number">100</span></span><br><span class="line"></span><br><span class="line">print(APPLE)    <span class="comment"># 100</span></span><br><span class="line">print(<span class="string">'a past:'</span>, a)  <span class="comment"># None</span></span><br><span class="line">fun()</span><br><span class="line">print(<span class="string">'a now:'</span>, a)   <span class="comment"># 20</span></span><br></pre></td></tr></table></figure>
<h2 id="dataType"><a href="#dataType" class="headerlink" title="dataType"></a>dataType</h2><table>
<thead>
<tr>
<th>序号</th>
<th>data type</th>
<th>example value</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>int</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>float</td>
<td>1.2</td>
</tr>
<tr>
<td>3</td>
<td>str</td>
<td>‘hello’ or “hello”</td>
</tr>
<tr>
<td>4</td>
<td>boolean</td>
<td>True/False </td>
</tr>
<tr>
<td>5</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>6</td>
<td>常量</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="if-else"><a href="#if-else" class="headerlink" title="if/else"></a>if/else</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">age = <span class="number">20</span></span><br><span class="line"><span class="keyword">if</span> age &gt;= <span class="number">6</span>:</span><br><span class="line">    print(<span class="string">'teenager'</span>)</span><br><span class="line"><span class="keyword">elif</span> age &gt;= <span class="number">18</span>:</span><br><span class="line">    print(<span class="string">'adult'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'kid'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>teenager
</code></pre><h3 id="if-while-遇-None"><a href="#if-while-遇-None" class="headerlink" title="if/while 遇 None"></a>if/while 遇 None</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v1 = <span class="keyword">None</span></span><br><span class="line"><span class="keyword">if</span> v1:</span><br><span class="line">   print(<span class="string">'v1'</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果 if / while 后面接着的语句数据类型 None, 将与 False 处理方式相同</p>
</blockquote>
<h3 id="if-while-遇-空集合"><a href="#if-while-遇-空集合" class="headerlink" title="if/while 遇 空集合"></a>if/while 遇 空集合</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = []</span><br><span class="line"><span class="keyword">if</span> A:</span><br><span class="line">    print(<span class="string">"A is empty !"</span>)</span><br><span class="line"></span><br><span class="line">A = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">if</span> A:</span><br><span class="line">    print(<span class="string">"A is not empty !"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>A is not empty !
</code></pre><blockquote>
<p>在 Python 中集合类型有 list、 tuple 、dict 和 set 等，如果该集合对象作为 if 或 while 判断语句, 则与 False 处理方式相同</p>
</blockquote>
<h2 id="While-For"><a href="#While-For" class="headerlink" title="While/For"></a>While/For</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = range(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">while</span> a:</span><br><span class="line">    print(a[<span class="number">-1</span>])</span><br><span class="line">    a = a[:len(a)<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<pre><code>4
3
2
1
0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">names = [<span class="string">'Michael'</span>, <span class="string">'Bob'</span>, <span class="string">'Tracy'</span>]</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> names:</span><br><span class="line">    print(name)</span><br></pre></td></tr></table></figure>
<pre><code>Michael
Bob
Tracy
</code></pre><h2 id="py-程序头部"><a href="#py-程序头部" class="headerlink" title="py 程序头部"></a>py 程序头部</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br></pre></td></tr></table></figure>
<p>第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释；</p>
<p>第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，在源代码中写的中文输出可能会乱码。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000" target="_blank" rel="external">morvanzhou python</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/python-basic/" target="_blank" rel="external">廖雪峰的官方网站</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera 6 - Machine Learning System Design *]]></title>
      <url>http://sggo.me/2017/05/29/ml/coursera-ng-w6-Machine-Learning-System-Design/</url>
      <content type="html"><![CDATA[<p>coursera week 6 - machine learning system design * </p>
<a id="more"></a>
<h2 id="1-Building-a-Spam-Classifier"><a href="#1-Building-a-Spam-Classifier" class="headerlink" title="1. Building a Spam Classifier"></a>1. Building a Spam Classifier</h2><h3 id="1-1-prioritize-what-to-work-on"><a href="#1-1-prioritize-what-to-work-on" class="headerlink" title="1.1 prioritize what to work on"></a>1.1 prioritize what to work on</h3><p>System Design Example:</p>
<p>Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam or not.</p>
<p><img src="/images/ml/ml-ng-w6-02-01.png" alt=""></p>
<p>So how could you spend your time to improve the accuracy of this classifier?</p>
<ul>
<li>Collect lots of data (for example “honeypot” project but doesn’t always work)</li>
<li>Develop sophisticated features (for example: using email header data in spam emails)</li>
<li>Develop algorithms to process your input in different ways (recognizing misspellings in spam).</li>
</ul>
<p>It is difficult to tell which of the options will be most helpful.</p>
<h3 id="1-2-error-analysis"><a href="#1-2-error-analysis" class="headerlink" title="1.2 error analysis"></a>1.2 error analysis</h3><p>The recommended approach to solving machine learning problems is to:</p>
<ul>
<li>Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.</li>
<li>Plot learning curves to decide if more data, more features, etc. are likely to help.</li>
<li>Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.</li>
</ul>
<p>For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and categorize them based on what type of emails they are. We could then try to come up with new cues and features that would help us classify these 100 emails correctly. Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. We could also see how classifying each word according to its root changes our error rate:</p>
<p><img src="/images/ml/ml-ng-w6-02-02.png" alt=""></p>
<p>It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm’s performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature. Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not.</p>
<h2 id="2-Handling-Skewed-Data"><a href="#2-Handling-Skewed-Data" class="headerlink" title="2. Handling Skewed Data"></a>2. Handling Skewed Data</h2><h3 id="2-1-error-for-skewed-classes"><a href="#2-1-error-for-skewed-classes" class="headerlink" title="2.1 error for skewed classes"></a>2.1 error for skewed classes</h3><p><img src="/images/ml/ml-ng-w6-02-03.png" alt=""></p>
<p>So for the problem of skewed classes precision recall gives us more direct insight into how the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms</p>
<h3 id="2-2-precision-and-recall"><a href="#2-2-precision-and-recall" class="headerlink" title="2.2 precision and recall"></a>2.2 precision and recall</h3><p><img src="/images/ml/ml-ng-w6-02-04.png" alt=""></p>
<h2 id="3-Using-Large-Data-Sets"><a href="#3-Using-Large-Data-Sets" class="headerlink" title="3. Using Large Data Sets"></a>3. Using Large Data Sets</h2><p><img src="/images/ml/ml-ng-w6-02-05.png" alt=""></p>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Shuping Yang's University of Maryland speech]]></title>
      <url>http://sggo.me/2017/05/28/English/english-Shuping-Yang-at-Maryyland/</url>
      <content type="html"><![CDATA[<p>Shuping Yang Maryland Graduation Speech About Air and Free</p>
<a id="more"></a>
<div class="video-container"><iframe src="//www.youtube.com/embed/9bMX-ctieHI" frameborder="0" allowfullscreen></iframe></div>
<h2 id="Speech-Full-Text"><a href="#Speech-Full-Text" class="headerlink" title="Speech Full Text"></a>Speech Full Text</h2><blockquote>
<p><strong>Good afternoon faculty, students, parents and friends.</strong></p>
<p>I am truly honored and grateful to speak at the commencement for the University of Maryland, Class of 2017. </p>
<p>People often ask me: Why did you come to the University of Maryland?<br>I always answer: Fresh air.</p>
<p>Five years ago, as I step off the plane from China, and left the terminal at Dallas Airport. I was ready to put on one of my five face masks, but when I took my first breath of American air. I put my mask away. </p>
<p>The air was so sweet and fresh, and utterly luxurious.</p>
<p>I was surprised by this. I grew up in a city in China, where I had to wear a face mask every time I went outside, otherwise, I might get sick.</p>
<p>However, the moment Iinhaled and exhaled outside the airport，I felt free.</p>
<p>No more fog on my glasses, no more difficult breathing, no more suppression.</p>
<p>Every breath was a delight. As I stand here today, I cannot help, but recall that feeling of freedom. </p>
</blockquote>
<p>&nbsp;</p>
<blockquote>
<p>At the University of Maryland, I will soon feel another kind of fresh air for which I will beforever grateful — the fresh air of free speech.</p>
<p>Before I came to the UnitedStates, I learned in history class about the Declaration of Independence, butthese words had no meaning to me— Life, Liberty and the Pursuit of happiness.</p>
<p>I was merely memorizing the words to get good grades.</p>
<p>These words sounded so strange, so abstract and foreign to me, until I came to University of Maryland.</p>
<p>I have learned the right to freely express oneself is sacred in American.</p>
<p>Each day in Maryland, I was encouraged to express my opinions on controversial issues. </p>
<p>I could challenge astatement made by my instructor. I could even rate my professors online.</p>
<p>But nothing prepared me forthe culture shock I experienced when I watched a university production of the play— Twilight: Los Angeles.</p>
<p>Twilight is a play by AnnaDeavere Smith about the 1992 Los Angeles Riots.</p>
<p>The riots followed acquittal of four Los Angeles police officers in the videotaped arrest and beating of Rodney King.</p>
<p>For six days, the city was in chaos as citizens took to the streets.</p>
<p>In Twilight, the student actors were openly talking about racism, sexism and politics.</p>
<p>I was shocked, I never thought such topic could be discussed openly.</p>
<p>The play was my first taste of political story telling, one that makes the audience think critically. </p>
<p>I have always had a burning desire to tell these kinds of stories, but I was convinced that only authorities on the narrative, only authorities could define the truth. </p>
<p>However, the opportunity toimmerse myself in the perse community at the University of Maryland exposed me to various, many different perspectives on truth.</p>
<p>I soon realized that here I have the opportunity to speak freely. </p>
<p>My voice matters. </p>
<p>Your voice matters.</p>
<p>Our voices matter.</p>
<p>Civil engagement is not at ask just for politicians. I have witnessed this when I saw my fellow student smarching in Washington DC, voting in the presidential election and raising money to support various causes.</p>
<p>I have seen that everyone has a right to participate and advocate for change.</p>
<p>I used to believe that one inpidual participation could not make a difference, but here we are, United Terps.</p>
<p>Together, we can push our society to be more just, open and peaceful.</p>
<p>Class of 2017, we are graduating from a university that embraces a liberal arts education that nurtures us to think critically, and also to care and feel for humanity. </p>
<p>We are equipped with the knowledge of various disciplines and we are ready to face  the challenges of our society.</p>
<p>Some of us may go to graduate school, some us may step into professions and some of us may begin ajourney of exploration.</p>
<p>But no matter what we do,remember, democracy and free speech should not be taken for granted.</p>
<p>Democracy and freedom are the fresh air that is worth fighting for.</p>
<p>Freedom is oxygen. Freedomis passion. Freedom is love.</p>
<p>And as a French philosopher Jean Paul Sartre once said: freedom is a choice, our future is dependent on the choices we make today and tomorrow.</p>
<p>We are all playwrights of the next chapters of our lives. Together, we write the human history.</p>
<p>My friends, enjoy the fresh air and never ever let it go.</p>
<p>Thank you.</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera 6 - Advice for Applying Machine Learning *]]></title>
      <url>http://sggo.me/2017/05/24/ml/coursera-ng-w6-Advice-for-Applying-Machine-Learning/</url>
      <content type="html"><![CDATA[<p>Evaluating a Hypothesis -&gt; Model Selection、Diagnosing Bias vs Variance -&gt; Regularization、Learning Curves</p>
<a id="more"></a>
<h2 id="1-Evaluating-a-Hypothesis"><a href="#1-Evaluating-a-Hypothesis" class="headerlink" title="1. Evaluating a Hypothesis"></a>1. Evaluating a Hypothesis</h2><p>Once we have done some <strong>trouble shooting</strong> for errors in our predictions by:</p>
<ul>
<li>Getting more training examples</li>
<li>Trying smaller sets of features</li>
<li>Trying additional features</li>
<li>Trying polynomial features</li>
<li>Increasing or decreasing $λ$</li>
</ul>
<p>We can move on to evaluate our new hypothesis.</p>
<p>A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70% of your data and the test set is the remaining 30%.</p>
<p>The new procedure using these two sets is then:</p>
<ol>
<li>Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set</li>
<li>Compute the test set error $J_{test}(\Theta)$</li>
</ol>
<p><strong>The test set error</strong></p>
<ol>
<li>For linear regression: $J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$</li>
<li>For classification ~ Misclassification error (aka 0/1 misclassification error):</li>
</ol>
<p>$$<br>err(h_\Theta(x),y) = \begin{matrix} 1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1\newline 0 &amp; \mbox otherwise \end{matrix}<br>$$</p>
<p>This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:</p>
<p>$$<br>\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})<br>$$</p>
<p>This gives us the proportion of the test data that was misclassified.</p>
<blockquote>
<p>inaccurate[ɪn’ækjərət]、procedure [prə’sidʒɚ] 比例<br>remaining [ri’men..]、 Misclassification [‘mis,klæsifi’keiʃən]<br>validation [,vælə’deʃən] 确认，批准</p>
</blockquote>
<h2 id="2-Model-Selection"><a href="#2-Model-Selection" class="headerlink" title="2. Model Selection"></a>2. Model Selection</h2><p><strong>Train/Validtion/Test Sets</strong></p>
<p>Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set.</p>
<p>Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.</p>
<p>One way to break down our dataset into the three sets is:</p>
<ul>
<li><font color="blue"> Training set: 60%</font></li>
<li><font color="blue"> Cross validation set: 20%</font></li>
<li><font color="blue"> Test set: 20%</font>

</li>
</ul>
<p>We can now calculate three separate error values for the three different sets using the following method:</p>
<ol>
<li>Optimize the parameters in Θ using the training set for each polynomial degree.</li>
<li>Find the polynomial degree d with the least error using the cross validation set.</li>
<li>Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);</li>
</ol>
<blockquote>
<p>This way, the degree of the polynomial d has not been trained using the test set.</p>
</blockquote>
<h2 id="3-Diagnosing-Bias-vs-Variance"><a href="#3-Diagnosing-Bias-vs-Variance" class="headerlink" title="3. Diagnosing Bias vs. Variance"></a>3. Diagnosing Bias vs. Variance</h2><p>In this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis.</p>
<ul>
<li>We need to distinguish whether <code>bias</code> or <code>variance</code> is the problem contributing to bad predictions.</li>
<li>High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.</li>
</ul>
<p>The training error will tend to <code>decrease</code> as we increase the degree d of the polynomial.</p>
<p>At the same time, the cross validation error will tend to decrease as we increase d up to a point, and then it will increase as d is increased, forming a convex curve.</p>
<p><strong>High bias (underfitting)</strong>: both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ will be high. Also, $J_{CV}(\Theta) \approx J_{train}(\Theta)$.</p>
<p><strong>High variance (overfitting)</strong>: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be much greater than $J_{train}(\Theta)$.</p>
<p>The is summarized in the figure below:</p>
<p><img src="/images/ml/coursera/ml-ng-w6-01-01.png" alt=""></p>
<h2 id="4-Regularization-Bias-Variance"><a href="#4-Regularization-Bias-Variance" class="headerlink" title="4. Regularization Bias/Variance"></a>4. Regularization Bias/Variance</h2><p>Note: [The regularization term below and through out the video should be $\frac \lambda {2m} \sum_{j=1}^n \theta_j ^2$ and NOT $\frac \lambda {2m} \sum_{j=1}^m \theta_j ^2$]</p>
<p><img src="/images/ml/coursera/ml-ng-w6-01-02.png" alt=""></p>
<p>In the figure above, we see that as λ increases, our fit becomes more rigid. On the other hand, as λ approaches 0, we tend to over overfit the data. So how do we choose our parameter λ to get it ‘just right’ ? In order to choose the model and the regularization term λ, we need to:</p>
<ol>
<li>Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});</li>
<li>Create a set of models with different degrees or any other variants.</li>
<li>Iterate through the λs and for each λ go through all the models to learn some Θ.</li>
<li>Compute the cross validation error using the learned Θ (computed with λ) on the $J_{CV}(\Theta)$ <strong>without</strong> regularization or λ = 0. <code>？？</code></li>
<li>Select the best combo that produces the lowest error on the cross validation set.</li>
<li>Using the best combo Θ and λ, apply it on $J_{test}(\Theta)$ to see if it has a good generalization of the problem.</li>
</ol>
<h2 id="5-Learning-Curves"><a href="#5-Learning-Curves" class="headerlink" title="5. Learning Curves"></a>5. Learning Curves</h2><p>Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence:</p>
<ul>
<li>As the training set gets larger, the error for a quadratic function increases.</li>
<li>The error value will plateau out after a certain m, or training set size.</li>
</ul>
<p><strong>Experiencing high bias:</strong></p>
<p>Low training set size: causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high.</p>
<p>Large training set size: causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$  to be high with $J_{train}(\Theta)$≈$J_{CV}(\Theta)$.</p>
<p>If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.</p>
<p><img src="/images/ml/coursera/ml-ng-w6-01-03.png" alt=""></p>
<p>me : 随着train sets的增加，高偏差会降低，因为更容易找到合适的 Hypothesis 去拟合数据。所以CV error下降。</p>
<p><strong>Experiencing high variance:</strong></p>
<p><strong>Low training set size:</strong> $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.</p>
<p><strong>Large training set size:</strong> $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta)$ &lt; $J_{CV}(\Theta)$ but the difference between them remains significant.</p>
<p>If a learning algorithm is suffering from high variance, getting more training data is likely to help.</p>
<p><img src="/images/ml/coursera/ml-ng-w6-01-04.png" alt=""></p>
<p>me : 随着 train sets 的增加，overfiting 越来越不容易。造成 CV error 下降。</p>
<blockquote>
<p>plateau out / leveling off 达到平稳状态<br><code>test error will be CV error on picture.</code></p>
</blockquote>
<h2 id="6-What-to-Do-Next-Revisited"><a href="#6-What-to-Do-Next-Revisited" class="headerlink" title="6. What to Do Next Revisited"></a>6. What to Do Next Revisited</h2><p><em>*Our decision process can be broken down as follows</em>:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Function</th>
<th style="text-align:center">Result</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Getting more training examples</td>
<td style="text-align:center">Fixes high variance</td>
</tr>
<tr>
<td style="text-align:center">Trying smaller sets of features</td>
<td style="text-align:center">Fixes high variance</td>
</tr>
<tr>
<td style="text-align:center">Increasing λ</td>
<td style="text-align:center">Fixes high variance</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Adding features</td>
<td style="text-align:center">Fixes high bias</td>
</tr>
<tr>
<td style="text-align:center">Adding polynomial features</td>
<td style="text-align:center">Fixes high bias</td>
</tr>
<tr>
<td style="text-align:center">Decreasing λ</td>
<td style="text-align:center">Fixes high bias</td>
</tr>
</tbody>
</table>
<h3 id="6-1-Diagnosing-NN"><a href="#6-1-Diagnosing-NN" class="headerlink" title="6.1 Diagnosing NN"></a>6.1 Diagnosing NN</h3><ul>
<li><p>A neural network with fewer parameters is <strong>prone to underfitting</strong>. It is also <strong>computationally cheaper</strong>.</p>
</li>
<li><p>A large neural network with more parameters is <strong>prone to overfitting</strong>. It is also <strong>computationally expensive</strong>. </p>
</li>
</ul>
<p>In this case you can use regularization (increase λ) to address the overfitting.</p>
<h3 id="6-2-Model-Complexity-Effects"><a href="#6-2-Model-Complexity-Effects" class="headerlink" title="6.2 Model Complexity Effects"></a>6.2 Model Complexity Effects</h3><ul>
<li><p>Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.</p>
</li>
<li><p>Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.</p>
</li>
<li><p>In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.</p>
</li>
</ul>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><ul>
<li><a href="https://www.coursera.org/learn/machine-learning/home/week/6" target="_blank" rel="external">coursera 6 @ng</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark ALS]]></title>
      <url>http://sggo.me/2017/05/07/spark/spark-ALS/</url>
      <content type="html"><![CDATA[<p><a href="http://spark.apache.org/examples.html" target="_blank" rel="external">Spark.apache.org</a><br><a href="http://blog.javachen.com/2015/06/07/spark-configuration.html" target="_blank" rel="external">Java Chen Spark</a></p>
<a id="more"></a>
<p>以下为手动计算流程 :</p>
<h3 id="4-2-启动-spark-shell"><a href="#4-2-启动-spark-shell" class="headerlink" title="4.2 启动 spark-shell"></a>4.2 启动 spark-shell</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SPARK_CLASSPATH=/opt/cloudera/parcels/CDH/lib/sqoop/mysql-connector-java-5.1.40.jar spark-shell</span><br></pre></td></tr></table></figure>
<h3 id="4-3-输入输出-相关变量"><a href="#4-3-输入输出-相关变量" class="headerlink" title="4.3 输入输出:相关变量"></a>4.3 输入输出:相关变量</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">val inputTable = <span class="string">"mds_user_coupon_bhv"</span></span><br><span class="line">val inputUrl = <span class="string">"jdbc:mysql://192.168.xxx.xx:3306/com_profile?user=your_name&amp;password=your_password"</span></span><br><span class="line">val outputTable = <span class="string">"mds_rs_shop_coupon_tmp"</span></span><br></pre></td></tr></table></figure>
<h3 id="4-4-核心程序代码"><a href="#4-4-核心程序代码" class="headerlink" title="4.4 核心程序代码"></a>4.4 核心程序代码</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">package com.x.rs.service</span><br><span class="line"></span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line">import java.util.Date</span><br><span class="line">import java.util.Properties</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.mllib.recommendation.&#123;ALS, Rating&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Date : 2017-04-20</span><br><span class="line">  * Author : Blair Chan</span><br><span class="line">  */</span><br><span class="line">object RsCouponCalc &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line"></span><br><span class="line">    println("start...")</span><br><span class="line"></span><br><span class="line">    if (args.length &lt; 3) &#123;</span><br><span class="line">      System.err.println("Usage: &lt;file&gt;")</span><br><span class="line">      System.exit(1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val inputTable = args(0) // Should be some file on your system // conf = new SparkConf().setAppName(appName).setMaster("local");</span><br><span class="line">    val inputUrl = args(1)</span><br><span class="line">    val outputTable = args(2)</span><br><span class="line"></span><br><span class="line">    val conf = new SparkConf().setAppName("SparkRsOne");</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line"></span><br><span class="line">    //    val rawData = sc.textFile(inputFile)</span><br><span class="line">    //    val rawRatings = rawData.map(_.split("\t").take(3))</span><br><span class="line"></span><br><span class="line">    val url = inputUrl</span><br><span class="line">    val prop = new Properties()</span><br><span class="line"></span><br><span class="line">    val dfForRawData = sqlContext.read.jdbc(url, inputTable, prop)</span><br><span class="line"></span><br><span class="line">    val ratings_tmp = dfForRawData.map &#123; row =&gt; (row(1).toString().toInt, row(4).toString().toInt, row(6).toString().toDouble) &#125;</span><br><span class="line"></span><br><span class="line">    val ratings = ratings_tmp.map &#123; case (uid, couponId, rating) =&gt; Rating(uid.toInt, couponId.toInt, rating.toDouble) &#125;</span><br><span class="line"></span><br><span class="line">    val model = ALS.train(ratings, 50, 10, 0.01)</span><br><span class="line"></span><br><span class="line">    model.userFeatures.count</span><br><span class="line"></span><br><span class="line">    val K = 10</span><br><span class="line"></span><br><span class="line">    model.recommendProductsForUsers(K)</span><br><span class="line"></span><br><span class="line">    val originResultRdd1 = model.recommendProductsForUsers(K)</span><br><span class="line"></span><br><span class="line">    val curDate = new Date()</span><br><span class="line">    val createDateString = new SimpleDateFormat("yyyy-MM-dd").format(curDate)</span><br><span class="line"></span><br><span class="line">    val originResultRdd2 = originResultRdd1.map(tuple =&gt; &#123;</span><br><span class="line">      val uid = tuple._1</span><br><span class="line">      val product = tuple._2.map &#123; case Rating(user, product, score) =&gt; (product.toString, score.toString) &#125;</span><br><span class="line">      (uid, product)</span><br><span class="line">    &#125;).flatMap &#123;</span><br><span class="line">      case (uid, product) =&gt; &#123;</span><br><span class="line">        product.map &#123; case (itemId, score) =&gt; Row.apply(uid.toLong, itemId.toString, score.toDouble, createDateString.toString) &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // println(originResultRdd2.first())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val schema = StructType(</span><br><span class="line">      StructField("uid", LongType) ::</span><br><span class="line">        StructField("coupon_id", LongType) ::</span><br><span class="line">        StructField("score", DoubleType) ::</span><br><span class="line">        StructField("calc_date", StringType) :: Nil)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val df = sqlContext.createDataFrame(originResultRdd2, schema)</span><br><span class="line"></span><br><span class="line">    df.insertIntoJDBC(url, outputTable, false)</span><br><span class="line">    // 设置为 true，则为 删除表，然后自动创建，再插入</span><br><span class="line"></span><br><span class="line">    println("end !")</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>DF 通过插入 RMDB.  schema 可以通过反射来使得程序扩展性提高。</p>
</blockquote>
<p><a href="http://www.cnblogs.com/yaohaitao/articles/5681984.html" target="_blank" rel="external">spark sql internet</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark SQL编程]]></title>
      <url>http://sggo.me/2017/04/28/spark/spark-SQL/</url>
      <content type="html"><![CDATA[<p>Spark SQL，可对不同格式的数据执行ETL操作（如JSON，Parquet，数据库）然后完成特定的查询操作。</p>
<a id="more"></a>
<ul>
<li>DataFrame</li>
<li>Data Sources</li>
<li>JDBC Server</li>
</ul>
<p>使用Spark SQL时，最主要的两个组件就是 <strong>DataFrame</strong> 和 <strong>SQLContext</strong>。</p>
<h2 id="1-DataFrame"><a href="#1-DataFrame" class="headerlink" title="1. DataFrame"></a>1. DataFrame</h2><p>DataFrame 是一个分布式的，按照命名列的形式组织的数据集合。DataFrame基于R语言中的dataframe概念，与关系型数据库中的数据库表类似。</p>
<blockquote>
<p>之前版本的Spark SQL API中的SchemaRDD已经更名为DataFrame</p>
</blockquote>
<p>调用将DataFrame的内容作为行RDD（RDD of Rows）返回的<a href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank" rel="external">rdd方法</a>，可以将DataFrame转换成RDD。</p>
<p><strong>创建 DataFrame</strong></p>
<p>可以通过如下 <font color="#c7254e">数据源创建 DataFrame</font> : </p>
<ul>
<li>已有的RDD</li>
<li>结构化数据文件</li>
<li>JSON数据集</li>
<li>Hive表</li>
<li>外部数据库</li>
</ul>
<p><strong>DataFrame API</strong></p>
<p>Spark SQL和DataFrame API已经在下述几种程序设计语言中实现：</p>
<ul>
<li><a href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.package" target="_blank" rel="external">Scala DataFrame API</a></li>
<li><a href="https://spark.apache.org/docs/1.3.0/api/java/index.html?org/apache/spark/sql/api/java/package-summary.html" target="_blank" rel="external">Java DataFrame API</a></li>
<li><a href="https://spark.apache.org/docs/1.3.0/api/python/pyspark.sql.html" target="_blank" rel="external">Python DataFrame API</a></li>
</ul>
<h2 id="2-SQLContext"><a href="#2-SQLContext" class="headerlink" title="2. SQLContext"></a>2. SQLContext</h2><p>SQLContext封装Spark中的所有关系型功能。可以用之前的示例中的现有SparkContext创建SQLContext。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext" target="_blank" rel="external">SQLContext</a><br><a href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.hive.HiveContext" target="_blank" rel="external">HiveContext</a></p>
</blockquote>
<h2 id="3-JDBC数据源"><a href="#3-JDBC数据源" class="headerlink" title="3. JDBC数据源"></a>3. JDBC数据源</h2><p>JDBC 数据源 可用于通过JDBC API读取关系型数据库中的数据。相比于使用JdbcRDD，应该将JDBC数据源的方式作为首选，因为JDBC数据源能够将结果作为DataFrame对象返回，直接用Spark SQL处理或与其他数据源连接。</p>
<p>为确保Spark Shell程序有足够的内存，可以在运行spark-shell命令时，加入driver-memory命令行参数，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-shell.cmd --driver-memory 1G</span><br></pre></td></tr></table></figure>
<h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><p><a href="http://www.infoq.com/cn/articles/apache-spark-sql" target="_blank" rel="external">用Apache Spark进行大数据处理——第二部分：Spark SQL</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Read Mysql 的四种方式]]></title>
      <url>http://sggo.me/2017/04/20/spark/spark-read-mysql-four-functions/</url>
      <content type="html"><![CDATA[<p>目前 <code>Spark</code> 支持四种方式从数据库中读取数据，这里以 <code>MySQL</code> 为例进行介绍。</p>
<a id="more"></a>
<h2 id="Startup-spark-shell"><a href="#Startup-spark-shell" class="headerlink" title="Startup spark-shell"></a>Startup spark-shell</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SPARK_CLASSPATH=/opt/cloudera/parcels/CDH/lib/sqoop/mysql-connector-java-5.1.40.jar spark-shell</span><br></pre></td></tr></table></figure>
<h2 id="1-不指定查询条件"><a href="#1-不指定查询条件" class="headerlink" title="1. 不指定查询条件"></a>1. 不指定查询条件</h2><h3 id="1-1-function-define"><a href="#1-1-function-define" class="headerlink" title="1.1 function define"></a>1.1 function define</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">jdbc</span></span>(url: <span class="type">String</span>, table: <span class="type">String</span>, properties: <span class="type">Properties</span>): <span class="type">DataFrame</span></span><br></pre></td></tr></table></figure>
<h3 id="1-2-detail-example"><a href="#1-2-detail-example" class="headerlink" title="1.2 detail example"></a>1.2 detail example</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> url = <span class="string">"jdbc:mysql://192.168.***.**:3306/your_lib_name?user= your_username&amp;password=your_password"</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.jdbc(url, <span class="string">"mds_user_coupon_bhv"</span>, prop )</span><br><span class="line">println(df.count())</span><br><span class="line">println(df.rdd.partitions.size)</span><br></pre></td></tr></table></figure>
<p>我们运行上面的程序，可以看到df.rdd.partitions.size输出结果是1，这个结果的含义是iteblog表的所有数据都是由RDD的一个分区处理的，所以说，如果你这个表很大，很可能会出现OOM</p>
<blockquote>
<p>Note : 这种方式在数据量大的时候不建议使用。</p>
</blockquote>
<h2 id="2-指定数据库字段的范围"><a href="#2-指定数据库字段的范围" class="headerlink" title="2. 指定数据库字段的范围"></a>2. 指定数据库字段的范围</h2><p>这种方式就是通过指定数据库中某个字段的范围，但是<code>这个字段必须是数字</code>，来看看这个函数的函数原型：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">jdbc</span></span>(</span><br><span class="line">    url: <span class="type">String</span>,</span><br><span class="line">    table: <span class="type">String</span>,</span><br><span class="line">    columnName: <span class="type">String</span>,</span><br><span class="line">    lowerBound: <span class="type">Long</span>,</span><br><span class="line">    upperBound: <span class="type">Long</span>,</span><br><span class="line">    numPartitions: <span class="type">Int</span>,</span><br><span class="line">    connectionProperties: <span class="type">Properties</span>): <span class="type">DataFrame</span></span><br></pre></td></tr></table></figure>
<p>… not finish</p>
<h2 id="3-根据任意字段进行分区"><a href="#3-根据任意字段进行分区" class="headerlink" title="3. 根据任意字段进行分区"></a>3. 根据任意字段进行分区</h2><p>…</p>
<h2 id="4-通过-load-获取"><a href="#4-通过-load-获取" class="headerlink" title="4. 通过 load 获取"></a>4. 通过 load 获取</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.load(<span class="string">"jdbc"</span>, <span class="type">Map</span>(<span class="string">"url"</span> -&gt; <span class="string">"jdbc:mysql://..."</span>, </span><br><span class="line">              <span class="string">"dbtable"</span> -&gt; <span class="string">"mds_user_coupon_bhv"</span>)</span><br><span class="line">         )</span><br></pre></td></tr></table></figure>
<p>换一种更正式的写法如下 :</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read.format(<span class="string">"jdbc"</span>).options(<span class="type">Map</span>(</span><br><span class="line">      <span class="string">"url"</span> -&gt; <span class="string">"jdbc:mysql://192.168.***.**:3306/your_lib_name?user= your_username&amp;password=your_password"</span>,</span><br><span class="line">      <span class="string">"dbtable"</span> -&gt; <span class="string">"mds_user_coupon_bhv"</span>)</span><br><span class="line">   ).load()</span><br></pre></td></tr></table></figure>
<p>options函数支持url、driver、dbtable、partitionColumn、lowerBound、upperBound以及numPartitions选项，这个和方法二的参数一致。其内部实现原理部分和方法二大体一致。同时load方法还支持json、orc等数据源的读取。</p>
<blockquote>
<p>Reading notes</p>
</blockquote>
<h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><blockquote>
<p>尊重原创，转载请注明： 转载自过往记忆（<a href="http://www.iteblog.com/）" target="_blank" rel="external">http://www.iteblog.com/）</a> </p>
</blockquote>
<p><a href="http://blog.csdn.net/mlljava1111/article/details/50432569" target="_blank" rel="external">Spark Read Mysql-csdn</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[New Mac Install Brew Iterm Zsh]]></title>
      <url>http://sggo.me/2017/03/22/ops/ops-new-macosx-install-soft/</url>
      <content type="html"><![CDATA[<p>新机 mac 安装一些常用软件</p>
<a id="more"></a>
<h2 id="1-install-homebrew"><a href="#1-install-homebrew" class="headerlink" title="1. install homebrew"></a>1. install homebrew</h2><p>Mac 下面的包管理工具，通过 Github 托管适合 Mac 的编译配置以及 Patch，可以方便的安装开发工具。</p>
<p>Mac 自带ruby 所以安装起来很方便，同时它也会自动把git也给你装上。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/bin/ruby <span class="_">-e</span> <span class="string">"<span class="variable">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)</span>"</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>安装过程可能会有点慢，完成之后，建议执行一下自检，brew doctor<br>看到<br>Your system is ready to brew.<br>那么恭喜你的 brew 已经可以开始使用了。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew install wget tree</span><br></pre></td></tr></table></figure>
<hr>
<p>brew 常用命令： （所有软件以PHP5.5为例子）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew update                        <span class="comment">#更新brew可安装包，建议每次执行一下</span></span><br><span class="line">brew search php55                  <span class="comment">#搜索php5.5</span></span><br><span class="line">brew tap josegonzalez/php          <span class="comment">#安装扩展&lt;gihhub_user/repo&gt; </span></span><br><span class="line">brew tap                          <span class="comment">#查看安装的扩展列表</span></span><br><span class="line">brew install php55                <span class="comment">#安装php5.5</span></span><br><span class="line">brew remove  php55                <span class="comment">#卸载php5.5</span></span><br><span class="line">brew upgrade php55                <span class="comment">#升级php5.5</span></span><br><span class="line">brew options php55                <span class="comment">#查看php5.5安装选项</span></span><br><span class="line">brew info    php55                <span class="comment">#查看php5.5相关信息</span></span><br><span class="line">brew home    php55                <span class="comment">#访问php5.5官方网站</span></span><br><span class="line">brew services list                <span class="comment">#查看系统通过 brew 安装的服务</span></span><br><span class="line">brew services cleanup              <span class="comment">#清除已卸载无用的启动配置文件</span></span><br><span class="line">brew services restart php55        <span class="comment">#重启php-fpm</span></span><br><span class="line"> </span><br><span class="line">注意：brew services 相关命令最好别经常用了，提示会被移除</span><br></pre></td></tr></table></figure>
<h2 id="2-install-zsh"><a href="#2-install-zsh" class="headerlink" title="2. install zsh"></a>2. install zsh</h2><p>ohmyzsh &amp; iTerm2 两个神器，在Mac os x下是一定要装的. </p>
<h3 id="2-1-install-onmyzsh"><a href="#2-1-install-onmyzsh" class="headerlink" title="2.1 install onmyzsh"></a>2.1 install onmyzsh</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -L http://install.ohmyz.sh | sh</span><br></pre></td></tr></table></figure>
<h3 id="2-2-install-zsh"><a href="#2-2-install-zsh" class="headerlink" title="2.2 install zsh"></a>2.2 install zsh</h3><p>在 Terminal 下，直接敲 zsh.</p>
<blockquote>
<p>下面请暂时忽略 </p>
</blockquote>
<hr>
<p>设置默认shell</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat /etc/shells </span><br><span class="line"># List of acceptable shells for chpass(1). </span><br><span class="line"># Ftpd will not allow users to connect who are not using </span><br><span class="line"># one of these shells. /bin/bash /bin/csh /bin/ksh /bin/sh /bin/tcsh /bin/zsh zsh --version zsh 5.0.2 (x86_64-apple-darwin13.0) chsh -s /bin/zsh</span><br></pre></td></tr></table></figure>
<p>虽然Mac自带了zsh，如果你想要最新版的zsh，那么你用 brew install zsh安装一个最新的吧。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/usr/local/bin/zsh --version</span><br></pre></td></tr></table></figure>
<p>zsh 5.0.5 (x86_64-apple-darwin13.3.0) 区别也不会很大， 默认的版本已经很新了。  </p>
<p>安装后最好备份 : cp ~/.zshrc ~/.zshrc.orig</p>
<h2 id="3-homebrew-cask"><a href="#3-homebrew-cask" class="headerlink" title="3. homebrew-cask"></a>3. homebrew-cask</h2><p>install brew cask</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew tap phinze/homebrew-cask</span><br></pre></td></tr></table></figure>
<p>cask常用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew cask search        #列出所有可以被安装的软件</span><br><span class="line">brew cask search php    #查找所有和php相关的应用</span><br><span class="line">brew cask list          #列出所有通过cask安装的软件</span><br><span class="line">brew cask info phpstorm #查看 phpstorm 的信息</span><br><span class="line">brew cask uninstall qq  #卸载 QQ</span><br></pre></td></tr></table></figure>
<blockquote>
<p>brew cask install sublime-text</p>
</blockquote>
<p>这里谈谈cask对比Mac App Store的优势：</p>
<ul>
<li>对常用软件支持更全面（特别是开发者），cask里面会给你一些惊喜；</li>
<li>软件更新速度快，一般都是最新版本 Store上很久很久才会更新版本；</li>
<li>命令安装感觉比打开Store方便，另外Store在国内的速度也是XXOO。</li>
</ul>
<blockquote>
<p>homebrew-cask 你可以先不安装</p>
</blockquote>
<h2 id="4-iterm2"><a href="#4-iterm2" class="headerlink" title="4. iterm2"></a>4. iterm2</h2><p> <a href="https://www.iterm2.com/" target="_blank" rel="external">https://www.iterm2.com/</a></p>
<h2 id="5-SimpleHTTPServer"><a href="#5-SimpleHTTPServer" class="headerlink" title="5. SimpleHTTPServer"></a>5. SimpleHTTPServer</h2><p>A computer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m http.server</span><br></pre></td></tr></table></figure>
<p>B computer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http://192.168.xx.xx:8000/your-filename</span><br></pre></td></tr></table></figure>
<blockquote>
<p>nc 瑞士军刀，也可以两台电脑传输文件</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Recommendation System - CF]]></title>
      <url>http://sggo.me/2017/02/28/ml/recommendation-six-mouth-CF/</url>
      <content type="html"><![CDATA[<p>recommendation system and application</p>
<a id="more"></a>
<p>推荐算法</p>
<ul>
<li>基于内容的推荐</li>
<li>协同过滤 CF</li>
<li>矩阵分解 与 隐语义模型</li>
</ul>
<h2 id="1-推荐系统-What"><a href="#1-推荐系统-What" class="headerlink" title="1. 推荐系统 What?"></a>1. 推荐系统 What?</h2><h3 id="1-1-数学定义"><a href="#1-1-数学定义" class="headerlink" title="1.1 数学定义"></a>1.1 数学定义</h3><ul>
<li>设 C 为全体用户集合</li>
<li>设 S 为全部商品/推荐内容集合</li>
<li>设 u 是评判把 $s_i$ 推荐 $c_i$ 的好坏评判函数</li>
</ul>
<blockquote>
<p>推荐是对于 $c∈C$ ,找到 $s∈S$ ,使得 $u$ 最大</p>
</blockquote>
<h3 id="1-2-人话版本"><a href="#1-2-人话版本" class="headerlink" title="1.2 人话版本"></a>1.2 人话版本</h3><p>根据用户的 :</p>
<ol>
<li>历史行为</li>
<li>社交关系</li>
<li>兴趣点</li>
<li>上下文环境</li>
<li>…</li>
</ol>
<blockquote>
<p>去判断用户的当前需求 / 感兴趣的 Item</p>
</blockquote>
<h2 id="2-推荐系统-Why"><a href="#2-推荐系统-Why" class="headerlink" title="2. 推荐系统 Why?"></a>2. 推荐系统 Why?</h2><p>Infomation Overload<br>用户需求不明确</p>
<h3 id="2-1-对用户"><a href="#2-1-对用户" class="headerlink" title="2.1 对用户"></a>2.1 对用户</h3><ul>
<li>找到好玩的东西</li>
<li>发现新鲜事物 Surprise</li>
</ul>
<h3 id="2-2-对商家"><a href="#2-2-对商家" class="headerlink" title="2.2 对商家"></a>2.2 对商家</h3><ul>
<li>个性化服务、提高粘性</li>
<li>增加营收</li>
</ul>
<h2 id="3-评定标准"><a href="#3-评定标准" class="headerlink" title="3. 评定标准"></a>3. 评定标准</h2><ul>
<li>准确度</li>
<li>覆盖度</li>
<li>多样性</li>
<li>评估标准</li>
</ul>
<h3 id="3-1-准确度-Top-N"><a href="#3-1-准确度-Top-N" class="headerlink" title="3.1 准确度 Top N"></a>3.1 准确度 Top N</h3><p>设 $R(u)$ 为根据训练建立的模型在测试集上的推荐, $T(u)$ 为测试集上用户的选择。</p>
<p>$$<br>Precision =  \frac{\sum_{u∈U} | R(u) \bigcap T(u) | }{\sum_{u∈U} R(u)}<br>$$</p>
<p>$$<br>Recall =  \frac{\sum_{u∈U} | R(u) \bigcap T(u) | }{\sum_{u∈U} T(u)}<br>$$</p>
<blockquote>
<p>Recall 说明 : 用户看的 80 篇新闻，你到底给我 <code>推</code> 出来多少篇</p>
</blockquote>
<h3 id="3-2-覆盖率"><a href="#3-2-覆盖率" class="headerlink" title="3.2 覆盖率"></a>3.2 覆盖率</h3><p>表示对物品长尾的发掘能力 (推荐系统希望消除马太效应)</p>
<p>$$<br>Coverage = \frac{|U_{u∈U} R(u)|}{|I|}<br>$$</p>
<blockquote>
<p>覆盖率是对平台所有物品所言，淘宝推荐等会关切这个指标。</p>
<p>非常独特的商品和新闻, 被看到的量, 是一条下滑曲线</p>
<p>希望个性化推荐，把小众的商品 也推荐展示出来</p>
</blockquote>
<h3 id="3-3-多样性"><a href="#3-3-多样性" class="headerlink" title="3.3 多样性"></a>3.3 多样性</h3><p>表示推荐列表中物品两两之间的不相似性.</p>
<p>$$<br>Diversity(R(u)) = 1 - \frac {\sum_{i, j \in R(u), {i \neq j}, {s(i, j)} }} {\frac {1}{2} |R(u)|(|R(u) - 1|)}<br>$$</p>
<p>$$<br>Diversity = \frac {1} {|U|} {\sum_{u \in U} {Diversity(R(u))}}<br>$$</p>
<blockquote>
<p>全是牛仔裤的话，用户审美疲劳, tag(纯棉 0，圆领 1) 连衣裙, 计算 vector 距离。品类不同，相似度设为 0.</p>
</blockquote>
<h3 id="3-4-评估标准"><a href="#3-4-评估标准" class="headerlink" title="3.4 评估标准"></a>3.4 评估标准</h3><ul>
<li>新颖度 : 给用户 Surprise</li>
<li>惊喜度 : 推荐和用户历史兴趣不相似，却满意的</li>
<li>信任度 : 提供可靠的推荐理由</li>
<li>实时性 : 实时更新程度 (context, session …)</li>
<li>…</li>
</ul>
<h2 id="4-相似度-距离定义"><a href="#4-相似度-距离定义" class="headerlink" title="4. 相似度/距离定义"></a>4. 相似度/距离定义</h2><ul>
<li>欧氏距离</li>
<li>Jaccard 相似度</li>
<li>余弦相似度</li>
<li>Pearson 相似度</li>
</ul>
<h3 id="4-1-欧氏距离"><a href="#4-1-欧氏距离" class="headerlink" title="4.1 欧氏距离"></a>4.1 欧氏距离</h3><p>$$<br>dist(X, Y) = \left( \sum_{i=1}^n {| x_i - y_i|}^p \right)^{\frac {1}{p}}<br>$$</p>
<h3 id="4-2-Jaccard-相似度"><a href="#4-2-Jaccard-相似度" class="headerlink" title="4.2 Jaccard 相似度"></a>4.2 Jaccard 相似度</h3><p>$$<br>J(A, B) = \frac {|A \bigcap B|}{|A \bigcup B|}<br>$$</p>
<blockquote>
<p>适用于 top N 推荐，要么 看，要么 没看</p>
</blockquote>
<h3 id="4-3-余弦相似度"><a href="#4-3-余弦相似度" class="headerlink" title="4.3 余弦相似度"></a>4.3 余弦相似度</h3><p>$$<br>\cos(\theta) = \frac {a^Tb}{|a| \cdot |b|}<br>$$</p>
<h3 id="4-4-Pearson-相似度"><a href="#4-4-Pearson-相似度" class="headerlink" title="4.4 Pearson 相似度"></a>4.4 Pearson 相似度</h3><p>$$<br>\frac { \sum_{i=1}^n (X_i - {\mu}_x) (Y_i - {\mu}_y)} {\sqrt{ \sum_{i=1}^n (X_i - {\mu}_x)^2} \sqrt{ \sum_{i=1}^n (Y_i - {\mu}_y)^2}}<br>$$</p>
<blockquote>
<p>(5, 6, 9)<br>(1, 2, 6)</p>
</blockquote>
<h2 id="5-推荐算法"><a href="#5-推荐算法" class="headerlink" title="5. 推荐算法"></a>5. 推荐算法</h2><ol>
<li>基于内容的推荐</li>
<li>协同过滤CF</li>
<li>隐语义模型</li>
</ol>
<h3 id="5-1-基于内容推荐"><a href="#5-1-基于内容推荐" class="headerlink" title="5.1 基于内容推荐"></a>5.1 基于内容推荐</h3><ul>
<li>基于用户喜欢的 Item 的属性 / 内容进行挖掘</li>
<li>基于分析内容，无需考虑当前user与其他user的行为的交互关联等</li>
<li>通常使用在 <code>文本相关</code> 的产品上进行推荐</li>
<li>Item 通过内容 (比如 关键词) 关联</li>
</ul>
<p>在推荐电影中，也可以使用，但是效果不见得好. 你需要手动对 Item 进行离线挖掘，拿出 tag</p>
<blockquote>
<p>电影题材 : 爱情 / 探险 / 动作  / 喜剧<br>标志特征 : 黄晓明 / 王宝强<br>年代 : 1995, 2016 …<br>关键词</p>
</blockquote>
<p>两个Item根据 tag 的分值，进行求 距离.</p>
<hr>
<p><strong>举例文本挖掘 :</strong></p>
<p>One : 对于每个要推荐的内容,我们需要建立一份资料 :</p>
<blockquote>
<p>比如词 $k_i$ 在文档 $d_j$ 中的权重 $w_{ij}$ (常用的方法比如 TF-IDF)</p>
<p>有一个词表 Item [$w_1$, $w_2$, … ,$w_{4000}$], 对每个 document 建立一个词表 vector。</p>
</blockquote>
<p>Two : 需要对用户也建立一份资料 :</p>
<blockquote>
<p>比如说定义一个权重向量 ($w_{c1},…,w_{ck}$) , 其中 $w_{ci}$ 表示第 $k_i$ 个词对用户 $c$ 的重要度<br>user 之前有看过的 小说 或 文档。(看过的文档放在一起搞一个doc_vector，或者 将 doc_vector 加权平均)</p>
</blockquote>
<p>Three : 计算匹配度 </p>
<blockquote>
<p>余弦距离公式</p>
</blockquote>
<p>$$<br>u(c, s) = \cos(\vec{w_c}, \vec{w_s}) = \frac{\vec{w_c} \cdot \vec{w_s}}{\vec{||w_c||} \times \vec{||w_s||}}<br>$$</p>
<blockquote>
<p>总结三步 :</p>
<ol>
<li>对 每个 document 建立 vector</li>
<li>对 每个 user 建立 vector</li>
<li>比对 user 向量，与 该user 没有看过的 document 向量 之间的相似度</li>
</ol>
<p>TF-IDF : 评估一个 word 对当前 document 的重要性。 当前升高而升高，所有doc中，升高而下降。</p>
</blockquote>
<p><strong>Sample Example : 基于书名进行书推荐</strong></p>
<p>一个用户对《Building data mining applications for CRM》感兴趣，从以下书中进行推荐 :</p>
<ol>
<li><code>Building data mining applications for CRM</code></li>
<li>Accelerating Customer Relationships: Using CRM and Relationship Technologies</li>
<li>Mastering Data Mining: The Art and Science of Customer Relationship Management</li>
<li>Data Mining Your Website</li>
<li>Introduction to marketing</li>
<li>Consumer behavior</li>
<li>marketing research, a handbook </li>
<li>Customer knowledge manag</li>
</ol>
<p><img src="/images/recommendation/rs-six-month-01.png" alt="基于内容的推荐"></p>
<blockquote>
<p>TF-IDF Normed Vectors 值 (0.187、0.316) 代表 <code>Data</code> 对于当前 <code>标题</code> 的重要程度. </p>
<p>根据 TF-IDF，当前书名 与 图书馆所有书名 出现的次数, (去掉停用词, 如 and) 计算出来的权重。</p>
<p>所以 <code>《Building data mining applications for CRM》</code> 已经有一个列 vector， 与 其他所有 书 计算 相似度，相似度 高的，为推荐最好的。</p>
<p>This is only a sample show example， not the real method of thing.</p>
</blockquote>
<p>计算这本书和其余7本书的相似度，推荐最近的，这里结果为 :</p>
<blockquote>
<p>rank 1 : Data Mining Your Website<br>􏰅rank 2 : Accelerating Custom Relationships: Using CRM …<br>rank 3 : Mastering Data Mining: The Art and Science… 􏰬􏰭􏰮􏰀􏰁…</p>
</blockquote>
<p>结论 : 基于内容的推荐，无需用户行为的交互关联。</p>
<h3 id="5-2-协同过滤-User-based"><a href="#5-2-协同过滤-User-based" class="headerlink" title="5.2 协同过滤 (User-based)"></a>5.2 协同过滤 (User-based)</h3><p>找到和 User 最近的 其他 User􏰼􏰽􏰟􏰲􏰳􏰧􏰨􏰒􏰠􏰾􏰲􏰳 􏱅􏰲􏰳􏱆􏱀􏱁􏱂􏰕, 找到他们 看/买过但当前 User 没看/买过的 item，根据距离加权打分<br>找得分最高的推荐</p>
<blockquote>
<p>不需要提前挖掘 Item， 找品味接近的 5 个 friends。</p>
</blockquote>
<p><img src="/images/recommendation/rs-six-month-02.png" alt="User-based CF"></p>
<p>近邻怎么找 ?</p>
<table>
<thead>
<tr>
<th>U</th>
<th>a</th>
<th>b</th>
<th>c</th>
<th>d</th>
<th>e</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>5</td>
<td>1</td>
<td>5</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>B</td>
<td>4</td>
<td></td>
<td>3</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<h3 id="5-3-协同过滤-Item-based"><a href="#5-3-协同过滤-Item-based" class="headerlink" title="5.3 协同过滤 (Item-based)"></a>5.3 协同过滤 (Item-based)</h3><blockquote>
<p>据现有 User 对 Item 的评级情况，来计算 Item 之间的某种相似度。已有Item 相似的 Item 被用来生成一个综合得分，而该得分用于评估未知物品的相似度。</p>
</blockquote>
<p>根据 User 对 Item 的行为，计算 item 和 item 相似度，找到和当前 item 最近的进行推荐。</p>
<p><img src="/images/recommendation/rs-six-month-03.png" alt="Item-based CF"></p>
<blockquote>
<p>Collaborative filtering 基于 近邻 的思想</p>
</blockquote>
<p>CF (Item-based) Summary :</p>
<ul>
<li>一个 User List ($u_i$, i = 1, …, n), 一个 Item List ($p_j$, j = 1, …, m)</li>
<li>一个 $n * m$ 矩阵 W, 每个元素 $W_{ij}$, 表示 ${User}_i$ 对 ${Item}_j$ 的打分</li>
<li>计算 item 和 item 之间的相似度</li>
</ul>
<p>$$<br>S(i, j) = \cos(\vec{i}, \vec{j}) = \frac{\vec{i} \cdot \vec{j}}{\vec{||i||} \times \vec{||j||}}<br>$$</p>
<ul>
<li>选取 Top K 推荐 或 加权预测得分</li>
</ul>
<p>$$<br>r_{xi} = \frac { {\sum_{j \in N (i;x)} S_{ij} \times r_{xj}}} { {\sum_{j \in N (i;x)} S_{ij}}}<br>$$</p>
<p><strong>Item-based Collaborative Filtering</strong></p>
<p><img src="/images/recommendation/rs-six-month-05.png" alt="CF-Item Collaborative Filtering"></p>
<blockquote>
<p>1 与 4 之间的相似度-0.10不对， 应该是 -0.83 等，数据有误。(按照 Pearson overlap 计算).</p>
<p>$r_{1.5}$ = (0.41*2 + 0.59*3) / (0.41 + 0.59) = 2.6</p>
<p>基于 Top N 来计算，这里是 Top 2.</p>
</blockquote>
<h4 id="User-CF-vs-Item-CF"><a href="#User-CF-vs-Item-CF" class="headerlink" title="User CF vs Item CF"></a>User CF vs Item CF</h4><table>
<thead>
<tr>
<th>属性</th>
<th>UserCF</th>
<th>ItemCF</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>性能</code></td>
<td>User多，计算User相似度矩阵代价很大</td>
<td>Item多，计算Item相似度矩阵代价很大</td>
</tr>
<tr>
<td><code>领域</code></td>
<td>User 个性化兴趣不太明显的领域</td>
<td>长尾物品丰富，用户个性化需求强烈的领域</td>
</tr>
<tr>
<td><code>实时性</code></td>
<td>用户有新行为，不一定推荐结果马上变化</td>
<td>用户有新行为，会导致推荐结果实时变化</td>
</tr>
<tr>
<td><code>冷启动</code></td>
<td>新用户行为，不能立即进行个性化推荐<br>(用户相似度是每隔一段时间离线计算的)</td>
<td>新用户对物品有行为，就可以给他推荐和该物品相关物品</td>
</tr>
<tr>
<td><code>推荐理由</code></td>
<td>很难提供令用户信服的推荐解释</td>
<td>利用用户的历史行为给用户做推荐解释</td>
</tr>
</tbody>
</table>
<h4 id="CF-优缺点"><a href="#CF-优缺点" class="headerlink" title="CF 优缺点"></a>CF 优缺点</h4><table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>基于用户行为，因此对推荐内容无需先验知识</td>
<td>需要大量的显性 / 隐性用户行为 (冷启动)</td>
</tr>
<tr>
<td>只需要用户和商品关联矩阵即可，结构简单</td>
<td>需要完全相同的商品关联，相似的不同</td>
</tr>
<tr>
<td>在用户行为丰富的情况下，效果好</td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>(a, c) (b, c) 通过传递关系，可以近似的计算出来 (a, b) 相似度，矩阵稀疏的二度关联。</p>
</blockquote>
<h2 id="6-冷启动问题"><a href="#6-冷启动问题" class="headerlink" title="6. 冷启动问题"></a>6. 冷启动问题</h2><h3 id="6-1-New-User"><a href="#6-1-New-User" class="headerlink" title="6.1 New User"></a>6.1 New User</h3><ul>
<li>推荐 非常 热门 的商品，收集一些信息</li>
<li>用户注册的时候，收集信息，或者互动游戏，确定你喜欢否</li>
</ul>
<h3 id="6-2-New-Item"><a href="#6-2-New-Item" class="headerlink" title="6.2 New Item"></a>6.2 New Item</h3><ul>
<li>根据本身属性，求与原来 Item 相似度</li>
<li>Item-based CF 可推荐出去</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle_Titanic]]></title>
      <url>http://sggo.me/2017/02/16/ml/sample-BPNN-Titanic-2017-02-16-1732-ipynb/</url>
      <content type="html"><![CDATA[<p>neural networks example - pybrain kaggle titanic</p>
<a id="more"></a>
<h2 id="泰坦尼克号"><a href="#泰坦尼克号" class="headerlink" title="泰坦尼克号"></a><a href="https://www.kaggle.com/c/titanic" target="_blank" rel="external">泰坦尼克号</a></h2><p>『Jack and Rose』的故事，豪华游艇倒了，大家都惊恐逃生，可是救生艇的数量有限，无法人人都有，副船长发话了『lady and kid first！』，所以是否获救其实并非随机，而是基于一些背景有rank先后的。<br><br>训练和测试数据是一些乘客的个人信息以及存活状况，要尝试根据它生成合适的模型并预测其他人的存活状况。<br></p>
<p>这是一个二分类问题，很多分类算法都可以解决。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">构建Pybrain神经网络的基本步骤：</span><br><span class="line"></span><br><span class="line">1. 构建神经网路</span><br><span class="line">2. 构造数据集</span><br><span class="line">3. 训练神经网络</span><br><span class="line">4. 预测测试集结果</span><br><span class="line">5. 验证和分析</span><br></pre></td></tr></table></figure>
<h2 id="1-分析问题"><a href="#1-分析问题" class="headerlink" title="1. 分析问题"></a>1. 分析问题</h2><blockquote>
<p>分析什么特征是更重要的</p>
</blockquote>
<p>先看看数据长什么样？ 还是用pandas加载数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这个ipython notebook主要是我解决Kaggle Titanic问题的思路和过程</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#数据分析</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#科学计算</span></span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> Series,DataFrame</span><br><span class="line"></span><br><span class="line">data_train = pd.read_csv(<span class="string">"Train.csv"</span>)</span><br><span class="line"><span class="keyword">print</span> data_train.columns</span><br><span class="line"><span class="keyword">print</span> <span class="string">"============ df info ============"</span></span><br><span class="line">data_train.info()</span><br></pre></td></tr></table></figure>
<pre><code>Index([u&apos;PassengerId&apos;, u&apos;Survived&apos;, u&apos;Pclass&apos;, u&apos;Name&apos;, u&apos;Sex&apos;, u&apos;Age&apos;,
       u&apos;SibSp&apos;, u&apos;Parch&apos;, u&apos;Ticket&apos;, u&apos;Fare&apos;, u&apos;Cabin&apos;, u&apos;Embarked&apos;],
      dtype=&apos;object&apos;)
============ df info ============
&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
PassengerId    891 non-null int64
Survived       891 non-null int64
Pclass         891 non-null int64
Name           891 non-null object
Sex            891 non-null object
Age            714 non-null float64
SibSp          891 non-null int64
Parch          891 non-null int64
Ticket         891 non-null object
Fare           891 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
dtypes: float64(2), int64(5), object(5)
memory usage: 83.6+ KB
</code></pre><p>我们看大概有以下这些字段</p>
<p>PassengerId =&gt; 乘客ID<br>Pclass =&gt; 乘客等级(1/2/3等舱位)<br>Name =&gt; 乘客姓名<br>Sex =&gt; 性别<br>Age =&gt; 年龄<br>SibSp =&gt; 堂兄弟/妹个数<br>Parch =&gt; 父母与小孩个数<br>Ticket =&gt; 船票信息<br>Fare =&gt; 票价<br>Cabin =&gt; 客舱<br>Embarked =&gt; 登船港口</p>
<p>上面的数据说什么了？它告诉我们，训练数据中总共有891名乘客，但是很不幸，我们有些属性的数据不全，比如说：</p>
<ul>
<li>Age（年龄）属性只有714名乘客有记录</li>
<li>Cabin（客舱）更是只有204名乘客是已知的</li>
</ul>
<p>再瞄一眼具体数据数值情况，得到数值型数据的一些分布(因为有些属性，比如姓名，是文本型；而另外一些属性，比如登船港口，是类目型。这些我们用下面的函数是看不到的)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_train.describe()</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>-</th>
<th>PassengerId</th>
<th>Survived</th>
<th>Pclass</th>
<th>Age</th>
<th>SibSp</th>
<th>Parch</th>
<th>Fare</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>891.000000</td>
<td>891.000000</td>
<td>891.000000</td>
<td>714.000000</td>
<td>891.000000</td>
<td>891.000000</td>
<td>891.000000</td>
</tr>
<tr>
<td>mean</td>
<td>446.000000</td>
<td>0.383838</td>
<td>2.308642</td>
<td>29.699118</td>
<td>0.523008</td>
<td>0.381594</td>
<td>32.204208</td>
</tr>
</tbody>
</table>
<p><font color="red">mean字段告诉我们，大概0.383838的人最后获救了，2/3等舱的人数比1等舱要多，平均乘客年龄大概是29.7岁(计算这个时候会略掉无记录的)等等…<font></font></font></p>
<ul>
<li><font color="blue">『对数据的认识太重要了！』<font></font></font></li>
<li><font color="red">『对数据的认识太重要了！』<font></font></font></li>
<li><font color="green">『对数据的认识太重要了！』<font></font></font></li>
</ul>
<p>总结 : </p>
<ol>
<li>Cabin，没看出什么明显特征，缺失值又太多</li>
<li>Age：可以尝试补全缺失的数据</li>
</ol>
<p><strong>通常遇到缺值的情况，我们会有几种常见的处理方式</strong><br><br></p>
<ol>
<li>如果缺值的样本占总数比例极高，我们可能就直接舍弃了，作为特征加入的话，可能反倒带入noise，影响最后的结果了</li>
<li>如果缺值的样本适中，而该属性非连续值特征属性(比如说类目属性)，那就把NaN作为一个新类别，加到类别特征中</li>
<li>如果缺值的样本适中，而该属性为连续值特征属性，有时候我们会考虑给定一个step(比如这里的age，我们可以考虑每隔2/3岁为一个步长)，然后把它离散化，之后把NaN作为一个type加到属性类目中。</li>
<li>有些情况下，缺失的值个数并不是特别多，那我们也可以试着根据已有的值，拟合一下数据，补充上。</li>
</ol>
<p>本例中，后两种处理方式应该都是可行的，我们先试试拟合补全吧(没有特别多的背景可供我们拟合，这不一定是一个好的选择)</p>
<p>我们这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据<br></p>
<ol>
<li>从数据来估计 Age 应该是比较重要的, 还有毕竟 副船长发话了『lady and kid first！』</li>
<li>Cabin 缺失值太多了，先按Cabin有无数据，将这个属性处理成Yes和No两种类型吧</li>
</ol>
<h2 id="2-准备数据"><a href="#2-准备数据" class="headerlink" title="2. 准备数据"></a>2. 准备数据</h2><ol>
<li>Age 与 cabin 补全</li>
<li>one-hot 编码</li>
<li>Feature Scaling</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"> </span><br><span class="line"><span class="comment">### 使用 RandomForestClassifier 填补缺失的年龄属性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_missing_ages</span><span class="params">(df)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 把已有的数值型特征取出来丢进Random Forest Regressor中</span></span><br><span class="line">    age_df = df[[<span class="string">'Age'</span>,<span class="string">'Fare'</span>, <span class="string">'Parch'</span>, <span class="string">'SibSp'</span>, <span class="string">'Pclass'</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 乘客分成已知年龄和未知年龄两部分</span></span><br><span class="line">    known_age = age_df[age_df.Age.notnull()].as_matrix()</span><br><span class="line">    unknown_age = age_df[age_df.Age.isnull()].as_matrix()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># y即目标年龄</span></span><br><span class="line">    y = known_age[:, <span class="number">0</span>] <span class="comment">## Age ‘s value list</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># X即特征属性值</span></span><br><span class="line">    X = known_age[:, <span class="number">1</span>:] <span class="comment">## Fare 的值 list</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># fit到RandomForestRegressor之中</span></span><br><span class="line">    rfr = RandomForestRegressor(random_state=<span class="number">0</span>, n_estimators=<span class="number">2000</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">    rfr.fit(X, y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用得到的模型进行未知年龄结果预测</span></span><br><span class="line">    predictedAges = rfr.predict(unknown_age[:, <span class="number">1</span>::])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用得到的预测结果填补原缺失数据</span></span><br><span class="line">    df.loc[ (df.Age.isnull()), <span class="string">'Age'</span> ] = predictedAges </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> df, rfr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_Cabin_type</span><span class="params">(df)</span>:</span></span><br><span class="line">    df.loc[ (df.Cabin.notnull()), <span class="string">'Cabin'</span> ] = <span class="string">"Yes"</span></span><br><span class="line">    df.loc[ (df.Cabin.isnull()), <span class="string">'Cabin'</span> ] = <span class="string">"No"</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">data_train, rfr = set_missing_ages(data_train)</span><br><span class="line">data_train = set_Cabin_type(data_train)</span><br><span class="line">data_train</span><br></pre></td></tr></table></figure>
<p><strong>one-hot 编码</strong></p>
<p>因为逻辑回归建模时，需要输入的特征都是数值型特征，我们通常会先对类目型的特征因子化/one-hot编码。 <br><br>什么叫做因子化/one-hot编码？举个例子：<font><br></font></p>
<p>以Embarked为例，原本一个属性维度，因为其取值可以是[‘S’,’C’,’Q‘]，而将其平展开为’Embarked_C’,’Embarked_S’, ‘Embarked_Q’三个属性<font><br></font></p>
<ol>
<li>原Embarked取S的，在此处的”Embarked_S”下取值为1，在’Embarked_C’, ‘Embarked_Q’下取值为0<br></li>
<li>原Embarked取C的，在此处的”Embarked_C”下取值为1，在’Embarked_S’, ‘Embarked_Q’下取值为0<br></li>
<li>原Embarked取Q的，在此处的”Embarked_Q”下取值为1，在’Embarked_C’, ‘Embarked_S’下取值为0<br></li>
</ol>
<p>我们使用pandas的”get_dummies”来完成这个工作，并拼接在原来的”data_train”之上，如下所示。<br></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 因为逻辑回归建模时，需要输入的特征都是数值型特征</span></span><br><span class="line"><span class="comment"># 我们先对类目型的特征离散/因子化</span></span><br><span class="line"><span class="comment"># 以Cabin为例，原本一个属性维度，因为其取值可以是['yes','no']，而将其平展开为'Cabin_yes','Cabin_no'两个属性</span></span><br><span class="line"><span class="comment"># 原本Cabin取值为yes的，在此处的'Cabin_yes'下取值为1，在'Cabin_no'下取值为0</span></span><br><span class="line"><span class="comment"># 原本Cabin取值为no的，在此处的'Cabin_yes'下取值为0，在'Cabin_no'下取值为1</span></span><br><span class="line"><span class="comment"># 我们使用pandas的get_dummies来完成这个工作，并拼接在原来的data_train之上，如下所示</span></span><br><span class="line">dummies_Cabin = pd.get_dummies(data_train[<span class="string">'Cabin'</span>], prefix= <span class="string">'Cabin'</span>)</span><br><span class="line"></span><br><span class="line">dummies_Embarked = pd.get_dummies(data_train[<span class="string">'Embarked'</span>], prefix= <span class="string">'Embarked'</span>)</span><br><span class="line"></span><br><span class="line">dummies_Sex = pd.get_dummies(data_train[<span class="string">'Sex'</span>], prefix= <span class="string">'Sex'</span>)</span><br><span class="line"></span><br><span class="line">dummies_Pclass = pd.get_dummies(data_train[<span class="string">'Pclass'</span>], prefix= <span class="string">'Pclass'</span>)</span><br><span class="line">df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=<span class="number">1</span>)</span><br><span class="line">df.drop([<span class="string">'Pclass'</span>, <span class="string">'Name'</span>, <span class="string">'Sex'</span>, <span class="string">'Ticket'</span>, <span class="string">'Cabin'</span>, <span class="string">'Embarked'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>PassengerId</th>
<th>…</th>
<th>Embarked_C</th>
<th>Embarked_Q</th>
<th>Embarked_S</th>
<th>Sex_female</th>
<th>Sex_male</th>
<th>Pclass_1</th>
<th>Pclass_2</th>
<th>Pclass_3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>…</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
</tbody>
</table>
<p><strong>Feature Scaling</strong></p>
<p><font color="red">Age和Fare两个属性，乘客的数值幅度变化，scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。<font></font></font></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 接下来我们要接着做一些数据预处理的工作，比如scaling，将一些变化幅度较大的特征化到[-1,1]之内</span></span><br><span class="line"><span class="comment"># 这样可以加速logistic regression的收敛</span></span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> preprocessing</span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">age_scale_param = scaler.fit(df[<span class="string">'Age'</span>])</span><br><span class="line">df[<span class="string">'Age_scaled'</span>] = scaler.fit_transform(df[<span class="string">'Age'</span>], age_scale_param)</span><br><span class="line">fare_scale_param = scaler.fit(df[<span class="string">'Fare'</span>])</span><br><span class="line">df[<span class="string">'Fare_scaled'</span>] = scaler.fit_transform(df[<span class="string">'Fare'</span>], fare_scale_param)</span><br><span class="line">df</span><br><span class="line">train_df = df.filter(regex=<span class="string">'Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*'</span>)</span><br><span class="line">train_df</span><br></pre></td></tr></table></figure>
<h2 id="3-构造神经网络"><a href="#3-构造神经网络" class="headerlink" title="3. 构造神经网络"></a>3. 构造神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> Series,DataFrame</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas.io.data <span class="keyword">as</span> wb</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pybrain.structure <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pybrain.datasets <span class="keyword">import</span> SupervisedDataSet</span><br><span class="line"><span class="keyword">from</span> pybrain.supervised.trainers <span class="keyword">import</span> BackpropTrainer</span><br><span class="line"></span><br><span class="line"><span class="comment"># createa neural network</span></span><br><span class="line">fnn = FeedForwardNetwork()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create three layers, input layer:2 input unit; hidden layer: 10 units; output layer: 1 output</span></span><br><span class="line">inLayer = LinearLayer(<span class="number">9</span>, name=<span class="string">'inLayer'</span>)</span><br><span class="line">hiddenLayer0 = SigmoidLayer(<span class="number">1</span>, name=<span class="string">'hiddenLayer0'</span>)</span><br><span class="line">outLayer = LinearLayer(<span class="number">1</span>, name=<span class="string">'outLayer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add three layers to the neural network</span></span><br><span class="line">fnn.addInputModule(inLayer)</span><br><span class="line">fnn.addModule(hiddenLayer0)</span><br><span class="line">fnn.addOutputModule(outLayer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># link three layers</span></span><br><span class="line">in_to_hidden0 = FullConnection(inLayer,hiddenLayer0)</span><br><span class="line">hidden0_to_out = FullConnection(hiddenLayer0, outLayer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add the links to neural network</span></span><br><span class="line">fnn.addConnection(in_to_hidden0)</span><br><span class="line">fnn.addConnection(hidden0_to_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make neural network come into effect</span></span><br><span class="line">fnn.sortModules()</span><br></pre></td></tr></table></figure>
<h2 id="4-构造数据集"><a href="#4-构造数据集" class="headerlink" title="4. 构造数据集"></a>4. 构造数据集</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># obtain the original data</span></span><br><span class="line"></span><br><span class="line">x1 = train_df[<span class="string">'SibSp'</span>]</span><br><span class="line">x2 = train_df[<span class="string">'Parch'</span>]</span><br><span class="line">x3 = train_df[<span class="string">'Sex_female'</span>]</span><br><span class="line">x4 = train_df[<span class="string">'Sex_male'</span>]</span><br><span class="line">x5 = train_df[<span class="string">'Pclass_1'</span>]</span><br><span class="line">x6 = train_df[<span class="string">'Pclass_2'</span>]</span><br><span class="line">x7 = train_df[<span class="string">'Pclass_3'</span>]</span><br><span class="line">x8 = train_df[<span class="string">'Age_scaled'</span>]</span><br><span class="line">x9 = train_df[<span class="string">'Fare_scaled'</span>]</span><br><span class="line"></span><br><span class="line">y = train_df[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># definite the dataset as two input , one output</span></span><br><span class="line">DS = SupervisedDataSet(<span class="number">9</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add data element to the dataset</span></span><br><span class="line"><span class="comment"># 往数据集内加样本点</span></span><br><span class="line"><span class="comment"># 假设x1，x2，x3是输入的三个维度向量，y是输出向量，并且它们的长度相同</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(len(train_df)):</span><br><span class="line">    DS.addSample([x1[i],x2[i],x3[i],x4[i],x5[i],x6[i],x7[i],x8[i],x9[i]],[y[i]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># you can get your input/output this way</span></span><br><span class="line"><span class="comment"># 如果要获得里面的输入／输出时，可以用</span></span><br><span class="line">X = DS[<span class="string">'input'</span>]</span><br><span class="line">Y = DS[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># split the dataset into train dataset and test dataset</span></span><br><span class="line"><span class="comment"># 如果要把数据集切分成训练集和测试集，可以用下面的语句，训练集：测试集＝8:2</span></span><br><span class="line"><span class="comment"># 为了方便之后的调用，可以把输入和输出拎出来</span></span><br><span class="line">dataTrain, dataTest = DS.splitWithProportion(<span class="number">0.8</span>)</span><br><span class="line">xTrain, yTrain = dataTrain[<span class="string">'input'</span>],dataTrain[<span class="string">'target'</span>]</span><br><span class="line">xTest, yTest = dataTest[<span class="string">'input'</span>], dataTest[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"bulid data sets end ！"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print xTest[1]</span></span><br><span class="line">type(xTest)</span><br></pre></td></tr></table></figure>
<pre><code>bulid data sets end ！
numpy.ndarray
</code></pre><h2 id="5-训练神经网络"><a href="#5-训练神经网络" class="headerlink" title="5. 训练神经网络"></a>5. 训练神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train the NN</span></span><br><span class="line"><span class="comment"># we use BP Algorithm</span></span><br><span class="line"><span class="comment"># verbose = True means print th total error</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"trainer start..."</span></span><br><span class="line">trainer = BackpropTrainer(fnn, dataTrain, verbose=<span class="keyword">False</span>,learningrate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"run..."</span></span><br><span class="line"><span class="comment"># set the epoch times to make the NN  fit</span></span><br><span class="line">trainer.trainUntilConvergence(maxEpochs=<span class="number">500</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"trainer end!"</span></span><br></pre></td></tr></table></figure>
<pre><code>trainer start...
run...
trainer end!
</code></pre><h2 id="6-预测-test-csv"><a href="#6-预测-test-csv" class="headerlink" title="6. 预测 test.csv"></a>6. 预测 test.csv</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_test = pd.read_csv(<span class="string">"test.csv"</span>)</span><br><span class="line">data_test.loc[ (data_test.Fare.isnull()), <span class="string">'Fare'</span> ] = <span class="number">0</span></span><br><span class="line"><span class="comment"># 接着我们对test_data做和train_data中一致的特征变换</span></span><br><span class="line"><span class="comment"># 首先用同样的RandomForestRegressor模型填上丢失的年龄</span></span><br><span class="line">tmp_df = data_test[[<span class="string">'Age'</span>,<span class="string">'Fare'</span>, <span class="string">'Parch'</span>, <span class="string">'SibSp'</span>, <span class="string">'Pclass'</span>]]</span><br><span class="line">null_age = tmp_df[data_test.Age.isnull()].as_matrix()</span><br><span class="line"><span class="comment"># 根据特征属性X预测年龄并补上</span></span><br><span class="line">X = null_age[:, <span class="number">1</span>:]</span><br><span class="line">predictedAges = rfr.predict(X)</span><br><span class="line">data_test.loc[ (data_test.Age.isnull()), <span class="string">'Age'</span> ] = predictedAges</span><br><span class="line"></span><br><span class="line">data_test = set_Cabin_type(data_test)</span><br><span class="line">dummies_Cabin = pd.get_dummies(data_test[<span class="string">'Cabin'</span>], prefix= <span class="string">'Cabin'</span>)</span><br><span class="line">dummies_Embarked = pd.get_dummies(data_test[<span class="string">'Embarked'</span>], prefix= <span class="string">'Embarked'</span>)</span><br><span class="line">dummies_Sex = pd.get_dummies(data_test[<span class="string">'Sex'</span>], prefix= <span class="string">'Sex'</span>)</span><br><span class="line">dummies_Pclass = pd.get_dummies(data_test[<span class="string">'Pclass'</span>], prefix= <span class="string">'Pclass'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=<span class="number">1</span>)</span><br><span class="line">df_test.drop([<span class="string">'Pclass'</span>, <span class="string">'Name'</span>, <span class="string">'Sex'</span>, <span class="string">'Ticket'</span>, <span class="string">'Cabin'</span>, <span class="string">'Embarked'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">df_test[<span class="string">'Age_scaled'</span>] = scaler.fit_transform(df_test[<span class="string">'Age'</span>], age_scale_param)</span><br><span class="line">df_test[<span class="string">'Fare_scaled'</span>] = scaler.fit_transform(df_test[<span class="string">'Fare'</span>], fare_scale_param)</span><br><span class="line">df_test</span><br><span class="line"></span><br><span class="line">test = df_test.filter(regex=<span class="string">'SibSp|Parch|Sex_female|Sex_male|Pclass_1|Pclass_2|Pclass_3|Age_scaled|Fare_scaled'</span>)</span><br><span class="line">xTest = test.as_matrix()</span><br><span class="line"><span class="comment"># prediction = fnn.activate(xTest[1])</span></span><br><span class="line"><span class="comment"># print("the prediction number is :",prediction," the real number is:  ",yTest[1])</span></span><br><span class="line">predict_resutl=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(len(xTest)):</span><br><span class="line">    res = fnn.activate(xTest[i])[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> (res &gt; <span class="number">0.5</span>) :</span><br><span class="line">        res = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span> :</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">    predict_resutl.append(res)</span><br><span class="line"></span><br><span class="line">final_res = np.array(predict_resutl).T</span><br><span class="line"></span><br><span class="line">result = pd.DataFrame(&#123;<span class="string">'PassengerId'</span>:data_test[<span class="string">'PassengerId'</span>].as_matrix(), <span class="string">'Survived'</span>:final_res&#125;)</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<h2 id="7-结果写入文件"><a href="#7-结果写入文件" class="headerlink" title="7. 结果写入文件"></a>7. 结果写入文件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result.to_csv(<span class="string">"logistic_regression_predictions.csv"</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="8-验证和分析"><a href="#8-验证和分析" class="headerlink" title="8. 验证和分析"></a>8. 验证和分析</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> mod <span class="keyword">in</span> fnn.modules:</span><br><span class="line">  <span class="keyword">print</span> (<span class="string">"Module:"</span>, mod.name)</span><br><span class="line">  <span class="keyword">if</span> mod.paramdim &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"--parameters:"</span>, mod.params)</span><br><span class="line">  <span class="keyword">for</span> conn <span class="keyword">in</span> fnn.connections[mod]:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"-connection to"</span>, conn.outmod.name)</span><br><span class="line">    <span class="keyword">if</span> conn.paramdim &gt; <span class="number">0</span>:</span><br><span class="line">       <span class="keyword">print</span> (<span class="string">"- parameters"</span>, conn.params)</span><br><span class="line">  <span class="keyword">if</span> hasattr(fnn, <span class="string">"recurrentConns"</span>):</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Recurrent connections"</span>)</span><br><span class="line">    <span class="keyword">for</span> conn <span class="keyword">in</span> fnn.recurrentConns:</span><br><span class="line">       <span class="keyword">print</span> (<span class="string">"-"</span>, conn.inmod.name, <span class="string">" to"</span>, conn.outmod.name)</span><br><span class="line">       <span class="keyword">if</span> conn.paramdim &gt; <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">print</span> (<span class="string">"- parameters"</span>, conn.params)</span><br></pre></td></tr></table></figure>
<pre><code>(&apos;Module:&apos;, &apos;outLayer&apos;)
(&apos;Module:&apos;, &apos;inLayer&apos;)
(&apos;-connection to&apos;, &apos;hiddenLayer0&apos;)
(&apos;- parameters&apos;, array([-0.47294263, -0.04334279,  0.77615167, -2.03865708,  2.38243634,
        0.69678661, -0.82062356, -0.70921933, -0.12044558]))
(&apos;Module:&apos;, &apos;hiddenLayer0&apos;)
(&apos;-connection to&apos;, &apos;outLayer&apos;)
(&apos;- parameters&apos;, array([ 1.08050317]))
</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://blog.csdn.net/han_xiaoyang/article/details/49797143" target="_blank" rel="external">寒小阳-Titanic</a></li>
<li><a href="http://www.zengmingxia.com/topics/computing/" target="_blank" rel="external">一蓑烟雨任平生</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Flexible Task Scheduling Framework]]></title>
      <url>http://sggo.me/2017/01/30/hadoop/hadoop-hive-bigdata-offline-demo/</url>
      <content type="html"><![CDATA[<p>本篇介绍一种 大数据离线开发模块的解决方案， 其实这是一个手写的适合离线调度的灵活小框架</p>
<p><a href="https://github.com/blair101/bigdata-tools/tree/master/bigdata-offline-demo" target="_blank" rel="external">blair’s github bigdata_offline_demo</a></p>
<a id="more"></a>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul>
<li>提供一些 best practice</li>
<li>提高各模块结构及代码的一致性</li>
<li>降低开发新模块的成本</li>
<li>便于离线大数据分析</li>
<li>当然它也适用于对任何离线Job进行调度</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这是一个 hive 配合 shell 等其他语言写成的灵活调度框架.</span><br><span class="line">   该示例模块架 适用于离线分析，特别是每天跑的crontab任务，或者是每周、每月跑的任务</span><br></pre></td></tr></table></figure>
<h2 id="1-代码规范"><a href="#1-代码规范" class="headerlink" title="1. 代码规范"></a>1. 代码规范</h2><ul>
<li>对 Hive 表的操作，如基本SQL不能满足需求，则建议优先采用 Java UDF/UDAF 的方式</li>
<li>非 Hive 操作, 推荐优先采用python版 streaming 方式或者Java语言实现方式</li>
<li>每个模块建议给出准确的输入、输出格式定义注释说明.</li>
</ul>
<h2 id="2-结构规范"><a href="#2-结构规范" class="headerlink" title="2. 结构规范"></a>2. 结构规范</h2><h3 id="2-1-模块目录"><a href="#2-1-模块目录" class="headerlink" title="2.1 模块目录"></a>2.1 模块目录</h3><ul>
<li>每个模块为一个目录，模块名 一般与其中 主代码目录 名称一致</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ~/ghome/xgitlab/group-data/bigdata-offline-demo [master ✗ (0cdd403)] [14:43:10]</span></span><br><span class="line">➜ ll</span><br><span class="line">total 40</span><br><span class="line">-rw-r--r--   1 blair  staff   106B Dec 15 14:12 LICENSE</span><br><span class="line">-rw-r--r--@  1 blair  staff   4.9K Dec 15 14:43 README.md</span><br><span class="line">drwxr-xr-x  10 blair  staff   320B Dec 15 14:02 bigdata-offline-demo</span><br><span class="line">drwxr-xr-x   4 blair  staff   128B Dec 15 14:11 docs</span><br></pre></td></tr></table></figure>
<p><strong>目录功能说明</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">bigdata-offline-demo</td>
<td style="text-align:center">主代码目录 名称</td>
</tr>
<tr>
<td style="text-align:center">README.md</td>
<td style="text-align:center">主要说明</td>
</tr>
<tr>
<td style="text-align:center">LICENSE</td>
<td style="text-align:center">许可说明</td>
</tr>
<tr>
<td style="text-align:center">docs</td>
<td style="text-align:center">详细文档</td>
</tr>
</tbody>
</table>
<h3 id="2-2-主代码目录"><a href="#2-2-主代码目录" class="headerlink" title="2.2 主代码目录"></a>2.2 主代码目录</h3><blockquote>
<p><strong>bigdata-offline-demo/bigdata-offline-demo/</strong></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ~/ghome/xgitlab/group-data/bigdata-offline-demo/bigdata-offline-demo [master ✗ (0cdd403)] [14:44:28]</span></span><br><span class="line">drwxr-xr-x   5 blair  staff   160B Dec 15 14:02 alert</span><br><span class="line">drwxr-xr-x   6 blair  staff   192B Dec 15 14:02 conf</span><br><span class="line">drwxr-xr-x   3 blair  staff    96B Dec 15 14:02 create_table</span><br><span class="line">drwxr-xr-x   3 blair  staff    96B Dec 15 14:08 crontab_job</span><br><span class="line">drwxr-xr-x   3 blair  staff    96B Dec 15 14:02 flag</span><br><span class="line">drwxr-xr-x  40 blair  staff   1.3K Dec 15 14:02 <span class="built_in">log</span></span><br><span class="line">drwxr-xr-x   3 blair  staff    96B Dec 15 14:08 script</span><br><span class="line">drwxr-xr-x   8 blair  staff   256B Dec 15 14:04 util</span><br></pre></td></tr></table></figure>
<p><strong>主代码目录功能说明</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">目录</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">── alert</td>
<td style="text-align:left">报警封装</td>
</tr>
<tr>
<td style="text-align:left">── conf</td>
<td style="text-align:left">配置文件</td>
</tr>
<tr>
<td style="text-align:left">──create_table</td>
<td style="text-align:left">建表脚本</td>
</tr>
<tr>
<td style="text-align:left">── crontab_job</td>
<td style="text-align:left">crontab 任务脚本。(crontab任务脚本以crontab_job为前缀, 检测任务依赖关系, 调用主逻辑脚本等)</td>
</tr>
<tr>
<td style="text-align:left">── flag</td>
<td style="text-align:left">标记文件。(标志该模块已经开始运行，或者运行完毕)</td>
</tr>
<tr>
<td style="text-align:left">── log</td>
<td style="text-align:left">日志文件。 (如脚本失败可根据log追查定位失败原因)</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">── script</td>
<td style="text-align:left">主脚本文件</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">── util</td>
<td style="text-align:left">工具脚本。(主要包含写log脚本, hadoop file / local file / hive check 等封装, 初始化环境目录等)</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">── jar</td>
<td style="text-align:left">jar包 (如无则不需要建立)</td>
</tr>
<tr>
<td style="text-align:left">── java</td>
<td style="text-align:left">java (udf udaf) 源代码。 (如无则不需要建立)</td>
</tr>
</tbody>
</table>
<ul>
<li>script目录 主要存放 shell 脚本，有需要也存放 python 脚本。</li>
<li>java 代码需要放入 java目录 </li>
<li><p>如果该模块，script目录脚本较多，􏰀可以在 script dir之下建立子目录􏰀如􏰂 :</p>
<pre><code>├── script
  ├── sub_module_1 子模块目录 
  ├── sub_module_2 子模块目录
</code></pre></li>
</ul>
<h3 id="2-2-项目模板"><a href="#2-2-项目模板" class="headerlink" title="2.2 项目模板"></a>2.2 项目模板</h3><ul>
<li>提供公共的项目模板. 􏰀统一shell脚本代码􏰄结构􏰄 日志􏰄配置规范􏰁 等</li>
<li><p>项目模板可仿照本模块􏰂 : </p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://gitlab.***/data/bigdata-offline-demo.git</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>1). 变量命名􏰂</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">自有变量采用小写􏰀. export出的环境变量采用大写􏰁</span><br><span class="line">  hive 表命名 : </span><br><span class="line">   1. 原始数据表，数据挖掘团队建立的 则 采用命名方式为 ods_dm (original data stream, data_mining)开头</span><br><span class="line">   2. 非原始数据表 数据挖掘团队建立的 则 采用命名方式为 mds_dm (modified data stream , data_mining)开头</span><br><span class="line">   3. 临时数据表 数据挖掘团队建立的 则 采用命名方式为 tmp_dm (temp data stream , data_mining)开头</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>2). 配置文件 (conf目录下)</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">default.conf􏰂 配置公共参数􏰂程序路径􏰄hadoop 用户等􏰁</span><br><span class="line">vars.conf    配置任务参数􏰂 hive表名􏰄参数设置, 以及其他变量等􏰁</span><br><span class="line">alert.conf􏰂   配置邮件报警接收人􏰁</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>3). 输入输出</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 如果存在输入源多种数据格式(rcfile+lzo+textfile)的情况􏰀推荐采用生成临时的统一格式的数据表的方式处理􏰁 </span><br><span class="line">2. hive 表 输出原则上均采用rcfile格式􏰁。</span><br><span class="line">      (当然现在存储便宜，所以很多时候为了操作方便，也可采用text格式存储，但仍然推荐ods层面表统一为rcfile格式)</span><br><span class="line">3. 非hive表 输出采用rcfile或者lzo格式均可􏰁</span><br></pre></td></tr></table></figure>
</li>
<li><p>4). hive 建表示例.</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">table_name=<span class="string">"<span class="variable">$&#123;table_ods_e_coupon&#125;</span>"</span></span><br><span class="line">hql=<span class="string">"create external table if not exists <span class="variable">$&#123;table_name&#125;</span></span><br><span class="line">(</span><br><span class="line">    id bigint COMMENT '序号',</span><br><span class="line">    mobile_number varchar(16) COMMENT '领取电子券的手机号',</span><br><span class="line">    coupon_order_number varchar(32) COMMENT '购买电子券时的订单编号',</span><br><span class="line">    dt string</span><br><span class="line">) COMMENT '实际电子券'</span><br><span class="line">row format delimited fields terminated by '\001' collection items terminated by ',' map keys terminated by ':' lines terminated by '\n'</span><br><span class="line">stored as rcfile</span><br><span class="line">location '<span class="variable">$&#123;OSS_URL&#125;</span>/<span class="variable">$&#123;ods_hive_dir&#125;</span>/<span class="variable">$&#123;table_name&#125;</span>';</span><br><span class="line">"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$hql</span>"</span></span><br><span class="line"><span class="variable">$&#123;HIVE&#125;</span> <span class="_">-e</span> <span class="string">"<span class="variable">$hql</span>"</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>  说明</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 推荐百万级以上记录的 hive 表建立时指定分区，一般string dt=yyyy-mm-dd  </span><br><span class="line">2. 数据量不大，又不是按照天增加很多数据量的表，则不需要指定分区   </span><br><span class="line">3. 分区 string dt 一般建议为 yyyy-MM-dd 格式  (dt 取自 : date, 默认意思为数据产生日期)</span><br><span class="line">4. hive 建表时指定格式，列与列之间分隔符</span><br><span class="line">5. $&#123;hive_dir&#125;, mds 表为 bucket_name/data_mining/dm/mds/ </span><br><span class="line">                  ods 表为 bucket_name/data_mining/dm/ods/</span><br><span class="line">                          bucket名称 + 部门或大项目组名称 + 团队名称 + 层级</span><br></pre></td></tr></table></figure>
<ul>
<li><p>5). 报警规范</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">check_success 为封装好的，检测上一条语句执行执行成功，后自动发报警的函数。</span><br><span class="line">(发报警邮件标题以 模块名--任务脚本路径名--任务状态及原因-日期 来设置。便于快速定位问题)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3-开发流程"><a href="#3-开发流程" class="headerlink" title="3. 开发流程"></a>3. 开发流程</h2><h3 id="3-1-拷贝模块-demo"><a href="#3-1-拷贝模块-demo" class="headerlink" title="3.1 拷贝模块 demo"></a>3.1 拷贝模块 demo</h3><ul>
<li>copy 本模块，并重命名模块名</li>
</ul>
<h3 id="3-2-修改配置-conf"><a href="#3-2-修改配置-conf" class="headerlink" title="3.2 修改配置 conf"></a>3.2 修改配置 conf</h3><ul>
<li>修改 conf/default.conf 中用不到的 rds 相关变量</li>
<li>修改 conf/vars.conf 中相关变量</li>
</ul>
<h3 id="3-3-建表语句"><a href="#3-3-建表语句" class="headerlink" title="3.3 建表语句"></a>3.3 建表语句</h3><p>根据 create_table/create_table_e_coupon.sh 样例建表脚本，编写属于你自己的建表脚本 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[data_mining@emr-gw bigdata-offline-demo]$ ll create_table/</span><br><span class="line">total 8</span><br><span class="line">-rwxr-xr-x 1 data_mining hadoop 7946 Dec 13 14:30 create_table_e_coupon.sh</span><br></pre></td></tr></table></figure>
<h3 id="3-4-主脚本"><a href="#3-4-主脚本" class="headerlink" title="3.4 主脚本"></a>3.4 主脚本</h3><ul>
<li>编写 script 下你的主脚本</li>
<li>根据 script/ods_dm_e_coupon.sh 样例脚本，编写属于你自己的主脚本 </li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[data_mining@emr-gw bigdata-offline-demo]$ ll script/</span><br><span class="line">total 144</span><br><span class="line">-rwxr-xr-x 1 data_mining hadoop   2753 Dec 15 16:58 ods_dm_e_coupon.sh</span><br></pre></td></tr></table></figure>
<blockquote>
<p>不用拉取数据的，则不需要脚本开头的 source ${util_dir}/my_functions 这句代码。</p>
</blockquote>
<h3 id="3-5-调度脚本"><a href="#3-5-调度脚本" class="headerlink" title="3.5 调度脚本"></a>3.5 调度脚本</h3><ul>
<li>编写 crontab_job 下你的调度脚本</li>
<li>仿照 crontab_job_ods_e_coupon.sh 编写你主脚本对应的 调度脚本</li>
</ul>
<h3 id="3-6-定时任务"><a href="#3-6-定时任务" class="headerlink" title="3.6 定时任务"></a>3.6 定时任务</h3><p>在 linux crontab 中，增加需要定时启动的你的调度脚本 crontab_job_your_script.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[data_mining@emr-gw online]$ crontab -l</span><br><span class="line">10 02 * * * source /etc/bashrc; sh /home/data_mining/hero/online/bigdata-offline-demo/bigdata-offline-demo/crontab_job/crontab_job_ods_e_coupon.sh</span><br></pre></td></tr></table></figure>
<p>注 : 如有其他问题，再商议</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/blair101/bigdata-tools/tree/master/bigdata-offline-demo" target="_blank" rel="external">blair’s github bigdata_offline_demo</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Machine Learning p4 - Build Recommendation System]]></title>
      <url>http://sggo.me/2016/11/23/spark/spark-machine-learning-p4-rs/</url>
      <content type="html"><![CDATA[<p>Spark build Recommendation System, 推荐引擎试图对用户与某类物品之间的联系建模</p>
<a id="more"></a>
<ul>
<li>推荐引擎的类型；</li>
<li>用用户偏好数据来建立一个推荐模型；</li>
<li>为用户进行推荐和求指定物品的类似物品；</li>
<li>评估该模型的预测能力。</li>
</ul>
<h2 id="1-推荐模型分类"><a href="#1-推荐模型分类" class="headerlink" title="1. 推荐模型分类"></a>1. 推荐模型分类</h2><ul>
<li>内容过滤</li>
<li>协同过滤</li>
<li>矩阵分解</li>
</ul>
<h3 id="1-1-内容过滤-类似物品"><a href="#1-1-内容过滤-类似物品" class="headerlink" title="1.1 内容过滤 - (类似物品)"></a>1.1 内容过滤 - (类似物品)</h3><p>利用物品相似度定义，来求出与该物品类似的物品。</p>
<blockquote>
<p>对用户的推荐可以根据用户的属性或描述得出，之后再通过相同的相似度定义来与物品属性做匹配。</p>
</blockquote>
<h3 id="1-2-协同过滤-估计未触"><a href="#1-2-协同过滤-估计未触" class="headerlink" title="1.2 协同过滤 - (估计未触)"></a>1.2 协同过滤 - (估计未触)</h3><p>协同过滤是一种利用大量已有的用户偏好来估计用户对其<code>未接触过</code>的物品的喜好程度。其内在思想是<strong>相似度的定义</strong>。</p>
<ul>
<li><p>基于用户</p>
<blockquote>
<p>如果两个用户表现出相似的偏好，认为他们的兴趣类似。要对他们中的一个用户推荐一个未知物品，便可选取若干与其类似的用户并根据他们的喜好计算出对各个物品的综合得分。</p>
</blockquote>
</li>
<li><p>基于物品</p>
<blockquote>
<p>据现有用户对物品的偏好或是评级情况，来计算物品之间的某种相似度。已有物品相似的物品被用来生成一个综合得分，而该得分用于评估未知物品的相似度。</p>
</blockquote>
</li>
</ul>
<p>基于<strong>用户</strong>或<strong>物品</strong>的方法得分取决于若干用户或是物品之间依据相似度所构成的集合（即邻居），故它们也常被称为KNN。</p>
<p>对“用户-物品”<code>偏好建模</code></p>
<h3 id="1-3-矩阵分解"><a href="#1-3-矩阵分解" class="headerlink" title="1.3 矩阵分解"></a>1.3 矩阵分解</h3><p>Spark推荐模型库 包含基于矩阵分解（matrix factorization）的实现，该模型在协同过滤中的表现十分出色。</p>
<h4 id="1-3-1-显式矩阵分解"><a href="#1-3-1-显式矩阵分解" class="headerlink" title="1.3.1 显式矩阵分解"></a>1.3.1 显式矩阵分解</h4><p>显式自身偏好数据</p>
<blockquote>
<p>这类数据包括如物品评级、赞、喜欢等用户对物品的评价。转换为以用户为行、物品为列的二维矩阵。</p>
<p>大部分情况下单个用户只会和少部分物品接触，所以该矩阵很稀疏。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tom, Star Wars, 5</span><br><span class="line">Jane, Titanic,　4</span><br><span class="line">Bill, Batman,　3</span><br><span class="line">Jane, Star Wars, 2</span><br><span class="line">Bill, Titanic, 3</span><br></pre></td></tr></table></figure>
<p><img src="/images/spark/spark-ml-4.1.png" alt="一个简单的电影评级矩阵"></p>
<p>用户-物品 矩阵的维度为 U × I</p>
<p><img src="/images/spark/spark-ml-4.2.jpg" alt="图4-2 一个稀疏的评级矩阵"> </p>
<p><strong>为了降维</strong></p>
<ul>
<li>表示用户的 U × k 维矩阵</li>
<li>表征物品的 I × k 维矩阵</li>
</ul>
<blockquote>
<p>这两个矩阵也称作因子矩阵, 乘积是原始评级矩阵的一个近似.</p>
<p>原始评级矩阵通常很稀疏，但因子矩阵却是稠密的</p>
</blockquote>
<p><img src="/images/spark/spark-ml-4.3.jpg" alt="图4-3 用户因子矩阵和物品因子矩阵"></p>
<blockquote>
<p>因子可能表示了某些含义，比如对电影的某个导演、种类、风格或某些演员的偏好。</p>
</blockquote>
<p><code>要计算给定用户对某个物品的预计评级</code> = 行（用户因子向量） 与 列（物品因子向量），两者点积</p>
<p><img src="/images/spark/spark-ml-4.4.jpg" alt="图4-4 用用户因子矩阵和物品因子矩阵计算推荐"></p>
<blockquote>
<p>物品之间相似度的计算，转换为对两物品因子向量之间相似度的计算</p>
</blockquote>
<p><img src="/images/spark/spark-ml-4.5.jpg" alt="图4-5 用物品因子矩阵计算相似度"></p>
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>因子分解类模型建立，求解容易</td>
<td>物品或是用户的因子向量可能达到数以百万计。在存储和计算能力有挑战。</td>
</tr>
</tbody>
</table>
<h4 id="1-3-2-隐式矩阵分解"><a href="#1-3-2-隐式矩阵分解" class="headerlink" title="1.3.2. 隐式矩阵分解"></a>1.3.2. 隐式矩阵分解</h4><p>隐含在用户与物品的交互之中。二元数据（比如用户是否观看了某个电影或是否购买了某个商品）和计数数据（比如用户观看某电影的次数）便是这类数据。</p>
<p>MLlib 处理隐式数据：</p>
<ol>
<li>一个二元偏好矩阵 P </li>
<li>一个信心权重矩阵 C</li>
</ol>
<p><img src="/images/spark/spark-ml-4.6.png" alt="图4-6 用物品因子矩阵计算相似度"></p>
<p>隐式模型仍然会创建一个用户因子矩阵和一个物品因子矩阵。但是，模型所求解的是偏好矩阵而非评级矩阵的近似。</p>
<p><strong>3. 最小二乘法</strong></p>
<p>最小二乘法（Alternating Least Squares，ALS）是一种求解矩阵分解问题的最优化方法。且相对容易并行化。</p>
<blockquote>
<p>ALS的实现原理是迭代式求解一系列最小二乘回归问题。在每一次迭代时，固定用户因子矩阵或是物品因子矩阵中的一个，然后用固定的这个矩阵以及评级数据来更新另一个矩阵。之后，被更新的矩阵被固定住，再更新另外一个矩阵。如此迭代，直到模型收敛（或是迭代了预设好的次数）。</p>
</blockquote>
<h2 id="2-提取有效特征"><a href="#2-提取有效特征" class="headerlink" title="2. 提取有效特征"></a>2. 提取有效特征</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt;./bin/spark-shell –-driver-memory <span class="number">2</span>g</span><br><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"/Users/hp/ghome/github/Recommendation/spark-ml/ml-100k/u.data"</span>)</span><br><span class="line">rawData.first()</span><br><span class="line"><span class="keyword">val</span> rawRatings = rawData.map(_.split(<span class="string">"\t"</span>).take(<span class="number">3</span>))</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.recommendation.<span class="type">ALS</span></span><br><span class="line"></span><br><span class="line"><span class="type">ALS</span>.</span><br><span class="line">asInstanceOf    isInstanceOf   main   toString        train           trainImplicit</span><br><span class="line"><span class="type">ALS</span>.train</span><br></pre></td></tr></table></figure>
<p>ALS模型需要一个由Rating记录构成的RDD，而Rating类则是对用户ID、影片ID（这里是通称product）和实际星级这些参数的封装。<br>我们可以调用map方法将原来的各ID和星级的数组转换为对应的Rating对象，从而创建所需的评级数据集。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.mllib.recommendation.<span class="type">Rating</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.recommendation.<span class="type">Rating</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ratings = rawRatings.map &#123; <span class="keyword">case</span> <span class="type">Array</span>(user, movie, rating) =&gt;</span><br><span class="line">     | <span class="type">Rating</span>(user.toInt, movie.toInt, rating.toDouble) &#125;</span><br><span class="line">ratings: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.mllib.recommendation.<span class="type">Rating</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; ratings.first()</span><br><span class="line">res3: org.apache.spark.mllib.recommendation.<span class="type">Rating</span> = <span class="type">Rating</span>(<span class="number">196</span>,<span class="number">242</span>,<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; ratings.take(<span class="number">10</span>)</span><br><span class="line">res4: <span class="type">Array</span>[org.apache.spark.mllib.recommendation.<span class="type">Rating</span>] = <span class="type">Array</span>(<span class="type">Rating</span>(<span class="number">196</span>,<span class="number">242</span>,<span class="number">3.0</span>), <span class="type">Rating</span>(<span class="number">186</span>,<span class="number">302</span>,<span class="number">3.0</span>), <span class="type">Rating</span>(<span class="number">22</span>,<span class="number">377</span>,<span class="number">1.0</span>), <span class="type">Rating</span>(<span class="number">244</span>,<span class="number">51</span>,<span class="number">2.0</span>), <span class="type">Rating</span>(<span class="number">166</span>,<span class="number">346</span>,<span class="number">1.0</span>), <span class="type">Rating</span>(<span class="number">298</span>,<span class="number">474</span>,<span class="number">4.0</span>), <span class="type">Rating</span>(<span class="number">115</span>,<span class="number">265</span>,<span class="number">2.0</span>), <span class="type">Rating</span>(<span class="number">253</span>,<span class="number">465</span>,<span class="number">5.0</span>), <span class="type">Rating</span>(<span class="number">305</span>,<span class="number">451</span>,<span class="number">3.0</span>), <span class="type">Rating</span>(<span class="number">6</span>,<span class="number">86</span>,<span class="number">3.0</span>))</span><br></pre></td></tr></table></figure>
<h2 id="3-训练推荐模型"><a href="#3-训练推荐模型" class="headerlink" title="3. 训练推荐模型"></a>3. 训练推荐模型</h2><p>从原始数据提取出这些简单特征后，便可训练模型。MLlib已实现模型训练的细节，这不需要我们担心。我们只需提供上述指定类型的新RDD以及其他所需参数来作为训练的输入即可。</p>
<h3 id="3-1-Movie-100k-train-model"><a href="#3-1-Movie-100k-train-model" class="headerlink" title="3.1 Movie-100k train model"></a>3.1 Movie-100k train model</h3><p>现在开始训练模型了，所需的其他参数有以下几个</p>
<ul>
<li><code>rank</code>：对应ALS模型中的因子个数，也就是在低阶近似矩阵中的隐含特征个数。因子个数一般越多越好。但它也会直接影响模型训练和保存时所需的内存开销，尤其是在用户和物品很多的时候。因此实践中该参数常作为训练效果与系统开销之间的调节参数。通常，其合理取值为10到200。</li>
<li><code>iterations</code>：对应运行时的迭代次数。ALS能确保每次迭代都能降低评级矩阵的重建误差，但一般经少数次迭代后ALS模型便已能收敛为一个比较合理的好模型。这样，大部分情况下都没必要迭代太多次（10次左右一般就挺好）。</li>
<li><code>lambda</code>：该参数控制模型的正则化过程，从而控制模型的过拟合情况。其值越高，正则化越严厉。该参数的赋值与实际数据的大小、特征和稀疏程度有关。和其他的机器学习模型一样，正则参数应该通过用非样本的测试数据进行交叉验证来调整。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> model = <span class="type">ALS</span>.train(ratings, <span class="number">50</span>, <span class="number">10</span>, <span class="number">0.01</span>)</span><br><span class="line">model: org.apache.spark.mllib.recommendation.<span class="type">MatrixFactorizationModel</span> = org.apache.spark.mllib.recommendation.<span class="type">MatrixFactorizationModel</span>@<span class="number">2e835760</span></span><br><span class="line"></span><br><span class="line">scala&gt; model.userFeatures</span><br><span class="line">res5: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Array</span>[<span class="type">Double</span>])] = users <span class="type">MapPartitionsRDD</span>[<span class="number">209</span>] at mapValues at <span class="type">ALS</span>.scala:<span class="number">255</span></span><br><span class="line"></span><br><span class="line">scala&gt; model.userFeatures.count</span><br><span class="line">res6: <span class="type">Long</span> = <span class="number">943</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>MatrixFactorizationModel</code> 对象将 用户因子和物品因子分别保存在一个 <code>(id,factor)</code> 对类型的RDD中。<br>它们分别称作<code>userFeatures</code> 和 <code>productFeatures</code>。</p>
</blockquote>
<h3 id="3-2-隐式反馈数据训练模型"><a href="#3-2-隐式反馈数据训练模型" class="headerlink" title="3.2 隐式反馈数据训练模型"></a>3.2 隐式反馈数据训练模型</h3><p>MLlib中标准的矩阵分解模型用于显式评级数据的处理。若要处理隐式数据，则可使用<code>trainImplicit</code>函数。其调用方式和标准的<code>train</code>模式类似，但多了一个可设置的<code>alpha</code>参数（也是一个正则化参数，<code>lambda</code>应通过测试和交叉验证法来设置）。</p>
<p>alpha参数指定了信心权重所应达到的基准线。该值越高则所训练出的模型越认为用户与他所没评级过的电影之间没有相关性。</p>
<h2 id="4-使用推荐模型"><a href="#4-使用推荐模型" class="headerlink" title="4. 使用推荐模型"></a>4. 使用推荐模型</h2><p>预测通常有两种：为某个用户推荐物品，或找出与某个物品相关或相似的其他物品。</p>
<h3 id="4-1-用户推荐"><a href="#4-1-用户推荐" class="headerlink" title="4.1 用户推荐"></a>4.1 用户推荐</h3><p>通过模型求出用户可能喜好程度最高的前K个商品。</p>
<ol>
<li>基于<strong>用户的模型</strong>，则会利用相似用户的评级来计算对某个用户的推荐。</li>
<li>基于<strong>物品的模型</strong>，则会依靠用户接触过的物品与候选物品之间的相似度来获得推荐。</li>
</ol>
<p>利用矩阵分解方法时，是直接对评级数据进行建模，所以预计得分可视作相应用户因子向量和物品因子向量的点积。</p>
<p><strong>1. 从MovieLens 100k数据集生成电影推荐</strong></p>
<p>MLlib的推荐模型基于矩阵分解，因此可用模型所求得的因子矩阵来计算用户对物品的预计评级。下面只针对利用MovieLens中显式数据做推荐的情形，使用隐式模型时的方法与之相同。</p>
<p><code>MatrixFactorizationModel</code>类 提供了一个<code>predict</code>函数，以方便地计算给定用户对给定物品的预期得分：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> predictedRating = model.predict(<span class="number">789</span>, <span class="number">123</span>)</span><br><span class="line"><span class="number">16</span>/<span class="number">05</span>/<span class="number">04</span> <span class="number">16</span>:<span class="number">13</span>:<span class="number">08</span> <span class="type">WARN</span> <span class="type">BLAS</span>: <span class="type">Failed</span> to load implementation from: com.github.fommil.netlib.<span class="type">NativeSystemBLAS</span></span><br><span class="line"><span class="number">16</span>/<span class="number">05</span>/<span class="number">04</span> <span class="number">16</span>:<span class="number">13</span>:<span class="number">08</span> <span class="type">WARN</span> <span class="type">BLAS</span>: <span class="type">Failed</span> to load implementation from: com.github.fommil.netlib.<span class="type">NativeRefBLAS</span></span><br><span class="line">predictedRating: <span class="type">Double</span> = <span class="number">1.8390368814083764</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> predictedRating = model.predict(<span class="number">789</span>, <span class="number">123</span>)</span><br><span class="line">predictedRating: <span class="type">Double</span> = <span class="number">1.8390368814083764</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> userId = <span class="number">789</span></span><br><span class="line">userId: <span class="type">Int</span> = <span class="number">789</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> <span class="type">K</span> = <span class="number">10</span></span><br><span class="line"><span class="type">K</span>: <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> topKRecs = model.recommendProducts(userId, <span class="type">K</span>)</span><br><span class="line">topKRecs: <span class="type">Array</span>[org.apache.spark.mllib.recommendation.<span class="type">Rating</span>] = <span class="type">Array</span>(<span class="type">Rating</span>(<span class="number">789</span>,<span class="number">180</span>,<span class="number">5.352418839062572</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">887</span>,<span class="number">5.289455638310055</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">484</span>,<span class="number">5.0301818688410025</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">475</span>,<span class="number">5.011219778604191</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">150</span>,<span class="number">5.003965038415291</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">663</span>,<span class="number">4.991126084946501</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">56</span>,<span class="number">4.974685008959871</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">48</span>,<span class="number">4.965402351329832</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">9</span>,<span class="number">4.963265626841469</span>), <span class="type">Rating</span>(<span class="number">789</span>,<span class="number">127</span>,<span class="number">4.963069165947614</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; println(topKRecs.mkString(<span class="string">"\n"</span>))</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">180</span>,<span class="number">5.352418839062572</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">887</span>,<span class="number">5.289455638310055</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">484</span>,<span class="number">5.0301818688410025</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">475</span>,<span class="number">5.011219778604191</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">150</span>,<span class="number">5.003965038415291</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">663</span>,<span class="number">4.991126084946501</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">56</span>,<span class="number">4.974685008959871</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">48</span>,<span class="number">4.965402351329832</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">9</span>,<span class="number">4.963265626841469</span>)</span><br><span class="line"><span class="type">Rating</span>(<span class="number">789</span>,<span class="number">127</span>,<span class="number">4.963069165947614</span>)</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Recommendation System What ?]]></title>
      <url>http://sggo.me/2016/11/22/ml/recommendation-what/</url>
      <content type="html"><![CDATA[<p>随着互联网的深入发展，越来越多的信息在互联网上传播，产生了严重的信息过载。如果不采用一定的手段，用户很难从如此多的信息流中找到对自己有价值的信息。</p>
<a id="more"></a>
<h2 id="1-推荐系统"><a href="#1-推荐系统" class="headerlink" title="1. 推荐系统 ?"></a>1. 推荐系统 ?</h2><p>为解决信息过载的问题，所以出现了如 yahoo 分类目录 和 google 搜索引擎 等公司</p>
<table>
<thead>
<tr>
<th style="text-align:center">solution</th>
<th style="text-align:center">type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">搜索引擎</td>
<td style="text-align:center">用户具有明确需求</td>
</tr>
<tr>
<td style="text-align:center">推荐系统</td>
<td style="text-align:center">用户不具明确需求</td>
</tr>
</tbody>
</table>
<h3 id="1-1-搜索引擎"><a href="#1-1-搜索引擎" class="headerlink" title="1.1 搜索引擎"></a>1.1 搜索引擎</h3><table>
<thead>
<tr>
<th style="text-align:center">Shop</th>
<th style="text-align:center">用户买花生流程</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">便利店 &nbsp;</td>
<td style="text-align:center">转一圈找到花生米,然后比较几个牌子的口碑或者价格找到自己喜欢的牌子，掏钱付款</td>
</tr>
<tr>
<td style="text-align:center">沃尔玛</td>
<td style="text-align:center">按分类指示牌到食品楼层，按指示牌到卖干果的货架，后仔细寻找你需要的花生米，找到后付款</td>
</tr>
<tr>
<td style="text-align:center">淘宝</td>
<td style="text-align:center">搜索框的东西里输入花生米3个字，然后你会看到一堆花生米，找到喜欢的牌子，付费，然后等待送货上门</td>
</tr>
</tbody>
</table>
<blockquote>
<p>用户具有明确需求 : 搜索引擎</p>
</blockquote>
<h3 id="1-2-推荐系统"><a href="#1-2-推荐系统" class="headerlink" title="1.2 推荐系统"></a>1.2 推荐系统</h3><p>推荐系统和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。推荐系统 的基本任务是联系<strong>用户</strong>和<strong>物品</strong>，解决信息过载的问题</p>
<p><img src="/images/recommendation/rsac-1-1.png" alt="rsac-1"></p>
<blockquote>
<p>用户不具明确需求 : 推荐系统</p>
</blockquote>
<h3 id="1-3-推荐系统分类"><a href="#1-3-推荐系统分类" class="headerlink" title="1.3 推荐系统分类"></a>1.3 推荐系统分类</h3><p>经典的分类方式:</p>
<ol>
<li>基于<code>Content</code>的推荐系统</li>
<li>基于<code>User</code>的推荐系统</li>
<li>基于<code>Item</code>的推荐系统</li>
<li>基于<code>Model</code>的推荐系统</li>
</ol>
<p><img src="/images/recommendation/rsac-1-2.png" alt="rsac-2"></p>
<p>按照算法也可分成 基于邻域的算法、基于图的算法、基于矩阵分解 或者 概率模型的算法</p>
<h2 id="2-个性化推荐系统应用"><a href="#2-个性化推荐系统应用" class="headerlink" title="2. 个性化推荐系统应用"></a>2. 个性化推荐系统应用</h2><table>
<thead>
<tr>
<th style="text-align:center">推荐系统的应用</th>
<th style="text-align:center">代表</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">电子商务</td>
<td style="text-align:center">Amazon、Taobao</td>
</tr>
<tr>
<td style="text-align:center">电影视频</td>
<td style="text-align:center">Youtube、Netflix</td>
</tr>
<tr>
<td style="text-align:center">个性化电台</td>
<td style="text-align:center">Pandora、豆瓣电台</td>
</tr>
<tr>
<td style="text-align:center">移动新闻</td>
<td style="text-align:center">Yahoo news、今日头条</td>
</tr>
<tr>
<td style="text-align:center">社交网络</td>
<td style="text-align:center">Facebook、Twitter</td>
</tr>
<tr>
<td style="text-align:center">基于位置的服务</td>
<td style="text-align:center">Maps nearby</td>
</tr>
<tr>
<td style="text-align:center">个性化AD</td>
<td style="text-align:center">Facebook、Sina、Ad百度联盟</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<h2 id="3-推荐系统评测"><a href="#3-推荐系统评测" class="headerlink" title="3. 推荐系统评测"></a>3. 推荐系统评测</h2><p><img src="/images/recommendation/rsac-1-3.png" alt="rsac-3"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Travel Note Thai]]></title>
      <url>http://sggo.me/2016/10/27/tools/health-life-thai/</url>
      <content type="html"><![CDATA[<p>提起泰王国，给我最深的印象就是可以让我感受到，到底什么是 <a href="https://zh.wikipedia.org/wiki/三字经" target="_blank" rel="external">真善美</a>。 <a id="more"></a>真是的 幸福国度、微笑国度，名不虚传。在我看来，那里的百姓是安居乐业，非常幸福。贫富差距不大，资源分配相对合理，小城市和<a href="https://zh.wikipedia.org/wiki/人再囧途之泰囧" target="_blank" rel="external">徐峥《泰囧》</a>清迈的大城市基础设施相差不大，感觉真的非常不错。我觉得去任何一个地方人文环境才是最重要的，其他的景色、景点是次要的，因为普通人都需要入世，需要沟通与交流。</p>
<p>我分别在 15年10月 和 16年9月 去泰国旅行，到过曼谷、巴蜀府、华欣、普吉岛 等地。非常感谢在那段异国他乡的时光里曾给予我帮助的朋友。</p>
<p><img src="/images/life/life-thai-01.jpg" width="800" height="550" img=""></p>
<h2 id="曼谷"><a href="#曼谷" class="headerlink" title="曼谷"></a>曼谷</h2><p>曼谷（泰文：กรุงเทพมหานคร；英文：Bangkok），是泰国首都和最大城市，东南亚第二大城市，为泰国政治、经济、贸易、交通、文化、科技、教育、宗教与各方面中心。</p>
<h2 id="芭堤雅"><a href="#芭堤雅" class="headerlink" title="芭堤雅"></a>芭堤雅</h2><p>芭提雅(Pattaya,又常被译为“芭堤雅”)，是中南半岛南端的泰国一处著名海景度假胜地。距离曼谷东南方154公里。</p>
<p>芭提雅近年来热度极高的海滩度假、旅游、养老圣地，享有“东方夏威夷”之誉，芭提雅具有最负盛名的人妖表演，缤纷无休的夜文化，吸引着全世界的游客。</p>
<blockquote>
<p>据说2018年轻轨修建完成，从曼谷到芭堤雅只需要20多分钟。</p>
</blockquote>
<h2 id="华欣"><a href="#华欣" class="headerlink" title="华欣"></a>华欣</h2><p>华欣（英语：Hua Hin，泰语：หัวหิน），泰国中部海滨小镇。距离泰国首都曼谷200多公里（北纬12°34′，东经99°57′），隶属于泰国巴蜀府。与芭堤雅隔岸相望，距离曼谷西南281公里，约3小时行程。它被称作是泰国最传统的海滨胜地，皇室贵族们每年都会到华欣住一段时间，当今泰皇就长期居住于此地的行宫。</p>
<h2 id="普吉岛"><a href="#普吉岛" class="headerlink" title="普吉岛"></a>普吉岛</h2><p>海滩日光浴 享慵懒慢时光<br>游海岛 潜入大海的心里去<br>泛舟泰国“小桂林”攀牙湾 </p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera Week 3 - Logistic Regression]]></title>
      <url>http://sggo.me/2016/10/24/ml/coursera-ng-w3-LR/</url>
      <content type="html"><![CDATA[<p>Logistic Regression</p>
<a id="more"></a>
<h2 id="1-Classification"><a href="#1-Classification" class="headerlink" title="1. Classification"></a>1. Classification</h2><blockquote>
<ol>
<li>Email: Spam / Not Spam</li>
<li>Online Transactions: Fraudulent (Yes/No)? </li>
<li>Tumor: Malignant / Benign ?  &emsp;&emsp;&emsp; [‘tju:mə(r)]  [mə’lɪgnənt] / [bɪ’naɪn]</li>
</ol>
<p>$ y \in {0, 1}$</p>
</blockquote>
<h2 id="2-Binary-Classification"><a href="#2-Binary-Classification" class="headerlink" title="2. Binary Classification"></a>2. Binary Classification</h2><p>Instead of our output vector y being a continuous range of values, it will only be 0 or 1.</p>
<p>y∈{0,1}</p>
<blockquote>
<p>Where 0 is usually taken as the “negative class” and 1 as the “positive class”, but you are free to assign any representation to it.</p>
</blockquote>
<p><strong>Hypothesis Representation</strong>  </p>
<blockquote>
<p>$$ 0 \leq h_\theta (x) \leq 1 $$<br>$$ \begin{align} &amp; h_\theta (x) =  g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}} \end{align} $$</p>
<p>“Sigmoid Function,” also called the “Logistic Function”:</p>
</blockquote>
<p><img src="/images/ml/coursera/ml-ng-w3-02.png" alt="Sigmoid Function"></p>
<blockquote>
<p>$h_θ$ will give us the probability that our output is 1. For example, $h_θ=0.7$ gives us the probability of 70% that our output is 1.</p>
</blockquote>
<p>$<br>\begin{align}<br>&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1<br>\end{align}<br>$</p>
<p>Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).</p>
<h2 id="3-Decision-Boundary"><a href="#3-Decision-Boundary" class="headerlink" title="3. Decision Boundary"></a>3. Decision Boundary</h2><p>$$<br>\begin{align}<br>&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \newline&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 \newline\end{align}<br>$$</p>
<table>
<thead>
<tr>
<th style="text-align:center"><font color="#0085a1"><strong>Logistic Function</strong></font></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$ \begin{align} &amp; g(z) \geq 0.5 \newline&amp; when \; z \geq 0 \end{align} $</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">$ \begin{align} z=0,  e^{0}=1 \Rightarrow  g(z)=1/2\newline z \to \infty, e^{-\infty} \to 0 \Rightarrow g(z)=1 \newline z \to -\infty, e^{\infty}\to \infty \Rightarrow g(z)=0 \end{align} $</td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center"><font color="#0085a1">g is $\theta^T X$, then that means</font>:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$ h_\theta(x) = g(\theta^T x) \geq 0.5 $ $ when \; \theta^T x \geq 0 $</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">$ \begin{align} &amp; \theta^T x \geq 0 \Rightarrow y = 1 \newline&amp; \theta^T x &lt; 0 \Rightarrow y = 0 \newline \end{align} $</td>
<td></td>
</tr>
</tbody>
</table>
<p>The decision boundary is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.</p>
<p><strong>Example:</strong></p>
<p>$<br>\begin{align}<br>&amp; \theta = \begin{bmatrix}5 \newline -1 \newline 0\end{bmatrix} \newline &amp; y = 1 \; if \; 5 + (-1) x_1 + 0 x_2 \geq 0 \newline &amp; 5 - x_1 \geq 0 \newline &amp; - x_1 \geq -5 \newline&amp; x_1 \leq 5 \newline \end{align}<br>$</p>
<p>In this case, our decision boundary is a straight vertical line placed on the graph where $x_1=5$, and everything to the left of that denotes $y = 1$, while everything to the right denotes $y = 0$.</p>
<p><strong>linear decision boundaries</strong></p>
<p><img src="/images/ml/coursera/ml-ng-w3-01.png" alt="linear decision boundaries"></p>
<p><strong>Non-linear decision boundaries</strong></p>
<p><img src="/images/ml/coursera/ml-ng-w3-03.png" alt="Non-linear decision boundaries"></p>
<h2 id="4-Cost-Function"><a href="#4-Cost-Function" class="headerlink" title="4. Cost Function"></a>4. Cost Function</h2><p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.</p>
<p><img src="/images/ml/coursera/ml-ng-w3-12.png" alt=" $h_\theta(x)$ Complex nonlinear function"></p>
<p><img src="/images/ml/coursera/ml-ng-w3-06.png" alt="Cost Function"></p>
<p><code>Instead, our cost function for logistic regression looks like</code>:</p>
<p>$<br>\begin{align}<br>&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}<br>\end{align}<br>$</p>
<p><img src="/images/ml/coursera/ml-ng-w3-04.png" alt="Cost Function"></p>
<p><img src="/images/ml/coursera/ml-ng-w3-05.png" alt="Cost Function"></p>
<p>$<br>\begin{align}<br>&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{  if  } h_\theta(x) = y \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{  if  } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{  if  } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline<br>\end{align}<br>$</p>
<p>If our correct answer ‘y’ is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.</p>
<p>If our correct answer ‘y’ is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.</p>
<h2 id="5-Cost-Function-amp-Gradient-Desc"><a href="#5-Cost-Function-amp-Gradient-Desc" class="headerlink" title="5. Cost Function &amp; Gradient Desc"></a>5. Cost Function &amp; Gradient Desc</h2><p><code>We can compress our cost function&#39;s two conditional cases into one case:</code></p>
<p>$ \mathrm{Cost}(h_\theta(x),y) = - y \cdot \log(h_\theta(x)) - (1 - y) \cdot \log(1 - h_\theta(x))$</p>
<p>We can fully write out our entire cost function as follows:</p>
<p>$<br>J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]<br>$</p>
<p>A <code>vectorized</code> implementation is:</p>
<p>$<br>\begin{align}<br>&amp; h = g(X\theta)\newline<br>&amp; J(\theta)  = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)<br>\end{align}<br>$</p>
<h3 id="5-1-Gradient-Descent"><a href="#5-1-Gradient-Descent" class="headerlink" title="5.1 Gradient Descent"></a>5.1 Gradient Descent</h3><p>Remember that the general form of gradient descent is:</p>
<p>$<br>\begin{align}<br>&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline &amp; \rbrace<br>\end{align}<br>$</p>
<p>We can work out the derivative part using calculus to get:</p>
<p>$<br>\begin{align}<br>&amp; Repeat \; \lbrace \newline<br>&amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace<br>\end{align}<br>$</p>
<blockquote>
<p><code>seen 5.2 detailed</code></p>
</blockquote>
<p><strong>A vectorized implementation is:</strong></p>
<p>$<br>\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})<br>$</p>
<h3 id="5-2-Partial-derivative-of-J-θ"><a href="#5-2-Partial-derivative-of-J-θ" class="headerlink" title="5.2 Partial derivative of J(θ)"></a>5.2 Partial derivative of J(θ)</h3><blockquote>
<p>partial [‘pɑːʃ(ə)l]<br>derivative [dɪ’rɪvətɪv]</p>
</blockquote>
<p>$<br>\begin{align}<br>\sigma(x)’&amp;=\left(\frac{1}{1+e^{-x}}\right)’=\frac{-(1+e^{-x})’}{(1+e^{-x})^2}=\frac{-1’-(e^{-x})’}{(1+e^{-x})^2}=\frac{0-(-x)’(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &amp;=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)=\sigma(x)(1 - \sigma(x))<br>\end{align}<br>$</p>
<p>Now we are ready to find out resulting partial derivative:</p>
<p>$<br>\begin{align}<br>\frac{\partial}{\partial \theta_j} J(\theta) &amp;= \frac{\partial}{\partial \theta_j} \frac{-1}{m}\sum_{i=1}^m \left [ y^{(i)} log (h_\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\theta(x^{(i)})) \right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} \frac{\partial}{\partial \theta_j} log (h_\theta(x^{(i)}))   + (1-y^{(i)}) \frac{\partial}{\partial \theta_j} log (1 - h_\theta(x^{(i)}))\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} h_\theta(x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - h_\theta(x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} \sigma(\theta^T x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - \sigma(\theta^T x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   + \frac{- (1-y^{(i)}) \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   - \frac{(1-y^{(i)}) h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\theta(x^{(i)}) x^{(i)}_j\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) - (1-y^{(i)}) h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} - y^{(i)} h_\theta(x^{(i)}) - h_\theta(x^{(i)}) + y^{(i)} h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} - h_\theta(x^{(i)}) \right ] x^{(i)}_j  \newline&amp;= \frac{1}{m}\sum_{i=1}^m \left [ h_\theta(x^{(i)}) - y^{(i)} \right ] x^{(i)}_j<br>\end{align}<br>$</p>
<p>The vectorized version;</p>
<p>$<br>\nabla J(\theta) = \frac{1}{m} \cdot  X^T \cdot \left(g\left(X\cdot\theta\right) - \vec{y}\right)<br>$</p>
<h2 id="6-Advanced-Optimization"><a href="#6-Advanced-Optimization" class="headerlink" title="6. Advanced Optimization"></a>6. Advanced Optimization</h2><blockquote>
<p>We can apply regularization to both linear regression and logistic regression. We will approach linear regression first.</p>
</blockquote>
<p><img src="/images/ml/coursera/ml-ng-w3-07.png" alt="Advanced Optimization"></p>
<p>We first need to provide a function that evaluates the following two functions for a given input value θ:</p>
<p>$<br>\begin{align}<br>&amp; J(\theta) \newline &amp; \dfrac{\partial}{\partial \theta_j}J(\theta)<br>\end{align}<br>$</p>
<p>We can write a single function that returns both of these…</p>
<h2 id="7-Multiclass-Classification"><a href="#7-Multiclass-Classification" class="headerlink" title="7. Multiclass Classification"></a>7. Multiclass Classification</h2><p><img src="/images/ml/coursera/ml-ng-w3-09.png" alt="Multiclass"></p>
<p>$<br>\begin{align}<br>&amp; y \in \lbrace0, 1 … n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline<br>\end{align}<br>$</p>
<p>Train a logistic regression classifier $h_\theta ^{(i)}(x)$ for each class $i$ to predict the probability that $y = i$ .</p>
<p>On a new input $x$, to make a prediction, pick the $i$ class that maximizes $\max_i( h_\theta ^{(i)}(x)$</p>
<p>@2017-02-10 review done.</p>
<h2 id="8-Regularization"><a href="#8-Regularization" class="headerlink" title="8. Regularization"></a>8. Regularization</h2><p><strong>The Problem of Overfitting</strong></p>
<p>Regularization is designed to address the problem of overfitting.</p>
<p><strong>High bias</strong> or <strong>underfitting</strong> is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or <strong>uses too few features</strong>. </p>
<p>eg. if we take $h_θ(x)=θ_0+θ_1 \cdot x_1+θ_2 \cdot x_2$ then we are making an initial assumption that a linear model will fit the training data well and will be able to generalize but that may not be the case.</p>
<p>At the other extreme, overfitting or high variance is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>
<p><img src="/images/ml/coursera/ml-ng-w3-10.png" alt="Regularization"></p>
<p>This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:</p>
<p>1) Reduce the number of features:</p>
<ul>
<li><p>a) Manually select which features to keep.</p>
</li>
<li><p>b) Use a model selection algorithm .</p>
</li>
</ul>
<p>2) Regularization</p>
<ul>
<li>Keep all the features, but reduce the parameters $θ_j$.</li>
</ul>
<p>Regularization works well when we have a lot of slightly useful features.</p>
<blockquote>
<p>to address 提出，去解决</p>
</blockquote>
<h2 id="9-Regularization-Cost-Function"><a href="#9-Regularization-Cost-Function" class="headerlink" title="9. Regularization Cost Function"></a>9. Regularization Cost Function</h2><p>If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.</p>
<p>Say we wanted to make the following function more quadratic [kwɒ’drætɪk]:</p>
<p>$<br>\theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4<br>$</p>
<p>We’ll want to eliminate the influence of $\theta_3x^3$ and $\theta_4x^4$. Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our cost function:</p>
<p><strong>$<br>min_\theta\ \dfrac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + 1000 \cdot \theta_3^2 + 1000\cdot\theta_4^2<br>$</strong></p>
<p>We’ve added two extra terms at the end to inflate the cost of $\theta_3$ and $\theta_4$. Now, in order for the cost function to get close to zero, we will have to reduce the values of $\theta_3$ and $\theta_4$ to near zero. This will in turn greatly reduce the values of $\theta_3x^3$ and $\theta_4x^4$ in our hypothesis function.</p>
<p>We could also regularize all of our theta parameters in a single summation:</p>
<p>$<br>min_\theta\ \dfrac{1}{2m}\ \left[ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2 \right]<br>$</p>
<p>The λ, or lambda, is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated. </p>
<blockquote>
<p>You can visualize the effect of regularization in this interactive plot : <a href="https://www.desmos.com/calculator/1hexc8ntqp" target="_blank" rel="external">https://www.desmos.com/calculator/1hexc8ntqp</a></p>
</blockquote>
<p>Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting.</p>
<blockquote>
<p>我们可以平滑我们的假设函数的输出，以减少过度拟合</p>
</blockquote>
<h2 id="10-Regularized-Linear"><a href="#10-Regularized-Linear" class="headerlink" title="10. Regularized Linear"></a>10. Regularized Linear</h2><h3 id="10-1-Gradient-Descent"><a href="#10-1-Gradient-Descent" class="headerlink" title="10.1 Gradient Descent"></a>10.1 Gradient Descent</h3><p>$<br>\begin{align}<br>&amp; \text{Repeat}\ \lbrace \newline<br>&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline<br>&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2…n\rbrace\newline<br>&amp; \rbrace<br>\end{align}<br>$</p>
<p>The term $\frac{\lambda}{m}\theta_j$ performs our regularization.</p>
<p><strong>$<br>\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$</strong></p>
<p>The first term in the above equation, $1 - \alpha\frac{\lambda}{m}$ will always be less than 1. Intuitively you can see it as reducing the value of $\theta_j$ by some amount on every update.</p>
<h3 id="10-2-Normal-Equation"><a href="#10-2-Normal-Equation" class="headerlink" title="10.2 Normal Equation"></a>10.2 Normal Equation</h3><p>Use less, temporarily ignored</p>
<h2 id="11-Regularized-Logistic"><a href="#11-Regularized-Logistic" class="headerlink" title="11. Regularized Logistic"></a>11. Regularized Logistic</h2><p><strong>Cost Function</strong></p>
<p>$<br>J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)})) \large]<br>$</p>
<p>We can regularize this equation by adding a term to the end:</p>
<p>$<br>J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2<br>$</p>
<p><strong>Gradient Descent</strong></p>
<p>Just like with linear regression, we will want to separately update $\theta_0$ and the rest of the parameters because we do not want to regularize $\theta_0$.</p>
<p>$<br>\begin{align}<br>&amp; \text{Repeat}\ \lbrace \newline&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2…n\rbrace\newline&amp; \rbrace\end{align}<br>$</p>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><p><a href="https://www.kaggle.com/c/titanic" target="_blank" rel="external">Kaggle_Titanic</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera Week 2 - Octave Tutorial By NG]]></title>
      <url>http://sggo.me/2016/10/21/ml/coursera-ng-w2-03/</url>
      <content type="html"><![CDATA[<p>ML:Octave Tutorial</p>
<a id="more"></a>
<h2 id="1-Basic-Operations"><a href="#1-Basic-Operations" class="headerlink" title="1. Basic Operations"></a>1. Basic Operations</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% Change Octave prompt  </span></span><br><span class="line">PS1(<span class="string">'&gt;&gt; '</span>);</span><br><span class="line"><span class="comment">%% Change working directory in windows example:</span></span><br><span class="line">cd <span class="string">'c:/path/to/desired/directory name'</span></span><br><span class="line"><span class="comment">%% Note that it uses normal slashes and does not use escape characters for the empty spaces.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% elementary operations</span></span><br><span class="line"><span class="number">5</span>+<span class="number">6</span></span><br><span class="line"><span class="number">3</span><span class="number">-2</span></span><br><span class="line"><span class="number">5</span>*<span class="number">8</span></span><br><span class="line"><span class="number">1</span>/<span class="number">2</span></span><br><span class="line"><span class="number">2</span>^<span class="number">6</span></span><br><span class="line"><span class="number">1</span> == <span class="number">2</span> <span class="comment">% false</span></span><br><span class="line"><span class="number">1</span> ~= <span class="number">2</span> <span class="comment">% true.  note, not "!="</span></span><br><span class="line"><span class="number">1</span> &amp;&amp; <span class="number">0</span></span><br><span class="line"><span class="number">1</span> || <span class="number">0</span></span><br><span class="line">xor(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% variable assignment</span></span><br><span class="line">a = <span class="number">3</span>; <span class="comment">% semicolon suppresses output</span></span><br><span class="line">b = <span class="string">'hi'</span>;</span><br><span class="line">c = <span class="number">3</span>&gt;=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Displaying them:</span></span><br><span class="line">a = <span class="built_in">pi</span></span><br><span class="line"><span class="built_in">disp</span>(a)</span><br><span class="line"><span class="built_in">disp</span>(sprintf(<span class="string">'2 decimals: %0.2f'</span>, a))</span><br><span class="line"><span class="built_in">disp</span>(sprintf(<span class="string">'6 decimals: %0.6f'</span>, a))</span><br><span class="line">format long</span><br><span class="line">a</span><br><span class="line">format short</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%%  vectors and matrices</span></span><br><span class="line">A = [<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">v = [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line">v = [<span class="number">1</span>; <span class="number">2</span>; <span class="number">3</span>]</span><br><span class="line">v = <span class="number">1</span>:<span class="number">0.1</span>:<span class="number">2</span>   <span class="comment">% from 1 to 2, with stepsize of 0.1. Useful for plot axes</span></span><br><span class="line">v = <span class="number">1</span>:<span class="number">6</span>       <span class="comment">% from 1 to 6, assumes stepsize of 1 (row vector)</span></span><br><span class="line"></span><br><span class="line">C = <span class="number">2</span>*<span class="built_in">ones</span>(<span class="number">2</span>,<span class="number">3</span>) <span class="comment">% same as C = [2 2 2; 2 2 2]</span></span><br><span class="line">w = <span class="built_in">ones</span>(<span class="number">1</span>,<span class="number">3</span>)   <span class="comment">% 1x3 vector of ones</span></span><br><span class="line">w = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">w = <span class="built_in">rand</span>(<span class="number">1</span>,<span class="number">3</span>) <span class="comment">% drawn from a uniform distribution </span></span><br><span class="line">w = <span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">3</span>)<span class="comment">% drawn from a normal distribution (mean=0, var=1)</span></span><br><span class="line">w = <span class="number">-6</span> + <span class="built_in">sqrt</span>(<span class="number">10</span>)*(<span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">10000</span>));  <span class="comment">% (mean = -6, var = 10) - note: add the semicolon</span></span><br><span class="line">hist(w)    <span class="comment">% plot histogram using 10 bins (default)</span></span><br><span class="line">hist(w,<span class="number">50</span>) <span class="comment">% plot histogram using 50 bins</span></span><br><span class="line"><span class="comment">% note: if hist() crashes, try "graphics_toolkit('gnu_plot')" </span></span><br><span class="line"></span><br><span class="line">I = <span class="built_in">eye</span>(<span class="number">4</span>)   <span class="comment">% 4x4 identity matrix</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% help function</span></span><br><span class="line">help <span class="built_in">eye</span></span><br><span class="line">help <span class="built_in">rand</span></span><br><span class="line">help help</span><br></pre></td></tr></table></figure>
<h2 id="2-Moving-Data-Around"><a href="#2-Moving-Data-Around" class="headerlink" title="2. Moving Data Around"></a>2. Moving Data Around</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% dimensions</span></span><br><span class="line">sz = <span class="built_in">size</span>(A) <span class="comment">% 1x2 matrix: [(number of rows) (number of columns)]</span></span><br><span class="line"><span class="built_in">size</span>(A,<span class="number">1</span>) <span class="comment">% number of rows</span></span><br><span class="line"><span class="built_in">size</span>(A,<span class="number">2</span>) <span class="comment">% number of cols</span></span><br><span class="line"><span class="built_in">length</span>(v) <span class="comment">% size of longest dimension</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% loading data</span></span><br><span class="line">pwd   <span class="comment">% show current directory (current path)</span></span><br><span class="line">cd <span class="string">'C:\Users\ang\Octave files'</span>  <span class="comment">% change directory </span></span><br><span class="line">ls    <span class="comment">% list files in current directory </span></span><br><span class="line">load q1y.dat   <span class="comment">% alternatively, load('q1y.dat')</span></span><br><span class="line">load q1x.dat</span><br><span class="line">who   <span class="comment">% list variables in workspace</span></span><br><span class="line">whos  <span class="comment">% list variables in workspace (detailed view) </span></span><br><span class="line">clear q1y      <span class="comment">% clear command without any args clears all vars</span></span><br><span class="line">v = q1x(<span class="number">1</span>:<span class="number">10</span>); <span class="comment">% first 10 elements of q1x (counts down the columns)</span></span><br><span class="line">save hello.mat v;  <span class="comment">% save variable v into file hello.mat</span></span><br><span class="line">save hello.txt v -ascii; <span class="comment">% save as ascii</span></span><br><span class="line"><span class="comment">% fopen, fread, fprintf, fscanf also work  [[not needed in class]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% indexing</span></span><br><span class="line">A(<span class="number">3</span>,<span class="number">2</span>)  <span class="comment">% indexing is (row,col)</span></span><br><span class="line">A(<span class="number">2</span>,:)  <span class="comment">% get the 2nd row. </span></span><br><span class="line">        <span class="comment">% ":" means every element along that dimension</span></span><br><span class="line">A(:,<span class="number">2</span>)  <span class="comment">% get the 2nd col</span></span><br><span class="line">A([<span class="number">1</span> <span class="number">3</span>],:) <span class="comment">% print all  the elements of rows 1 and 3</span></span><br><span class="line"></span><br><span class="line">A(:,<span class="number">2</span>) = [<span class="number">10</span>; <span class="number">11</span>; <span class="number">12</span>]     <span class="comment">% change second column</span></span><br><span class="line">A = [A, [<span class="number">100</span>; <span class="number">101</span>; <span class="number">102</span>]]; <span class="comment">% append column vec</span></span><br><span class="line">A(:) <span class="comment">% Select all elements as a column vector.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Putting data together </span></span><br><span class="line">A = [<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</span><br><span class="line">B = [<span class="number">11</span> <span class="number">12</span>; <span class="number">13</span> <span class="number">14</span>; <span class="number">15</span> <span class="number">16</span>] <span class="comment">% same dims as A</span></span><br><span class="line">C = [A B]  <span class="comment">% concatenating A and B matrices side by side</span></span><br><span class="line">C = [A, B] <span class="comment">% concatenating A and B matrices side by side</span></span><br><span class="line">C = [A; B] <span class="comment">% Concatenating A and B top and bottom</span></span><br></pre></td></tr></table></figure>
<h2 id="3-Computing-on-Data"><a href="#3-Computing-on-Data" class="headerlink" title="3. Computing on Data"></a>3. Computing on Data</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% initialize variables</span></span><br><span class="line">A = [<span class="number">1</span> <span class="number">2</span>;<span class="number">3</span> <span class="number">4</span>;<span class="number">5</span> <span class="number">6</span>]</span><br><span class="line">B = [<span class="number">11</span> <span class="number">12</span>;<span class="number">13</span> <span class="number">14</span>;<span class="number">15</span> <span class="number">16</span>]</span><br><span class="line">C = [<span class="number">1</span> <span class="number">1</span>;<span class="number">2</span> <span class="number">2</span>]</span><br><span class="line">v = [<span class="number">1</span>;<span class="number">2</span>;<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">%% matrix operations</span></span><br><span class="line">A * C  <span class="comment">% matrix multiplication</span></span><br><span class="line">A .* B <span class="comment">% element-wise multiplication</span></span><br><span class="line"><span class="comment">% A .* C  or A * B gives error - wrong dimensions</span></span><br><span class="line">A .^ <span class="number">2</span> <span class="comment">% element-wise square of each element in A</span></span><br><span class="line"><span class="number">1.</span>/v   <span class="comment">% element-wise reciprocal</span></span><br><span class="line"><span class="built_in">log</span>(v)  <span class="comment">% functions like this operate element-wise on vecs or matrices </span></span><br><span class="line"><span class="built_in">exp</span>(v)</span><br><span class="line"><span class="built_in">abs</span>(v)</span><br><span class="line"></span><br><span class="line">-v  <span class="comment">% -1*v</span></span><br><span class="line"></span><br><span class="line">v + <span class="built_in">ones</span>(<span class="built_in">length</span>(v), <span class="number">1</span>)  </span><br><span class="line"><span class="comment">% v + 1  % same</span></span><br><span class="line"></span><br><span class="line">A'  <span class="comment">% matrix transpose</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% misc useful functions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% max  (or min)</span></span><br><span class="line">a = [<span class="number">1</span> <span class="number">15</span> <span class="number">2</span> <span class="number">0.5</span>]</span><br><span class="line">val = max(a)</span><br><span class="line">[val,ind] = max(a) <span class="comment">% val -  maximum element of the vector a and index - index value where maximum occur</span></span><br><span class="line">val = max(A) <span class="comment">% if A is matrix, returns max from each column</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% compare values in a matrix &amp; find</span></span><br><span class="line">a &lt; <span class="number">3</span> <span class="comment">% checks which values in a are less than 3</span></span><br><span class="line"><span class="built_in">find</span>(a &lt; <span class="number">3</span>) <span class="comment">% gives location of elements less than 3</span></span><br><span class="line">A = <span class="built_in">magic</span>(<span class="number">3</span>) <span class="comment">% generates a magic matrix - not much used in ML algorithms</span></span><br><span class="line">[r,c] = <span class="built_in">find</span>(A&gt;=<span class="number">7</span>)  <span class="comment">% row, column indices for values matching comparison</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% sum, prod</span></span><br><span class="line">sum(a)</span><br><span class="line">prod(a)</span><br><span class="line"><span class="built_in">floor</span>(a) <span class="comment">% or ceil(a)</span></span><br><span class="line">max(<span class="built_in">rand</span>(<span class="number">3</span>),<span class="built_in">rand</span>(<span class="number">3</span>))</span><br><span class="line">max(A,[],<span class="number">1</span>) -  maximum along columns(defaults to columns - max(A,[]))</span><br><span class="line">max(A,[],<span class="number">2</span>) - maximum along rows</span><br><span class="line">A = <span class="built_in">magic</span>(<span class="number">9</span>)</span><br><span class="line">sum(A,<span class="number">1</span>)</span><br><span class="line">sum(A,<span class="number">2</span>)</span><br><span class="line">sum(sum( A .* <span class="built_in">eye</span>(<span class="number">9</span>) ))</span><br><span class="line">sum(sum( A .* <span class="built_in">flipud</span>(<span class="built_in">eye</span>(<span class="number">9</span>)) ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">% Matrix inverse (pseudo-inverse)</span></span><br><span class="line">pinv(A)        <span class="comment">% inv(A'*A)*A'</span></span><br></pre></td></tr></table></figure>
<h2 id="4-Plotting-Data"><a href="#4-Plotting-Data" class="headerlink" title="4. Plotting Data"></a>4. Plotting Data</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; t=[<span class="number">0</span>:<span class="number">0.01</span>:<span class="number">0.98</span>];</span><br><span class="line">&gt;&gt; t</span><br><span class="line">t =</span><br><span class="line"></span><br><span class="line"> Columns <span class="number">1</span> through <span class="number">10</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.00000</span>   <span class="number">0.01000</span>   <span class="number">0.02000</span>   <span class="number">0.03000</span>   <span class="number">0.04000</span>   <span class="number">0.05000</span>   <span class="number">0.06000</span>   <span class="number">0.07000</span>   <span class="number">0.08000</span>   <span class="number">0.09000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">11</span> through <span class="number">20</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.10000</span>   <span class="number">0.11000</span>   <span class="number">0.12000</span>   <span class="number">0.13000</span>   <span class="number">0.14000</span>   <span class="number">0.15000</span>   <span class="number">0.16000</span>   <span class="number">0.17000</span>   <span class="number">0.18000</span>   <span class="number">0.19000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">21</span> through <span class="number">30</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.20000</span>   <span class="number">0.21000</span>   <span class="number">0.22000</span>   <span class="number">0.23000</span>   <span class="number">0.24000</span>   <span class="number">0.25000</span>   <span class="number">0.26000</span>   <span class="number">0.27000</span>   <span class="number">0.28000</span>   <span class="number">0.29000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">31</span> through <span class="number">40</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.30000</span>   <span class="number">0.31000</span>   <span class="number">0.32000</span>   <span class="number">0.33000</span>   <span class="number">0.34000</span>   <span class="number">0.35000</span>   <span class="number">0.36000</span>   <span class="number">0.37000</span>   <span class="number">0.38000</span>   <span class="number">0.39000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">41</span> through <span class="number">50</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.40000</span>   <span class="number">0.41000</span>   <span class="number">0.42000</span>   <span class="number">0.43000</span>   <span class="number">0.44000</span>   <span class="number">0.45000</span>   <span class="number">0.46000</span>   <span class="number">0.47000</span>   <span class="number">0.48000</span>   <span class="number">0.49000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">51</span> through <span class="number">60</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.50000</span>   <span class="number">0.51000</span>   <span class="number">0.52000</span>   <span class="number">0.53000</span>   <span class="number">0.54000</span>   <span class="number">0.55000</span>   <span class="number">0.56000</span>   <span class="number">0.57000</span>   <span class="number">0.58000</span>   <span class="number">0.59000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">61</span> through <span class="number">70</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.60000</span>   <span class="number">0.61000</span>   <span class="number">0.62000</span>   <span class="number">0.63000</span>   <span class="number">0.64000</span>   <span class="number">0.65000</span>   <span class="number">0.66000</span>   <span class="number">0.67000</span>   <span class="number">0.68000</span>   <span class="number">0.69000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">71</span> through <span class="number">80</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.70000</span>   <span class="number">0.71000</span>   <span class="number">0.72000</span>   <span class="number">0.73000</span>   <span class="number">0.74000</span>   <span class="number">0.75000</span>   <span class="number">0.76000</span>   <span class="number">0.77000</span>   <span class="number">0.78000</span>   <span class="number">0.79000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">81</span> through <span class="number">90</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.80000</span>   <span class="number">0.81000</span>   <span class="number">0.82000</span>   <span class="number">0.83000</span>   <span class="number">0.84000</span>   <span class="number">0.85000</span>   <span class="number">0.86000</span>   <span class="number">0.87000</span>   <span class="number">0.88000</span>   <span class="number">0.89000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">91</span> through <span class="number">99</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.90000</span>   <span class="number">0.91000</span>   <span class="number">0.92000</span>   <span class="number">0.93000</span>   <span class="number">0.94000</span>   <span class="number">0.95000</span>   <span class="number">0.96000</span>   <span class="number">0.97000</span>   <span class="number">0.98000</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; y1=<span class="built_in">sin</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*t);</span><br><span class="line">&gt;&gt; plot(t,y1)</span><br><span class="line">&gt;&gt; y2=<span class="built_in">cos</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*t);</span><br><span class="line">&gt;&gt; plot(t,y2)</span><br><span class="line">&gt;&gt; hold on</span><br><span class="line">&gt;&gt; plot(t,y1)</span><br><span class="line">&gt;&gt; plot(t,y2,<span class="string">'r'</span>)</span><br><span class="line">&gt;&gt; xlabel(<span class="string">'time'</span>)</span><br><span class="line">&gt;&gt; ylabel(<span class="string">'value'</span>)</span><br><span class="line">&gt;&gt; legend(<span class="string">'sin'</span>,<span class="string">'cos'</span>)</span><br><span class="line">&gt;&gt; title(<span class="string">'my plot'</span>)</span><br><span class="line">&gt;&gt; print -dpng <span class="string">'myPlot.png'</span></span><br><span class="line">warning: print.m: fig2dev binary is not available.</span><br><span class="line">Some output formats are not available.</span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; figure(<span class="number">2</span>); plot(t, y2)</span><br><span class="line">&gt;&gt; subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">&gt;&gt; plot(t,y1)</span><br><span class="line">&gt;&gt; subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">&gt;&gt; plot(t,y2)</span><br><span class="line">&gt;&gt; axis([<span class="number">0.5</span> <span class="number">1</span> <span class="number">-1</span> <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/images/ml/coursera/ml-ng-w2-11.png" alt="matric"></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; clf;</span><br><span class="line">&gt;&gt; A = <span class="built_in">magic</span>(<span class="number">5</span>)</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">17</span>   <span class="number">24</span>    <span class="number">1</span>    <span class="number">8</span>   <span class="number">15</span></span><br><span class="line">   <span class="number">23</span>    <span class="number">5</span>    <span class="number">7</span>   <span class="number">14</span>   <span class="number">16</span></span><br><span class="line">    <span class="number">4</span>    <span class="number">6</span>   <span class="number">13</span>   <span class="number">20</span>   <span class="number">22</span></span><br><span class="line">   <span class="number">10</span>   <span class="number">12</span>   <span class="number">19</span>   <span class="number">21</span>    <span class="number">3</span></span><br><span class="line">   <span class="number">11</span>   <span class="number">18</span>   <span class="number">25</span>    <span class="number">2</span>    <span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; imagesc(A)</span><br><span class="line">&gt;&gt; imagesc(A), colorbar, colormap gray;</span><br></pre></td></tr></table></figure>
<p><img src="/images/ml/coursera/ml-ng-w2-12.png" alt="matric"></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; imagesc(<span class="built_in">magic</span>(<span class="number">15</span>)), colorbar, colormap gray;</span><br><span class="line">&gt;&gt; a=<span class="number">1</span>,b=<span class="number">2</span>,c=<span class="number">3</span></span><br><span class="line">a =  <span class="number">1</span></span><br><span class="line">b =  <span class="number">2</span></span><br><span class="line">c =  <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/ml/coursera/ml-ng-w2-13.png" alt="matric"></p>
<h2 id="5-Control-statements-for-while-if"><a href="#5-Control-statements-for-while-if" class="headerlink" title="5. Control statements:for,while,if"></a>5. Control statements:for,while,if</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">v = <span class="built_in">zeros</span>(<span class="number">10</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">10</span>, </span><br><span class="line">    v(<span class="built_in">i</span>) = <span class="number">2</span>^<span class="built_in">i</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"><span class="comment">% Can also use "break" and "continue" inside for and while loops to control execution.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> <span class="built_in">i</span> &lt;= <span class="number">5</span>,</span><br><span class="line">  v(<span class="built_in">i</span>) = <span class="number">100</span>; </span><br><span class="line">  <span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> true, </span><br><span class="line">  v(<span class="built_in">i</span>) = <span class="number">999</span>; </span><br><span class="line">  <span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">i</span> == <span class="number">6</span>,</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> v(<span class="number">1</span>)==<span class="number">1</span>,</span><br><span class="line">  <span class="built_in">disp</span>(<span class="string">'The value is one!'</span>);</span><br><span class="line"><span class="keyword">elseif</span> v(<span class="number">1</span>)==<span class="number">2</span>,</span><br><span class="line">  <span class="built_in">disp</span>(<span class="string">'The value is two!'</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">disp</span>(<span class="string">'The value is not one or two!'</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>run example :</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line">&gt;&gt; <span class="keyword">while</span> <span class="built_in">i</span> &lt;= <span class="number">5</span>,</span><br><span class="line">     v(<span class="built_in">i</span>) = <span class="number">100</span>;</span><br><span class="line">     <span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span>;</span><br><span class="line">   <span class="keyword">end</span>;</span><br><span class="line">&gt;&gt; v</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">    <span class="number">100</span></span><br><span class="line">    <span class="number">100</span></span><br><span class="line">    <span class="number">100</span></span><br><span class="line">    <span class="number">100</span></span><br><span class="line">    <span class="number">100</span></span><br><span class="line">     <span class="number">64</span></span><br><span class="line">    <span class="number">128</span></span><br><span class="line">    <span class="number">256</span></span><br><span class="line">    <span class="number">512</span></span><br><span class="line">   <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line">&gt;&gt; <span class="keyword">while</span> true,</span><br><span class="line">     v(<span class="built_in">i</span>) = <span class="number">999</span>;</span><br><span class="line">     <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">i</span> == <span class="number">6</span>,</span><br><span class="line">       <span class="keyword">break</span>;</span><br><span class="line">     <span class="keyword">end</span>;</span><br><span class="line">   <span class="keyword">end</span>;</span><br><span class="line">&gt;&gt; v</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">    <span class="number">999</span></span><br><span class="line">    <span class="number">999</span></span><br><span class="line">    <span class="number">999</span></span><br><span class="line">    <span class="number">999</span></span><br><span class="line">    <span class="number">999</span></span><br><span class="line">     <span class="number">64</span></span><br><span class="line">    <span class="number">128</span></span><br><span class="line">    <span class="number">256</span></span><br><span class="line">    <span class="number">512</span></span><br><span class="line">   <span class="number">1024</span></span><br><span class="line">&gt;&gt; v(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">999</span></span><br><span class="line">&gt;&gt; v(<span class="number">1</span>) = <span class="number">2</span>;</span><br><span class="line">&gt;&gt; <span class="keyword">if</span> v(<span class="number">1</span>) == <span class="number">1</span>,</span><br><span class="line">     <span class="built_in">disp</span>(<span class="string">'The value is one'</span>);</span><br><span class="line">   <span class="keyword">elseif</span> v(<span class="number">1</span>) == <span class="number">2</span>,</span><br><span class="line">     <span class="built_in">disp</span>(<span class="string">'The value is two'</span>);</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">     <span class="built_in">disp</span>(<span class="string">'The value is not 1 or 2'</span>);</span><br><span class="line">   <span class="keyword">end</span>;</span><br><span class="line">The value is two</span><br></pre></td></tr></table></figure>
<h2 id="6-Functions"><a href="#6-Functions" class="headerlink" title="6. Functions"></a>6. Functions</h2><p>To call the function in Octave, do either:</p>
<p>1) Navigate to the directory of the functionName.m file and call the function:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">% Navigate to directory:</span><br><span class="line">  cd /path/to/function</span><br><span class="line"></span><br><span class="line">  % Call the function:</span><br><span class="line">  functionName(args)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>simple compute line regression</p>
</blockquote>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; x = [<span class="number">1</span> <span class="number">1</span>; <span class="number">1</span> <span class="number">2</span>; <span class="number">1</span> <span class="number">3</span>;]</span><br><span class="line">x =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">1</span></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">1</span>   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; y = [<span class="number">1</span>; <span class="number">2</span>; <span class="number">3</span>]</span><br><span class="line">y =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; theta = [<span class="number">0</span>;<span class="number">1</span>];</span><br><span class="line">&gt;&gt; <span class="built_in">j</span> = costFunctionJ(x,y,theta)</span><br><span class="line"><span class="built_in">j</span> = <span class="number">0</span></span><br><span class="line">&gt;&gt; theta = [<span class="number">0</span>:<span class="number">0</span>];</span><br><span class="line">&gt;&gt; <span class="built_in">j</span> = costFunctionJ(x, y, theta)</span><br><span class="line">warning: operator -: automatic broadcasting operation applied</span><br><span class="line"><span class="built_in">j</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">2.3333</span>   <span class="number">2.3333</span></span><br><span class="line">   <span class="number">2.3333</span>   <span class="number">2.3333</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>应该为一个 2.3333， 而不是矩阵 2.3333</p>
</blockquote>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; (<span class="number">1</span>^<span class="number">2</span> + <span class="number">2</span>^<span class="number">2</span> + <span class="number">3</span>^<span class="number">2</span>) / (<span class="number">2</span>*<span class="number">3</span>)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">2.3333</span></span><br><span class="line">&gt;&gt; [<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]*[<span class="number">-1</span>;<span class="number">-2</span>;<span class="number">-3</span>]</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">14</span></span><br><span class="line">&gt;&gt; <span class="number">14</span> / <span class="number">6</span></span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">2.3333</span></span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<h2 id="7-Vectorization"><a href="#7-Vectorization" class="headerlink" title="7. Vectorization"></a>7. Vectorization</h2><p>Vectorization is the process of taking code that relies on loops and converting it into matrix operations. It is more efficient, more elegant, and more concise.</p>
<p>As an example, let’s compute our prediction from a hypothesis. Theta is the vector of fields for the hypothesis and x is a vector of variables.</p>
<p>With loops:</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">prediction = <span class="number">0.0</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:n+<span class="number">1</span>,</span><br><span class="line">  prediction += theta(<span class="built_in">j</span>) * x(<span class="built_in">j</span>);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<p>With vectorization:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">prediction = theta&apos; * x;</span><br></pre></td></tr></table></figure>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><blockquote>
<ol>
<li>coursera week 2 learning notes</li>
<li><a href="http://blog.chenming.info/blog/2012/07/15/learn-octave/" target="_blank" rel="external">学习一点</a></li>
</ol>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Country And City]]></title>
      <url>http://sggo.me/2016/10/15/English/english-knowledge-map-l1/</url>
      <content type="html"><![CDATA[<p>Country, Capital, main city</p>
<a id="more"></a>
<h2 id="Country-And-City"><a href="#Country-And-City" class="headerlink" title="Country And City"></a>Country And City</h2><table>
<thead>
<tr>
<th style="text-align:center">country</th>
<th style="text-align:center">capital</th>
<th style="text-align:center">main city</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Britain　</td>
<td style="text-align:center">London</td>
<td style="text-align:center">London</td>
</tr>
<tr>
<td style="text-align:center">The United States</td>
<td style="text-align:center">Washington</td>
<td style="text-align:center">New York、Boston、Los Angeles、San Francisco、Seattle、Chicago、 Miami、Hawaii</td>
</tr>
<tr>
<td style="text-align:center">Canada</td>
<td style="text-align:center">Ottawa</td>
<td style="text-align:center">Toronto [tə’rɒntəʊ]</td>
</tr>
<tr>
<td style="text-align:center">Japan</td>
<td style="text-align:center">Tokyo</td>
<td style="text-align:center">Osaka、Fukuoka、Hokkaido、Nagoya</td>
</tr>
<tr>
<td style="text-align:center">South Korea</td>
<td style="text-align:center">Seoul [səʊl]</td>
<td style="text-align:center">Busan </td>
</tr>
<tr>
<td style="text-align:center">Australien</td>
<td style="text-align:center">Canberra</td>
<td style="text-align:center">Sydney、Melbourne</td>
</tr>
<tr>
<td style="text-align:center">New Zealand</td>
<td style="text-align:center">Wellington</td>
<td style="text-align:center">Auckland</td>
</tr>
<tr>
<td style="text-align:center">Singapore</td>
<td style="text-align:center">Singapore</td>
<td style="text-align:center">Singapore</td>
</tr>
<tr>
<td style="text-align:center">Malaysia</td>
<td style="text-align:center">Kuala Lumpur</td>
<td style="text-align:center">Kuala Lumpur [ˈkwɑ:ləˈlumpuə]、Malacca</td>
</tr>
<tr>
<td style="text-align:center">Thailand</td>
<td style="text-align:center">Bangkok</td>
<td style="text-align:center">chiengmai、huahin、Phuket、Pattaya</td>
</tr>
<tr>
<td style="text-align:center">Philippines</td>
<td style="text-align:center">Manila  [mə’nilə]</td>
</tr>
<tr>
<td style="text-align:center">India</td>
<td style="text-align:center">New Delhi [‘deli]</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Indonesia</td>
<td style="text-align:center">Jakarta</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Spain [spein]</td>
<td style="text-align:center">Madrid</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Germany</td>
<td style="text-align:center">Berlin</td>
<td style="text-align:center">Munich [‘mju:nik]</td>
</tr>
<tr>
<td style="text-align:center">France</td>
<td style="text-align:center">Paris</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Italy</td>
<td style="text-align:center">Rome</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Russia</td>
<td style="text-align:center">Moscow</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Mexico</td>
<td style="text-align:center">Mexico</td>
<td style="text-align:center">-</td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera Week 2 - Octave learning]]></title>
      <url>http://sggo.me/2016/10/12/ml/coursera-ng-w2-02/</url>
      <content type="html"><![CDATA[<p>Octave Tutorial, Octave Learning</p>
<a id="more"></a>
<h2 id="1-var"><a href="#1-var" class="headerlink" title="1. var"></a>1. var</h2><blockquote>
<p>不像matlab有图形界面，octave只提供了命令行接口。 要启动octave，只需要在命令行输入octave即可。</p>
</blockquote>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="number">2</span> * (<span class="number">3</span> + <span class="number">5</span>)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">16</span></span><br><span class="line">&gt;&gt; <span class="number">2</span> ^ (<span class="number">3</span> + <span class="number">5</span>)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">256</span></span><br><span class="line">&gt;&gt; x = <span class="number">2</span> * <span class="number">3</span></span><br><span class="line">x =  <span class="number">6</span></span><br><span class="line">&gt;&gt; who</span><br><span class="line">Variables in the current scope:</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span>  x</span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">disp</span>(x)</span><br><span class="line"> <span class="number">6</span></span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<h2 id="2-constant"><a href="#2-constant" class="headerlink" title="2. constant"></a>2. constant</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt; <span class="built_in">pi</span></span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">3.1416</span></span><br><span class="line">&gt;&gt; e</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">2.7183</span></span><br><span class="line">&gt;&gt; format long</span><br><span class="line">&gt;&gt; <span class="built_in">pi</span></span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">3.14159265358979</span></span><br><span class="line">&gt;&gt; format short</span><br><span class="line">&gt;&gt; <span class="built_in">pi</span></span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">3.1416</span></span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>octave系统定义了圆周率pi和自然指数e这两个常量, octave 可以定义显示结果</p>
</blockquote>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="number">3</span>/<span class="number">0</span></span><br><span class="line">warning: division by zero</span><br><span class="line"><span class="built_in">ans</span> = Inf</span><br><span class="line">&gt;&gt; <span class="number">0</span>/<span class="number">0</span></span><br><span class="line">warning: division by zero</span><br><span class="line"><span class="built_in">ans</span> = NaN</span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>系统定义了Inf和NaN（注意要区分大小写）。Inf(Infinity)表示被零除的结果，NaN(Not a Number)表示零除零的结果。</p>
</blockquote>
<h2 id="3-workspace"><a href="#3-workspace" class="headerlink" title="3. workspace"></a>3. workspace</h2><p>使用save命令保存当前工作区到文件 work1</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; save work1</span><br><span class="line">&gt;&gt; load work1</span><br><span class="line">&gt;&gt; <span class="built_in">pi</span></span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">3.1416</span></span><br></pre></td></tr></table></figure>
<h2 id="4-semicolon"><a href="#4-semicolon" class="headerlink" title="4. semicolon"></a>4. semicolon</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">octave:<span class="number">32</span>&gt; x = <span class="number">2</span> * <span class="number">3</span></span><br><span class="line">x =  <span class="number">6</span></span><br><span class="line">octave:<span class="number">33</span>&gt; x = <span class="number">2</span> * <span class="number">3</span>;</span><br><span class="line">octave:<span class="number">34</span>&gt; <span class="built_in">disp</span>(x)</span><br><span class="line"> <span class="number">6</span></span><br></pre></td></tr></table></figure>
<h2 id="5-matrix"><a href="#5-matrix" class="headerlink" title="5. matrix"></a>5. matrix</h2><p>矩阵使用方括号([])括起来，维度使用分号(;)分割。 同一维度之间的分隔符可以是空格或逗号(,)</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">octave:<span class="number">35</span>&gt; x = [ <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> ]</span><br><span class="line">x =</span><br><span class="line"></span><br><span class="line">   <span class="number">2</span>   <span class="number">3</span>   <span class="number">5</span></span><br><span class="line"></span><br><span class="line">octave:<span class="number">36</span>&gt; y = [ <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span> ]</span><br><span class="line">y =</span><br><span class="line"></span><br><span class="line">   <span class="number">2</span>   <span class="number">3</span>   <span class="number">5</span></span><br><span class="line"></span><br><span class="line">octave:<span class="number">37</span>&gt; z = [ <span class="number">2</span>; <span class="number">3</span>; <span class="number">5</span> ]</span><br><span class="line">z =</span><br><span class="line"></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line">   <span class="number">5</span></span><br><span class="line"></span><br><span class="line">octave:<span class="number">39</span>&gt; a = [ <span class="number">1</span> <span class="number">2</span>; <span class="number">1</span>, <span class="number">3</span>; <span class="number">1</span>   <span class="number">5</span> ]</span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">1</span>   <span class="number">3</span></span><br><span class="line">   <span class="number">1</span>   <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>使用冒号表达式快速构造连续的向量</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">octave:<span class="number">43</span>&gt; v = <span class="number">2</span>:<span class="number">5</span></span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">   <span class="number">2</span>   <span class="number">3</span>   <span class="number">4</span>   <span class="number">5</span></span><br><span class="line">   </span><br><span class="line">octave:<span class="number">44</span>&gt; v = <span class="number">2</span>:<span class="number">0.3</span>:<span class="number">3</span></span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">    <span class="number">2.0000</span>    <span class="number">2.3000</span>    <span class="number">2.6000</span>    <span class="number">2.9000</span></span><br></pre></td></tr></table></figure>
<p>构造矩阵的函数</p>
<blockquote>
<p><code>linspace(start, end, N)</code> 产生N个均匀分布于start和end之间的向量。 在绘图时用于产生x坐标特别有用。</p>
<p><code>logspace(start, end, N)</code> 产生N个指数分布于10^start和10^end之间的向量。 在绘图时用于产生x坐标特别有用。</p>
<p>zeros(M, N)</p>
<p>zeros(N) = zeros(N, N)。</p>
<p>ones(M, N)</p>
<p>ones(N) = ones(N, N)。</p>
<p>rand(M, N) 值位于0~1的随机数的矩阵。</p>
<p>rand(N) = rand(N, N)。</p>
</blockquote>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">octave:<span class="number">66</span>&gt; x = <span class="built_in">linspace</span> (<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">x =</span><br><span class="line"></span><br><span class="line"> Columns <span class="number">1</span> through <span class="number">4</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">3.00000000000000</span>   <span class="number">3.25000000000000</span>   <span class="number">3.50000000000000</span>   <span class="number">3.75000000000000</span></span><br><span class="line"></span><br><span class="line"> Column <span class="number">5</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">4.00000000000000</span></span><br><span class="line"></span><br><span class="line">octave:<span class="number">67</span>&gt; <span class="built_in">logspace</span> (<span class="number">1</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line"> Columns <span class="number">1</span> through <span class="number">4</span>:</span><br><span class="line"></span><br><span class="line">    <span class="number">10.0000000000000</span>    <span class="number">15.8489319246111</span>    <span class="number">25.1188643150958</span>    <span class="number">39.8107170553497</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">5</span> and <span class="number">6</span>:</span><br><span class="line"></span><br><span class="line">    <span class="number">63.0957344480193</span>   <span class="number">100.0000000000000</span></span><br></pre></td></tr></table></figure>
<h2 id="6-matrix-operation"><a href="#6-matrix-operation" class="headerlink" title="6. matrix operation"></a>6. matrix operation</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">A + B</span><br><span class="line">A - B</span><br><span class="line">A * B</span><br><span class="line">A \ B</span><br></pre></td></tr></table></figure>
<p>说明：A\B为矩阵左除，用于求解线性方程Wx=b，其中W为一个nxn的矩阵，b为一个n维的列向量。 求解线性方式示例：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">octave:<span class="number">15</span>&gt; W = [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>; <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>; <span class="number">3</span> <span class="number">4</span> <span class="number">6</span> <span class="number">2</span>; <span class="number">2</span> <span class="number">7</span> <span class="number">10</span> <span class="number">5</span>];</span><br><span class="line">octave:<span class="number">16</span>&gt; b = [<span class="number">3</span>; <span class="number">5</span>; <span class="number">5</span>; <span class="number">8</span>];</span><br><span class="line">octave:<span class="number">17</span>&gt; x = W\b</span><br><span class="line">x =</span><br><span class="line"></span><br><span class="line">   <span class="number">1.0000</span></span><br><span class="line">   <span class="number">3.0000</span></span><br><span class="line">  <span class="number">-2.0000</span></span><br><span class="line">   <span class="number">1.0000</span></span><br></pre></td></tr></table></figure>
<p><strong>6.1 matrix transpose</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">octave:<span class="number">9</span>&gt; x = <span class="built_in">rand</span>(<span class="number">3</span>)</span><br><span class="line">x =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.0052581</span>   <span class="number">0.4446771</span>   <span class="number">0.3970036</span></span><br><span class="line">   <span class="number">0.7844458</span>   <span class="number">0.3317067</span>   <span class="number">0.9633000</span></span><br><span class="line">   <span class="number">0.0577080</span>   <span class="number">0.9015905</span>   <span class="number">0.0344771</span></span><br><span class="line"></span><br><span class="line">octave:<span class="number">10</span>&gt; x'</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.0052581</span>   <span class="number">0.7844458</span>   <span class="number">0.0577080</span></span><br><span class="line">   <span class="number">0.4446771</span>   <span class="number">0.3317067</span>   <span class="number">0.9015905</span></span><br><span class="line">   <span class="number">0.3970036</span>   <span class="number">0.9633000</span>   <span class="number">0.034477</span></span><br></pre></td></tr></table></figure>
<h2 id="7-plotting"><a href="#7-plotting" class="headerlink" title="7. plotting"></a>7. plotting</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; t=[<span class="number">0</span>:<span class="number">0.01</span>:<span class="number">0.98</span>];</span><br><span class="line">&gt;&gt; t</span><br><span class="line">t =</span><br><span class="line"></span><br><span class="line"> Columns <span class="number">1</span> through <span class="number">10</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.00000</span>   <span class="number">0.01000</span>   <span class="number">0.02000</span>   <span class="number">0.03000</span>   <span class="number">0.04000</span>   <span class="number">0.05000</span>   <span class="number">0.06000</span>   <span class="number">0.07000</span>   <span class="number">0.08000</span>   <span class="number">0.09000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">11</span> through <span class="number">20</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.10000</span>   <span class="number">0.11000</span>   <span class="number">0.12000</span>   <span class="number">0.13000</span>   <span class="number">0.14000</span>   <span class="number">0.15000</span>   <span class="number">0.16000</span>   <span class="number">0.17000</span>   <span class="number">0.18000</span>   <span class="number">0.19000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">21</span> through <span class="number">30</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.20000</span>   <span class="number">0.21000</span>   <span class="number">0.22000</span>   <span class="number">0.23000</span>   <span class="number">0.24000</span>   <span class="number">0.25000</span>   <span class="number">0.26000</span>   <span class="number">0.27000</span>   <span class="number">0.28000</span>   <span class="number">0.29000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">31</span> through <span class="number">40</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.30000</span>   <span class="number">0.31000</span>   <span class="number">0.32000</span>   <span class="number">0.33000</span>   <span class="number">0.34000</span>   <span class="number">0.35000</span>   <span class="number">0.36000</span>   <span class="number">0.37000</span>   <span class="number">0.38000</span>   <span class="number">0.39000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">41</span> through <span class="number">50</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.40000</span>   <span class="number">0.41000</span>   <span class="number">0.42000</span>   <span class="number">0.43000</span>   <span class="number">0.44000</span>   <span class="number">0.45000</span>   <span class="number">0.46000</span>   <span class="number">0.47000</span>   <span class="number">0.48000</span>   <span class="number">0.49000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">51</span> through <span class="number">60</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.50000</span>   <span class="number">0.51000</span>   <span class="number">0.52000</span>   <span class="number">0.53000</span>   <span class="number">0.54000</span>   <span class="number">0.55000</span>   <span class="number">0.56000</span>   <span class="number">0.57000</span>   <span class="number">0.58000</span>   <span class="number">0.59000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">61</span> through <span class="number">70</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.60000</span>   <span class="number">0.61000</span>   <span class="number">0.62000</span>   <span class="number">0.63000</span>   <span class="number">0.64000</span>   <span class="number">0.65000</span>   <span class="number">0.66000</span>   <span class="number">0.67000</span>   <span class="number">0.68000</span>   <span class="number">0.69000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">71</span> through <span class="number">80</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.70000</span>   <span class="number">0.71000</span>   <span class="number">0.72000</span>   <span class="number">0.73000</span>   <span class="number">0.74000</span>   <span class="number">0.75000</span>   <span class="number">0.76000</span>   <span class="number">0.77000</span>   <span class="number">0.78000</span>   <span class="number">0.79000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">81</span> through <span class="number">90</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.80000</span>   <span class="number">0.81000</span>   <span class="number">0.82000</span>   <span class="number">0.83000</span>   <span class="number">0.84000</span>   <span class="number">0.85000</span>   <span class="number">0.86000</span>   <span class="number">0.87000</span>   <span class="number">0.88000</span>   <span class="number">0.89000</span></span><br><span class="line"></span><br><span class="line"> Columns <span class="number">91</span> through <span class="number">99</span>:</span><br><span class="line"></span><br><span class="line">   <span class="number">0.90000</span>   <span class="number">0.91000</span>   <span class="number">0.92000</span>   <span class="number">0.93000</span>   <span class="number">0.94000</span>   <span class="number">0.95000</span>   <span class="number">0.96000</span>   <span class="number">0.97000</span>   <span class="number">0.98000</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; y1=<span class="built_in">sin</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*t);</span><br><span class="line">&gt;&gt; plot(t,y1)</span><br><span class="line">&gt;&gt; y2=<span class="built_in">cos</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*t);</span><br><span class="line">&gt;&gt; plot(t,y2)</span><br><span class="line">&gt;&gt; hold on</span><br><span class="line">&gt;&gt; plot(t,y1)</span><br><span class="line">&gt;&gt; plot(t,y2,<span class="string">'r'</span>)</span><br><span class="line">&gt;&gt; xlabel(<span class="string">'time'</span>)</span><br><span class="line">&gt;&gt; ylabel(<span class="string">'value'</span>)</span><br><span class="line">&gt;&gt; legend(<span class="string">'sin'</span>,<span class="string">'cos'</span>)</span><br><span class="line">&gt;&gt; title(<span class="string">'my plot'</span>)</span><br><span class="line">&gt;&gt; print -dpng <span class="string">'myPlot.png'</span></span><br><span class="line">warning: print.m: fig2dev binary is not available.</span><br><span class="line">Some output formats are not available.</span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; figure(<span class="number">2</span>); plot(t, y2)</span><br><span class="line">&gt;&gt; subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">&gt;&gt; plot(t,y1)</span><br><span class="line">&gt;&gt; subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">&gt;&gt; plot(t,y2)</span><br><span class="line">&gt;&gt; axis([<span class="number">0.5</span> <span class="number">1</span> <span class="number">-1</span> <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/images/ml/coursera/ml-ng-w2-11.png" alt="matric"></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; clf;</span><br><span class="line">&gt;&gt; A = <span class="built_in">magic</span>(<span class="number">5</span>)</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">17</span>   <span class="number">24</span>    <span class="number">1</span>    <span class="number">8</span>   <span class="number">15</span></span><br><span class="line">   <span class="number">23</span>    <span class="number">5</span>    <span class="number">7</span>   <span class="number">14</span>   <span class="number">16</span></span><br><span class="line">    <span class="number">4</span>    <span class="number">6</span>   <span class="number">13</span>   <span class="number">20</span>   <span class="number">22</span></span><br><span class="line">   <span class="number">10</span>   <span class="number">12</span>   <span class="number">19</span>   <span class="number">21</span>    <span class="number">3</span></span><br><span class="line">   <span class="number">11</span>   <span class="number">18</span>   <span class="number">25</span>    <span class="number">2</span>    <span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; imagesc(A)</span><br><span class="line">&gt;&gt; imagesc(A), colorbar, colormap gray;</span><br></pre></td></tr></table></figure>
<p><img src="/images/ml/coursera/ml-ng-w2-12.png" alt="matric"></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; imagesc(<span class="built_in">magic</span>(<span class="number">15</span>)), colorbar, colormap gray;</span><br><span class="line">&gt;&gt; a=<span class="number">1</span>,b=<span class="number">2</span>,c=<span class="number">3</span></span><br><span class="line">a =  <span class="number">1</span></span><br><span class="line">b =  <span class="number">2</span></span><br><span class="line">c =  <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/ml/coursera/ml-ng-w2-13.png" alt="matric"></p>
<h2 id="8-ng"><a href="#8-ng" class="headerlink" title="8. ng"></a>8. ng</h2><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = [<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>;]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line">&gt;&gt; </span><br><span class="line">save hello.mat v; (压缩比例很大)</span><br><span class="line">save hello.txt v -ascii <span class="comment">% save as text(ASCII)</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; who</span><br><span class="line">Variables in the current scope:</span><br><span class="line"></span><br><span class="line">A</span><br><span class="line"></span><br><span class="line">&gt;&gt; whos</span><br><span class="line">Variables in the current scope:</span><br><span class="line"></span><br><span class="line">   Attr Name        Size                     Bytes  Class</span><br><span class="line">   ==== ====        ====                     =====  =====</span><br><span class="line">        A           <span class="number">3</span>x2                         <span class="number">48</span>  double</span><br><span class="line"></span><br><span class="line">Total is <span class="number">6</span> elements using <span class="number">48</span> bytes</span><br><span class="line"></span><br><span class="line">&gt;&gt; clear</span><br><span class="line">&gt;&gt; A(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">6</span></span><br><span class="line">&gt;&gt; A(:,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">4</span></span><br><span class="line">   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(<span class="number">2</span>,:)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A([<span class="number">1</span> <span class="number">3</span>], :)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(:,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">4</span></span><br><span class="line">   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(:,<span class="number">2</span>) = [<span class="number">10</span>; <span class="number">11</span>; <span class="number">12</span>]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>   <span class="number">10</span></span><br><span class="line">    <span class="number">3</span>   <span class="number">11</span></span><br><span class="line">    <span class="number">5</span>   <span class="number">12</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A = [A, [<span class="number">100</span>; <span class="number">101</span>; <span class="number">102</span>]];</span><br><span class="line">&gt;&gt; A</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>    <span class="number">10</span>   <span class="number">100</span></span><br><span class="line">     <span class="number">3</span>    <span class="number">11</span>   <span class="number">101</span></span><br><span class="line">     <span class="number">5</span>    <span class="number">12</span>   <span class="number">102</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; [<span class="number">100</span>;<span class="number">101</span>;<span class="number">102</span>]</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">100</span></span><br><span class="line">   <span class="number">101</span></span><br><span class="line">   <span class="number">102</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">size</span>(A)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">3</span>   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(:)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">3</span></span><br><span class="line">     <span class="number">5</span></span><br><span class="line">    <span class="number">10</span></span><br><span class="line">    <span class="number">11</span></span><br><span class="line">    <span class="number">12</span></span><br><span class="line">   <span class="number">100</span></span><br><span class="line">   <span class="number">101</span></span><br><span class="line">   <span class="number">102</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A = [<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>;]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; B = [<span class="number">11</span> <span class="number">12</span>; <span class="number">13</span> <span class="number">14</span>; <span class="number">15</span> <span class="number">16</span>]</span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">   <span class="number">11</span>   <span class="number">12</span></span><br><span class="line">   <span class="number">13</span>   <span class="number">14</span></span><br><span class="line">   <span class="number">15</span>   <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; C = [A B]</span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>    <span class="number">2</span>   <span class="number">11</span>   <span class="number">12</span></span><br><span class="line">    <span class="number">3</span>    <span class="number">4</span>   <span class="number">13</span>   <span class="number">14</span></span><br><span class="line">    <span class="number">5</span>    <span class="number">6</span>   <span class="number">15</span>   <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; D = [A;B]</span><br><span class="line">D =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>    <span class="number">2</span></span><br><span class="line">    <span class="number">3</span>    <span class="number">4</span></span><br><span class="line">    <span class="number">5</span>    <span class="number">6</span></span><br><span class="line">   <span class="number">11</span>   <span class="number">12</span></span><br><span class="line">   <span class="number">13</span>   <span class="number">14</span></span><br><span class="line">   <span class="number">15</span>   <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">size</span>(D)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">6</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; [A, B]</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>    <span class="number">2</span>   <span class="number">11</span>   <span class="number">12</span></span><br><span class="line">    <span class="number">3</span>    <span class="number">4</span>   <span class="number">13</span>   <span class="number">14</span></span><br><span class="line">    <span class="number">5</span>    <span class="number">6</span>   <span class="number">15</span>   <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; [A B]</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>    <span class="number">2</span>   <span class="number">11</span>   <span class="number">12</span></span><br><span class="line">    <span class="number">3</span>    <span class="number">4</span>   <span class="number">13</span>   <span class="number">14</span></span><br><span class="line">    <span class="number">5</span>    <span class="number">6</span>   <span class="number">15</span>   <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">&gt;&gt;</span><br><span class="line">&gt;&gt;&gt;&gt; A .* B</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">11</span>   <span class="number">24</span></span><br><span class="line">   <span class="number">39</span>   <span class="number">56</span></span><br><span class="line">   <span class="number">75</span>   <span class="number">96</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A .^ <span class="number">2</span></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>    <span class="number">4</span></span><br><span class="line">    <span class="number">9</span>   <span class="number">16</span></span><br><span class="line">   <span class="number">25</span>   <span class="number">36</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; v = [<span class="number">1</span>; <span class="number">2</span>; <span class="number">3</span>]</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="number">1</span> ./ v</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1.00000</span></span><br><span class="line">   <span class="number">0.50000</span></span><br><span class="line">   <span class="number">0.33333</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="number">1</span> ./ A</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1.00000</span>   <span class="number">0.50000</span></span><br><span class="line">   <span class="number">0.33333</span>   <span class="number">0.25000</span></span><br><span class="line">   <span class="number">0.20000</span>   <span class="number">0.16667</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">log</span>(v)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.00000</span></span><br><span class="line">   <span class="number">0.69315</span></span><br><span class="line">   <span class="number">1.09861</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">exp</span>(v)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">2.7183</span></span><br><span class="line">    <span class="number">7.3891</span></span><br><span class="line">   <span class="number">20.0855</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">abs</span>(v)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">abs</span>([<span class="number">-1</span>; <span class="number">-2</span>; <span class="number">-3</span>])</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; V = v</span><br><span class="line">V =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; V</span><br><span class="line">V =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; V</span><br><span class="line">V =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; -V</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">  <span class="number">-1</span></span><br><span class="line">  <span class="number">-2</span></span><br><span class="line">  <span class="number">-3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; V + <span class="built_in">ones</span>(<span class="built_in">length</span>(V))</span><br><span class="line">warning: operator +: automatic broadcasting operation applied</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span></span><br><span class="line">   <span class="number">4</span>   <span class="number">4</span>   <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">length</span>(V)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">3</span></span><br><span class="line">&gt;&gt; <span class="built_in">ones</span>(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; V + <span class="built_in">ones</span>(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line">   <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; V + <span class="number">2</span></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">3</span></span><br><span class="line">   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; V</span><br><span class="line">V =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A'</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">3</span>   <span class="number">5</span></span><br><span class="line">   <span class="number">2</span>   <span class="number">4</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; a  = [<span class="number">1</span> <span class="number">15</span> <span class="number">2</span> <span class="number">0.5</span>]</span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">    <span class="number">1.00000</span>   <span class="number">15.00000</span>    <span class="number">2.00000</span>    <span class="number">0.50000</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; val = max(a)</span><br><span class="line">val =  <span class="number">15</span></span><br><span class="line">&gt;&gt; [val, ind] = max(a)</span><br><span class="line">val =  <span class="number">15</span></span><br><span class="line">ind =  <span class="number">2</span></span><br><span class="line">&gt;&gt; max(A)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; a</span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">    <span class="number">1.00000</span>   <span class="number">15.00000</span>    <span class="number">2.00000</span>    <span class="number">0.50000</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; a &lt; <span class="number">3</span></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">find</span>(a &lt; <span class="number">3</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A = magix(<span class="number">3</span>)</span><br><span class="line">error: <span class="string">'magix'</span> undefined near line <span class="number">1</span> column <span class="number">5</span></span><br><span class="line">&gt;&gt; A = <span class="built_in">magic</span>(<span class="number">3</span>)</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span>   <span class="number">1</span>   <span class="number">6</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">5</span>   <span class="number">7</span></span><br><span class="line">   <span class="number">4</span>   <span class="number">9</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; [r, c] = <span class="built_in">find</span>(A &gt;= <span class="number">7</span>)</span><br><span class="line">r =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">c =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">7</span></span><br><span class="line">&gt;&gt; sum(a)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">18.500</span></span><br><span class="line">&gt;&gt; prod(a)</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">15</span></span><br><span class="line">&gt;&gt; <span class="built_in">floor</span>(a)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>   <span class="number">15</span>    <span class="number">2</span>    <span class="number">0</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">ceil</span>(a)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>   <span class="number">15</span>    <span class="number">2</span>    <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">rand</span>(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.708800</span>   <span class="number">0.905101</span>   <span class="number">0.837562</span></span><br><span class="line">   <span class="number">0.264139</span>   <span class="number">0.265985</span>   <span class="number">0.671546</span></span><br><span class="line">   <span class="number">0.411435</span>   <span class="number">0.058028</span>   <span class="number">0.454436</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(<span class="built_in">rand</span>(<span class="number">3</span>), <span class="built_in">rand</span>(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.87641</span>   <span class="number">0.74541</span>   <span class="number">0.92027</span></span><br><span class="line">   <span class="number">0.61292</span>   <span class="number">0.57756</span>   <span class="number">0.95694</span></span><br><span class="line">   <span class="number">0.26555</span>   <span class="number">0.76822</span>   <span class="number">0.63566</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span>   <span class="number">1</span>   <span class="number">6</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">5</span>   <span class="number">7</span></span><br><span class="line">   <span class="number">4</span>   <span class="number">9</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(A, [], <span class="number">1</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span>   <span class="number">9</span>   <span class="number">7</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(A, [], <span class="number">2</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span></span><br><span class="line">   <span class="number">7</span></span><br><span class="line">   <span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(A)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span>   <span class="number">9</span>   <span class="number">7</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(max(A))</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">9</span></span><br><span class="line">&gt;&gt; A(:)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line">   <span class="number">4</span></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">5</span></span><br><span class="line">   <span class="number">9</span></span><br><span class="line">   <span class="number">6</span></span><br><span class="line">   <span class="number">7</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(A(:))</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">9</span></span><br><span class="line">&gt;&gt;</span><br><span class="line">&gt;&gt;</span><br><span class="line">&gt;&gt; A = <span class="built_in">magic</span>(<span class="number">9</span>)</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">47</span>   <span class="number">58</span>   <span class="number">69</span>   <span class="number">80</span>    <span class="number">1</span>   <span class="number">12</span>   <span class="number">23</span>   <span class="number">34</span>   <span class="number">45</span></span><br><span class="line">   <span class="number">57</span>   <span class="number">68</span>   <span class="number">79</span>    <span class="number">9</span>   <span class="number">11</span>   <span class="number">22</span>   <span class="number">33</span>   <span class="number">44</span>   <span class="number">46</span></span><br><span class="line">   <span class="number">67</span>   <span class="number">78</span>    <span class="number">8</span>   <span class="number">10</span>   <span class="number">21</span>   <span class="number">32</span>   <span class="number">43</span>   <span class="number">54</span>   <span class="number">56</span></span><br><span class="line">   <span class="number">77</span>    <span class="number">7</span>   <span class="number">18</span>   <span class="number">20</span>   <span class="number">31</span>   <span class="number">42</span>   <span class="number">53</span>   <span class="number">55</span>   <span class="number">66</span></span><br><span class="line">    <span class="number">6</span>   <span class="number">17</span>   <span class="number">19</span>   <span class="number">30</span>   <span class="number">41</span>   <span class="number">52</span>   <span class="number">63</span>   <span class="number">65</span>   <span class="number">76</span></span><br><span class="line">   <span class="number">16</span>   <span class="number">27</span>   <span class="number">29</span>   <span class="number">40</span>   <span class="number">51</span>   <span class="number">62</span>   <span class="number">64</span>   <span class="number">75</span>    <span class="number">5</span></span><br><span class="line">   <span class="number">26</span>   <span class="number">28</span>   <span class="number">39</span>   <span class="number">50</span>   <span class="number">61</span>   <span class="number">72</span>   <span class="number">74</span>    <span class="number">4</span>   <span class="number">15</span></span><br><span class="line">   <span class="number">36</span>   <span class="number">38</span>   <span class="number">49</span>   <span class="number">60</span>   <span class="number">71</span>   <span class="number">73</span>    <span class="number">3</span>   <span class="number">14</span>   <span class="number">25</span></span><br><span class="line">   <span class="number">37</span>   <span class="number">48</span>   <span class="number">59</span>   <span class="number">70</span>   <span class="number">81</span>    <span class="number">2</span>   <span class="number">13</span>   <span class="number">24</span>   <span class="number">35</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(A,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(A,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">eye</span>(<span class="number">9</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">Diagonal Matrix</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">47</span>   <span class="number">58</span>   <span class="number">69</span>   <span class="number">80</span>    <span class="number">1</span>   <span class="number">12</span>   <span class="number">23</span>   <span class="number">34</span>   <span class="number">45</span></span><br><span class="line">   <span class="number">57</span>   <span class="number">68</span>   <span class="number">79</span>    <span class="number">9</span>   <span class="number">11</span>   <span class="number">22</span>   <span class="number">33</span>   <span class="number">44</span>   <span class="number">46</span></span><br><span class="line">   <span class="number">67</span>   <span class="number">78</span>    <span class="number">8</span>   <span class="number">10</span>   <span class="number">21</span>   <span class="number">32</span>   <span class="number">43</span>   <span class="number">54</span>   <span class="number">56</span></span><br><span class="line">   <span class="number">77</span>    <span class="number">7</span>   <span class="number">18</span>   <span class="number">20</span>   <span class="number">31</span>   <span class="number">42</span>   <span class="number">53</span>   <span class="number">55</span>   <span class="number">66</span></span><br><span class="line">    <span class="number">6</span>   <span class="number">17</span>   <span class="number">19</span>   <span class="number">30</span>   <span class="number">41</span>   <span class="number">52</span>   <span class="number">63</span>   <span class="number">65</span>   <span class="number">76</span></span><br><span class="line">   <span class="number">16</span>   <span class="number">27</span>   <span class="number">29</span>   <span class="number">40</span>   <span class="number">51</span>   <span class="number">62</span>   <span class="number">64</span>   <span class="number">75</span>    <span class="number">5</span></span><br><span class="line">   <span class="number">26</span>   <span class="number">28</span>   <span class="number">39</span>   <span class="number">50</span>   <span class="number">61</span>   <span class="number">72</span>   <span class="number">74</span>    <span class="number">4</span>   <span class="number">15</span></span><br><span class="line">   <span class="number">36</span>   <span class="number">38</span>   <span class="number">49</span>   <span class="number">60</span>   <span class="number">71</span>   <span class="number">73</span>    <span class="number">3</span>   <span class="number">14</span>   <span class="number">25</span></span><br><span class="line">   <span class="number">37</span>   <span class="number">48</span>   <span class="number">59</span>   <span class="number">70</span>   <span class="number">81</span>    <span class="number">2</span>   <span class="number">13</span>   <span class="number">24</span>   <span class="number">35</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A .* <span class="built_in">eye</span>(<span class="number">9</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">47</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">    <span class="number">0</span>   <span class="number">68</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">    <span class="number">0</span>    <span class="number">0</span>    <span class="number">8</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>   <span class="number">20</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>   <span class="number">41</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>   <span class="number">62</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>   <span class="number">74</span>    <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>   <span class="number">14</span>    <span class="number">0</span></span><br><span class="line">    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>   <span class="number">35</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(sum(A .* <span class="built_in">eye</span>(<span class="number">9</span>)))</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">369</span></span><br><span class="line">&gt;&gt; <span class="built_in">flipud</span>(<span class="built_in">eye</span>(<span class="number">9</span>))</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">Permutation Matrix</span><br><span class="line"></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line">   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(sum(A.*<span class="built_in">flipud</span>(<span class="built_in">eye</span>(<span class="number">9</span>))))</span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">369</span></span><br><span class="line">&gt;&gt; A</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">47</span>   <span class="number">58</span>   <span class="number">69</span>   <span class="number">80</span>    <span class="number">1</span>   <span class="number">12</span>   <span class="number">23</span>   <span class="number">34</span>   <span class="number">45</span></span><br><span class="line">   <span class="number">57</span>   <span class="number">68</span>   <span class="number">79</span>    <span class="number">9</span>   <span class="number">11</span>   <span class="number">22</span>   <span class="number">33</span>   <span class="number">44</span>   <span class="number">46</span></span><br><span class="line">   <span class="number">67</span>   <span class="number">78</span>    <span class="number">8</span>   <span class="number">10</span>   <span class="number">21</span>   <span class="number">32</span>   <span class="number">43</span>   <span class="number">54</span>   <span class="number">56</span></span><br><span class="line">   <span class="number">77</span>    <span class="number">7</span>   <span class="number">18</span>   <span class="number">20</span>   <span class="number">31</span>   <span class="number">42</span>   <span class="number">53</span>   <span class="number">55</span>   <span class="number">66</span></span><br><span class="line">    <span class="number">6</span>   <span class="number">17</span>   <span class="number">19</span>   <span class="number">30</span>   <span class="number">41</span>   <span class="number">52</span>   <span class="number">63</span>   <span class="number">65</span>   <span class="number">76</span></span><br><span class="line">   <span class="number">16</span>   <span class="number">27</span>   <span class="number">29</span>   <span class="number">40</span>   <span class="number">51</span>   <span class="number">62</span>   <span class="number">64</span>   <span class="number">75</span>    <span class="number">5</span></span><br><span class="line">   <span class="number">26</span>   <span class="number">28</span>   <span class="number">39</span>   <span class="number">50</span>   <span class="number">61</span>   <span class="number">72</span>   <span class="number">74</span>    <span class="number">4</span>   <span class="number">15</span></span><br><span class="line">   <span class="number">36</span>   <span class="number">38</span>   <span class="number">49</span>   <span class="number">60</span>   <span class="number">71</span>   <span class="number">73</span>    <span class="number">3</span>   <span class="number">14</span>   <span class="number">25</span></span><br><span class="line">   <span class="number">37</span>   <span class="number">48</span>   <span class="number">59</span>   <span class="number">70</span>   <span class="number">81</span>    <span class="number">2</span>   <span class="number">13</span>   <span class="number">24</span>   <span class="number">35</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A = <span class="built_in">magic</span>(<span class="number">3</span>)</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span>   <span class="number">1</span>   <span class="number">6</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">5</span>   <span class="number">7</span></span><br><span class="line">   <span class="number">4</span>   <span class="number">9</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; temp = pinv(A)</span><br><span class="line">temp =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.147222</span>  <span class="number">-0.144444</span>   <span class="number">0.063889</span></span><br><span class="line">  <span class="number">-0.061111</span>   <span class="number">0.022222</span>   <span class="number">0.105556</span></span><br><span class="line">  <span class="number">-0.019444</span>   <span class="number">0.188889</span>  <span class="number">-0.102778</span></span><br></pre></td></tr></table></figure>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><blockquote>
<ol>
<li>coursera week 2 learning notes</li>
<li><a href="http://blog.chenming.info/blog/2012/07/15/learn-octave/" target="_blank" rel="external">学习一点</a></li>
</ol>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera Week 2 - Linear Regression with Multiple Variables]]></title>
      <url>http://sggo.me/2016/10/08/ml/coursera-ng-w2-01-Linear-Regression/</url>
      <content type="html"><![CDATA[<p>coursera week 2 - linear regression with multiple variables 1</p>
<a id="more"></a>
<h2 id="1-Multiple-Features"><a href="#1-Multiple-Features" class="headerlink" title="1. Multiple Features"></a>1. Multiple Features</h2><p><img src="/images/ml/coursera/ml-ng-w2-01.png" alt="Multiple Features"></p>
<p>$<br>\begin{align}x_j^{(i)} &amp;= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \newline<br>x^{(i)}&amp; = \text{the column vector of all the feature inputs of the }i^{th}\text{ training example} \newline<br>m &amp;= \text{the number of training examples} \newline<br>n &amp;= \left| x^{(i)} \right| ; \text{(the number of features)} \end{align}<br>$</p>
<blockquote>
<p>Macdown Version 0.6.4 (786) MathJax the same this web</p>
</blockquote>
<h3 id="1-1-hypothesis-function"><a href="#1-1-hypothesis-function" class="headerlink" title="1.1 hypothesis function"></a>1.1 hypothesis function</h3><p>Now define the multivariable form of the hypothesis function as follows, accommodating these multiple features:</p>
<p>$<br>h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n<br>$</p>
<p><strong>multivariable hypothesis function</strong></p>
<p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:</p>
<blockquote>
<p>$<br>\begin{align}<br>h_\theta(x) =\begin{bmatrix}\theta_0 \hspace{2em}  \theta_1 \hspace{2em}  …  \hspace{2em}  \theta_n\end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T x<br>\end{align}<br>$</p>
</blockquote>
<p>The training examples are stored in X row-wise, like such:</p>
<blockquote>
<p>$<br>\begin{align}<br>X =<br>\begin{bmatrix}x^{(1)}_0 &amp; x^{(1)}_1  \newline x^{(2)}_0 &amp; x^{(2)}_1  \newline<br>x^{(3)}_0 &amp; x^{(3)}_1 \end{bmatrix}&amp;,\theta = \begin{bmatrix}\theta_0 \newline<br>\theta_1 \newline<br>\end{bmatrix}<br>\end{align}<br>$</p>
</blockquote>
<p>You can calculate the hypothesis as a column vector of size (m x 1) with:</p>
<blockquote>
<p>$<br>h_\theta(X) = X \theta<br>$</p>
<p>For the rest of these notes,  X will represent a matrix of training examples $x_{(i)}$ </p>
</blockquote>
<h2 id="2-Cost-function"><a href="#2-Cost-function" class="headerlink" title="2. Cost function"></a>2. Cost function</h2><p>For the parameter vector θ (of type $\mathbb{R}^{n+1}$ or in $\mathbb{R}^{(n+1) \times 1}$, the cost function is:</p>
<p>$<br>J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (x^{(i)}) - y^{(i)} \right)^2<br>$</p>
<p><code>The vectorized version is:</code></p>
<p>$<br>J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})<br>$</p>
<blockquote>
<p>vectorized version is very good!</p>
</blockquote>
<h2 id="3-Gradient-Desc-Multivariable"><a href="#3-Gradient-Desc-Multivariable" class="headerlink" title="3. Gradient Desc Multivariable"></a>3. Gradient Desc Multivariable</h2><p><code>Matrix Notation</code></p>
<p>The Gradient Descent rule can be expressed as:</p>
<p>$<br>\theta := \theta - \alpha \nabla J(\theta)<br>$</p>
<p>Where $\nabla J(\theta)$ is a column vector of the form:</p>
<p>$<br>\nabla J(\theta)  = \begin{bmatrix}\frac{\partial J(\theta)}{\partial \theta_0}   \newline \frac{\partial J(\theta)}{\partial \theta_1}   \newline \vdots   \newline \frac{\partial J(\theta)}{\partial \theta_n} \end{bmatrix}<br>$</p>
<p>The j-th component of the gradient is the summation of the product of two terms:</p>
<p>$<br>\begin{align}<br>\; &amp;\frac{\partial J(\theta)}{\partial \theta_j} &amp;=&amp;  \frac{1}{m} \sum\limits_{i=1}^{m}  \left(h_\theta(x^{(i)}) - y^{(i)} \right) \cdot x_j^{(i)} \newline<br>\; &amp; &amp;=&amp; \frac{1}{m} \sum\limits_{i=1}^{m}   x_j^{(i)} \cdot \left(h_\theta(x^{(i)}) - y^{(i)}  \right)<br>\end{align}<br>$</p>
<blockquote>
<p>在数学中，一个多变量的函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定。</p>
</blockquote>
<p>Sometimes, the summation of the product of two terms can be expressed as the product of two vectors.</p>
<blockquote>
<p>$<br>\begin{align}\; &amp;\frac{\partial J(\theta)}{\partial \theta_j} = \frac1m  \vec{x_j}^{T} (X\theta - \vec{y}) \newline<br>&amp;\nabla J(\theta)  =  \frac 1m X^{T} (X\theta - \vec{y}) \newline<br>\end{align}<br>$</p>
</blockquote>
<p>Finally, the matrix notation (vectorized) of the Gradient Descent rule is:</p>
<p>$<br>\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})<br>$</p>
<p>The gradient descent equation itself is generally the same form; we just have to repeat it for our ‘n’ features:</p>
<p>$<br>\begin{align}<br>&amp; \text{repeat until convergence:} \; \lbrace \newline<br>\; &amp; \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}  (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}\newline<br>\; &amp; \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \newline<br>\; &amp; \theta_2 := \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \newline<br>&amp; \cdots<br>\newline \rbrace<br>\end{align}<br>$</p>
<p>In other words:</p>
<p>$<br>\begin{align}<br>&amp; \text{repeat until convergence:} \; \lbrace \newline \;<br>&amp; \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \;  &amp; \text{for j := 0..n}<br>\newline \rbrace<br>\end{align}<br>$</p>
<h3 id="3-1-Feature-Scaling"><a href="#3-1-Feature-Scaling" class="headerlink" title="3.1 Feature Scaling"></a>3.1 Feature Scaling</h3><p>Idea : Make sure features are on a similar scale 特征缩放</p>
<p><img src="/images/ml/coursera/ml-ng-w2-02.png" alt="Multiple Features"></p>
<blockquote>
<p>Get every feature into approximately a $-1 \leq x_i \leq 1$ range.</p>
<p>Replace $x_i$ with $x_i - u_i$ to make features have approximately zero mean (Do not apply to $x_0$ = 1).</p>
<p>如果多个特征值，大多处在一个相近的范围，梯度下降就能更快的收敛。</p>
</blockquote>
<p>因为 2000/5 比较大，所以轮廓图，使得椭圆更加的瘦长，好比 $J(\theta)$ 收敛的更慢。</p>
<h3 id="3-2-learning-rate"><a href="#3-2-learning-rate" class="headerlink" title="3.2 learning rate"></a>3.2 learning rate</h3><p>$<br>\begin{align}<br>\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)<br>\end{align}<br>$</p>
<ul>
<li>Debugging : How to make sure gradient descent is working correctly</li>
<li>How to choose learning rate $\alpha$</li>
</ul>
<p><img src="/images/ml/coursera/ml-ng-w2-03.png" alt="Multiple Features"></p>
<p><strong>Summary</strong></p>
<ul>
<li>if $\alpha$ is too small: slow convergence [kən’vɜːdʒəns] 收敛</li>
<li>if $\alpha$ is too large: $J(\theta)$ may not decrease on every iteration; may not converge.</li>
</ul>
<p>To choose $\alpha$, try</p>
<p>…, 0.001, 0.01, 0.1, 1, …</p>
<h2 id="4-Polynomial-Regression"><a href="#4-Polynomial-Regression" class="headerlink" title="4. Polynomial Regression"></a>4. Polynomial Regression</h2><p><img src="/images/ml/coursera/ml-ng-w2-04.png" alt="Polynomial"></p>
<h3 id="4-1-Polynomial-Regression"><a href="#4-1-Polynomial-Regression" class="headerlink" title="4.1 Polynomial Regression"></a>4.1 Polynomial Regression</h3><p><img src="/images/ml/coursera/ml-ng-w2-05.png" alt="Polynomial [,pɒlɪ&#39;nəʊmɪəl]"></p>
<blockquote>
<p>Feature normalization is very important</p>
</blockquote>
<h3 id="4-2-Choice-of-features"><a href="#4-2-Choice-of-features" class="headerlink" title="4.2 Choice of features"></a>4.2 Choice of features</h3><p><img src="/images/ml/coursera/ml-ng-w2-06.png" alt="Choice of features"></p>
<p>@2017-02-10 review done</p>
<h2 id="5-Normal-Equation"><a href="#5-Normal-Equation" class="headerlink" title="5. Normal Equation"></a>5. Normal Equation</h2><p><img src="/images/ml/coursera/ml-ng-w2-07.png" alt="Normal Equation"></p>
<blockquote>
<p>$\theta = (X^T X)^{-1}X^T y$</p>
</blockquote>
<h3 id="5-1-num-and-vector"><a href="#5-1-num-and-vector" class="headerlink" title="5.1 num and vector"></a>5.1 num and vector</h3><p><img src="/images/ml/coursera/ml-ng-w2-08.png" alt="Normal Equation"></p>
<blockquote>
<p>$<br>J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})<br>$</p>
<p>$<br>\begin{align}\; &amp;\frac{\partial J(\theta)}{\partial \theta_j} = \frac1m  \vec{x_j}^{T} (X\theta - \vec{y}) \newline<br>&amp;\nabla J(\theta)  =  \frac 1m X^{T} (X\theta - \vec{y}) \newline<br>\end{align}<br>$</p>
</blockquote>
<h3 id="5-2-house-price-example"><a href="#5-2-house-price-example" class="headerlink" title="5.2 house price example"></a>5.2 house price example</h3><p><img src="/images/ml/coursera/ml-ng-w2-09.png" alt="Normal Equation"></p>
<blockquote>
<p>$<br>\begin{align}<br>\nabla J(\theta)  =  \frac 1m X^{T} (X\theta - \vec{y}) \newline<br>\end{align}<br>$</p>
<p>令 $\nabla J(\theta)  =  0 $</p>
<p>So, $\theta = (X^T X)^{-1}X^T y $</p>
</blockquote>
<h3 id="5-3-m-training-n-features"><a href="#5-3-m-training-n-features" class="headerlink" title="5.3 $m$ training, $n$ features"></a>5.3 $m$ training, $n$ features</h3><table>
<thead>
<tr>
<th>Gradient Descent</th>
<th>Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Need to choose $\alpha$</td>
<td>No need to choose $\alpha$ </td>
</tr>
<tr>
<td>Needs many iterations</td>
<td>Don’t need to iterate</td>
</tr>
<tr>
<td>Works well even when $n$ is large</td>
<td>Slow if $n$ is very large</td>
</tr>
</tbody>
</table>
<blockquote>
<p>it is usually around ten thousand that I might start to consider switching over to gradient descents or maybe, some other algorithms that we’ll talk about later in this class</p>
</blockquote>
<h3 id="5-4-X-T-X-is-non-invertible"><a href="#5-4-X-T-X-is-non-invertible" class="headerlink" title="5.4 $X^T X$ is non-invertible"></a>5.4 $X^T X$ is non-invertible</h3><p> $\theta = (X^T X)^{-1}X^T y $</p>
<p>What $X^T X$ is non-invertible? （singular / degenerate）</p>
<blockquote>
<p>When $X^T X$ is non-invertible, this is very few.</p>
</blockquote>
<p><strong>What $X^T X$ is non-invertible?</strong></p>
<p><img src="/images/ml/coursera/ml-ng-w2-10.png" alt="non-invertible"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera Week 1 - Linear Algebra Matrices And Vectors]]></title>
      <url>http://sggo.me/2016/09/30/ml/coursera-ng-w1-03-Linear-Algebra/</url>
      <content type="html"><![CDATA[<p>coursera week 1 - <code>matrices</code> and <code>vectors</code></p>
<a id="more"></a>
<h2 id="1-Matrix-Elements"><a href="#1-Matrix-Elements" class="headerlink" title="1. Matrix Elements"></a>1. Matrix Elements</h2><p>$$<br>A =<br>\begin{bmatrix}<br>   1 &amp; 2 \\<br>   4 &amp; 5 \\<br>   7 &amp; 8<br>  \end{bmatrix} \tag{fmt.1  R^{32}}<br>$$</p>
<p>$ A_{ij} = $ “$i, j$ entry”  in the $i^{th}$ row, $j^{th}$ column</p>
<p>$ A_{32} = 8 $</p>
<h2 id="2-Vector-A-n-n-1-matrix"><a href="#2-Vector-A-n-n-1-matrix" class="headerlink" title="2. Vector $A_n$ n*1 matrix"></a>2. Vector $A_n$ n*1 matrix</h2><p>$$<br>y =<br>\begin{bmatrix}<br>   460 \\<br>   232 \\<br>   315 \\<br>   178<br>  \end{bmatrix} \tag{fmt.2}<br>$$</p>
<blockquote>
<p>$R^4$ 4 dimensional vector<br>$y_i = i^{th} element$</p>
</blockquote>
<h3 id="2-1-math-1-indexed"><a href="#2-1-math-1-indexed" class="headerlink" title="2.1 math 1-indexed"></a>2.1 math 1-indexed</h3><p>$$<br>y =<br>\begin{bmatrix}<br>   y1 \\<br>   y2 \\<br>   y3 \\<br>   y4<br>  \end{bmatrix} \tag{fmt.3}<br>$$</p>
<h3 id="2-2-machine-learning-0-indexed"><a href="#2-2-machine-learning-0-indexed" class="headerlink" title="2.2 machine-learning 0-indexed"></a>2.2 machine-learning 0-indexed</h3><p>$$<br>y =<br>\begin{bmatrix}<br>   y0 \\<br>   y1 \\<br>   y2 \\<br>   y3<br>  \end{bmatrix} \tag{fmt.4}<br>$$</p>
<h2 id="3-Matrix-Addition"><a href="#3-Matrix-Addition" class="headerlink" title="3. Matrix Addition"></a>3. Matrix Addition</h2><p>$$ \begin{bmatrix} 1 &amp; 0 \\ 2 &amp; 5 \\ 3 &amp; 1 \end{bmatrix} + \begin{bmatrix} 4 &amp; 0.5 \\ 2 &amp; 5 \\ 0 &amp; 1 \end{bmatrix} =<br>\begin{bmatrix}<br>   5 &amp; 0.5 \\<br>   4 &amp; 10 \\<br>   3 &amp; 2<br> \end{bmatrix} $$</p>
<h2 id="4-Scalar-Multiplication"><a href="#4-Scalar-Multiplication" class="headerlink" title="4. Scalar Multiplication"></a>4. Scalar Multiplication</h2><p>$$ 3 \times \begin{bmatrix} 1 &amp; 0 \\ 2 &amp; 5 \\ 3 &amp; 1 \end{bmatrix}<br>= \begin{bmatrix}<br>   3 &amp; 0 \\<br>   6 &amp; 15 \\<br>   9 &amp; 3<br> \end{bmatrix}<br>$$</p>
<h2 id="5-Combination-of-Operands"><a href="#5-Combination-of-Operands" class="headerlink" title="5. Combination of Operands"></a>5. Combination of Operands</h2><p>$$ 3 \times<br>\begin{bmatrix}<br>1 \\<br>4 \\<br>2<br>\end{bmatrix}<br>+<br>\begin{bmatrix}<br>0 \\<br>0 \\<br>5<br>\end{bmatrix} -<br>\begin{bmatrix}<br>   3 \\<br>   0 \\<br>   2<br> \end{bmatrix} / 3 =<br>\begin{bmatrix}<br>2 \\<br>12 \\<br>31/3<br>\end{bmatrix}<br>$$</p>
<h2 id="6-Matrix-Vector-Multiplication"><a href="#6-Matrix-Vector-Multiplication" class="headerlink" title="6. Matrix Vector Multiplication"></a>6. Matrix Vector Multiplication</h2><p>$$<br>\begin{bmatrix}<br>1 &amp; 3 \\<br>4 &amp; 0 \\<br>2 &amp; 1<br>\end{bmatrix}<br>\times<br>\begin{bmatrix}<br>1 \\<br>5<br>\end{bmatrix} =<br>\begin{bmatrix}<br>16 \\<br>4 \\<br>7<br>\end{bmatrix}<br>$$</p>
<p><strong>Matrix Vector Multiplication Fmt :</strong></p>
<p><img src="/images/ml/coursera/ml-ng-w1-03-1.png" alt="Matrix Vector"></p>
<h3 id="6-1-House-sizes-example"><a href="#6-1-House-sizes-example" class="headerlink" title="6.1 House sizes example"></a>6.1 House sizes example</h3><p>$$<br>h_{\theta}  (x) = -40 + 0.25 x<br>$$</p>
<table>
<thead>
<tr>
<th>House sizes</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>2104</td>
<td>?</td>
</tr>
<tr>
<td>1416</td>
<td>?</td>
</tr>
<tr>
<td>1534</td>
<td>?</td>
</tr>
<tr>
<td>852</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>$$ \begin{bmatrix}<br>1 &amp; 2104 \\<br>1 &amp; 1416 \\<br>1 &amp; 1534 \\<br>1 &amp; 852<br>\end{bmatrix} \times<br>\begin{bmatrix}<br>-40 \\<br>0.25<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>-40 \times 1 + 0.25 \times 2104 \\<br>… \\<br>… \\<br>…<br>\end{bmatrix}<br>$$</p>
<h2 id="7-Practice-Example"><a href="#7-Practice-Example" class="headerlink" title="7. Practice Example"></a>7. Practice Example</h2><p>$$ \begin{bmatrix}<br>1 &amp; 3 &amp; 2 \\<br>4 &amp; 0 &amp; 1<br>\end{bmatrix}<br>\begin{bmatrix}<br>1 &amp; 3 \\<br>0 &amp; 1 \\<br>5 &amp; 2<br>\end{bmatrix} =<br>\begin{bmatrix}<br>   11 &amp; 10 \\<br>   9 &amp; 14<br> \end{bmatrix} $$</p>
<blockquote>
<p>$ A_{2 \times 3} \times A_{3 \times 2} = A_{2 \times 2} $</p>
</blockquote>
<p><img src="/images/ml/coursera/ml-ng-w1-03-2.png" alt="Matrix"></p>
<h2 id="8-House-Example"><a href="#8-House-Example" class="headerlink" title="8. House Example"></a>8. House Example</h2><p><img src="/images/ml/coursera/ml-ng-w1-03-3.png" alt="Matrix"></p>
<h2 id="9-Matrix-A-times-B-neq-B-times-A"><a href="#9-Matrix-A-times-B-neq-B-times-A" class="headerlink" title="9. Matrix $A \times B \neq B \times A$"></a>9. Matrix $A \times B \neq B \times A$</h2><p>But, 结合律，可以的</p>
<p>$ A \times B \times C = (A \times B) \times C = A \times (B \times C) $</p>
<h2 id="10-Identity-Matrix"><a href="#10-Identity-Matrix" class="headerlink" title="10. Identity Matrix"></a>10. Identity Matrix</h2><p>Denoted I (or I_{n*n}).</p>
<h3 id="10-1-2-times-2"><a href="#10-1-2-times-2" class="headerlink" title="10.1 $2 \times 2$"></a>10.1 $2 \times 2$</h3><p>$$<br>\begin{bmatrix}<br>   1 &amp; 0 \\<br>   0 &amp; 1<br>  \end{bmatrix} \tag{fmt.1  R^{32}}<br>$$</p>
<h3 id="10-2-3-times-3"><a href="#10-2-3-times-3" class="headerlink" title="10.2 $3 \times 3$"></a>10.2 $3 \times 3$</h3><p>$$<br>\begin{bmatrix}<br>   1 &amp; 0 &amp; 0 \\<br>   0 &amp; 1 &amp; 0 \\<br>   0 &amp; 0 &amp; 1<br>  \end{bmatrix} \tag{fmt.1  R^{32}}<br>$$</p>
<blockquote>
<p>$Z \times I = I \times Z = Z$</p>
</blockquote>
<h2 id="11-Matrix-Inverse"><a href="#11-Matrix-Inverse" class="headerlink" title="11. Matrix Inverse"></a>11. Matrix Inverse</h2><p>$3 \times 3^{-1} = 1$</p>
<blockquote>
<p>Not all numbers have an inverse.</p>
</blockquote>
<p>if $A$ is an $m \times m$ matrix, and if it has an inverse</p>
<p>$A \times A^{-1} = A^{-1} \times A = I$</p>
<p>$$<br>A =<br>\begin{bmatrix}<br>   3 &amp; 4 \\<br>   2 &amp; 16 \\<br>  \end{bmatrix}<br>$$</p>
<p>$$<br>A^{-1} =<br>\begin{bmatrix}<br>   0.4 &amp; -0.1 \\<br>   -0.05 &amp; 0.075 \\<br>  \end{bmatrix}<br>$$</p>
<p>$ A \times A^{-1} = I_{2 \times 2} $</p>
<p>$$<br>I_{2 \times 2} =<br>\begin{bmatrix}<br>   1 &amp; 0 \\<br>   0 &amp; 1 \\<br>  \end{bmatrix}<br>$$</p>
<h2 id="12-Matrix-Transpose"><a href="#12-Matrix-Transpose" class="headerlink" title="12. Matrix Transpose"></a>12. Matrix Transpose</h2><p>$$<br>A =<br>\begin{bmatrix}<br>   1 &amp; 2 &amp; 0 \\<br>   3 &amp; 5 &amp; 9 \\<br>  \end{bmatrix}<br>$$</p>
<p>$$<br>A^T =<br>\begin{bmatrix}<br>   1 &amp; 3 \\<br>   2 &amp; 5 \\<br>   0 &amp; 9<br>  \end{bmatrix}<br>$$</p>
<p>Let $A$ be an $m \times n$ matrix, and let $B = A^T$.<br>Then $B$ is an $n \times m$ matrix, and $B_{ij} = A_{ji}$</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera Week 1 - Linear Regression Cost Function & Gradient descent]]></title>
      <url>http://sggo.me/2016/09/28/ml/coursera-ng-w1-02-cost-function-gradient-descent/</url>
      <content type="html"><![CDATA[<p>Linear Regression Cost Function &amp; Gradient descent</p>
<a id="more"></a>
<h2 id="1-Linear-Regression"><a href="#1-Linear-Regression" class="headerlink" title="1. Linear Regression"></a>1. Linear Regression</h2><p><img src="/images/ml/coursera/ml-ng-w1-02-1.png" alt="How to choose parameters"></p>
<h2 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h2><blockquote>
<p>Choose $\theta_0，\theta_1$ so that $h_{\theta} (x) $ is close to $y$ for our training examples ${(x, y)}$</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">Title</th>
<th style="text-align:center">fmt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Hypothesis</td>
<td style="text-align:center">$h_{\theta}  (x) = \theta_0 + \theta_1 x$</td>
</tr>
<tr>
<td style="text-align:center">Parameters</td>
<td style="text-align:center">$\theta_0 、\theta_1$</td>
</tr>
<tr>
<td style="text-align:center">Cost Function</td>
<td style="text-align:center">$J(\theta_0，\theta_1) = {\frac {1} {2m}} \sum_{i=1}^m (h_{\theta} (x^{i}) - (y^{i}))^2$</td>
</tr>
<tr>
<td style="text-align:center">Goal</td>
<td style="text-align:center">$minimize J(\theta_0，\theta_1)$</td>
</tr>
</tbody>
</table>
<h2 id="3-Simplified-Fmt"><a href="#3-Simplified-Fmt" class="headerlink" title="3. Simplified Fmt"></a>3. Simplified Fmt</h2><blockquote>
<p>$\theta_0$ = 0</p>
</blockquote>
<p><strong>hypothesis function $h_{\theta} (x)$  cost function $J(\theta_1)$</strong></p>
<p><img src="/images/ml/coursera/ml-ng-w1-02-3.png" alt="cost"></p>
<h2 id="4-Cost-function-visable"><a href="#4-Cost-function-visable" class="headerlink" title="4. Cost function visable"></a>4. Cost function visable</h2><p><img src="/images/ml/coursera/ml-ng-w1-02-4.png" alt="cost"></p>
<blockquote>
<p>把 x, y 想象成向量，确定的向量，向量再想象为一个确定的数，总之它是一个二次函数，抽象的想一下，会不会理解</p>
</blockquote>
<ul>
<li>contour plots</li>
<li>contour figures</li>
</ul>
<p><img src="/images/ml/coursera/ml-ng-w1-02-5.png" alt="cost"></p>
<h2 id="5-Gradient-descent-target"><a href="#5-Gradient-descent-target" class="headerlink" title="5. Gradient descent target"></a>5. Gradient descent target</h2><p><img src="/images/ml/coursera/ml-ng-w1-02-6.png" alt="Gradient descent"></p>
<h2 id="6-Gradient-descent-visable"><a href="#6-Gradient-descent-visable" class="headerlink" title="6. Gradient descent visable"></a>6. Gradient descent visable</h2><p><img src="/images/ml/coursera/ml-ng-w1-02-7.png" alt="Local optimization"></p>
<p><strong>Convex function</strong></p>
<p><img src="/images/ml/coursera/ml-ng-w1-02-8.png" alt="Global optimization"></p>
<h2 id="7-Gradient-descent-algorithm"><a href="#7-Gradient-descent-algorithm" class="headerlink" title="7. Gradient descent algorithm"></a>7. Gradient descent algorithm</h2><blockquote>
<p>$ \alpha $ : learning rate</p>
</blockquote>
<p><img src="/images/ml/coursera/ml-ng-w1-02-9.png" alt="Gradient descent"></p>
<h2 id="8-Gradient-descent-only-theta-1"><a href="#8-Gradient-descent-only-theta-1" class="headerlink" title="8. Gradient descent only $ \theta_{1} $"></a>8. Gradient descent only $ \theta_{1} $</h2><p><img src="/images/ml/coursera/ml-ng-w1-02-10.png" alt="Gradient descent for one param : $ \theta\_{1} $"></p>
<p><img src="/images/ml/coursera/ml-ng-w1-02-11.png" alt="Gradient descent"></p>
<p><img src="/images/ml/coursera/ml-ng-w1-02-12.png" alt="Gradient descent"></p>
<p><img src="/images/ml/coursera/ml-ng-w1-02-13.png" alt="Gradient descent"></p>
<h2 id="9-Linear-Regression-Model"><a href="#9-Linear-Regression-Model" class="headerlink" title="9. Linear Regression Model"></a>9. Linear Regression Model</h2><p><img src="/images/ml/coursera/ml-ng-w1-02-14.png" alt="Gradient descent"></p>
<h3 id="9-1-Batch-Gradient-Descent"><a href="#9-1-Batch-Gradient-Descent" class="headerlink" title="9.1 Batch Gradient Descent"></a>9.1 Batch Gradient Descent</h3><blockquote>
<p>Batch : Each step of gradient descent uses all the training examples</p>
</blockquote>
<p><img src="/images/ml/coursera/ml-ng-w1-02-15.png" alt="Gradient descent"></p>
<blockquote>
<p>Coursera Learning Notes</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Coursera Week 1 - Machine Learning Introduction]]></title>
      <url>http://sggo.me/2016/09/20/ml/coursera-ng-w1-01-introduce/</url>
      <content type="html"><![CDATA[<p>Machine-learning, Grew out of work in Artificial Intelligence, New capability for computers</p>
<a id="more"></a>
<h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><ul>
<li>Grew out of work in Artificial Intelligence</li>
<li>New capability for computers</li>
</ul>
<blockquote>
<p>search engine, recommendation system, image recognition</p>
<p>web click data, medical records , biology, engineering</p>
<p>Natural Language Processing (NLP), Computer Vision</p>
</blockquote>
<p><strong>Machine Learning definition</strong></p>
<p>Field of study that gives computers the ability to learn without being explicitly programmed. by ArthurSamuel(1959)</p>
<h2 id="1-Supervised-learning"><a href="#1-Supervised-learning" class="headerlink" title="1. Supervised learning"></a>1. Supervised learning</h2><p><img src="/images/ml/coursera/ml-ng-w1-01-1.png" alt="Supervised"></p>
<h2 id="2-Regression-amp-Classification"><a href="#2-Regression-amp-Classification" class="headerlink" title="2. Regression &amp; Classification"></a>2. Regression &amp; Classification</h2><p><img src="/images/ml/coursera/ml-ng-w1-01-2.png" alt="Classification"></p>
<h2 id="3-Unsupervised-learning"><a href="#3-Unsupervised-learning" class="headerlink" title="3. Unsupervised learning"></a>3. Unsupervised learning</h2><p><img src="/images/ml/coursera/ml-ng-w1-01-3.png" alt="Unsupervised"></p>
<p><strong>Unsupervised Examples</strong></p>
<p><img src="/images/ml/coursera/ml-ng-w1-01-4.png" alt="news.google"></p>
<blockquote>
<p>What Google News does is everyday it goes and looks at tens of thousands or hundreds of thousands of new stories on the web and it groups them into cohesive news stories. </p>
</blockquote>
<h2 id="4-Experience"><a href="#4-Experience" class="headerlink" title="4. Experience"></a>4. Experience</h2><p><strong>Xiaoyang 语录</strong> :</p>
<p>『解决一个问题的方法和思路不止一种』<br>『没有所谓的机器学习算法优劣，也没有绝对高性能的机器学习算法，只有在特定的场景、数据和特征下更合适的机器学习算法。』</p>
<p><strong>Andrew Ng 语录</strong></p>
<p>应用机器学习，不要一上来就试图做到完美，先lu一个baseline的model出来，再进行后续的分析步骤，一步步提高，所谓后续步骤可能包括『分析model现在的状态(欠/过拟合)，分析我们使用的feature的作用大小，进行feature selection，以及我们模型下的bad case和产生的原因』等等。</p>
<p><strong>Kaggle大神们 experience 总结</strong> ：</p>
<ol>
<li>『对数据的认识太重要了！』</li>
<li>『数据中的特殊点/离群点的分析和处理太重要了！』</li>
<li>『特征工程(feature engineering)太重要了！在很多Kaggle的场景下，甚至比model本身还要重要』</li>
<li>『要做模型融合(model ensemble)啊啊啊！』</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java程序员需要知道的计算机原理 (not finish)]]></title>
      <url>http://sggo.me/2016/09/13/java/java-special-arms-p2-computer-principle/</url>
      <content type="html"><![CDATA[<p>Java特种兵 - Java程序员需要知道的计算机原理，Reading Notes</p>
<a id="more"></a>
<h2 id="1-计算机原理"><a href="#1-计算机原理" class="headerlink" title="1. 计算机原理"></a>1. 计算机原理</h2><p>计算机总体体系结构的变化，一直不是特别大，基础原理将引导我们从整体上认识计算机本身。</p>
<h2 id="2-CPU"><a href="#2-CPU" class="headerlink" title="2. CPU"></a>2. CPU</h2><p>每个进程 or 线程 发出请求, 最后会由 CPU 来分配时间片处理，处理时 操作数 传递给 CPU, CPU 计算将其回写到本地变量。这个本地变量通常会存在程序所谓的 栈 中，多次对其操作，它可能会被 Cache 到 CPU 的缓存之中。CPU 有 寄存器，一级缓存，二级缓存 … , 其实设计这些组件就是为了那四个字 <code>就近原则</code>。</p>
<h3 id="2-1-Cpu-联系-Java"><a href="#2-1-Cpu-联系-Java" class="headerlink" title="2.1 Cpu 联系 Java"></a>2.1 Cpu 联系 Java</h3><blockquote>
<p>在 编译阶段，Java 就可以决定方法的 LocalVariable 的个数，在方法调用的时候，就可直接分配一个 LocalVariable 区域，这个空间是基于 slot 来分配的，每个 slot 占用 32 bit，boolean 占用 1 slot，long and double 占 2 个 slot。</p>
</blockquote>
<p>LocalVariableTable :</p>
<table>
<thead>
<tr>
<th>Start</th>
<th>Length</th>
<th>Slot</th>
<th>Name</th>
<th>Signature</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>9</td>
<td>0</td>
<td>args</td>
<td>Ljava/lang/String;</td>
</tr>
<tr>
<td>2</td>
<td>7</td>
<td>1</td>
<td>a</td>
<td>I</td>
</tr>
<tr>
<td>..</td>
<td>..</td>
<td>..</td>
<td>..</td>
<td>..</td>
</tr>
</tbody>
</table>
<ul>
<li>Start :  代表LocalVariable在虚指令作用域的起始位置</li>
<li>Length : 代表LocalVariable在虚指令作用域的长度(如第1个本地变量args是9条指令的作用域)</li>
</ul>
<h3 id="2-2-多核"><a href="#2-2-多核" class="headerlink" title="2.2 多核"></a>2.2 多核</h3><p>为了多个计算中心同时做事情，增加效率。</p>
<p><strong>考虑需要和解决的问题</strong></p>
<p>  一个指令来了，哪个Cpu来处理？ 同一份数据被多个 Cpu 处理，如何协调并让其他Cpu都知道。<br>  当发起一个计算请求，例如一个中断，这么多 Cpu 会干什么？</p>
<blockquote>
<p>中断 : 指当出现需要时，CPU暂时停止当前程序的执行转而执行处理新情况的程序和执行过程。</p>
</blockquote>
<p>Cpu 的计算速度非常快，OS不希望它等待或者停止，所以在出现 I/O 等待 (网络I/O 和 磁盘I/O), 它中途基本不参与，而以事件注册的方式来实现回调，对于某些执行时间长的task，Cpu会分配一些时间片执行其他的task.</p>
<blockquote>
<p>当 Cpu 不断去切换 task 处理时，这就会涉及到 <code>上下文切换</code>.</p>
</blockquote>
<h3 id="2-3-Cache-line"><a href="#2-3-Cache-line" class="headerlink" title="2.3 Cache line"></a>2.3 Cache line</h3><p>办事就近原则，可以一次办多件事情 或 一件事情的多个步骤可以一次办完。</p>
<blockquote>
<p>Cache line 就是 将 连续的一段内存区域 进行 Cache，不是每次就 Cache 一个内存单元，而是一系列内存单元。计算机中，通常以连续 64bit 进行Cache。</p>
</blockquote>
<p>感悟 : Cache Line 就是 一次拿一批信息去处理。 Cache line 的目的是为了 快速访问。</p>
<h3 id="2-4-缓存一致性协议"><a href="#2-4-缓存一致性协议" class="headerlink" title="2.4 缓存一致性协议"></a>2.4 缓存一致性协议</h3><p>当有 来自于 内存 中的同一份数据 Cache 在多个 CPU 中，且要求这些数据的读写一致时，多个 CPU 之间就需要遵循缓存共享的一致性原则。</p>
<blockquote>
<p>相当于大家都来修改一份设计报告，大家都拷贝了一份，回家修改，每个人修改要及时让其他人都知道。有点像版本控制</p>
</blockquote>
<table>
<thead>
<tr>
<th>内存单元的状态</th>
</tr>
</thead>
<tbody>
<tr>
<td>Modified</td>
<td>修改</td>
</tr>
<tr>
<td>Exclusive</td>
<td>独占</td>
</tr>
<tr>
<td>Shared</td>
<td>共享</td>
</tr>
<tr>
<td>Invalid</td>
<td>失效</td>
</tr>
</tbody>
</table>
<p>多个 CPU 通过总线相互连接，每个 CPU 的cache处理器 要响应本身所对应的CPU读写操作外，还需要监听总线上其他CPU操作，通过监听对自己的Cache做处理，形成虚共享，这个协议叫做 MESI 协议。</p>
<blockquote>
<p>一个数据修改了，它需要告诉其他 CPU 这份数据被修改了，现在Intel通过 QPI 来完成。不同CPU之间交互需要时间 20~40 ns 级别。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VolatileInteger</span> </span>&#123;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="keyword">int</span> number;</span><br><span class="line">&#125;</span><br><span class="line">VolatileInteger[] values = <span class="keyword">new</span> VolatileInteger[<span class="number">10</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">  values[i] = <span class="keyword">new</span> VolatileInteger();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的例子，很可能使的每个CPU可能只修改到某个元素，但会有大量 QPI 存在。</p>
<blockquote>
<p> QPI : Quick Path Interconnect</p>
</blockquote>
<h3 id="2-5-Context-switch"><a href="#2-5-Context-switch" class="headerlink" title="2.5 Context switch"></a>2.5 Context switch</h3><p>线程已经执行了一部分内容，需要记录下它的内容和状态，中途由于调度算法。</p>
<p>CPU 调度 的最基本单位是线程，Java也是基于多线程模式。由于多线程模型中多个线程共享进程的资源，所以Java程序，如某一个线程占用资源过大时，就可能导致整个JVM进程挂掉。(影响都是相对的)</p>
<blockquote>
<p>在实际运行中会有代码段和数据段，内容切换时要保存这些运行中的上下文信息，再使用的时候，再加载回来。</p>
<p>日志写操作 (如 : log4j) 都采用 <code>异步</code> 模式 实现，而程序通常不直接参与这个过程。 实现方式 (日志写操作只是将日志写入一个消息队列中，由单独的线程来完成写操作)</p>
</blockquote>
<h3 id="2-6-并发与争用"><a href="#2-6-并发与争用" class="headerlink" title="2.6 并发与争用"></a>2.6 并发与争用</h3><p>只要是服务器端程序，迟早会遇到并发。当 并发 时，就会存在对各种资源的争用，包括对各个部件(如CPU)的争用。</p>
<blockquote>
<p>Web 程序也会经常遇到并发问题的，编写者，没有遇到是因为 Web 容器帮助处理好了线程的本地变量分配，我们几乎不用关注并发。</p>
</blockquote>
<h4 id="2-6-1-临界区"><a href="#2-6-1-临界区" class="headerlink" title="2.6.1 临界区"></a>2.6.1 临界区</h4><blockquote>
<p>当程序出现 “加锁” 时 (如 Java的 synchronized) 说明这一块是临界区，只允许一个线程访问，其他线程来回进行等待队列。<br>争用带来的是同步的开销，它会发出许多指令要求所有CPU处理中不允许其他线程进入临界区，且需要将等待线程放入队列阻塞。<br>争用 CPU 的访问也不仅仅体现在锁上面，CPU本身数量也有限。 单个CPU会对任务进行 基于 时间片、优先级、任务大小分别调度。</p>
</blockquote>
<h4 id="2-6-2-线程池数"><a href="#2-6-2-线程池数" class="headerlink" title="2.6.2 线程池数"></a>2.6.2 线程池数</h4><p>在理想的 CPU 密集型系统，线程数是 CPU数+1 / CPU-1</p>
<ul>
<li>系统CPU密集度</li>
</ul>
<p>一般系统分为 计算密集型 和 I/O 密集型。</p>
<blockquote>
<p>系统中关键程序访问总共花费 120ms, I/O操作 占用 100ms，100ms时间内 CPU是可以被其他线程访问的。此时，这个程序在单核系统中的线程数理论上可以设置为 6， 在多核系统就是乘以CPU个数 左右这个数字。</p>
</blockquote>
<p>一般这个线程数的决定是通过测试的。</p>
<h4 id="2-6-3-锁"><a href="#2-6-3-锁" class="headerlink" title="2.6.3 锁"></a>2.6.3 锁</h4><p><code>锁</code> 就是临界区的范围，有粒度。如果在 锁  内部发送 I/O, 大循环、递归等，那么就必须等待这些操作处理完成后才能有下一个线程进入处理。如果这段程序是 <code>关键程序</code>, 当系统真正并发的时候，很多线程都会阻塞在这里。这时要计算线程数，要看锁对象 是不是静态对象或Class, (如果是，则是一个JVM全局锁)，无论配置多少线程效果都一样。<code>锁是全局的，无论多少个CPU也是无济于事的。</code></p>
<blockquote>
<p>结论 : 锁尽量不要设置为 全局锁，能用粒度控制，尽量粒度控制</p>
</blockquote>
<h4 id="2-6-4-JVM自身调节"><a href="#2-6-4-JVM自身调节" class="headerlink" title="2.6.4 JVM自身调节"></a>2.6.4 JVM自身调节</h4><p>不论 CPU 跑多快，如 JVM hold不住节奏，不断做GC，那么如何配置线程池，系统性能还是上不来。</p>
<p>可以根据 JVM 运行日志中，平均做 Young GC 的时间间隔 (通过 Young GC 与 运行时长对比)，以及系统的QPS，来估算每个请求大致占用的内存大小，有时不准，但具有参考价值。</p>
<p><strong>如何计算</strong></p>
<blockquote>
<p>Eden 空间的大小我们是知道的。通常一个请求分配空间都在 Eden 区域，Eden区域满发生Young GC。Young GC 时间间隔就是Eden满的时间间隔，例如 3s， 进一步通过 QPS*3 得到多少个请求可以填充满 Eden 区域。这可初步估算每个请求占用的内存空间。</p>
</blockquote>
<h2 id="3-内存"><a href="#3-内存" class="headerlink" title="3. 内存"></a>3. 内存</h2><p>基本所有的程序猿与程序媛都知道，它是跑程序的地方。</p>
<p>磁盘存储 与 CPU 之间的桥梁。拥有比 CPU缓存 大几百倍、上千倍的空间。CPU三级缓存也就 几十M。</p>
<blockquote>
<p>磁盘 到达 CPU需要经过 (主板-南桥、北桥) 才能到达CPU。很慢。</p>
</blockquote>
<p>内存的容量都是 GB 单位，大量程序运行都依赖内存，又 OS 来管理和调度。</p>
<blockquote>
<p>地址位数、逻辑地址、虚拟地址、物理地址、线性地址、内核区域等，很多人看到这，直接疯了，但是你需要淡定。</p>
</blockquote>
<h3 id="3-1-虚拟地址"><a href="#3-1-虚拟地址" class="headerlink" title="3.1 虚拟地址"></a>3.1 虚拟地址</h3><p>所有的程序中使用的地址都是虚拟地址 (在段式管理中也叫逻辑地址)，这些地址在不同的进程之间是可以重复的。</p>
<blockquote>
<p>程序为什么要使用 虚拟地址?</p>
<p>C语言来说，编译后的指令中，许多调用的地址在编译阶段就得确定下来，许多方法入口和变量位置在编译时确定了虚拟地址，真正运行时要由OS来分配实际的地址给程序。<br>使用虚拟地址后，地址是可以被复用的，程序不关心与其他进程是否会使用同一地址，OS会分配确保。</p>
</blockquote>
<h3 id="3-2-分段机制"><a href="#3-2-分段机制" class="headerlink" title="3.2 分段机制"></a>3.2 分段机制</h3><p>也就是为进程分配的一段内存区 (连续的区域)，它的 <code>起始位置</code> + <code>逻辑地址</code> = 线性地址 (就是物理地址)</p>
<blockquote>
<p>本进程访问其他进程内存，内存不能read 错误。</p>
</blockquote>
<h3 id="3-3-分页机制"><a href="#3-3-分页机制" class="headerlink" title="3.3 分页机制"></a>3.3 分页机制</h3><p>分页机制 可支撑较大的内存，物理上大多将其划分为 4KB/页</p>
<h3 id="3-4-Java-Heap"><a href="#3-4-Java-Heap" class="headerlink" title="3.4 Java Heap"></a>3.4 Java Heap</h3><p>Java语言，主要看 Heap 区域，系统参数设置为 -Xms, -Xmx 时，JVM 通常是申请一个连续的虚拟地址。OS预先分配的物理内存空间是  -Xms 的大小， -Xmx 许多空间真正使用的时候才会分配。</p>
<blockquote>
<p>32 bit 系统中，1.5GB 的 Heap 区域是比较合适的。64bit 空间不会受到限制 (JVM也必须换成64bit模式)</p>
</blockquote>
<h2 id="4-Disk"><a href="#4-Disk" class="headerlink" title="4. Disk"></a>4. Disk</h2><p>disk 一直在拖着计算机的后腿, SSD好一些.</p>
<h2 id="5-Cache"><a href="#5-Cache" class="headerlink" title="5. Cache"></a>5. Cache</h2><p>CPU 有 cache，系统架构有 cache，存储上有cache，分布式上有 cache，数据库上有cache，ORM框架上有cache …</p>
<blockquote>
<p>Cache 就是  <code>靠近原则</code></p>
</blockquote>
<h2 id="6-网络与数据库"><a href="#6-网络与数据库" class="headerlink" title="6. 网络与数据库"></a>6. 网络与数据库</h2><h3 id="6-1-Java-基本-IO"><a href="#6-1-Java-基本-IO" class="headerlink" title="6.1 Java 基本 IO"></a>6.1 Java 基本 IO</h3><p>只要是内存程序的通信，都可以理解为 <strong>I</strong>nput / <strong>O</strong>utput</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Entropy 介绍]]></title>
      <url>http://sggo.me/2016/09/12/ml/entropy-base/</url>
      <content type="html"><![CDATA[<p>Entropy 熵的基本概念及含义</p>
<a id="more"></a>
<h2 id="1-Entropy-的含义？"><a href="#1-Entropy-的含义？" class="headerlink" title="1. Entropy 的含义？"></a>1. Entropy 的含义？</h2><blockquote>
<p>自然界的事物，如果任其自身发展，最终都会达到尽可能的平衡或互补状态</p>
<p>一盒火柴，（人为或外力）有序地将其摆放在一个小盒子里，如果不小心火柴盒打翻了，火柴会“散乱”地洒在地板上。此时火柴虽然很乱，但这是它自身发展的结果。</p>
</blockquote>
<p>熵Entropy是描述事物无序性的参数，熵越大则无序性越强。</p>
<blockquote>
<p>在信息论中，我们用熵表示一个随机变量的不确定性，那么如何量化信息的不确定性呢？<br>设一次随机事件（用随机变量$X$表示）它可能会有 $x_1，x_2，⋯，x_m$ 共 m 个不同的结果，每个结果出现的概率分别为 $p_1，p_2，⋯，p_m$，那么 $X$ 的不确定度，即信息Entropy为：</p>
</blockquote>
<p>$$<br>H(X) =\sum_{i=1}^{m} p_i \cdot \log_{2} \frac{1}{p_i} = - \sum_{i=1}^{m} p_i \cdot \log_{2} p_i<br>$$</p>
<h2 id="2-Entropy"><a href="#2-Entropy" class="headerlink" title="2. Entropy"></a>2. Entropy</h2><p>Information Entropy [‘entrəpɪ]</p>
<p>Entropy 表示的是不确定度的度量，如果某个数据集的类别的不确定程度越高，则其 entropy 就越大。</p>
<p><strong><em>example</em></strong> : </p>
<p>将一个立方体A抛向空中，记落地时着地的面为 $c$, $c$ 的取值为{1,2,3,4,5,6} </p>
<blockquote>
<p>$$<br>info(c) = - (1/6 \cdot log_{2}(1/6)+…+1/6 \cdot log_{2}(1/6)) = -1 \cdot log(1/6) = 2.58；<br>$$</p>
</blockquote>
<p>四面体抛入空中 :</p>
<blockquote>
<p>$$<br>info(c) = - (1/4 \cdot log_{2}(1/4)+…+1/4 \cdot log_{2}(1/4)) = -1 \cdot log(1/4) = 2；<br>$$</p>
</blockquote>
<p>球体抛入空中 :</p>
<blockquote>
<p>$$<br>info(c) = -1 \cdot log(1) = 0；<br>$$<br>此时表示不确定程度为0，也就是着地时向下的面是确定的。</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[数学之美 19 数学模型的重要性 & 20 最大熵模型]]></title>
      <url>http://sggo.me/2016/09/12/ml/reading-beauty-of-mathematics-19&20-entropy/</url>
      <content type="html"><![CDATA[<p>《数学之美》 19 数学模型的重要性 &amp; 20 最大熵模型 Reading Notes</p>
<a id="more"></a>
<h2 id="1-数学模型的重要性"><a href="#1-数学模型的重要性" class="headerlink" title="1. 数学模型的重要性"></a>1. 数学模型的重要性</h2><blockquote>
<p>伟大的天文学家托勒密<br>哥白尼、伽利略、牛顿</p>
</blockquote>
<p>吴军博士的总结</p>
<ol>
<li>一个正确的数学模型应当在形式上是简单的</li>
<li>一个正确的模型一开始可能还不如一个精雕细琢过的错误模型准确，但如认定大方向是对的，就应该坚持下去</li>
<li>大量准确的数据对研发很重要</li>
<li>正确的模型也可能受 噪声 干扰，而显得不准确；这时应该找到噪声的根源，这也许能通往重大的发现。</li>
</ol>
<h2 id="2-不把所有鸡蛋放到一个篮子里"><a href="#2-不把所有鸡蛋放到一个篮子里" class="headerlink" title="2. 不把所有鸡蛋放到一个篮子里"></a>2. 不把所有鸡蛋放到一个篮子里</h2><p>人们常说不要把所有的鸡蛋放在一个篮子里，可以降低风险。在信息处理中，这个原理同样适用。数学上这个原理称为 - The Maximum Entropy Principle.</p>
<p>Wang XiaoBo 是 王小波 or 王晓波， 需要根据 <code>上下文</code></p>
<p>数学上解决该问题最漂亮的方法就是 ： Maximum Entropy，它相当于 行星运动的椭圆模型。</p>
<p>Maximum Entropy 的原理很简单，就是保留 全部的不确定性，将风险降到最小。</p>
<h3 id="2-1-Maximum-Entropy"><a href="#2-1-Maximum-Entropy" class="headerlink" title="2.1 Maximum Entropy"></a>2.1 Maximum Entropy</h3><p> 对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况<code>不要做任何主观假设</code>。</p>
<p>匈牙利数学家，信息论最高奖香农奖得主 希萨 Csiszar 证明 : 对任一组不自相矛盾的信息，这个最大熵模型存在，且唯一。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[SpringMVC Demo]]></title>
      <url>http://sggo.me/2016/09/11/java/java-springMVC-mybatis-demo/</url>
      <content type="html"><![CDATA[<p>写一个入门整洁的编写 java 后端程序的代码架, 主要使用了 java + springMvc + mybatis + logback + spring task 等技术.</p>
<p><a href="https://github.com/blair101/language/tree/master/java/springMVC_demo" target="_blank" rel="external">blair’s github springMvc_demo</a></p>
<a id="more"></a>
<h2 id="Starting-Point"><a href="#Starting-Point" class="headerlink" title="Starting Point"></a>Starting Point</h2><p>为一些新手和我个人备份，写一个入门<code>整洁</code>的编写 java 后端程序的代码架.</p>
<blockquote>
<p>写这样的程序，最重要的是 <code>整洁与约定规范</code> 而不是多么高深的技术，让接手你代码的人不痛苦，这才是成功</p>
</blockquote>
<h2 id="Involved-Tech"><a href="#Involved-Tech" class="headerlink" title="Involved Tech"></a>Involved Tech</h2><ul>
<li>Java</li>
<li>Restful</li>
<li>SpringMVC</li>
<li>Mybatis</li>
<li>logback</li>
<li>Spring-task</li>
</ul>
<blockquote>
<p>logback : 一个“可靠、通用、快速而又灵活的Java日志框架”。<br>Spring-task 编写非web程序，仅仅是后台需要定时跑的任务，经常被用到。</p>
</blockquote>
<h2 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run"></a>How to Run</h2><h3 id="1-修改数据库连接信息"><a href="#1-修改数据库连接信息" class="headerlink" title="1. 修改数据库连接信息"></a>1. 修改数据库连接信息</h3><p>编辑 ~/resources/props/db.properties 将其中的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># main mysql lib dataSource</span><br><span class="line">main.jdbc.driverClassName=com.mysql.jdbc.Driver</span><br><span class="line">main.jdbc.url=jdbc:mysql://192.168.***.**:3306/testdb01</span><br><span class="line">main.jdbc.username=your_username</span><br><span class="line">main.jdbc.password=your_password</span><br></pre></td></tr></table></figure>
<p>改为你自己的 dataSource 连接信息</p>
<h3 id="2-数据库中建立你用到的表"><a href="#2-数据库中建立你用到的表" class="headerlink" title="2. 数据库中建立你用到的表"></a>2. 数据库中建立你用到的表</h3><p>参见语句  ~/resources/sql/projects.sql 在你的 数据库 中执行其中语句，建立 table <code>user</code>.</p>
<h3 id="3-确认需要的环境已准备好"><a href="#3-确认需要的环境已准备好" class="headerlink" title="3. 确认需要的环境已准备好"></a>3. 确认需要的环境已准备好</h3><blockquote>
<p>确认 ~/resources/logback.xml 中，日志的打印路径，是否适合你的环境<br>我这里是 /data0/www/logs/ ， 如有需要改变，请自行更改。（如果为 windows 环境，请注意路径是否正确）</p>
</blockquote>
<h3 id="4-编译-打包-启动jetty"><a href="#4-编译-打包-启动jetty" class="headerlink" title="4. 编译-打包-启动jetty"></a>4. 编译-打包-启动jetty</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  github ll</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 13 hp staff 442 Sep 10 14:03 language/</span><br><span class="line">➜  github cd language/java/springMVC_demo</span><br><span class="line">➜  springMVC_demo git:(master) ✗ ll</span><br><span class="line">total 24</span><br><span class="line">-rw-r--r-- 1 hp staff   665 Sep 11 15:52 README.md</span><br><span class="line">-rw-r--r-- 1 hp staff 10712 Sep 11 15:10 pom.xml</span><br><span class="line">drwxr-xr-x 4 hp staff   136 Sep 10 13:30 src/</span><br><span class="line">➜  springMVC_demo git:(master) ✗ mvn clean</span><br><span class="line">➜  springMVC_demo git:(master) ✗ mvn compile</span><br><span class="line">➜  springMVC_demo git:(master) ✗ mvn clean package</span><br><span class="line">➜  springMVC_demo git:(master) ✗ mvn jetty:run</span><br><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Building x_demo Maven Webapp 1.0-SNAPSHOT</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] FrameworkServlet &apos;mvc-dispatcher&apos;: initialization completed in 572 ms</span><br><span class="line">[INFO] Started SelectChannelConnector@0.0.0.0:8080</span><br><span class="line">[INFO] Started Jetty Server</span><br><span class="line">[INFO] Starting scanner at interval of 5 seconds.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>当然你也可以通过 IDEA -&gt; Maven Projects -&gt; Plugins -&gt; jetty:run 启动 (或者 Tomcat 启动)</p>
</blockquote>
<p>启动成功后，这时你可以在你的浏览器分别访问以下接口，查看效果了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://localhost:8080/</span><br><span class="line">http://localhost:8080/user/getusers</span><br><span class="line">http://localhost:8080/user/addusers</span><br><span class="line">http://localhost:8080/user/getusers</span><br><span class="line">http://localhost:8080/user/getuser/2</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">http://localhost:8080/user/getuser/2</span><br><span class="line">&#123;</span><br><span class="line">  &quot;status&quot;: 0,</span><br><span class="line">  &quot;errmsg&quot;: &quot;success&quot;,</span><br><span class="line">  &quot;data&quot;: &#123;</span><br><span class="line">    &quot;id&quot;: 2,</span><br><span class="line">    &quot;firstName&quot;: &quot;Andy&quot;,</span><br><span class="line">    &quot;lastName&quot;: &quot;Wong&quot;,</span><br><span class="line">    &quot;age&quot;: 31</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在你测试的时候，如果你想在浏览器中看到格式化后的json，请自行安装 chrome 相关的json插件等。</p>
</blockquote>
<h2 id="Desc"><a href="#Desc" class="headerlink" title="Desc"></a>Desc</h2><ol>
<li><p>展示了前后端开发如何用json进行交互的主流方法，标志状态位与错误信息，返回结果呈现给前端，用一个 JsonResult 类来封装，同时在 web 层，用MappingJackson2HttpMessageConverter 配置，可自动将 Map\<string, object\=""> 转换为 json 呈现给前端等。</string,></p>
</li>
<li><p>代码编写比较规范，用了主流日志框架，将 info 与 error 日志分开打印，不吞异常。分层规范，还特意写了两个数据源如何配置的样例等。</p>
</li>
<li><p>DAO 层不写实现，只写接口，用 Mybatis 来承接，类对象与数据库表 直接自动转换识别，包含了 数据库表字段下划线与java类字段驼峰标识 如何匹配等。</p>
</li>
</ol>
<h2 id="Attentions"><a href="#Attentions" class="headerlink" title="Attentions"></a>Attentions</h2><blockquote>
<p>注意： 版本控制中，涉及的敏感 库地址，用户名，密码 等 不上传.</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/blair101/language/tree/master/java/springMVC_demo" target="_blank" rel="external">blair’s github springMVC_demo</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Machine Learning p3 - 数据的获取、处理与准备]]></title>
      <url>http://sggo.me/2016/09/09/spark/spark-machine-learning-p3/</url>
      <content type="html"><![CDATA[<p>《Spark Machine Learing》 Reading Notes ： Spark上数据的获取、处理与准备</p>
<a id="more"></a>
<p>MovieStream 包括网站提供的电影数据、用户的服务信息数据以及行为数据。</p>
<p>这些数据涉及电影和相关内容（比如标题、分类、图片、演员和导演）、用户信息（比如用户属性、位置和其他信息）以及用户活动数据（比如浏览数、预览的标题和次数、评级、评论，以及如赞、分享之类的社交数据，还有包括像Facebook和Twitter之类的社交网络属性）。</p>
<p>其外部数据来源则可能包括天气和地理定位信息，以及如IMDB和Rotten Tomators之类的第三方电影评级与评论信息等。</p>
<p>一个预测精准的好模型有着极高的商业价值（Netflix Prize 和 <strong>Kaggle</strong> 上机器学习比赛的成功就是很好的见证）</p>
<p><strong>focus on</strong></p>
<ul>
<li>数据的处理、清理、探索和可视化方法；</li>
<li>原始数据转换为可用于机器学习算法特征的各种技术；</li>
<li>学习如何使用外部库或Spark内置函数来正则化输入特征.</li>
</ul>
<h2 id="1-获取公开数据集"><a href="#1-获取公开数据集" class="headerlink" title="1. 获取公开数据集"></a>1. 获取公开数据集</h2><p><strong>UCL机器学习知识库</strong></p>
<blockquote>
<p>包括近300个不同大小和类型的数据集，可用于分类、回归、聚类和推荐系统任务。数据集列表位于：<a href="http://archive.ics.uci.edu/ml/。" target="_blank" rel="external">http://archive.ics.uci.edu/ml/。</a></p>
</blockquote>
<p><strong>Amazon AWS公开数据集</strong></p>
<blockquote>
<p>包含的通常是大型数据集，可通过Amazon S3访问。这些数据集包括人类基因组项目、Common Crawl网页语料库、维基百科数据和Google Books Ngrams。<br>相关信息可参见：<a href="http://aws.amazon.com/publicdatasets/。" target="_blank" rel="external">http://aws.amazon.com/publicdatasets/。</a></p>
</blockquote>
<p><strong>Kaggle</strong></p>
<blockquote>
<p>这里集合了Kaggle举行的各种机器学习竞赛所用的数据集。<br>它们覆盖分类、回归、排名、推荐系统以及图像分析领域，可从Competitions区域下载：<a href="http://www.kaggle.com/competitions。" target="_blank" rel="external">http://www.kaggle.com/competitions。</a></p>
</blockquote>
<p><strong>KDnuggets</strong></p>
<blockquote>
<p>这里包含一个详细的公开数据集列表，其中一些上面提到过的。<br>该列表位于：<a href="http://www.kdnuggets.com/datasets/index.html。" target="_blank" rel="external">http://www.kdnuggets.com/datasets/index.html。</a></p>
</blockquote>
<p><strong>MovieLens 100k数据集</strong></p>
<p>MovieLens 100k数据集包含表示多个用户对多部电影的10万次评级数据，也包含电影元数据和用户属性信息</p>
<p><a href="http://files.grouplens.org/datasets/movielens/ml-100k.zip" target="_blank" rel="external">http://files.grouplens.org/datasets/movielens/ml-100k.zip</a></p>
<p>ml-100k/  u.user（用户属性文件）、u.item（电影元数据）和u.data（用户对电影的评级）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;unzip ml-100k.zip</span><br><span class="line">  inflating: ml-100k/allbut.pl</span><br><span class="line">  inflating: ml-100k/mku.sh</span><br><span class="line">  inflating: ml-100k/README</span><br><span class="line">  ...</span><br><span class="line">  inflating: ml-100k/ub.base</span><br><span class="line">  inflating: ml-100k/ub.test</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>u.user</strong></p>
<p>user.id、age、gender、occupation、ZIP code</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;head -5 u.user</span><br><span class="line">  1|24|M|technician|85711</span><br><span class="line">  2|53|F|other|94043</span><br><span class="line">  3|23|M|writer|32067</span><br><span class="line">  4|24|M|technician|43537</span><br><span class="line">  5|33|F|other|15213</span><br></pre></td></tr></table></figure>
<p><strong>u.item</strong></p>
<p>movie id、title、release date以及若干与IMDB link和电影分类相关的属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;head -5 u.item</span><br><span class="line">  1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20 Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0</span><br><span class="line">  2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title- exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0</span><br><span class="line">  3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title- exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0</span><br><span class="line">  4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title- exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0</span><br><span class="line">  5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title- exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0</span><br></pre></td></tr></table></figure>
<p><strong>u.data</strong></p>
<p>user id、movie id、rating（从1到5）和timestamp属性，各属性间用制表符（\t）分隔</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;head -5 u.data</span><br><span class="line">196    242    3    881250949</span><br><span class="line">186    302    3    891717742</span><br><span class="line">22     377    1    878887116</span><br><span class="line">244    51     2    880606923</span><br><span class="line">166    346    1    886397596</span><br></pre></td></tr></table></figure>
<h2 id="2-探索与可视化数据"><a href="#2-探索与可视化数据" class="headerlink" title="2. 探索与可视化数据"></a>2. 探索与可视化数据</h2><p>IPython的安装方法可参考如下指引：<a href="http://ipython.org/install.html。" target="_blank" rel="external">http://ipython.org/install.html。</a></p>
<p>如果这是你第一次使用IPython，这里有一个教程：<a href="http://ipython.org/ipython-doc/stable/interactive/tutorial.html。" target="_blank" rel="external">http://ipython.org/ipython-doc/stable/interactive/tutorial.html。</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;IPYTHON=1 IPYTHON_OPTS=&quot;--pylab&quot; ./bin/pyspark</span><br></pre></td></tr></table></figure>
<blockquote>
<p>终端里的IPython 2.3.1 – An enhanced Interactive Python和Using matplotlib backend: MacOSX输出行表示IPython和pylab均已被PySpark启用。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 1.5.2</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 2.7.10 (default, Jul 14 2015 19:46:27)</span><br><span class="line">SparkContext available as sc, HiveContext available as sqlContext.</span><br><span class="line"></span><br><span class="line">In [1]:</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以将样本代码输入到IPython终端，也可通过IPython提供的Notebook 应用来完成。Notebook支持HTML显示，且在IPython终端的基础上提供了一些增强功能，如即时绘图、HTML标记，以及独立运行代码片段的功能。</p>
<p>IPython Notebook 使用指南：<a href="http://ipython.org/ipython-doc/stable/interactive/notebook.html" target="_blank" rel="external">http://ipython.org/ipython-doc/stable/interactive/notebook.html</a></p>
</blockquote>
<h3 id="2-1-探索用户数据"><a href="#2-1-探索用户数据" class="headerlink" title="2.1 探索用户数据"></a>2.1 探索用户数据</h3><figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="code"><pre><span class="line">user_data = sc.textFile(&quot;/Users/hp/ghome/ml/ml-100k/u.user&quot;)</span><br><span class="line">user_data.first()</span><br><span class="line">user_data.take(5)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="code"><pre><span class="line">user_fields = user_data.map(lambda line: line.split(&quot;|&quot;))</span><br><span class="line">num_users = user_fields.map(lambda fields: fields[0]).count()</span><br><span class="line">num_genders = user_fields.map(lambda fields: fields[2]).distinct().count()</span><br><span class="line">num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()</span><br><span class="line">num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count()</span><br><span class="line">print &quot;Users: %d, genders: %d, occupations: %d, ZIP codes: %d&quot; % (num_users, num_genders, num_occupations, num_zipcodes)</span><br></pre></td></tr></table></figure>
<p>Output</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Users: 943, genders: 2, occupations: 21, ZIP codes: 795</span><br></pre></td></tr></table></figure>
<p>matplotlib的hist个直方图，以分析用户年龄的分布情况：</p>
<p><strong>age distribution</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ages = user_fields.map(lambda x: int(x[1])).collect()</span><br><span class="line">hist(ages, bins=20, color=&apos;lightblue&apos;, normed=True)</span><br><span class="line">fig = matplotlib.pyplot.gcf()</span><br><span class="line">fig.set_size_inches(16, 10)</span><br></pre></td></tr></table></figure>
<p><img src="/images/spark/spark-ml-3.1.png" alt="screenshow?key=15055650f47cff956148"></p>
<p><strong>occupation distribution</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()</span><br><span class="line"></span><br><span class="line">x_axis1 = np.array([c[0] for c in count_by_occupation])</span><br><span class="line"></span><br><span class="line">y_axis1 = np.array([c[1] for c in count_by_occupation])</span><br><span class="line"></span><br><span class="line">print x_axis1</span><br><span class="line">[u&apos;administrator&apos; u&apos;retired&apos; u&apos;lawyer&apos; u&apos;none&apos; u&apos;student&apos; u&apos;technician&apos;</span><br><span class="line"> u&apos;programmer&apos; u&apos;salesman&apos; u&apos;homemaker&apos; u&apos;writer&apos; u&apos;doctor&apos;</span><br><span class="line"> u&apos;entertainment&apos; u&apos;marketing&apos; u&apos;executive&apos; u&apos;scientist&apos; u&apos;educator&apos;</span><br><span class="line"> u&apos;healthcare&apos; u&apos;librarian&apos; u&apos;artist&apos; u&apos;other&apos; u&apos;engineer&apos;]</span><br><span class="line"></span><br><span class="line">print y_axis1</span><br><span class="line">[ 79  14  12   9 196  27  66  12   7  45   7  18  26  32  31  95  16  51</span><br><span class="line">  28 105  67]</span><br></pre></td></tr></table></figure>
<p>plt.xticks(rotation=30)之类的代码 是 美化条形图</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pos = np.arange(len(x_axis))</span><br><span class="line">width = 1.0</span><br><span class="line"></span><br><span class="line">ax = plt.axes()</span><br><span class="line">ax.set_xticks(pos + (width / 2))</span><br><span class="line">ax.set_xticklabels(x_axis)</span><br><span class="line"></span><br><span class="line">plt.bar(pos, y_axis, width, color=&apos;lightblue&apos;)</span><br><span class="line">plt.xticks(rotation=30)</span><br><span class="line">fig = matplotlib.pyplot.gcf()</span><br><span class="line">fig.set_size_inches(16, 10)</span><br></pre></td></tr></table></figure>
<p><img src="/images/spark/spark-ml-3.2.png" alt="screenshow?key=15057f015ac5712d9a83"></p>
<p>Spark对RDD提供了一个名为countByValue的便捷函数</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="code"><pre><span class="line">count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()</span><br><span class="line">print &quot;Map-reduce approach:&quot;</span><br><span class="line">print dict(count_by_occupation2)</span><br><span class="line">print &quot;&quot;</span><br><span class="line">print &quot;countByValue approach:&quot;</span><br><span class="line">print dict(count_by_occupation)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-探索电影数据"><a href="#2-2-探索电影数据" class="headerlink" title="2.2 探索电影数据"></a>2.2 探索电影数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movie_data = sc.textFile(&quot;/PATH/ml-100k/u.item&quot;)</span><br><span class="line">print movie_data.first()</span><br><span class="line">num_movies = movie_data.count()</span><br><span class="line">print &quot;Movies: %d&quot; % num_movies</span><br></pre></td></tr></table></figure>
<p>1|Toy Story (1995)|01-Jan-1995||<a href="http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0" target="_blank" rel="external">http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0</a><br>Movies: 1682</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def convert_year(x):</span><br><span class="line">  try:</span><br><span class="line">    return int(x[-4:])</span><br><span class="line">  except:</span><br><span class="line">    return 1900</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movie_fields = movie_data.map(lambda lines: lines.split(&quot;|&quot;))</span><br><span class="line">years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))</span><br><span class="line"></span><br><span class="line">years_filtered = years.filter(lambda x: x != 1900)</span><br><span class="line"></span><br><span class="line">movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()</span><br><span class="line">values = movie_ages.values()</span><br><span class="line">bins = movie_ages.keys()</span><br><span class="line">hist(values, bins=bins, color=&apos;lightblue&apos;, normed=True)</span><br><span class="line">fig = matplotlib.pyplot.gcf()</span><br><span class="line">fig.set_size_inches(16,10)</span><br></pre></td></tr></table></figure>
<p><img src="/images/spark/spark-ml-3.3.png" alt="screenshow?key=150556f33e22a36bb651"></p>
<h3 id="2-3-探索评级数据"><a href="#2-3-探索评级数据" class="headerlink" title="2.3 探索评级数据"></a>2.3 探索评级数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rating_data = sc.textFile(&quot;/Users/hp/ghome/ml/ml-100k/u.data&quot;)</span><br><span class="line">print rating_data.first()</span><br><span class="line">num_ratings = rating_data.count()</span><br><span class="line">print &quot;Ratings: %d&quot; % num_ratings</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rating_data = rating_data.map(lambda line: line.split(&quot;\t&quot;))</span><br><span class="line">ratings = rating_data.map(lambda fields: int(fields[2]))</span><br><span class="line">max_rating = ratings.reduce(lambda x, y: max(x, y))</span><br><span class="line">min_rating = ratings.reduce(lambda x, y: min(x, y))</span><br><span class="line">mean_rating = ratings.reduce(lambda x, y: x + y) / num_ratings</span><br><span class="line">median_rating = np.median(ratings.collect())</span><br><span class="line">ratings_per_user = num_ratings / num_users</span><br><span class="line">ratings_per_movie = num_ratings / num_movies</span><br><span class="line">print &quot;Min rating: %d&quot; % min_rating</span><br><span class="line">print &quot;Max rating: %d&quot; % max_rating</span><br><span class="line">print &quot;Average rating: %2.2f&quot; % mean_rating</span><br><span class="line">print &quot;Median rating: %d&quot; % median_rating</span><br><span class="line">print &quot;Average # of ratings per user: %2.2f&quot; % ratings_per_user</span><br><span class="line">print &quot;Average # of ratings per movie: %2.2f&quot; % ratings_per_movie</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Max rating: 5<br>Average rating: 3.00<br>Median rating: 4<br>Average # of ratings per user: 106.00<br>Average # of ratings per movie: 59.00</p>
</blockquote>
<p>Spark对RDD也提供一个名为states的函数。该函数包含一个数值变量用于做类似的统计：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ratings.stats()</span><br><span class="line"></span><br><span class="line">其输出为：</span><br><span class="line">(count: 100000, mean: 3.52986, stdev: 1.12566797076, max: 5.0, min: 1.0)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">count_by_rating = ratings.countByValue()</span><br><span class="line">x_axis = np.array(count_by_rating.keys())</span><br><span class="line">y_axis = np.array([float(c) for c in count_by_rating.values()])</span><br><span class="line"># 这里对y轴正则化，使它表示百分比</span><br><span class="line">y_axis_normed = y_axis / y_axis.sum()</span><br><span class="line">pos = np.arange(len(x_axis))</span><br><span class="line">width = 1.0</span><br><span class="line"></span><br><span class="line">ax = plt.axes()</span><br><span class="line">ax.set_xticks(pos + (width / 2))</span><br><span class="line">ax.set_xticklabels(x_axis)</span><br><span class="line"></span><br><span class="line">plt.bar(pos, y_axis_normed, width, color=&apos;lightblue&apos;)</span><br><span class="line">plt.xticks(rotation=30)</span><br><span class="line">fig = matplotlib.pyplot.gcf()</span><br><span class="line">fig.set_size_inches(16, 10)</span><br></pre></td></tr></table></figure>
<p><img src="/images/spark/spark-ml-3.4.png" alt="screenshow?key=1505422e3494afb95855"></p>
<p><strong>各个用户评级次数的分布情况</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))).groupByKey()</span><br><span class="line"></span><br><span class="line">user_ratings_byuser = user_ratings_grouped.map(lambda (k, v): (k, len(v)))</span><br><span class="line">user_ratings_byuser.take(10)</span><br><span class="line"></span><br><span class="line">Out[91]:</span><br><span class="line">[(2, 62),</span><br><span class="line"> (4, 24),</span><br><span class="line"> (6, 211),</span><br><span class="line"> (8, 59),</span><br><span class="line"> (10, 184),</span><br><span class="line"> (12, 51),</span><br><span class="line"> (14, 98),</span><br><span class="line"> (16, 140),</span><br><span class="line"> (18, 277),</span><br><span class="line"> (20, 48)]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">user_ratings_byuser_local = user_ratings_byuser.map(lambda (k, v): v).collect()</span><br><span class="line">hist(user_ratings_byuser_local, bins=200, color=&apos;lightblue&apos;, normed=True)</span><br><span class="line">fig = matplotlib.pyplot.gcf()</span><br><span class="line">fig.set_size_inches(16,10)</span><br></pre></td></tr></table></figure>
<p><img src="/images/spark/spark-ml-3.5.png" alt="screenshow?key=15056b5ffb7672cee5d1"></p>
<h2 id="3-处理与转换数据"><a href="#3-处理与转换数据" class="headerlink" title="3. 处理与转换数据"></a>3. 处理与转换数据</h2><p><strong>非规整数据和缺失数据的填充</strong></p>
<h2 id="4-从数据中提取有用特征"><a href="#4-从数据中提取有用特征" class="headerlink" title="4. 从数据中提取有用特征"></a>4. 从数据中提取有用特征</h2><p>在完成对数据的初步探索、处理和清理后，便可从中提取可供机器学习模型训练用的特征。</p>
<p>特征（<code>feature</code>）指那些用于<strong><em>模型训练的变量</em></strong>。每一行数据包含可供提取到训练样本中的各种信息。</p>
<p>几乎所有机器学习模型都是与用向量表示的数值特征打交道；需将原始数据转换为数值。</p>
<p>特征可以概括地分为如下几种。</p>
<ul>
<li>数值特征（numerical feature）：这些特征通常为实数或整数，比如之前例子中提到的年龄。</li>
<li>类别特征（categorical feature）：我们数据集中的用户性别、职业或电影类别便是这类。</li>
<li>文本特征（text feature）：它们派生自数据中的文本内容，比如电影名、描述或是评论。</li>
<li>其他特征：… 地理位置则可由经纬度或地理散列（geohash）表示。</li>
</ul>
<h3 id="4-1-数值特征"><a href="#4-1-数值特征" class="headerlink" title="4.1 数值特征"></a>4.1 数值特征</h3><p>原始的数值和一个数值特征之间的区别是什么？</p>
<p>机器学习模型中所学习的是各个特征所对应的向量的权值。这些权值在<code>特征值</code>到输出或是<code>目标变量</code>（指在监督学习模型中）is very important。</p>
<p>当数值特征仍处于原始形式时，其可用性相对较低，但可以转化为更有用的表示形式。</p>
<p>如 (位置信息 : 原始位置信息（比如用经纬度表示的），信息可用性很低。 然若对位置进行聚合（比如聚焦为一个city or country），和特定输出 之间存在某种关联。</p>
<h3 id="4-2-类别特征"><a href="#4-2-类别特征" class="headerlink" title="4.2 类别特征"></a>4.2 类别特征</h3><p>将类别特征表示为数字形式，常可借助 k 之1（1-of-k）方法进行</p>
<p>比如，可取<code>occupation</code> 所有可能取值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">all_occupations = user_fields.map(lambda fields: fields[3]). distinct().collect()</span><br><span class="line">all_occupations.sort()</span><br></pre></td></tr></table></figure>
<p>然可依次对各可能的职业分配序号（注意 从0开始编号）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">idx = <span class="number">0</span></span><br><span class="line">all_occupations_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> o <span class="keyword">in</span> all_occupations:</span><br><span class="line">    all_occupations_dict[o] = idx</span><br><span class="line">    idx +=<span class="number">1</span></span><br><span class="line"><span class="comment"># 看一下“k之1”编码会对新的例子分配什么值</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Encoding of 'doctor': %d"</span> % all_occupations_dict[<span class="string">'doctor'</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Encoding of 'programmer': %d"</span> % all_occupations_dict[<span class="string">'programmer'</span>]</span><br></pre></td></tr></table></figure>
<p>其输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Encoding of &apos;doctor&apos;: 2</span><br><span class="line">Encoding of &apos;programmer&apos;: 14</span><br></pre></td></tr></table></figure>
<h3 id="4-3-派生特征"><a href="#4-3-派生特征" class="headerlink" title="4.3 派生特征"></a>4.3 派生特征</h3><p>从原始数据派生特征的例子包括计算平均值、中位值、方差、和、差、最大值或最小值以及计数。从电影的发行年份和当前年份派生了新的movie age特征的。这类转换背后的想法常常是对数值数据进行某种概括，并期望它能让模型学习更容易。</p>
<p>数值特征到类别特征的转换也很常见，比如划分为区间特征。进行这类转换的变量常见的有年龄、地理位置和时间。</p>
<p><strong>如 ： 将时间戳转为类别特</strong></p>
<p>电影评级发生的时间</p>
<p>[‘afternoon’, ‘evening’, ‘morning’, ‘morning’, ‘morning’]</p>
<h3 id="4-4-文本特征"><a href="#4-4-文本特征" class="headerlink" title="4.4 文本特征"></a>4.4 文本特征</h3><p>文本特征也是一种类别特征或派生特征</p>
<p>NLP 便是专注于文本内容的处理、表示和建模的一个领域。</p>
<p>介绍一种简单且标准化的文本特征提取方法。该方法被称为词袋（bag-of-word）表示法。</p>
<p>词袋法将一段文本视为由其中的文本或数字组成的集合，其处理过程如下。</p>
<p><strong>bag-of-word</strong></p>
<p><strong>(1) 分词（tokenization）</strong></p>
<p>首先会应用某些分词方法来将文本分隔为一个由词（一般如单词、数字等）组成的集合。</p>
<p><strong>(2) 删除停用词（stop words removal)</strong></p>
<p>删除常见的单词，比如the、and和but（这些词被称作停用词）。</p>
<p><strong>(3) 提取词干（stemming）</strong>：</p>
<p>是指将各个词简化为其基本的形式或者干词。常见的例子如复数变为单数（比如dogs变为dog等）。提取的方法有很多种，文本处理算法库中常常会包括多种词干提取方法。</p>
<p><strong>(4) 向量化（vectorization）</strong> ：</p>
<p>向量来表示处理好的词。二元向量可能是最为简单的表示方式。它用1和0来分别表示是否存在某个词。从根本上说，这与之前提到的 k 之1编码相同。与 k 之1相同，它需要一个词的字典来实现词到索引序号的映射。随着遇到的词增多，各种词可能达数百万。由此，使用稀疏矩阵来表示就很关键。这种表示只记录某个词是否出现过，从而节省内存和磁盘空间，以及计算时间。</p>
<p><strong>提取简单的文本特征</strong></p>
<p>参见 : <a href="http://www.ituring.com.cn/tupubarticle/5567" target="_blank" rel="external">http://www.ituring.com.cn/tupubarticle/5567</a></p>
<p>现在每一个电影标题都被转换为一个稀疏向量。</p>
<h3 id="4-5-正则化特征"><a href="#4-5-正则化特征" class="headerlink" title="4.5 正则化特征"></a>4.5 正则化特征</h3><p>在将特征提取为向量形式后，一种常见的预处理方式是将数值数据正则化（normalization）。其背后的思想是将各个数值特征进行转换，以将它们的值域规范到一个标准区间内。正则化的方法有如下几种。</p>
<ul>
<li>正则化特征：这实际上是对数据集中的单个特征进行转换。比如减去平均值（特征对齐）或是进行标准的正则转换（以使得该特征的平均值和标准差分别为0和1）。</li>
<li>正则化特征向量：这通常是对数据中的某一行的所有特征进行转换，以让转换后的特征向量的长度标准化。也就是缩放向量中的各个特征以使得向量的范数为1（常指一阶或二阶范数）。</li>
</ul>
<p>向量正则化可通过numpy的norm函数来实现。具体来说，先计算一个随机向量的二阶范数，然后让向量中的每一个元素都除该范数，从而得到正则化后的向量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.random.seed(42)</span><br><span class="line">x = np.random.randn(10)</span><br><span class="line">norm_x_2 = np.linalg.norm(x)</span><br><span class="line">normalized_x = x / norm_x_2</span><br><span class="line">print &quot;x:\n%s&quot; % x</span><br><span class="line">print &quot;2-Norm of x: %2.4f&quot; % norm_x_2</span><br><span class="line">print &quot;Normalized x:\n%s&quot; % normalized_x</span><br><span class="line">print &quot;2-Norm of normalized_x: %2.4f&quot; % np.linalg.norm(normalized_x)</span><br></pre></td></tr></table></figure>
<p>其输出应该如下（上面将随机种子的值设为42，保证每次运行的结果相同）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x: [ 0.49671415 -0.1382643  0.64768854  1.52302986 -0.23415337 -0.23413696</span><br><span class="line">1.57921282  0.76743473 -0.46947439  0.54256004]</span><br><span class="line">2-Norm of x: 2.5908</span><br><span class="line">Normalized x: [ 0.19172213 -0.05336737  0.24999534  0.58786029 -0.09037871 -0.09037237  0.60954584  0.29621508 -0.1812081  0.20941776]</span><br><span class="line">2-Norm of normalized_x: 1.0000</span><br></pre></td></tr></table></figure>
<p><strong>用 MLlib 正则化特征</strong></p>
<p>Spark在其MLlib机器学习库中内置了一些函数用于特征的缩放和标准化。它们包括供标准正态变换的<code>StandardScaler</code>，以及提供与上述相同的特征向量正则化的 <code>Normalizer</code>。</p>
<p>比较一下MLlib的Normalizer与我们自己函数的结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.mllib.feature import Normalizer</span><br><span class="line">normalizer = Normalizer()</span><br><span class="line">vector =sc.parallelize([x])</span><br></pre></td></tr></table></figure>
<p>在导入所需的类后，会要初始化Normalizer（其默认使用与之前相同的二阶范数）。注意用Spark时，大部分情况下Normalizer所需的输入为一个RDD（它包含numpy数值或MLlib向量）。作为举例，我们会从x向量创建一个单元素的RDD。</p>
<p>之后将会对我们的RDD调用Normalizer的transform函数。由于该RDD只含有一个向量，可通过first函数来返回向量到驱动程序。接着调用toArray函数来将该向量转换为numpy数组：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">normalized_x_mllib = normalizer.transform(vector).first().toArray()</span><br><span class="line">#最后来看一下之前打印过的那些值，并做个比较：</span><br><span class="line"></span><br><span class="line">print &quot;x:\n%s&quot; % x</span><br><span class="line">print &quot;2-Norm of x: %2.4f&quot; % norm_x_2</span><br><span class="line">print &quot;Normalized x MLlib:\n%s&quot; % normalized_x_mllib</span><br><span class="line">print &quot;2-Norm of normalized_x_mllib: %2.4f&quot; % np.linalg.norm(normalized_x_mllib)</span><br></pre></td></tr></table></figure>
<p>相比自己编写的函数，使用 MLlib内置的函数 更方便</p>
<h3 id="4-6-用软件包提取特征"><a href="#4-6-用软件包提取特征" class="headerlink" title="4.6 用软件包提取特征"></a>4.6 用软件包提取特征</h3><p>特征提取可借助的软件包有scikit-learn、gensim、scikit-image、matplotlib、Python的NLTK、Java编写的OpenNLP以及用Scala编写的Breeze和Chalk。Breeze自Spark 1.0开始就成为Spark的一部分了。Breeze有线性代数功能。</p>
<h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a>5. 小结</h2><p>了解 如何导入、处理和清理数据，如何将原始数据转为<strong>特征向量</strong>以供模型训练的常见方法</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Machine Learning p2 - 设计机器学习系统]]></title>
      <url>http://sggo.me/2016/09/08/spark/spark-machine-learning-p2-design-ml-sys/</url>
      <content type="html"><![CDATA[<p>《Spark Machine Learing》 Reading Notes ： 如何设计机器学习系统 moiveStream</p>
<a id="more"></a>
<style>
img {
        display: block !important;
        height: 400px;
        width: 500px;
        margin-left: 180px !important;
}
</style>

<h2 id="1-原始-MovieStream-介绍"><a href="#1-原始-MovieStream-介绍" class="headerlink" title="1. 原始 MovieStream 介绍"></a>1. 原始 MovieStream 介绍</h2><p><img src="/images/spark/spark-ml-2.1.jpg" alt="MovieStream"></p>
<h3 id="1-1-个性化"><a href="#1-1-个性化" class="headerlink" title="1.1 个性化"></a>1.1 个性化</h3><p><code>个性化</code> 是根据各因素来改变用户体验和<code>呈现给用户内容</code>。这些因素可能包括用户的行为数据和外部因素。</p>
<p><code>推荐</code>（recommendation）, 常指向用户呈现一个他们可能感兴趣的物品列表。</p>
<p>个性化和推荐十分相似, 根据因素改变搜索的呈现不同用户不同内容，这是隐式个性化</p>
<h3 id="1-2-客户细分"><a href="#1-2-客户细分" class="headerlink" title="1.2 客户细分"></a>1.2 客户细分</h3><p>目标营销用与推荐类似的方法从用户群中找出要营销的对象。一般来说，推荐和个性化的应用场景都是一对一，根据用户的特征进行分组，并可能参考行为数据。也可能使用了某种机器学习模型，比如 <code>聚类</code>。</p>
<h3 id="1-3-预测建模"><a href="#1-3-预测建模" class="headerlink" title="1.3 预测建模"></a>1.3 预测建模</h3><p><code>预测性分析</code> 从某种意义上说还覆盖推荐、个性化和目标营销。用预测建模（predictive modeling）来表示其他做预测的模型。借助活动记录、收入数据以及内容属性，MovieStream 可以创建一个回归模型（regression model）来预测新电影的市场表现。</p>
<p>另外，我们也可使用分类模型（classificaiton model）来对只有部分数据的新电影自动分配标签、关键字或分类。</p>
<h2 id="2-机器学习模型的种类"><a href="#2-机器学习模型的种类" class="headerlink" title="2. 机器学习模型的种类"></a>2. 机器学习模型的种类</h2><p><code>supervised learning</code>：这种方法使用已标记数据来学习。<code>推荐引擎</code>、<code>回归</code>和<code>分类</code>便是例子。它们所使用的标记数据可以是用户对电影的评级（对推荐来说）、电影标签（对上述分类例子来说）或是收入数字（对回归预测来说）.</p>
<p><code>unsupervised learning</code>：一些模型的学习过程不需要标记数据，我们称其为无监督学习。这类模型试图学习或是提取数据背后的结构或从中抽取最为重要的特征。<code>聚类</code>、<code>降维</code>和<code>文本处理</code>的某些特征提取都是无监督学习.</p>
<h2 id="3-数据驱动ML系统的组成"><a href="#3-数据驱动ML系统的组成" class="headerlink" title="3. 数据驱动ML系统的组成"></a>3. 数据驱动ML系统的组成</h2><p><img src="/images/spark/spark-ml-2.2.jpg" alt="机器学习流程"></p>
<h3 id="3-1-数据获取与存储"><a href="#3-1-数据获取与存储" class="headerlink" title="3.1 数据获取与存储"></a>3.1 数据获取与存储</h3><p>MovieStream 的数据通常来自用户活动.</p>
<p>要存储的数据包括：原始数据、即时处理后的数据，以及可用于生产系统的最终建模结果。</p>
<p><strong>数据存储</strong></p>
<ol>
<li><strong>文件系统</strong> : 如 HDFS、Amazon S3 等；</li>
<li><strong>SQL数据库</strong> : 如 MySQL、PostgreSQL；</li>
<li><strong>NoSQL</strong> : -如 HBase、Cassandra、Mongodb；</li>
<li><strong>搜索引擎</strong> : 如 Solr 、Elasticsearch；</li>
<li><strong>流数据</strong> : – 如 Kafka、Flume、Amazon Kinesis</li>
</ol>
<h3 id="3-2-数据清理与转换"><a href="#3-2-数据清理与转换" class="headerlink" title="3.2 数据清理与转换"></a>3.2 数据清理与转换</h3><p>大部分机器学习模型所处理的都是 <code>feature</code>。特征 通常是输入变量所对应的可用于模型的数值表示。</p>
<p>原始数据 预处理 几种 情况</p>
<ol>
<li>数据过滤</li>
<li>合并多个数据源</li>
<li>数据汇总</li>
</ol>
<p>对许多模型类型来说，这种表示就是包含 <strong>数值数据的</strong> <code>向量</code> or <code>矩阵</code>。</p>
<p>将类别数据（比如地理位置所在的国家或是电影的类别）编码为对应的数值表示。</p>
<ol>
<li>文本数据提取有用信息。</li>
<li>处理图像或是音频数据。</li>
<li>数值数据常被转换为类别数据以减少某个变量的可能值的数目。例如将年龄分为 601, 602…</li>
<li>对特征进行正则化、标准化，以保证同一模型的不同输入变量的值域相同。</li>
</ol>
<p>这些数据清理、探索、聚合和转换步骤，都能通过Spark核心API、SparkSQL引擎和其他外部Scala、Java或Python包做到。借助 Spark 的 Hadoop功能 还能实现上述多种存储系统上的读写。</p>
<h3 id="3-3-模型训练与测试回路"><a href="#3-3-模型训练与测试回路" class="headerlink" title="3.3 模型训练与测试回路"></a>3.3 模型训练与测试回路</h3><p>当数据已转换为可用于模型的形式，便可开始模型的训练和测试。</p>
<p>在训练数据集上运行模型并在测试数据集（即为评估模型而预留的数据，在训练阶段模型没接触过该数据）上测试其效果，这个过程一般相对直接，被称作交叉验证（cross-validation）。</p>
<p>Spark MLlib 来实现对各种机器学习方法的模型训练、评估以及交叉验证。</p>
<h3 id="3-4-模型部署与整合"><a href="#3-4-模型部署与整合" class="headerlink" title="3.4 模型部署与整合"></a>3.4 模型部署与整合</h3><p>通过训练测试循环找出最佳模型后，要让它能得出可付诸实践的预测，还需将其部署到生产系统中。</p>
<p>这个过程一般要将已训练的模型导入特定的数据存储中。</p>
<h3 id="3-5-模型监控与反馈"><a href="#3-5-模型监控与反馈" class="headerlink" title="3.5 模型监控与反馈"></a>3.5 模型监控与反馈</h3><p>监控机器学习系统在生产环境下的表现十分重要。</p>
<p>同样值得注意的是，模型准确度和预测效果只是现实中系统表现的一部分。</p>
<p>我们可以尽可能在生产系统中部署不同的模型，通过调整它们而优化业务指标。实践中，这通常通过在线分割测试（<code>live split test</code>）进行。</p>
<p>模型反馈（<code>model feedback</code>），指通过用户的行为来对模型的预测进行反馈的过程。在现实系统中，模型的应用将影响用户的决策和潜在行为，从而反过来将从根本上改变模型自己将来的训练数据。</p>
<h3 id="3-6-批处理-实时方案选择"><a href="#3-6-批处理-实时方案选择" class="headerlink" title="3.6 批处理/实时方案选择"></a>3.6 批处理/实时方案选择</h3><p>常见的批处理方法。模型用所有数据或一部分数据进行周期性的重新训练。由于上述流程会花费一定的时间，这就使得批处理方法难以在新数据到达时立即完成模型的更新。</p>
<p>存在一类名为在线学习（online learning）的机器学习方法。它们在新数据到达时便能立即更新模型，从而使实时系统成为可能。常见的例子有对线性模型的在线优化算法，如<code>随机梯度下降法</code>。</p>
<h2 id="4-机器学习系统架构"><a href="#4-机器学习系统架构" class="headerlink" title="4. 机器学习系统架构"></a>4. 机器学习系统架构</h2><p><img src="/images/spark/spark-ml-2.3.jpg" alt="机器学习系统架构"></p>
<p>机器学习流程示意图的内容：</p>
<ul>
<li>收集与用户、用户行为和电影标题有关的数据；</li>
<li>将这些数据转为特征；</li>
<li>模型训练，包括训练-测试和模型选择环节；</li>
<li>将已训练模型部署到在线服务系统，并用于离线处理；</li>
<li>通过推荐和目标页面将模型结果反馈到MovieStream站点；</li>
<li>将模型结果返回到MovieStream的个性化营销渠道；</li>
<li>使用离线模型来为MovieSteam的各个团队提供工具，以帮助其理解用户的行为、内容目录的特点和业务收入的驱动因素。</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[JVM 跨平台与字节码原理初步]]></title>
      <url>http://sggo.me/2016/08/16/java/java-special-arms-p3-jvm/</url>
      <content type="html"><![CDATA[<p>Java特种兵 - JVM 跨平台与字节码原理，Reading Notes</p>
<a id="more"></a>
<p>用到 JVM 的场景</p>
<ol>
<li>Out of memory 时，团队高手不在</li>
<li>系统服务器架构，老大问你 投入多少服务器成本，VM 分配多大， 如何分配?</li>
</ol>
<h2 id="1-javap-命令"><a href="#1-javap-命令" class="headerlink" title="1. javap 命令"></a>1. javap 命令</h2><blockquote>
<p>通过这种方式认知比 Java 更低一个抽象层次的逻辑，虚指令有时候更好解释问题。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">test1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String a = <span class="string">"a"</span> + <span class="string">"b"</span> + <span class="number">1</span>;</span><br><span class="line">        String b = <span class="string">"ab1"</span>;</span><br><span class="line">        System.out.println(a == b); <span class="comment">// true 编译时优化</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  p3jvm git:(master) ✗ <span class="built_in">pwd</span></span><br><span class="line">/Users/hp/ghome/github/language/java/jsarms/p3jvm</span><br><span class="line">➜  p3jvm git:(master) ✗ javac</span><br><span class="line">Usage: javac &lt;options&gt; &lt;<span class="built_in">source</span> files&gt;</span><br><span class="line"><span class="built_in">where</span> possible options include:</span><br><span class="line">  -g                         Generate all debugging info</span><br><span class="line">  -g:none                    Generate no debugging info</span><br><span class="line">  -g:&#123;lines,vars,<span class="built_in">source</span>&#125;     Generate only some debugging info</span><br><span class="line">  -nowarn                    Generate no warnings</span><br><span class="line">  -verbose                   Output messages about what the compiler is doing</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  p3jvm git:(master) ✗ javac -g:vars,lines StringTest.java</span><br><span class="line">➜  p3jvm git:(master) ✗ javap -verbose StringTest</span><br><span class="line">Classfile /Users/hp/ghome/github/language/java/jsarms/p3jvm/StringTest.class</span><br><span class="line">  Last modified Aug 16, 2016; size 559 bytes</span><br><span class="line">  MD5 checksum 772d18512cb982c953e7db8c72522918</span><br><span class="line">public class StringTest</span><br><span class="line">  minor version: 0</span><br><span class="line">  major version: 51</span><br><span class="line">  flags: ACC_PUBLIC, ACC_SUPER</span><br><span class="line">Constant pool:</span><br><span class="line">   <span class="comment">#1 = Methodref          #6.#21         //  java/lang/Object."&lt;init&gt;":()V</span></span><br><span class="line">   <span class="comment">#2 = String             #22            //  ab1</span></span><br><span class="line">   <span class="comment">#3 = Fieldref           #23.#24        //  java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line">   <span class="comment">#4 = Methodref          #25.#26        //  java/io/PrintStream.println:(Z)V</span></span><br><span class="line">   <span class="comment">#5 = Class              #27            //  StringTest</span></span><br><span class="line">   <span class="comment">#6 = Class              #28            //  java/lang/Object</span></span><br><span class="line">   <span class="comment">#7 = Utf8               &lt;init&gt;</span></span><br><span class="line">   <span class="comment">#8 = Utf8               ()V</span></span><br><span class="line">   <span class="comment">#9 = Utf8               Code</span></span><br><span class="line">  <span class="comment">#10 = Utf8               LineNumberTable</span></span><br><span class="line">  <span class="comment">#11 = Utf8               LocalVariableTable</span></span><br><span class="line">  <span class="comment">#12 = Utf8               this</span></span><br><span class="line">  <span class="comment">#13 = Utf8               LStringTest;</span></span><br><span class="line">  <span class="comment">#14 = Utf8               test1</span></span><br><span class="line">  <span class="comment">#15 = Utf8               a</span></span><br><span class="line">  <span class="comment">#16 = Utf8               Ljava/lang/String;</span></span><br><span class="line">  <span class="comment">#17 = Utf8               b</span></span><br><span class="line">  <span class="comment">#18 = Utf8               StackMapTable</span></span><br><span class="line">  <span class="comment">#19 = Class              #29            //  java/lang/String</span></span><br><span class="line">  <span class="comment">#20 = Class              #30            //  java/io/PrintStream</span></span><br><span class="line">  <span class="comment">#21 = NameAndType        #7:#8          //  "&lt;init&gt;":()V</span></span><br><span class="line">  <span class="comment">#22 = Utf8               ab1</span></span><br><span class="line">  <span class="comment">#23 = Class              #31            //  java/lang/System</span></span><br><span class="line">  <span class="comment">#24 = NameAndType        #32:#33        //  out:Ljava/io/PrintStream;</span></span><br><span class="line">  <span class="comment">#25 = Class              #30            //  java/io/PrintStream</span></span><br><span class="line">  <span class="comment">#26 = NameAndType        #34:#35        //  println:(Z)V</span></span><br><span class="line">  <span class="comment">#27 = Utf8               StringTest</span></span><br><span class="line">  <span class="comment">#28 = Utf8               java/lang/Object</span></span><br><span class="line">  <span class="comment">#29 = Utf8               java/lang/String</span></span><br><span class="line">  <span class="comment">#30 = Utf8               java/io/PrintStream</span></span><br><span class="line">  <span class="comment">#31 = Utf8               java/lang/System</span></span><br><span class="line">  <span class="comment">#32 = Utf8               out</span></span><br><span class="line">  <span class="comment">#33 = Utf8               Ljava/io/PrintStream;</span></span><br><span class="line">  <span class="comment">#34 = Utf8               println</span></span><br><span class="line">  <span class="comment">#35 = Utf8               (Z)V</span></span><br><span class="line">// 以上是 Constant pool， 仅仅是陈列操作，并没有开始执行任务，看下面开始</span><br><span class="line">&#123;</span><br><span class="line">  public StringTest();</span><br><span class="line">    flags: ACC_PUBLIC</span><br><span class="line">    Code:</span><br><span class="line">      stack=1, locals=1, args_size=1 // 所有方法都会有。</span><br><span class="line">      // stack 为栈顶的单位大小 (每个大小为 1 slot，4 byte)</span><br><span class="line">      // locals=1，非静态方法，本地变量增加 this</span><br><span class="line">         0: aload_0</span><br><span class="line">         1: invokespecial <span class="comment">#1                  // Method java/lang/Object."&lt;init&gt;":()V</span></span><br><span class="line">         4: <span class="built_in">return</span></span><br><span class="line">      LineNumberTable:</span><br><span class="line">        line 1: 0</span><br><span class="line">      LocalVariableTable:</span><br><span class="line">        Start  Length  Slot  Name   Signature</span><br><span class="line">               0       5     0  this   LStringTest;</span><br><span class="line"></span><br><span class="line">  public static void <span class="built_in">test</span>1();</span><br><span class="line">    flags: ACC_PUBLIC, ACC_STATIC</span><br><span class="line">    Code:</span><br><span class="line">      stack=3, locals=2, args_size=0 </span><br><span class="line">      // stack=3，本地栈slot个数为3，String需要load，String.out 占用一个再。当对比发生 boolean 时，两个String引用栈顶pop</span><br><span class="line">      // locals=2， 因为只有两个 String</span><br><span class="line">      // args_size=0，方法没有入口参数</span><br><span class="line">         0: ldc           <span class="comment">#2                  // String ab1</span></span><br><span class="line">         // 引用常量池内容</span><br><span class="line">         2: astore_0</span><br><span class="line">         // 将栈顶引用值，写入第 1 个 slot 所在的本地变量</span><br><span class="line">         3: ldc           <span class="comment">#2                  // String ab1</span></span><br><span class="line">         5: astore_1</span><br><span class="line">         6: getstatic     <span class="comment">#3                  // Field java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line">         // 获取静态域，放入栈顶，此时静态域是 System.out 对象</span><br><span class="line">         9: aload_0</span><br><span class="line">        10: aload_1</span><br><span class="line">        11: <span class="keyword">if</span>_acmpne     18</span><br><span class="line">        14: iconst_1</span><br><span class="line">        15: goto          19</span><br><span class="line">        18: iconst_0</span><br><span class="line">        19: invokevirtual <span class="comment">#4                  // Method java/io/PrintStream.println:(Z)V</span></span><br><span class="line">        22: <span class="built_in">return</span></span><br><span class="line">      LineNumberTable:</span><br><span class="line">        line 4: 0</span><br><span class="line">        line 5: 3</span><br><span class="line">        line 6: 6</span><br><span class="line">        line 7: 22</span><br><span class="line">      LocalVariableTable:</span><br><span class="line">        Start  Length  Slot  Name   Signature</span><br><span class="line">               3      20     0     a   Ljava/lang/String;</span><br><span class="line">               6      17     1     b   Ljava/lang/String;</span><br><span class="line">      // 本地变量列表 LocalVariableTable. from javac -g:vars</span><br><span class="line">      StackMapTable: number_of_entries = 2</span><br><span class="line">           frame_<span class="built_in">type</span> = 255 /* full_frame */</span><br><span class="line">          offset_delta = 18</span><br><span class="line">          locals = [ class java/lang/String, class java/lang/String ]</span><br><span class="line">          stack = [ class java/io/PrintStream ]</span><br><span class="line">           frame_<span class="built_in">type</span> = 255 /* full_frame */</span><br><span class="line">          offset_delta = 0</span><br><span class="line">          locals = [ class java/lang/String, class java/lang/String ]</span><br><span class="line">          stack = [ class java/io/PrintStream, int ]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">➜  p3jvm git:(master) ✗</span><br></pre></td></tr></table></figure>
<h2 id="2-Java-字节码结构"><a href="#2-Java-字节码结构" class="headerlink" title="2. Java 字节码结构"></a>2. Java 字节码结构</h2><p>javac 命令本身只是一个引导器，它引导编译器程序的运行。编译器本身是一个java程序 <code>com.sun.tools.javac.main.JavaCompiler</code>, 该类完成 java 源文件 的 Parser、Annotation process、检查、泛型处理、语法转换等，最终胜出 Class 文件。</p>
<p>Java 字节码文件主体结构: </p>
<table>
<thead>
<tr>
<th><strong>Class 文件头部</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Constant pool</td>
<td></td>
</tr>
<tr>
<td>当前Clas的描述信息</td>
<td></td>
</tr>
<tr>
<td>属性列表</td>
<td></td>
</tr>
<tr>
<td>方法列表</td>
<td></td>
</tr>
<tr>
<td>…</td>
<td></td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Decision Tree Part1]]></title>
      <url>http://sggo.me/2016/08/16/ml/5-decisionTree-part1/</url>
      <content type="html"><![CDATA[<p>决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值。</p>
<a id="more"></a>
<h2 id="1-Classification-Introduce"><a href="#1-Classification-Introduce" class="headerlink" title="1. Classification Introduce"></a>1. Classification Introduce</h2><blockquote>
<p>分类有着广泛的应用，如医学疾病判别、垃圾邮件过滤、垃圾短信拦截、客户分析等等。分类问题可以分为两类</p>
</blockquote>
<h3 id="1-1-归类-离散"><a href="#1-1-归类-离散" class="headerlink" title="1.1 归类 : 离散"></a>1.1 归类 : 离散</h3><p>归类 是指对<code>离散数据</code>的分类，比如对根据一个人的笔迹判别这个是男还是女，这里的 Category 只有两个，类别是离散的集合空间{男，女}的。</p>
<h3 id="1-2-预测-连续"><a href="#1-2-预测-连续" class="headerlink" title="1.2 预测 : 连续"></a>1.2 预测 : 连续</h3><p>预测 是指对<code>连续数据</code>的分类，比如预测明天8点天气的湿度情况，天气的湿度在随时变化，8点时的天气是一个具体值，它不属于某个有限集合空间。预测也叫回归分析，在金融领域有着广泛应用。</p>
<blockquote>
<p>虽然对离散数据和连续数据的处理方式有所不同，但其实他们之间相互转化，比如我们可以根据比较的某个特征值判断，如果值大于0.5就认定为男性，小于等于0.5就认为是女性，这样就转化为连续处理方式；将天气湿度值分段处理也就转化为离散数据。</p>
</blockquote>
<p><strong>数据分类</strong> 分两个步骤：</p>
<ol>
<li>构造模型，利用训练数据集 训练 分类器；</li>
<li>利用建好的分类器模型对测试数据进行分类。</li>
</ol>
<blockquote>
<p>好的分类器具有很好的泛化能力，即它不仅在训练数据集上能达到很高的正确率，而且能在未见过得测试数据集也能达到较高的正确率。如果一个分类器只是在训练数据上表现优秀，但在测试数据上表现稀烂，这个分类器就已经过拟合了，它只是把训练数据记下来了，并没有抓到整个数据空间的特征。</p>
</blockquote>
<h2 id="2-Decision-Tree’-Classification"><a href="#2-Decision-Tree’-Classification" class="headerlink" title="2. Decision Tree’ Classification"></a>2. Decision Tree’ Classification</h2><p>代表性的例子说明 :</p>
<table>
<thead>
<tr>
<th style="text-align:center">ID</th>
<th style="text-align:center">阴晴(F)</th>
<th style="text-align:center">温度(F)</th>
<th style="text-align:center">湿度(F)</th>
<th style="text-align:center">刮风(F)</th>
<th style="text-align:center">是否玩（C）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">hot</td>
<td style="text-align:center">high</td>
<td style="text-align:center">false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">hot</td>
<td style="text-align:center">high</td>
<td style="text-align:center">true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">overcast</td>
<td style="text-align:center">hot</td>
<td style="text-align:center">high</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">high</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">cool</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">cool</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">overcast</td>
<td style="text-align:center">cool</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">high</td>
<td style="text-align:center">false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">cool</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">sunny</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">overcast</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">high</td>
<td style="text-align:center">true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center">overcast</td>
<td style="text-align:center">hot</td>
<td style="text-align:center">normal</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">rainy</td>
<td style="text-align:center">mild</td>
<td style="text-align:center">high</td>
<td style="text-align:center">true</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
<p>利用ID3算法中的 Info Gain Feature Selection，递归的学习一棵决策树，得到树结构如下</p>
<p><img src="/images/ml/decision-tree/decision-tree-2.png" width="560" height="400" img=""></p>
<blockquote>
<p>决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个Feature属性上的测试，每个分支代表这个Feature属性在某个值域上的输出，而每个叶节点存放一个 Category 。使用 DT 进行决策的过程就是从 root 开始，测试待分类项中相应的 Feature 属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的 Category 作为决策结果。</p>
</blockquote>
<p>Feature Selection，如何量化最优Feature? <code>-&gt;</code> 导致 DT Algorithm 出现了 ID3、C4.5、C5.0、CART 等。</p>
<h2 id="3-Decision-Tree’-Build"><a href="#3-Decision-Tree’-Build" class="headerlink" title="3. Decision Tree’ Build"></a>3. Decision Tree’ Build</h2><p>构造 Decision Tree 的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。</p>
<blockquote>
<p>构造决策树的过程本质上就是根据 <strong>data-feature</strong> 将 数据集(D) 分类的递归过程，我们需要解决的第一个问题就是，<strong>当前 数据集(D) 上哪个 Feature 在划分数据分类时起决定性作用</strong></p>
</blockquote>
<h3 id="3-1-构造-DT-流程"><a href="#3-1-构造-DT-流程" class="headerlink" title="3.1 构造 DT 流程"></a>3.1 构造 DT 流程</h3><p>训练数据集 $D = \{ (x^{(1)}，y^{(1)}) ， (x^{(2)}，y^{(2)})， ⋯ ， (x^{(m)}，y^{(m)}) \}$ (Feature用离散值表示)<br>候选特征集 $F = \{f^1，f^2， ⋯，f^n \}$</p>
<p>开始建立 Root节点，将所有训练数据都置于根节点（$m$条样本）。从 feature集合 $F$ 中选择一个最优特征 $f^∗$，按照 $f^∗$ 取值将 训练数据集(D) 切分成若干子集，使得各个自己有一个在当前条件下最好的分类。</p>
<p>如果子集中样本类别基本相同，那么构建叶节点，并将数据子集划分给对应的叶节点；如果子集中样本类别差异较大，不能被基本正确分类，需要在剩下的特征集合 $（F−{f^∗}）$ 中选择新的最优特征，继续对数据子集进行切分。如此递归地进行下去，直至所有数据自己都能被基本正确 Category，或者没有合适的最优特征为止。</p>
<p>这样最终结果是每个子集都被分到叶节点上，对应着一个明确的类别。那么，递归生成的层级结构即为一棵 DT。</p>
<h3 id="3-2-伪代码构造-DT"><a href="#3-2-伪代码构造-DT" class="headerlink" title="3.2 伪代码构造 DT"></a>3.2 伪代码构造 DT</h3><p>  输入 : 训练数据集 $D = \{ (x^{(1)}，y^{(1)}) ， (x^{(2)}，y^{(2)})， ⋯ ， (x^{(m)}，y^{(m)}) \}$(Feature用离散值表示)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;候选特征集 $F = \{f^1，f^2， ⋯，f^n \}$</p>
<p>  输出 : T(D, F)</p>
<p><img src="/images/ml/decision-tree/decision-tree-3.png" width="760" height="400" img=""></p>
<p>决策树学习过程中递归的每一步，在选择最优特征后，根据特征取值切割当前节点的数据集，得到若干数据子集。<br>算法的时间复杂度是O(k<em>|D|</em>log(|D|))，k为属性个数，|D|为记录集D的记录数。</p>
<h2 id="4-Feature-Selection"><a href="#4-Feature-Selection" class="headerlink" title="4. Feature Selection"></a>4. Feature Selection</h2><p>递归地选择最优feature，根据feature取值切割数据集，使得对应的数据子集有一个较好的分类。从伪代码中也可以看出，在决策树学习过程中，最重要的是第07行，即如何选择最优feature？也就是我们常说的feature选择问题。</p>
<p>在这里，希望随着feature选择过程地不断进行，决策树的分支节点所包含的样本尽可能属于同一类别，即希望节点的”纯度（purity）”越来越高。</p>
<blockquote>
<p>如子集中的样本都属于同一个类别，是最好的结果；如果说大多数的样本类型相同，只有少部分样本不同，也可以接受。</p>
</blockquote>
<p>那么如何才能做到选择的特征对应的样本子集纯度最高呢？</p>
<table>
<thead>
<tr>
<th style="text-align:center">Algorithm</th>
<th style="text-align:center">Feature 选择方法</th>
<th style="text-align:center">Author</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ID3</td>
<td style="text-align:center">Information gain</td>
<td style="text-align:center">Quinlan. 1986</td>
</tr>
<tr>
<td style="text-align:center">C4.5</td>
<td style="text-align:center">Gain ratio</td>
<td style="text-align:center">Quinlan. 1993.</td>
</tr>
<tr>
<td style="text-align:center">CART</td>
<td style="text-align:center">回归树： 最小二乘<br>分类树： 基尼指数 Gini index</td>
<td style="text-align:center">Breiman. 1984<br>(Classification and Regression Tree 分类与回归树)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>ID3 (Iterative Dichotomiser)</p>
</blockquote>
<h3 id="4-1-Information-Gain"><a href="#4-1-Information-Gain" class="headerlink" title="4.1 Information Gain"></a>4.1 Information Gain</h3><p>信息增益（Information Gain）衡量 Feature 的重要性是根据当前 Feature 为划分带来多少信息量，带来的信息越多，该 Feature 就越重要，此时节点的”纯度”也就越高。</p>
<p><a href="/2016/08/18/ml-entropy-base/">Infomation Entropy</a></p>
<blockquote>
<p>对一个分类系统来说，假设类别 $C$ 可能的取值为 $c_1，c_2，⋯，c_k$（$k$是类别总数），每一个类别出现的概率分别是 $p(c_1)，p(c_2)，⋯，p(c_k)$。此时，分类系统的 Entropy 可以表示为:</p>
<p>$$<br>info(C) = -  \sum_{i=1}^k p(c_i) \cdot log_2 p(c_i) \qquad (fml.4.1.1)<br>$$</p>
<p>分类系统的作用就是输出一个特征向量（文本特征、ID特征 等）属于哪个 Category 的值，而这个值可能是 $c_1，c_2，⋯，c_k$ ，因此这个值所携带的信息量就是 (fml.4.1.1) 公式这么多</p>
</blockquote>
<p><strong>Condition Entropy</strong></p>
<p>假设 离散特征 $f$ 的取值有 $I$ 个，$info(C|f=f_i)$ 表示特征 $f$ 被取值为 $f_i$ 时的<strong><em>Condition Entropy</em></strong>； $info(C|f)$ 是指特征 $f$ 被固定时的 <strong><em>Condition Entropy</em></strong>。二者之间的关系是：</p>
<blockquote>
<p>$$<br>\begin{align}<br>info(C|f) &amp; = p_1 \cdot info(C|f=f_1) + p_2 \cdot info(C|f=f_2) + … + p_k \cdot info(C|f=f_{k}) \\ &amp; = \sum_{i=1}^{I} p_i \cdot info(C|f=f_i) \end{align}  \quad (fml.4.1.2)<br>$$<br>假设总样本数有 $m$ 条，特征 $f=f_i$ 时的样本数 $m_i，p_i=\frac{m_i}{m}$.</p>
</blockquote>
<p><strong>如何求 $P(C|f=f_i)$</strong> ?</p>
<blockquote>
<p>二分类情况 :</p>
<p>以二分类为例（正例为1，负例为0），总样本数为 $m$ 条，特征 $f$ 的取值为 $I$ 个，其中特征 $f=f_i$ 对应的样本数为 $m_i$ 条，其中正例 $m_{i1}$ 条，负例 $m_{i0}$ 条 $m_i = m_{i0} + m_{i1}$ 。那么有：</p>
<p>$$<br>\begin{align} info(C|f=f_i) &amp; = - \frac{m_{i1}}{m_i} \cdot log_{2} \frac{m_{i1}}{m_i} - \frac{m_{i0}}{m_i} \cdot log_{2} \frac{m_{i0}}{m_i} \end{align} \qquad (fml.4.1.3)<br>$$<br>多分类情况:</p>
<p>$$<br>\begin{align} info(C|f=f_i) = -\sum_{j=0}^{k-1} \frac{m_{ij}}{m_i} \cdot log_{2} \frac{m_{ij}}{m_i} \end{align} \qquad (fml.4.1.4)<br>$$</p>
<p>公式$\frac{m_{ij}}{m_i}$物理含义是当 $f=f_i$ 且 $C=c_j$ 的概率，即条件概率 $p(c_j|f_i)$</p>
<p>因此，<strong><em>Condition Entropy</em></strong> 计算公式为：</p>
<p>$$<br>\begin{align} info(C|f) &amp; = \sum_{i=1}^{I} p(f_i) \cdot info(C|f=f_i) \\ &amp; = - \sum_{i=1}^{I} p(f_i) \cdot \underline { \sum_{j=0}^{k-1} p(c_j|f_i) \cdot log_2 p(c_j|f_i) } \qquad (fml.4.1.5)<br>\end{align}<br>$$</p>
</blockquote>
<p>特征 $f$ 给系统带来的 info gain 等于系统原有的 Entropy 与固定特征 $f$ 的 <strong><em>Condition Entropy</em></strong> 之差，公式表示如下:</p>
<blockquote>
<p>$$<br>\begin{align} IG(F) &amp; = E(C) - E(C|F) \\ &amp; = -\sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) + \sum_{i=1}^{I} p(f_i) \cdot \underline { \sum_{j=0}^{k-1} p(c_j|f_i) \cdot log_2 p(c_j|f_i) } \end{align}  \qquad (fml.4.1.6)<br>$$</p>
<p>$n$ 表示特征 $f$ 取值个数，$k$ 表示类别 $C$ 个数，$\sum_{j=0}^{n-1} \frac{m_{ij}}{m_i} \cdot log_{2} \frac{m_{ij}}{m_i}$ 表示每一个类别对应的 Entropy 。</p>
</blockquote>
<hr>
<p><strong>下面以天气数据为例,通过 <code>Info gain</code> 选择最优 feature 的过程 :</strong></p>
<blockquote>
<p>根据 阴晴、温度、湿度 和 刮风 来决定是否出去玩。样本中总共有 14 条记录，取值为 <code>是</code>(9个正样本)、<code>否</code>(5个负样本)，用 $S(9+,5−)$ 表示.</p>
<p>(1). 分类系统的 Entropy :<br>$$<br>Entropy(S) = info(9,5) = (-\frac{9}{14} _ llog_2 (\frac{9}{14})) + (- \frac{5}{14} _ llog_2 (\frac{5}{14})) = 0.940位   \quad (exp.4.1.1)<br>$$<br>(2). 如果以特征”阴晴”作为根节点。“阴晴”取值为{sunny, overcast, rainy}, 分别对应的正负样本数分别为(2+,3-), (4+,0-), (3+,2-)，那么在这三个节点上的 info Entropy 分别为：<br>$$<br>\begin{align} &amp; Entropy(S| “阴晴”=sunny) = info(2,3) = 0.971位  \quad(exp.4.1.1) \\ &amp; Entropy(S| “阴晴”=overcast) = info(4,0) = 0位  \;\;\quad(exp.4.1.2) \\ &amp; Entropy(S| “阴晴”=rainy) = info(3,2) = 0.971位  \;\quad(exp.4.1.3) \end{align}<br>$$</p>
<p>以 Feature “阴晴” 为根节点，平均信息值（即 <strong>Condition Entropy</strong>）为：<br>$$<br>Entropy(S|“阴晴”) = \frac{5}{14} * 0.971 + \frac{4}{14} * 0 + \frac{5}{14} * 0.971 = 0.693位 \quad (exp.4.1.4)<br>$$</p>
<p>以 Feature “阴晴” 为条件，计算得到的 <strong>Condition Entropy</strong> 代表了期望的信息总量，即对于一个新样本判定其属于哪个类别所必需的信息量。</p>
<p>(3). 计算特征“阴晴”“阴晴”对应的信息增益:<br>$$<br>IG( “阴晴”) = Entropy(S) - Entropy(S| “阴晴”) = 0.247位 \quad\quad(exp.4.1.5)<br>$$</p>
<p>同样的计算方法，可得每个特征对应的信息增益，即<br>$$<br>IG(“刮风”) = Entropy(S) - Entropy(S|“刮风”) = 0.048位 \qquad\qquad(exp.4.1.6) \\ IG(“湿度”) = Entropy(S) - Entropy(S|“湿度”) = 0.152位 \qquad\qquad(exp.4.1.7) \\ IG(“温度”) = Entropy(S) - Entropy(S|“温度”) = 0.029位 \qquad\qquad(exp.4.1.8)<br>$$</p>
<p>显然，Feature “阴晴” 的 info gain 最大，于是把它作为划分特征。基于“阴晴”对根节点进行划分的结果，如图4.5所示（决策树学习过程部分）。决策树学习算法对子节点进一步划分，重复上面的计算步骤。</p>
</blockquote>
<p><img src="/images/ml/decision-tree/decision-tree-2.png" width="560" height="400" img=""></p>
<h3 id="4-2-Gain-ratio"><a href="#4-2-Gain-ratio" class="headerlink" title="4.2 Gain ratio"></a>4.2 Gain ratio</h3><p>与 Info Gain 不同，<code>Gain ratio</code> 的计算考虑了 F 分裂数据集后所产生的子节点的数量和规模，而<strong>忽略任何有关类别的信息</strong>。</p>
<blockquote>
<p>以 info gain 示例为例，按照 特征“阴晴” 将数据集分裂成3个子集，规模分别为5、4和5，因此不考虑子集中所包含的类别，产生一个分裂信息为：</p>
<p>$$<br>SplitInfo(“阴晴”) = info(5,4,5) = 1.577位 \qquad (exp.4.2.1)<br>$$<br><strong>Split Information Entropy</strong> 可简单地理解为表示信息分支所需要的信息量。 </p>
<p>那么 Info Gain ratio ：<br>$$<br>IG_{ratio}(F) = \frac {IG(F)} {SplitInfo(F)} \qquad (exp.4.2.2)<br>$$<br>在这里，特征 “阴晴”的 Gain ratio 为 $IG_{ratio}( “阴晴”)=\frac{0.247}{1.577} = 0.157$。减少信息增益方法对取值数较多的特征的影响。(可以减少过拟合，这等于是对 某特征取值过多的一个惩罚)</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>-(math.log((<span class="number">5.0</span>/<span class="number">14.0</span>), <span class="number">2</span>) * (<span class="number">5.0</span>/<span class="number">14.0</span>) * <span class="number">2</span> + (<span class="number">4.0</span>/<span class="number">14.0</span>) * (math.log((<span class="number">4.0</span>/<span class="number">14.0</span>), <span class="number">2</span>)))</span><br><span class="line"><span class="number">1.577406282852345</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.cnblogs.com/fengfenggirl/p/classsify_decision_tree.html" target="_blank" rel="external">逗比算法工程师</a>、<a href="http://www.52caml.com/" target="_blank" rel="external">算法杂货铺</a>、<a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="external">52caml</a></li>
<li><a href="http://blog.csdn.net/ljp812184246/article/details/47402639" target="_blank" rel="external">决策树ID3、C4.5、CART算法：信息熵，区别，剪枝理论总结</a></li>
<li>《机器学习导论》《统计学习方法》</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[中国人起名学问]]></title>
      <url>http://sggo.me/2016/08/13/tools/Chinese-named/</url>
      <content type="html"><![CDATA[<p>我对中国人起名学问的自我研究记录</p>
<a id="more"></a>
<h2 id="1-五行八字介绍"><a href="#1-五行八字介绍" class="headerlink" title="1. 五行八字介绍"></a>1. 五行八字介绍</h2><h3 id="1-1-五行"><a href="#1-1-五行" class="headerlink" title="1.1 五行"></a>1.1 五行</h3><p>　五行就是周易中常说的木、火、土、金、水。易经中各类事物均可以五行来区分，如干支、地支、季节、方位、人体、颜色、味道等等。五行有相生与相克的特质，很多的事物可藉由五行的运算，了解元素之兴衰，来判别事物的起伏变化和你一生的好坏吉凶。</p>
<h3 id="1-2-三才"><a href="#1-2-三才" class="headerlink" title="1.2 三才"></a>1.2 三才</h3><p>　　三才配置即天、人、地。中国传统文化理解人立足于大地之上，在天之下，受到上天之眷顾，大地的滋养，而生生不息。亦有顺应天时、地利、人和的行事哲理。三才配置在姓名学中，占有很大的分量。</p>
<h3 id="1-3-五格"><a href="#1-3-五格" class="headerlink" title="1.3 五格"></a>1.3 五格</h3><p>　　五格配置是指天格、地格、人格、外格、总格共五格之间的关系。天格是由祖先流传而来，单独出现对人生没有多大影响;人格是姓名剖象数理的中心所在，对人生的影响最大;人格与地格结合的数理则为基础运。地格主要是36岁前的人生，也叫前运力，外格代表人的外围，吉凶无谓。总格是36岁以后的人生，也是后运力。五格配置在姓名学中占有主要位置。</p>
<h2 id="2-判断五行所缺"><a href="#2-判断五行所缺" class="headerlink" title="2. 判断五行所缺"></a>2. 判断五行所缺</h2><blockquote>
<p>起名字，需要根据五行八字所缺少的五行来用相应的属性字(更准确的说法是用<code>喜用神</code>)来填补，所以需要判断五行所缺。</p>
</blockquote>
<p><a href="http://www.360doc.com/content/15/0313/16/15585030_454852610.shtml" target="_blank" rel="external">解析四柱八字的精髓喜用神</a><br><a href="http://baike.baidu.com/view/1373942.htm" target="_blank" rel="external">喜用神百科</a><br><a href="http://blog.sina.com.cn/s/blog_6e775646010136qp.html" target="_blank" rel="external">如何知道自己八字的喜神，用神</a></p>
<blockquote>
<p>八字就是从这个平衡理论，去分析人一生的起落</p>
</blockquote>
<p>　　知道了五行的知识，我们也知道很多人的八字五行都不太齐全，有的不只缺一个，还会缺两个甚至三个。比如我们常听说的“五行缺金”之类的话，那么如何知道起名字五行缺什么呢?</p>
<h3 id="2-1-判断五行八字及喜用神"><a href="#2-1-判断五行八字及喜用神" class="headerlink" title="2.1 判断五行八字及喜用神"></a>2.1 判断五行八字及喜用神</h3><p>查询五行八字是为了看八字中缺少五行中的哪一行(<strong>以喜用神为准</strong>)，然后用名中字尽量补齐 : <code>金、木、水、火、土</code></p>
<p><strong>以 2015年10月11日13时30分 为例查询检验判断五行八字</strong></p>
<p>以下网址查询结果一致 :</p>
<ul>
<li><a href="http://www.sheup.com/shengchenbazi.php" target="_blank" rel="external">三藏八字查询</a></li>
<li><a href="http://www.zhyw.net/myweb/bz/bazi.htm" target="_blank" rel="external">周易网</a>  </li>
<li><a href="http://www.69jk.cn/tools/bazi/" target="_blank" rel="external">中国健康网</a></li>
</ul>
<p>以下网址查询结果一致 :</p>
<ul>
<li><a href="http://www.meimingteng.com/Tool/Bazi.aspx" target="_blank" rel="external">美名腾</a></li>
<li><a href="http://m.zhouyi.cc/bazi/xys/xiyongsheng.php" target="_blank" rel="external">易安居算命网</a></li>
<li><a href="http://www.fututa.com/" target="_blank" rel="external">浮图塔</a></li>
<li><a href="http://bazi.dosame.com/" target="_blank" rel="external">生辰八字喜用神查询</a></li>
<li><a href="http://sm.wonyoo.com" target="_blank" rel="external">中华忘忧网</a></li>
</ul>
<blockquote>
<p> 这类网址很多，如 : <a href="http://www.meimingteng.com/Tool/Bazi.aspx" target="_blank" rel="external">美名腾</a>、<a href="http://www.pcbaby.com.cn/tools/scbz/" target="_blank" rel="external">太平洋亲子网</a>、<a href="http://www.zhyw.net/myweb/bz/bazi.htm" target="_blank" rel="external">周易网</a>、<a href="http://www.69jk.cn/tools/bazi/" target="_blank" rel="external">中国健康网</a>、<a href="http://www.sheup.com/shengchenbazi.php" target="_blank" rel="external">三藏八字查询</a> 等等。请自行判断查询结果准确性。</p>
<p>周易网、中国健康网、三藏八字查询结果 与 老爷书 一致</p>
</blockquote>
<p><code>喜用神</code>查询你也可以参考如下网址查询，但准确性请自行裁断。</p>
<ul>
<li><a href="http://www.meimingteng.com/Tool/Bazi.aspx" target="_blank" rel="external">美名腾</a></li>
<li><a href="http://m.zhouyi.cc/bazi/xys/xiyongsheng.php" target="_blank" rel="external">易安居</a></li>
<li><a href="http://www.fututa.com/" target="_blank" rel="external">浮图塔</a></li>
<li><a href="http://www.babyqiming.com/zybz/bz.php" target="_blank" rel="external">babyqiming喜用神</a></li>
<li><a href="http://v.youku.com/v_show/id_XNDAwMTAwNDQ0.html" target="_blank" rel="external">优酷视频: 怎么样算八字 什么是八字喜用神 四柱</a></li>
</ul>
<h3 id="2-2-五行不缺的情况"><a href="#2-2-五行不缺的情况" class="headerlink" title="2.2 五行不缺的情况"></a>2.2 五行不缺的情况</h3><p>五行不缺的情况，参见 <a href="http://baike.baidu.com/view/1373942.htm" target="_blank" rel="external">喜用神</a>。<br>五行缺少的情况，参见 <a href="http://baike.baidu.com/view/1373942.htm" target="_blank" rel="external">喜用神</a>，但在多数情况下，缺少的一个就为它的喜用神，具体参见喜用神查询。 </p>
<blockquote>
<p>喜用神查询 与 八字五行 查询，请仔细辨别查询的结果可靠性。</p>
</blockquote>
<h2 id="3-姓名笔画吉凶"><a href="#3-姓名笔画吉凶" class="headerlink" title="3. 姓名笔画吉凶"></a>3. 姓名笔画吉凶</h2><blockquote>
<p>推荐 31 或者 23画</p>
</blockquote>
<p>二十三画　　　旭日升天，名显四方，渐次进展，终成大业。　　（吉）<br>三十一画　　　此数大吉，名利双收，渐进向上，大业成就。　　（吉）</p>
<p><a href="http://blog.sina.com.cn/s/blog_4d4f386c0102vg9r.html" target="_blank" rel="external">详见姓名笔画吉凶大全</a></p>
<blockquote>
<p>起名中用字的笔画数参见 <a href="http://tool.httpcn.com/KangXi/" target="_blank" rel="external">康子字典</a> (非简体字或繁体字)</p>
</blockquote>
<h2 id="4-手机APP的运用"><a href="#4-手机APP的运用" class="headerlink" title="4. 手机APP的运用"></a>4. 手机APP的运用</h2><ol>
<li>美名腾智能起名 APP</li>
<li>…</li>
</ol>
<blockquote>
<p>运用相关手机app，可以更方便的辅助想出好名字</p>
</blockquote>
<h2 id="5-姓名打分测试"><a href="#5-姓名打分测试" class="headerlink" title="5. 姓名打分测试"></a>5. 姓名打分测试</h2><p>打分测试这种网站比较多，根据我个人的经验判别我推荐 <a href="http://www.sheup.com/xingming_dafen.php" target="_blank" rel="external">三藏网打分测试</a>，根据三藏网查询的结果，可以看到 三才五格的解析以及康熙字典的笔画数目等。</p>
<hr>
<p>举例如下 :</p>
<p><img src="/images/life/life-named-xin.png" alt="example"></p>
<h2 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a>6. 小结</h2><p>综上所述: 起名字只需要在名字中，使用喜用神的字 并且 三才五格打分 都比较不错的情况下，方为好名字。</p>
<blockquote>
<p>个人意见 : 名字还是 顺耳，脱俗 的名字是最重要的。</p>
</blockquote>
<p><strong>起名选择字的方法一如如下图片所示，可参考百度搜索的姓名学</strong></p>
<p><img src="/images/life/life-named-searchword.png" alt="选择字方法参见"></p>
<h2 id="7-Reference-article"><a href="#7-Reference-article" class="headerlink" title="7. Reference article"></a>7. Reference article</h2><ul>
<li><a href="http://www.zhyw.net/myweb/bz/bazi.htm" target="_blank" rel="external">周易网</a>  </li>
<li><a href="http://www.69jk.cn/tools/bazi/" target="_blank" rel="external">中国健康网</a></li>
<li><a href="http://www.meimingteng.com/Tool/Bazi.aspx" target="_blank" rel="external">美名腾</a></li>
<li><a href="http://www.pcbaby.com.cn/tools/scbz/" target="_blank" rel="external">太平洋亲子网</a></li>
<li><a href="http://www.360doc.com/content/15/0313/16/15585030_454852610.shtml" target="_blank" rel="external">解析四柱八字的精髓喜用神</a>  </li>
<li><a href="http://www.babyqiming.com/zybz/bz.php" target="_blank" rel="external">不一定准确的喜用神查询</a></li>
<li><a href="http://www.chinesefortunecalendar.com/CAb5.htm" target="_blank" rel="external">八字五行算命和人生起伏圖</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_4d4f386c0102vg9r.html" target="_blank" rel="external">详见姓名笔画吉凶大全</a></li>
<li><a href="http://tool.httpcn.com/KangXi/" target="_blank" rel="external">康子字典</a></li>
<li><a href="http://www.sheup.com/xingming_dafen.php" target="_blank" rel="external">三藏网</a></li>
<li><a href="http://www.7mingzi.com/hanziwuxing-zi-%E9%99%88/" target="_blank" rel="external">起名网</a></li>
<li>…</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">公历：****年**月**日(星期四)11点</span><br><span class="line">农历：丙申年四月十三日午时</span><br><span class="line">春节：2月8日</span><br><span class="line">节前：乙未年</span><br><span class="line">节后：丙申年</span><br><span class="line">八字：丙申　癸巳　辛丑　甲午</span><br><span class="line">五行：火金　水火　金土　木火</span><br><span class="line">方位：南西　北南　西中　东南</span><br><span class="line">生肖：猴</span><br><span class="line">92 三才配置 吉， 天地人外总格 大吉  31画</span><br><span class="line">陈俊妃</span><br><span class="line">陈俊帆</span><br><span class="line">陈泊羊</span><br><span class="line">陈泊亦</span><br><span class="line">陈音竹</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python Data Mining and Analysis environment]]></title>
      <url>http://sggo.me/2016/08/02/python/py-language-1-data-analysis-environment/</url>
      <content type="html"><![CDATA[<p>这是用 Python 进行数据分析挖掘的一小部分，包括 高维数组、数值计算、机器学习、神经网络 和 语言模型等。</p>
<a id="more"></a>
<h2 id="1-Python-data-analysis-intro"><a href="#1-Python-data-analysis-intro" class="headerlink" title="1. Python data analysis intro"></a>1. Python data analysis intro</h2><p><a href="http://www.python.org" target="_blank" rel="external">Python</a></p>
<ul>
<li>优雅的语法和动态类型</li>
<li>拥有高级数据结构、OO</li>
<li>Functional Programming</li>
<li>解释性、胶水语言，开发效率高</li>
<li>库丰富, NumPy, SciPy, Matplotlib, Pandas</li>
<li>适合于 Scientific Computing、Mathematical Modeling、Data mining …</li>
</ul>
<p><strong>import <code>future</code> feature</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from __futrue__ import print_function</span><br><span class="line">from __futrue__ import division</span><br></pre></td></tr></table></figure>
<p><strong>install third package</strong></p>
<blockquote>
<p>三种方式  </p>
<ol>
<li>下载源代码自行安装 : 安装灵活， 但需要自行解决上级依赖问题。  </li>
<li>用 pip 安装 : 比较方便，自动解决上级依赖问题  </li>
<li>系统自带的安装方式 : apt-get or brew ..</li>
</ol>
</blockquote>
<h3 id="1-1-Install-pip"><a href="#1-1-Install-pip" class="headerlink" title="1.1 Install pip"></a>1.1 Install pip</h3><blockquote>
<p>pip 是安装python包的工具，提供了安装包，列出已经安装的包，升级包以及卸载包的功能。<br>pip 是对easy_install的取代，提供了和easy_install相同的查找包的功能</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">which</span> python</span><br><span class="line">wget https://bootstrap.pypa.io/get-pip.py</span><br><span class="line">sudo python get-pip.py</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">修改pip源 （可选）</span><br><span class="line">由于天朝原因,使用pip安装一些模块会特别慢甚至无法下载,因此我们需要修改pip的源到国内的一些镜像地址.</span><br><span class="line">cd ~</span><br><span class="line">mkdir .pip</span><br><span class="line">vim pip.conf</span><br><span class="line">添加以下两行</span><br><span class="line">[global]</span><br><span class="line">index-url = http://pypi.v2ex.com/simple</span><br><span class="line">把index-url的值设置为自己实际源的地址.</span><br><span class="line">至此pip源修改成功,以后使用pip安装模块时都会从这个源去下载安装.</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  tar.gz ll</span><br><span class="line">-rw-r--r-- 1 hp staff   1138794 Mar 11 16:09 pip-8.1.0.tar.gz</span><br><span class="line">-rw-r--r-- 1 hp staff    630700 Mar 11 13:38 setuptools-18.1.tar.gz</span><br><span class="line">tar -xvf setuptools-18.1.tar.gz</span><br><span class="line">tar -xvf pip-8.1.0.tar.gz</span><br><span class="line"><span class="built_in">cd</span> setuptools-18.1</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br><span class="line"><span class="built_in">cd</span> pip-9.0.1/</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>
<blockquote>
<p>9.0.1 见 <a href="https://pypi.python.org/pypi/pip" target="_blank" rel="external">https://pypi.python.org/pypi/pip</a></p>
</blockquote>
<p><strong>ipython</strong></p>
<blockquote>
<p>sudo pip install –upgrade ipython –ignore-installed six<br>sudo pip install notebook</p>
</blockquote>
<p>startup ipython notebook</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ipython notebook</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=<span class="string">"notebook --ip=192.168.140.159"</span> $SPARK_HOME/bin/pyspark</span><br></pre></td></tr></table></figure>
<h2 id="2-Python-Tools-for-data-analysis"><a href="#2-Python-Tools-for-data-analysis" class="headerlink" title="2. Python Tools for data analysis"></a>2. Python Tools for data analysis</h2><table>
<thead>
<tr>
<th>Extension lib</th>
<th>introduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numpy</td>
<td>提供数组支持，以及相应的高效处理函数</td>
</tr>
<tr>
<td>Scipy</td>
<td>提供矩阵支持，以及矩阵相关的数值计算模块</td>
</tr>
<tr>
<td>Matplotlib</td>
<td>数据可视化工具，作图库</td>
</tr>
<tr>
<td>Pandas</td>
<td>数据分析和探索工具</td>
</tr>
<tr>
<td>StatsModels</td>
<td>统计建模和计量经济学，包括描述统计，统计模型估计和推断</td>
</tr>
<tr>
<td>Scikit-Learn</td>
<td>支持回归，分类，聚类 等强大的机器学习库</td>
</tr>
<tr>
<td>Keras</td>
<td>深度学习库，用于建立神经网络以及 deep learning model</td>
</tr>
<tr>
<td>Gensim</td>
<td>用来做 text topic model 的库</td>
</tr>
<tr>
<td>Pillow</td>
<td>旧版的PIL, 图片处理相关</td>
</tr>
<tr>
<td>OpenCV</td>
<td>video 处理相关</td>
</tr>
<tr>
<td>GMPY2</td>
<td>高精度计算相关</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="2-1-Numpy"><a href="#2-1-Numpy" class="headerlink" title="2.1 Numpy"></a>2.1 Numpy</h3><p><a href="www.numpy.prg">Numpy</a> 提供了数据功能, 后续 Scipy、Matplotlib、Pandas 等都依赖于它。    </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sudo pip install numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line">print(<span class="string">"hello world 3.0 !"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-Scipy"><a href="#2-2-Scipy" class="headerlink" title="2.2 Scipy"></a>2.2 Scipy</h3><blockquote>
<p>Numpy 提供了多维数据功能，但它只是数组，并不是矩阵。Scipy 提供了真正的矩阵，以及大量矩阵运算的对象和函数。<br>Scipy 依赖于 Numpy</p>
</blockquote>
<h3 id="2-3-Matplotlib"><a href="#2-3-Matplotlib" class="headerlink" title="2.3 Matplotlib"></a>2.3 Matplotlib</h3><blockquote>
<p>著名的绘图库，主要用于二维绘图，当然也可以进行三维绘图。<br>sudo pip install matplotlib </p>
</blockquote>
<h3 id="2-4-Pandas"><a href="#2-4-Pandas" class="headerlink" title="2.4 Pandas"></a>2.4 Pandas</h3><p>Pandas 是 Python 下最强大的数据分析 Tool，没有之一。Pandas 构建在 Numpy 之上。  </p>
<p><strong>Pandas Function</strong></p>
<ol>
<li>类SQL，CRUD</li>
<li>数据处理函数</li>
<li>时间序列分析功能</li>
<li>灵活处理缺失数据</li>
</ol>
<blockquote>
<p>sudo pip install pandas<br>sudo pip install xlrd<br>sudo pip install xlwt<br>《利用python进行数据分析》讲解详细，针对 Pandas。  </p>
</blockquote>
<p>Pandas 基本的数据结构是 : Series 和 DataFrame (它的每一列都是一个Series)。每个 Series 都会有一个对应的 Index，用来标记元。(Index类似于 SQL 主键)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">s = pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]) <span class="comment"># 创建一个序列 s</span></span><br><span class="line">d2 = pd.DataFrame(s)</span><br><span class="line"></span><br><span class="line">d = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], columns=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]) <span class="comment"># 创建一个 table</span></span><br><span class="line"></span><br><span class="line">d.head() <span class="comment"># 默认预览前 5 行</span></span><br><span class="line"></span><br><span class="line">d.describe() <span class="comment"># 数据基本统计量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文件</span></span><br><span class="line">pd.read_excel(<span class="string">'data.xls'</span>) <span class="comment"># 读取 Excel 文件, 创建 DataFrame.</span></span><br><span class="line"><span class="comment"># pd.read_csv('data.csv', encoding='utf-8') # 读取文本, 一般指定 encoding</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-StatsModels"><a href="#2-5-StatsModels" class="headerlink" title="2.5 StatsModels"></a>2.5 StatsModels</h3><blockquote>
<p>StatsModels 主要是对，数据的读取、处理、探索，更加注重数据的统计建模分析，有 R 语言味道。<br>StatsModels 与 Pandas 结合, 成为 Python 下强大的数据挖掘组合。<br>sudo pip install StatsModels</p>
</blockquote>
<h3 id="2-6-Scikit-Learn"><a href="#2-6-Scikit-Learn" class="headerlink" title="2.6 Scikit-Learn"></a>2.6 Scikit-Learn</h3><blockquote>
<p>Scikit-Learn 强大的 ML 工具包。包括 数据预处理、分类、回归、聚类、预测 和 模型分析等。<br>Scikit-Learn 依赖于 Numpy、Scipy、Matplotlib。</p>
</blockquote>
<p><strong>install</strong></p>
<p>pip install scikit-learn 用 pip 安装这个包之后，在使用的时候会出现 ValueError: numpy.dtype has the wrong 等错误。</p>
<p><strong>solution fun</strong></p>
<p>sudo pip install cython<br>git clone <a href="https://github.com/scikit-learn/scikit-learn" target="_blank" rel="external">https://github.com/scikit-learn/scikit-learn</a><br>sudo make<br>sudo python setup.py install</p>
<blockquote>
<p>不安装 cython ，安装 scikit-learn 会报错。<br>这种方式 安装 scikit-learn 过程中的一些错误或警告不需要管。安装完成测试使用正常<br>pip list<br>scikit-learn (0.18.dev0)<br>scipy (0.13.0b1)</p>
</blockquote>
<h3 id="2-7-Keras"><a href="#2-7-Keras" class="headerlink" title="2.7 Keras"></a>2.7 Keras</h3><blockquote>
<p>神经网络model</p>
</blockquote>
<h3 id="2-8-Gensim"><a href="#2-8-Gensim" class="headerlink" title="2.8 Gensim"></a>2.8 Gensim</h3><blockquote>
<p>topic modelling for humans！NLP</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Elasticsearch * 入门]]></title>
      <url>http://sggo.me/2016/05/17/elasticsearch/es-indoor/</url>
      <content type="html"><![CDATA[<p>Elasticsearch 是一个基于Apache Lucene(TM)的开源搜索引擎 、 实时分布式搜索 和 分析引擎。</p>
<a id="more"></a>
<blockquote>
<p>Lucene 是 成熟的<code>全文索引与信息检索(IR)库</code>，采用Java实现。信息检索式指文档搜索、文档内信息搜索或者文档相关的元数据搜索等操作。。</p>
<p>Solr是一个基于Lucene <code>java库的企业级搜索服务器</code>，包含XML/HTTP，JSON API，高亮查询结果，缓存，复制，还有一个WEB管理界面。Solr运行在Servlet容器中</p>
<p>2010 年 Elasticsearch 出现公开版本</p>
</blockquote>
<p><strong>Elasticsearch 涉及的技术</strong></p>
<ul>
<li>全文搜索</li>
<li>分析系统</li>
<li>分布式数据库</li>
</ul>
<p><strong>谁在使用 Elasticsearch?</strong></p>
<ul>
<li><a href="https://zh.wikipedia.org/wiki/Wikipedia:%E9%A6%96%E9%A1%B5" target="_blank" rel="external">维基百科</a></li>
<li><a href="http://stackoverflow.com/" target="_blank" rel="external">StackOverflow</a></li>
<li><a href="https://github.com/libean" target="_blank" rel="external">Github</a><br>…</li>
</ul>
<h2 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h2><p>Elasticsearch 是 开源搜索引擎.</p>
<p><strong>Elasticsearch 不仅是全文搜索，还是：</strong></p>
<ul>
<li>分布式 实时文件存储，每个字段都被索引并可被搜索</li>
<li>分布式 实时分析搜索引擎</li>
<li>可扩展服务器，处理<strong>PB</strong>级结构化或非结构化数据</li>
</ul>
<p>这些功能被集成到一个服务里面，应用可通过 <code>RESTful API</code>、各种语言的<code>客户端</code>、<code>命令行</code> 与之交互。</p>
<p>Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的<code>RESTful API</code>来隐藏Lucene的复杂性，从而让全文搜索变得简单。</p>
<p><a href="http://es.xiaoleilu.com/010_Intro/10_Installing_ES.html" target="_blank" rel="external">more_info_install_elaticsearch</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[deploy@node196 config]$ ll</span><br><span class="line">total 20</span><br><span class="line">-rw-rw-r--  1 deploy deploy 13915 May 10 10:02 elasticsearch.yml</span><br><span class="line">-rw-rw-r--. 1 deploy deploy  2054 Jul 16  2015 logging.yml</span><br><span class="line">[deploy@node196 config]$ pwd</span><br><span class="line">/home/deploy/elasticsearch-1.7.1/config</span><br></pre></td></tr></table></figure>
<p><strong>编辑 elasticsearch.yml</strong> </p>
<p>替代cluster.name的默认值，这样可以防止一个新启动的节点加入到相同网络中的另一个同名的集群中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cluster.name: elasticsearch_your-company-name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#################################### Node #####################################</span><br><span class="line"></span><br><span class="line"># Node names are generated dynamically on startup, so you&apos;re relieved</span><br><span class="line"># from configuring them manually. You can tie this node to a specific name:</span><br><span class="line">#</span><br><span class="line">node.name: &quot;node196&quot;</span><br></pre></td></tr></table></figure>
<h2 id="2-API"><a href="#2-API" class="headerlink" title="2. API"></a>2. API</h2><h3 id="2-1-Java-API"><a href="#2-1-Java-API" class="headerlink" title="2.1 Java API"></a>2.1 Java API</h3><p><strong>节点客户端(node client)</strong></p>
<p>节点客户端以无数据节点(none data node)身份加入集群，换言之，它自己不存储任何数据，但是它知道数据在集群中的具体位置，并且能直接转发请求到对应的节点上。</p>
<p><strong>传输客户端(Transport client)</strong></p>
<p>更轻量的传输客户端 能发送请求到远程集群。它自己不加入集群，只是简单转发请求给集群中的节点。</p>
<blockquote>
<p>两个Java客户端都通过 9300端口 与 集群交互，使用 Elasticsearch传输协议(<code>Elasticsearch Transport Protocol</code>)。集群中的节点之间也通过 9300 port 进行通信。</p>
</blockquote>
<p><a href="https://www.elastic.co/guide/index.html" target="_blank" rel="external">more_info_Java-API</a></p>
<h3 id="2-2-RESTful-API"><a href="#2-2-RESTful-API" class="headerlink" title="2.2 RESTful API"></a>2.2 RESTful API</h3><ul>
<li>基于 HTTP 协议，以 JSON 为数据交互格式的 RESTful API</li>
</ul>
<blockquote>
<p>向 Elasticsearch 发出的请求的组成部分与其它普通的HTTP请求是一样的：<br>curl -X<verb> ‘<protocol>://<host>:<port>/<path></path>?<query_string>‘ -d ‘<body>‘</body></query_string></port></host></protocol></verb></p>
<p>VERB HTTP方法：GET, POST, PUT, HEAD, DELETE<br>…</p>
</blockquote>
<p>example : 查询集群中 文档数量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[deploy@node196 config]$ curl -u username:passwd -XGET &apos;localhost:9200/_count?pretty&apos; -d &apos;</span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;     &quot;query&quot;: &#123;</span><br><span class="line">&gt;         &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">&gt;     &#125;</span><br><span class="line">&gt; &#125;&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;count&quot; : 100001234,</span><br><span class="line">  &quot;_shards&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 376,</span><br><span class="line">    &quot;successful&quot; : 376,</span><br><span class="line">    &quot;failed&quot; : 0</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -u name:pass -X DELETE http://ip:9200/your_index</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /coupon_seeker/coupon_seeker/_search?q=source:dianping</span><br><span class="line"></span><br><span class="line">curl -u name:pass -XGET &apos;http://192.168.181.xxx:9200/coupon_seeker/coupon_seeker/_search?q=source:dianping</span><br></pre></td></tr></table></figure>
<p>有条件的精确匹配删除命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -u name:pass -XDELETE &apos;http://192.168.181.xxx:9200/coupon_seeker/coupon_seeker/_query?pretty=true&apos; -d &apos;&#123;&quot;query&quot;:&#123;&quot;match&quot;:&#123;source:&quot;dianping&quot;&#125;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure>
<h2 id="3-文档"><a href="#3-文档" class="headerlink" title="3. 文档"></a>3. 文档</h2><p><strong>面向文档</strong></p>
<p>Elasticsearch is document oriented，这意味着它可以存储整个 <code>object</code> 或 <code>document</code>。</p>
<p>Elasticsearch 还可以 索引(index) 每个文档的内容使之可以被 <strong>搜索</strong>。</p>
<p>可对 <code>document</code> （而非成行成列的数据）进行 <code>index</code>、<code>搜索</code>、<code>排序</code>、<code>过滤</code>。</p>
<p>这种理解数据的方式与以往完全不同，这也是 Elasticsearch 能够执行复杂的全文搜索的原因之一。</p>
<p><strong>JSON</strong> (JavaScript Object Notation)，文档序列化格式</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"email"</span>:      <span class="string">"john@smith.com"</span>,</span><br><span class="line">    <span class="attr">"first_name"</span>: <span class="string">"John"</span>,</span><br><span class="line">    <span class="attr">"last_name"</span>:  <span class="string">"Smith"</span>,</span><br><span class="line">    <span class="attr">"info"</span>: &#123;</span><br><span class="line">        <span class="attr">"bio"</span>:         <span class="string">"Eco-warrior and defender of the weak"</span>,</span><br><span class="line">        <span class="attr">"age"</span>:         <span class="number">25</span>,</span><br><span class="line">        <span class="attr">"interests"</span>: [ <span class="string">"dolphins"</span>, <span class="string">"whales"</span> ]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"join_date"</span>: <span class="string">"2014/05/01"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如下 user对象很复杂，但它的结构和对象的含义已经被完整的体现在JSON中了，在Elasticsearch中将对象转化为 JSON 并做 index索引 要比在表结构中做相同的事情简单的多。</p>
<h2 id="5-索引"><a href="#5-索引" class="headerlink" title="5. 索引"></a>5. 索引</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hdfs@node196 data_analysis]$ curl -u username:passwd -XPUT http://node190:9200/megacorp/employee/1 -d &apos;</span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;     &quot;first_name&quot; : &quot;John&quot;,</span><br><span class="line">&gt;     &quot;last_name&quot; :  &quot;Smith&quot;,</span><br><span class="line">&gt;     &quot;age&quot; :        25,</span><br><span class="line">&gt;     &quot;about&quot; :      &quot;I love to go rock climbing&quot;,</span><br><span class="line">&gt;     &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]</span><br><span class="line">&gt; &#125;&apos;</span><br><span class="line">&#123;&quot;_index&quot;:&quot;megacorp&quot;,&quot;_type&quot;:&quot;employee&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:1,&quot;created&quot;:true&#125;[hdfs@node196 data_analysis]$</span><br></pre></td></tr></table></figure>
<ul>
<li>indexing</li>
<li>search</li>
<li>aggregations  /ˌæɡrɪˈɡeɪʃən/</li>
</ul>
<p><strong>Elasticsearch能做的事</strong></p>
<p>场景: 假设我们刚好在Megacorp工作，这时人力资源部门出于某种目的需要让我们创建一个员工目录，这个目录用于促进人文关怀和用于实时协同工作，所以它有以下不同的需求：</p>
<ul>
<li>数据能够包含多个值的标签、数字和纯文本。</li>
<li>检索任何员工的所有信息。</li>
<li>支持结构化搜索，例如查找30岁以上的员工。</li>
<li>支持简单的全文搜索和更复杂的短语(phrase)搜索</li>
<li>高亮搜索结果中的关键字</li>
<li>能够利用图表管理分析这些数据</li>
</ul>
<p><strong>索引员工文档</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; Columns</span><br><span class="line">Elasticsearch -&gt; Indices   -&gt; Types  -&gt; Documents -&gt; Fields</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>Elasticsearch</th>
<th>Relational DB</th>
</tr>
</thead>
<tbody>
<tr>
<td>Indices</td>
<td>Databases</td>
</tr>
<tr>
<td>Types</td>
<td>Tables</td>
</tr>
<tr>
<td>Documents</td>
<td>Rows</td>
</tr>
<tr>
<td>Fields</td>
<td>Columns</td>
</tr>
</tbody>
</table>
<p><strong>Elasticsearch</strong></p>
<blockquote>
<p>索引」含义的区分</p>
<p>  <strong>index_num.</strong> : index (数据库)，它是相关文档存储的地方，</p>
<p>  <strong>index_verb.</strong> 「索引一个文档」表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL 中的 INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。</p>
<p><strong>倒排索引</strong> : 传统数据库为特定列增加一个索引，例如 B-Tree索引 来加速检索。Elasticsearch和Lucene使用倒排索引(inverted index)的数据结构来达到相同目的。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT /megacorp/employee/1</span><br><span class="line">&#123;</span><br><span class="line">    &quot;first_name&quot; : &quot;John&quot;,</span><br><span class="line">    &quot;last_name&quot; :  &quot;Smith&quot;,</span><br><span class="line">    &quot;age&quot; :        25,</span><br><span class="line">    &quot;about&quot; :      &quot;I love to go rock climbing&quot;,</span><br><span class="line">    &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-1-检索文档"><a href="#4-1-检索文档" class="headerlink" title="4.1 检索文档"></a>4.1 检索文档</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/1</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot; :   &quot;megacorp&quot;,</span><br><span class="line">  &quot;_type&quot; :    &quot;employee&quot;,</span><br><span class="line">  &quot;_id&quot; :      &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot; : 1,</span><br><span class="line">  &quot;found&quot; :    true,</span><br><span class="line">  &quot;_source&quot; :  &#123;</span><br><span class="line">      &quot;first_name&quot; :  &quot;John&quot;,</span><br><span class="line">      &quot;last_name&quot; :   &quot;Smith&quot;,</span><br><span class="line">      &quot;age&quot; :         25,</span><br><span class="line">      &quot;about&quot; :       &quot;I love to go rock climbing&quot;,</span><br><span class="line">      &quot;interests&quot;:  [ &quot;sports&quot;, &quot;music&quot; ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-2-简单搜索"><a href="#4-2-简单搜索" class="headerlink" title="4.2 简单搜索"></a>4.2 简单搜索</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br></pre></td></tr></table></figure>
<p>查询 last_name 为 Smith 的记录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search?q=last_name:Smith</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   ...</span><br><span class="line">   &quot;hits&quot;: &#123;</span><br><span class="line">      &quot;total&quot;:      2,</span><br><span class="line">      &quot;max_score&quot;:  0.30685282,</span><br><span class="line">      &quot;hits&quot;: [</span><br><span class="line">         &#123;</span><br><span class="line">            ...</span><br><span class="line">            &quot;_source&quot;: &#123;</span><br><span class="line">               &quot;first_name&quot;:  &quot;John&quot;,</span><br><span class="line">               &quot;last_name&quot;:   &quot;Smith&quot;,</span><br><span class="line">               &quot;age&quot;:         25,</span><br><span class="line">               &quot;about&quot;:       &quot;I love to go rock climbing&quot;,</span><br><span class="line">               &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]</span><br><span class="line">            &#125;</span><br><span class="line">         &#125;,</span><br><span class="line">         &#123;</span><br><span class="line">            ...</span><br><span class="line">            &quot;_source&quot;: &#123;</span><br><span class="line">               &quot;first_name&quot;:  &quot;Jane&quot;,</span><br><span class="line">               &quot;last_name&quot;:   &quot;Smith&quot;,</span><br><span class="line">               &quot;age&quot;:         32,</span><br><span class="line">               &quot;about&quot;:       &quot;I like to collect rock albums&quot;,</span><br><span class="line">               &quot;interests&quot;: [ &quot;music&quot; ]</span><br><span class="line">            &#125;</span><br><span class="line">         &#125;</span><br><span class="line">      ]</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-3-使用DSL语句查询"><a href="#4-3-使用DSL语句查询" class="headerlink" title="4.3 使用DSL语句查询"></a>4.3 使用DSL语句查询</h3><p> DSL(Domain Specific Language特定领域语言) </p>
<p><strong>查询字符串等价于</strong>  q=last_name:Smith <strong>DSL查询 : </strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot; : &#123;</span><br><span class="line">        &quot;match&quot; : &#123;</span><br><span class="line">            &quot;last_name&quot; : &quot;Smith&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-4-更复杂的搜索"><a href="#4-4-更复杂的搜索" class="headerlink" title="4.4 更复杂的搜索"></a>4.4 更复杂的搜索</h3><p><strong>filter range</strong></p>
<p>GET /megacorp/employee/_search<br>{<br>    “query” : {<br>        “filtered” : {<br>            “filter” : {<br>                “range” : {<br>                    “age” : { “gt” : 30 } <1><br>                }<br>            },<br>            “query” : {<br>                “match” : {<br>                    “last_name” : “smith” <2><br>                }<br>            }<br>        }<br>    }<br>}</2></1></p>
<h3 id="4-5-全文搜索"><a href="#4-5-全文搜索" class="headerlink" title="4.5 全文搜索"></a>4.5 全文搜索</h3><p>一种传统数据库难以实现的功能</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot; : &#123;</span><br><span class="line">        &quot;match&quot; : &#123;</span><br><span class="line">            &quot;about&quot; : &quot;rock climbing&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Result :</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   ...</span><br><span class="line">   &quot;hits&quot;: &#123;</span><br><span class="line">      &quot;total&quot;:      2,</span><br><span class="line">      &quot;max_score&quot;:  0.16273327,</span><br><span class="line">      &quot;hits&quot;: [</span><br><span class="line">         &#123;</span><br><span class="line">            ...</span><br><span class="line">            &quot;_score&quot;:         0.16273327, &lt;1&gt;</span><br><span class="line">            &quot;_source&quot;: &#123;</span><br><span class="line">               &quot;first_name&quot;:  &quot;John&quot;,</span><br><span class="line">               &quot;last_name&quot;:   &quot;Smith&quot;,</span><br><span class="line">               &quot;age&quot;:         25,</span><br><span class="line">               &quot;about&quot;:       &quot;I love to go rock climbing&quot;,</span><br><span class="line">               &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]</span><br><span class="line">            &#125;</span><br><span class="line">         &#125;,</span><br><span class="line">         &#123;</span><br><span class="line">            ...</span><br><span class="line">            &quot;_score&quot;:         0.016878016, &lt;2&gt;</span><br><span class="line">            &quot;_source&quot;: &#123;</span><br><span class="line">               &quot;first_name&quot;:  &quot;Jane&quot;,</span><br><span class="line">               &quot;last_name&quot;:   &quot;Smith&quot;,</span><br><span class="line">               &quot;age&quot;:         32,</span><br><span class="line">               &quot;about&quot;:       &quot;I like to collect rock albums&quot;,</span><br><span class="line">               &quot;interests&quot;: [ &quot;music&quot; ]</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line">      ]</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-6-短语搜索-–-phrases"><a href="#4-6-短语搜索-–-phrases" class="headerlink" title="4.6 短语搜索 – phrases"></a>4.6 短语搜索 – phrases</h3><p>想要确切的匹配若干个单词或者短语(phrases), 例如  我们想要查询同时包含”rock”和”climbing”（并且是相邻的）的员工记录。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot; : &#123;</span><br><span class="line">        &quot;match_phrase&quot; : &#123;</span><br><span class="line">            &quot;about&quot; : &quot;rock climbing&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong><em>增加高亮</em></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot; : &#123;</span><br><span class="line">        &quot;match_phrase&quot; : &#123;</span><br><span class="line">            &quot;about&quot; : &quot;rock climbing&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;highlight&quot;: &#123;</span><br><span class="line">        &quot;fields&quot; : &#123;</span><br><span class="line">            &quot;about&quot; : &#123;&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-aggregations"><a href="#5-aggregations" class="headerlink" title="5. aggregations"></a>5. aggregations</h2><p>聚合相当于 group by</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match&quot;: &#123;</span><br><span class="line">      &quot;last_name&quot;: &quot;smith&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;all_interests&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;interests&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">&quot;all_interests&quot;: &#123;</span><br><span class="line">   &quot;buckets&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">         &quot;key&quot;: &quot;music&quot;,</span><br><span class="line">         &quot;doc_count&quot;: 2</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">         &quot;key&quot;: &quot;sports&quot;,</span><br><span class="line">         &quot;doc_count&quot;: 1</span><br><span class="line">      &#125;</span><br><span class="line">   ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>聚合也允许分级汇总。例如，让我们统计每种兴趣下职员的平均年龄</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;all_interests&quot; : &#123;</span><br><span class="line">            &quot;terms&quot; : &#123; &quot;field&quot; : &quot;interests&quot; &#125;,</span><br><span class="line">            &quot;aggs&quot; : &#123;</span><br><span class="line">                &quot;avg_age&quot; : &#123;</span><br><span class="line">                    &quot;avg&quot; : &#123; &quot;field&quot; : &quot;age&quot; &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>教程小结</strong></p>
<p> 为了保持简短，还有很多的特性未提及——像 推荐、定位、渗透、模糊 以及 部分匹配等。但这也突出了构建高级搜索功能是多么的容易。无需配置，只需要添加数据然后开始搜索！</p>
<h2 id="6-分布式的特性"><a href="#6-分布式的特性" class="headerlink" title="6. 分布式的特性"></a>6. 分布式的特性</h2><p>Elasticsearch 你不需要知道任何关于分布式系统、分片、集群发现或者其他大量的分布式概念。所有的教程你即可以运行在你的笔记本上，也可以运行在拥有100个节点的集群上，其工作方式是一样的。</p>
<p>Elasticsearch 致力于隐藏分布式系统的复杂性。以下这些操作都是在底层自动完成的：</p>
<ul>
<li>将你的文档分区到不同的容器或者分片(shards)中，它们可存于一或多个节点中。</li>
<li>将分片均匀的分配到各个节点，对索引和搜索做负载均衡。</li>
<li>冗余每一个分片，防止硬件故障造成的数据丢失。</li>
<li>将集群中任意一个节点上的请求路由到相应数据所在的节点。</li>
<li>无论是增加节点，还是移除节点，分片都可以做到无缝的扩展和迁移。</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Elasticsearch installation plugins]]></title>
      <url>http://sggo.me/2016/05/17/elasticsearch/es-install-plugins/</url>
      <content type="html"><![CDATA[<p> Elasticsearch 扩展性非常好，有很多官方和第三方开发的插件</p>
<a id="more"></a>
<h2 id="1-Elasticsearch-Install"><a href="#1-Elasticsearch-Install" class="headerlink" title="1. Elasticsearch-Install"></a>1. Elasticsearch-Install</h2><p>官网 : <a href="https://www.elastic.co/" target="_blank" rel="external">https://www.elastic.co/</a></p>
<p><strong>Install es</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">download elasticsearch-1.7.5.tar.gz</span><br><span class="line">cd usr/local/mySoft/deploy</span><br><span class="line">tar -xvf elasticsearch-1.7.5.tar.gz</span><br><span class="line">ln -s /usr/local/mySoft/deploy/elasticsearch-1.7.5/ elasticsearch</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">vim ~/.zshrc</span><br><span class="line">export ES_HOME=/usr/local/xSoft/elasticsearch</span><br></pre></td></tr></table></figure>
<p><strong>Config es</strong></p>
<p>$ES_HOME/config/elasticsearch.yml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cluster.name: elasticsearch_x</span><br><span class="line">node.name=test-node1</span><br></pre></td></tr></table></figure>
<p><strong>Startup</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/elasticsearch</span><br><span class="line"></span><br><span class="line">./bin/elasticsearch -d -Xms512m -Xmx512m</span><br></pre></td></tr></table></figure>
<blockquote>
<p>运行之后，会产生 data 和 logs 目录</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  elasticsearch ll</span><br><span class="line">total 28</span><br><span class="line">-rw-r--r--  1 hp staff 11358 Feb  2 17:24 LICENSE.txt</span><br><span class="line">-rw-r--r--  1 hp staff   150 Feb  2 17:24 NOTICE.txt</span><br><span class="line">-rw-r--r--  1 hp staff  8700 Feb  2 17:24 README.textile</span><br><span class="line">drwxr-xr-x 14 hp staff   476 May 26 15:42 bin/</span><br><span class="line">drwxr-xr-x  4 hp staff   136 May 27 11:03 config/</span><br><span class="line">drwxr-xr-x  3 hp staff   102 May 26 11:01 data/</span><br><span class="line">drwxr-xr-x 26 hp staff   884 May 26 09:58 lib/</span><br><span class="line">drwxr-xr-x  7 hp staff   238 May 27 09:58 logs/</span><br><span class="line">drwxr-xr-x  7 hp staff   238 May 27 10:48 plugins/</span><br><span class="line">➜  elasticsearch</span><br></pre></td></tr></table></figure>
<p><strong>Verify</strong></p>
<p>open <a href="http://ip:9200/" target="_blank" rel="external">http://ip:9200/</a></p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"status"</span> : <span class="number">200</span>,</span><br><span class="line">  <span class="attr">"name"</span> : <span class="string">"node01"</span>,</span><br><span class="line">  <span class="attr">"cluster_name"</span> : <span class="string">"elasticsearch_x"</span>,</span><br><span class="line">  <span class="attr">"version"</span> : &#123;</span><br><span class="line">    <span class="attr">"number"</span> : <span class="string">"1.7.5"</span>,</span><br><span class="line">    <span class="attr">"build_hash"</span> : <span class="string">"00f95f4ffca6de89d68b7ccaf80d148f1f70e4d4"</span>,</span><br><span class="line">    <span class="attr">"build_timestamp"</span> : <span class="string">"2016-02-02T09:55:30Z"</span>,</span><br><span class="line">    <span class="attr">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"lucene_version"</span> : <span class="string">"4.10.4"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-Elasticsearch-Head"><a href="#2-Elasticsearch-Head" class="headerlink" title="2. Elasticsearch-Head"></a>2. Elasticsearch-Head</h2><p>ElasticSearch-Head 是一个与Elastic集群（Cluster）相交互的 Web 前台。</p>
<p><img src="/images/elastic/es-header.png" alt="header.png"></p>
<p>ES-Head的主要作用</p>
<ul>
<li>它展现ES集群的拓扑结构，并且可以通过它来进行索引（Index）和节点（Node）级别的操作</li>
<li>它提供一组针对集群的查询API，并将结果以json和表格形式返回</li>
<li>它提供一些快捷菜单，用以展现集群的各种状态</li>
</ul>
<p><strong>Install-Verify</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">elasticsearch/bin/plugin install mobz/elasticsearch-head</span><br><span class="line">open ip:9200/_plugin/head/</span><br><span class="line">open ip:9200/_cluster/health?pretty</span><br></pre></td></tr></table></figure>
<h2 id="3-Elasticsearch-Kopf"><a href="#3-Elasticsearch-Kopf" class="headerlink" title="3. Elasticsearch-Kopf"></a>3. Elasticsearch-Kopf</h2><p>Kopf是一个ElasticSearch的管理工具，它也提供了对ES集群操作的API。</p>
<p><img src="/images/elastic/es-kopf.png" alt="613455-20160224102628443-1084839027.png"></p>
<p><strong>Install-Verify</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf/&#123;branch|version&#125;</span><br><span class="line">open http://localhost:9200/_plugin/kopf</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="4-Elasticsearch-bigdesk"><a href="#4-Elasticsearch-bigdesk" class="headerlink" title="4. Elasticsearch-bigdesk"></a>4. Elasticsearch-bigdesk</h2><p>Bigdesk为Elastic集群提供动态的图表与统计数据。</p>
<p><img src="/images/elastic/es-bigdesk.jpeg" alt="613455-20160224102646365-1432943551.jpg"></p>
<p><strong>Install-Verify</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin/plugin -install lukas-vlcek/bigdesk</span><br><span class="line">删除bin/plugin --remove bigdesk</span><br><span class="line">open ip:9200/_plugin/bigdesk</span><br><span class="line">open ip:9200/_cluster/state?pretty</span><br></pre></td></tr></table></figure>
<h2 id="5-Elasticsearch-service"><a href="#5-Elasticsearch-service" class="headerlink" title="5. Elasticsearch-service"></a>5. Elasticsearch-service</h2><p>elasticsearch 作为一个系统service应用 ，可以安装elasticsearch-servicewrapper插件</p>
<p><a href="https://github.com/elastic/elasticsearch-servicewrapper" target="_blank" rel="external">github-es-service</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/elasticsearch/elasticsearch-servicewrapper</span><br><span class="line"></span><br><span class="line">下载该插件后，解压缩。将service目录拷贝到elasticsearch安装目录的bin目录下。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  service ll</span><br><span class="line">total 76</span><br><span class="line">-rwxr-xr-x  1 hp staff 55710 May 26 15:42 elasticsearch*</span><br><span class="line">-rw-r--r--  1 hp staff  2610 May 26 15:42 elasticsearch.bat</span><br><span class="line">-rw-r--r--  1 hp staff  4754 May 26 15:42 elasticsearch.conf</span><br><span class="line">-rwxr-xr-x  1 hp staff    64 May 26 15:42 elasticsearch32*</span><br><span class="line">-rwxr-xr-x  1 hp staff    64 May 26 15:42 elasticsearch64*</span><br><span class="line">drwxr-xr-x 16 hp staff   544 May 26 15:42 exec/</span><br><span class="line">drwxr-xr-x 17 hp staff   578 May 26 15:42 lib/</span><br><span class="line">➜  service</span><br></pre></td></tr></table></figure>
<p>运行这个插件的好处是：elasticsearch 需要的jvm参数和其它配置都已经配置好了，非常方便。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sh elasticsearch start;</span><br><span class="line">sh elasticsearch restart;</span><br><span class="line">sh elasticsearch stop;</span><br></pre></td></tr></table></figure>
<p>在实际生产环境中，该插件基本把参数都配置好了。我们只需要修改一下jvm分配的内存空间就好了，如 :</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set.default.ES_HEAP_SIZE=16384</span><br><span class="line">set.default.ES_MIN_MEM=16384</span><br><span class="line">set.default.ES_MAX_MEM=19660</span><br></pre></td></tr></table></figure>
<blockquote>
<p>第一次运行 elaticsearch 会产生 data-dir 与 log-dir</p>
<p>service log 在 logs/service.log 中。</p>
<p><a href="https://github.com/elastic/elasticsearch-servicewrapper" target="_blank" rel="external">more_info-service</a></p>
<p>Mac OS X Mountain Lion missing 32-bit Java<br>apple 6 maybe could</p>
</blockquote>
<hr>
<h2 id="6-Http-basic-server-plugin"><a href="#6-Http-basic-server-plugin" class="headerlink" title="6. Http-basic-server-plugin"></a>6. Http-basic-server-plugin</h2><p>不要裸奔，穿一套比基尼吧。</p>
<p>做一个简单的HTTP认证，elasticsearch-http-basic 提供了针对 ES HTTP 连接 的 IP白名单、密码权限 和  信任代理功能。</p>
<p>github :<br><a href="https://github.com/Asquera/elasticsearch-http-basic" target="_blank" rel="external">Asquera_http_basic</a></p>
<p><strong>Install-Verify</strong></p>
<p>elasticsearch-http-basic还不支持ES标准的bin/plugin install [github-name]/[repo-name]的安装方式, 所以按照如下方式安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p plugins/http-basic; </span><br><span class="line">mv elasticsearch-http-basic-1.5.1.jar plugins/http-basic</span><br></pre></td></tr></table></figure>
<p><strong>Config http-basic param</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http.basic.enabled: true</span><br><span class="line">http.basic.user: &quot;admin&quot;</span><br><span class="line">http.basic.password: &quot;admin&quot;</span><br><span class="line">http.basic.ipwhitelist: [&quot;localhost&quot;, &quot;127.0.0.1&quot;]</span><br><span class="line">http.basic.trusted_proxy_chains: []</span><br><span class="line">http.basic.log: true</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="7-Elasticsearch-sql"><a href="#7-Elasticsearch-sql" class="headerlink" title="7. Elasticsearch-sql"></a>7. Elasticsearch-sql</h2><p><img src="/images/elastic/es-sql.jpeg" alt="图片描述"></p>
<p><strong>install</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./plugin -u https://github.com/NLPchina/elasticsearch-sql/releases/download/1.4.5/elasticsearch-sql-1.4.5.zip --install sql</span><br></pre></td></tr></table></figure>
<p><strong>Verify</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">open http://node01:9200/_plugin/sql/</span><br></pre></td></tr></table></figure>
<p><strong>./bin/plugin –list</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  elasticsearch ./bin/plugin --list</span><br><span class="line">Installed plugins:</span><br><span class="line">    - bigdesk</span><br><span class="line">    - head</span><br><span class="line">    - http-basic</span><br><span class="line">    - jdbc</span><br><span class="line">    - kopf</span><br><span class="line">    - license</span><br><span class="line">    - shield</span><br><span class="line">    - sql</span><br></pre></td></tr></table></figure>
<h2 id="8-Elasticsearch-jdbc"><a href="#8-Elasticsearch-jdbc" class="headerlink" title="8. Elasticsearch-jdbc"></a>8. Elasticsearch-jdbc</h2><p>关系型数据库的同步插件</p>
<p><strong>install</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./plugin --install jdbc --url http://xbib.org/repository/org/xbib/elasticsearch/plugin/elasticsearch-river-jdbc/1.5.0.5/elasticsearch-river-jdbc-1.5.0.5-plugin.zip</span><br></pre></td></tr></table></figure>
<p>download and add mysql-driver</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -o mysql-connector-java-5.1.33.zip -L &apos;http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.33.zip/from/http://cdn.mysql.com/&apos;</span><br><span class="line"></span><br><span class="line">cp mysql-connector-java-5.1.33-bin.jar $ES_HOME/plugins/jdbc/</span><br><span class="line"></span><br><span class="line">chmod 644 $ES_HOME/plugins/jdbc/*</span><br></pre></td></tr></table></figure>
<p>停止river</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XDELETE &apos;localhost:9200/_river/my_jdbc_river/&apos;</span><br></pre></td></tr></table></figure>
<p><strong>Verify</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">open http://node01:9200/_nodes/node01/plugins?pretty=true</span><br></pre></td></tr></table></figure>
<h2 id="9-Basic-operation"><a href="#9-Basic-operation" class="headerlink" title="9. Basic operation"></a>9. Basic operation</h2><p><strong>查看该节点安装的所有插件列表</strong></p>
<p><strong><em><a href="http://node01:9200/_nodes/node01/plugins?pretty=true" target="_blank" rel="external">http://node01:9200/_nodes/node01/plugins?pretty=true</a></em></strong></p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"cluster_name"</span> : <span class="string">"elasticsearch_x"</span>,</span><br><span class="line">  <span class="attr">"nodes"</span> : &#123;</span><br><span class="line">    <span class="attr">"nSitXzd8QvSxQRz3mni3BA"</span> : &#123;</span><br><span class="line">      <span class="attr">"name"</span> : <span class="string">"node01"</span>,</span><br><span class="line">      <span class="attr">"transport_address"</span> : <span class="string">"inet[/192.168.181.35:9300]"</span>,</span><br><span class="line">      <span class="attr">"host"</span> : <span class="string">"unix.local"</span>,</span><br><span class="line">      <span class="attr">"ip"</span> : <span class="string">"192.168.181.35"</span>,</span><br><span class="line">      <span class="attr">"version"</span> : <span class="string">"1.7.5"</span>,</span><br><span class="line">      <span class="attr">"build"</span> : <span class="string">"00f95f4"</span>,</span><br><span class="line">      <span class="attr">"http_address"</span> : <span class="string">"inet[/192.168.181.35:9200]"</span>,</span><br><span class="line">      <span class="attr">"plugins"</span> : [ &#123;</span><br><span class="line">        <span class="attr">"name"</span> : <span class="string">"sql"</span>,</span><br><span class="line">        <span class="attr">"version"</span> : <span class="string">"1.4.5"</span>,</span><br><span class="line">        <span class="attr">"description"</span> : <span class="string">"Use sql to query elasticsearch."</span>,</span><br><span class="line">        <span class="attr">"url"</span> : <span class="string">"/_plugin/sql/"</span>,</span><br><span class="line">        <span class="attr">"jvm"</span> : <span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"site"</span> : <span class="literal">true</span></span><br><span class="line">      &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span> : <span class="string">"http-basic-server-plugin"</span>,</span><br><span class="line">        <span class="attr">"version"</span> : <span class="string">"NA"</span>,</span><br><span class="line">        <span class="attr">"description"</span> : <span class="string">"HTTP Basic Server Plugin"</span>,</span><br><span class="line">        <span class="attr">"jvm"</span> : <span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"site"</span> : <span class="literal">false</span></span><br><span class="line">      &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span> : <span class="string">"bigdesk"</span>,</span><br><span class="line">        <span class="attr">"version"</span> : <span class="string">"NA"</span>,</span><br><span class="line">        <span class="attr">"description"</span> : <span class="string">"No description found."</span>,</span><br><span class="line">        <span class="attr">"url"</span> : <span class="string">"/_plugin/bigdesk/"</span>,</span><br><span class="line">        <span class="attr">"jvm"</span> : <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"site"</span> : <span class="literal">true</span></span><br><span class="line">      &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span> : <span class="string">"head"</span>,</span><br><span class="line">        <span class="attr">"version"</span> : <span class="string">"NA"</span>,</span><br><span class="line">        <span class="attr">"description"</span> : <span class="string">"No description found."</span>,</span><br><span class="line">        <span class="attr">"url"</span> : <span class="string">"/_plugin/head/"</span>,</span><br><span class="line">        <span class="attr">"jvm"</span> : <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"site"</span> : <span class="literal">true</span></span><br><span class="line">      &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span> : <span class="string">"kopf"</span>,</span><br><span class="line">        <span class="attr">"version"</span> : <span class="string">"1.5.7-SNAPSHOT"</span>,</span><br><span class="line">        <span class="attr">"description"</span> : <span class="string">"kopf - simple web administration tool for ElasticSearch"</span>,</span><br><span class="line">        <span class="attr">"url"</span> : <span class="string">"/_plugin/kopf/"</span>,</span><br><span class="line">        <span class="attr">"jvm"</span> : <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"site"</span> : <span class="literal">true</span></span><br><span class="line">      &#125; ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>XPUT data</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">curl -u admin:admin -XPUT http://node01:9200/megacorp/employee/1 -d '</span><br><span class="line"> &#123;</span><br><span class="line">     "first_name" : "John",</span><br><span class="line">     "last_name" :  "Smith",</span><br><span class="line">     "age" :        25,</span><br><span class="line">     "about" :      "I love to go rock climbing",</span><br><span class="line">     "interests": [ "sports", "music" ]</span><br><span class="line"> &#125;'</span><br></pre></td></tr></table></figure>
<p><strong>XGET data</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">curl -XGET 'localhost:9200/_count?pretty' -d '</span><br><span class="line"> &#123;</span><br><span class="line">     "query": &#123;</span><br><span class="line">         "match_all": &#123;&#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;'</span><br></pre></td></tr></table></figure>
<p>output</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"count"</span> : <span class="number">1</span>,</span><br><span class="line">  <span class="attr">"_shards"</span> : &#123;</span><br><span class="line">    <span class="attr">"total"</span> : <span class="number">5</span>,</span><br><span class="line">    <span class="attr">"successful"</span> : <span class="number">5</span>,</span><br><span class="line">    <span class="attr">"failed"</span> : <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="10-Reference-article"><a href="#10-Reference-article" class="headerlink" title="10. Reference article"></a>10. Reference article</h2><ol>
<li><a href="http://www.cnblogs.com/richaaaard/p/5212044.html" target="_blank" rel="external">csdn-004-Elasticsearch插件的介绍</a></li>
<li><a href="http://blog.csdn.net/shenfuli/article/details/49094935" target="_blank" rel="external">插件安装Head、Kopf与Bigdesk</a></li>
<li><a href="http://www.chepoo.com/elasticsearch-service-install.html" target="_blank" rel="external">chepoo.com/elasticsearch-service</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html" target="_blank" rel="external">elastic.co/guide/</a></li>
<li><a href="https://github.com/NLPchina/elasticsearch-sql" target="_blank" rel="external">NLPchina/elasticsearch-sql</a></li>
<li><a href="https://github.com/elasticfence/elasticsearch-http-user-auth" target="_blank" rel="external">elasticsearch-http-user-auth</a> (这个我没有使用)</li>
<li><a href="http://guoze.me/2015/05/28/elasticsearch-http-basic-authentication/" target="_blank" rel="external">建造者说</a></li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[点到平面的距离公式]]></title>
      <url>http://sggo.me/2016/05/17/ml/math-distance-formula-of-point-to-plain/</url>
      <content type="html"><![CDATA[<p>平面的一般式方程, 向量的模（长度）, 向量的数量积, 点到平面的距离</p>
<a id="more"></a>
<p><a href="https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F" target="_blank" rel="external">维基百科_Vector</a></p>
<h2 id="1-平面的一般式方程"><a href="#1-平面的一般式方程" class="headerlink" title="1. 平面的一般式方程"></a>1. 平面的一般式方程</h2><p>Ax +By +Cz + D = 0</p>
<p>其中n = (A, B, C)是平面的法向量，D是将平面平移到坐标原点所需距离（所以D=0时，平面过原点）</p>
<h2 id="2-向量的模（长度）"><a href="#2-向量的模（长度）" class="headerlink" title="2. 向量的模（长度）"></a>2. 向量的模（长度）</h2><p>给定一个向量V（x, y, z),则|V| = sqrt(x <em> x + y </em> y + z * z)</p>
<blockquote>
<p>在数学中，矢量常采用更为抽象的矢量空间（也称为线性空间）来定义，而定义具有物理意义上的大小和方向的矢量概念则需要引进了范数和内积的欧几里得空间。<br>范数， 模长</p>
</blockquote>
<h2 id="3-向量的数量积-点积-内积"><a href="#3-向量的数量积-点积-内积" class="headerlink" title="3. 向量的数量积/点积/内积"></a>3. 向量的数量积/点积/内积</h2><p>给定两个向量V1(x1, y1, z1)和V2(x2, y2, z2)则他们的内积是 V1V2 = x1x2 + y1y2 + z1z2</p>
<blockquote>
<p>数量积被广泛应用于物理中，如做功就是用力的矢量乘位移的矢量，即<img src="/images/math/math-pointdis.png" alt=""></p>
</blockquote>
<h2 id="4-点到平面的距离-Yes"><a href="#4-点到平面的距离-Yes" class="headerlink" title="4. 点到平面的距离(Yes)"></a>4. 点到平面的距离(Yes)</h2><p>求点到直线的距离不再是难事，有图有真相</p>
<p><img src="/images/math/math-pointToPlane.jpeg" alt="Distance formula of point to plane"></p>
<blockquote>
<p>如果法向量是单位向量的话，那么分母为1</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Machine Learning p1 - Spark编程入门]]></title>
      <url>http://sggo.me/2016/04/25/spark/spark-machine-learning-p1/</url>
      <content type="html"><![CDATA[<p>Spark 的环境搭建与运行, 接触了 RDD 与 SparkContext, 启动 Spark-Shell 以及如何使用 Scala、Python 编写 Spark 程序. </p>
<a id="more"></a>
<p><strong>Apache Spark</strong></p>
<p>Spark 的设计目标 即: <code>迭代式+低延迟</code> 适合 Machine Learning 算法的特性<br>Spark 分布式计算框架, 将中间数据和结果保存在内存中<br>Spark 提供函数式API, 并兼容 Hadoop 生态<br>Spark 框架对 资源调度、任务提交、执行、跟踪， 节点间通信以及数据并行处理的内在底层操作都进行了抽象</p>
<blockquote>
<p>简化了海量数据的存储(HDFS) 和 计算(MR) 流程。MapReduce 缺点, 如: 启动任务时的高开销、对中间数据 和 计算结果 写入磁盘的依赖。这使得 Hadoop 不适合 迭代式 或 低延迟 的任务。</p>
</blockquote>
<p><strong>Spark 的四种运行模式</strong></p>
<ol>
<li>本地单机模式 – Spark 进程 all run in one JVM</li>
<li>集群单机模式 – 使用 Spark 自己内置的 任务调度框架</li>
<li>基于 Mesos 一个开源集群计算框架</li>
<li>基于 YARN 与 Hadoop2 关联形成集群计算和资源调度框架</li>
</ol>
<h2 id="1-Spark运行"><a href="#1-Spark运行" class="headerlink" title="1. Spark运行"></a>1. Spark运行</h2><p>运行示例程序来测试是否一切正常：</p>
<blockquote>
<p>./bin/run-example org.apache.spark.examples.SparkPi</p>
</blockquote>
<p>该命令将在本地单机模式下执行SparkPi这个示例。在该模式下，所有的Spark进程均运行于同一个JVM中，而并行处理则通过多线程来实现。默认情况下，该示例会启用与本地系统的CPU核心数目相同的线程。</p>
<p>要在本地模式下设置并行的级别，以local[N]的格式来指定一个master变量即可。比如只使用两个线程时，可输入如下命令：</p>
<blockquote>
<p>MASTER=local[2] ./bin/run-example org.apache.spark.examples.SparkPi</p>
</blockquote>
<h2 id="2-Spark集群"><a href="#2-Spark集群" class="headerlink" title="2. Spark集群"></a>2. Spark集群</h2><p>Spark集群由两类程序构成: </p>
<ol>
<li>一个驱动程序</li>
<li>多个执行程序</li>
</ol>
<blockquote>
<p>本地模式下所有的处理都运行在同一个JVM内，而在集群模式时它们通常运行在不同的节点上。</p>
</blockquote>
<p>一个采用单机模式的Spark集群包括：</p>
<ol>
<li>一个运行Spark单机主进程和驱动程序的 Master；</li>
<li>各自运行一个执行程序进程的多个 Worker。</li>
</ol>
<p>比如在一个Spark单机集群上运行，只需传入主节点的URL即可：</p>
<blockquote>
<p>MASTER=spark://IP:PORT ./bin/run-example org.apache.spark.examples.SparkPi<br>其中的IP和PORT分别是主节点IP地址和端口号。这是告诉Spark让示例程序运行在主节点所对应的集群上</p>
</blockquote>
<h2 id="3-Spark编程模型"><a href="#3-Spark编程模型" class="headerlink" title="3. Spark编程模型"></a>3. Spark编程模型</h2><h3 id="3-1-SparkContext类"><a href="#3-1-SparkContext类" class="headerlink" title="3.1 SparkContext类"></a>3.1 SparkContext类</h3><p><strong>SparkContext类与SparkConf类</strong></p>
<p>任何Spark程序的编写都是从SparkContext开始的。SparkContext的初始化需要一个SparkConf对象，后者包含了Spark集群配置的各种参数（比如主节点的URL）。</p>
<p>初始化后，我们便可用SparkContext对象所包含的各种方法来创建和操作RDD。Spark shell（在Scala和Python下可以，但不支持Java）能自动完成上述初始化。若要用Scala代码来实现的话，可参照下面的代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Test Spark App"</span>).setMaster(<span class="string">"local[4]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>
<p>这段代码会创建一个4线程的SparkContext对象，并将其相应的任务命名为Test Spark APP。我们也可通过如下方式调用SparkContext的简单构造函数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local[4]"</span>, <span class="string">"Test Spark App"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-Spark-shell"><a href="#3-2-Spark-shell" class="headerlink" title="3.2 Spark shell"></a>3.2 Spark shell</h3><p>Spark支持 用 Scala or Python REPL（Read-Eval-Print-Loop，即交互式shell）来进行交互式的程序编写。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/spark-shell</span><br></pre></td></tr></table></figure>
<p>会启动Scala shell 并初始化一个SparkContext对象。我们可以通过 sc这个Scala值来调用这个对象</p>
<h3 id="3-3-RDD"><a href="#3-3-RDD" class="headerlink" title="3.3 RDD"></a>3.3 RDD</h3><p>一个 RDD 代表一系列的“记录”（严格来说，某种类型的对象）。<br>这些记录被分配或分区到一个集群的多个节点上（在本地模式下，可以类似地理解为单个进程里的多个线程上）。</p>
<p>Spark中的RDD具备容错性，即当某个节点或任务失败时（因非用户代码原因而引起，如硬件故障、网络不通等），RDD会在余下的节点上自动重建，以便任务能最终完成。</p>
<p><strong>1. 创建RDD</strong></p>
<p>RDD可从现有的集合创建 ：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> collection = <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>)</span><br><span class="line"><span class="keyword">val</span> rddFromCollection = sc.parallelize(collection)</span><br></pre></td></tr></table></figure>
<p>RDD也可以基于Hadoop的输入源创建，比如本地文件系统、HDFS。基于Hadoop的RDD可以使用任何实现了Hadoop InputFormat接口的输入格式，包括文本文件、其他Hadoop标准格式、HBase等。以下举例说明如何用一个本地文件系统里的文件创建RDD：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val rddFromTextFile = sc.textFile(&quot;LICENSE&quot;)</span><br></pre></td></tr></table></figure>
<p>上述代码中的textFile函数（方法）会返回一个RDD对象。该对象的每一条记录都是一个表示文本文件中某一行文字的String（字符串）对象。</p>
<p><strong>2. Spark操作</strong></p>
<p>在Spark编程模式下，所有的操作被分为 <code>transformation</code> 和 <code>action</code> 两种。</p>
<p><strong>transformation</strong> 操作是对一个数据集里的所有记录执行某种函数，从而使记录发生改变；</p>
<p><strong>action</strong> 通常是运行某些计算或聚合操作，并将结果返回运行 SparkContext 的那个驱动程序。</p>
<p>Spark 的操作通常采用<code>函数式</code>风格。</p>
<p>Spark程序中最常用的转换操作便是map操作。该操作对一个RDD里的每一条记录都执行某个函数，从而将输入映射成为新的输出。</p>
<p>比如，下面这段代码便对一个从本地文本文件创建的RDD进行操作。它对该RDD中的每一条记录都执行size函数。<br>创建一个这样的由若干String构成的RDD对象。通过map函数，我们将每一个字符串都转换为一个整数，从而返回一个由若干Int构成的RDD对象。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; rddFromTextFile.count</span><br><span class="line">res2: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> intsFromStringsRDD = rddFromTextFile.map(line =&gt; line.size)</span><br><span class="line">intsFromStringsRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; intsFromStringsRDD.count</span><br><span class="line">res3: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sumOfRecords = intsFromStringsRDD.sum</span><br><span class="line">sumOfRecords: <span class="type">Double</span> = <span class="number">17062.0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> numRecords = intsFromStringsRDD.count</span><br><span class="line">numRecords: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> aveLengthOfRecord = sumOfRecords / numRecords</span><br><span class="line">aveLengthOfRecord: <span class="type">Double</span> = <span class="number">58.034013605442176</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价于</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> aveLengthOfRecordChained = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</span><br></pre></td></tr></table></figure>
<blockquote>
<p>示例中 <strong>=&gt;</strong> 是Scala下表示匿名函数的语法。语法 <strong>line =&gt; line.size</strong> 表示以 <strong>=&gt;</strong> 操作符左边的部分作为输入，对其执行一个函数，并以 <strong>=&gt;</strong> 操作符右边代码的执行结果为输出。在这个例子中，输入为line，输出则是 <strong>line.size</strong> 函数的执行结果。在Scala语言中，这种将一个String对象映射为一个Int的函数被表示为String =&gt; Int。</p>
</blockquote>
<p>Spark的大多数操作都会返回一个新RDD，但多数的Action操作则是返回计算的结果</p>
<blockquote>
<p>注 : Spark 中的转换操作是延后的。也就是说，在RDD上调用一个转换操作并不会立即触发相应的计算。 只有必要时才计算结果并将其返回给驱动程序，从而提高了Spark的效率。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> transformedRDD = rddFromTextFile.map(line =&gt; line.size).</span><br><span class="line">     | filter(size =&gt; size &gt; <span class="number">10</span>).map(size =&gt; size * <span class="number">2</span>)</span><br><span class="line">transformedRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at map at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<p>没有触发任何计算，也没有结果被返回。<br>如果我们现在在新的RDD上调用一个执行操作，比如sum，该计算将会被触发：</p>
<p><strong><em>触发计算</em></strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> computation = transformedRDD.sum</span><br><span class="line">computation: <span class="type">Double</span> = <span class="number">34106.0</span></span><br></pre></td></tr></table></figure>
<p><strong>3. RDD缓存策略</strong></p>
<p>Spark最为强大的功能之一便是能够把数据缓存在集群的内存里。这通过调用RDD的cache函数来实现：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; rddFromTextFile.cache</span><br><span class="line">res4: rddFromTextFile.<span class="keyword">type</span> = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> aveLengthOfRecordChainedFromCached = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</span><br><span class="line">aveLengthOfRecordChainedFromCached: <span class="type">Double</span> = <span class="number">58.034013605442176</span></span><br></pre></td></tr></table></figure>
<p>在RDD首次调用一个执行操作时，这个操作对应的计算会立即执行，数据会从数据源里读出并保存到内存。因此，首次调用cache函数所需要的时间会部分取决于Spark从输入源读取数据所需要的时间。但是，当下一次访问该数据集的时候，数据可以直接从内存中读出从而减少低效的I/O操作，加快计算。多数情况下，这会取得数倍的速度提升。</p>
<blockquote>
<p>Spark支持更为细化的缓存策略。通过persist函数可以指定Spark的数据缓存策略。关于RDD缓存的更多信息可参见：<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence。</a></p>
</blockquote>
<h3 id="3-4-广播变量和累加器"><a href="#3-4-广播变量和累加器" class="headerlink" title="3.4 广播变量和累加器"></a>3.4 广播变量和累加器</h3><p>Spark的另一个核心功能是能创建两种特殊类型的变量：<strong>广播变量</strong> 和 累加器。</p>
<p>广播变量（broadcast variable）为只读变量，它由运行SparkContext的驱动程序创建后发送给会参与计算的节点。对那些需要让各工作节点高效地访问相同数据的应用场景，比如机器学习，这非常有用。</p>
<p>Spark下创建广播变量只需在SparkContext上调用一个方法即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val broadcastAList = sc.broadcast(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;))</span><br><span class="line">broadcastAList: org.apache.spark.broadcast.Broadcast[List[String]] = Broadcast(11)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<p><strong>广播变量</strong> 也可以被非驱动程序所在的节点（即工作节点）访问，访问的方法是调用该变量的<code>value</code>方法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastAList = sc.broadcast(<span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>))</span><br><span class="line">broadcastAList: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">List</span>[<span class="type">String</span>]] = <span class="type">Broadcast</span>(<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(<span class="type">List</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)).map(x =&gt; broadcastAList.value ++ x).collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">List</span>[<span class="type">Any</span>]] = <span class="type">Array</span>(<span class="type">List</span>(a, b, c, d, e, <span class="number">1</span>), <span class="type">List</span>(a, b, c, d, e, <span class="number">2</span>), <span class="type">List</span>(a, b, c, d, e, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，collect 函数一般仅在的确需要将整个结果集返回驱动程序并进行后续处理时才有必要调用。如果在一个非常大的数据集上调用该函数，可能耗尽驱动程序的可用内存，进而导致程序崩溃。</p>
</blockquote>
<p>高负荷的处理应尽可能地在整个集群上进行，从而避免驱动程序成为系统瓶颈。然而在不少情况下，将结果收集到驱动程序的确是有必要的。很多机器学习算法的迭代过程便属于这类情况。</p>
<p><strong>累加器</strong>（accumulator）也是一种被广播到工作节点的变量。累加器与广播变量的关键不同，是后者只能读取而前者却可累加。</p>
<blockquote>
<p>关于累加器的更多信息，可参见《Spark编程指南》：<a href="http://spark.apache.org/docs/latest/programming-guide.html#shared-variables。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#shared-variables。</a></p>
</blockquote>
<h2 id="4-Spark-Scala-编程入门"><a href="#4-Spark-Scala-编程入门" class="headerlink" title="4. Spark Scala 编程入门"></a>4. Spark Scala 编程入门</h2><p><a href="https://github.com/blair1/spark/tree/master/Spark-Machine-Learning_8519OSCode/Chapter%2001/scala-spark-app" target="_blank" rel="external">scala-spark-app</a></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * A simple Spark app in Scala</span><br><span class="line"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local[2]"</span>, <span class="string">"First Spark App"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)</span></span><br><span class="line">    <span class="keyword">val</span> data = sc.textFile(<span class="string">"data/UserPurchaseHistory.csv"</span>)</span><br><span class="line">      .map(line =&gt; line.split(<span class="string">","</span>))</span><br><span class="line">      .map(purchaseRecord =&gt; (purchaseRecord(<span class="number">0</span>), purchaseRecord(<span class="number">1</span>), purchaseRecord(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's count the number of purchases</span></span><br><span class="line">    <span class="keyword">val</span> numPurchases = data.count()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's count how many unique users made purchases</span></span><br><span class="line">    <span class="keyword">val</span> uniqueUsers = data.map &#123; <span class="keyword">case</span> (user, product, price) =&gt; user &#125;.distinct().count()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's sum up our total revenue</span></span><br><span class="line">    <span class="keyword">val</span> totalRevenue = data.map &#123; <span class="keyword">case</span> (user, product, price) =&gt; price.toDouble &#125;.sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's find our most popular product</span></span><br><span class="line">    <span class="keyword">val</span> productsByPopularity = data</span><br><span class="line">      .map &#123; <span class="keyword">case</span> (user, product, price) =&gt; (product, <span class="number">1</span>) &#125;</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">      .collect()</span><br><span class="line">      .sortBy(-_._2)</span><br><span class="line">    <span class="keyword">val</span> mostPopular = productsByPopularity(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// finally, print everything out</span></span><br><span class="line">    println(<span class="string">"Total purchases: "</span> + numPurchases)</span><br><span class="line">    println(<span class="string">"Unique users: "</span> + uniqueUsers)</span><br><span class="line">    println(<span class="string">"Total revenue: "</span> + totalRevenue)</span><br><span class="line">    println(<span class="string">"Most popular product: %s with %d purchases"</span>.format(mostPopular._1, mostPopular._2))</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-Spark-Java-编程入门"><a href="#5-Spark-Java-编程入门" class="headerlink" title="5. Spark Java 编程入门"></a>5. Spark Java 编程入门</h2><p>Java API与Scala API本质上很相似。Scala代码可以很方便地调用Java代码，但某些Scala代码却无法在Java里调用，特别是那些使用了隐式类型转换、默认参数和采用了某些Scala反射机制的代码。</p>
<p>SparkContext有了对应的Java版本JavaSparkContext，而RDD则对应JavaRDD。<br>Spark提供对Java 8匿名函数（lambda）语法的支持。</p>
<p>用Scala编写时，键/值对记录的RDD能支持一些特别的操作（比如reduceByKey和saveAsSequenceFile）。这些操作可以通过隐式类型转换而自动被调用。用Java编写时，则需要特别类型的JavaRDD来支持这些操作。它们包括用于键/值对的JavaPairRDD，以及用于数值记录的JavaDoubleRDD。</p>
<p>Java 8 RDD以及Java 8 lambda表达式更多信息可参见《Spark编程指南》：<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations。</a></p>
<h2 id="6-Spark-Python-编程入门"><a href="#6-Spark-Python-编程入门" class="headerlink" title="6. Spark Python 编程入门"></a>6. Spark Python 编程入门</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="string">"""用Python编写的一个简单Spark应用"""</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line">sc = SparkContext(<span class="string">"local[2]"</span>, <span class="string">"First Spark App"</span>)</span><br><span class="line"><span class="comment"># 将CSV格式的原始数据转化为(user,product,price)格式的记录集</span></span><br><span class="line">data = sc.textFile(<span class="string">"data/UserPurchaseHistory.csv"</span>).map(<span class="keyword">lambda</span> line:</span><br><span class="line">line.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> record: (record[<span class="number">0</span>], record[<span class="number">1</span>], record[<span class="number">2</span>]))</span><br><span class="line"><span class="comment"># 求总购买次数</span></span><br><span class="line">numPurchases = data.count()</span><br><span class="line"><span class="comment"># 求有多少不同客户购买过商品</span></span><br><span class="line">uniqueUsers = data.map(<span class="keyword">lambda</span> record: record[<span class="number">0</span>]).distinct().count()</span><br><span class="line"><span class="comment"># 求和得出总收入</span></span><br><span class="line">totalRevenue = data.map(<span class="keyword">lambda</span> record: float(record[<span class="number">2</span>])).sum()</span><br><span class="line"><span class="comment"># 求最畅销的产品是什么</span></span><br><span class="line">products = data.map(<span class="keyword">lambda</span> record: (record[<span class="number">1</span>], <span class="number">1.0</span>)).</span><br><span class="line">reduceByKey(<span class="keyword">lambda</span> a, b: a + b).collect()</span><br><span class="line">mostPopular = sorted(products, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Total purchases: %d"</span> % numPurchases</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Unique users: %d"</span> % uniqueUsers</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Total revenue: %2.2f"</span> % totalRevenue</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Most popular product: %s with %d purchases"</span> % (mostPopular[<span class="number">0</span>], mostPopular[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>匿名函数在Python语言中亦称lambda函数，lambda也是语法表达上的关键字。</p>
<p>用Scala编写时，一个将输入x映射为输出y的匿名函数表示为x =&gt; y，而在Python中则是lambda x : y。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  python-spark-app git:(master) ✗ <span class="built_in">pwd</span></span><br><span class="line">/Users/hp/ghome/hadoop-spark/spark/Spark-Machine-Learning_8519OSCode/Chapter01/python-spark-app</span><br><span class="line">➜  python-spark-app git:(master) ✗ <span class="variable">$SPARK_HOME</span>/bin/spark-submit pythonapp.py</span><br><span class="line">Using Spark<span class="string">'s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">16/08/26 15:56:02 INFO SparkContext: Running Spark version 1.5.2</span><br><span class="line">...</span><br><span class="line">Total purchases: 5</span><br><span class="line">Unique users: 4</span><br><span class="line">Total revenue: 39.91</span><br><span class="line">Most popular product: iPhone Cover with 2 purchases</span><br><span class="line">16/08/26 15:56:07 INFO SparkUI: Stopped Spark web UI at http://192.168.143.84:4040</span><br><span class="line">...</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Spark的Python API几乎覆盖了所有Scala API所能提供的功能. 但的确有些特性，比如Spark Streaming和个别的API方法，暂不支持。<br><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">具体参见《Spark编程指南》的Python部分</a></p>
</blockquote>
<h2 id="7-小结"><a href="#7-小结" class="headerlink" title="7. 小结"></a>7. 小结</h2><p>体会了 函数式 编程的威力， scala、python 都可以。java 不适合写 spark 程序</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[SBT Hello]]></title>
      <url>http://sggo.me/2016/03/16/spark/spark-scala-sbt-hello/</url>
      <content type="html"><![CDATA[<ol>
<li>什么是 SBT ?</li>
<li>SBT 项目工程目录</li>
<li>SBT 编译打包 Scala HelloWorld</li>
</ol>
<a id="more"></a>
<h2 id="1-SBT-What"><a href="#1-SBT-What" class="headerlink" title="1. SBT, What?"></a>1. SBT, What?</h2><p>SBT 是 Simple Build Tool 的简称. SBT 可以认为是 Scala 世界的 maven。</p>
<p>SBT的着迷特性，比如：</p>
<ol>
<li>DSL build构建, 并可混合构建 Java 和 Scala 项目；</li>
<li>通过触发执行 (trigger execution) 特性支持持续的编译与测试；</li>
<li>可以重用 Maven 或者 ivy的repository 进行依赖管理；</li>
<li>增量编译、并行任务等等…</li>
</ol>
<h2 id="2-Hello-SBT"><a href="#2-Hello-SBT" class="headerlink" title="2. Hello, SBT"></a>2. Hello, SBT</h2><p>一个极致简单的 Scala项目 （hello simple project）</p>
<p>hello/HelloWorld.scala</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        println(<span class="string">"Hello, SBT"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>sbt run</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  hello git:(master) ✗ sbt</span><br><span class="line">[info] Set current project to hello (in build file:/Users/hp/ghome/Spark-Scala/hello/)</span><br><span class="line">&gt; run</span><br><span class="line">[info] Updating &#123;file:/Users/hp/ghome/Spark-Scala/hello/&#125;hello...</span><br><span class="line">[info] Resolving org.fusesource.jansi#jansi;1.4 ...</span><br><span class="line">[info] Done updating.</span><br><span class="line">[info] Compiling 1 Scala source to /Users/hp/ghome/Spark-Scala/hello/target/scala-2.10/classes...</span><br><span class="line">[info] Running HelloWorld</span><br><span class="line">Hello, SBT</span><br><span class="line">[success] Total time: 3 s, completed 2016-3-17 9:38:44</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
<h2 id="3-SBT-项目工程结构详解"><a href="#3-SBT-项目工程结构详解" class="headerlink" title="3. SBT 项目工程结构详解"></a>3. SBT 项目工程结构详解</h2><p>一个典型的SBT项目工程结构如下图所示：</p>
<p><img src="https://image-static.segmentfault.com/396/971/3969713528-56ea0c71e094e_articlex" alt="segmentfault"></p>
<p><strong>build.sbt 详解</strong></p>
<p>build.sbt 相当于 maven-pom.xml，它是build定义文件。 </p>
<p>SBT 运行 使用 2 种形式 的 build 定义文件，</p>
<ol>
<li>one, put your project’s base directory，– build.sbt， a simple build definition； </li>
<li>other one, put project directory，can Use Scala language, more expressive。</li>
</ol>
<p>一个简单的build.sbt文件内容如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">name := <span class="string">"hello"</span>      <span class="comment">// 项目名称</span></span><br><span class="line"></span><br><span class="line">organization := <span class="string">"xxx.xxx.xxx"</span>  <span class="comment">// 组织名称</span></span><br><span class="line"></span><br><span class="line">version := <span class="string">"0.0.1-SNAPSHOT"</span>  <span class="comment">// 版本号</span></span><br><span class="line"></span><br><span class="line">scalaVersion := <span class="string">"2.9.2"</span>   <span class="comment">// 使用的Scala版本号</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其它build定义</span></span><br></pre></td></tr></table></figure>
<p> name 和 version的定义是必须的，因为如果想生成jar包的话，这两个属性的值将作为jar包名称的一部分, 各行之间以空行分隔。<br>除了定义以上项目相关信息，我们还可以在build.sbt中添加项目依赖：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 添加源代码编译或者运行期间使用的依赖</span><br><span class="line">libraryDependencies += &quot;ch.qos.logback&quot; % &quot;logback-core&quot; % &quot;1.0.0&quot;</span><br><span class="line"></span><br><span class="line">libraryDependencies += &quot;ch.qos.logback&quot; % &quot;logback-classic&quot; % &quot;1.0.0&quot;</span><br><span class="line"></span><br><span class="line">// 或者</span><br><span class="line"></span><br><span class="line">libraryDependencies ++= Seq(</span><br><span class="line">                            &quot;ch.qos.logback&quot; % &quot;logback-core&quot; % &quot;1.0.0&quot;,</span><br><span class="line">                            &quot;ch.qos.logback&quot; % &quot;logback-classic&quot; % &quot;1.0.0&quot;,</span><br><span class="line">                            ...</span><br><span class="line">                            )</span><br><span class="line"></span><br><span class="line">// 添加测试代码编译或者运行期间使用的依赖</span><br><span class="line">libraryDependencies ++= Seq(&quot;org.scalatest&quot; %% &quot;scalatest&quot; % &quot;1.8&quot; % &quot;test&quot;)</span><br></pre></td></tr></table></figure>
<p>当然， build.sbt文件中还可以定义很多东西，比如添加插件，声明额外的repository，声明各种编译参数等等</p>
<p><strong>project目录即相关文件介绍</strong></p>
<p>project目录下的几个文件可以根据情况添加。</p>
<p>build.properties 文件声明使用的要使用哪个版本的SBT来编译当前项目， 最新的sbt boot launcher可以能够兼容编译所有0.10.x版本的SBT构建项目，比如如果我使用的是0.12版本的sbt，但却想用0.11.3版本的sbt来编译当前项目，则可以在build.properties文件中添加sbt.version=0.11.3来指定。</p>
<p>plugins.sbt 文件用来声明当前项目希望使用哪些插件来增强当前项目使用的sbt的功能，比如像assembly功能，清理ivy local cache功能，都有相应的sbt插件供使用， 要使用这些插件只需要在 plugins.sbt 中声明即可.</p>
<p>为了能够成功加载这些sbt插件，我们将他们的查找位置添加到resolovers当中.</p>
<p><strong>其他</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ touch build.sbt</span><br><span class="line">$ mkdir src</span><br><span class="line">$ mkdir src/main</span><br><span class="line">$ mkdir src/main/java</span><br><span class="line">$ mkdir src/main/resources</span><br><span class="line">$ mkdir src/main/scala</span><br><span class="line">$ mkdir src/test</span><br><span class="line">$ mkdir src/test/java</span><br><span class="line">$ mkdir src/test/resources</span><br><span class="line">$ mkdir src/test/scala</span><br><span class="line">$ mkdir project</span><br><span class="line">$ ...</span><br></pre></td></tr></table></figure>
<p>可以使用giter8来自动化以上步骤.<br>giter8的更多信息可参考<a href="https://github.com//giter8" target="_blank" rel="external">https://github.com//giter8</a>.</p>
<h2 id="4-SBT-Cmd"><a href="#4-SBT-Cmd" class="headerlink" title="4. SBT Cmd"></a>4. SBT Cmd</h2><ol>
<li>actions – 显示对当前工程可用的命令</li>
<li>update – 下载依赖</li>
<li>compile – 编译代码</li>
<li>test – 运行测试代码</li>
<li>package – 创建一个可发布的jar包</li>
<li>publish-local – 把构建出来的jar包安装到本地的ivy缓存</li>
<li>publish – 把jar包发布到远程仓库（如果配置了的话)</li>
</ol>
<p>more cmd</p>
<ol>
<li>test-failed – 运行失败的spec</li>
<li>test-quick – 运行所有失败的以及/或者是由依赖更新的spec</li>
<li>clean-cache – 清除所有的sbt缓存。类似于sbt的clean命令</li>
<li>clean-lib – 删除lib_managed下的所有内容</li>
</ol>
<h2 id="5-Scala-HelloWorld"><a href="#5-Scala-HelloWorld" class="headerlink" title="5. Scala HelloWorld"></a>5. Scala HelloWorld</h2><p>SBT Scala HelloWorld 具体请看 : <a href="https://github.com/blair1/language/tree/master/scala/ScalaWorld" target="_blank" rel="external">Scala-Projects/HelloWorld</a></p>
<p>➜  HelloWorld&gt; sbt package</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[info] Loading project definition from /Users/hp/spark/HelloWorld/project</span><br><span class="line">[info] Set current project to HelloWorld (in build file:/Users/hp/spark/HelloWorld/)</span><br><span class="line">[info] Packaging /Users/hp/spark/HelloWorld/target/scala-2.11/helloworld_2.11-0.0.1-SNAPSHOT.jar ...</span><br><span class="line">[info] Done packaging.</span><br><span class="line">[success] Total time: 1 s, completed 2016-3-17 9:05:44</span><br></pre></td></tr></table></figure>
<p>➜  HelloWorld&gt; sbt run</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[info] Loading project definition from /Users/hp/spark/HelloWorld/project</span><br><span class="line">[info] Set current project to HelloWorld (in build file:/Users/hp/spark/HelloWorld/)</span><br><span class="line">[info] Running Hi</span><br><span class="line">Hi!</span><br><span class="line">[success] Total time: 1 s, completed 2016-3-17 9:07:43</span><br></pre></td></tr></table></figure>
<h2 id="6-Spark-HelloWorld"><a href="#6-Spark-HelloWorld" class="headerlink" title="6. Spark HelloWorld"></a>6. Spark HelloWorld</h2><p>Spark HelloWorld 具体请看 : <a href="https://github.com/blair101/bigdata-tools/tree/master/spark/HelloWorld" target="_blank" rel="external">Spark-Projects/HelloWorld</a></p>
<p>➜  HelloWorld&gt; sbt compile<br>➜  HelloWorld&gt; sbt package</p>
<pre><code class="shell">$SPARK_HOME/bin/spark-submit \
  --class &quot;HelloWorld&quot; \
    target/scala-2.11/helloworld_2.11-1.0.jar
</code></pre>
<h2 id="7-Referenced"><a href="#7-Referenced" class="headerlink" title="7. Referenced"></a>7. Referenced</h2><p>参考 : <a href="http://www.scala-sbt.org/0.13/docs/zh-cn/Getting-Started.html" target="_blank" rel="external">scala-sbt.org/0.13/docs/zh-cn/Getting-Started.html</a><br>参考 : <a href="https://github.com/CSUG/real_world_scala/blob/master/02_sbt.markdown" target="_blank" rel="external">CSUG/real_world_scala/blob/master/02_sbt.markdown</a><br>参考 : <a href="http://www.scala-sbt.org/0.13.1/docs/Getting-Started/Hello.html" target="_blank" rel="external">scala-sbt.org/0.13.1/docs/Getting-Started</a><br>参考 : <a href="http://article.yeeyan.org/view/442873/404261" target="_blank" rel="external">译言网</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[大数据平台CDH集群在线安装]]></title>
      <url>http://sggo.me/2016/03/14/hadoop/hadoop-cdh-install-online/</url>
      <content type="html"><![CDATA[<p>介绍了 CDH 集群的搭建与安装，其中 Server 安装步骤非常准确, Agent 需要进一步验证.</p>
<a id="more"></a>
<p>标签： Cloudera-Manager CDH Hadoop 部署 集群</p>
<blockquote>
<p>摘要：管理、部署Hadoop集群需要工具，Cloudera Manager便是其一。本文详细记录了以在线方式部署CDH集群&gt;的步骤。</p>
</blockquote>
<p>以Apache Hadoop为主导的大数据技术的出现，使得中小型公司对于大数据的存储与处理也拥有了武器。</p>
<p>目前Hadoop比较流行的主要有2个版本，Apache和Cloudera版本。</p>
<p>Apache Hadoop：维护人员比较多，更新频率比较快，但是稳定性比较差。<br>Cloudera Hadoop（CDH）：CDH：Cloudera公司的发行版本，基于Apache Hadoop的二次开发，优化了组件兼容和交互接口、简化安装配置、增加Cloudera兼容特性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">大数据平台CDH集群 cdh-5.70-rpm_install 详细过程</span><br></pre></td></tr></table></figure>
<h1 id="Part-1-install-cdh-server"><a href="#Part-1-install-cdh-server" class="headerlink" title="Part 1 install cdh server"></a>Part 1 install cdh server</h1><h2 id="1-1-Ready-install-resources"><a href="#1-1-Ready-install-resources" class="headerlink" title="1.1 Ready install resources"></a>1.1 Ready install resources</h2><ol>
<li>CentOS Linux release 7.1.1503 (Core) cm-5.7.0 </li>
<li>cloudera-manager-installer.bin</li>
<li>adduser deploy</li>
</ol>
<p>centos7.1 在安装过程时，网络配置，设置静态IP</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>
<p>设置静态ip，以及指定ip地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DEVICE=&quot;eth0&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">IPADDR=192.168.1.110</span><br><span class="line">NM_CONTROLLED=&quot;yes&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">DNS1=8.8.8.8</span><br><span class="line">DNS2=8.8.4.4</span><br><span class="line">GATEWAY=192.168.1.1</span><br></pre></td></tr></table></figure>
<h2 id="1-2-网络配置（所有节点）"><a href="#1-2-网络配置（所有节点）" class="headerlink" title="1.2 网络配置（所有节点）"></a>1.2 网络配置（所有节点）</h2><p><strong>修改hostname为 cdh-server7</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">　　RedHat 的 hostname，就修改 /etc/sysconfig/network文件，将里面的 HOSTNAME 这一行修改成 HOSTNAME=NEWNAME，其中 NEWNAME 就是你要设置的 hostname。</span><br><span class="line"></span><br><span class="line">　　Debian发行版的 hostname 的配置文件是 /etc/hostname</span><br></pre></td></tr></table></figure>
<p><strong>修改ip与主机名的对应关系</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# vi /etc/hosts #修改ip与主机名的对应关系:</span><br><span class="line">192.168.181.190 node190</span><br><span class="line">192.168.181.198 node198</span><br><span class="line">192.168.181.196 node196</span><br></pre></td></tr></table></figure>
<p><strong>重启网络服务生效</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# service network restart</span><br></pre></td></tr></table></figure>
<p><strong>关闭SELINUX</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看SELINUX状态</span><br><span class="line"></span><br><span class="line">[root@cdh-server7 ~]#getenforce</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">若 SELINUX 没有关闭，按照下述方式关闭</span><br><span class="line"></span><br><span class="line">vi /etc/selinux/config</span><br><span class="line">修改SELinux=disabled。重启生效，可以等后面都设置完了重启主机</span><br><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"># SELINUX= can take one of these three values:</span><br><span class="line">#       enforcing - SELinux security policy is enforced.</span><br><span class="line">#       permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line">#       disabled - SELinux is fully disabled.</span><br><span class="line">SELINUX=disabled</span><br><span class="line"># SELINUXTYPE= type of policy in use. Possible values are:</span><br><span class="line">#       targeted - Only targeted network daemons are protected.</span><br><span class="line">#       strict - Full SELinux protection.</span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# ping www.baidu.com</span><br></pre></td></tr></table></figure>
<p>以上步骤执行完毕后，重启主机</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<p>重启后再次检查下以上几点，确保环境配置正确。</p>
<h2 id="1-3-卸载-openjdk-所有节点"><a href="#1-3-卸载-openjdk-所有节点" class="headerlink" title="1.3 卸载 openjdk (所有节点)"></a>1.3 卸载 openjdk (所有节点)</h2><blockquote>
<p>注意 : 如果没有openjdk, 则不需要卸载，默认 centos7 没有</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep java</span><br><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep jdk</span><br><span class="line"></span><br><span class="line"># if exist java or jdk, uninstall, erase it.  example under this...</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="1-4-卸载-centOS7-默认mysql"><a href="#1-4-卸载-centOS7-默认mysql" class="headerlink" title="1.4 卸载 centOS7 默认mysql"></a>1.4 卸载 centOS7 默认mysql</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep mariadb</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps mariadb-libs-5.5.41-2.el7_0.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="1-5-Cloudera-Manager安装"><a href="#1-5-Cloudera-Manager安装" class="headerlink" title="1.5 Cloudera Manager安装"></a>1.5 Cloudera Manager安装</h2><p>下载资源文件<a href="https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo" target="_blank" rel="external">https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo</a></p>
<p>将cloudera-manager.repo文件拷贝到所有节点的/etc/yum.repos.d/文件夹下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node196 ]# cd /home/deploy/cdh</span><br><span class="line">[root@node196 cdh]# wget https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo</span><br><span class="line">[root@cdh-server7 cdh]# mv cloudera-manager.repo /etc/yum.repos.d/</span><br></pre></td></tr></table></figure>
<p>验证repo文件是否起效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum list|grep cloudera</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# yum list | grep cloudera</span><br><span class="line">cloudera-manager-agent.x86_64           5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">cloudera-manager-daemons.x86_64         5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">cloudera-manager-server.x86_64          5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">cloudera-manager-server-db-2.x86_64     5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">enterprise-debuginfo.x86_64             5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">oracle-j2sdk1.7.x86_64                  1.7.0+update67-1               cloudera-manager</span><br></pre></td></tr></table></figure>
<p>如果列出的不是你安装的版本，执行下面命令重试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum clean all </span><br><span class="line">yum list | grep cloudera</span><br></pre></td></tr></table></figure>
<p>上传下列 <strong>rpm 包</strong> 到 [root@cdh-server7] 的 /home/deploy/cdh/cloudera-rpms (任意目录)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /home/deploy/cdh/cloudera-rpms</span><br><span class="line">cloudera-manager-agent-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">cloudera-manager-daemons-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">cloudera-manager-server-5.7.0-1.cm560.p0.54.el7.x86_64.rpm   ## agent not use</span><br><span class="line">cloudera-manager-server-db-2-5.7.0-1.cm560.p0.54.el7.x86_64.rpm  ## agent not use</span><br><span class="line">enterprise-debuginfo-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm</span><br></pre></td></tr></table></figure>
<blockquote>
<p>说明 : 可从<a href="https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5/RPMS/x86_64/" target="_blank" rel="external">https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5/RPMS/x86_64/</a> 下载相关rpm包</p>
</blockquote>
<p>切换到rpms目录下，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cd /home/deploy/cdh/cloudera-rpms/</span><br><span class="line">[root@cdh-server7 cloudera-rpms]# yum -y install *.rpm</span><br></pre></td></tr></table></figure>
<h2 id="1-6-拷贝资源包到目标目录"><a href="#1-6-拷贝资源包到目标目录" class="headerlink" title="1.6 拷贝资源包到目标目录"></a>1.6 拷贝资源包到目标目录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从 http://archive.cloudera.com/cdh5/parcels/5.7.0/ 下载资源包</span><br></pre></td></tr></table></figure>
<p>将之前下载的Parcel那3个文件拷贝到/opt/cloudera/parcel-repo目录下（如果没有该目录，请自行创建）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cp CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel /opt/cloudera/parcel-repo/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel</span><br><span class="line">[root@cdh-server7 cdh]# cp CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha</span><br><span class="line">[root@cdh-server7 cdh]# cp manifest.json /opt/cloudera/parcel-repo/manifest.json</span><br></pre></td></tr></table></figure>
<h2 id="1-7-配置-java-环境变量"><a href="#1-7-配置-java-环境变量" class="headerlink" title="1.7 配置 java 环境变量"></a>1.7 配置 java 环境变量</h2><p>设置JAVA_HOME</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]#vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera/</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@cdh-server7 cdh]#source /etc/profile</span><br></pre></td></tr></table></figure>
<p>关闭防火墙</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]#systemctl stop firewalld.service  #centos7,关闭防火墙</span><br></pre></td></tr></table></figure>
<p>以上步骤执行完毕后，重启主机</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<h2 id="1-8-安装CM-只在主节点"><a href="#1-8-安装CM-只在主节点" class="headerlink" title="1.8 安装CM (只在主节点)"></a>1.8 安装CM (只在主节点)</h2><p><strong>以下两步骤请只在主节点上执行 :</strong></p>
<ul>
<li><p>进入该目录，给bin文件赋予可执行权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# chmod a+x ./cloudera-manager-installer.bin</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装CM (该步骤, 可能是不需要的)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# ./cloudera-manager-installer.bin</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>开始启动server端</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cd /etc/init.d/</span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-server-db start</span><br><span class="line"></span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-server start</span><br><span class="line">Starting cloudera-scm-server:                              [  OK  ]</span><br><span class="line">[root@cdh-server7 init.d]# tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意 :<br> 机器重启之后，默认启动会导致异常<br> 需要按照该先启动cloudera-scm-server-db，再启动cloudera-scm-server的顺序执行</p>
</blockquote>
<h2 id="1-9-浏览器访问验证-主节点"><a href="#1-9-浏览器访问验证-主节点" class="headerlink" title="1.9 浏览器访问验证(主节点)"></a>1.9 浏览器访问验证(主节点)</h2><p>CM安装成功后浏览器输入<a href="http://ip:7180" target="_blank" rel="external">http://ip:7180</a>, 用户名和密码都输入admin，进入web管理界面。</p>
<p>通过浏览器访问验证</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://192.168.181.190:7180/</span><br></pre></td></tr></table></figure>
<p>如果打不开改网页，等待2分钟后。这个服务启动是需要一定时间的。</p>
<p>选择部署的版本，这里我们选择免费版的就可以了。</p>
<blockquote>
<p>如果不会设置，那么请参考 最靠谱的安装指南 <a href="http://www.jianshu.com/p/57179e03795f" target="_blank" rel="external">http://www.jianshu.com/p/57179e03795f</a></p>
</blockquote>
<p>安装服务时，数据库选择默认的嵌入式数据库</p>
<h1 id="Part-2-安装-agent"><a href="#Part-2-安装-agent" class="headerlink" title="Part 2 安装 agent"></a>Part 2 安装 agent</h1><blockquote>
<p>this step is similar， but I can’t be sure, exactly right. </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">安装 agent ，可以在单独的机器，主节点，可以只当做主，随意你</span><br></pre></td></tr></table></figure>
<blockquote>
<p>为agent做配置,启动agent (所有节点)<br>agent 不需要装server，其他绝大部分步骤和 安装 server 相同。</p>
</blockquote>
<h2 id="2-1-网络配置"><a href="#2-1-网络配置" class="headerlink" title="2.1 网络配置"></a>2.1 网络配置</h2><p><strong>修改ip与主机名的对应关系</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-agent1 ~]# vi /etc/hosts #修改ip与主机名的对应关系:</span><br><span class="line">192.168.181.190 cdh-server7(node190)</span><br><span class="line">192.168.181.198 cdh-agent1(node198)</span><br><span class="line">192.168.181.196 cdh-agent2(node196)</span><br></pre></td></tr></table></figure>
<p><strong>重启网络服务生效</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# service network restart</span><br></pre></td></tr></table></figure>
<p><strong>关闭SELINUX</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看SELINUX状态</span><br><span class="line"></span><br><span class="line">[root@cdh-server7 ~]#getenforce</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">若 SELINUX 没有关闭，按照下述方式关闭</span><br><span class="line"></span><br><span class="line">vi /etc/selinux/config</span><br><span class="line">修改SELinux=disabled。重启生效，可以等后面都设置完了重启主机</span><br><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"># SELINUX= can take one of these three values:</span><br><span class="line">#       enforcing - SELinux security policy is enforced.</span><br><span class="line">#       permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line">#       disabled - SELinux is fully disabled.</span><br><span class="line">SELINUX=disabled</span><br><span class="line"># SELINUXTYPE= type of policy in use. Possible values are:</span><br><span class="line">#       targeted - Only targeted network daemons are protected.</span><br><span class="line">#       strict - Full SELinux protection.</span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# ping www.baidu.com</span><br></pre></td></tr></table></figure>
<h2 id="2-2-卸载-openjdk-所有节点"><a href="#2-2-卸载-openjdk-所有节点" class="headerlink" title="2.2 卸载 openjdk (所有节点)"></a>2.2 卸载 openjdk (所有节点)</h2><blockquote>
<p>注意 : 如果没有openjdk, 则不需要卸载，默认 centos7 没有</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep java</span><br><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep jdk</span><br><span class="line"></span><br><span class="line"># if exist java or jdk, uninstall, erase it.  example under this...</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="2-3-卸载centOS7默认mysql"><a href="#2-3-卸载centOS7默认mysql" class="headerlink" title="2.3 卸载centOS7默认mysql"></a>2.3 卸载centOS7默认mysql</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep mariadb</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps mariadb-libs-5.5.41-2.el7_0.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="2-4-cloudera-manager-repo"><a href="#2-4-cloudera-manager-repo" class="headerlink" title="2.4 cloudera-manager.repo"></a>2.4 cloudera-manager.repo</h2><blockquote>
<p>上传cloudera-manager.repo 到 cdh-agent1</p>
</blockquote>
<p>[root@cdh-agent1 cdh]# cp cloudera-manager.repo /etc/yum.repos.d/</p>
<p><strong>transparent_hugepage</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
<p><strong>vi /etc/rc.local 在文件尾放入 如下两条语句</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x /etc/rc.local</span><br></pre></td></tr></table></figure>
<p><strong>调整swappiness</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo 10 &gt; /proc/sys/vm/swappiness</span><br><span class="line"># vi /etc/sysctl.conf</span><br><span class="line">vm.swappiness = 10</span><br></pre></td></tr></table></figure>
<h2 id="2-5-cdh-cloudera-rpms"><a href="#2-5-cdh-cloudera-rpms" class="headerlink" title="2.5 ~/cdh/cloudera-rpms"></a>2.5 ~/cdh/cloudera-rpms</h2><blockquote>
<p>上传下列rpm包到cdh-agent1的/home/deploy/cdh/cloudera-rpms</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cloudera-manager-agent-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">cloudera-manager-daemons-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">enterprise-debuginfo-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@cdh-agent1 init.d]# cd /home/deploy/cdh/cloudera-rpms/</span><br><span class="line">[root@cdh-agent1 init.d]# yum -y install *.rpm</span><br></pre></td></tr></table></figure>
<p><strong>设置JAVA_HOME</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]#vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera/</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@cdh-server7 cdh]#source /etc/profile</span><br></pre></td></tr></table></figure>
<p>关闭防火墙</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]#systemctl stop firewalld.service  #centos7,关闭防火墙</span><br></pre></td></tr></table></figure>
<p>以上步骤执行完毕后，重启主机</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-agent1 init.d]# vi /etc/cloudera-scm-agent/config.ini</span><br><span class="line"></span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</span><br><span class="line"># Hostname of the CM server.</span><br><span class="line">#server_host=localhost</span><br><span class="line">server_host=cdh-server7(node190)</span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cd /etc/init.d/</span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-agent start</span><br><span class="line">Starting cloudera-scm-agent:                               [  OK  ]</span><br><span class="line">[root@cdh-server deploy]# tail -f /var/log//cloudera-scm-agent/cloudera-scm-agent.log</span><br></pre></td></tr></table></figure>
<hr>
<p>注意 : </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">安装YARN NodeManager失败时，需要删除 /yarn /var/lib/hadoop-yarn 目录再重新添加</span><br></pre></td></tr></table></figure>
<hr>
<p>CDH最靠谱的安装指南 : <a href="http://www.jianshu.com/p/57179e03795f" target="_blank" rel="external">http://www.jianshu.com/p/57179e03795f</a></p>
<h1 id="Part-3-恢复启动-Our-集群"><a href="#Part-3-恢复启动-Our-集群" class="headerlink" title="Part 3 恢复启动 Our 集群"></a>Part 3 恢复启动 Our 集群</h1><h2 id="3-1-确定-firewalld-close"><a href="#3-1-确定-firewalld-close" class="headerlink" title="3.1 确定 firewalld close"></a>3.1 确定 firewalld close</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start firewalld.service#启动firewall</span><br><span class="line">systemctl stop firewalld.service#停止firewall</span><br><span class="line">systemctl disable firewalld.service#禁止firewall开机启动</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意 : 操作之前确定 firewalld 是关闭的</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node19x flag]$ vim /etc/rc.local (/etc/rc.local 对应貌似相对dir /ect/init.d)</span><br><span class="line"></span><br><span class="line">  1 #!/bin/bash</span><br><span class="line">  2 # THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES</span><br><span class="line">  3 #</span><br><span class="line">  4 # It is highly advisable to create own systemd services or udev rules</span><br><span class="line">  5 # to run scripts during boot instead of using this file.</span><br><span class="line">  6 #</span><br><span class="line">  7 # In contrast to previous versions due to parallel execution during boot</span><br><span class="line">  8 # this script will NOT be run after all other services.</span><br><span class="line">  9 #</span><br><span class="line"> 10 # Please note that you must run &apos;chmod +x /etc/rc.d/rc.local&apos; to ensure</span><br><span class="line"> 11 # that this script will be executed during boot.</span><br><span class="line"> 12</span><br><span class="line"> 13 touch /var/lock/subsys/local</span><br><span class="line"> 14 echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"> 15 echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line"> 16 service ntpd start</span><br><span class="line"> 17 service elasticsearch start</span><br></pre></td></tr></table></figure>
<h2 id="3-2-启动server端、cm"><a href="#3-2-启动server端、cm" class="headerlink" title="3.2 启动server端、cm"></a>3.2 启动server端、cm</h2><p>only at server node</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cd /etc/init.d/</span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-server-db start</span><br><span class="line"></span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-server start</span><br><span class="line">Starting cloudera-scm-server:                              [  OK  ]</span><br><span class="line">[root@cdh-server7 init.d]# tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</span><br><span class="line"></span><br><span class="line">// 等待日志 7180 启动成功， 访问 : http://node190:7180/cmf/home</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意 :<br>机器重启之后，默认启动会导致异常<br>需要按照该先启动cloudera-scm-server-db，再启动cloudera-scm-server的顺序执行</p>
</blockquote>
<p>一般以下 agent 是自动启动的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node190 init.d]# ./cloudera-scm-agent start</span><br><span class="line">cloudera-scm-agent is already running</span><br><span class="line">node190:./cloudera-scm-agent start</span><br><span class="line">node19x:./cloudera-scm-agent start</span><br><span class="line">node19x:./cloudera-scm-agent start</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="3-3-CM页面上启动各服务"><a href="#3-3-CM页面上启动各服务" class="headerlink" title="3.3 CM页面上启动各服务"></a>3.3 CM页面上启动各服务</h2><ol>
<li>CM 页面上重启 service monitor</li>
<li>CM 页面上重启 host monitor</li>
<li>CM 页面上启动各项服务 (如 : ZK, Flume, YARN, HDFS, Hive, Sqoop, Spark etc..)</li>
</ol>
<hr>
<h2 id="3-4-各个节点启动-ES"><a href="#3-4-各个节点启动-ES" class="headerlink" title="3.4 各个节点启动 ES"></a>3.4 各个节点启动 ES</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[deploy@node190 init.d]# ll</span><br><span class="line">total 44</span><br><span class="line">-rwxr-xr-x  1 root root  8671 Apr  2 04:52 cloudera-scm-agent</span><br><span class="line">lrwxrwxrwx. 1 root root    58 Apr 18 16:55 elasticsearch -&gt; /home/deploy/elasticsearch-1.7.1/bin/service/elasticsearch</span><br><span class="line">-rw-r--r--. 1 root root 13948 Sep 16  2015 functions</span><br><span class="line">-rwxr-xr-x. 1 root root  2989 Sep 16  2015 netconsole</span><br><span class="line">-rwxr-xr-x. 1 root root  6630 Sep 16  2015 network</span><br><span class="line">-rw-r--r--. 1 root root  1160 Apr  1 00:45 README</span><br></pre></td></tr></table></figure>
<p><strong>deploy</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/deploy/elasticsearch-1.7.1/bin/service</span><br><span class="line">[deploy@node190 init.d]<span class="comment"># ./elasticsearch start</span></span><br><span class="line">[deploy@node19x init.d]<span class="comment"># ./elasticsearch start</span></span><br><span class="line">[deploy@node19x init.d]<span class="comment"># ./elasticsearch start</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://node190:9200/_plugin/bigdesk/#cluster</span><br></pre></td></tr></table></figure>
<blockquote>
<p>等待同步数据完成，一般会很快，等待 Status 从 RED 变为 green 状态</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://node190:9200/_plugin/head/</span><br></pre></td></tr></table></figure>
<h2 id="3-5-启动-kibana"><a href="#3-5-启动-kibana" class="headerlink" title="3.5 启动 kibana"></a>3.5 启动 kibana</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[deploy@node196 ~]#</span><br><span class="line">cd /home/deploy/kibana-4.1.1-linux-x64</span><br><span class="line">    ./bin/kibana &gt; kibana.log 2&gt;&amp;1 &amp;              --@deploy</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Sqoop introduce]]></title>
      <url>http://sggo.me/2016/02/16/hadoop/hadoop-sqoop-learn-use01/</url>
      <content type="html"><![CDATA[<p>Sqoop 即 SQL to Hadoop, 是一款方便的在传统关系数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输.</p>
<a id="more"></a>
<h2 id="1-Sqoop-what"><a href="#1-Sqoop-what" class="headerlink" title="1. Sqoop what ?"></a>1. Sqoop what ?</h2><p>sqoop 即 SQL to Hadoop ，是一款方便的在传统关系数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输，发展至今主要演化了二大版本，sqoop1和sqoop2。 </p>
<p>sqoop : clouder 公司开发</p>
<p><strong>生产背景</strong></p>
<ol>
<li>mysql  导入 Hadoop </li>
<li>Hadoop 导入 mysql</li>
</ol>
<p>注 : 以上 Hadoop 指 Hive、HBase、HDFS 等</p>
<h2 id="2-Sqoop-特点"><a href="#2-Sqoop-特点" class="headerlink" title="2. Sqoop 特点"></a>2. Sqoop 特点</h2><p>sqoop架构非常简单，其整合了Hive、Hbase和Oozie，通过map-reduce任务来传输数据，从而提供并发特性和容错。</p>
<p>   Sqoop 由两部分组成：客户端(client)和服务端(server)。需要在集群的其中某个节点上安装server，该节点的服务端可以作为其他 Sqoop 客户端的入口点。</p>
<p>   在 server 端的节点上必须安装有 Hadoop。client 可以安装在任意数量的机子上。在装有客户端的机子上不需要安装 Hadoop。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop 官网 : https://sqoop.apache.org</span><br><span class="line"></span><br><span class="line">1.4.5官方文档 : https://sqoop.apache.org/docs/1.4.5/</span><br><span class="line"></span><br><span class="line">sqoop2不推荐的原因 : http://blog.csdn.net/robbyo/article/details/50737356</span><br></pre></td></tr></table></figure>
<h2 id="3-Sqoop-优缺点"><a href="#3-Sqoop-优缺点" class="headerlink" title="3. Sqoop 优缺点"></a>3. Sqoop 优缺点</h2><p><strong>优点</strong></p>
<ol>
<li>高效可控的利用资源，任务并行度，超时时间。</li>
<li>数据类型映射与转化，可自动进行，用户也可自定义 .</li>
<li>支持多种主流数据库，MySQL,Oracle，SQL Server，DB2等等 。</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>基于命令行的操作方式，易出错，且不安全。</li>
<li>数据传输和数据格式是紧耦合的，这使得connector无法支持所有的数据格式</li>
<li>用户名和密码暴漏出来</li>
</ol>
<h2 id="4-Sqoop-原理"><a href="#4-Sqoop-原理" class="headerlink" title="4. Sqoop 原理"></a>4. Sqoop 原理</h2><h3 id="4-1-Sqoop的import原理"><a href="#4-1-Sqoop的import原理" class="headerlink" title="4.1 Sqoop的import原理"></a>4.1 Sqoop的import原理</h3><p>Sqoop 在 import 时，需要制定 split-by 参数。</p>
<p>Sqoop 根据不同的 split-by参数值 来进行切分, 然后将切分出来的区域分配到不同 map 中。每个map中再处理数据库中获取的一行一行的值，写入到 HDFS 中。同时split-by 根据不同的参数类型有不同的切分方法，如比较简单的int型，Sqoop会取最大和最小split-by字段值，然后根据传入的 num-mappers来确定划分几个区域。 </p>
<p>比如 select max(split_by),min(split-by) from 得到的 max(split-by)和 min(split-by) 分别为 1000 和 1, 而 num-mappers 为 2 的话，则会分成两个区域 (1,500) 和 (501-100), 同时也会分成 2个sql 给 2个map 去进行导入操作，分别为 select XXX from table where split-by&gt;=1 and split-by<500 和="" select="" xxx="" from="" table="" where="" split-by="">=501 and split-by&lt;=1000。最后每个map各自获取各自SQL中的数据进行导入工作。</500></p>
<h3 id="4-2-Sqoop的export原理"><a href="#4-2-Sqoop的export原理" class="headerlink" title="4.2. Sqoop的export原理"></a>4.2. Sqoop的export原理</h3><p>根据 mysql 表名称，生成一个以表名称命名的 Java类，该类继承了 sqoopRecord的，是一个只有 Map 的 MR，且自定义了输出字段。</p>
<p>sqoop export –connect jdbc:mysql://$url:3306/$3?characterEncoding=utf8 –username $username –password $password –table $1 –export-dir $2 –input-fields-terminated-by ‘|’ –null-non-string ‘0’ –null-string ‘0’;</p>
<h2 id="5-Sqoop-使用实例"><a href="#5-Sqoop-使用实例" class="headerlink" title="5. Sqoop 使用实例"></a>5. Sqoop 使用实例</h2><p><strong>环境</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop: sqoop-1.4.5+cdh5.3.6+78</span><br><span class="line">hive : hive-0.13.1+cdh5.3.6+397</span><br><span class="line">hbase: hbase-0.98.6+cdh5.3.6+115</span><br></pre></td></tr></table></figure>
<h3 id="5-1-Mysql-to-Hadoop"><a href="#5-1-Mysql-to-Hadoop" class="headerlink" title="5.1. Mysql to Hadoop"></a>5.1. Mysql to Hadoop</h3><ul>
<li>Mysql to Hdfs</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">  --connect $&#123;jdbc_url&#125; --username $&#123;jdbc_username&#125; --password  $&#123;jdbc_passwd&#125; \</span><br><span class="line">  --query &quot;$&#123;exec_sql&#125;&quot; \</span><br><span class="line">  --split-by $&#123;id&#125; -m 10 \</span><br><span class="line">  --target-dir $&#123;target_dir&#125; \</span><br><span class="line">  --fields-terminated-by &quot;\001&quot; --lines-terminated-by &quot;\n&quot; \</span><br><span class="line">  --hive-drop-import-delims \</span><br><span class="line">  --null-string &apos;\\N&apos; --null-non-string &apos;\\N&apos;</span><br></pre></td></tr></table></figure>
<ul>
<li>Mysql To Hive</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">  --connect $&#123;jdbc_url&#125; \</span><br><span class="line">  --username $&#123;jdbc_username&#125; --password  $&#123;jdbc_passwd&#125; \</span><br><span class="line">  --table $&#123;jdbc_table&#125; --fields-terminated-by &quot;\001&quot; --lines-terminated-by &quot;\n&quot; \</span><br><span class="line">  --hive-import --hive-overwrite --hive-table $&#123;hive_table&#125; \</span><br><span class="line">  --null-string &apos;\\N&apos; --null-non-string &apos;\\N&apos;</span><br></pre></td></tr></table></figure>
<ul>
<li>Mysql To HBase</li>
</ul>
<h3 id="5-2-Hadoop-to-Mysql"><a href="#5-2-Hadoop-to-Mysql" class="headerlink" title="5.2 Hadoop to Mysql"></a>5.2 Hadoop to Mysql</h3><ul>
<li>Hdfs To Mysql</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> -D sqoop.export.records.per.statement=10 \</span><br><span class="line">--connect jdbc:mysql://192.168.***.**:3306/***?autoReconnect=<span class="literal">true</span> </span><br><span class="line">--username *** </span><br><span class="line">--password *** </span><br><span class="line">--table mds_dm_rs_shop_result \</span><br><span class="line">--fields-terminated-by <span class="string">'\t'</span> </span><br><span class="line">--export-dir <span class="string">"/dc_ext/xbd/dm/mds/mds_dm_rs_shop_result/dt=20170410"</span> </span><br><span class="line">--null-string <span class="string">'\\N'</span> </span><br><span class="line">--null-non-string <span class="string">'\\N'</span>;</span><br></pre></td></tr></table></figure>
<p><strong>refence article</strong></p>
<p><a href="http://www.zihou.me/html/2014/01/28/9114.html" target="_blank" rel="external">Sqoop中文文档</a><br><a href="http://www.aboutyun.com/thread-12684-1-1.html" target="_blank" rel="external">Hive to Mysql 常遇九大问题总结</a> </p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hive Introduce 1]]></title>
      <url>http://sggo.me/2016/02/15/hadoop/hadoop-hive-brief/</url>
      <content type="html"><![CDATA[<ol>
<li>初步了解 Hadoop 生态圈</li>
<li>初步了解 Hive 架构图</li>
</ol>
<a id="more"></a>
<h2 id="1-Hive-Introduce"><a href="#1-Hive-Introduce" class="headerlink" title="1. Hive Introduce"></a>1. Hive Introduce</h2><h3 id="1-1-Hive-Preface"><a href="#1-1-Hive-Preface" class="headerlink" title="1.1 Hive Preface"></a>1.1 Hive Preface</h3><p><strong>Hadoop</strong></p>
<ol>
<li>Hadoop 生态系统 是 处理大数据集而产生的解决方案。</li>
<li>Hadoop 实现计算模型 MapReduce, 可将计算任务分割成多个处理单元，这个计算模型下面是一个 HDFS。</li>
</ol>
<p><strong>Hive</strong></p>
<ol>
<li>Hive 提供了一个 Hive查询语言 HiveQL, 查询转换为 MapReduce job</li>
<li>Hive 适合做数据仓库，可离线维护海量数据，可对数据进行挖掘, 形成报告等</li>
<li>Hadoop、HDFS 设计本身限制了 Hive 所能胜任的工作, Hive 不支持记录级别的更新、插入 或者 删除 操作。</li>
</ol>
<p><strong>Hive 运行架构</strong></p>
<ol>
<li>使用 HQL 作为查询接口；</li>
<li>使用 MapReduce 作为执行层；</li>
<li>使用 HDFS 作为存储层；</li>
</ol>
<h3 id="1-2-Hadoop-Mapreduce"><a href="#1-2-Hadoop-Mapreduce" class="headerlink" title="1.2 Hadoop / Mapreduce"></a>1.2 Hadoop / Mapreduce</h3><p><code>Input -&gt; Mappers -&gt; Sort,Shuffle -&gt; Reducers -&gt; Output</code></p>
<h3 id="1-3-Hive-系统架构"><a href="#1-3-Hive-系统架构" class="headerlink" title="1.3 Hive 系统架构"></a>1.3 Hive 系统架构</h3><p><img src="/images/hadoop/hive-02.png" alt="Hive 系统架构"></p>
<h2 id="2-Hive-架构组件分析"><a href="#2-Hive-架构组件分析" class="headerlink" title="2. Hive 架构组件分析"></a>2. Hive 架构组件分析</h2><p><strong>本章重点 :</strong></p>
<ol>
<li>初步了解 Hive 的工作流</li>
<li>初步了解 hive 的工作组件</li>
</ol>
<h3 id="2-1-元数据存储Metastore"><a href="#2-1-元数据存储Metastore" class="headerlink" title="2.1 元数据存储Metastore"></a>2.1 元数据存储Metastore</h3><ul>
<li><p>Hive的数据由两部分组成：数据文件 和 元数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">元数据存储，Derby只能用于一个Hive连接，一般存储在MySQL。</span><br><span class="line"></span><br><span class="line">元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-2-驱动-Driver"><a href="#2-2-驱动-Driver" class="headerlink" title="2.2 驱动 (Driver)"></a>2.2 驱动 (Driver)</h3><ul>
<li>编译器</li>
<li>优化器</li>
<li>执行器</li>
</ul>
<p>用户通过下面的接口提交Hive给Driver，由Driver进行HQL语句解析，此时从Metastore中获取表的信息，先生成逻辑计划，再生成物理计划，再由Executor生成Job交给Hadoop运行，然后由Driver将结果返回给用户。</p>
<p>编译器（Hive的核心）：1，语义解析器（ParseDriver），将查询字符串转换成解析树表达式；2，语法解析器（SemanticAnalyzer），将解析树转换成基于语句块的内部查询表达式；3，逻辑计划生成器（Logical Plan Generator），将内部查询表达式转换为逻辑计划，这些计划由逻辑操作树组成，操作符是Hive的最小处理单元，每个操作符处理代表一道HDFS操作或者是MR作业；4，查询计划生成器（QueryPlan Generator），将逻辑计划转化成物理计划（MR Job）。</p>
<p>优化器：优化器是一个演化组件，当前它的规则是：列修剪，谓词下压。</p>
<p>执行器：编译器将操作树切分成一个Job链（DAG），执行器会顺序执行其中所有的Job；如果Task链不存在依赖关系，可以采用并发执行的方式进行Job的执行。</p>
<h3 id="2-3-接口"><a href="#2-3-接口" class="headerlink" title="2.3 接口"></a>2.3 接口</h3><p><strong>CLI、HWI、ThriftServer</strong></p>
<ol>
<li><p>CLI：为命令行工具，默认服务。bin/hive或bin/hive–service cli；</p>
</li>
<li><p>HWI：为Web接口，可以用过浏览器访问Hive，默认端口9999，启动方式为bin/hive –service hwi;</p>
</li>
<li><p>ThriftServer：通过Thrift对外提供服务，默认端口是10000，启动方式为bin/hive –service hiveserver;</p>
</li>
</ol>
<p><strong> 连接hive-metastore(如mysql)的三种方式 </strong></p>
<ol>
<li>单用户模式。此模式连到数据库Derby，一般用于Unit Test。<br><img src="/images/hadoop/hive-longdis-model.jpeg" alt="单用户模式"></li>
<li>多用户模式。通过网络连接到一个数据库中，是最经常使用到的模式。<br><img src="/images/hadoop/hive-more-user-model.jpeg" alt="多用户模式"></li>
<li>远程服务器模式。用于非Java客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer访问元数据库。<br><img src="/images/hadoop/hive-longdis-model.jpeg" alt="远程服务器模式"></li>
</ol>
<h3 id="2-4-其他服务"><a href="#2-4-其他服务" class="headerlink" title="2.4 其他服务"></a>2.4 其他服务</h3><p><strong>bin/hive –service -help</strong></p>
<ol>
<li><p>metastore   (bin/hive –service metastore)</p>
</li>
<li><p>hiveserver2（bin/hive –service hiveserver2）</p>
</li>
</ol>
<p><strong>HiveServer2</strong></p>
<ol>
<li><p>HiveServer2是HieServer改进版本，它提供给新的ThriftAPI来处理JDBC或者ODBC客户端，进行Kerberos身份验证，多个客户端并发</p>
</li>
<li><p>HS2还提供了新的CLI：BeeLine，是Hive 0.11引入的新的交互式CLI，基于SQLLine，可以作为Hive JDBC Client 端访问HievServer2，启动一个beeline就是维护了一个session.</p>
</li>
</ol>
<p><strong>Hive下载地址</strong></p>
<ol>
<li><p>cdh-hive : <a href="https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/hive/hive-exec/0.13.1-cdh5.3.6/" target="_blank" rel="external">hive0.13.1-cdh5.3.6 jar 包</a> (没用)</p>
</li>
<li><p>apache-hive : <a href="http://archive.apache.org/dist/hive/" target="_blank" rel="external">Apache-Hive</a></p>
</li>
</ol>
<p><strong>Hive-Beeline 试验成功</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">下载 apache-hive-0.13.1-bin, apache-hadoop2.5，配置 HADOOP_HOME, 启动 </span><br><span class="line"></span><br><span class="line">➜  ./apache-hive-0.13.1-bin/bin/beeline</span><br><span class="line">Beeline version 0.13.1 by Apache Hive</span><br><span class="line">beeline&gt; !connect jdbc:hive2://node190:10000 hdfs 1</span><br><span class="line">scan complete in 3ms</span><br><span class="line">Connecting to jdbc:hive2://node190:10000</span><br><span class="line">Connected to: Apache Hive (version 0.13.1-cdh5.3.6)</span><br><span class="line">Driver: Hive JDBC (version 0.13.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://node190:10000&gt; select count(*) from ods_dm_shop_tmp;</span><br><span class="line">+-------+</span><br><span class="line">|  _c0  |</span><br><span class="line">+-------+</span><br><span class="line">| 1091  |</span><br><span class="line">+-------+</span><br><span class="line">1 row selected (24.815 seconds)</span><br><span class="line">0: jdbc:hive2://node190:10000&gt;</span><br><span class="line"></span><br><span class="line">说明 : beeline 可以成功，用代码 jdbc 就可以成功</span><br><span class="line"></span><br><span class="line">安装 hadoop 参考了 《Spark大数据处理》高彦杰@著, 不用配置直接绿色简单版</span><br></pre></td></tr></table></figure>
<p><strong>Hive table</strong></p>
<p>  table 中的一个 Partition 对应表下的一个子目录<br>  每一个 Bucket 对应一个文件；<br>  Hive的默认数据仓库目录是/user/hive/warehouse<br>  在hive-site.xml中由hive.metastore.warehouse.dir项定义；</p>
<h2 id="reference-article"><a href="#reference-article" class="headerlink" title="reference article"></a>reference article</h2><p>参考 : <a href="http://blog.csdn.net/lalaguozhe/article/details/11776055" target="_blank" rel="external">CSDN - Hive Server 2 调研，安装和部署</a><br>参考 : <a href="http://www.geedoo.info/beeline-abnormal-connection-hiveserver2.html" target="_blank" rel="external">极豆技术博客 - Beeline连接hiveserver2异常</a><br>参考 : <a href="http://blog.csdn.net/skywalker_only/article/details/38366347" target="_blank" rel="external">Hive学习之HiveServer2 JDBC客户端</a><br>参考 : <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline" target="_blank" rel="external">HiveServer2 Clients beeline</a><br>参考 : <a href="http://www.aboutyun.com/blog-6-1855.html" target="_blank" rel="external">Beeline连接hiveserver2异常</a><br>参考 : <a href="http://blog.csdn.net/skywalker_only/article/details/38335235" target="_blank" rel="external">Hive学习之HiveServer2服务端配置与启动</a></p>
<p><strong>other tmp</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## Chap 7 HiveQL 视图 ##</span><br><span class="line">## Chap 8 HiveQL 索引 ##</span><br><span class="line">## Chap 9 模式设计 ##</span><br><span class="line">## Chap 10 调优 ##</span><br><span class="line">## Chap 11 其他文件格式和压缩方法 ##</span><br><span class="line">## Chap 12 开发 ##</span><br><span class="line">## Chap 13 函数 ##</span><br><span class="line">## Chap 14 Streaming ##</span><br><span class="line">## Chap 15 自定义Hive文件和记录格式 ##</span><br><span class="line">## Chap 16 Hive 的 Thrift 服务 ##</span><br><span class="line">## Chap 11 其他文件格式和压缩方法 ##</span><br></pre></td></tr></table></figure>
<hr>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Introduce and Install]]></title>
      <url>http://sggo.me/2016/02/01/spark/spark-introduce-and-install/</url>
      <content type="html"><![CDATA[<p>介绍 Spark 的历史，介绍 Spark 的安装与部署，介绍 Spark 的代码架构 等</p>
<a id="more"></a>
<p>Spark 发源于 美国加州大学伯克利分校 AMPLap 大数据分析平台<br>Spark 立足于内存计算、从多迭代批量处理出发<br>Spark 兼顾数据仓库、流处理、图计算 等多种计算范式，大数据系统领域全栈计算平台  </p>
<p><a href="http://spark.apache.org" target="_blank" rel="external">spark.apache.org</a> </p>
<blockquote>
<p>University of California, Berkeley </p>
</blockquote>
<h2 id="1-Spark-的历史与发展"><a href="#1-Spark-的历史与发展" class="headerlink" title="1. Spark 的历史与发展"></a>1. Spark 的历史与发展</h2><ul>
<li>2009 年 : Spark 诞生于 AMPLab  </li>
<li>2014-02 : Apache 顶级项目  </li>
<li>2014-05 : Spark 1.0.0 发布</li>
</ul>
<h2 id="2-Spark-之于-Hadoop"><a href="#2-Spark-之于-Hadoop" class="headerlink" title="2. Spark 之于 Hadoop"></a>2. Spark 之于 Hadoop</h2><p> Spark 是 MapReduce 的替代方案, 且兼容 HDFS、Hive 等分布式存储层。</p>
<p> Spark 相比 Hadoop MapReduce 的优势如下 :</p>
<ol>
<li>中间结果输出</li>
<li>数据格式和内存布局</li>
<li>执行策略  </li>
<li>任务调度的开销</li>
</ol>
<blockquote>
<p>Spark用事件驱动类库AKKA来启动任务, 通过线程池复用线程避免进线程启动切换开销</p>
</blockquote>
<h2 id="3-Spark-能带来什么"><a href="#3-Spark-能带来什么" class="headerlink" title="3. Spark 能带来什么 ?"></a>3. Spark 能带来什么 ?</h2><ol>
<li>打造全栈多计算范式的高效数据流水线</li>
<li>轻量级快速处理, 并支持 Scala、Python、Java</li>
<li>与 HDFS 等 存储层 兼容</li>
</ol>
<h2 id="4-Spark-安装与部署"><a href="#4-Spark-安装与部署" class="headerlink" title="4. Spark 安装与部署"></a>4. Spark 安装与部署</h2><p>Spark 主要使用 HDFS 充当持久化层，所以完整的安装 Spark 需要先安装 Hadoop.<br>Spark 是计算框架, 它主要使用 HDFS 充当持久化层。</p>
<p><strong>Linux 集群安装 Spark</strong></p>
<ol>
<li>安装 JDK</li>
<li>安装 Scala</li>
<li>配置 SSH 免密码登陆 (可选)</li>
<li>安装 Hadoop</li>
<li>安装 Spark</li>
<li>启动 Spark 集群</li>
</ol>
<p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">Spark官网下载</a></p>
<h3 id="4-1-安装-Spark"><a href="#4-1-安装-Spark" class="headerlink" title="4.1 安装 Spark"></a>4.1 安装 Spark</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1). download  spark-1.5.2-bin-hadoop2.6.tgz</span><br><span class="line"></span><br><span class="line">(2). tar -xzvf spark-1.5.2-bin-hadoop2.6.tgz</span><br><span class="line"></span><br><span class="line">(3). 配置 conf/spark-env.sh</span><br><span class="line">    1) 详细复杂参数配置参见 官网 Configuration</span><br><span class="line">    2) vim conf/spark-env.sh</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home</span><br><span class="line">    <span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/Cellar/scala/2.11.5</span><br><span class="line">    <span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/xSoft/spark</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_IP=ip</span><br><span class="line">    <span class="built_in">export</span> MASTER=spark://ip:7077</span><br><span class="line"></span><br><span class="line">    <span class="built_in">export</span> SPARK_EXECUTOR_INSTANCES=2</span><br><span class="line">    <span class="built_in">export</span> SPARK_EXECUTOR_CORES=1</span><br><span class="line"></span><br><span class="line">    <span class="built_in">export</span> SPARK_WORKER_MEMORY=1000m</span><br><span class="line">    <span class="built_in">export</span> SPARK_EXECUTOR_MEMORY=300m</span><br><span class="line"></span><br><span class="line">    <span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$&#123;SPARK_HOME&#125;</span>/lib</span><br><span class="line"></span><br><span class="line">(4). 配置 conf/slaves (测试可选)</span><br><span class="line">(5). 一般需要 startup ssh server.</span><br></pre></td></tr></table></figure>
<h3 id="4-2-启动-Spark-集群"><a href="#4-2-启动-Spark-集群" class="headerlink" title="4.2 启动 Spark 集群"></a>4.2 启动 Spark 集群</h3><p>在 Spark 根目录启动 Spark</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/start-all.sh</span><br><span class="line">./sbin/stop-all.sh</span><br></pre></td></tr></table></figure>
<p>启动后 jps 查看 会有 Master 进程存在</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  spark-1.5.2-bin-hadoop2.6  jps</span><br><span class="line">11262 Jps</span><br><span class="line">11101 Master</span><br><span class="line">11221 Worker</span><br></pre></td></tr></table></figure>
<h3 id="4-3-Spark-集群初试"><a href="#4-3-Spark-集群初试" class="headerlink" title="4.3 Spark 集群初试"></a>4.3 Spark 集群初试</h3><p>可以通过两种方式运行 Spark 样例 :</p>
<ul>
<li>以 ./run-example 的方式执行</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/xSoft/spark</span><br><span class="line">➜  spark ./sbin/start-all.sh</span><br><span class="line">➜  spark ./bin/run-example org.apache.spark.examples.SparkPi</span><br></pre></td></tr></table></figure>
<ul>
<li>以 ./Spark Shell 方式执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; import org.apache.spark._</span><br><span class="line">import org.apache.spark._</span><br><span class="line"></span><br><span class="line">scala&gt; object SparkPi &#123;</span><br><span class="line">     |</span><br><span class="line">     |   def main(args: Array[String]) &#123;</span><br><span class="line">     |</span><br><span class="line">     |     val slices = 2</span><br><span class="line">     |     val n = 100000 * slices</span><br><span class="line">     |</span><br><span class="line">     |     val count = sc.parallelize(1 to n, slices).map &#123; i =&gt;</span><br><span class="line">     |</span><br><span class="line">     |       val x = math.random * 2 - 1</span><br><span class="line">     |       val y = math.random * 2 - 1</span><br><span class="line">     |</span><br><span class="line">     |       if (x * x + y * y &lt; 1) 1 else 0</span><br><span class="line">     |</span><br><span class="line">     |     &#125;.reduce(_ + _)</span><br><span class="line">     |</span><br><span class="line">     |     println(&quot;Pi is rounghly &quot; + 4.0 * count / n)</span><br><span class="line">     |</span><br><span class="line">     |   &#125;</span><br><span class="line">     | &#125;</span><br><span class="line">defined module SparkPi</span><br><span class="line">scala&gt;</span><br><span class="line"></span><br><span class="line">// Spark Shell 已默认将 SparkContext 类初始化为对象 sc, 用户代码可直接使用。</span><br><span class="line">// Spark 自带的交互式的 Shell 程序，方便进行交互式编程。</span><br></pre></td></tr></table></figure>
<ul>
<li><p>通过 Web UI 查看集群状态</p>
<pre><code>http：//masterIp:8080
</code></pre></li>
</ul>
<p><img src="/images/spark/spark-introduce-05.png" width="740" height="400" img=""></p>
<h3 id="4-4-Spark-quick-start"><a href="#4-4-Spark-quick-start" class="headerlink" title="4.4 Spark quick start"></a>4.4 Spark quick start</h3><p>quick-start : <a href="https://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="external">https://spark.apache.org/docs/latest/quick-start.html</a></p>
<p>./bin/spark-shell</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val textFile = sc.textFile(&quot;README.md&quot;)</span><br><span class="line">textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3</span><br><span class="line">RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let’s start with a few actions:</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.count() // Number of items in this RDD</span><br><span class="line">res0: Long = 126</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.first() // First item in this RDD</span><br><span class="line">res1: String = # Apache Spark</span><br></pre></td></tr></table></figure>
<h2 id="5-Spark-生态-BDAS"><a href="#5-Spark-生态-BDAS" class="headerlink" title="5. Spark 生态 BDAS"></a>5. Spark 生态 BDAS</h2><ul>
<li>Spark 框架、架构、计算模型、数据管理策略</li>
<li>Spark BDAS 项目及其子项目进行了简要介绍</li>
<li>Spark 生态系统包含的多个子项目 : SparkSql、Spark Streaming、GraphX、MLlib</li>
</ul>
<p><img src="/images/spark/spark-introduce-01.png" alt="Spark EcoSystem = BDAS = 伯克利数据分析栈"></p>
<ul>
<li>Spark 是 BDAS 核心, 是一 大数据分布式编程框架</li>
</ul>
<h2 id="6-Spark-架构"><a href="#6-Spark-架构" class="headerlink" title="6. Spark 架构"></a>6. Spark 架构</h2><ul>
<li>Spark 的代码结构</li>
<li>Spark 的架构</li>
<li>Spark 运行逻辑</li>
</ul>
<h3 id="6-1-Spark-的代码结构"><a href="#6-1-Spark-的代码结构" class="headerlink" title="6.1 Spark 的代码结构"></a>6.1 Spark 的代码结构</h3><p><img src="/images/spark/spark-introduce-02.jpeg" alt="spark code"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scheduler：文件夹中含有负责整体的Spark应用、任务调度的代码。</span><br><span class="line">broadcast：含有Broadcast（广播变量）的实现代码，API中是Java和Python API的实现。</span><br><span class="line"></span><br><span class="line">deploy：含有Spark部署与启动运行的代码。</span><br><span class="line">common：不是一个文件夹，而是代表Spark通用的类和逻辑实现，有5000行代码。</span><br><span class="line"></span><br><span class="line">metrics：是运行时状态监控逻辑代码，Executor中含有Worker节点负责计算的逻辑代码。</span><br><span class="line">partial：含有近似评估代码。</span><br></pre></td></tr></table></figure>
<h3 id="6-2-Spark-的架构"><a href="#6-2-Spark-的架构" class="headerlink" title="6.2 Spark 的架构"></a>6.2 Spark 的架构</h3><p>Spark架构采用了分布式计算中的Master-Slave模型。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Role</th>
<th style="text-align:center">description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Master</td>
<td style="text-align:center">对应集群中的含有Master进程的节点, 集群的控制器</td>
</tr>
<tr>
<td style="text-align:center">Slave</td>
<td style="text-align:center">集群中含有Worker进程的节点</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Client</td>
<td style="text-align:center">作为用户的客户端负责提交应用</td>
</tr>
<tr>
<td style="text-align:center">Driver</td>
<td style="text-align:center">运行Application的main()函数并创建SparkContext。负责作业的调度，即Task任务的分发</td>
</tr>
<tr>
<td style="text-align:center">Worker</td>
<td style="text-align:center">管理计算节点和创建Executor，启动Executor 或 Driver. 接收主节点命令与进行状态汇报</td>
</tr>
<tr>
<td style="text-align:center">Executor</td>
<td style="text-align:center">Worker node执行任务的组件,负责 Task 的执行,用于启动线程池运行任务</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">ClusterManager</td>
<td style="text-align:center">Standalone 模式中为 Master, 控制整个集群, 监控Worker</td>
</tr>
<tr>
<td style="text-align:center">SparkContext</td>
<td style="text-align:center">整个应用的上下文, 控制App的生命周期</td>
</tr>
<tr>
<td style="text-align:center">RDD</td>
<td style="text-align:center">Spark的基本计算单元，一组RDD可形成执行的 DAG</td>
</tr>
</tbody>
</table>
<p><img src="/images/spark/spark-introduce-03.jpeg" alt="spark"></p>
<table>
<thead>
<tr>
<th style="text-align:center">Num</th>
<th style="text-align:center">Spark App 流程 </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td style="text-align:center">Client 提交应用</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td style="text-align:center">Master 找到一个 Worker 启动 Driver</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td style="text-align:center">Driver 向 Master 或者 资源管理器申请资源，之后将应用转化为 RDD Graph </td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td style="text-align:center">DAGScheduler 将 RDD Graph 转化为 Stage的有向无环图 提交给 TaskScheduler</td>
</tr>
<tr>
<td style="text-align:center">5.</td>
<td style="text-align:center">TaskScheduler 提交 task 给Executor执行</td>
</tr>
<tr>
<td style="text-align:center">6.</td>
<td style="text-align:center">在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行 </td>
</tr>
</tbody>
</table>
<blockquote>
<p>在执行阶段，Driver 会将 Task 和 Task所依赖的file 和 jar 序列化后传递给对应的 Worker机器，同时 Executor对相应数据分区的任务进行处理。</p>
</blockquote>
<h2 id="7-小结"><a href="#7-小结" class="headerlink" title="7. 小结"></a>7. 小结</h2><p>由于 Spark 主要使用 HDFS 充当持久化层，所以完整的使用 Spark 需要预先安装 Hadoop.</p>
<p>Spark 将分布式的内存数据抽象为弹性分布式数据集 (RDD), 并在其上实现了丰富的算子，从而对 RDD 进行计算，最后将 算子序列 转化为 DAG 进行执行和调度。</p>
<blockquote>
<p>Spark的Python API几乎覆盖了所有Scala API所能提供的功能. 但的确有些特性，比如Spark Streaming和个别的API方法，暂不支持。<br><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">具体参见《Spark编程指南》的Python部分</a></p>
</blockquote>
<p>体会了 函数式 编程. 个人认为 scala、python 比较适合写 spark 程序.</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kettle ETL]]></title>
      <url>http://sggo.me/2016/01/22/hadoop/ops-etl-kettle/</url>
      <content type="html"><![CDATA[<p>Kettle 的使用初步介绍</p>
<a id="more"></a>
<ol>
<li>ETL 是数据抽取（Extract）、清洗（Cleaning）、转换（Transform）、装载（Load）的过程。</li>
<li>ETL 是构建 <strong>DW</strong> 的重要一环，用户从数据源抽取出数据，经 数据清洗,按照预定义好的 DW模型，将数据加载到 DW 中去。</li>
<li>ETL 是将业务系统的数据经过抽取、清洗转换之后加载到 DW 的过程，目的是将企业中的分散零乱、标准不统一的数据到一起，为企业的决策提供分析依据。</li>
<li>ETL 是 <strong>BI</strong> 项目中一个重要环节。</li>
</ol>
<p><strong>ETL的设计分三个部分：</strong></p>
<ol>
<li>数据抽取</li>
<li>数据的清洗转换</li>
<li>数据的加载</li>
</ol>
<h2 id="1-Kettle-开源的-ETL-工具"><a href="#1-Kettle-开源的-ETL-工具" class="headerlink" title="1. Kettle 开源的 ETL 工具"></a>1. Kettle 开源的 ETL 工具</h2><h3 id="1-1-Kettle-的介绍"><a href="#1-1-Kettle-的介绍" class="headerlink" title="1-1. Kettle 的介绍"></a>1-1. Kettle 的介绍</h3><p>  ETL（Extract-Transform-Load的缩写，即数据抽取、转换、装载的过程， 我们经常会遇到各种数据的处理，转换，迁移，所以掌握一种 ETL 工具的使用必不可少。</p>
<p>  Kettle 支持图形化的GUI设计界面，然后可以以工作流的形式流转，熟练它可以减少非常多的研发工作量，提高工作效率。</p>
<p>  Kettle 允许你管理来自不同数据库的数据，通过提供一个图形化的用户环境来描述你想做什么。</p>
<p>  Kettle 中有两种脚本文件，transformation 和 job.</p>
<ul>
<li>transformation 完成针对数据的基础转换.</li>
<li>job 则完成整个工作流的控制。</li>
</ul>
<h3 id="1-2-Kettle-家族产品"><a href="#1-2-Kettle-家族产品" class="headerlink" title="1-2. Kettle 家族产品"></a>1-2. Kettle 家族产品</h3><p> <strong> Kettle家族目前包括 4 个产品：Spoon、Pan、CHEF、Kitchen。</strong></p>
<p> Spoon 允许你通过图形界面来设计 ETL 转换过程（Transformation）。</p>
<p> Pan   允许你批量运行由 Spoon 设计的 ETL 转换 (例如使用一个时间调度器)。Pan 是一后台执行的程序，没图界面。</p>
<p> Chef  允许你创建任务（Job）。 任务通过允许每个转换，任务，脚本等等，更有利于自动化更新数据仓库的复杂工作。任务通过允许每个转换，任务，脚本等等。任务将会被检查，看看是否正确地运行了。</p>
<p> Kitchen 允许你批量使用由 Chef 设计的任务 (例如使用一个时间调度器)。Kitchen 也是后台运行的程序。</p>
<h2 id="2-下载和部署安装"><a href="#2-下载和部署安装" class="headerlink" title="2. 下载和部署安装"></a>2. 下载和部署安装</h2><p>Kettle可以在<a href="http://kettle.pentaho.org/" target="_blank" rel="external">http://kettle.pentaho.org/</a> 网站下载<br><br>下载 kettle 压缩包，因 kettle 为绿色软件，解压缩到任意本地路径即可</p>
<p>安装需要 : JDK、JAVA_HOME、CLASSPATH、PENTAHO_JAVA_HOME 等环境变量。</p>
<blockquote>
<p>如需连接mysql，则需将 mysql-connector-java-5.1.38.jar 放入到 lib 中。</p>
</blockquote>
<h3 id="2-1-kettle-windows-安装"><a href="#2-1-kettle-windows-安装" class="headerlink" title="2-1 kettle windows 安装"></a>2-1 kettle windows 安装</h3><ul>
<li>建议在 windows 下使用操作练习 kettle<br> windows 对图形化 支持好 </li>
<li>直接启动 Spoon.bat 即可</li>
</ul>
<h3 id="2-2-kettle-Linux-安装"><a href="#2-2-kettle-Linux-安装" class="headerlink" title="2-2 kettle Linux 安装"></a>2-2 kettle Linux 安装</h3><p> linux 图形化不强，如需要在 linux 中查看一下 kettle 资源库是否连接正常，以及在 linux 上调度 kettle 的 job，就需要在 Linux上 配置 kettle 环境了。</p>
<p>验证 kettle 部署成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd data-integration</span><br><span class="line">输入命令./kitchen.sh。如果出现帮助信息说明部署成功</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如出现错误，请 chmod +x *.sh，再试。</p>
</blockquote>
<h3 id="2-3-kettle-osx-安装"><a href="#2-3-kettle-osx-安装" class="headerlink" title="2-3 kettle osx 安装"></a>2-3 kettle osx 安装</h3><p> 已经存在</p>
<h2 id="3-应用场景"><a href="#3-应用场景" class="headerlink" title="3. 应用场景"></a>3. 应用场景</h2><p>这里简单概括一下几种具体的应用场景，按网络环境划分主要包括：</p>
<h3 id="3-1-表视图模式："><a href="#3-1-表视图模式：" class="headerlink" title="3-1 表视图模式："></a>3-1 表视图模式：</h3><p>  这种情况我们经常遇到，就是在同一网络环境下，我们对各种数据源的表数据进行抽取、过滤、清洗等，例如历史数据同步、异构系统数据交互、数据对称发布或备份等都归属于这个模式；传统的实现方式一般都要进行研发（一小部分例如两个相同表结构的表之间的数据同步，如果sqlserver数据库可以通过发布/订阅实现），涉及到一些复杂的一些业务逻辑如果我们研发出来还容易出各种bug；</p>
<h3 id="3-2-前置机模式"><a href="#3-2-前置机模式" class="headerlink" title="3-2 前置机模式"></a>3-2 前置机模式</h3><p>  数据交换的双方 A 和 B 网络不通，但是 A 和 B 都可以和前置机 C 连接..</p>
<h3 id="3-3-文件模式"><a href="#3-3-文件模式" class="headerlink" title="3-3 文件模式"></a>3-3 文件模式</h3><p>  数据交互的双方 A 和 B 是完全的物理隔离，这样就只能通过以文件的方式来进行数据交互了，例如 XML 格式.</p>
<h2 id="4-DEMO实战"><a href="#4-DEMO实战" class="headerlink" title="4. DEMO实战"></a>4. DEMO实战</h2><h3 id="4-1-简单表同步"><a href="#4-1-简单表同步" class="headerlink" title="4-1 简单表同步"></a>4-1 简单表同步</h3><p>功能描述 : 数据库 TestDB01 中的 UsersA表 到 数据库TestDB02 的UsersB表；<br>实现流程 : 建立一个转换和一个作业Job；</p>
<p><strong>一、建立转换</strong></p>
<ol>
<li><p>进入主界面，新建一个转换，转换的后缀名为 ktr.<br>  创建 DB连接，选择新建 DB连接, Test按钮测试是否配置正确！</p>
<p>  我们需要建立两个 DB连接，分别为 TestDB01 和 TestDB02；</p>
<p>  (如报错可以 : 下载 mysql-connect jar 放入 lib 目录下)</p>
</li>
<li><p>建立步骤和步骤关系 <strong>:</strong> [输入] -&gt; [表输入]<br>  点击核心对象，我们从步骤树中选择【表输入】, 这样拖拽一个 表输入<br>  之后，我们双击表输入之后，我们自己可以随意写一个 sql 语句，这个语句表示<br>  可以在这个库中随意组合，只要 sql 语句没有错误即可，我这里只是最简单的把<br>  TestA 中的所有数据查出来，语句为 select * from usersA。</p>
</li>
<li><p>建立步骤和步骤关系 <strong>:</strong> [输出] -&gt; [插入/更新]<br>  同上类似</p>
</li>
<li><p>建立 连接 关系<br>  然后在【表输入】上同时按住 shift 键和鼠标左键滑向【插入/更新】，这样建立两个步骤之间的连接</p>
</li>
<li><p>运行<br>  建立好转换之后，我们可以直接运行(点击上面的小三角形)这个转换，检查一下是否有错，如图，有错误都会在下面的控制台上输出。</p>
</li>
</ol>
<p><strong>二、建立作业 :</strong></p>
<p>如果我们需要让这个转换定时执行怎么办呢，那么我们需要建立一个作业job</p>
<ol>
<li><p>新建 Job</p>
<p> 文件-&gt;新建-&gt;Job</p>
</li>
<li><p>在 Job 中 添加 转换</p>
<p> 在新建的作业中, 打开刚才新建的 [简单表同步] 的 transformation</p>
</li>
<li><p>添加 START</p>
<p> 通用 -&gt; START</p>
<p> 使 START 关联 -&gt;  [简单表同步] Transformation</p>
</li>
<li><p>这样我们在【Start】步骤上面双击</p>
<p> 设置时间间隔、定时执行 等需要的参数</p>
<p>这样这个作业就制定好了，点击保存之后，就可以在图形化界面上点击开始执行了。</p>
</li>
</ol>
<h2 id="5-win-linux-后台运行"><a href="#5-win-linux-后台运行" class="headerlink" title="5. win/linux 后台运行"></a>5. win/linux 后台运行</h2><h3 id="5-1-win-后台运行"><a href="#5-1-win-后台运行" class="headerlink" title="5-1 win 后台运行"></a>5-1 win 后台运行</h3><p>simpleTableSync.bat</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@echo off </span><br><span class="line"></span><br><span class="line">if &quot;%1&quot; == &quot;h&quot; goto begin </span><br><span class="line"></span><br><span class="line">mshta vbscript:createobject(&quot;wscript.shell&quot;).run(&quot;%~nx0 h&quot;,0)(window.close)&amp;&amp;exit </span><br><span class="line"></span><br><span class="line">:begin</span><br><span class="line">C:</span><br><span class="line">cd C:\WorkSoft\data-integration</span><br><span class="line">kitchen /file:C:\WorkJob\ETL\tSyncTestJob.kjb /level:Basic&gt;&gt;C:\WorkJob\ETL\MyTest.log /level:Basic&gt;&gt;C:\WorkJob\ETL\MyTest.log</span><br></pre></td></tr></table></figure>
<h3 id="5-2-linux-后台运行"><a href="#5-2-linux-后台运行" class="headerlink" title="5-2 linux 后台运行"></a>5-2 linux 后台运行</h3><p>simpleTableSync.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#################################################################</span><br><span class="line">#</span><br><span class="line"># @date:   2016.01.28</span><br><span class="line"># @desc:   simpleTableSync @kettle</span><br><span class="line">#</span><br><span class="line">#################################################################</span><br><span class="line"></span><br><span class="line">cd `dirname $0`/.. &amp;&amp; wk_dir=`pwd` &amp;&amp; cd -</span><br><span class="line">source $&#123;wk_dir&#125;/util/env</span><br><span class="line"></span><br><span class="line">echo_ex &quot;$&#123;data_integration&#125;/kitchen.sh -file=$&#123;data_dir&#125;/tSyncTestJob.kjb&quot;</span><br><span class="line">$&#123;data_integration&#125;/kitchen.sh -file=$&#123;data_dir&#125;/tSyncTestJob.kjb</span><br><span class="line">check_success</span><br><span class="line"></span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure>
<p>注意 : kjb 与 ktr 最好放在一个目录下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hdfs@node196 simpleTableSync]$ cd data/</span><br><span class="line">[hdfs@node196 data]$ ll</span><br><span class="line">total 24</span><br><span class="line">-rw-rw-r--. 1 hdfs hdfs  6944 Jan 29 18:22 tSyncTestJob.kjb</span><br><span class="line">-rw-rw-r--. 1 hdfs hdfs 13450 Jan 29 18:22 tSyncTestTrans.ktr</span><br></pre></td></tr></table></figure>
<blockquote>
<p>从 win 拷贝过来的文件，fileformat 可能是 dos 格式，可以 :set ff=unix.</p>
</blockquote>
<h2 id="Reference-article"><a href="#Reference-article" class="headerlink" title="Reference article"></a>Reference article</h2><p><a href="http://www.cnblogs.com/limengqiang/archive/2013/01/16/KettleApply2.html#sz" target="_blank" rel="external">kettle系列</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Logback 入门初步]]></title>
      <url>http://sggo.me/2015/12/27/java/java-logback-indoor/</url>
      <content type="html"><![CDATA[<p>Logback 一个开源日志组件, SLF4J 这个简单的日志前端接口（Façade）来替代 Jakarta Commons-Logging 。</p>
<a id="more"></a>
<p>Logback 一个开源日志组件。<br>Logback 当前分成三个模块：logback-core  logback- classic  和  logback-access。</p>
<h2 id="1-logback-简介"><a href="#1-logback-简介" class="headerlink" title="1. logback 简介"></a>1. logback 简介</h2><p>Ceki在Java日志领域世界知名。他创造了Log4J ，这个最早的Java日志框架即便在JRE内置日志功能的竞争下仍然非常流行。随后他又着手实现SLF4J 这个“简单的日志前端接口（Façade）”来替代Jakarta Commons-Logging 。</p>
<p>Logback，一个“可靠、通用、快速而又灵活的Java日志框架”。</p>
<p><strong>官网网址 :</strong> <a href="http://logback.qos.ch/" target="_blank" rel="external">http://logback.qos.ch/</a></p>
<h2 id="2-工程使用需要的-jar"><a href="#2-工程使用需要的-jar" class="headerlink" title="2. 工程使用需要的 jar"></a>2. 工程使用需要的 jar</h2><p>要在工程里面使用 logback , 只需要以下jar文件：</p>
<pre><code>(1). slf4j-api.jar       
(2). logback-access.jar
(3). logback-classic.jar
(4). logback-core.jar

logback-core    是其它两个模块的基础模块。   
logback-classic 是 log4j 的一个 改良版本。   
logback-access  与Servlet容器集成提供通过Http来访问日志功能
</code></pre><p>logback-classic 完整实现 SLF4J API 使你可以很方便地更换成其它日志系统如 log4j 或 JDK Logging。</p>
<h2 id="3-logback-常用配置详解"><a href="#3-logback-常用配置详解" class="headerlink" title="3. logback 常用配置详解"></a>3. logback 常用配置详解</h2><h3 id="3-1-根节点-lt-configuration-gt"><a href="#3-1-根节点-lt-configuration-gt" class="headerlink" title="3.1 根节点&lt; configuration &gt;"></a>3.1 根节点&lt; configuration &gt;</h3><table>
<thead>
<tr>
<th>configuration</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>scan</td>
<td>当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。</td>
</tr>
<tr>
<td>scanPeriod</td>
<td>设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。</td>
</tr>
<tr>
<td>debug</td>
<td>当此属性设置为true时，将打印出 logback 内部日志信息，实时查看logback运行状态。默认值为false。</td>
</tr>
</tbody>
</table>
<h2 id="4-logback-配置示例"><a href="#4-logback-配置示例" class="headerlink" title="4. logback 配置示例"></a>4. logback 配置示例</h2><h3 id="4-1-Myself-resources-logback-xml-example"><a href="#4-1-Myself-resources-logback-xml-example" class="headerlink" title="4.1 Myself resources/logback.xml example"></a>4.1 Myself resources/logback.xml example</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"3600 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"AppName"</span> <span class="attr">value</span>=<span class="string">"your_app_name"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LogParentDir"</span> <span class="attr">value</span>=<span class="string">"/home/www/logs/"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$&#123;AppName&#125;<span class="tag">&lt;/<span class="name">contextName</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"infoAppender"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LogParentDir&#125;/$&#123;AppName&#125;/infoLogFile.%d&#123;yyyy-MM-dd&#125;.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"errorAppender"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LogParentDir&#125;/$&#123;AppName&#125;/errorLogFile.%d&#123;yyyy-MM-dd&#125;.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--其中appender的配置表示打印到控制台--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.x.dmt"</span> <span class="attr">level</span>=<span class="string">"ERROR"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"errorAppender"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--设置addtivity为false，将此loger的打印信息不向上级传递；--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.x.dmt.service"</span> <span class="attr">level</span>=<span class="string">"INFO"</span> <span class="attr">additivity</span>=<span class="string">"fasle"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"infoAppender"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 注意: logger 同名情况, 级别低的,需要放在下面,否则级别高的会覆盖级别低的权限,早晨级别低的打印不出来日志 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<hr>
<p><a href="http://aub.iteye.com/blog/1101260" target="_blank" rel="external">更多参见 iteye1101260</a></p>
<p><a href="http://logback.qos.ch/" target="_blank" rel="external">官方网址</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Work 常用的命令积累]]></title>
      <url>http://sggo.me/2015/05/29/ops/work-used-commands/</url>
      <content type="html"><![CDATA[<p>2015年工作时候的常用的命令积累</p>
<a id="more"></a>
<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><ol>
<li>找出 =与&amp; 之间的字符串 并输出</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk &apos;&#123;print $3&#125;&apos;</span><br><span class="line">     (1) awk -F&quot;[=|&amp;]&quot; &apos;&#123;print $2&#125;&apos; file1 &gt; file2</span><br><span class="line">     (2) awk -F&quot;[:|\n]&quot; &apos;&#123;print $2&#125;&apos; file1 &gt; file2</span><br><span class="line">         awk -F&quot;[\^|\^]&quot; &apos;&#123;print $2&#125;&apos; file1 &gt; file2</span><br><span class="line">     (3) sed &apos;/\^/d&apos; 试试</span><br><span class="line"></span><br><span class="line">awk -F&quot;[crowd_list&quot;:|&quot;attachment&quot;]&quot; &apos;&#123;print $2&#125;&apos; file1 &gt; file2</span><br></pre></td></tr></table></figure>
<h2 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h2><p>1). shell 中 cut 命令的用法</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> root:x:0:0:root:/root:/bin/bash | cut <span class="_">-d</span> : <span class="_">-f</span> 6-7</span><br></pre></td></tr></table></figure>
<p>2). 如何查看一个目录占用的空间</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">du -sh *</span><br></pre></td></tr></table></figure>
<p>3). 后台运行脚本的命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nohup sh create_redis.sh &gt; create_redis.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>4). 查找目录下的所有文件中是否含有某个字符串 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find .|xargs grep -ri <span class="string">"IBM"</span></span><br></pre></td></tr></table></figure>
<p>5). 查找目录下的所有文件中是否含有某个字符串,并且只打印出文件名 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find .|xargs grep -ri <span class="string">"IBM"</span> <span class="_">-l</span></span><br></pre></td></tr></table></figure>
<p>6). 让一个变量具有双引号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&quot;$&#123;plat&#125;:$version_init&quot;&apos;</span><br></pre></td></tr></table></figure>
<p>7). 批量替换字符串</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">"s/oldString/newString/g"</span>  `grep oldString -rl ./`</span><br></pre></td></tr></table></figure>
<p>8). VIM tab ^I 替换</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:set listchars=tab:\ \ ,eol:$</span><br></pre></td></tr></table></figure>
<h2 id="git"><a href="#git" class="headerlink" title="git"></a>git</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> http://****/dm/dmp_engine.git （注意这个地址是 HTTP 不是 SSH）    </span><br><span class="line"><span class="built_in">cd</span> dmp_engine</span><br><span class="line">git branch -r</span><br><span class="line">git checkout 分支名，这是重新 down 分支的步骤</span><br><span class="line"></span><br><span class="line">git add filename</span><br><span class="line">git commit -m “a<span class="string">"</span><br><span class="line">git push origin 分支名 或者</span><br><span class="line">git push -u origin origin/dev_dmp_online_serving</span><br><span class="line">git checkout branch_name 切换到这个分支之下</span><br><span class="line"></span><br><span class="line">git checkout -b online_redis3.0 master (从 master 拉下 新分支, 新分支名叫 online_redis3.0)</span></span><br></pre></td></tr></table></figure>
<h2 id="linux-os-info"><a href="#linux-os-info" class="headerlink" title="linux os_info"></a>linux os_info</h2><p>查看 内存 与 CPU 信息</p>
<p>1). 查看内存</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/meminfo</span><br></pre></td></tr></table></figure>
<p>2). 查看物理CPU的个数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#cat /proc/cpuinfo | grep "physical id" | sort | uniq | wc -l</span></span><br></pre></td></tr></table></figure>
<p>3). 查看逻辑CPU的个数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#cat /proc/cpuinfo | grep "processor" | wc -l</span></span><br></pre></td></tr></table></figure>
<p>4). 查看CPU是几核</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#cat /proc/cpuinfo | grep "cores" | uniq</span></span><br></pre></td></tr></table></figure>
<p>5). 查看CPU的主频</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#cat /proc/cpuinfo | grep MHz | uniq</span></span><br></pre></td></tr></table></figure>
<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive <span class="_">-e</span> <span class="string">"select attributes['lt_its'],attributes['ad_clicks'] from new_algo_user_attributes where dt='20150419' and platform='pc' and attributes['lt_its']&lt;&gt;'NULL' limit 10"</span></span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hive 中 udf、udaf 和 udtf 的使用]]></title>
      <url>http://sggo.me/2015/02/01/hadoop/hadoop-hive-udf-udaf/</url>
      <content type="html"><![CDATA[<p>Hive 是基于 Hadoop 中的 MapReduce，提供 HQL 查询的数据仓库. </p>
<p>Hive 是一个很开放的系统，很多内容都支持用户定制. 如 : 文件格式、MR脚本、自定义函数、自定义聚合函数 等.</p>
<a id="more"></a>
<h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h2><p>编写 UDF函数 的时候需要注意一下几点：</p>
<ol>
<li>自定义 UDF 需要继承 org.apache.hadoop.hive.ql.UDF</li>
<li>需要实现 <code>evaluate</code> 函数</li>
</ol>
<p>以下是两个数求和函数的UDF。evaluate函数代表两个整型数据相加</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> hive.connect;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Add</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">evaluate</span><span class="params">(Integer a, Integer b)</span> </span>&#123;  </span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == a || <span class="keyword">null</span> == b) &#123;  </span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;  </span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">return</span> a + b;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><p>函数类需要继承 <strong>UDAF</strong> 类，内部类 <strong>Evaluator</strong> 需要实现 <strong>UDAFEvaluator</strong> 接口.</p>
<p>Evaluator 需要实现 init、iterate、terminatePartial、merge、terminate 这几个函数.</p>
<ol>
<li><code>init</code>函数实现接口 UDAFEvaluator 的 init 函数.</li>
<li><code>iterate</code>接收传入的参数，并进行内部的轮转。其返回类型为 boolean.</li>
<li><code>terminatePartial</code>无参数，其为 iterate 函数轮转结束后，返回轮转数据.</li>
<li><code>merge</code> 接收 terminatePartial 的返回结果，进行数据 merge 操作，其返回类型为boolean.</li>
<li><code>terminate</code> 返回最终的聚集函数结果.</li>
</ol>
<p><a href="https://github.com/blair101/bigdata/tree/master/hadoop/hive_udf_udaf" target="_blank" rel="external">下面是一个简单的 UDAF 的 demo</a></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.x.user_bhv;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.Maps;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDAF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDAFEvaluator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UDAFMergeIntToIntMap</span> <span class="keyword">extends</span> <span class="title">UDAF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PartialResult</span> </span>&#123;</span><br><span class="line">        Map&lt;Integer, Integer&gt; attributes;</span><br><span class="line"></span><br><span class="line">        PartialResult() &#123;</span><br><span class="line">            attributes = Maps.newHashMap();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UnitIdUDAFEvaluator</span> <span class="keyword">implements</span> <span class="title">UDAFEvaluator</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> PartialResult partialResult;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">UnitIdUDAFEvaluator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>();</span><br><span class="line">            init();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"map init"</span>);</span><br><span class="line">            partialResult = <span class="keyword">new</span> PartialResult();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">iterate</span><span class="params">(Map&lt;Integer, Integer&gt; attributes_args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (attributes_args == <span class="keyword">null</span> || attributes_args.isEmpty()) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Map.Entry&lt;Integer, Integer&gt; entry : attributes_args.entrySet()) &#123;</span><br><span class="line">                <span class="keyword">this</span>.partialResult.attributes.put(entry.getKey(), entry.getValue());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> PartialResult <span class="title">terminatePartial</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.partialResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">merge</span><span class="params">(PartialResult other)</span> </span>&#123; <span class="comment">// 参数不可能为 null</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Map.Entry&lt;Integer, Integer&gt; entry : other.attributes.entrySet()) &#123;</span><br><span class="line">                <span class="keyword">this</span>.partialResult.attributes.put(entry.getKey(), entry.getValue());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Map&lt;Integer, Integer&gt; <span class="title">terminate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (partialResult == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> HashMap&lt;Integer, Integer&gt;();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">this</span>.partialResult.attributes;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 Hive 脚本中的使用示例 :</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hql=<span class="string">"ADD jar <span class="variable">$&#123;jar_dir&#125;</span>/user_bhv_for_hive.jar;</span><br><span class="line">    CREATE TEMPORARY FUNCTION merge_int_to_int_map AS 'com.x.user_bhv.UDAFMergeIntToIntMap';</span><br><span class="line">    INSERT OVERWRITE TABLE <span class="variable">$&#123;table_user_buy_category&#125;</span></span><br><span class="line">    SELECT</span><br><span class="line">        mobile_number,</span><br><span class="line">        merge_int_to_int_map (level1_id_count_map)</span><br><span class="line">    FROM </span><br><span class="line">        ods_dm_e_coupon</span><br><span class="line">    GROUP BY mobile_number</span></span><br></pre></td></tr></table></figure>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ol>
<li>重载 evaluate 函数.</li>
<li>UDF 函数中参数类型可以为Writable，也可为java中的基本数据对象.</li>
<li>UDF 支持变长的参数.</li>
<li>Hive 支持隐式类型转换.</li>
<li>客户端退出时，创建的临时函数自动销毁.</li>
<li>evaluate函数必须要返回类型值，空的话返回null，不能为void类型.</li>
<li>UDF 和 UDAF 都可以重载.</li>
<li>查看函数 SHOW FUNCTIONS.</li>
</ol>
<blockquote>
<p>UDAF: User Defined Aggregation Function</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://blog.csdn.net/liuj2511981/article/details/8523084" target="_blank" rel="external">Hive 中 UDF、UDAF 和 UDTF 使用</a></li>
<li><a href="https://github.com/blair101/bigdata/tree/master/hadoop/hive_udf_udaf" target="_blank" rel="external">bliar’s github hive udaf demo</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MapReduce for Python]]></title>
      <url>http://sggo.me/2015/01/30/hadoop/hadoop-mr-for-python/</url>
      <content type="html"><![CDATA[<p>我们可以用 hadoop-streaming 的方式，通过 python 等其他语言来编写 MR 程序.</p>
<a id="more"></a>
<h2 id="Map阶段：mapper-py"><a href="#Map阶段：mapper-py" class="headerlink" title="Map阶段：mapper.py"></a>Map阶段：mapper.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    line = line.strip()</span><br><span class="line">    words = line.split()</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        print(<span class="string">"%s"</span> % word)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 这里仅仅是一个例子，只输出了第一列</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>为了是脚本可执行，增加mapper.py的可执行权限</p>
</blockquote>
<p>当然，<code>Map</code>阶段， 你也可以不作处理原样输出: 只写一个 <code>cat</code></p>
<h2 id="Reduce阶段：reducer-py"><a href="#Reduce阶段：reducer-py" class="headerlink" title="Reduce阶段：reducer.py"></a>Reduce阶段：reducer.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># Copyright 2013 x Inc. All Rights Reserved</span></span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'Blair Chan'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> constant</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> EsHelper <span class="keyword">import</span> EsHelper</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_user_basic_consume_info</span><span class="params">(items,  esHelper)</span>:</span></span><br><span class="line"></span><br><span class="line">    basic_consume_info_doc = get_user_basic_consume_info_doc(items)</span><br><span class="line">    <span class="keyword">if</span> basic_consume_info_doc <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">    _id = basic_consume_info_doc[<span class="string">'mobile_number'</span>]</span><br><span class="line">    basic_consume_info_index = <span class="string">"basic_consume_info_index"</span></span><br><span class="line"></span><br><span class="line">    esHelper.index(index=basic_consume_info_index, doc_type=basic_consume_info_index, id=_id, data=basic_consume_info_doc)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user_basic_consume_info_doc</span><span class="params">(items)</span>:</span></span><br><span class="line">    doc = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        doc = &#123;</span><br><span class="line">            <span class="string">"mobile_number"</span>: items[<span class="number">0</span>],</span><br><span class="line">            <span class="string">"first_consume_time"</span>: items[<span class="number">1</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">except</span> BaseException <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">"Exist Exception : %s About get_user_basic_consume_info_doc, mobile_number: %s"</span> % (str(e), mobile_number))</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> doc</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    esHelper = EsHelper(constant.ES_URL)</span><br><span class="line">    success_sum = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line"></span><br><span class="line">        line = line.strip()</span><br><span class="line">        items = line.split(<span class="string">'\001'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(items) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        insert_user_basic_consume_info(items, esHelper)</span><br><span class="line">        success_sum = success_sum + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Success:%d"</span> % success_sum)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat data.txt | python mapper.py | sort | reducer.py</span><br></pre></td></tr></table></figure>
<h2 id="提交Hadoop"><a href="#提交Hadoop" class="headerlink" title="提交Hadoop"></a>提交Hadoop</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">cd</span> `dirname <span class="variable">$0</span>`/.. &amp;&amp; wk_dir=`<span class="built_in">pwd</span>` &amp;&amp; <span class="built_in">cd</span> -</span><br><span class="line"><span class="built_in">source</span> <span class="variable">$&#123;wk_dir&#125;</span>/util/env</span><br><span class="line"></span><br><span class="line">input_file=<span class="string">"<span class="variable">$&#123;OSS_URL&#125;</span>/<span class="variable">$&#123;mds_hive_dir&#125;</span>/<span class="variable">$&#123;table_user_basic_consume_info&#125;</span>/*"</span></span><br><span class="line">output_file=<span class="string">"<span class="variable">$&#123;OSS_URL&#125;</span>/<span class="variable">$&#123;tmp_hive_dir&#125;</span>/<span class="variable">$&#123;table_user_basic_consume_info&#125;</span>/dt=<span class="variable">$&#123;d1&#125;</span>"</span></span><br><span class="line">reducer=<span class="string">"reducer.py"</span></span><br><span class="line">reducer_depend1=<span class="string">"constant.py"</span></span><br><span class="line">reducer_depend2=<span class="string">"EsHelper.py"</span></span><br><span class="line">archive=<span class="string">"<span class="variable">$&#123;OSS_URL&#125;</span>/share/packages/elasticsearch-5.0.0.tar.gz#elasticsearch-5.0.0"</span> </span><br><span class="line"><span class="comment">## archive 表示的依赖包需要上传到 hdfs 上，#后面表示的是解压后的目录名</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$&#123;HADOOP&#125;</span> fs -rmr <span class="variable">$&#123;output_file&#125;</span></span><br><span class="line"></span><br><span class="line">cmd=<span class="string">"<span class="variable">$&#123;HADOOP&#125;</span> jar <span class="variable">$&#123;hadoop_streaming_jar&#125;</span></span><br><span class="line">     -D mapred.map.tasks=100</span><br><span class="line">     -D mapred.reduce.tasks=100</span><br><span class="line">     -D stream.map.input.ignoreKey=true</span><br><span class="line">     -input <span class="variable">$&#123;input_file&#125;</span></span><br><span class="line">     -output <span class="variable">$&#123;output_file&#125;</span></span><br><span class="line">     -file <span class="variable">$&#123;reducer&#125;</span></span><br><span class="line">     -file <span class="variable">$&#123;reducer_depend1&#125;</span></span><br><span class="line">     -file <span class="variable">$&#123;reducer_depend2&#125;</span></span><br><span class="line">     -mapper cat</span><br><span class="line">     -reducer <span class="variable">$&#123;reducer&#125;</span></span><br><span class="line">     -cacheArchive <span class="variable">$&#123;archive&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span>_ex <span class="string">"<span class="variable">$cmd</span>"</span></span><br><span class="line"><span class="variable">$cmd</span></span><br><span class="line">check_success</span><br></pre></td></tr></table></figure>
<blockquote>
<p>hadoop_streaming_jar=”/home/data_mining/share/packages/hadoop2/hadoop-streaming-2.7.2.jar”</p>
<p>以上仅仅是一个例子，虽然插入 ES 出现异常，但本篇仅仅说明如何用 python 写 mapreduce 程序</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.cnblogs.com/kaituorensheng/p/3826114.html" target="_blank" rel="external">用python写MapReduce函数——以WordCount为例</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux, profile / bashrc Brief Introduce]]></title>
      <url>http://sggo.me/2014/05/18/ops/ops-linux-bashrc-profile/</url>
      <content type="html"><![CDATA[<p>/etc/profile、/etc/bashrc、~/.bash_profile、~/.bashrc </p>
<a id="more"></a>
<table>
<thead>
<tr>
<th style="text-align:center">config file</th>
<th style="text-align:center">desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">/etc/profile，/etc/bashrc</td>
<td style="text-align:center">系统全局环境变量设定</td>
</tr>
<tr>
<td style="text-align:center">~/.profile，~/.bashrc</td>
<td style="text-align:center">用户家目录下的私有环境变量设定 </td>
</tr>
</tbody>
</table>
<h2 id="1-login-env-steps"><a href="#1-login-env-steps" class="headerlink" title="1. login env steps"></a>1. login env steps</h2><blockquote>
<p>以下是 登入系统,环境设定档 流程</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">Read step</th>
<th style="text-align:center">desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">/etc/profile</td>
<td style="text-align:center">/etc/profile.d 和 /etc/inputrc 。 从/etc/profile.d目录的配置文件搜集shell的设置</td>
</tr>
<tr>
<td style="text-align:center">~/.bash_profile</td>
<td style="text-align:center">~/.bash_profile，如无则读取 ~/.bash_login，如无则读取 ~/.profile</td>
</tr>
<tr>
<td style="text-align:center">~/.bashrc</td>
<td style="text-align:center">~/.bashrc (交互式 non-login 方式进入 bash 运行的)</td>
</tr>
</tbody>
</table>
<h2 id="2-profile-与-bashrc"><a href="#2-profile-与-bashrc" class="headerlink" title="2. .profile 与 .bashrc"></a>2. .profile 与 .bashrc</h2><p>~/.profile 与 ~/.bashrc </p>
<h3 id="2-1-相同点"><a href="#2-1-相同点" class="headerlink" title="2.1 相同点"></a>2.1 相同点</h3><p>都具有个性化定制功能</p>
<blockquote>
<p>~/.profile 可以设定本用户专有的路径，环境变量，等，它只能登入的时候执行一次<br>~/.bashrc 也是某用户专有设定文档，可以设定路径，命令别名，每次shell script的执行都会使用它一次 </p>
</blockquote>
<h3 id="2-2-bashrc-和-profile-的区别"><a href="#2-2-bashrc-和-profile-的区别" class="headerlink" title="2.2 bashrc 和 profile 的区别"></a>2.2 bashrc 和 profile 的区别</h3><p><strong>交互式模式</strong></p>
<blockquote>
<p>shell等待你的输入，并且执行你提交的命令。 shell与用户进行交互 登录、执行命令、签退、shell终止 </p>
<ul>
<li>~/.bash_profile 是交互式、login 方式进入 bash 运行的 </li>
<li>~/.bashrc 是交互式 non-login 方式进入 bash 运行的 </li>
</ul>
</blockquote>
<p><strong>非交互式模式</strong></p>
<blockquote>
<p>shell不与你进行交互，是读取存在文件中的命令,并且执行它们。当它读到文件的结尾，shell终止</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://blog.chinaunix.net/uid-26435987-id-3400127.html" target="_blank" rel="external">blog.chinaunix.net/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Qunar Trainee Summary]]></title>
      <url>http://sggo.me/2014/05/07/ops/my-dev-trainee/</url>
      <content type="html"><![CDATA[<p>an trainee in qunar</p>
<a id="more"></a>
<p>  时间过得真快，转眼间，我在去哪儿网实习的这段时间马上就要过去了，在这段实习的日子真的学会了很多，经过这次培训，我学会了java，了解了guava，学习了SVN， git版本管理工具， Maven项目管理及自动构建工具， SpringMVC的开发，mybatis 的使用，Servlet， jsp， html， http 的初步了解，动态代理与AOP， 单元测试与自动化测试，数据库建表规范， Linux系统的使用等等等等，熟悉了公司内部的开发框架，了解公司开发规范，适应公司文化，环境。</p>
<p>  在这些天实习的日子，最初本想不走 java 路线的，觉得封装好严重，你根本不知道那个包是怎么回事，就可以拿过来一用，就出来了一个看起来比较高大上的作品，学很短的时间就可以搞出个东西来，觉得没有C、C++或者是算法，来得那么爽。但是经过培训后，我才发现了java的可爱之处与强大，以前对java有误解，完全是自己的无知与不懂，遇到了酒店事业部的 思雨大神，更是感受到了学习java这方面的知识，可以很牛很牛，学任何一门技术都具有更为广阔的空间，实习初期，由于我对java知识之前是没有任何经验的，所以面对严格快速正规的培训，真的感到压力很大，幸好小组的分享还有做作业遇到问题，组内外同学的帮助，才可以跟下来，由于初学，节奏很快，那么多知识，在不到2个月的时间内全部学熟是很难做到的，我现在对 SpringMVC 还有一些 Guava，web的知识，还不是很熟练，以后一定要找时间在自己补习下这方面的知识。并把自己在公司学到的一些知识，介绍给学校的一些还在迷茫懵懂的学妹，学弟们。在公司两个多月的实习生活，对我来说不仅仅是技术和能力上的提升，更多的还有思想和态度的转变。</p>
<p>  最后，首先，要感谢公司给我们应届生提供的这个培训平台，可以让我学到这么多的专业知识，认识到更多的志同道合的朋友；感谢丫丫姐，辛苦操劳的尽可能给我们提供欢愉的学习环境，以及邀请很多的优秀讲师；感谢全体给我们讲课的讲师们，感谢我的团队以及周边的同事，在工作学习的繁忙之余，还可以带来各种各样的欢声笑语，感谢老何与david，让我学到了严谨，负责，认真的工作态度与生活态度。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[校招面试的代码部分 (剑指offer & 笔试面试)]]></title>
      <url>http://sggo.me/2013/09/05/acm/interview_coder_section/</url>
      <content type="html"><![CDATA[<p>校招面试时，需要准备的 50 道常考题目的写代码部分。</p>
<a id="more"></a>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;list&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;deque&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">struct</span> Node &#123;</span><br><span class="line">    Node* lchild;</span><br><span class="line">    Node* rchild;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="1-八皇后的实现"><a href="#1-八皇后的实现" class="headerlink" title="1. 八皇后的实现"></a>1. 八皇后的实现</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">105</span>;</span><br><span class="line"><span class="keyword">int</span> pos[N];</span><br><span class="line"><span class="keyword">int</span> num = <span class="number">8</span>, cnt = <span class="number">0</span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ok</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(pos[i] == pos[n]) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">abs</span>(pos[n] - pos[i]) == n-i) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n == num) &#123;</span><br><span class="line">        cnt++;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num; i++) &#123;</span><br><span class="line">        pos[n] = i;</span><br><span class="line">        <span class="keyword">if</span>(ok(n)) &#123;</span><br><span class="line">            dfs(n+<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-Fire-Net"><a href="#2-Fire-Net" class="headerlink" title="2. Fire Net"></a>2. Fire Net</h2><p>dfs+回溯 如果题目数据大一点还要用到二分匹配。我好喜欢的一道题目！</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">5</span>; <span class="comment">// hdoj 1045</span></span><br><span class="line"><span class="keyword">char</span> <span class="built_in">map</span>[N][N];</span><br><span class="line"><span class="keyword">int</span> n, mmax = <span class="number">-1</span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ok</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">map</span>[x][y] != <span class="string">'.'</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = y<span class="number">-1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>[x][i] == <span class="string">'X'</span>) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>[x][i] == <span class="string">'B'</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = x<span class="number">-1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>[i][y] == <span class="string">'X'</span>) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>[i][y] == <span class="string">'B'</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(pos == n*n) &#123;</span><br><span class="line">        <span class="keyword">if</span>(num &gt; mmax) mmax = num;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> x = pos / n;</span><br><span class="line">    <span class="keyword">int</span> y = pos % n;</span><br><span class="line">    <span class="keyword">if</span>(ok(x, y)) &#123;</span><br><span class="line">        <span class="built_in">map</span>[x][y] = <span class="string">'B'</span>;</span><br><span class="line">        dfs(pos+<span class="number">1</span>, num+<span class="number">1</span>);</span><br><span class="line">        <span class="built_in">map</span>[x][y] = <span class="string">'.'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    dfs(pos+<span class="number">1</span>, num);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>二分匹配</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> M = <span class="number">30</span>;</span><br><span class="line"><span class="keyword">char</span> str[N][N];</span><br><span class="line"><span class="keyword">int</span> p[N][N];</span><br><span class="line"><span class="keyword">int</span> rp[N][N];</span><br><span class="line"><span class="keyword">int</span> mmap[M][M];</span><br><span class="line"><span class="keyword">int</span> pre[M];</span><br><span class="line"><span class="keyword">bool</span> vis[M];</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">int</span> x_id, y_id;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> u)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= y_id; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(mmap[u][i] &amp;&amp; !vis[i]) &#123;</span><br><span class="line">            vis[i] = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span>(pre[i] == <span class="number">0</span> || dfs(pre[i])) &#123;</span><br><span class="line">                pre[i] = u;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxMatch</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(pre, <span class="number">0</span>, <span class="keyword">sizeof</span>(pre));</span><br><span class="line">    <span class="keyword">int</span> num = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= x_id; i++) &#123;</span><br><span class="line">        <span class="built_in">memset</span>(vis, <span class="number">0</span>, <span class="keyword">sizeof</span>(vis));</span><br><span class="line">        <span class="keyword">if</span>(dfs(i)) num++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> num;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; n, n) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%s"</span>, str[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(p, <span class="number">0</span>, <span class="keyword">sizeof</span>(p));</span><br><span class="line">        <span class="built_in">memset</span>(rp, <span class="number">0</span>, <span class="keyword">sizeof</span>(rp));</span><br><span class="line">        x_id = <span class="number">0</span>, y_id = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span>(k &lt; n) &#123;</span><br><span class="line">                <span class="keyword">if</span>(str[i][k] == <span class="string">'.'</span>) &#123;</span><br><span class="line">                    ++x_id;</span><br><span class="line">                    <span class="keyword">while</span>(k &lt; n &amp;&amp; str[i][k] != <span class="string">'X'</span>) &#123;</span><br><span class="line">                        p[i][k] = x_id;</span><br><span class="line">                        k++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> k++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span>(k &lt; n) &#123;</span><br><span class="line">                <span class="keyword">if</span>(str[k][i] == <span class="string">'.'</span>) &#123;</span><br><span class="line">                    ++y_id;</span><br><span class="line">                    <span class="keyword">while</span>(k &lt; n &amp;&amp; str[k][i] != <span class="string">'X'</span>) &#123;</span><br><span class="line">                        rp[k][i] = y_id;</span><br><span class="line">                        k++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> k++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(mmap, <span class="number">0</span>, <span class="keyword">sizeof</span>(mmap));</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span>(str[i][j] == <span class="string">'.'</span>) &#123;</span><br><span class="line">                    mmap[p[i][j]][rp[i][j]] = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, maxMatch());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-根据前序中序重新建立二叉树"><a href="#3-根据前序中序重新建立二叉树" class="headerlink" title="3. 根据前序中序重新建立二叉树"></a>3. 根据前序中序重新建立二叉树</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Node* <span class="title">f3</span><span class="params">(<span class="keyword">int</span>* pre, <span class="keyword">int</span>* ino, <span class="keyword">int</span> len)</span> </span>&#123; </span><br><span class="line">    <span class="comment">// pre : 1, 2, 4, 7, 3, 5, 6, 8    </span></span><br><span class="line">    <span class="comment">// ino : 4, 7, 2, 1, 5, 3, 8, 6</span></span><br><span class="line">    <span class="keyword">if</span>(pre == <span class="literal">NULL</span> || ino == <span class="literal">NULL</span> || len &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> r_v = pre[<span class="number">0</span>];</span><br><span class="line">    Node* root = <span class="keyword">new</span> Node();</span><br><span class="line">    root-&gt;value = r_v;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; ; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(ino[i] == r_v) <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    root-&gt;lchild = f3(pre+<span class="number">1</span>, ino, i);</span><br><span class="line">    root-&gt;rchild = f3(pre+i+<span class="number">1</span>, ino+i+<span class="number">1</span>, len<span class="number">-1</span>-i);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4-二叉树中和为某一值的所有路径"><a href="#4-二叉树中和为某一值的所有路径" class="headerlink" title="4. 二叉树中和为某一值的所有路径"></a>4. 二叉树中和为某一值的所有路径</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f4</span><span class="params">(Node*, <span class="keyword">int</span>, <span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f4</span><span class="params">(Node* root, <span class="keyword">int</span> exSum)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; V;</span><br><span class="line">    <span class="keyword">int</span> curSum = <span class="number">0</span>;</span><br><span class="line">    f4(root, exSum, curSum, V);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f4</span><span class="params">(Node* root, <span class="keyword">int</span> exSum, <span class="keyword">int</span> curSum, vecotr&lt;<span class="keyword">int</span>&gt;&amp; path)</span> </span>&#123;</span><br><span class="line">    curSum += root-&gt;value;</span><br><span class="line">    path.push_back(root-&gt;value);</span><br><span class="line">    <span class="keyword">if</span>(curSum == exSum &amp;&amp; root-&gt;lchild == <span class="literal">NULL</span> &amp;&amp; root-&gt;rchild == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="comment">//; 打印vector中的路径</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(root-&gt;lchild) &#123;</span><br><span class="line">        f4(root-&gt;lchild, exSum, curSum, path);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(root-&gt;rchild) &#123;</span><br><span class="line">        f4(root-&gt;rchild, exSum, curSum, path);</span><br><span class="line">    &#125;</span><br><span class="line">    curSum -= root-&gt;value;</span><br><span class="line">    path.pop_back();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-树2-是否是-树1-的子结构"><a href="#5-树2-是否是-树1-的子结构" class="headerlink" title="5. 树2 是否是 树1 的子结构"></a>5. 树2 是否是 树1 的子结构</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">son</span><span class="params">(Node* p1, Node* p2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(p2 == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p1 == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p1-&gt;value == p2-&gt;value) &#123;</span><br><span class="line">        <span class="keyword">return</span> son(p1-&gt;lchild, p2-&gt;lchild) &amp;&amp; son(p1-&gt;rchild, p2-&gt;rchild);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">f5</span><span class="params">(Node* root1, Node* root2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root2 == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(root1 == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(root1-&gt;value == root2-&gt;value) &#123;</span><br><span class="line">        <span class="keyword">return</span> son(root1, root2);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line">    flag = f5(root-&gt;lchild, root2);</span><br><span class="line">    <span class="keyword">if</span>(!flag) &#123;</span><br><span class="line">        flag = f5(root-&gt;rchild, root2);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> flag;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="6-二叉搜索树后序遍历的结果"><a href="#6-二叉搜索树后序遍历的结果" class="headerlink" title="6. 二叉搜索树后序遍历的结果"></a>6. 二叉搜索树后序遍历的结果</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">f6</span><span class="params">(<span class="keyword">int</span>* sec, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(sec == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(len &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> i, rv = sec[len<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len<span class="number">-1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(sec[i] &gt; rv) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = i; j &lt; len<span class="number">-1</span>; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(sec[j] &lt; rv) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> f6(sec, i) &amp;&amp; f6(sec+i, len-i<span class="number">-1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-二叉树两节点的最低公共祖先"><a href="#7-二叉树两节点的最低公共祖先" class="headerlink" title="7. 二叉树两节点的最低公共祖先"></a>7. 二叉树两节点的最低公共祖先</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;Node*&gt; V1;</span><br><span class="line"><span class="built_in">vector</span>&lt;Node*&gt; V2;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">getNodePath</span><span class="params">(Node* root, Node* tar, <span class="built_in">vector</span>&lt;Node*&gt;&amp; V)</span> </span>&#123; <span class="comment">// 根左右，回溯</span></span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    V.push_back(root);</span><br><span class="line">    <span class="keyword">if</span>(root == tar) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(root-&gt;lchild) &#123;</span><br><span class="line">        flag = getNodePath(root-&gt;lchild, tar, V);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!flag &amp;&amp; root-&gt;rchild) &#123;</span><br><span class="line">        flag = getNodePath(root-&gt;rchild, tar, V);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!flag) &#123;</span><br><span class="line">        V.pop_back();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> flag;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">Node* <span class="title">getCom</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Node*&gt;&amp; V1, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Node*&gt;&amp; V2)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;Node*&gt;::const_iterator it1 = V1.begin();</span><br><span class="line">    <span class="built_in">vector</span>&lt;Node*&gt;::const_iterator it2 = V2.begin();</span><br><span class="line">    Node* pLast = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">while</span>(it != V1.end() &amp;&amp; it2 != V2.end()) &#123;</span><br><span class="line">        <span class="keyword">if</span>(*it != *it2) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        pLast = *it1;</span><br><span class="line">        ++it1;</span><br><span class="line">        ++it2;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pLast;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="8-从二叉树的深度扩展判断平衡"><a href="#8-从二叉树的深度扩展判断平衡" class="headerlink" title="8. 从二叉树的深度扩展判断平衡"></a>8. 从二叉树的深度扩展判断平衡</h2><p>从二叉树的深度 扩展到 判断 是否为平衡二叉树</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isBalance</span><span class="params">(Node* root, <span class="keyword">int</span>* dep)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        *dep = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(isBalance(root-&gt;lchild, &amp;left) &amp;&amp; isBalance(root-&gt;rchild, &amp;right)) &#123;</span><br><span class="line">        <span class="keyword">int</span> diff = left - right;</span><br><span class="line">        <span class="keyword">if</span>(diff &gt;= <span class="number">-1</span> &amp;&amp; diff &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">            *dep = <span class="number">1</span> + (left &gt; right ? left : right);</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="9-二叉搜索树与双向链表"><a href="#9-二叉搜索树与双向链表" class="headerlink" title="9. 二叉搜索树与双向链表"></a>9. 二叉搜索树与双向链表</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">convert</span><span class="params">(Node* root, Node*&amp; pLast)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(root-&gt;lchild) &#123;</span><br><span class="line">        convert(root-&gt;lchild, pLast);</span><br><span class="line">    &#125;</span><br><span class="line">    Node* pCur = root;</span><br><span class="line">    pCur-&gt;lchild = pLast;</span><br><span class="line">    <span class="keyword">if</span>(pLast) &#123;</span><br><span class="line">        pLast-&gt;rchild = pCur;</span><br><span class="line">    &#125;</span><br><span class="line">    pLast = pCur;</span><br><span class="line">    <span class="keyword">if</span>(root-&gt;rchild) &#123;</span><br><span class="line">        convert(root-&gt;rchild, pLast);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="10-strcpy-的实现，要考虑地址重叠"><a href="#10-strcpy-的实现，要考虑地址重叠" class="headerlink" title="10. strcpy 的实现，要考虑地址重叠"></a>10. strcpy 的实现，要考虑地址重叠</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">my_strcpy</span><span class="params">(<span class="keyword">char</span> *dst, <span class="keyword">const</span> <span class="keyword">char</span>* src)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">size_t</span> size = <span class="built_in">strlen</span>(src);</span><br><span class="line">    <span class="keyword">if</span>(dst == <span class="literal">NULL</span> || src == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span> *psrc;</span><br><span class="line">    <span class="keyword">char</span> *pdst;</span><br><span class="line">    <span class="keyword">if</span>(src &lt; dst &amp;&amp; src + size &gt; dst) &#123;</span><br><span class="line">        psrc = src + size - <span class="number">1</span>;</span><br><span class="line">        pdst = dst + size - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(size--) &#123;</span><br><span class="line">            *pdst-- = *psrc--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        psrc = src;</span><br><span class="line">        pdst = dst;</span><br><span class="line">        <span class="keyword">while</span>(size--) &#123;</span><br><span class="line">            *pdst++ = *psrc++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dst;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_memcpy</span><span class="params">(<span class="keyword">void</span>* dst, <span class="keyword">const</span> <span class="keyword">void</span>* src, size_t n)</span></span>; <span class="comment">// 区域不可以重叠</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_memmove</span><span class="params">(<span class="keyword">void</span>* dst, <span class="keyword">const</span> <span class="keyword">void</span>* src, size_t n)</span></span>; <span class="comment">// 区域可以重叠</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">my_strcmp</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* src1, <span class="keyword">const</span> <span class="keyword">char</span>* src2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(!(ret = *src1 - *src2) &amp;&amp; *src2 &amp;&amp; *str1) &#123;</span><br><span class="line">        ++src1; ++src2;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(ret &gt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">my_strstr</span><span class="params">(<span class="keyword">char</span>* src1, <span class="keyword">const</span> <span class="keyword">char</span>* src2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(src2 == <span class="literal">NULL</span> || !*src2) <span class="keyword">return</span> src1;</span><br><span class="line">    <span class="keyword">if</span>(src1 == <span class="literal">NULL</span> || !*src2) <span class="keyword">return</span> <span class="literal">NULL</span>; <span class="comment">//</span></span><br><span class="line">    <span class="keyword">int</span> pj = kmp(<span class="built_in">strlen</span>(str1), <span class="built_in">strlen</span>(src2));</span><br><span class="line">    <span class="keyword">if</span>(pj == <span class="number">-1</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">return</span> src1 + pj;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">my_strstr2</span><span class="params">(<span class="keyword">char</span>* src1, <span class="keyword">const</span> <span class="keyword">char</span>* src2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(src2 == <span class="literal">NULL</span> || !*src2) <span class="keyword">return</span> src1;</span><br><span class="line">    <span class="keyword">if</span>(src1 == <span class="literal">NULL</span> || !*src1) <span class="keyword">return</span> <span class="literal">NULL</span>; <span class="comment">//</span></span><br><span class="line">        <span class="keyword">while</span>(*str1) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; *(str1+i) == *(str2+i); ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span>(!*(str2+i+<span class="number">1</span>)) <span class="keyword">return</span> str1;</span><br><span class="line">        &#125;</span><br><span class="line">        str1++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="11-C-句柄类的实现"><a href="#11-C-句柄类的实现" class="headerlink" title="11. C++ 句柄类的实现"></a>11. C++ 句柄类的实现</h2><p>为了避免每个使用指针的类自己去控制引用计数，可以用一个类把指针封装起来。这个类对象可以出现在用户类使用指针的任何地方，而表现为一个指针的行为。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdexcept&gt;</span>    </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;  </span><br><span class="line"><span class="keyword">class</span> Handle &#123;  </span><br><span class="line"><span class="keyword">private</span> :  </span><br><span class="line">    T *ptr;  </span><br><span class="line">    <span class="keyword">size_t</span> *use;  </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">decrUse</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">        --(*use);  </span><br><span class="line">        <span class="keyword">if</span>(*use == <span class="number">0</span>) &#123;  </span><br><span class="line">            <span class="keyword">delete</span> ptr;  </span><br><span class="line">            <span class="keyword">delete</span> use;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line"><span class="keyword">public</span> :  </span><br><span class="line">    Handle(T *p = <span class="number">0</span>) : ptr(p), use(<span class="keyword">new</span> <span class="keyword">size_t</span>(<span class="number">1</span>)) &#123;&#125;  </span><br><span class="line">    Handle(<span class="keyword">const</span> Handle&amp; rhs) : ptr(rhs.ptr), use(rhs.use) &#123; ++(*use); &#125; <span class="comment">// 初始化用到  </span></span><br><span class="line">    Handle&amp; <span class="keyword">operator</span>= (<span class="keyword">const</span> Handle&amp; rhs) &#123; <span class="comment">// 覆盖以前的值  </span></span><br><span class="line">        ++(*rhs.use);  </span><br><span class="line">        decrUse();  </span><br><span class="line">        ptr = rhs.ptr;  </span><br><span class="line">        use = rhs.use;  </span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    T* <span class="keyword">operator</span>-&gt; () &#123;  </span><br><span class="line">        <span class="keyword">if</span>(ptr) <span class="keyword">return</span> ptr;  </span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">"NULL point"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">const</span> T* <span class="keyword">operator</span>-&gt; () <span class="keyword">const</span> &#123;  </span><br><span class="line">        <span class="keyword">if</span>(ptr) <span class="keyword">return</span> ptr;  </span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">"NULL point"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    T&amp; <span class="keyword">operator</span>* () &#123;  </span><br><span class="line">        <span class="keyword">if</span>(ptr) <span class="keyword">return</span> *ptr;  </span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">"NULL point"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">const</span> T&amp; <span class="keyword">operator</span>* () <span class="keyword">const</span> &#123;  </span><br><span class="line">        <span class="keyword">if</span>(ptr) <span class="keyword">return</span> *ptr;  </span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">"NULL point"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    ~Handle() &#123;  </span><br><span class="line">        decrUse();  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line"><span class="keyword">class</span> A &#123;  </span><br><span class="line"><span class="keyword">public</span> :  </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Hello! , I am A!!! \n"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">try</span> &#123;  </span><br><span class="line">        <span class="function">Handle <span class="title">t</span><span class="params">(new A)</span></span>;  </span><br><span class="line">        t-&gt;show();  </span><br><span class="line">    &#125; <span class="keyword">catch</span> (<span class="keyword">const</span> exception&amp; err) &#123;  </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; err.what() &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>句柄类理解补充</strong> ：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> TA &#123;</span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> *pi;</span><br><span class="line">    <span class="keyword">char</span> *pc;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><strong>句柄类写 对应于 </strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> TA &#123;</span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    Handle&lt;<span class="keyword">int</span>&gt; pi;</span><br><span class="line">    Handle&lt;<span class="keyword">int</span>&gt; pc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="12-6种排序的手写"><a href="#12-6种排序的手写" class="headerlink" title="12. 6种排序的手写"></a>12. 6种排序的手写</h2><p>堆排序</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">input ： <span class="number">8</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heapify</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> i, <span class="keyword">int</span> size)</span> </span>&#123; <span class="comment">// 堆化的维持需要用递归  </span></span><br><span class="line">    <span class="keyword">int</span> ls = <span class="number">2</span>*i, rs = <span class="number">2</span>*i + <span class="number">1</span>;  </span><br><span class="line">    <span class="keyword">int</span> large = i;  </span><br><span class="line">    <span class="keyword">if</span>(ls &lt;= size &amp;&amp; a[ls] &gt; a[i]) &#123;  </span><br><span class="line">        large = ls;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">if</span>(rs &lt;= size &amp;&amp; a[rs] &gt; a[large]) &#123;  </span><br><span class="line">        large = rs;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">if</span>(large != i) &#123;  </span><br><span class="line">        swap(a[i], a[large]);  </span><br><span class="line">        heapify(a, large, size);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">buildHeap</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> size)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = size/<span class="number">2</span>; i &gt;= <span class="number">1</span>; i--) &#123;  </span><br><span class="line">        heapify(a, i, size);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heapSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> size)</span> </span>&#123;  </span><br><span class="line">    buildHeap(a, size);  </span><br><span class="line">    <span class="keyword">int</span> len = size;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = len; i &gt;= <span class="number">2</span>; i--) &#123;  </span><br><span class="line">        swap(a[i], a[<span class="number">1</span>]);  </span><br><span class="line">        len--;  </span><br><span class="line">        heapify(a, <span class="number">1</span>, len);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>快速排序</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span>(left &lt; right) &#123;  </span><br><span class="line">        <span class="keyword">int</span> l = left, r = right, x = a[l];  </span><br><span class="line">        <span class="keyword">while</span>(<span class="number">1</span>) &#123;  </span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[r] &gt;= x) r--;  </span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[l] &lt;= x) l++;  </span><br><span class="line">            <span class="keyword">if</span>(l &gt;= r) <span class="keyword">break</span>;  </span><br><span class="line">            swap(a[r], a[l]);  </span><br><span class="line">        &#125;  </span><br><span class="line">        swap(a[left], a[l]);  </span><br><span class="line">        quickSort(a, left, l<span class="number">-1</span>);  </span><br><span class="line">        quickSort(a, l+<span class="number">1</span>, right);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>归并排序</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span>(l &gt;= r) <span class="keyword">return</span>;  </span><br><span class="line">    <span class="keyword">int</span> mid = (l+r) / <span class="number">2</span>;  </span><br><span class="line">    mergeSort(a, l, mid);  </span><br><span class="line">    mergeSort(a, mid+<span class="number">1</span>, r);  </span><br><span class="line">    <span class="keyword">int</span> *arr = <span class="keyword">new</span> <span class="keyword">int</span>[r-l+<span class="number">1</span>];  </span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">int</span> i = l, j = mid+<span class="number">1</span>;  </span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid &amp;&amp; j &lt;= r) &#123;  </span><br><span class="line">        <span class="keyword">if</span>(a[i] &lt;= a[j]) &#123;  </span><br><span class="line">            arr[k++] = a[i++];  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">else</span> &#123;  </span><br><span class="line">            arr[k++] = a[j++]; <span class="comment">// ans += (mid+1-i);  </span></span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid) arr[k++] = a[i++];  </span><br><span class="line">    <span class="keyword">while</span>(j &lt;= r) arr[k++] = a[j++];  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = l; i &lt;= r; i++) &#123;  </span><br><span class="line">        a[i] = arr[i-l];  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">delete</span> []arr;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>插入排序</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insertSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> len)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> j;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; i++) &#123;<span class="comment">// 新抓的每张扑克牌  </span></span><br><span class="line">        <span class="keyword">int</span> temp = a[i];  </span><br><span class="line">        <span class="keyword">for</span>(j = i<span class="number">-1</span>; a[j] &gt; temp &amp;&amp; j &gt;= <span class="number">0</span>; j--) &#123;  </span><br><span class="line">            a[j+<span class="number">1</span>] = a[j];  </span><br><span class="line">        &#125;  </span><br><span class="line">        a[j+<span class="number">1</span>] = temp;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>冒泡排序</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubbleSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> len)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; len-i; j++) &#123;  </span><br><span class="line">            <span class="keyword">if</span>(a[j] &gt; a[j+<span class="number">1</span>]) swap(a[j], a[j+<span class="number">1</span>]);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>选择排序</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">selectSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> len)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> i, j, k;  </span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len<span class="number">-1</span>; i++) &#123;  </span><br><span class="line">        k = i;  </span><br><span class="line">        <span class="keyword">for</span>(j = i+<span class="number">1</span>; j &lt; len; j++) &#123;  </span><br><span class="line">            <span class="keyword">if</span>(a[j] &lt; a[k]) k = j;  </span><br><span class="line">        &#125;  </span><br><span class="line">        swap(a[i], a[k]);  <span class="comment">// 将第i位小的数放入i位置  </span></span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="13-字符串的全排列"><a href="#13-字符串的全排列" class="headerlink" title="13. 字符串的全排列"></a>13. 字符串的全排列</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">res</span><span class="params">(<span class="keyword">char</span> *str, <span class="keyword">char</span> *pStr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(*pStr == <span class="string">'\0'</span>) <span class="built_in">cout</span> &lt;&lt; str &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">char</span> *p = pStr; p != <span class="string">'\0'</span>; ++p) &#123;</span><br><span class="line">        <span class="keyword">char</span> tmp = *p;</span><br><span class="line">        *p = *pStr;</span><br><span class="line">        *pStr = tmp;</span><br><span class="line">        res(str, pStr+<span class="number">1</span>);</span><br><span class="line">        tmp = *p;</span><br><span class="line">        *p = *pStr;</span><br><span class="line">        *pStr = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="14-字典树-trie-并查集-KMP"><a href="#14-字典树-trie-并查集-KMP" class="headerlink" title="14. 字典树 trie / 并查集 / KMP"></a>14. 字典树 trie / 并查集 / KMP</h2><ol>
<li>根节点不包含字符</li>
<li>从根节点到某一节点，路径经过的字符连接起来，为该节点对应的字符串</li>
<li>每个节点的所有子节点包含的字符都不相同</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> kind = <span class="number">26</span>;</span><br><span class="line"><span class="keyword">char</span> str[N][<span class="number">25</span>];</span><br><span class="line"><span class="keyword">struct</span> Node &#123;</span><br><span class="line">    <span class="keyword">int</span> num;</span><br><span class="line">    <span class="keyword">bool</span> tail;</span><br><span class="line">    Node* next[kind];</span><br><span class="line">    Node() : num(<span class="number">1</span>), tail(<span class="literal">false</span>) &#123;</span><br><span class="line">        <span class="built_in">memset</span>(next, <span class="number">0</span>, <span class="keyword">sizeof</span>(next));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(Node* root, <span class="keyword">char</span> *s)</span> </span>&#123;</span><br><span class="line">    Node* p = root;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, index;</span><br><span class="line">    <span class="keyword">while</span>(s[i]) &#123;</span><br><span class="line">        index = s[i] - <span class="string">'a'</span>;</span><br><span class="line">        <span class="keyword">if</span>(p-&gt;next[index] == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            p-&gt;next[index] = <span class="keyword">new</span> Node();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            p-&gt;next[index]-&gt;num++;</span><br><span class="line">        &#125;</span><br><span class="line">        p = p-&gt;next[index];</span><br><span class="line">        ++i;</span><br><span class="line">    &#125;</span><br><span class="line">    p-&gt;tail = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>并查集</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10005</span>;</span><br><span class="line"><span class="keyword">struct</span> Node &#123;</span><br><span class="line">    <span class="keyword">int</span> par, sum;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">int</span> SUM;</span><br><span class="line">Node p[<span class="number">2</span>*N+<span class="number">5</span>];</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">makeSet</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">2</span>*n; i++) &#123;</span><br><span class="line">        p[i].par = i;</span><br><span class="line">        p[i].sum = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    SUM = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x == p[x].par) <span class="keyword">return</span> x;</span><br><span class="line">    <span class="keyword">return</span> p[x].par = find(p[x].par);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">union1</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fx = find(x);</span><br><span class="line">    <span class="keyword">int</span> fy = find(y);</span><br><span class="line">    <span class="keyword">if</span>(fx != fy) &#123;</span><br><span class="line">        p[fx].par = fy;</span><br><span class="line">        p[fy].sum += p[fx].sum;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p[fy].sum &gt; SUM) &#123;</span><br><span class="line">        SUM = p[fy].sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>KMP</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">1005</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> M = <span class="number">105</span>;</span><br><span class="line"><span class="keyword">char</span> s[N];</span><br><span class="line"><span class="keyword">char</span> t[M], next[M];</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">get_next</span><span class="params">(<span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">-1</span>;</span><br><span class="line">    next[<span class="number">0</span>] = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; len - <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(j == <span class="number">-1</span> || t[i] == t[j]) &#123;</span><br><span class="line">            i++, j++, next[i] = j;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> j = next[j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kmp</span><span class="params">(<span class="keyword">int</span> sl ,<span class="keyword">int</span> tl)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; sl &amp;&amp; j &lt; tl) &#123;</span><br><span class="line">        <span class="keyword">if</span>(j == <span class="number">-1</span> || s[i] == t[j]) &#123;</span><br><span class="line">            i++, j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> j = next[j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(j == tl) <span class="keyword">return</span> i - j + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="15-二叉树非递归先根遍历和中序遍历"><a href="#15-二叉树非递归先根遍历和中序遍历" class="headerlink" title="15. 二叉树非递归先根遍历和中序遍历"></a>15. 二叉树非递归先根遍历和中序遍历</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fPre</span><span class="params">(Node* root)</span> </span>&#123; <span class="comment">// 先根遍历    根-&gt;左-&gt;右</span></span><br><span class="line">    Node* p = root;</span><br><span class="line">    <span class="built_in">stack</span>&lt;Node*&gt;    S;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; p-&gt;value &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">            S.push(p);</span><br><span class="line">            p = p-&gt;lchild;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(S.empty()) <span class="keyword">return</span>;</span><br><span class="line">            p = S.top(); S.pop();</span><br><span class="line">            p = p-&gt;rchild;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fIno</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">    Node* p = root;</span><br><span class="line">    <span class="built_in">stack</span>&lt;Node*&gt; S;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            S.push(p);</span><br><span class="line">            p = p-&gt;lchild;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(S.empty()) <span class="keyword">return</span>;</span><br><span class="line">            p = S.top(); S.pop();</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; p-&gt;value &lt;&lt;    <span class="string">' '</span>;</span><br><span class="line">            p = p-&gt;rchild;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="16-链表的归并，翻转，找环，-查找"><a href="#16-链表的归并，翻转，找环，-查找" class="headerlink" title="16. 链表的归并，翻转，找环， 查找"></a>16. 链表的归并，翻转，找环， 查找</h2><h3 id="16-1-链表的归并"><a href="#16-1-链表的归并" class="headerlink" title="16.1 链表的归并"></a>16.1 链表的归并</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> Node &#123; Node *next; <span class="keyword">int</span> value; &#125;; </span><br><span class="line"><span class="comment">/* 你一定要相信递归是一个强大的思想 */</span></span><br><span class="line"><span class="function">Node* <span class="title">mergeList</span><span class="params">(Node* head1, Node* head2)</span> </span>&#123; <span class="comment">// 有序链表的合并</span></span><br><span class="line">    <span class="keyword">if</span>(head1 == <span class="literal">NULL</span>) <span class="keyword">return</span> head2;</span><br><span class="line">    <span class="keyword">if</span>(head2 == <span class="literal">NULL</span>) <span class="keyword">return</span> head1;</span><br><span class="line">    Node* tmp;</span><br><span class="line">    <span class="keyword">if</span>(head1-&gt;value &lt; head2-&gt;value) &#123;</span><br><span class="line">        tmp = head1;</span><br><span class="line">        head1 = head1-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        tmp = head2;</span><br><span class="line">        head2 = head2-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    tmp-&gt;next = mergeList(head1, head2);</span><br><span class="line">    <span class="keyword">return</span> tmp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">Node* <span class="title">mergeSort</span><span class="params">(Node* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(head == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    Node* r_head = head;</span><br><span class="line">    Node* head1 = head;</span><br><span class="line">    Node* head2 = head;</span><br><span class="line">    <span class="keyword">while</span>(head2-&gt;next != <span class="literal">NULL</span> &amp;&amp; head2-&gt;next-&gt;next != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        head1 = head1-&gt;next;</span><br><span class="line">        head2 = head2-&gt;next-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(head1-&gt;next == <span class="literal">NULL</span>) <span class="keyword">return</span> r_head; <span class="comment">// 只有一个节点</span></span><br><span class="line">    head2 = head1-&gt;next;</span><br><span class="line">    head1 = head;</span><br><span class="line">    r_head = mergeList(mergeSort(head1), mergeSort(head2));</span><br><span class="line">    <span class="keyword">return</span> r_head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="16-2-链表的翻转"><a href="#16-2-链表的翻转" class="headerlink" title="16.2 链表的翻转"></a>16.2 链表的翻转</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Node* head;   </span><br><span class="line"><span class="function">Node* <span class="title">reverse</span><span class="params">(Node* head)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span>(head == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;  </span><br><span class="line">    Node* tmp = head;  </span><br><span class="line">    Node*p;  </span><br><span class="line">    <span class="keyword">while</span>(tmp-&gt;next != <span class="literal">NULL</span>) &#123;  </span><br><span class="line">        p = tmp-&gt;next;  </span><br><span class="line">        tmp-&gt;next = p-&gt;next;  </span><br><span class="line">        p-&gt;next = head;  </span><br><span class="line">        head = p;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> head;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>16_3 查找链表的第 k 个节点 [过]</li>
<li>16_4 O(1) 时间删除链表节点</li>
<li>16_5 在单链表末尾添加一个节点 (陷阱 ： 链表为空)</li>
<li>16_6 删除链表中第一个指定值的节点。(健壮性)</li>
<li>16_7 复杂链表的复制。 空间-O(1) 时间O(n)<blockquote>
<p>   (1), 对原链表任意节点N，创建N’, 将其放在 N 之后。<br>   (2), N 的Sib指向 S，复制 N’的Sib, 指向 S 的下一个节点S’。<br>   (3), 拆开链表，奇数位一个链表，偶数位一个。</p>
</blockquote>
</li>
<li>16_8 linkStack</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> Node &#123; <span class="keyword">char</span> value; Node* next; &#125;;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">push</span><span class="params">(Node* top, <span class="keyword">char</span> x)</span> </span>&#123;</span><br><span class="line">    Node* tmp = <span class="keyword">new</span> Node();</span><br><span class="line">    <span class="keyword">if</span>(tmp == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    tmp-&gt;value = x;</span><br><span class="line">    tmp-&gt;next = top-&gt;next; <span class="comment">// 插入一个节点so easy!</span></span><br><span class="line">    top-&gt;next = tmp;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">pop</span><span class="params">(Node* top)</span> </span>&#123;</span><br><span class="line">    Node* tmp = top-&gt;next;</span><br><span class="line">    <span class="keyword">if</span>(tmp == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    top-&gt;next = tmp-&gt;next;</span><br><span class="line">    <span class="keyword">delete</span> tmp;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">char</span> <span class="title">get_top</span><span class="params">(Node* top)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(top-&gt;next != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> top-&gt;next-&gt;value;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'#'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="17-数组的一些题目"><a href="#17-数组的一些题目" class="headerlink" title="17. 数组的一些题目"></a>17. 数组的一些题目</h2><ul>
<li>17_01 找最小的 k 个元素</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">105</span>;</span><br><span class="line"><span class="keyword">int</span> a[N] = &#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">8</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">part</span> <span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(left &lt; right) &#123;</span><br><span class="line">        <span class="keyword">int</span> x = a[<span class="number">0</span>], l = left, r = right;</span><br><span class="line">        <span class="keyword">while</span>(l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[r] &gt;= x) r--;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[l] &lt;= x) l++;</span><br><span class="line">            <span class="keyword">if</span>(l &gt;= r) <span class="keyword">break</span>;</span><br><span class="line">            swap(a[l], a[r]);</span><br><span class="line">        &#125;</span><br><span class="line">        swap(x, a[l]);</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> left;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_k</span><span class="params">(<span class="keyword">int</span> *input, <span class="keyword">int</span> n, <span class="keyword">int</span> k)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">if</span>(input == <span class="literal">NULL</span> || k &gt; n || k &lt;= <span class="number">0</span> || n &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">int</span> start = <span class="number">0</span>, end = n    - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> index = part(input, start, end);</span><br><span class="line">    <span class="keyword">while</span>(index != k<span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(index &gt; k<span class="number">-1</span>) end = index - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(index &lt; k - <span class="number">1</span>) start = index + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">        index = part(input, start, end);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>17_02 数组中出现次数超过一半的数字 发帖水王</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[N] = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">core</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(a == <span class="literal">NULL</span> || len &lt; <span class="number">1</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> result = a[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> times = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span>(times == <span class="number">0</span>) &#123;</span><br><span class="line">            result = a[i];</span><br><span class="line">            times = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(a[i] == result) times++;</span><br><span class="line">            <span class="keyword">else</span> --times;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result; <span class="comment">// 在数据合法的情况下。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>17_03 顺时针打印矩阵 （col &gt; start<em>2 &amp;&amp; row &gt; start</em>2）</li>
<li>17_04 调整数组顺序使奇数位于偶数前面 (bool (*func)(int))</li>
<li>17_05 二维数组中的查找 （剑指）</li>
<li>17_06 旋转数组的最小元素。 [3, 4, 5, 1, 2], [1, 2, 3, 4, 5]</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[N] = &#123;<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_min</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(a == <span class="literal">NULL</span> || len &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> p1 = <span class="number">0</span>, p2 = len<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> mid = p1;</span><br><span class="line">    <span class="keyword">while</span>(a[p1] &gt;= a[p2]) &#123;</span><br><span class="line">        <span class="keyword">if</span>(p2 - p1 == <span class="number">1</span>) &#123;mid = p2; <span class="keyword">break</span>;&#125;</span><br><span class="line">        mid = (p1 + p2) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span>(a[p1] &lt;= a[mid]) p1 = mid;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(a[p2] &gt;= a[mid]) p2 = mid;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> mid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="18-涉及-栈，队列的的-3-道题"><a href="#18-涉及-栈，队列的的-3-道题" class="headerlink" title="18. 涉及 栈，队列的的 3 道题"></a>18. 涉及 栈，队列的的 3 道题</h2><h3 id="18-1-包含-min-函数的栈"><a href="#18-1-包含-min-函数的栈" class="headerlink" title="18.1 包含 min 函数的栈"></a>18.1 包含 min 函数的栈</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> StackWithMin &#123;</span><br><span class="line"><span class="keyword">public</span> :</span><br><span class="line">    StackWithMin() &#123;&#125;</span><br><span class="line">    <span class="keyword">virtual</span> ~StackWithMin() &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> size_t <span class="title">size</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">const</span> T&amp;)</span></span>;</span><br><span class="line">    <span class="function">T <span class="title">top</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function">T <span class="title">min</span><span class="params">()</span></span>;</span><br><span class="line"><span class="keyword">private</span> :</span><br><span class="line">    <span class="built_in">deque</span>&lt;T&gt; m_data;</span><br><span class="line">    <span class="built_in">deque</span>&lt;T&gt; m_min;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">size_t</span> StackWithMin&lt;T&gt;::size() <span class="keyword">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> m_data.size();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> StackWithMin&lt;T&gt;::pop() &#123;</span><br><span class="line">    assert(!m_data.empty() &amp;&amp; !m_min.empty());</span><br><span class="line">    m_data.pop_front();</span><br><span class="line">    m_min.pop_front();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> StackWithMin&lt;T&gt;::push(<span class="keyword">const</span> T&amp; value) &#123; <span class="comment">// core 就是这个函数</span></span><br><span class="line">    m_data.push_front(value);</span><br><span class="line">    <span class="keyword">if</span>(m_min.empty() || value &lt; m_min[<span class="number">0</span>]) &#123;</span><br><span class="line">        m_min.push_front(m_min[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> m_min.push_front(m_min[<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">T StackWithMin&lt;T&gt;::top() <span class="keyword">const</span> &#123;</span><br><span class="line">    assert(!m_data.empty() &amp;&amp; !m_min.empty());</span><br><span class="line">    <span class="keyword">return</span> m_data[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">T StackWithMin&lt;T&gt;::min() &#123;</span><br><span class="line">    assert(!m_data.empty() &amp;&amp; !m_min.empty());</span><br><span class="line">    <span class="keyword">return</span> m_min[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="18-2-两个栈实现队列"><a href="#18-2-两个栈实现队列" class="headerlink" title="18.2 两个栈实现队列"></a>18.2 两个栈实现队列</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> Queue &#123;</span><br><span class="line"><span class="keyword">public</span> :</span><br><span class="line">    Queue() &#123;&#125;</span><br><span class="line">    ~Queue() &#123;&#125;</span><br><span class="line">    <span class="function">T <span class="title">front</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">const</span> T&amp;)</span></span>;</span><br><span class="line"><span class="keyword">private</span> :</span><br><span class="line">    <span class="built_in">stack</span>&lt;T&gt; S1;</span><br><span class="line">    <span class="built_in">stack</span>&lt;T&gt; S2;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">T Queue&lt;T&gt;::front() &#123; <span class="comment">// core 就是这个函数</span></span><br><span class="line">    <span class="keyword">if</span>(!S2.empty()) &#123;</span><br><span class="line">        T tmp = S2.top();</span><br><span class="line">        <span class="keyword">return</span> tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        assert(!S1.empty());</span><br><span class="line">        <span class="keyword">while</span>(!S1.empty()) &#123;</span><br><span class="line">            S2.push(S1.top());</span><br><span class="line">            S1.pop();</span><br><span class="line">        &#125;</span><br><span class="line">        T tmp = S2.top(); S2.pop();</span><br><span class="line">        <span class="keyword">return</span> tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> Queue&lt;T&gt;::pop() &#123;</span><br><span class="line">    <span class="keyword">if</span>(!S2.empty()) &#123;</span><br><span class="line">        S2.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        assert(!S1.empty());</span><br><span class="line">        <span class="keyword">while</span>(!S1.empty()) &#123;</span><br><span class="line">            S2.push(S1.top());</span><br><span class="line">            S1.pop();</span><br><span class="line">        &#125;</span><br><span class="line">        S2.pop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> Queue&lt;T&gt;::push(<span class="keyword">const</span> T&amp; value) &#123;</span><br><span class="line">    S1.push(value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="18-3-栈的-push-pop-序列"><a href="#18-3-栈的-push-pop-序列" class="headerlink" title="18.3 栈的 push pop 序列"></a>18.3 栈的 push pop 序列</h3><p>// 1 2 3 4 5<br>// 4 3 5 1 2</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span> *pPush, <span class="keyword">int</span> *pPop, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(pPush != <span class="literal">NULL</span> &amp;&amp; pPop != <span class="literal">NULL</span> &amp;&amp; len &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">int</span> *pNextPush = pPush;</span><br><span class="line">        <span class="keyword">int</span> *pNextPop = pPop;</span><br><span class="line">        <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; S;</span><br><span class="line">        <span class="keyword">while</span>(pNextPop - pPop &lt; len) &#123;</span><br><span class="line">            <span class="keyword">while</span>(S.empty() || S.top() != *pNextPop) &#123;</span><br><span class="line">                <span class="keyword">if</span>(pNextPush - pPush == len) <span class="keyword">break</span>;</span><br><span class="line">                S.push(*pNextPush);</span><br><span class="line">                ++pNextPush;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(S.top() != *pNextPop) <span class="keyword">break</span>;</span><br><span class="line">            S.pop();</span><br><span class="line">            ++pNextPop;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(pNextPop - pPop == len) flag = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> flag;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="19-蓄水池原理-随即取样问题"><a href="#19-蓄水池原理-随即取样问题" class="headerlink" title="19. 蓄水池原理 随即取样问题"></a>19. 蓄水池原理 随即取样问题</h2><p>要求从 N 个元素中随机的抽取 k 个元素，其中 N 无法确定。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i=k+<span class="number">1</span> to N</span><br><span class="line">    M = random（<span class="number">1</span>, i);</span><br><span class="line">    <span class="keyword">if</span>(M &lt; k) swap(Mth value, ith value);</span><br></pre></td></tr></table></figure>
<h2 id="20-最长上升子序列"><a href="#20-最长上升子序列" class="headerlink" title="20. 最长上升子序列"></a>20. 最长上升子序列</h2><ul>
<li>m_14 poj 3903 最长上升子序列 dp n^2 vs （二分 nlogn）</li>
</ul>
<blockquote>
<p>F[t] 表示从 1 到t这一段中以 t 结尾的最长上升子序列的长度.<br>F[t] = max(1, F[j]+1); (j = 1, 2, …, t-1. 且 A[j] &lt; A[t]). </p>
</blockquote>
<p>此方程让我想到可以对比连续子数组的最大和。   </p>
<blockquote>
<p> dp: F[i] = max(a[i], F[i-1]+a[i]);  </p>
</blockquote>
<h2 id="21-寻找丑数"><a href="#21-寻找丑数" class="headerlink" title="21. 寻找丑数"></a>21. 寻找丑数</h2><p>不在非丑数上浪费时间，只看在丑数上的方法！ </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">pun[<span class="number">0</span>] = <span class="number">1</span>; ind = <span class="number">1</span>; p2 = p3 = p5 = pun; <span class="keyword">while</span>;</span><br></pre></td></tr></table></figure>
<h2 id="22-圆圈中最后剩下的数字。"><a href="#22-圆圈中最后剩下的数字。" class="headerlink" title="22. 圆圈中最后剩下的数字。"></a>22. 圆圈中最后剩下的数字。</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cirnm</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n &lt; <span class="number">1</span> || m &lt; <span class="number">1</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, count = <span class="number">0</span>; </span><br><span class="line">    <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; L;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) L.push_back(i);</span><br><span class="line">    <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt;::iterator lcur = L.begin(), next;</span><br><span class="line">    <span class="keyword">while</span>(!L.empty()) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; m; ++i) &#123;</span><br><span class="line">            lcur++;</span><br><span class="line">            <span class="keyword">if</span>(lcur == L.end()) lcur = L.begin();</span><br><span class="line">        &#125;</span><br><span class="line">        next = lcur + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(next == L.end()) next = L.begin();</span><br><span class="line">        L.erase(lcur);</span><br><span class="line">        count++;</span><br><span class="line">        <span class="keyword">if</span>(count == n - <span class="number">1</span>) <span class="keyword">break</span>;</span><br><span class="line">        lcur = next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> *lcur;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="23-从-1-到-n-的整数中-1-出现的次数"><a href="#23-从-1-到-n-的整数中-1-出现的次数" class="headerlink" title="23. 从 1 到 n 的整数中 1 出现的次数"></a>23. 从 1 到 n 的整数中 1 出现的次数</h2><blockquote>
<p>f(10^n-1) = n * 10^(n-1)</p>
</blockquote>
<h2 id="24-数值的整数次方"><a href="#24-数值的整数次方" class="headerlink" title="24. 数值的整数次方"></a>24. 数值的整数次方</h2><blockquote>
<p>a^n = a^n/2 * a^n/2;（偶数）<br>奇数的话再乘以 a.（递归神技）</p>
</blockquote>
<h2 id="25-一些面试小题"><a href="#25-一些面试小题" class="headerlink" title="25. 一些面试小题"></a>25. 一些面试小题</h2><p>面试题10 : 打印 1 到最大的 n位数。<br>面试题11 : 替换空格。(从后向先，先计算出长度！) ‘ ‘ -&gt; %20<br>面试题12 : 无符号整数二进制中 1 的个数。 (n-1)&amp;n     O(1的个数)。<br>面试题13 : 求 1+2+3+…+n, 要求不能使用，乘除法，for,while,if,else,switch,case。(n&gt;1)&amp;&amp;(i=res(n-1)+n);<br>面试题14 : 不能用 +, -, *, %, /,实现int add(int num1, int num2); （^, &amp;&lt;&lt;1）<br>面试题15 : 子数组的最大乘积。计算任意(N-1)个数的组合乘积最大的一组。P(0, 正，负)</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">res</span><span class="params">(<span class="keyword">char</span> *number, <span class="keyword">int</span> len, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(index == len<span class="number">-1</span>) &#123;</span><br><span class="line">        printNum(number, len); <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        number[index+<span class="number">1</span>] = <span class="string">'0'</span>+<span class="number">1</span>;</span><br><span class="line">        res(number, len, index+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">char</span> *str = <span class="keyword">new</span> <span class="keyword">char</span>[n+<span class="number">1</span>];</span><br><span class="line">    str[n] = <span class="string">'\0'</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        str[<span class="number">0</span>] = <span class="string">'0'</span> + i;</span><br><span class="line">        res(str, n, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> []str;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="26-只在-heap-stack-分配空间的类"><a href="#26-只在-heap-stack-分配空间的类" class="headerlink" title="26. 只在 heap,stack 分配空间的类"></a>26. 只在 heap,stack 分配空间的类</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> HeapOnly &#123;</span><br><span class="line"><span class="keyword">public</span> :</span><br><span class="line">    HeapOnly() &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">delete</span> <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span> :</span><br><span class="line">    ~HeapOnly() &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> StackOnly &#123;</span><br><span class="line"><span class="keyword">public</span> :</span><br><span class="line">    StackOnly() &#123;&#125;</span><br><span class="line">    ~StackOnly() &#123;&#125;</span><br><span class="line"><span class="keyword">private</span> :</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(size_t size)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="27-String-的复制控制"><a href="#27-String-的复制控制" class="headerlink" title="27. String 的复制控制"></a>27. String 的复制控制</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> String &#123; <span class="comment">// 一切都是深拷贝！  </span></span><br><span class="line"><span class="keyword">private</span> :  </span><br><span class="line">    <span class="keyword">char</span> *m_data;  </span><br><span class="line"><span class="keyword">public</span> :  </span><br><span class="line">    String(<span class="keyword">const</span> <span class="keyword">char</span> *str = <span class="literal">NULL</span>);  </span><br><span class="line">    String(<span class="keyword">const</span> String&amp;);  </span><br><span class="line">    String&amp; <span class="keyword">operator</span> =(<span class="keyword">const</span> String&amp; rhs);  </span><br><span class="line">    ~String();  </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; m_data &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">String::String(<span class="keyword">const</span> <span class="keyword">char</span> *str) &#123; <span class="comment">// 记住这里你是不能写 m_data = str, 不用犯这种想当然的错误！  </span></span><br><span class="line">    <span class="keyword">if</span>(str == <span class="literal">NULL</span>) &#123;  </span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">1</span>];  </span><br><span class="line">        m_data[<span class="number">0</span>] = <span class="string">'\0'</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">else</span> &#123;  </span><br><span class="line">        <span class="keyword">int</span> len = <span class="built_in">strlen</span>(str);  </span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[len+<span class="number">1</span>];  </span><br><span class="line">        <span class="built_in">strcpy</span>(m_data, str);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line">String::String(<span class="keyword">const</span> String&amp; rhs) &#123;  </span><br><span class="line">    <span class="keyword">if</span>(rhs.m_data == <span class="literal">NULL</span>) &#123;  </span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">1</span>];  </span><br><span class="line">        m_data[<span class="number">0</span>] = <span class="string">'\0'</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">else</span> &#123;  </span><br><span class="line">        <span class="keyword">int</span> len = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt; (<span class="built_in">strlen</span>(rhs.m_data));  </span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[len+<span class="number">1</span>];  </span><br><span class="line">        <span class="built_in">strcpy</span>(m_data, rhs.m_data);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">String&amp; String::<span class="keyword">operator</span> =(<span class="keyword">const</span> String&amp; rhs) &#123;  </span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span> == &amp;rhs) <span class="keyword">return</span> *<span class="keyword">this</span>; <span class="comment">// *别落下！*  </span></span><br><span class="line">    <span class="keyword">delete</span> []m_data;  <span class="comment">// *一定要先释放以前的！*  </span></span><br><span class="line">    <span class="keyword">if</span>(rhs.m_data == <span class="literal">NULL</span>) &#123;  </span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">1</span>];  </span><br><span class="line">        m_data[<span class="number">0</span>] = <span class="string">'\0'</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">else</span> &#123;  </span><br><span class="line">        <span class="keyword">int</span> len = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt; (<span class="built_in">strlen</span>(rhs.m_data));  </span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[len+<span class="number">1</span>];  </span><br><span class="line">        <span class="built_in">strcpy</span>(m_data, rhs.m_data);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">String::~String() &#123;  </span><br><span class="line">    <span class="keyword">delete</span> []m_data;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="function">String <span class="title">s</span><span class="params">(<span class="string">"hello"</span>)</span></span>;  </span><br><span class="line">    <span class="function">String <span class="title">s1</span><span class="params">(<span class="string">"hi"</span>)</span></span>;  </span><br><span class="line">    s = s1;  </span><br><span class="line">    s.show();  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="28-c-单件模式的实现"><a href="#28-c-单件模式的实现" class="headerlink" title="28. c++ 单件模式的实现"></a>28. c++ 单件模式的实现</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> L &#123;  </span><br><span class="line"><span class="keyword">public</span> :  </span><br><span class="line">    <span class="function"><span class="keyword">static</span> L* <span class="title">instance</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">        <span class="keyword">if</span>(!pi) &#123;  </span><br><span class="line">            pi = <span class="keyword">new</span> L();  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">return</span> pi;  </span><br><span class="line">    &#125;  </span><br><span class="line">    ~L() &#123;  </span><br><span class="line">        <span class="keyword">if</span>(pi) &#123;  </span><br><span class="line">            <span class="keyword">delete</span> pi;  </span><br><span class="line">            pi = <span class="number">0</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;;  </span><br><span class="line">    <span class="keyword">static</span> L* pi;  </span><br><span class="line"><span class="keyword">private</span> :  </span><br><span class="line">    L() &#123;&#125;;   <span class="comment">// 防止产生实例          </span></span><br><span class="line">    L(<span class="keyword">const</span> L&amp;); <span class="comment">// 防止复制构造另一个实例  </span></span><br><span class="line">    L&amp; <span class="keyword">operator</span>= (<span class="keyword">const</span> L&amp;); <span class="comment">// 防止赋值构造出另一个实例  </span></span><br><span class="line">&#125;;  </span><br><span class="line">L* L::pi = <span class="literal">NULL</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    L::pi = L::instance();  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="29-矩阵相乘"><a href="#29-矩阵相乘" class="headerlink" title="29. 矩阵相乘"></a>29. 矩阵相乘</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">a[m][n] * b[n][m] = c[m][m];</span><br><span class="line">a :    a b c</span><br><span class="line">       d e f</span><br><span class="line">b:     <span class="number">1</span> <span class="number">2</span></span><br><span class="line">       <span class="number">3</span> <span class="number">4</span></span><br><span class="line">       <span class="number">5</span> <span class="number">6</span></span><br><span class="line">结果 c :  <span class="number">1</span>a+<span class="number">3</span>b+<span class="number">5</span>c  <span class="number">2</span>a+<span class="number">4</span>b+<span class="number">6</span>c</span><br><span class="line">         <span class="number">1</span>d+<span class="number">3e+5</span>f  <span class="number">2</span>d+<span class="number">4e+6</span>f</span><br><span class="line">         </span><br><span class="line"><span class="keyword">int</span> a[<span class="number">2</span>][<span class="number">3</span>] = &#123;&#123;<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>&#125;, &#123;<span class="number">400</span>, <span class="number">500</span>, <span class="number">600</span>&#125;&#125;;  </span><br><span class="line"><span class="keyword">int</span> b[<span class="number">3</span>][<span class="number">2</span>] = &#123;&#123;<span class="number">1</span>, <span class="number">2</span>&#125;, &#123;<span class="number">3</span>, <span class="number">4</span>&#125;, &#123;<span class="number">5</span>, <span class="number">6</span>&#125;&#125;;  </span><br><span class="line"><span class="keyword">int</span> c[<span class="number">2</span>][<span class="number">2</span>];  </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span> m, <span class="keyword">int</span> n)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; ++i) &#123;  </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; m; j++) &#123;  </span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; n; k++) &#123;  </span><br><span class="line">                c[i][j] += a[i][k]*b[k][j];  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; ++i) &#123;  </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; m; ++j) &#123;  </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; c[i][j] &lt;&lt; <span class="string">' '</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    solve(<span class="number">2</span>, <span class="number">3</span>);  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line">Output :</span><br><span class="line">  <span class="number">1400</span> <span class="number">2600</span> </span><br><span class="line">  <span class="number">3200</span> <span class="number">6200</span></span><br></pre></td></tr></table></figure>
<h2 id="30-数组相关的一些经典题目"><a href="#30-数组相关的一些经典题目" class="headerlink" title="30. 数组相关的一些经典题目"></a>30. 数组相关的一些经典题目</h2><p>1.1   如何用递归实现数组求和 return n == 0 ? 0 : getSum(a, n-1) + a[n-1];<br>1.2   如何用一个for循环打印出一个二维数组 (过)<br>1.4   如何用递归算法判断一个数组是否是递增 return (a[n-1] &gt; a[n-2]) &amp;&amp; isIncrease(a, n-1);<br>1.5   如何分别使用递归与非递归实现二分查找算法(过)<br>1.6   如何在排序数组中，找出给定数字出现的次数(二分好的题目) </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> len, <span class="keyword">int</span> num, <span class="keyword">bool</span> isLeft)</span></span>;</span><br></pre></td></tr></table></figure>
<p>1.7   如何计算两个有序整型数组的交集</p>
<blockquote>
<p>情况1 : 两数组长度相差不悬殊，解决方法：哈希其中一数组，在遍历另一个数组。<br>情况2 : 相差悬殊。小的数组中的数字，分别在长数组上二分查找。</p>
</blockquote>
<p>1.8   如何找出数组中重复次数最多的数, 好题 map，遍历，if(++m[a[i]] &gt;= m[val]) val = a[i] )<br>1.9   如何在O(n)的时间复杂度内找出数组中出现次数超过了一半的数(过，但是非常好的题目！，有一个count)<br>1.11 如何判断一个数组中的数值是否连续相邻 (未描述完整，解决 ： 非0最大 非0最小 &lt;= 4)<br>1.12 如何找出数组中出现奇数次的元素 (过)<br>1.13 如何找出数列中符合条件的数对的个数 (排序，两边夹-O(nlogn) / 计数排序-O(n))<br>1.14 如何寻找出数列中缺失的数 (异或 O(2*n) / (n+1)*n / 2 sum O(n))<br>1.15 如何判定数组是否存在重复元素 a[n],元素取值范围也是 1~n  </p>
<blockquote>
<p>(解析 ： bitmap, 费空间 /  茂旭神之法，死磕到底O(2n)</p>
</blockquote>
<p>1.16 如何重新排列数组使得数组左边为奇数，右边为偶数 (类似于快排的思想)</p>
<p>1.17 如何把一个整型数组中重复的数字去掉</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// sort1 1 1 2 2 3, 遍历数组，k=0, i=1; </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(a[k] != a[i])) &#123;</span><br><span class="line">    a[k+<span class="number">1</span>]=a[i];</span><br><span class="line">    k++</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1.18 如何找出一个数组中第二大的数</p>
<blockquote>
<p>(解析 ：法1 : 堆筛。 法2 : 遍历，用两个变量记录 maxv=a[0] ,sec_max=MIN, 更新)</p>
</blockquote>
<p>1.19 如何最少的比较次数寻找数组中的最小值和最大值</p>
<p>《编程之美》 168P，给的证明比较次数为 1.5N 次</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getMaxMin</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> low, <span class="keyword">int</span> high, <span class="keyword">int</span>&amp; maxv, <span class="keyword">int</span>&amp; minv)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> mid, max1, max2, min1, min2;  </span><br><span class="line">    <span class="keyword">if</span>(high - low == <span class="number">1</span> || high - low == <span class="number">0</span>) &#123;  </span><br><span class="line">        a[low] &gt; a[high] ? (maxv = a[low], minv = a[high]) : (maxv = a[high], minv = a[low]);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">else</span> &#123;  </span><br><span class="line">        mid = (high + low) / <span class="number">2</span>;  </span><br><span class="line">        getMaxMin(a, low, mid, max1, min1);  </span><br><span class="line">        getMaxMin(a, mid+<span class="number">1</span>, high, max2, min2);  </span><br><span class="line">        maxv = max2 &gt; max1 ? max2 : max1;  </span><br><span class="line">        minv = min2 &gt; min1 ? min1 : min2;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1.20 如何将数组的后面m个数移动为前面m个数。 解析 ： 不就是左旋转！ OK！<br>1.21 如何计算出序列的前n项数据</p>
<blockquote>
<p>描述 ： 正整数序列Q中的每个元素都至少能被正整数 a 和 b 中的一个整除。 求 序列前 n 项数据？</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">解析 ： 类似于归并的思想</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; N; k++) &#123;</span><br><span class="line">    tmpA = a*i; tmpB = b*j; </span><br><span class="line">    <span class="keyword">if</span>(tmpA &lt;= tmpB) &#123;</span><br><span class="line">        Q[k] = tmpA; i++;</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">else</span> ..</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1.23 如何判断一个整数x是否可以表示成n（n≥2）个连续正整数的和</p>
<blockquote>
<p>解析 ：</p>
<p>x = m + (m+1) + (m+2) + … + (m+n-1), 推出 : m = (2*x / n - n + 1) / 2; m &gt;= 1;</p>
<p>也就是判断 (2*x / n - n + 1) 是不是偶数的问题。</p>
<p>tmp = m; if(tmp == (int)tmp) 就 OK！</p>
</blockquote>
<h2 id="31-匈牙利算法入门基于-dfs-实现"><a href="#31-匈牙利算法入门基于-dfs-实现" class="headerlink" title="31. 匈牙利算法入门基于 dfs 实现"></a>31. 匈牙利算法入门基于 dfs 实现</h2><p>poj 1274 The Perfect Stall (匈牙利入门)</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span>  </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">205</span>;  </span><br><span class="line"><span class="keyword">bool</span> <span class="built_in">map</span>[N][N];  </span><br><span class="line"><span class="keyword">int</span> mat[N];  </span><br><span class="line"><span class="keyword">bool</span> vis[N];  </span><br><span class="line"><span class="keyword">int</span> n, m;  </span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> u)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= m; i++) &#123;  </span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>[u][i] &amp;&amp; !vis[i]) &#123;  </span><br><span class="line">            vis[i] = <span class="literal">true</span>;  </span><br><span class="line">            <span class="keyword">if</span>(mat[i] == <span class="number">0</span> || dfs(mat[i])) &#123;  </span><br><span class="line">                mat[i] = u;  </span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxMatch</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="built_in">memset</span>(mat, <span class="number">0</span>, <span class="keyword">sizeof</span>(mat));  </span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;  </span><br><span class="line">        <span class="built_in">memset</span>(vis, <span class="number">0</span>, <span class="keyword">sizeof</span>(vis));  </span><br><span class="line">        <span class="keyword">if</span>(dfs(i)) count++;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> count;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;n, &amp;m) == <span class="number">2</span>) &#123;  </span><br><span class="line">        <span class="built_in">memset</span>(<span class="built_in">map</span>, <span class="number">0</span>, <span class="keyword">sizeof</span>(<span class="built_in">map</span>));  </span><br><span class="line">        <span class="keyword">int</span> ca, j;  </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;  </span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;ca);  </span><br><span class="line">            <span class="keyword">while</span>(ca--) &#123;  </span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;j);  </span><br><span class="line">                <span class="built_in">map</span>[i][j] = <span class="number">1</span>;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, maxMatch());  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="32-链表经典的一些题目"><a href="#32-链表经典的一些题目" class="headerlink" title="32. 链表经典的一些题目"></a>32. 链表经典的一些题目</h2><p>2.5 如何找出单链表中的倒数第k个元素(过)<br>2.6 如何实现单链表反转 (很好的题目，三指针法！)<br>2.7 如何从尾到头输出单链表(过)<br>2.9 如何进行单链表排序(很好 ： 归并排序法)<br>2.10 如何实现单链表交换任意两个元素（不包括表头）(非常好的题目)<br>2.11 如何检测一个较大的单链表是否有环<br>2.12 如何判断两个单链表（无环）是否交叉<br>2.13 如何删除单链表中的重复结点<br>2.15 什么是循环链表<br>2.16 如何实现双向链表的插入、删除操作<br>2.18 如何删除结点的前驱结点<br>2.19 如何实现双向循环链表的删除与插入操作 343<br>2.20 如何在不知道头指针的情况下将结点删除 344</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux 鸟哥的私房菜 读书心得]]></title>
      <url>http://sggo.me/2013/08/29/ops/linux-bird/</url>
      <content type="html"><![CDATA[<p>Linux 鸟哥的私房菜 读书心得 ： 实践与观察才是王道</p>
<a id="more"></a>
<h2 id="0-计算机概论"><a href="#0-计算机概论" class="headerlink" title="0. 计算机概论"></a>0. 计算机概论</h2><p>计算机：接受用户输入的指令与数据，经过处理器的数据与逻辑单元运算处理后，以产生或存储成有用的信息。</p>
<p><strong>计算机硬件的五大单元 </strong></p>
<ol>
<li>输入单元 (鼠标，键盘等)</li>
<li>控制器</li>
<li>运算器 (算术逻辑，控制，记忆)</li>
<li>存储器</li>
<li>输出单元 (屏幕，打印机等)</li>
</ol>
<blockquote>
<p>CPU 为一个具有特定功能的芯片，里头含有微指令集。<br>CPU 读取的数据都是从内存中读取来的，内存的数据是由输入单元传输进来的。<br>CPU 处理完毕的数据要写入内存，内存再到输出单元。</p>
</blockquote>
<p><strong>接口设备</strong></p>
<ol>
<li>主板 (设备连接一起，让其协调，通信)</li>
<li>存储设备(硬盘，软盘，光盘，磁带) </li>
<li>显示设备 </li>
<li>网络设备</li>
</ol>
<blockquote>
<p>硬盘最小物理量512bytes,最小的组成单位为扇区sector </p>
</blockquote>
<p><strong>个人计算机的架构与设计</strong></p>
<p>x86开发商 (intel,AMD) 的cpu架构并不兼容，主板芯片组设计不相同。</p>
<p>主板上最重要的就是芯片组，芯片组通常又分为两个桥接器来控制各组件的通信。</p>
<p>北桥 : 负责连接较快的 cpu,内存和显卡 等组件。 (AMD : 内存直接与cpu通信，不经过北桥)<br>南桥 : 负责连接硬盘，USB，网卡等。</p>
<p>北桥的总线称为系统总线，是内存传输的主要信道。 与总线宽度类似，cpu每次能够处理的数据量称为字组大小(word size),字组大小依据cpu的设计而有32位与64位。</p>
<blockquote>
<p>显卡厂商直接在显卡上面嵌入一个3D加速的芯片，这就是GPU称谓的由来。<br>CMOS是电脑主板上的一块可读写的RAM芯片。因为可读写的特性.</p>
<p>所以在电脑主板上用来保存BIOS设置完电脑硬件参数后的数据，这个芯片仅仅是用来存放数据的。<br>BIOS为写入到主板上某一块闪存或EEPROM的程序，在开机的时候运行，以加载CMOS当中的参数。</p>
</blockquote>
<p>软件是计算机的灵魂。</p>
<blockquote>
<p>机器程序与编译程序。 C/C++ – 编译器 –&gt; 机器码</p>
</blockquote>
<p><strong>操作系统 OS</strong></p>
<p>如果我们能够将硬件都驱动，并且提供一个开发软件的参考接口来给工程师开发软件的话，那就是OS</p>
<p>OS 内核：</p>
<blockquote>
<p>OS其实是一组程序，这组程序的重点在于管理计算机的所有活动以及驱动系统中的所有硬件。<br>OS内核 – 开机后常驻内存。</p>
</blockquote>
<p>系统调用：</p>
<blockquote>
<p>System Call – 开发软件，参考内核的相关功能。</p>
<p>应用程序 – 系统调用 – OS内核 – 硬件</p>
</blockquote>
<p>内核功能：</p>
<blockquote>
<ol>
<li>系统调用接口</li>
<li>程序管理</li>
<li>内存管理</li>
<li>文件系统管理</li>
<li>设备驱动</li>
</ol>
</blockquote>
<h2 id="1-Linux-介绍"><a href="#1-Linux-介绍" class="headerlink" title="1. Linux 介绍"></a>1. Linux 介绍</h2><p> GNU (Richard Mathew Stallman - 史托曼) 伟大的人物。 GNU 通用公共许可证 GPL - General Public License</p>
<ul>
<li>Emacs</li>
<li>GCC (GNU C Compiler )</li>
<li>GLIBC (GNU C Library)</li>
<li>Bash shell</li>
</ul>
<blockquote>
<p>Linus Torvalds - Linux 参考 Minix(from 谭宁邦教授) </p>
<p>Linux 参考了 POSIX规范(可携式操作系统接口)，重点在于规范内核与应用程序之间的接口。</p>
</blockquote>
<p><strong>Linux 如何学习：</strong></p>
<blockquote>
<p>实践再实践 – 要增加自己的体力，就只有运动；要增加自己的知识，就只有读书。</p>
<p>鸟哥的建议 『 （1）建议兴趣 （2）成就感 (被认可，被认同) 』</p>
</blockquote>
<h2 id="2-Linux-文件命令与权限"><a href="#2-Linux-文件命令与权限" class="headerlink" title="2. Linux 文件命令与权限"></a>2. Linux 文件命令与权限</h2><p>Linux : 多用户，多任务环境。 owner, group, others 且 各有 read, write, execute 等权限。</p>
<blockquote>
<p> /etc/passwd   /etc/shadow   /etc/group</p>
</blockquote>
<p><strong>文件命令</strong></p>
<ul>
<li>文件处理命令  [ ls, cp, mv, rm, cat, ln, file]  </li>
<li>文件内容查阅  [ cat, more, less, head, tail]</li>
<li>权限管理命令  [ chmod  u+r g-w o=x 777,  chown [-R] robby filename, chgrp 组名 filename, umask -S]</li>
<li>文件搜索命令  [ which, find, locate, updatedb, grep ] which -a在哪, whereis -b(数据库) locate 显示所有</li>
<li>压缩解压命令  [ gzip, gunzip, tar, zip, bzip2]</li>
<li>网络通信命令  [ write, wall, ping, ifconfig] </li>
<li>帮助命令     [ man, info, whatis  apropos, help]</li>
</ul>
<blockquote>
<p>whereis 与 which [不同是可以看到命令所在帮助文档位置]</p>
</blockquote>
<p><strong>文件类型</strong></p>
<p>1, (普通文件，纯文本文件(ASCII)， - 二进制文件，数据格式文件)<br>2, 目录 d directory<br>3, 连接文件 ( l 软链接文件link )<br>4, 设备与设备文件(b,c)<br>5, 套接字<br>6, 管道</p>
<p><strong>文件权限</strong></p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  drwxr-xr-x      2       hp      hp     4096    2012-12-25 16:18  dlinux</span><br><span class="line">[文件类型与权限] [硬链接数] [所有者] [所属组] [文件容量]    [修改日期]      [文件名]</span><br><span class="line"> </span><br><span class="line"> 4096 - 文件大小 [不是很准确，标记目录本身的大小，不是目录总大小] 数据块(512字节1单位)</span><br></pre></td></tr></table></figure>
</blockquote>
<p>特殊情况：</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hp@ubuntu:~/2014$ ls -ld /usr/bin/passwd</span><br><span class="line">-rwsr-xr-x 1 root root 42824  2月 11  2012 /usr/bin/passwd</span><br><span class="line">s 与 t 这两个权限与系统的帐号以及系统的进程较为相关。以后再研究。</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>组织数据方式</strong></p>
<blockquote>
<p>每种OS都有自己的，比如 NTFS, ext3 …</p>
<p>存储数据的最小单位就叫做数据块，这一这样理解！！</p>
<p>文件存储时，至少要占用一个数据块。 （你 60斤，也得做一个椅子，200斤做一个，600斤可能做两个！）</p>
<p>分你做什么，数据块越小存取速度越小，数据块越大存取的时候浪费空间越大。</p>
<p>分你做什么，调数据块多大合适。。。有个别时候！！</p>
</blockquote>
<h2 id="3-Linux-文件搜索命令"><a href="#3-Linux-文件搜索命令" class="headerlink" title="3. Linux 文件搜索命令"></a>3. Linux 文件搜索命令</h2><p>(1) 命令所在路径 : /usr/bin/find  语法:find [搜索路径] [搜寻关键字]</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find /etc -name init*  在目录/etc中查找init开头的文件</span><br><span class="line">$ find / -size +204800  在根目录下查找大于100MB的文件 block=512B</span><br><span class="line">              大于 ： +， 小于 ： - ， 等于 ： 不写 +, -；</span><br><span class="line">$ find / -user sam      在根目录下查找所有者为sam的文件</span><br></pre></td></tr></table></figure>
<p>(2) 按照时间查找  find /etc -mmin -120  （-之内， +超过 ）</p>
<p> 天 ctime, atime, mtime ， 分钟 cmin , amin,  mmin</p>
<blockquote>
<p> c-change 改变， 表示文件属性被修改过，所有者，所属组，权限<br> a-access 访问<br> m-modify 修改， 表示文件内容被修改过</p>
<p>我们刚讲了 4 个find的选项，其实不下 40 个！要学会看文档！</p>
</blockquote>
<p>(3) 连接符  -a and 逻辑与 -o or 逻辑或</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find /etc -ctime -1   在/etc下查找24小时内被修改过属性的文件和目录</span><br><span class="line">$ find /etc -size +163840 <span class="_">-a</span> -size -204800   在/etc下找大于80MB小于100MB的文件</span><br><span class="line">$ find /etc -name init* <span class="_">-a</span> -type f [ 二进制文件 ]</span><br><span class="line">$ find /etc -name init* <span class="_">-a</span> -type l [ 软链接文件 ]</span><br><span class="line">   </span><br><span class="line">    -type 文件类型  f 二进制文件  l 软链接文件  d 目录</span><br><span class="line">    连接符 find ..... -exec 命令 &#123;&#125; \;</span><br><span class="line">                           &#123;&#125; find 查询的结果  \ 转义符 【ls \ls】</span><br><span class="line"></span><br><span class="line">$ find /etc -name inittab -exec ls <span class="_">-l</span> &#123;&#125; \; 在/etc下查找inittab文件并显示其详细信息</span><br><span class="line">$ find /<span class="built_in">test</span> -name testfile3 -exec rm &#123;&#125; \;</span><br><span class="line">$ find /etc -name inittab -ok ls <span class="_">-l</span> &#123;&#125; \;  能询问一下 -ok</span><br><span class="line">$ find /etc -name inittab <span class="_">-a</span> -type f -exec ls <span class="_">-l</span> &#123;&#125; \;</span><br><span class="line">$ find . -inum 16 -exec rm &#123;&#125; \;</span><br></pre></td></tr></table></figure>
<p>(4) locate 功能描述:寻找文件或目录</p>
<blockquote>
<p>范例: $ locate file  列出所有跟file相关的文件</p>
</blockquote>
<p>(5) updatedb  执行权限:root  语法: updatedb  功能描述: 建立整个系统目录文件的数据库  </p>
<blockquote>
<p>范例: # updateb</p>
</blockquote>
<p>(6) grep  功能描述 : 在文件中搜寻字串匹配的行并输出  </p>
<h2 id="4-Linux-文件系统管理"><a href="#4-Linux-文件系统管理" class="headerlink" title="4. Linux 文件系统管理"></a>4. Linux 文件系统管理</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hp@ubuntu:/$ df -m</span><br><span class="line">文件系统        1M-块  已用  可用 已用% 挂载点</span><br><span class="line">/dev/loop0      14692  4746  9210   35% /</span><br><span class="line">udev             2977     1  2977    1% /dev</span><br><span class="line">tmpfs            1195     1  1194    1% /run</span><br><span class="line">none                5     1     5    1% /run/lock</span><br><span class="line">none             2986     1  2985    1% /run/shm</span><br><span class="line">/dev/sda8       20490 17188  3303   84% /host</span><br><span class="line">/dev/sda7      105036 24898 80139   24% /media/Studty</span><br><span class="line">hp@ubuntu:/$ du -h /etc/services</span><br><span class="line">20K    /etc/services</span><br><span class="line">hp@ubuntu:/$ du -sh ~/dlinux</span><br><span class="line">76K    /home/hp/dlinux</span><br><span class="line">hp@ubuntu:/$ file /etc/services</span><br><span class="line">/etc/services: ASCII English text</span><br></pre></td></tr></table></figure>
<p>特殊权限: 粘着位 <strong><em>t</em></strong></p>
<ol>
<li>粘着位的定义: 当权限为777的目录被授予粘着位,用户只能在此目录下删除自己是所有者的文件。</li>
<li>查看分区使用情况:df</li>
<li>查看文件、目录大小:du</li>
<li>查看文件详细时间参数:stat</li>
<li>校验文件md5值:md5sum</li>
<li>检测修复文件系统:fsck、e2fsck<pre><code>(单用户模式卸载文件系统后执行)
</code></pre></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hp@ubuntu:/$ df -h</span><br><span class="line"> 文件系统        容量  已用  可用 已用% 挂载点</span><br><span class="line"> /dev/loop0       15G  4.7G  9.0G   35% /</span><br><span class="line"> udev            3.0G  4.0K  3.0G    1% /dev</span><br><span class="line"> tmpfs           1.2G 1000K  1.2G    1% /run</span><br><span class="line"> none            5.0M  4.0K  5.0M    1% /run/lock</span><br><span class="line"> none            3.0G  804K  3.0G    1% /run/shm</span><br><span class="line"> /dev/sda8        21G   17G  3.3G   84% /host</span><br><span class="line"> /dev/sda7       103G   25G   79G   24% /media/Studty</span><br></pre></td></tr></table></figure>
<p>添加硬盘分区</p>
<ol>
<li>划分分区(fdisk)   </li>
<li>创建文件系统 (mkfs)</li>
<li>尝试挂载 (mount) [mount 物理设备名 挂载点(空目录)]</li>
<li>写入配置文件 (/etc/fstab)</li>
</ol>
<h2 id="5-Linux-文件系统目录结构"><a href="#5-Linux-文件系统目录结构" class="headerlink" title="5. Linux 文件系统目录结构"></a>5. Linux 文件系统目录结构</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/bin  : 基础系统所需要的最基础的命令就是放在这里。比如 ls、cp、mkdir等命令；</span><br><span class="line">         功能和/usr/bin类似，这个目录中的文件都是可执行的，普通用户都可以使用的命令。</span><br><span class="line">/boot : Linux的内核及引导系统程序所需要的文件。</span><br><span class="line">         启动装载文件存放位置，如kernels,initrd,grub。一般是一个独立的分区。</span><br><span class="line">/dev  :  一些必要的设备,声卡、磁盘等。还有如 /dev/null. /dev/console /dev/zero /dev/full 等。</span><br><span class="line">/etc  :  系统的配置文件存放地. </span><br><span class="line">/home :  用户工作目录，和个人配置文件，如个人环境变量等.</span><br><span class="line">/lib  :  库文件存放地。bin和sbin需要的库文件。类似windows的DLL。</span><br><span class="line">/media : 可拆卸的媒介挂载点，如CD-ROMs、移动硬盘、U盘，系统默认会挂载到这里来。</span><br><span class="line">/mnt  : 临时挂载文件系统。比如有cdrom 等目录。</span><br><span class="line">/opt  : 可选的应用程序包。 第三方软件</span><br><span class="line">/proc : 操作系统运行时，进程（正在运行中的程序）信息及内核信息（比如cpu、硬盘分区、内存信息等）存放在这里。</span><br><span class="line">/root : Root用户的工作目录</span><br><span class="line">/sbin : 和bin类似，是一些可执行文件，不过不是所有用户都需要的，一般是系统管理所需要使用得到的。</span><br><span class="line">/tmp  : 系统的临时文件，一般系统重启不会被保存。</span><br><span class="line">/usr  : 包含了系统用户工具和程序。</span><br><span class="line">/usr/bin ： 非必须的普通用户可执行命令</span><br><span class="line">/usr/include ： 标准头文件</span><br><span class="line">/usr/lib  : /usr/bin/ 和 /usr/sbin/的库文件</span><br><span class="line">/usr/sbin : 非必须的可执行文件</span><br><span class="line">/usr/src  : 内核源码</span><br><span class="line">/srv : 该目录存放一些服务启动之后需要提取的数据</span><br></pre></td></tr></table></figure>
<h2 id="6-VIM-编辑器"><a href="#6-VIM-编辑器" class="headerlink" title="6. VIM 编辑器"></a>6. VIM 编辑器</h2><table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th style="text-align:center">命令类型</th>
<th style="text-align:center">命令详情</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td style="text-align:center">[插入命令]</td>
<td style="text-align:center">a, A, i, I, o, O</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td style="text-align:center">[定位命令]</td>
<td style="text-align:center">h, j, k, l, <code>$</code>, 0, H, M, L  :set nu 设置行号，gg 到第一行 G 到最后一行</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td style="text-align:center">[删除命令]</td>
<td style="text-align:center">x, nx, dd, ndd, dG[光标处删除到文件尾], D, :n1, n2d[删除指定范围的行]</td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td style="text-align:center">[复制和剪切命令]</td>
<td style="text-align:center">yy, Y, nyy, nY,  dd[剪切]， ndd[剪切n行], p, P [粘贴在当前光标所在行下或行上]</td>
</tr>
<tr>
<td style="text-align:center">5.</td>
<td style="text-align:center">[替换和取消命令]</td>
<td style="text-align:center">r, R, u， r 取代光标所在处字符  R 从光标所在处开始替换字符，按Esc结束  u 取消上一步操作</td>
</tr>
<tr>
<td style="text-align:center"><br><br> 6.</td>
<td style="text-align:center"><br><br> [搜索和替换命令]</td>
<td style="text-align:center">/string 按 n  是下一个  从前向后， N 从后向前  <br><br> :%s/old/new/g  全文替换指定字符串 :n1,n2s/old/new/c 询问替换 <br><br> :set ic 搜索的时候就不区分大小写了！  :set noic</td>
</tr>
<tr>
<td style="text-align:center">7.</td>
<td style="text-align:center">[保存和退出命令]</td>
<td style="text-align:center">[ZZ 最常用 ] :wq 保存退出</td>
</tr>
<tr>
<td style="text-align:center">8.</td>
<td style="text-align:center">[vi 中另外比较有趣的命令]</td>
<td style="text-align:center">:r !命令  导入命令执行结果到当前vi中, 在vi中执行命令 :!命令</td>
</tr>
<tr>
<td style="text-align:center">9.</td>
<td style="text-align:center">分屏</td>
<td style="text-align:center">sp, vsp (水平)  ctrl+w *2</td>
</tr>
<tr>
<td style="text-align:center">10.</td>
<td style="text-align:center">定义快捷键</td>
<td style="text-align:center">:map 快捷键 触发命令  范例: : map ^P I#<esc>  ^P = ctrl+v+p  : map ^B 0x</esc></td>
</tr>
<tr>
<td style="text-align:center">11.</td>
<td style="text-align:center">vi 的配置文件</td>
<td style="text-align:center">~/.vimrc</td>
</tr>
</tbody>
</table>
<h2 id="7-引导流程"><a href="#7-引导流程" class="headerlink" title="7. 引导流程"></a>7. 引导流程</h2><ol>
<li>Linux引导流程  </li>
<li>Linux运行级别</li>
<li>Linux启动服务管理  </li>
<li>GRUB配置与应用</li>
</ol>
<p>系统引导流程:</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">固件 firmware(CMOS/BIOS) → POST 加电自检</span><br><span class="line">                ↓  [CMOS是固化在主板上的那段程序， BIOS 操作CMOS的那个界面]</span><br><span class="line">自举程序 BootLoader(GRUB) → 载入内核  linux-grub /etc/grub.conf / win-ntldr [nt内核代号,loader] bootini [里面记载了启动信息]</span><br><span class="line">                ↓  载入内核，OS的核心-内核[存储CPU文件进程...管理]-心脏大脑  指定linux内核存放的位置。ls /boot</span><br><span class="line">载入内核 Kernel → 驱动硬件</span><br><span class="line">                ↓ [内核只做两件事情，1驱动硬件2启动init. 内核保存最多的是驱动程序]</span><br><span class="line">启动进程 init</span><br><span class="line">                ↓ [init是第一个可以存在和启动的进程]</span><br><span class="line">读取执行配置文件/etc/inittab</span><br></pre></td></tr></table></figure>
</blockquote>
<p>引导流程说明:</p>
<blockquote>
<p>firmware自检之后，发现硬件们都没有什么问题之后，然后 firmware 读取 MBR[主引导记录]，位于 0柱面0磁头1扇区， 跳到 Master boot record 去读取数据。载入MBR 中一个很重要的数据叫做 Bootloader, 也称做自举程序或自启动程序。 下面是 Partition table 磁盘分区表，下面是 Magic Number 结束标志字</p>
<p>init 启动后读取 inittab文件, 执行缺省运行级别, 从而继续引导过程。在UNIX系统中, init时第一个可以存在的进程, 它的PID恒为1, 但它也必须向一个更高级的功能负责: PID为0的内核调度器(Kernelscheduler),从而获得CPU时间。</p>
</blockquote>
<p>扩展 ：</p>
<blockquote>
<p>在Linux里面不允许存在 孤儿进程，在linux系统中init是所有进程的父进程。<br>僵尸进程[Z]  儿子死了，父亲不知道，这个子进程就会变成 Z。</p>
</blockquote>
<p>Linux运行级别 0 ~ 6 说明 ：</p>
<blockquote>
<p>0 关机 1 字符单用户 2，3 字符界面的多用户模式[广泛使用的服务器的模式]<br>4 自定义 5 图形化的多用户 6 reboot</p>
</blockquote>
<h2 id="8-学习bash"><a href="#8-学习bash" class="headerlink" title="8. 学习bash"></a>8. 学习bash</h2><p>Linux bash东西非常多，包括变量的设置与使用，bash操作环境，数据流重定向功能，还有好用的管道命令。</p>
<blockquote>
<p>1, 硬件   2，内核程序   3，应用程序</p>
<p>man, chmod, chown, vi, fdisk, mkfs等命令，这些命令都是独立的应用程序。</p>
</blockquote>
<p>bash的主要优点 :</p>
<p>   (1) 命令记忆功能 (history)  .bash_history (注销系统后，命令记忆会记录到.bash_history)<br>   (2) 命令与文件补全功能 (tab)<br>   (3) 命令别名设置功能 (alias) alias lm=’ls -al’<br>   (4) 作业控制，前台，后台控制 (job control, foreground, background)<br>   (5) 程序脚本 (shell script)<br>   (6) 通配符 (Wildcard)</p>
<p>shell 变量:</p>
<blockquote>
<p>echo 变量显示  如 ： echo ${PATH}<br>父子进程 export 设置成环境变量..<br>unset 取消变量的设置值。 uname -r 显示内核版本<br>“version=$(uname -r)” 来替代 “version=<code>uname -r</code>“ 比较好。</p>
</blockquote>
<p>环境变量的功能:</p>
<blockquote>
<p>env 与 export 查看环境变量<br>HOME， SHELL， HISTSIZE， PATH， LANG(语系数据)， RANDOM(随机数发生器)<br>export 自定义变量转成环境变量 export name<br>子进程会定义父进程的环境变量，不会继承父进程的自定义变量</p>
</blockquote>
<p>bash 可以限制用户的某些系统资源，可打开的文件数量，可使用的CPU时间，可使用的内存总量等</p>
<blockquote>
<p>文件系统及程序的限制关系 : ulimit<br>ulimit -H -S -a -c -f -d -l -t -u -s 等。</p>
</blockquote>
<p>数据流重定向</p>
<blockquote>
<p>1, stdin  代码为 0 ，使用 &lt; 或 &lt;&lt;<br>2, stdout 代码为 1 ，使用 &gt; 或 &gt;&gt;<br>3, stderr 代码为 2 ，使用 2&gt; 或 2&gt;&gt;</p>
<p>/dev/null 垃圾桶黑洞设备<br> &amp;&amp; 与 ||<br> cut 命令 例子 ： echo $PATH | cut -d ‘:’ -f 3,5<br> export | cut -c 12-<br> cut 的主要用途在于将同一行里面的数据进行分解，用于分析一些日志。<br> grep -a -c -i -n -v ‘targe’ filename</p>
</blockquote>
<p>排序命令： sort, wc, uniq</p>
<blockquote>
<p>last | cut -d ‘ ‘ -f 1 | sort | uniq -c</p>
<p>hp@ubuntu:~$ last | tee last.list | cut -d “ “ -f1</p>
<p>tee 命令可以让 standard output 转存到一个文件内，并将同样的数据继续送到屏幕中去处理</p>
</blockquote>
<p>字符转换命令：  tr, col, join, paste, expand</p>
<blockquote>
<p> (1) tr :  last | tr ‘[a-z]’ ‘[A-Z]’<br>        ： cat /etc/passwd | tr -d ‘:’<br> (2) col -x 将tab键转换成对等的空格  -b 在文字内有 / 时，仅保留 / 后面接的那个字符<br>        ： cat /etc/man.config | col -x | cat -A | more<br> (3) join [-ti12] file1 file2 </p>
<p> (4) paste [-d] file1 file2<br>           ： -d : 后面可以接分隔字符，默认是以 [tab] 来分隔的。<br>           ： -  : 如果file部分写成 -, 代表来自 standard input 的数据的意思。<br> (5) expand 就是将 [tab] 按键转成空格键，可以这样做 ： expand [-t] file<br>           ：  expand -t 6 - | cat -A<br>           ： unexpand</p>
</blockquote>
<p>切割命令： split  参数 -b : 后面可接欲切割成的文件大小，可加单位 b, k, m 等；</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ split -b 300k  /etc/termcap termcap</span><br><span class="line">$ ll -k termcap*</span><br><span class="line">$ cat termcap* &gt;&gt; termcapback</span><br><span class="line">$ ls -al / | split <span class="_">-l</span> 10 - lsroot</span><br><span class="line">$ wc <span class="_">-l</span> lsroot*</span><br></pre></td></tr></table></figure>
<p>参数代换： xargs 可以读入stdin的数据，并且以空格符或断行字符进行分辨，将stdin的数据分隔成args</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hp@ubuntu:~$ cut <span class="_">-d</span> <span class="string">':'</span> <span class="_">-f</span>1 /etc/passwd | head -n 3 | xargs -p</span><br><span class="line">/bin/<span class="built_in">echo</span> root daemon bin ?...y</span><br><span class="line">root daemon bin</span><br><span class="line">hp@ubuntu:~$ <span class="built_in">echo</span> <span class="string">"3  4:asd"</span> | xargs </span><br><span class="line">3 4:asd</span><br></pre></td></tr></table></figure>
<p>关于减号 - 的用途</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tar -cvf - /home | tar -xvf - </span><br><span class="line"></span><br><span class="line">某些命令需要用到文件名来处理，该stdin, stdout可以利用减号“-”来替代。</span><br></pre></td></tr></table></figure>
<h2 id="9-进程管理"><a href="#9-进程管理" class="headerlink" title="9. 进程管理"></a>9. 进程管理</h2><p>进程的概念  进程管理命令  计划任务</p>
<h3 id="9-1-进程和程序的区别"><a href="#9-1-进程和程序的区别" class="headerlink" title="9.1 进程和程序的区别"></a>9.1 进程和程序的区别</h3><ol>
<li>程序是静态概念, 它作为一种软件资源长期保存; </li>
<li>进程是程序的执行过程, 它是动态概念, 有一定的生命期, 是动态产生和消亡的;</li>
<li>子进程是由一个进程所产生的进程,产生这个子进程的进程称为父进程;</li>
<li>在Linux系统中,使用系统调用fork创建进程。fork复制的内容包括父进程的数据和堆栈段以及父进程的进程环境;</li>
<li>父进程终止子进程自然终止.</li>
</ol>
<h3 id="9-2-前台进程和后台进程"><a href="#9-2-前台进程和后台进程" class="headerlink" title="9.2 前台进程和后台进程"></a>9.2 前台进程和后台进程</h3><p>前台进程:</p>
<blockquote>
<p> 在Shell提示处打入命令后, 创建一个子进程, 运行命令, Shell等待命令退出, 然后返回到对用户给出提示符。 这条命令与Shell异步运行, 即在前台运行, 用户在它完成之前不能执行另一个命令。</p>
</blockquote>
<p>后台进程: </p>
<blockquote>
<p> 在Shell提示处打入命令, 若后随一个&amp;, Shell创建的子进程运行此命令, 但不等待命令退出, 而直接返回到对用户给出提示。 这条命令与Shell同步运行, 即在后台运行。 后台进程必须是非交互式的。</p>
</blockquote>
<h3 id="9-3-进程状态"><a href="#9-3-进程状态" class="headerlink" title="9.3 进程状态"></a>9.3 进程状态</h3><table>
<thead>
<tr>
<th style="text-align:center">进程状态</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">就绪</td>
<td style="text-align:center">进程已经分配到资源,但因为其它进程正占用CPU。</td>
</tr>
<tr>
<td style="text-align:center">等待</td>
<td style="text-align:center">因等待某种事件而暂时不能运行的状态。</td>
</tr>
<tr>
<td style="text-align:center">运行</td>
<td style="text-align:center">进程分配到CPU,正在处理器上运行。</td>
</tr>
</tbody>
</table>
<p>进程状态细化</p>
<blockquote>
<p>用户态运行 : 在CPU上执行用户代码<br>核心态运行 : 在CPU上执行核心代码<br>在内存就绪 : 具备运行条件,只等调度程序为它分配CPU<br>在内存睡眠 : 因等待某一事件的发生,而在内存中排队等待<br>在外存就绪 : 就绪进程被交换到外存上继续处于就绪状态<br>在外存睡眠 : 睡眠进程被交换到外存上继续等待<br>在内存暂停 : 因调用stop程序而进入跟踪暂停状态,等待其父进程发送命令。<br>在外存暂停 : 处于跟踪暂停态的进程被交换到外存上</p>
</blockquote>
<p>创建态</p>
<blockquote>
<p>新进程正在被创建、但尚未完毕的中间状态</p>
</blockquote>
<p>终止态</p>
<blockquote>
<p>进程终止自己</p>
</blockquote>
<p><strong>查看用户信息 w (命令)</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hp@ubuntu:~$ w</span><br><span class="line"> 22:02:45 up 22 min,  2 users,  load average: 0.27, 0.27, 0.28</span><br><span class="line">USER     TTY      FROM              LOGIN@   IDLE   JCPU   PCPU  WHAT</span><br><span class="line">hp       tty7                      21:40    22:33   1:16   0.34s gnome-session -</span><br><span class="line">hp       pts/0    :0               22:02    0.00s   0.37s  0.01s w</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:center">选项</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">JCPU</td>
<td style="text-align:center">以终端代号来区分,该终端所有相关的进程执行时,所消耗的CPU时间会显示在这里</td>
</tr>
<tr>
<td style="text-align:center">PCPU</td>
<td style="text-align:center">CPU执行程序耗费的时间</td>
</tr>
<tr>
<td style="text-align:center">WHAT</td>
<td style="text-align:center">用户正在执行的操作查看个别用户信息:w 用户名</td>
</tr>
<tr>
<td style="text-align:center">load average</td>
<td style="text-align:center">分别显示系统在过去1、5、15分钟内的平均负载程度。</td>
</tr>
<tr>
<td style="text-align:center">FROM</td>
<td style="text-align:center">显示用户从何处登录系统,“:0”的显示代表该用户时从X Window下,打开文本模式窗口登录的</td>
</tr>
<tr>
<td style="text-align:center">IDLE</td>
<td style="text-align:center">用户闲置的时间。这是一个计时器,一旦用户执行任何操作,该计时器便会被重置</td>
</tr>
</tbody>
</table>
<p>查看系统中的进程 ps 常用选项</p>
<blockquote>
<p>a : 显示所有用户的进程<br>u : 显示用户名和启动时间<br>x : 显示没有控制终端的进程<br>e : 显示所有进程,包括没有控制终端的进程<br>l : 长格式显示<br>w : 宽行显示,可以使用多个w进行加宽显示</p>
</blockquote>
<p>ps 常用输出信息的含义</p>
<blockquote>
<p>TIME: 进程自从启动以来启用CPU的总时间<br>COMMAND/CMD: 进程的命令名<br>USER:用户名<br>%CPU:占用CPU时间和总时间的百分比<br>%MEM:占用内存与系统内存总量的百分比</p>
</blockquote>
<p>ps 应用实例</p>
<blockquote>
<p>ps 查看隶属于自己的进程<br>ps -u or -l 查看隶属于自己进程详细信息<br>ps -le or -aux 查看所有用户执行的进程的详细信息<br>ps -aux –sort pid 可按进程执行的时间、<br>    PID、UID等对进程进行排序</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ps -aux | grep sam</span></span><br><span class="line"><span class="comment"># ps -uU sam 查看系统中指定用户执行的进程</span></span><br><span class="line"><span class="comment"># ps -le | grep init 查看指定进程信息</span></span><br><span class="line"><span class="comment"># pstree</span></span><br></pre></td></tr></table></figure>
<h3 id="9-4-kill-–-杀死进程"><a href="#9-4-kill-–-杀死进程" class="headerlink" title="9.4 kill – 杀死进程"></a>9.4 kill – 杀死进程</h3><ol>
<li>为什么要杀死进程</li>
<li>该进程占用了过多的CPU时间</li>
<li>该进程缩住了一个终端,使其他前台进程无法运行</li>
<li>运行时间过长,但没有预期效果</li>
<li>产生了过多到屏幕或磁盘文件的输出</li>
<li>无法正常退出, 关闭进程:kill 进程号</li>
</ol>
<blockquote>
<p>1). kill -9 进程号(强行关闭)  kill -s 9 进程号 [前简化]<br>2). kill -1 进程号(重启进程)<br>3). 关闭图形程序: xkill<br>4). 结束所有进程: killall<br>5). 查找服务进程号: pgrep 服务名称<br>6). 关闭进程: pkill 进程名称</p>
</blockquote>
<p>启动的程序 stop 也可以关闭 , 重启 /etc/rc.d/init.d/httpd restart</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat/proc/cpuinfo</span></span><br><span class="line"><span class="comment"># pgrep httpd 检测但它所有进程的 pid</span></span><br><span class="line"><span class="comment"># pkill httpd 也可以关闭，很方便</span></span><br></pre></td></tr></table></figure>
<h3 id="9-5-进程优先级，挂起与恢复"><a href="#9-5-进程优先级，挂起与恢复" class="headerlink" title="9.5 进程优先级，挂起与恢复"></a>9.5 进程优先级，挂起与恢复</h3><p>nice</p>
<blockquote>
<p> 指定程序的运行优先级<br> 格式:nice -n command<br> 例如:nice -5 myprogram</p>
</blockquote>
<p>renice</p>
<blockquote>
<p> 改变一个正在运行的进程的优先级<br> 格式:renice n pid<br> 例如:renice -5 777<br> 优先级取值范围为(-20,19)</p>
</blockquote>
<p>nohup</p>
<blockquote>
<p>使进程在用户退出登陆后仍旧继续执行,nohup命令将执行后的数据信息和<br>错误信息默认储存到文件 nohup.out 中<br>格式: nohup program &amp;</p>
</blockquote>
<p>进程的挂起和恢复</p>
<blockquote>
<p>进程的中止(挂起)和终止</p>
<p>  挂起(Ctrl+Z)[类似差不多暂停]<br>  终止(Ctrl+C)</p>
</blockquote>
<p>进程的恢复</p>
<blockquote>
<p>  恢复到前台继续运行(fg)   复到后台继续运行(bg)   查看被挂起 /后台的进程(jobs)</p>
</blockquote>
<h3 id="9-6-top-作用"><a href="#9-6-top-作用" class="headerlink" title="9.6 top 作用"></a>9.6 top 作用</h3><p>进程状态显示和进程控制,每5秒钟自动刷新一次(动态显示)</p>
<blockquote>
<p>常用选项:</p>
<p> d : 指定刷新的时间间隔<br> c : 显示整个命令行而不仅仅显示命令名</p>
</blockquote>
<p>top常用命令:</p>
<blockquote>
<p> u : 查看指定用户的进程<br> k : 终止执行中的进程<br> h or ?:获得帮助<br> r : 重新设置进程优先级<br> s : 改变刷新的时间间隔<br> W : 将当前设置写入~/.toprc文件中</p>
</blockquote>
<h3 id="9-7-计划任务"><a href="#9-7-计划任务" class="headerlink" title="9.7 计划任务"></a>9.7 计划任务</h3><ul>
<li>at 安排作业在某一时刻执行一次</li>
<li>batch 安排作业在系统负载不重时执行一次</li>
<li>cron 安排周期性运行的作业</li>
</ul>
<p>at命令的功能和格式</p>
<blockquote>
<p> 功能: 安排一个或多个命令在指定的时间运行一次<br> at 的命令格式及参数<br> at [-f 文件名] 时间 / at -d or atrm  删除队列中的任务 / at -l or atq 查看队列中的任务</p>
</blockquote>
<p>进程处理方式</p>
<blockquote>
<p>standalone 独立运行  xinetd 进程托管  atd、crond 计划任务</p>
</blockquote>
<h2 id="Linux-书架"><a href="#Linux-书架" class="headerlink" title="Linux 书架"></a>Linux 书架</h2><ul>
<li>入门类 ：《鸟哥的Linux私房菜》</li>
<li>编程类 ：《Advanced Linux Programming》-&gt; 《Advanced Programming in the UNIX Environment》</li>
<li>内核类 ：《Linux Kernel Development》</li>
<li>工具类 ：《Handbook of Open Source Tools》</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java SE Learning Notes for Exception]]></title>
      <url>http://sggo.me/2013/07/25/java/java-se-2.4-notes-exception/</url>
      <content type="html"><![CDATA[<p>Java SE <code>异常处理</code>部分的概要笔记: <code>手中无剑</code>, <code>心中有剑</code>.</p>
<a id="more"></a> 
<h2 id="第四章-异常处理"><a href="#第四章-异常处理" class="headerlink" title="第四章 异常处理"></a>第四章 异常处理</h2><ul>
<li><p>Java 异常是 Java 提供的用于处理程序中错误的一种机制。<br>java.lang….Exceptions<br>写程序有问题要有友好界面<br>医生开单子 {<br>  1, 鼻腔内感觉异常<br>  2, 体温持续升高<br>  3, 分泌乳白色液体<br>  直接说感冒不就得了么？<br>}<br>e.printStackTrace(); 非常好！给程序员读。堆栈信息都打印出来！  </p>
<p>java.lang.Throwable { 开车在上山走，  </p>
<pre><code>1, Error         山爆发 JVM 出问题。  
2, Exception {   你可以处理的 -- 刹车坏啦！修好再走。。。  
  1, ...  
  2, RuntimeException  (经常出，不用逮) 压路面上的小石子  
</code></pre><p>  }<br>一个 try 可以跟多个catch<br>所以 { 一个茶壶可以跟多个茶碗，一个男人可以三妻四妾。}<br>try {<br>  // 可能抛出异常的语句<br>  语句一；<br>  语句二；<br>} catch(someEx e) {<br>  语句；<br>  }<br>  catch() {  </p>
<pre><code>语句  
</code></pre><p>  }<br>  finally {<br>  }<br>一 ： 打开<br>二 ： 关闭<br>finally : 一般进行资源的清除工作。。。！  </p>
</li>
</ul>
<p>我处理不了的事情 ： 我交给上一级部门去处理！<br>当时 catch 到 Ex 的时候，你至少要做出一种处理。要不那是危险的编程习惯！<br>main() 抛出 就是交给 java 运行时系统啦！ 它会把堆栈信息打出来！  </p>
<p>一个图 ： 五个关键字 {<br>    try, catch, finally, throw, throws<br>}<br>一点问题 {<br>    先逮大的，后逮小的，报错。<br>}<br>使用自定义异常  </p>
<p>程序中可以使用 throw - 方法后 throws<br>如果throw抛出异常之后,方法就结束啦！  </p>
<p>注意 ： 重写方法需要抛出与原方法所抛出异常类型一致异常或不抛出异常。  </p>
<ul>
<li>总结 ：{  <ul>
<li>一个图  </li>
<li>五个关键字  </li>
<li>先逮小的，再逮大的。  </li>
<li>异常与重写的关系<br>}  </li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://blog.csdn.net/robbyo/article/category/1328994/14" target="_blank" rel="external">csdn robbyo java</a>            </li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java SE Learning Notes for OO]]></title>
      <url>http://sggo.me/2013/07/24/java/java-se-2.3-notes-oo/</url>
      <content type="html"><![CDATA[<p>Java SE <code>面向对象</code>部分的概要笔记: <code>手中无剑</code>, <code>心中有剑</code>.</p>
<p>定义类  生成对象  ， 定义方法  被调用 </p>
<a id="more"></a> 
<h2 id="1-static"><a href="#1-static" class="headerlink" title="1. static"></a>1. static</h2><p><code>static var</code> 知道了内存，你就知道了一切.  </p>
<ul>
<li>局部变量 分配在 <strong>stack</strong> memory</li>
<li>成员变量 分配在 <strong>heap</strong> memory</li>
</ul>
<p><code>static var</code> 为类对象共享的变量 在数据区  </p>
<ul>
<li>非静态变量 专属于某一个对象  </li>
<li>静态方法不再是针对某一个对象进行调用, 所以不能访问非静态成员。  </li>
</ul>
<h2 id="2-package"><a href="#2-package" class="headerlink" title="2. package"></a>2. package</h2><p>包名起名方法 ： 公司域名倒过来.</p>
<ul>
<li>必须保证该类 class 文件位于正确的目录下.  </li>
<li>必须class文件的最上层包的父目录位于classpath下.  </li>
<li>执行一个类需要写全包名.  </li>
</ul>
<p><strong>SDK 主要的包介绍</strong>  </p>
<pre><code>* java.lang - 包含一些 java 语言的核心类， 如 : String, Math, Integer, System, Thread.  
* java.net  - 包含执行与网络相关的操作的类  
* java.io   - 包含能提供多种输入/输出功能的类  
* java.util - 包含一些实用工具类.  
</code></pre><h2 id="3-类的继承与权限控制"><a href="#3-类的继承与权限控制" class="headerlink" title="3. 类的继承与权限控制"></a>3. 类的继承与权限控制</h2><pre><code>* 修饰符      类内部     同一包内      子类     任何地方  
* private      Yes  
* default      Yes      Yes  
* protected    Yes      Yes         Yes  
* public       Yes      Yes         Yes       Yes    
</code></pre><blockquote>
<p>分析内存 : 子类对象包含一个父类对象.<br>重写方法不能使用比被重写方法更严格的访问权限 – 其实这和多态有关  </p>
</blockquote>
<p>super 关键字  &amp; 继承中的构造方法</p>
<pre><code>如果调用 super 必须写在构造方法的第一行  
如果没调用，系统自动调用 super(), 如果没调，父类中又没写参数为空这个构造方法则出错。  
</code></pre><ul>
<li><p>Object 类</p>
<pre><code>instanceof 是一个操作符  
equals方法 J2SDK 提供的一些类 如 String , Date 重写了Object 的 equals() 方法.  
</code></pre></li>
<li><p>对象转型 casting</p>
<pre><code>* 一个基类的引用类型变量可以指向 “其子类的对象”。  
* 一个基类的引用不可以访问其子类新增加的成员  
* 可以使用 引用 变量 instanceof 类名 来判断该引用型变量所&quot;指向&quot;的对象是否属于该类或该类的子类。  
* upcasting / downcasting  

    内存分析 - 明白了内存你就明白了一切！  
</code></pre></li>
</ul>
<ul>
<li><p>动态绑定, 池绑定, 多态</p>
<pre><code>* 动态绑定的机制 是 实际类型 new 的是！  
* 深一点 -- 是对象内部有一个指针。。。。。。  
* 动态绑定的机制是 ： 实际类型，还是引用类型。是调用实际类型，不是引用类型。  

* 实际地址才会绑定到那个方法上。 方法在  code segment  
* 只有在运行期间(不是在编译期间)，运行出对象来，才能判断调用哪一个。。。。  
</code></pre></li>
</ul>
<blockquote>
<p>这是面向对象核心中的核心。核心中的核心 ! 带来的莫大好处: 可扩展性达到了非常非常的极致好！</p>
<p>多态总结:</p>
<ol>
<li>要有<strong>继承</strong>  </li>
<li>要有<strong>重写</strong>  </li>
<li>父类引用指向子类对象  </li>
</ol>
</blockquote>
<h2 id="4-Abstract-class"><a href="#4-Abstract-class" class="headerlink" title="4. Abstract class"></a>4. Abstract class</h2><ul>
<li>abstract 修饰class时，这个类叫做抽象类.</li>
<li>abstract 修饰方法时，该方法叫做抽象方法.</li>
<li>abstract class 必须被继承，抽象方法必须被重写. 含有抽象方法的类必须声明为抽象类.</li>
<li>abstract class 不能被实例化, 抽象方法只需声明，而不需要实现.</li>
</ul>
<h2 id="5-Final-关键字"><a href="#5-Final-关键字" class="headerlink" title="5. Final 关键字"></a>5. Final 关键字</h2><ul>
<li>final 的变量的值不能够被改变. final 的成员变量、局部变量(形参)</li>
<li><p>final 的方法不能够被重写， final 的类不能够被继承.</p>
</li>
<li><p>系统中的 final class 例如： </p>
<pre><code>public final class String  
public final class Math  
public final class Boolean
</code></pre></li>
</ul>
<h2 id="6-interface-一种特殊的抽象类"><a href="#6-interface-一种特殊的抽象类" class="headerlink" title="6. interface: 一种特殊的抽象类"></a>6. interface: 一种特殊的抽象类</h2><ul>
<li>变量全是: public static final int id = 1;</li>
<li>方法全是: abstract function()  </li>
</ul>
<ul>
<li>java.lang - Comparable 我看就像 cmp 一样！(个人认为)  <pre><code>Interface Comparable&lt;T&gt; 可以扩展  
</code></pre></li>
</ul>
<ul>
<li><strong>interface</strong> <strong>interface</strong> 可以相互继承  </li>
<li><strong>class</strong> <strong>interface</strong> 只能是 implement 关系  </li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://blog.csdn.net/robbyo/article/category/1328994/14" target="_blank" rel="external">csdn robbyo java</a>            </li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java SE Learning Notes for Hello World]]></title>
      <url>http://sggo.me/2013/02/02/java/java-se-2.2-notes-app/</url>
      <content type="html"><![CDATA[<p>第一个 Java 程序 HelloWorld.java.  <code>手中无剑</code>, <code>心中有剑</code>.</p>
<a id="more"></a>
<h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><p>HelloWorld.java</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class HelloWorld &#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    System.out.println(&quot;Hello Java.&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line">  * 这里是注释</span><br><span class="line">  */</span><br></pre></td></tr></table></figure>
<p>一个源文件中最多只能有一个public类. 如果源文件 文件包含一个public class 它必需按该 class_name 命名  </p>
<h2 id="Java-程序设计"><a href="#Java-程序设计" class="headerlink" title="Java 程序设计"></a>Java 程序设计</h2><p><strong>data type</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                                      -- 整数类型 (byte, short, int, long)  </span><br><span class="line">                          -- 数值型 --     </span><br><span class="line">                         |            -- 浮点类型 (float, double)  </span><br><span class="line">           --基本数据类型  -- 字符型 (char)  </span><br><span class="line">          |              |  </span><br><span class="line">          |               -- 布尔型 (boolean)  </span><br><span class="line">数据类型 --                           </span><br><span class="line">          |               -- 类 (class)  </span><br><span class="line">          |              |  </span><br><span class="line">           --引用数据类型  -- 接口 (interface)  </span><br><span class="line">                         |  </span><br><span class="line">                          -- 数组 (array)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>java 中定义了 <strong>4类 8种</strong> 基本数据类型<br>boolean 类型只允许取值 true / false , 不可以用 0 或 非0 替代。<br>char 采用 Unicode 编码 (全球语言统一编码), 每个字符占两个字节  </p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java SE Learning Notes for Environment]]></title>
      <url>http://sggo.me/2013/02/02/java/java-se-2.1-notes-env/</url>
      <content type="html"><![CDATA[<p>Java 环境的搭建 <code>手中无剑</code>, <code>心中有剑</code>.</p>
<a id="more"></a>
<h2 id="1-JDK"><a href="#1-JDK" class="headerlink" title="1. JDK"></a>1. JDK</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MS=/usr/<span class="built_in">local</span>/xsoft/software</span><br><span class="line"></span><br><span class="line"><span class="comment">### JAVA ###</span></span><br><span class="line">JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk/Contents/Home</span><br><span class="line">JAVA_BIN=<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line">PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/jre/lib/rt.jar:<span class="variable">$JAVA_HOME</span>/jre/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/jre/lib/tools.jar</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME JAVA_BIN PATH CLASSPATH</span><br></pre></td></tr></table></figure>
<h2 id="2-Maven"><a href="#2-Maven" class="headerlink" title="2. Maven"></a>2. Maven</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">### Maven ###</span></span><br><span class="line">M2_HOME=/usr/<span class="built_in">local</span>/xsoft/software/apache-maven</span><br><span class="line">MAVEN_HOME=<span class="variable">$M2_HOME</span></span><br><span class="line">M3_HOME=<span class="variable">$M2_HOME</span></span><br><span class="line">PATH=<span class="variable">$M3_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> MAVEN_HOME M2_HOME PATH</span><br><span class="line"><span class="comment">#MAVEN_OPTS=-Xms128m -Xmx512m</span></span><br></pre></td></tr></table></figure>
<h2 id="3-Tomcat"><a href="#3-Tomcat" class="headerlink" title="3. Tomcat"></a>3. Tomcat</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">### Tomcat ###</span></span><br><span class="line">CATALINA_HOME=/usr/<span class="built_in">local</span>/xsoft/software/apache-tomcat</span><br><span class="line">PATH=<span class="variable">$CATALINA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> CATALINA_HOME PATH</span><br></pre></td></tr></table></figure>
<h2 id="4-IDE"><a href="#4-IDE" class="headerlink" title="4. IDE"></a>4. IDE</h2><ul>
<li><a href="https://www.jetbrains.com/idea/" target="_blank" rel="external">Intellij IDEA</a></li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="/2017/10/21/ops-zsh-config/">Blair Zsh Config</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java SE Learning Notes for All]]></title>
      <url>http://sggo.me/2013/02/02/java/java-se-2.5-notes-all/</url>
      <content type="html"><![CDATA[<p>这篇主要是记录了我学习 Java SE 的概要笔记: <code>手中无剑</code>, <code>心中有剑</code>, <code>摘花飞叶可以伤人</code>.</p>
<a id="more"></a>
<h2 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1. 环境配置"></a>1. 环境配置</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">### JAVA ###</span></span><br><span class="line">JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk/Contents/Home</span><br><span class="line">JAVA_BIN=<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line">PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/jre/lib/rt.jar:<span class="variable">$JAVA_HOME</span>/jre/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/jre/lib/tools.jar</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME JAVA_BIN PATH CLASSPATH</span><br></pre></td></tr></table></figure>
<h2 id="2-基础"><a href="#2-基础" class="headerlink" title="2. 基础"></a>2. 基础</h2><p>分析设计 ：  </p>
<pre><code>1. 这个东西有 哪些类  
2. 类的 属性和方法  
3. 类与类之间的关系  
4. 实现  
</code></pre><h2 id="3-面向对象"><a href="#3-面向对象" class="headerlink" title="3. 面向对象"></a>3. 面向对象</h2><ul>
<li>定义类  生成对象 </li>
<li>定义方法  被调用  </li>
</ul>
<h3 id="3-1-static"><a href="#3-1-static" class="headerlink" title="3.1 static"></a>3.1 static</h3><p><code>static var</code> 知道了内存，你就知道了一切.  </p>
<ul>
<li>局部变量 分配在 <strong>stack</strong> memory</li>
<li>成员变量 分配在 <strong>heap</strong> memory</li>
</ul>
<p><code>static var</code> 为类对象共享的变量 在数据区  </p>
<ul>
<li>非静态变量 专属于某一个对象  </li>
<li>静态方法不再是针对某一个对象进行调用, 所以不能访问非静态成员。  </li>
</ul>
<h3 id="3-2-package"><a href="#3-2-package" class="headerlink" title="3.2 package"></a>3.2 package</h3><p>包名起名方法 ： 公司域名倒过来.</p>
<ul>
<li>必须保证该类 class 文件位于正确的目录下.  </li>
<li>必须class文件的最上层包的父目录位于classpath下.  </li>
<li>执行一个类需要写全包名.  </li>
</ul>
<p><strong>SDK 主要的包介绍</strong>  </p>
<pre><code>* java.lang - 包含一些 java 语言的核心类， 如 : String, Math, Integer, System, Thread.  
* java.net  - 包含执行与网络相关的操作的类  
* java.io   - 包含能提供多种输入/输出功能的类  
* java.util - 包含一些实用工具类.  
</code></pre><h3 id="3-3-类的继承与权限控制"><a href="#3-3-类的继承与权限控制" class="headerlink" title="3.3 类的继承与权限控制"></a>3.3 类的继承与权限控制</h3><pre><code>* 修饰符      类内部     同一包内      子类     任何地方  
* private      Yes  
* default      Yes      Yes  
* protected    Yes      Yes         Yes  
* public       Yes      Yes         Yes       Yes    
</code></pre><blockquote>
<p>分析内存 : 子类对象包含一个父类对象.<br>重写方法不能使用比被重写方法更严格的访问权限 – 其实这和多态有关  </p>
</blockquote>
<p>super 关键字  &amp; 继承中的构造方法</p>
<pre><code>如果调用 super 必须写在构造方法的第一行  
如果没调用，系统自动调用 super(), 如果没调，父类中又没写参数为空这个构造方法则出错。  
</code></pre><ul>
<li><p>Object 类</p>
<pre><code>instanceof 是一个操作符  
equals方法 J2SDK 提供的一些类 如 String , Date 重写了Object 的 equals() 方法.  
</code></pre></li>
<li><p>对象转型 casting</p>
<pre><code>* 一个基类的引用类型变量可以指向 “其子类的对象”。  
* 一个基类的引用不可以访问其子类新增加的成员  
* 可以使用 引用 变量 instanceof 类名 来判断该引用型变量所&quot;指向&quot;的对象是否属于该类或该类的子类。  
* upcasting / downcasting  

    内存分析 - 明白了内存你就明白了一切！  
</code></pre></li>
</ul>
<ul>
<li><p>动态绑定, 池绑定, 多态</p>
<pre><code>* 动态绑定的机制 是 实际类型 new 的是！  
* 深一点 -- 是对象内部有一个指针。。。。。。  
* 动态绑定的机制是 ： 实际类型，还是引用类型。是调用实际类型，不是引用类型。  

* 实际地址才会绑定到那个方法上。 方法在  code segment  
* 只有在运行期间(不是在编译期间)，运行出对象来，才能判断调用哪一个。。。。  
</code></pre></li>
</ul>
<blockquote>
<p>这是面向对象核心中的核心。核心中的核心 ! 带来的莫大好处: 可扩展性达到了非常非常的极致好！</p>
<p>多态总结:</p>
<ol>
<li>要有<strong>继承</strong>  </li>
<li>要有<strong>重写</strong>  </li>
<li>父类引用指向子类对象  </li>
</ol>
</blockquote>
<h3 id="3-4-Abstract-class"><a href="#3-4-Abstract-class" class="headerlink" title="3.4 Abstract class"></a>3.4 Abstract class</h3><ul>
<li>abstract 修饰class时，这个类叫做抽象类.</li>
<li>abstract 修饰方法时，该方法叫做抽象方法.</li>
<li>abstract class 必须被继承，抽象方法必须被重写. 含有抽象方法的类必须声明为抽象类.</li>
<li>abstract class 不能被实例化, 抽象方法只需声明，而不需要实现.</li>
</ul>
<h3 id="3-5-Final-关键字"><a href="#3-5-Final-关键字" class="headerlink" title="3.5 Final 关键字"></a>3.5 Final 关键字</h3><ul>
<li>final 的变量的值不能够被改变. final 的成员变量、局部变量(形参)</li>
<li><p>final 的方法不能够被重写， final 的类不能够被继承.</p>
</li>
<li><p>系统中的 final class 例如： </p>
<pre><code>public final class String  
public final class Math  
public final class Boolean
</code></pre></li>
</ul>
<h3 id="3-6-接口-一种特殊的抽象类"><a href="#3-6-接口-一种特殊的抽象类" class="headerlink" title="3.6 接口: 一种特殊的抽象类"></a>3.6 接口: 一种特殊的抽象类</h3><ul>
<li>变量全是: public static final int id = 1;</li>
<li>方法全是: abstract function()  </li>
</ul>
<ul>
<li>java.lang - Comparable 我看就像 cmp 一样！(个人认为)  <pre><code>Interface Comparable&lt;T&gt; 可以扩展  
</code></pre></li>
</ul>
<ul>
<li><strong>interface</strong> <strong>interface</strong> 可以相互继承  </li>
<li><strong>class</strong> <strong>interface</strong> 只能是 implement 关系  </li>
</ul>
<h2 id="第四章-异常处理"><a href="#第四章-异常处理" class="headerlink" title="第四章 异常处理"></a>第四章 异常处理</h2><ul>
<li><p>Java 异常是 Java 提供的用于处理程序中错误的一种机制。<br>java.lang….Exceptions<br>写程序有问题要有友好界面<br>医生开单子 {<br>  1, 鼻腔内感觉异常<br>  2, 体温持续升高<br>  3, 分泌乳白色液体<br>  直接说感冒不就得了么？<br>}<br>e.printStackTrace(); 非常好！给程序员读。堆栈信息都打印出来！  </p>
<p>java.lang.Throwable { 开车在上山走，  </p>
<pre><code>1, Error         山爆发 JVM 出问题。  
2, Exception {   你可以处理的 -- 刹车坏啦！修好再走。。。  
  1, ...  
  2, RuntimeException  (经常出，不用逮) 压路面上的小石子  
</code></pre><p>  }<br>一个 try 可以跟多个catch<br>所以 { 一个茶壶可以跟多个茶碗，一个男人可以三妻四妾。}<br>try {<br>  // 可能抛出异常的语句<br>  语句一；<br>  语句二；<br>} catch(someEx e) {<br>  语句；<br>  }<br>  catch() {  </p>
<pre><code>语句  
</code></pre><p>  }<br>  finally {<br>  }<br>一 ： 打开<br>二 ： 关闭<br>finally : 一般进行资源的清除工作。。。！  </p>
</li>
</ul>
<p>我处理不了的事情 ： 我交给上一级部门去处理！<br>当时 catch 到 Ex 的时候，你至少要做出一种处理。要不那是危险的编程习惯！<br>main() 抛出 就是交给 java 运行时系统啦！ 它会把堆栈信息打出来！  </p>
<p>一个图 ： 五个关键字 {<br>    try, catch, finally, throw, throws<br>}<br>一点问题 {<br>    先逮大的，后逮小的，报错。<br>}<br>使用自定义异常  </p>
<p>程序中可以使用 throw - 方法后 throws<br>如果throw抛出异常之后,方法就结束啦！  </p>
<p>注意 ： 重写方法需要抛出与原方法所抛出异常类型一致异常或不抛出异常。  </p>
<ul>
<li>总结 ：{  <ul>
<li>一个图  </li>
<li>五个关键字  </li>
<li>先逮小的，再逮大的。  </li>
<li>异常与重写的关系<br>}  </li>
</ul>
</li>
</ul>
<h2 id="5-数组"><a href="#5-数组" class="headerlink" title="5. 数组"></a>5. 数组</h2><ul>
<li>四维空间<br>int[] a, int a[];<br>内存分析 - 知道了内存你就明白了一切！<br>本来无一物 ： 何处装数组。  </li>
<li>动态初始化  </li>
<li><p>静态初始化 int a[] = {3, 9, 8}; 内部过程屏蔽掉啦！  </p>
<p>ipconfig<br>ipconfig -all  这里 -all 就是命令行参数。  </p>
<p>基础类型一般分配在栈上面！包装类，用于把基础类型包装成对象，则分配在堆上了。<br>例如 类 Double, Integer  </p>
<p>约瑟夫环 - 面向过程 和 面向对象 写法  </p>
<p>另一个比较精巧的算法 ： 用数组来模拟链表<br>算法和逻辑思维能力不是一朝一夕能完成的。<br>排序算法紧跟着的是 - 搜索算法  </p>
</li>
</ul>
<hr>
<p>  你这里是通过对象square1调用的方法getsquare()<br>    public static void main(String[] args){<br>        getsquare(); //这里会出错<br>    }<br>    是的。其实main函数可以简单认为跟本类没什么关系，只是调用本类的<br>    其它静态方法时不用写类名而已。所以，要调用其它非静态方法，都要<br>    先实例化，就像别的类来调用一样。<br>——– 我有些懂啦！ 但还是不太懂，我能理解啦！ (个人理解)——–  </p>
<pre><code>二维数组 ： 可以看成以数组为元素的数组  
* 数组的拷贝 {  
}  
大公司剥削人剥削得最厉害！  
</code></pre><p>  明白内存你就明白了一切！。。。  </p>
<p>  总结 {  </p>
<pre><code>* 数组内存的布局  
* 常见算法  
</code></pre><p>  }<br>}  </p>
<h2 id="6-常用类"><a href="#6-常用类" class="headerlink" title="6. 常用类"></a>6. 常用类</h2><p>本章内容 {  </p>
<pre><code>* 字符串相关类 (String, StringBuffer)  
* 基本数据类型包装类  
* Math类  
* File类  
* 枚举类  

* java.lang.String 类代表不可变的字符序列  

    String s1 = &quot;helo&quot;;  
    String s3 = &quot;hello&quot;;  
    s1 == s3 true  
    字符串常量 - data seg 区  
    data segment 编译器有优化  

    如果是 new s1 == s3 false  
          s1.equals(s3) true  
    字符串你可以看成是一个字符数组！  

    String 类常用方法 {  
        * 静态重载方法 public static String valueOf(...)  
        * public String[] spllit(String regex)  
    }  
    String.valueOf(Object obj);  多态的存在  
    toString  
    java.lang.StringBuffer 代表可变的字符序列  

    * 基本数据类型包装类  
        基本数据 ： 栈  -&gt; 包装 --&gt; 堆上面  
        * 包装类 {  
            * 写程序要循序渐进方法  
        }  
    * Math 类 { java.lang.Math 其中方法的参数和返回值类型都为 double }  
    * File 类 { java.io.File 类代表系统文件名 (路径和文件名)   
            File 类的常见构造方法 ：  
            * public File(String pathname)     
                以 pathname 为路径创建 File 对象, 如果 pathname 是相对路径，则默认的当前路径在系统属性 user.dir 中存储  
            * public File(String parent, String child)  
            * File 的静态属性 String separator 存储了当前系统的路径分隔符。  
                原型 ： public static final String separator 但是事实上无论在哪 你写 / 都没有问题  

                注意 \ 在 java 里面是转义字符  
        }  
     * Enum - java.lang.Enum 枚举类型 {  
            1, 只能够取特定值中的一个   
            2, 使用 enum 关键字  
            3, 是 java.lang.Enum  
            4, 举例 ： TestEnum.java  
        }  
总结~~~ API 和 金庸的书差不多！  
</code></pre><h2 id="7-容器"><a href="#7-容器" class="headerlink" title="7. 容器"></a>7. 容器</h2><pre><code>第七章 容器 {  
    * 容器的概念       -        数组是么， 当然是！  
    * 容器API  
    * Collection 接口  
    * Iterator 接口  
    * 增强的 for 循环  
    * Set 接口  
    * List接口 和 Comparable接口  
    * Collections 类  
    * Map 接口  
    * 自动打包 / 解包  
    * 泛型 (JDK1.5新增)  
    -----  
    * J2SDk 所提供的容器位于 java.util 包内。  
    * 容器API的类图如下图所示：  
    --------------------------------------------------  


    1136 1136 1136   --  一个图, 一个类, 三个知识点，六个接口  

           &lt;&lt;interface&gt;&gt;  
            Collection  
           /           \                        &lt;&lt;interface&gt;&gt;  
</code></pre><p> &lt;<interface>&gt;      &lt;<interface>&gt;                 ^<br>      Set                List                     |<br>       ^                  ^                       |<br>       |           <strong>___</strong>|<strong>__</strong>                 |<br>         HashSet   LinkedList    ArrayList          HashMap  </interface></interface></p>
<pre><code>1136  1136  1136  一个图, 一个类, 三个知识点，六个接口  

---------------------------------------------------  


* Collection 接口 -- 定义了存取一组对象的方法, 其子接口 Set 和 List 分别定  
    义了存储方式。  

    * Set 中的数据对象没有顺序且不可以重复。  
    * List中的数据对象有顺序且可以重复  

* Map 接口定义了存储 “键 (key) -- 值 (value) 映射&quot;对&quot;的方法。  

Collection 方法举例  
    * 容器类对象在调用 remove, contains 等方法时需要比较对象是否相等  
        这会涉及到对象类型的 equals 方法和 hashCode 方法，对于自定义的  
        类型，需要要重写 equals 和 hashCode 方法以实现自定义的对象相等  
        规则。  
        *　注意 ： 相等的对象应该具有相等的 hashcodes  
    * ---  
    ArrayList 底层是一个数组  
</code></pre><p>哈哈哈哈哈 ： 装入的是对象，因为对象在堆上，栈里面的内容随时可能被清空！  </p>
<pre><code>    hashCode 能直接定位到那个对象  

    toyreb  

    Iterator - 接口最小化原则  
    我这大管家在做操作的时候 ： 连主人做任何的操作都不让操作啦！因为 iterater 执行了锁定，谁也不让谁看！  

    JDK 1.5 增强的 for 循环  

Set {  
    HashSet, TreeSet 一个以 hash 表实现， 一个以 树 结构实现  
}  
List {  
    Object set(int index, Object element)  
    int indexof(Object o);  
    int lastIndexof(Object o);  
}  
</code></pre><p><em>*</em> 一个类 {<br>    Collections  – java.util.Collections 提供了一些静态方法实现了基于List容器的一些常用算法  </p>
<pre><code>例如 {  
    void sort(List)  
    ...  
    ...  
    ...  
}  
LinkedList -- 逆序的时候效率较 ArrayList 高！  

对于 特定的 对象怎么确定谁大谁小。 {  
    对象间可以比较大小  
    通过 接口 只能看见对象的一点】  
    Comparable 接口  -- 所有可以实现排序的类 都实现了 Comparable 接口  
}  
public int compareTo(Object obj)  

泛型规定 - 只能传 “猫”  

vector / hashtable 以前遗留下来的。效率特别低  


Map 接口 {  
    Map 接口的实现类有 HashMap 和 TreeMap 等。 {hashmap 用 hash表来实现， TreeMap 用二叉树来实现-红黑}  
    Map 类中存储的键 - 值对通过键来标识，所以键值不能重复。{  
        不能重复 ： 是equals()  
                                equals() 太慢， 所以我们用 hashCode() 来比较  
                            }  
    }  
JDK 1.5 之后 {  
    可以自动打包和解包  
</code></pre><h1 id="Auto-boxing-unboxing"><a href="#Auto-boxing-unboxing" class="headerlink" title="- Auto-boxing / unboxing"></a>- Auto-boxing / unboxing</h1><pre><code>    * 在合适的时机自动打包 , 解包  
        * 自动将基础类型转化为对象  
        * 自动将对象转换为基础类型  
    * TestMap2.java  
    }  

示例练习 {  
    TestArgsWords.java  
}  
</code></pre><p>JDK 1.5 泛型  </p>
<ul>
<li>起因 ：  <ul>
<li>JDK 1.4 以前类型不明确  <ul>
<li>装入集合类型都被当作 Object 对待, 从而失去自己的实际类型。  </li>
<li>从集合中取出时往往需要转型, 效率低, 容易产生错误。  </li>
</ul>
</li>
</ul>
</li>
<li>解决办法 ：  <ul>
<li>在定义集合的时候同时定义集合中对象的类型  </li>
<li>示例 ： BasicGeneric.java  <ul>
<li>可以在定义 Collection 的时候指定  </li>
<li>也可以在循环时用 Iterator 指定  </li>
</ul>
</li>
</ul>
</li>
<li><p>好处 ：  </p>
<ul>
<li><p>增强程序的可读性和稳定性  </p>
<p>什么时候可以指定自己的类型 ： 你看 API， 他跟你就可以跟  </p>
</li>
</ul>
</li>
<li>总结 {<br>  1136  <ul>
<li>一个图  </li>
<li>一个类  <ul>
<li>Collections  <ul>
<li>三个知识点  </li>
</ul>
</li>
</ul>
</li>
<li>For  </li>
<li>Generic  </li>
<li>Auto-boxing / unboxing  <ul>
<li>六个接口 {<br>1, Collection {<br>2, Set,<br>3, List<br>}<br>4, Map<br>5, Iterator<br>6, Comparable<br>}  </li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="8-IO-流"><a href="#8-IO-流" class="headerlink" title="8. IO 流"></a>8. IO 流</h2><pre><code>能帮助你建立文件，不能帮你建目录  

读到内存区 -   

* 转换流 {  
    中文 windos 编码 JBK  
    当前系统默认的 编码是 JBK  
    IOS8859_1 包含的所有西欧语言 --&gt; 后来才推出 UniCode (国际标准化组织为了包含全球)  
    JBK   JB2312   JB18030  东方人自己的编码 - 国标码 - 就是汉字，你可以认为  
    日文，韩文 都有自己的编码  - 台湾有自己的 大五码  
    拉丁1， 2， 3， 4， 5. 6. 7. 8. 9 都同意啦！包括俄罗斯， 但是中文还不行 ---- &gt; UniCode  
    FileOutputStream() 构造方法自己去查  
* System.in {  
    System 类   --   in 是 InputStream 类型  
    public static final InputStream in   抽象类 类型，  又是父类引用指向子类对象  
    InputStreamReader isr = new InputStreamReader(System.in);  
    System.in -&gt; 管道直接堆到黑窗口上  
    BufferedReader br = new BufferedReader(isr);  

    wait()   
    运行后 ： 等待在那 - 阻塞式的方法 很多   
    readLine() 有点特殊  
    其实是 System.in 比较特殊  -- 标准输入 - 等待着标准输入 {  
        你不输入 - 我可不就等着么，当然这个也叫做同步方法。  
        你不输入，我就不能干别的  
        同步式的  
    }  

* 数据流 {  
    请你把 long 类型的数， 写到 --&gt; 文件里面去  
    readUTF()  
    UTF8 比较省空间  
}  
* 打印流 {  
    System.out   
    out - public static final PrintStream  
    默认在我们的黑窗口输出  
    语言代表人的思维 - 能够促进人的思维  
    log4J 著名的日志开发包  
}  

* Object 流 {  
    把整个 Object 全部写入硬盘被  
    在 VC 上叫做系列化  
    存盘点。  
    挨着排的序列化  
    再一点一点读出来  

    Serializable 接口  --- 标记性的接口  
    transient int k = 15;  
    相当于这个 k 是透明的。在序列化的时候不给于考虑，读的时候读默认值。  

    * Serializable 接口  
    * Externalizable 接口  extends  Serializable   
}  
</code></pre><p>}  </p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java SE Introduce]]></title>
      <url>http://sggo.me/2013/02/02/java/java-se-1-introduce/</url>
      <content type="html"><![CDATA[<p>Java是一种广泛使用的计算机编程语言，拥有跨平台、面向对象、泛型编程的特性.</p>
<a id="more"></a>
<ul>
<li>Java data type</li>
<li>OO</li>
<li>Exception</li>
<li>Java Array  </li>
<li>Java 常用类 </li>
<li>Java 容器类</li>
<li>Collection / Generic</li>
<li>Java I/O Stream</li>
<li>Java Thread</li>
<li>Java TCP/UDP, socket</li>
</ul>
<h2 id="1-Java-概述"><a href="#1-Java-概述" class="headerlink" title="1. Java 概述"></a>1. Java 概述</h2><ul>
<li>Java 运行机制</li>
<li>JDK &amp; JRE</li>
<li>Java env install</li>
<li>Java Basic Content</li>
</ul>
<blockquote>
<p>conclude : 计算机语言朝着人类易于理解的方向发展  </p>
</blockquote>
<h2 id="2-Java-特点"><a href="#2-Java-特点" class="headerlink" title="2. Java 特点"></a>2. Java 特点</h2><ul>
<li>一种 OO 语言  </li>
<li>一种平台无关的语言, 提供程序运行的解释环境  </li>
<li>一种健壮的语言, 吸收了C/C++语言的优点， 但去掉了其影响程序健壮性的部分(如: 指针， 内存的申请与释放等)。  </li>
</ul>
<h2 id="3-Java程序运行机制"><a href="#3-Java程序运行机制" class="headerlink" title="3. Java程序运行机制"></a>3. Java程序运行机制</h2><p><strong>Java 2种核心机制</strong></p>
<ul>
<li>Java Virtual Machine</li>
<li>Garbage collection</li>
</ul>
<blockquote>
<p>JVM 可理解成一个以字节码为机器指令的CPU<br>JVM 机制屏蔽了底层运行平台的差别, 实现了”一次编译, 随处运行”。</p>
<p>x.java –编译–&gt; x.class –执行–&gt; JVM</p>
<p>Java语言消除了程序员回收无用内存空间的责任;<br>它提供一种系统级线程跟踪存储空间的分配情况，并在JVM的空闲时, 检查并释放那些可被释放的存储器空间。  </p>
</blockquote>
<h2 id="4-JDK-amp-JRE-amp-env-install"><a href="#4-JDK-amp-JRE-amp-env-install" class="headerlink" title="4. JDK &amp; JRE &amp; env install"></a>4. JDK &amp; JRE &amp; env install</h2><ul>
<li>Software Development Kit (软件开发包)  开发需要 JDK  </li>
<li>Java Runtime Environment  用户只需 JRE  </li>
</ul>
<p><code>/etc/profile</code> or  <code>~/.zshrc</code> or  <code>~/.zprofile</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">### JAVA ###</span><br><span class="line">JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home</span><br><span class="line">JAVA_BIN=$JAVA_HOME/bin</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jar</span><br><span class="line">export JAVA_HOME JAVA_BIN PATH CLASSPATH</span><br></pre></td></tr></table></figure>
<blockquote>
<p>classpath : java在编译和运行时要找的class所在的路径<br>建议你的 JDK 装在不带空格的目录里面</p>
</blockquote>
<h2 id="5-命名规则"><a href="#5-命名规则" class="headerlink" title="5. 命名规则"></a>5. 命名规则</h2><ol>
<li>类名首字母大写  </li>
<li>变量名和方法名的首字母小写  </li>
<li>运用驼峰标识   </li>
</ol>
<p>HelloWorld.java</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class HelloWorld &#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    System.out.println(&quot;Hello Java.&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line">  * 这里是注释</span><br><span class="line">  */</span><br></pre></td></tr></table></figure>
<blockquote>
<p>一个源文件中最多只能有一个public类. 其它类的个数不限，如果源文件 文件包含一个public class 它必需按该 class-name 命名  </p>
</blockquote>
<h2 id="6-Java-程序设计"><a href="#6-Java-程序设计" class="headerlink" title="6. Java 程序设计"></a>6. Java 程序设计</h2><p><strong>data type</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                                      -- 整数类型 (byte, short, int, long)  </span><br><span class="line">                          -- 数值型 --     </span><br><span class="line">                         |            -- 浮点类型 (float, double)  </span><br><span class="line">           --基本数据类型  -- 字符型 (char)  </span><br><span class="line">          |              |  </span><br><span class="line">          |               -- 布尔型 (boolean)  </span><br><span class="line">数据类型 --                           </span><br><span class="line">          |               -- 类 (class)  </span><br><span class="line">          |              |  </span><br><span class="line">           --引用数据类型  -- 接口 (interface)  </span><br><span class="line">                         |  </span><br><span class="line">                          -- 数组 (array)</span><br></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>java 中定义了 <strong>4类 8种</strong> 基本数据类型<br>boolean 类型只允许取值 true / false , 不可以用 0 或 非0 替代。<br>char 采用 Unicode 编码 (全球语言统一编码), 每个字符占两个字节  </p>
</blockquote>
<h2 id="7-Array-amp-Method"><a href="#7-Array-amp-Method" class="headerlink" title="7. Array &amp; Method"></a>7. Array &amp; Method</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class Test &#123;  </span><br><span class="line">    public static void main(String[] args) &#123;  </span><br><span class="line">        Date[] days;  </span><br><span class="line">        days = new Date[3];  </span><br><span class="line">        for (int i = 0; i &lt; 3; i++) &#123;  </span><br><span class="line">            days[i] = new Date(2004, 4, i+1);  </span><br><span class="line">        &#125;</span><br><span class="line">        // </span><br><span class="line">        int[] a = &#123;1, 2, 3, 4, 5, 6, 7&#125;;  </span><br><span class="line">        for (int i = 0; i &lt; a.length; i++) &#123;  </span><br><span class="line">            System.out.print(a[i] + &quot; &quot;);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">class Date &#123;  </span><br><span class="line">    int year;  </span><br><span class="line">    int month;  </span><br><span class="line">    int day;  </span><br><span class="line">    Date(int y, int m, int d) &#123;  </span><br><span class="line">        year = y;  </span><br><span class="line">        month = m;  </span><br><span class="line">        day = d;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="http://blog.csdn.net/robbyo/article/details/16942921" target="_blank" rel="external">面向过程-约瑟夫环</a></p>
<p><a href="http://blog.csdn.net/robbyo/article/details/16967715" target="_blank" rel="external">面向对象-约瑟夫环</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Trie Tree POJ 2001]]></title>
      <url>http://sggo.me/2013/01/17/acm/Trie_Tree-POJ-2001/</url>
      <content type="html"><![CDATA[<p>trie tree poj 2001 shortest prefixes</p>
<a id="more"></a>
<h2 id="POJ-2001-Shortest-Prefixes"><a href="#POJ-2001-Shortest-Prefixes" class="headerlink" title="POJ 2001 Shortest Prefixes"></a><a href="http://poj.org/problem?id=2001" target="_blank" rel="external">POJ 2001 Shortest Prefixes</a></h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;deque&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bitset&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;list&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iterator&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;functional&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;climits&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cassert&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUG puts(<span class="string">"here!!!"</span>);  </span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">1010</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> kind = <span class="number">26</span>;  </span><br><span class="line"><span class="keyword">char</span> str[N][<span class="number">25</span>];  </span><br><span class="line"><span class="keyword">struct</span> Node &#123;  </span><br><span class="line">    <span class="keyword">int</span> num;  </span><br><span class="line">    <span class="keyword">bool</span> tail;  </span><br><span class="line">    Node* next[kind];  </span><br><span class="line">    Node() : num(<span class="number">1</span>), tail(<span class="literal">false</span>) &#123;  </span><br><span class="line">        <span class="built_in">memset</span>(next, <span class="number">0</span>, <span class="keyword">sizeof</span>(next));  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(Node* root, <span class="keyword">char</span> *s)</span> </span>&#123;  </span><br><span class="line">    Node* p = root;  </span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, index;  </span><br><span class="line">    <span class="keyword">while</span>(s[i]) &#123;  </span><br><span class="line">        index = s[i] - <span class="string">'a'</span>;  </span><br><span class="line">        <span class="keyword">if</span>(p-&gt;next[index] == <span class="literal">NULL</span>) &#123;  </span><br><span class="line">            p-&gt;next[index] = <span class="keyword">new</span> Node();  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">else</span> p-&gt;next[index]-&gt;num++;  </span><br><span class="line">        p = p-&gt;next[index];  </span><br><span class="line">        i++;  </span><br><span class="line">    &#125;  </span><br><span class="line">    p-&gt;tail = <span class="literal">true</span>;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">solve</span><span class="params">(Node* root, <span class="keyword">int</span> count)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;  </span><br><span class="line">        Node* p = root;  </span><br><span class="line">        <span class="keyword">int</span> len = <span class="built_in">strlen</span>(str[i]);  </span><br><span class="line">        <span class="keyword">char</span>* s = str[i];  </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; s &lt;&lt; <span class="string">' '</span>;  </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; len; j++) &#123;  </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; s[j];  </span><br><span class="line">            <span class="keyword">int</span> index = s[j] - <span class="string">'a'</span>;  </span><br><span class="line">            <span class="keyword">if</span>(p-&gt;next[index]-&gt;num == <span class="number">1</span>) &#123;  </span><br><span class="line">                <span class="keyword">break</span>;  </span><br><span class="line">            &#125;  </span><br><span class="line">            p = p-&gt;next[index];  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    Node* root = <span class="keyword">new</span> Node(); <span class="comment">// 根节点不包含任何字符  </span></span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, count = <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%s"</span>, str[i]) == <span class="number">1</span>) &#123;  </span><br><span class="line">        insert(root, str[i]);  </span><br><span class="line">        i++;  </span><br><span class="line">    &#125;  </span><br><span class="line">    count = i;  </span><br><span class="line">    solve(root, count);  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Union-Find Sets POJ 1703]]></title>
      <url>http://sggo.me/2013/01/17/acm/union-find_sets-POJ-1703/</url>
      <content type="html"><![CDATA[<p>union-find sets poj 1703 find them, catch them 帮派之争</p>
<a id="more"></a>
<h2 id="Find-them-Catch-them"><a href="#Find-them-Catch-them" class="headerlink" title="Find them, Catch them"></a><a href="http://poj.org/problem?id=1703" target="_blank" rel="external">Find them, Catch them</a></h2><h3 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">5 5</span><br><span class="line">A 1 2</span><br><span class="line">D 1 2</span><br><span class="line">A 1 2</span><br><span class="line">D 2 4</span><br><span class="line">A 1 4</span><br></pre></td></tr></table></figure>
<h3 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Not sure yet.</span><br><span class="line">In different gangs.</span><br><span class="line">In the same gang.</span><br></pre></td></tr></table></figure>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;deque&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bitset&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;list&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iterator&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;functional&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;climits&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cassert&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MID(x,y) ( ( x + y ) &gt;&gt; 1 )  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> L(x) ( x &lt;&lt; 1 )  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> R(x) ( x &lt;&lt; 1 | 1 )  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUG puts(<span class="string">"here!!!"</span>);  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> STOP system(<span class="string">"pause"</span>);  </span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">100005</span>;  </span><br><span class="line"><span class="keyword">int</span> f[N+N];  </span><br><span class="line"><span class="keyword">int</span> n, m;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span>(f[x] &lt; <span class="number">0</span>) <span class="keyword">return</span> x;  </span><br><span class="line">    <span class="keyword">return</span> f[x] = find(f[x]);  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> loop;  </span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; loop;  </span><br><span class="line">    <span class="keyword">while</span>(loop--) &#123;  </span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;n, &amp;m);  </span><br><span class="line">        <span class="built_in">memset</span>(f, <span class="number">255</span>, <span class="keyword">sizeof</span>(f));  </span><br><span class="line">        <span class="keyword">while</span>(m--) &#123;  </span><br><span class="line">            <span class="keyword">int</span> a, b;  </span><br><span class="line">            <span class="keyword">char</span> s[<span class="number">3</span>];  </span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%s%d%d"</span>, s, &amp;a, &amp;b);  </span><br><span class="line">            <span class="keyword">if</span>(s[<span class="number">0</span>] == <span class="string">'A'</span>) &#123;  </span><br><span class="line">                <span class="keyword">if</span>(find(a) != find(b) &amp;&amp; find(a) != find(b+n)) &#123;  </span><br><span class="line">                    <span class="built_in">printf</span>(<span class="string">"Not sure yet.\n"</span>);  </span><br><span class="line">                &#125;  </span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(find(a) == find(b)) &#123;  </span><br><span class="line">                    <span class="built_in">printf</span>(<span class="string">"In the same gang.\n"</span>);  </span><br><span class="line">                &#125;  </span><br><span class="line">                <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"In different gangs.\n"</span>);  </span><br><span class="line">            &#125;  </span><br><span class="line">            <span class="keyword">else</span> &#123;  </span><br><span class="line">                <span class="keyword">if</span>(find(a) != find(b+n)) &#123;  </span><br><span class="line">                    f[find(a)] = find(b+n);  </span><br><span class="line">                    f[find(b)] = find(a+n);  </span><br><span class="line">                &#125;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Union-Find Sets HDU 1856]]></title>
      <url>http://sggo.me/2013/01/17/acm/union-find_sets-hdu_1856/</url>
      <content type="html"><![CDATA[<p>union-find sets hdu 1856 more is better</p>
<a id="more"></a>
<p><strong><a href="http://acm.hdu.edu.cn/showproblem.php?pid=1856" target="_blank" rel="external">More is better</a></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Mr Wang wants some boys to help him with a project. ...</span><br></pre></td></tr></table></figure>
<p><strong><font color="#2561c2">Sample Input</font></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4</span><br><span class="line">1 2</span><br><span class="line">3 4</span><br><span class="line">5 6</span><br><span class="line">1 6</span><br><span class="line">4</span><br><span class="line">1 2</span><br><span class="line">3 4</span><br><span class="line">5 6</span><br><span class="line">7 8</span><br></pre></td></tr></table></figure>
<p><strong><font color="#2561c2">Sample Output</font></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p><strong><font color="#2561c2">Code</font></strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;deque&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bitset&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;list&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iterator&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;functional&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;climits&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cassert&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUG puts(<span class="string">"here!!!"</span>);  </span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">100005</span>;  </span><br><span class="line"><span class="keyword">struct</span> Node &#123;  </span><br><span class="line">    <span class="keyword">int</span> par;  </span><br><span class="line">    <span class="keyword">int</span> sum;  </span><br><span class="line">&#125;;  </span><br><span class="line"><span class="keyword">int</span> SUM;  </span><br><span class="line">  </span><br><span class="line">Node p[<span class="number">2</span>*N + <span class="number">5</span>];  </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">makeSet</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">2</span>*n; i++) &#123;  </span><br><span class="line">        p[i].par = i;  </span><br><span class="line">        p[i].sum = <span class="number">1</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    SUM = <span class="number">1</span>;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> a)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span>(a == p[a].par) <span class="keyword">return</span> a;  </span><br><span class="line">    <span class="keyword">return</span> p[a].par = find(p[a].par);  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">union1</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> fa = find(a);  </span><br><span class="line">    <span class="keyword">int</span> fb = find(b);  </span><br><span class="line">    <span class="keyword">if</span>(fa != fb) &#123;  </span><br><span class="line">        p[fa].par = fb;  </span><br><span class="line">        p[fb].sum += p[fa].sum;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">if</span>(p[fb].sum &gt; SUM) &#123;  </span><br><span class="line">        SUM = p[fb].sum;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> n, a, b;  </span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) == <span class="number">1</span>) &#123;  </span><br><span class="line">        makeSet(n);  </span><br><span class="line">        <span class="keyword">while</span>(n--) &#123;  </span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;a, &amp;b);  </span><br><span class="line">            union1(a, b);  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, SUM);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[KMP / HDU 1711 找到匹配的位置并返回]]></title>
      <url>http://sggo.me/2013/01/17/acm/kmp-for-hdu_1711/</url>
      <content type="html"><![CDATA[<p>kmp hdu 1711 number sequence</p>
<a id="more"></a>
<h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a><font color="#2561c2">Description</font></h3><p>Given two sequences of numbers : a[1], a[2], …… , a[N], and b[1], b[2], …… , b[M] (1 &lt;= M &lt;= 10000, 1 &lt;= N &lt;= 1000000). Your task is to find a number K which make a[K] = b[1], a[K + 1] = b[2], …… , a[K + M – 1] = b[M]. If there are more than one K exist, output the smallest one.</p>
<h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a><font color="#2561c2">Input</font></h3><p>The first line of input is a number T which indicate the number of cases. Each case contains three lines. The first line is two numbers N and M (1 &lt;= M &lt;= 10000, 1 &lt;= N &lt;= 1000000). The second line contains N integers which indicate a[1], a[2], …… , a[N]. The third line contains M integers which indicate b[1], b[2], …… , b[M]. All integers are in the range of [-1000000, 1000000].</p>
<h3 id="Output"><a href="#Output" class="headerlink" title="Output"></a><font color="#2561c2">Output</font></h3><p>For each test case, you should output one line which only contain K described above. If no such K exists, output -1 instead.</p>
<h3 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a><font color="#2561c2">Sample Input</font></h3><p>2<br>13 5<br>1 2 1 2 3 1 2 3 1 3 2 1 2<br>1 2 3 1 3<br>13 5<br>1 2 1 2 3 1 2 3 1 3 2 1 2<br>1 2 3 2 1</p>
<h3 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a><font color="#2561c2">Sample Output</font></h3><p>6<br>-1</p>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a><font color="#2561c2">Code</font></h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;deque&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bitset&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;list&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iterator&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;functional&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;climits&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cassert&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUG puts(<span class="string">"here!!!"</span>);  </span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">1000005</span>;  </span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> M = <span class="number">10005</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">int</span> s[N];  </span><br><span class="line"><span class="keyword">int</span> t[M];  </span><br><span class="line"><span class="keyword">int</span> next[M];  </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getNext</span><span class="params">(<span class="keyword">int</span> len)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> i, j;  </span><br><span class="line">    i = <span class="number">0</span>, j = <span class="number">-1</span>;  </span><br><span class="line">    next[<span class="number">0</span>] = <span class="number">-1</span>;  </span><br><span class="line">    <span class="keyword">while</span>(i &lt; len<span class="number">-1</span>) &#123;  </span><br><span class="line">        <span class="keyword">if</span>(j == <span class="number">-1</span> || t[i] == t[j]) &#123;  </span><br><span class="line">            i++, j++, next[i] = j;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">else</span> &#123;  </span><br><span class="line">            j = next[j];  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kmp</span><span class="params">(<span class="keyword">int</span> sl, <span class="keyword">int</span> tl)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">while</span>(i &lt; sl &amp;&amp; j &lt; tl) &#123;  </span><br><span class="line">        <span class="keyword">if</span>(j == <span class="number">-1</span> || s[i] == t[j]) &#123;  </span><br><span class="line">            i++, j++;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">else</span> j = next[j];  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">if</span>(j == tl) <span class="keyword">return</span> i-j+<span class="number">1</span>;  </span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">-1</span>;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="comment">// abcabcababcabcabdef  </span></span><br><span class="line"><span class="comment">// abcabcabd  </span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> T, n, m, ans;  </span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; T;  </span><br><span class="line">    <span class="keyword">while</span>(T--) &#123;  </span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;  </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;  </span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, s+i);  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;  </span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, t+i);  </span><br><span class="line">        &#125;  </span><br><span class="line">        getNext(m);  </span><br><span class="line">        ans = kmp(n, m);  </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[需要的气质品格]]></title>
      <url>http://sggo.me/2013/01/17/tools/being-eq-gentleman/</url>
      <content type="html"><![CDATA[<p>做一个 成熟、绅士、稳重 的人，我最近越来越觉得它非常非常非常重要…</p>
<a id="more"></a>
<h2 id="一：沉稳"><a href="#一：沉稳" class="headerlink" title="一：沉稳"></a>一：沉稳</h2><p>（1）不要随便显露你的情绪。<br>（2）不要逢人就诉说你的困难和遭遇。<br>（3）在征询别人的意见之前，自己先思考，但不要先讲。<br>（4）不要一有机会就唠叨你的不满。<br>（5）重要的决定尽量有别人商量，最好隔一天再发布。<br>（6）讲话不要有任何的慌张，走路也是。  </p>
<h2 id="二：细心"><a href="#二：细心" class="headerlink" title="二：细心"></a>二：细心</h2><p>（1）对身边发生的事情，常思考它们的因果关系。<br>（2）对做不到位的执行问题，要发掘它们的根本症结。<br>（3）对习以为常的做事方法，要有改进或优化的建议。<br>（4）做什么事情都要养成有条不紊和井然有序的习惯。<br>（5）经常去找几个别人看不出来的毛病或弊端。<br>（6）自己要随时随地对有所不足的地方补位。   </p>
<h2 id="三：胆识"><a href="#三：胆识" class="headerlink" title="三：胆识"></a>三：胆识</h2><p>（1）不要常用缺乏自信的词句<br>（2）不要常常反悔，轻易推翻已经决定的事。<br>（3）在众人争执不休时，不要没有主见。<br>（4）整体氛围低落时，你要乐观、阳光。<br>（5）做任何事情都要用心，因为有人在看着你。<br>（6）事情不顺的时候，歇口气，重新寻找突破口，就结束也要干净利落。   </p>
<h2 id="四：大度"><a href="#四：大度" class="headerlink" title="四：大度"></a>四：大度</h2><p>（1）不要刻意把有可能是伙伴的人变成对手。<br>（2）对别人的小过失、小错误不要斤斤计较。<br>（3）在金钱上要大方，学习三施（财施、法施、无畏施）<br>（4）不要有权力的傲慢和知识的偏见。<br>（5）任何成果和成就都应和别人分享。<br>（6）必须有人牺牲或奉献的时候，自己走在前面。  </p>
<h2 id="五：诚信"><a href="#五：诚信" class="headerlink" title="五：诚信"></a>五：诚信</h2><p>（1）做不到的事情不要说，说了就努力做到。<br>（2）虚的口号或标语不要常挂嘴上。<br>（3）针对客户提出的“不诚信”问题，拿出改善的方法。<br>（4）停止一切“不道德”的手段。<br>（5）耍弄小聪明，要不得！<br>（6）计算一下产品或服务的诚信代价，那就是品牌成本。   </p>
<h2 id="六：担当"><a href="#六：担当" class="headerlink" title="六：担当"></a>六：担当</h2><p>（1）检讨任何过失的时候，先从自身或自己人开始反省。<br>（2）事项结束后，先审查过错，再列述功劳。<br>（3）认错从上级开始，表功从下级启动.<br>（4）着手一个计划，先将权责界定清楚，而且分配得当。<br>（5）对“怕事”的人或组织要挑明了说。<br>（6）因为勇于承担责任所造成的损失，公司应该承担   </p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://blog.csdn.net/robbyo/article/details/8514334" target="_blank" rel="external">转自网络</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux C语言的一些知识学习]]></title>
      <url>http://sggo.me/2012/05/19/acm/linux-c/</url>
      <content type="html"><![CDATA[<p>Linux C语言的一些知识, 涉及 计算机程序，语言发展，编译链接，数据，函数库等等.</p>
<a id="more"></a>
<h2 id="1-程序"><a href="#1-程序" class="headerlink" title="1. 程序"></a>1. 程序</h2><ul>
<li>什么是程序？</li>
<li>程序的作用是什么？</li>
</ul>
<blockquote>
<p>我们必须通过指令，指挥计算机执行我们想要它做的动作。 而依照顺序执行的一组指令就是程序。</p>
</blockquote>
<h3 id="1-1-程序特征"><a href="#1-1-程序特征" class="headerlink" title="1.1 程序特征"></a>1.1 程序特征</h3><p>1, 程序是与计算机沟通的语言<br>2, 程序是由特定语法与关键字构成<br>3, 程序是一行一行执行的<br>4, 程序的执行, 从入口点开始, 原则上是由上而下, 从左而右执行。</p>
<blockquote>
<p>入口点可能是一个方法或者是函数。。。</p>
</blockquote>
<h2 id="2-程序语言的演化"><a href="#2-程序语言的演化" class="headerlink" title="2. 程序语言的演化"></a>2. 程序语言的演化</h2><p>1, 机器语言   –  计算机通过电信号模式 0/1/0/1…开关开关…<br>2, 汇编语言<br>3, 高级语言<br>4, 第四代语言<br>5, 自然语言   –  智能语言，开机，它就开机。关机，它就关机。</p>
<h3 id="2-1-机器语言"><a href="#2-1-机器语言" class="headerlink" title="2.1 机器语言"></a>2.1 机器语言</h3><p> 1, 是计算机唯一能够执行的语言。<br> 2, 其他语言必须先转化为机器语言<br> 3, 指令有 0 与 1 组成, 称为机器码。<br> 4, 指令难记忆但执行速度最快。<br> 5, 不同类型机器有不同的机器码, 不具备移植性。</p>
<h3 id="2-2-高级语言"><a href="#2-2-高级语言" class="headerlink" title="2.2 高级语言"></a>2.2 高级语言</h3><p> 1, 离机器越来越远<br> 2, 语法接近人类的自然语言<br> 3, 执行的单位不是指令,  而是语句, 一行语句。<br> 4, 开发效率越来越高。<br> 5, 必须编译成机器码<br> 6, 移植性比较好。</p>
<h3 id="2-3-第四代语言"><a href="#2-3-第四代语言" class="headerlink" title="2.3 第四代语言"></a>2.3 第四代语言</h3><p> 1, 4GL, 也称为面向问题的程序语言。<br> 2, 仅需告诉计算机 “做什么”, 不需要指挥计算机”如何做”<br> 3, 大大提高开发效率<br> 4, 如 SQL 语言及各种查询语言。</p>
<h3 id="2-4-自然语言"><a href="#2-4-自然语言" class="headerlink" title="2.4 自然语言"></a>2.4 自然语言</h3><p> 1, 运用人工智能, 以接近口语的指令指挥计算机，如语音。<br> 2, 还没有成熟。</p>
<h2 id="3-编译与链接"><a href="#3-编译与链接" class="headerlink" title="3. 编译与链接"></a>3. 编译与链接</h2><h3 id="3-1-编译"><a href="#3-1-编译" class="headerlink" title="3.1 编译"></a>3.1 编译</h3><p> 1, 把源代码转化成机器码的过程。<br> 2, 之前还会进行前期处理。<br> 3, 过程中会进行语法检查。<br> 4, 编译器 :  完成编译动作的程序</p>
<h3 id="3-2-连接"><a href="#3-2-连接" class="headerlink" title="3.2 连接"></a>3.2 连接</h3><p> 1, 将可执行文件与包含文件 / 用到的函数库建立关联的过程。</p>
<h2 id="4-基本概念"><a href="#4-基本概念" class="headerlink" title="4. 基本概念"></a>4. 基本概念</h2><h3 id="4-1-数据"><a href="#4-1-数据" class="headerlink" title="4.1 数据"></a>4.1 数据</h3><p> 是计算机程序处理的对象, 可以是整数, 实数, 字符, 也可以是图像, 声音等的编码表示。</p>
<h3 id="4-2-数据结构"><a href="#4-2-数据结构" class="headerlink" title="4.2 数据结构"></a>4.2 数据结构</h3><p> 指的是数据与数据间存在一种或多种特定关系。 与数据结构密切相关的便是数据的类型和数据的存放。</p>
<ul>
<li>程序设计</li>
<li>编写程序的过程。</li>
<li>软件</li>
<li>程序 + 文档。</li>
</ul>
<h2 id="5-Linux-C-概述"><a href="#5-Linux-C-概述" class="headerlink" title="5. Linux C 概述"></a>5. Linux C 概述</h2><p>Linux 和 C 天生有不解之源, Linux 的操作系统内核就主要是用 C 写的, 另外 Linux 下的很多软件也是用 C 写的，特别是一些著名的服务软件, 如何 MySQL, Apache 等。</p>
<h3 id="5-1-开发环境的构成"><a href="#5-1-开发环境的构成" class="headerlink" title="5.1 开发环境的构成"></a>5.1 开发环境的构成</h3><p>1, 编辑器 ： 选择 VI<br>2, 编译器 ： 选择 GNU C/C++ 编译器 gcc (免费开源的工具 - linux发型版本多数都自动安装)<br>3, 调试器 ： 应用广泛的 gdb (JDK 学习的 GDB)<br>4, 函数库 ： glibc<br>5, 系统头文件 ： glibc_header</p>
<h3 id="5-2-编译器-gcc"><a href="#5-2-编译器-gcc" class="headerlink" title="5.2 编译器 gcc"></a>5.2 编译器 gcc</h3><p>gcc (GNU CCompiler) 是GNU推出的功能强大 , 性能优越的多平台编译器, gcc 编译器能将C , C++语言源程序编译, 连接成可执行文件, 以下是 gcc 支持编译的一些源文件的后缀及其解释。</p>
<blockquote>
<p>.c 为后缀的文件, C语言源代码文件<br>.h 为后缀的文件, 是程序所包含的头文件。<br>.i 为后缀的文件, 是已经预处理过的 C 源代码文件;<br>.o 为后缀的文件, 是编译后的目标文件。<br>.s 为后缀的文件, 是汇编语言源代码文件。</p>
</blockquote>
<p><strong>gcc -o hello hello.c</strong></p>
<blockquote>
<ul>
<li>用 gcc 来编译我们的源程序</li>
<li>-o 选项要求编译器给我们输出的可执行文件名为 hello</li>
<li>hello.c 是我们的源程序文件。</li>
<li>-c 选项 表示我们只要求编译器输出目标代码, 而不必要输出可执行文件</li>
<li>-g 选项表示我们要求编译器在编译的时候提供我们以后对程序进行调试的信息。</li>
<li>执行 ./hello 就可以看到程序的输出结构。 (在当前目录下去找)</li>
</ul>
</blockquote>
<h3 id="5-3-函数库-glibc"><a href="#5-3-函数库-glibc" class="headerlink" title="5.3 函数库 glibc"></a>5.3 函数库 glibc</h3><p>要构建一个完整的 C 开发环境, Glibc 是必不可少的, 它是 Linux 下 C 的主要函数库. glibc 有两种安装方式：</p>
<p> A. 安装成测试用的函数库, 在编译程序时用不同的选项来试用新的函数库。</p>
<p> B. 安装成主要的 C 函数库, 所有新编译程序均用的函数库</p>
<p><strong>glibc 含几个附加包</strong> : Linuxthreads， localedate 和 crypt， 他们的文件名随版本的不同而类似于下列 ：</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">glibc-2.06.tar.gz</span><br><span class="line">glibc-linuxthreads-2.0.6.tar.gz</span><br><span class="line">glibc-localedate-2.0.6.tar.gz</span><br><span class="line">glibc-crypt-2.0.6.tar.gz</span><br></pre></td></tr></table></figure>
<p><strong>glibc 是提供系统调用和基本函数的Ｃ库</strong>, 比如 open, malloc, printf 等等。 所有动态连接的程序都要用到它.</p>
<blockquote>
<ul>
<li>系统头文件 ： glibc_header</li>
<li>缺少系统头文件，很多用到系统功能的C程序将无法编译。</li>
<li>如果用户在安装过程中少装了这些包，就会无法编译C程序，解决方法：</li>
<li>方法一：不推荐：重装一遍Linux系统。</li>
<li>方法二：通过找一些Rpm包来迅速安装Linux的C开发环境。</li>
</ul>
</blockquote>
<p><strong>C开发环境对应的rpm包</strong></p>
<p>由于gcc包需依赖 binutils和cpp包，另外make包也是在编译中常用的，所以一共需要8个包来完成安装，它们是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cpp-2.96-110.i386.rpm</span><br><span class="line">binutils-2.11.93.0.2-11. i386.rpm</span><br><span class="line">Glibc-2.2.5-31. i386.rpm</span><br><span class="line">Glibc-kernheaders-2.4-7.14. i386.rpm</span><br><span class="line">Glibc-common-2.2.5-34</span><br><span class="line">Glibc-devel-2.2.5-34- i386.rpm</span><br><span class="line">Gcc-2.96-110.i386.rpm</span><br><span class="line">Make-3.79.1-8. i386.rpm</span><br></pre></td></tr></table></figure>
<blockquote>
<p>查看glibc的版本：ls /lib/libc-*<br>查看gcc版本号： gcc –version</p>
</blockquote>
<h3 id="5-4-C-程序的组成"><a href="#5-4-C-程序的组成" class="headerlink" title="5.4 C 程序的组成"></a>5.4 C 程序的组成</h3><p>对于一个 C 程序, 安装完成后可以分成三个组成 ：</p>
<ul>
<li>可执行文件</li>
<li>包含文件</li>
<li>库文件</li>
</ul>
<p>可执行文件是最终运行的命令, 包含文件是该 C 程序 include 的定义文件, 库文件则是该 C 程序自定义的库。</p>
<p>比如用 RPM 安装的 Mysql 数据库 ：</p>
<blockquote>
<ul>
<li>可执行文件放在 /usr/bin 下</li>
<li>包含文件放在  /usr/include/mysql 下</li>
<li>库文件在     /usr/lib/mysql 下</li>
</ul>
</blockquote>
<p>只有系统可以找到程序对应的包含文件和库文件, 程序可执行文件才能正常运行。</p>
<h2 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a>6. 小结</h2><blockquote>
<ol>
<li>开发环境的构成</li>
<li>GNU 的 gcc 编译器</li>
<li>glibc 函数库</li>
<li>Linux 下 C 程序开发过程</li>
<li>第一个 C 程序</li>
<li>C 程序结构</li>
</ol>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[学用 g++ (初步)]]></title>
      <url>http://sggo.me/2012/03/22/acm/g++/</url>
      <content type="html"><![CDATA[<p>gcc 和 g++ 都是GNU(组织)的一个编译器.</p>
<a id="more"></a>
<h2 id="链接库"><a href="#链接库" class="headerlink" title="链接库"></a>链接库</h2><p>动态链接库 (通常以 .so 结尾) 和 静态链接库 (通常以 .a 结尾)</p>
<blockquote>
<p>两者的差别仅在程序执行时所需的代码是在运行时加载的, 还是在编译时加载的, 默认情况下, g++ 在链接时优先使用动态链接库, 只有当动态链接库不存在时才考虑使用静态链接库.</p>
<p>如果需要的话可以在编译时加上 -static 选项, 强制使用静态链接库。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">g++ foo.cpp -L /home/xiaowp/lib -static -lfoo -o foo</span><br></pre></td></tr></table></figure>
<h2 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h2><p>代码优化指的是编译器用过分析源代码, 找出其中尚未达到最优的部分,</p>
<p>然后对其重新进行组合, 目的是改善程序的执行性能.</p>
<p>g++ 通过编译选项 -On 来控制优化代码的生成 (n 一般 0 ~ 2,3)</p>
<h2 id="学用-g"><a href="#学用-g" class="headerlink" title="学用 g++"></a>学用 g++</h2><p>GCC (GNC Compiler Collection) 是 linux 下最主要的编译工具, GCC 不仅功能强大, 结构也异常灵活.</p>
<p>g++ 是 gcc 中的一个工具, 专门来编译 C++ 语言的。</p>
<blockquote>
<p>$ g++ hello.cpp -o hello  (hello 是编译成的可执行文件)<br>$ ./hello  (运行 hello)</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[寄存器, 内存, 存储器, Cache 的区别]]></title>
      <url>http://sggo.me/2012/03/18/ops/ops-linux-computer-memory/</url>
      <content type="html"><![CDATA[<p>计算机存储中的 寄存器 内存 存储器 cache区别， 从范围来看，它们所指的范畴就不一样。</p>
<a id="more"></a>
<h2 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h2><p>中央处理器内的组成部份。它跟CPU有关。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。</p>
<h2 id="存储器"><a href="#存储器" class="headerlink" title="存储器"></a>存储器</h2><p>存储器范围最大，它几乎涵盖了所有关于存储的范畴。你所说的寄存器，内存，都是存储器里面的一种。凡是有存储能力的硬件，都可以称之为存储器，这是自然，硬盘更加明显了，它归入外存储器行列，由此可见——。</p>
<h2 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h2><p>既专业名上的内存储器，它不是个什么神秘的东西，它也只是存储器中的沧海一粟，它包涵的范围也很大，一般分为只读存储器和随机存储器，以及最强悍的高速缓冲存储器（CACHE），只读存储器应用广泛，它通常是一块在硬件上集成的可读芯片，作用是识别与控制硬件，它的特点是只可读取，不能写入。随机存储器的特点是可读可写，断电后一切数据都消失，我们所说的内存条就是指它了。</p>
<h2 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h2><p>在CPU中速度非常块，而容量却很小的一种存储器，它是计算机存储器中最强悍的存储器。由于技术限制，容量很难提升，一般都不过兆。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>转自 : hp_carrot</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Graph Theory 近期小结]]></title>
      <url>http://sggo.me/2011/12/29/acm/graph_theory_summary/</url>
      <content type="html"><![CDATA[<p>关于图论的学习总结, 在本博可以详细的体现，做的大部分都是非常经典的图论算法入门题。</p>
<a id="more"></a>
<p>断断续续，也要接近一个月的时间啦！我今天终于把图论的大部分经典算法 复习+学习 小完毕。 因为中间学校总会有一些杂事在影响我的进度，所以是几经周折，断断续续，但是值得庆幸的是我一直没有间断过，这方面的学习。</p>
<p>本人图论所涉猎的算法如下 {</p>
<ol>
<li>拓扑排序  (Topology-h1285 )</li>
<li>最小生成树 (Kruska-p1287 / Prim-p1287 )</li>
<li>最短路 ( Dijkstra-p2387 / Bellman_Ford-z3033 / Bellman_Ford-p3259 / SPFA-template / SPFA-邻接矩阵-p3259 / SPFA-邻接表-p3259 / Floyd-template / Floyd-p1502 )</li>
<li>二分图 (Hungary . poj 1274 / poj 2446 / zoj 1654   KM_CA_poj 2195O(n^4)/ KM_O(n^3)_p2195 )</li>
<li>网络流 (EK_入门<em>p1273 / Dinic</em>入门_p1273 / 最小费用最大流(SPFA+EK)_p2195)</li>
<li>强连通分量与缩点(有向图) Korasaju_p2186 / Korasaju and Tarjan_p1236 。 </li>
<li>图的割点与桥(无向图)  ( Tarjan割点和桥的示例程序。 Sample : poj 144 求割点个数)</li>
<li>双连通分量(无向图)  (Tarjan点双连通示例程序  / tarjan_边双连通-缩点p3352。  Sample : poj 3352 求加入最少的边使其变成边双连通分支)</li>
<li>2-sat(这个我目前只能求判定性的问题) : poj 3207 / poj 3678 / poj 2723</li>
</ol>
<p><strong>Sample:</strong></p>
<p>Sample : poj 2186 求有多少顶点是由任何顶点出发都可达。<br>Sample : poj 1236 </p>
<ol>
<li>至少要选几个顶点，才能做到从这些顶点出发，可以到达全部顶点 </li>
<li>至少要加多少条边，才能使得从任何一个顶点出发，都能到达全部顶点</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[致敬奋斗的自己]]></title>
      <url>http://sggo.me/2011/11/29/tools/Chicken-soup/</url>
      <content type="html"><![CDATA[<p>这是关于一些励志的句子， 现在看那时候的我，真的是超励志， 超刻苦</p>
<a id="more"></a>
<blockquote>
<p>三军可夺帅也，匹夫不可夺志也！</p>
<p>天行健，君子以自强不息！ </p>
<p>宝剑锋自磨砺出，梅花香自苦寒来！</p>
<p>耐心候好运，好运常会来！ </p>
<p>不经一番寒彻骨，怎得梅花扑鼻香！ </p>
<p>成事不在于力量的大小，而在于能坚持多久！ </p>
<p>绳锯木断，水滴石穿！ </p>
<p>蚓无爪牙之利，筋骨之强，上食埃土，下饮黄泉，用心一也！ </p>
<p>疾风知劲草，烈火炼真金。</p>
<p>不经寒霜苦，安能香袭人？</p>
<p>锋自磨砺出，玉乃雕琢成。</p>
<p>人而不苦练，焉能艺精深？</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Shortest Path]]></title>
      <url>http://sggo.me/2011/10/27/acm/shortest-path/</url>
      <content type="html"><![CDATA[<p>shortest path ： dijstra 、 Bellman 、 Floyd 、 SPFA</p>
<a id="more"></a>
<h2 id="1-dijstra"><a href="#1-dijstra" class="headerlink" title="1. dijstra"></a>1. dijstra</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> data[M][M]; <span class="comment">// init INF</span></span><br><span class="line"><span class="keyword">int</span> lowc[M];</span><br><span class="line"><span class="keyword">int</span> vis[M];</span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">djst</span><span class="params">(<span class="keyword">int</span> p)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">        vis[i] = <span class="number">0</span>;</span><br><span class="line">        lowc[i] = data[p][i];</span><br><span class="line">    &#125;</span><br><span class="line">    vis[p] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n<span class="number">-1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> minc = INF, c = <span class="number">0</span>, lk;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(vis[j] == <span class="number">0</span> &amp;&amp; lowc[j] &lt; minc) &#123;</span><br><span class="line">                minc = lowc[j];</span><br><span class="line">                c = j;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(c == <span class="number">1</span>) <span class="keyword">break</span>;</span><br><span class="line">        vis[c] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(vis[j] == <span class="number">0</span> &amp;&amp; data[c][j] + minc &gt; <span class="number">0</span> &amp;&amp; data[c][j] + minc &lt; lowc[j]) &#123;</span><br><span class="line">                lowc[j] = data[c][j] + minc;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; lowc[<span class="number">1</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-Bellman"><a href="#2-Bellman" class="headerlink" title="2. Bellman"></a>2. Bellman</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INF ((long long)(1))&lt;&lt;62</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 301</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">struct</span> edge&#123;</span><br><span class="line">   <span class="keyword">int</span> u;</span><br><span class="line">   <span class="keyword">int</span> v;</span><br><span class="line">   <span class="keyword">long</span> <span class="keyword">long</span> w; <span class="comment">// 注意</span></span><br><span class="line">&#125;e[N*N];</span><br><span class="line"><span class="keyword">int</span> m, n;</span><br><span class="line"><span class="keyword">long</span> <span class="keyword">long</span> d[<span class="number">1005</span>];</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">bellman_ford</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> di)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">     <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        d[i] = INF;</span><br><span class="line">     &#125;</span><br><span class="line">     d[s] = <span class="number">0</span>;</span><br><span class="line">     <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n<span class="number">-1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">1</span>; j &lt;= m; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(d[e[j].u] != INF &amp;&amp; d[e[j].u]+e[j].w &lt; d[e[j].v])    <span class="comment">// 对边进行操作 、松弛</span></span><br><span class="line">                d[e[j].v] = d[e[j].u] + e[j].w;</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">for</span>(j = <span class="number">1</span>; j &lt;= m; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(d[e[j].u] != INF &amp;&amp; (d[e[j].v] &gt; d[e[j].u]+e[j].w))    <span class="comment">// 很理解</span></span><br><span class="line">            d[e[j].v] = -INF;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span>(d[di] == INF || d[di] == -INF) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-Floyd"><a href="#3-Floyd" class="headerlink" title="3. Floyd"></a>3. Floyd</h2><h2 id="4-SPFA"><a href="#4-SPFA" class="headerlink" title="4. SPFA"></a>4. SPFA</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF = <span class="number">0x7fffffff</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">5501</span>;</span><br><span class="line"><span class="keyword">struct</span> edge &#123;</span><br><span class="line">    <span class="keyword">int</span> to;</span><br><span class="line">    <span class="keyword">int</span> w;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">vector</span>&lt;edge&gt; p[N]; <span class="comment">// vector 实现邻接表</span></span><br><span class="line"><span class="keyword">int</span> d[N];</span><br><span class="line"><span class="keyword">bool</span> inque[N];     <span class="comment">// 记录顶点是否在队列中，SPFA算法可以入队列多次</span></span><br><span class="line"><span class="keyword">int</span> cnt[N];        <span class="comment">// 记录顶点入队的次数</span></span><br><span class="line"><span class="keyword">int</span> n, m, q;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">SPFA</span><span class="params">(<span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; Q;</span><br><span class="line">    <span class="keyword">while</span>(!Q.empty()) Q.pop();</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt;= n; i++) &#123;</span><br><span class="line">        d[i] = INF;</span><br><span class="line">    &#125;</span><br><span class="line">    d[s] = <span class="number">0</span>;      <span class="comment">// 源点的距离为 0</span></span><br><span class="line">    <span class="built_in">memset</span>(inque, <span class="number">0</span>, <span class="keyword">sizeof</span>(inque));</span><br><span class="line">    <span class="built_in">memset</span>(cnt, <span class="number">0</span>, <span class="keyword">sizeof</span>(cnt));</span><br><span class="line">    Q.push(s);</span><br><span class="line">    inque[s] = <span class="literal">true</span>;</span><br><span class="line">    cnt[s]++;      <span class="comment">// 源点入队列的次数增加</span></span><br><span class="line">    <span class="keyword">while</span>(!Q.empty()) &#123;</span><br><span class="line">        <span class="keyword">int</span> t = Q.front();</span><br><span class="line">        Q.pop();</span><br><span class="line">        inque[t] = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; p[t].size(); i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> to = p[t][i].to;</span><br><span class="line">            <span class="keyword">if</span>(d[t] &lt; INF &amp;&amp; d[to] &gt; d[t] + p[t][i].w) &#123;</span><br><span class="line">                d[to] = d[t] + p[t][i].w;</span><br><span class="line">                cnt[to]++;</span><br><span class="line">                <span class="keyword">if</span>(cnt[to] &gt;= n) &#123;  <span class="comment">//当一个点入队的次数&gt;=n时就证明出现了负环</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(!inque[to]) &#123;</span><br><span class="line">                    Q.push(to);</span><br><span class="line">                    inque[to] = <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Minimum Spanning Tree]]></title>
      <url>http://sggo.me/2011/10/27/acm/mst/</url>
      <content type="html"><![CDATA[<p>data structure - minimal spanning tree</p>
<a id="more"></a>
<h2 id="1-prim"><a href="#1-prim" class="headerlink" title="1. prim"></a>1. prim</h2><blockquote>
<p><a href="http://acm.hdu.edu.cn/showproblem.php?pid=1233" target="_blank" rel="external">http://acm.hdu.edu.cn/showproblem.php?pid=1233</a></p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF = <span class="number">0x7fffffff</span>; <span class="comment">// max int value</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">101</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> <span class="built_in">map</span>[N][N];</span><br><span class="line"><span class="keyword">int</span> dis[N];</span><br><span class="line"><span class="keyword">bool</span> vis[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">prim</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(vis, <span class="literal">false</span>, <span class="keyword">sizeof</span>(vis));</span><br><span class="line">    <span class="built_in">memset</span>(dis, <span class="number">0</span>, <span class="keyword">sizeof</span>(dis));</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>[<span class="number">1</span>][i] != <span class="number">-1</span>) &#123;</span><br><span class="line">            dis[i] = <span class="built_in">map</span>[<span class="number">1</span>][i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> dis[i] = INF;</span><br><span class="line">    &#125;</span><br><span class="line">    vis[<span class="number">1</span>] = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n<span class="number">-1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> minv = INF, c = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(!vis[j] &amp;&amp; dis[j] &lt; minv) &#123;</span><br><span class="line">                minv = dis[j];</span><br><span class="line">                c = j;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        vis[c] = <span class="literal">true</span>;</span><br><span class="line">        sum += minv;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(!vis[j] &amp;&amp; <span class="built_in">map</span>[c][j] != <span class="number">-1</span> &amp;&amp;  <span class="built_in">map</span>[c][j] &lt; dis[j]) &#123;</span><br><span class="line">                dis[j] = <span class="built_in">map</span>[c][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> n, m;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span> == <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) &amp;&amp; n != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">memset</span>(<span class="built_in">map</span>, <span class="number">255</span>, <span class="keyword">sizeof</span>(<span class="built_in">map</span>));</span><br><span class="line">        <span class="keyword">int</span> a, b, c;</span><br><span class="line">        m = (n * (n<span class="number">-1</span>)) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d%d%d"</span>, &amp;a, &amp;b, &amp;c);</span><br><span class="line">            <span class="built_in">map</span>[a][b] = <span class="built_in">map</span>[b][a] = c;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, prim(n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-kruskal"><a href="#2-kruskal" class="headerlink" title="2. kruskal"></a>2. kruskal</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> pre[N];</span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"><span class="keyword">struct</span> Edge &#123;</span><br><span class="line">    <span class="keyword">int</span> u, v;</span><br><span class="line">    <span class="keyword">int</span> w;</span><br><span class="line">&#125;e[N];</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="keyword">const</span> Edge a, <span class="keyword">const</span> Edge b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a.w &lt; b.w;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">make_set</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= n; i++)</span><br><span class="line">        pre[i] = i;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find_set</span><span class="params">(<span class="keyword">int</span> a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(pre[a] == a) <span class="keyword">return</span> a;</span><br><span class="line">    <span class="keyword">return</span> pre[a] = find_set(pre[a]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">kruskal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    sort(e, e + m, cmp);</span><br><span class="line">    make_set(n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, fu, fv, cnt_e; i &lt; m; i++) &#123;</span><br><span class="line">        fu = find_set(e[i].u);</span><br><span class="line">        fv = find_set(e[i].v);</span><br><span class="line">        <span class="keyword">if</span> (fu != fv) &#123;</span><br><span class="line">            sum += e[i].w;</span><br><span class="line">            cnt_e++;</span><br><span class="line">            <span class="keyword">if</span> (cnt_e == n<span class="number">-1</span>) <span class="keyword">break</span>;</span><br><span class="line">            pre[fv] = fu;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; sum &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Six Kinds of Sort algorithms]]></title>
      <url>http://sggo.me/2011/10/27/acm/six-sort/</url>
      <content type="html"><![CDATA[<p>input ： 8, 5, 4, 9, 2, 3, 6</p>
<a id="more"></a>
<h2 id="1-heapSort"><a href="#1-heapSort" class="headerlink" title="1. heapSort"></a>1. heapSort</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heapify</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> i, <span class="keyword">int</span> size)</span> </span>&#123; <span class="comment">// 堆化的维持需要用递归</span></span><br><span class="line">    <span class="keyword">int</span> ls = <span class="number">2</span>*i, rs = <span class="number">2</span>*i + <span class="number">1</span>; </span><br><span class="line">    <span class="keyword">int</span> large = i;</span><br><span class="line">    <span class="keyword">if</span>(ls &lt;= size &amp;&amp; a[ls] &gt; a[i]) &#123;</span><br><span class="line">        large = ls; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(rs &lt;= size &amp;&amp; a[rs] &gt; a[large]) &#123;</span><br><span class="line">        large = rs; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(large != i) &#123;</span><br><span class="line">        swap(a[i], a[large]);</span><br><span class="line">        heapify(a, large, size);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">buildHeap</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> size)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = size/<span class="number">2</span>; i &gt;= <span class="number">1</span>; i--) &#123;</span><br><span class="line">        heapify(a, i, size);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heapSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">    buildHeap(a, size);</span><br><span class="line">    <span class="keyword">int</span> len = size;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = len; i &gt;= <span class="number">2</span>; i--) &#123;</span><br><span class="line">        swap(a[i], a[<span class="number">1</span>]);</span><br><span class="line">        len--;</span><br><span class="line">        heapify(a, <span class="number">1</span>, len); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-quickSort"><a href="#2-quickSort" class="headerlink" title="2. quickSort"></a>2. quickSort</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(left &lt; right) &#123; <span class="comment">// exit. good idea!</span></span><br><span class="line">        <span class="keyword">int</span> l = left, r = right, x = a[l];</span><br><span class="line">        <span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[r] &gt;= x) r--;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[l] &lt;= x) l++;</span><br><span class="line">            <span class="keyword">if</span>(l &gt;= r) <span class="keyword">break</span>;</span><br><span class="line">            swap(a[r], a[l]);</span><br><span class="line">        &#125;</span><br><span class="line">        swap(a[left], a[l]);</span><br><span class="line">        quickSort(a, left, l<span class="number">-1</span>);</span><br><span class="line">        quickSort(a, l+<span class="number">1</span>, right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-mergeSort"><a href="#3-mergeSort" class="headerlink" title="3. mergeSort"></a>3. mergeSort</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123; <span class="comment">//  8, 5, 4, 9, 2, 3, 6</span></span><br><span class="line">    <span class="keyword">if</span>(l &gt;= r) <span class="keyword">return</span>;   <span class="comment">// exit.</span></span><br><span class="line">    <span class="keyword">int</span> mid = (l+r) / <span class="number">2</span>; <span class="comment">// overflow  &lt;-&gt;  l + (r-l)/2</span></span><br><span class="line">    mergeSort(a, l, mid);</span><br><span class="line">    mergeSort(a, mid+<span class="number">1</span>, r);  </span><br><span class="line">    <span class="keyword">int</span> *arr = <span class="keyword">new</span> <span class="keyword">int</span>[r-l+<span class="number">1</span>];  </span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> i = l, j = mid + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid &amp;&amp; j &lt;= r) &#123;  </span><br><span class="line">        <span class="keyword">if</span>(a[i] &lt;= a[j]) &#123;</span><br><span class="line">            arr[k++] = a[i++]; </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            arr[k++] = a[j++]; <span class="comment">// ans += (mid-i+1);  </span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid) arr[k++] = a[i++];</span><br><span class="line">    <span class="keyword">while</span>(j &lt;= r) arr[k++] = a[j++];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = l; i &lt;= r; i++) &#123;</span><br><span class="line">        a[i] = arr[i-l];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> []arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4-insertSort"><a href="#4-insertSort" class="headerlink" title="4. insertSort"></a>4. insertSort</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insertSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> j;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; i++) &#123;<span class="comment">// 新抓的每张扑克牌  </span></span><br><span class="line">        <span class="keyword">int</span> temp = a[i];</span><br><span class="line">        <span class="keyword">for</span>(j = i<span class="number">-1</span>; a[j] &gt; temp &amp;&amp; j &gt;= <span class="number">0</span>; j--) &#123;  </span><br><span class="line">            a[j+<span class="number">1</span>] = a[j];</span><br><span class="line">        &#125;</span><br><span class="line">        a[j+<span class="number">1</span>] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-bubbleSort"><a href="#5-bubbleSort" class="headerlink" title="5. bubbleSort"></a>5. bubbleSort</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubbleSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; len-i; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(a[j] &gt; a[j+<span class="number">1</span>]) swap(a[j], a[j+<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="6-selectSort"><a href="#6-selectSort" class="headerlink" title="6. selectSort"></a>6. selectSort</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">selectSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j, k;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len<span class="number">-1</span>; i++) &#123;  </span><br><span class="line">        k = i;  </span><br><span class="line">        <span class="keyword">for</span>(j = i+<span class="number">1</span>; j &lt; len; j++) &#123;  </span><br><span class="line">            <span class="keyword">if</span>(a[j] &lt; a[k]) k = j;  </span><br><span class="line">        &#125;  </span><br><span class="line">        swap(a[i], a[k]);  <span class="comment">// 将第i位小的数放入i位置  </span></span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[搜索的经典 POJ 2386 Lake Counting]]></title>
      <url>http://sggo.me/2010/09/27/acm/dfs-POJ-2386/</url>
      <content type="html"><![CDATA[<p>经典的搜索，寻找水泡 poj 2386 <a href="http://poj.org/problem?id=2386" target="_blank" rel="external">Lake Counting</a></p>
<a id="more"></a>
<h2 id="Lake-Counting"><a href="#Lake-Counting" class="headerlink" title="Lake Counting"></a><a href="http://poj.org/problem?id=2386" target="_blank" rel="external">Lake Counting</a></h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAX = <span class="number">101</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> asd[<span class="number">202</span>];</span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"><span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">char</span> c;</span><br><span class="line"><span class="keyword">int</span> <span class="built_in">map</span>[MAX][MAX];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">digui</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (a &gt;= <span class="number">0</span> &amp; &amp; a &lt; n &amp; &amp; b &gt;= <span class="number">0</span> &amp; &amp; b &lt; m &amp; &amp; <span class="built_in">map</span>[a][b] == <span class="number">-2</span>) &#123;</span><br><span class="line">        <span class="built_in">map</span>[a][b] = k; <span class="comment">// 这是很重要的标记</span></span><br><span class="line">        digui(a + <span class="number">1</span>, b, k);</span><br><span class="line">        digui(a, b + <span class="number">1</span>, k);</span><br><span class="line">        digui(a - <span class="number">1</span>, b, k);</span><br><span class="line">        digui(a, b - <span class="number">1</span>, k);</span><br><span class="line">        digui(a + <span class="number">1</span>, b + <span class="number">1</span>, k);</span><br><span class="line">        digui(a + <span class="number">1</span>, b - <span class="number">1</span>, k);</span><br><span class="line">        digui(a - <span class="number">1</span>, b + <span class="number">1</span>, k);</span><br><span class="line">        digui(a - <span class="number">1</span>, b - <span class="number">1</span>, k);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; m; j++)&#123;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; c;</span><br><span class="line">            <span class="keyword">if</span> (c == <span class="string">'W'</span>) <span class="built_in">map</span>[i][j] = <span class="number">-2</span>; <span class="comment">// 这的标记要慎重，最好用负数，不避免和K重复</span></span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">map</span>[i][j] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; m; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">map</span>[i][j] == <span class="number">-2</span>) &#123;</span><br><span class="line">                ans++;</span><br><span class="line">                digui(i, j, ans);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[格式化输出 printf 与 输出流 cout]]></title>
      <url>http://sggo.me/2010/09/25/acm/cout_vs_printf/</url>
      <content type="html"><![CDATA[<p>printf 是 格式化输出 ， cout 是输出流</p>
<p>今天无意中发现 cout 和 printf 是有点区别的，一个是输出语句，一个是输出函数</p>
<a id="more"></a>
<h2 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> a=<span class="number">0</span>; a&lt;<span class="number">24</span>; a++) &#123;</span><br><span class="line">    </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"++++++++++++\n"</span>);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"============\n"</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"############\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"@@@@@@@@@@@\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">++++++++++++</span><br><span class="line"></span><br><span class="line">++++++++++++</span><br><span class="line"></span><br><span class="line">############</span><br><span class="line"></span><br><span class="line">############</span><br><span class="line"></span><br><span class="line">@@@@@@@@@@@</span><br><span class="line"></span><br><span class="line">============</span><br><span class="line"></span><br><span class="line">============</span><br></pre></td></tr></table></figure>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>cout 先是把输出结果存到缓存区，然后一次性输出，其实 cout 输出的时候也是调用了printf函数；<br>pintf函数是每次输出结果; 这就是为了在 TIME程序中 cout运行的时间要比printf函数快的原因；</p>
<blockquote>
<p>注意： cout&lt;&lt;”==\n”; 和 cout&lt;&lt;”==”&lt;&lt;endl; 也有不同，\n只是一个字符，而endl会将缓存区的数据全部输出并清零；在TIME程序中，如果采用cout&lt;&lt;”============”&lt;&lt;endl;结构反而会比printf慢，因为多了一道调用缓存手续</p>
</blockquote>
<h2 id="形象比喻"><a href="#形象比喻" class="headerlink" title="形象比喻"></a>形象比喻</h2><p>下面打个很形象的比喻，大家都会明白, 比如有100个鸡蛋，需要从A拿到B点，相当于程序中的输出:</p>
<blockquote>
<ul>
<li>printf把鸡蛋一次一个用手拿到B点</li>
<li>cout&lt;&lt;”  \n” 先把鸡蛋全部放到篮子，然后一次性拿过去取出</li>
<li>cout&lt;&lt;” “&lt;&lt;endl 先把鸡蛋放到篮子，然后一次一个拿过去再取出</li>
</ul>
</blockquote>
<p>尤其可见哪个快那个慢显而易见</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[格式化输入 scanf 与 输入流 cin]]></title>
      <url>http://sggo.me/2010/09/24/acm/cin_vs_scanf/</url>
      <content type="html"><![CDATA[<p>scanf 是格式化输入，printf是格式化输出。</p>
<p>cin是输入流，cout是输出流。效率稍低，但书写简便。</p>
<a id="more"></a>
<h2 id="格式化输出-与-流输出"><a href="#格式化输出-与-流输出" class="headerlink" title="格式化输出 与 流输出"></a>格式化输出 与 流输出</h2><ul>
<li>格式化输出效率比较高，但是写代码麻烦。</li>
<li>流输出操作效率稍低，但书写简便。</li>
</ul>
<blockquote>
<p>cout之所以效率低，是先把要输出的东西存入缓冲区，再输出，导致效率降低。</p>
</blockquote>
<p><strong>缓冲区比较抽象，举个例子吧：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">'a'</span>;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; i;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>运行结果什么都没看到输出，输入一个整型比如3再按回车后ab同时显示出来了。<br>但是这样的情况并不是经常发生，是在一些比较大型的工程中偶尔出现，原因是字符a先到了缓冲区，但是没输出，等输入了i，b进入缓冲区后再一并输出的。流输入也是差不多的。</p>
</blockquote>
<p><strong>C++ 中 iostream.h 与 stdio.h</strong></p>
<blockquote>
<p>iostream.h 和 stdio.h 是 C++ 的两个头文件，里面是对于一些类，操作符，函数的定义，头文件本身没有好坏之分吧~~</p>
<p>只是发展到C++，都比较提倡用iostream.h罢了，因为这样写代码简单。</p>
</blockquote>
]]></content>
    </entry>
    
  
  
</search>
