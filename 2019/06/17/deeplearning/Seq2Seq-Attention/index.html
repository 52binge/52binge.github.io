<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Seq2Seq and Attention - Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="Seq2Seq">
<meta property="og:type" content="article">
<meta property="og:title" content="Seq2Seq and Attention">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;2019&#x2F;06&#x2F;17&#x2F;deeplearning&#x2F;Seq2Seq-Attention&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Seq2Seq-00.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W3-3.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W3-4.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Seq2Seq-03.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;chatbot&#x2F;seq2seq-5.jpeg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W3-5.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W3-6_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W3-7_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W3-8_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W3-10_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W3-11_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;chatbot&#x2F;seq2seq-6.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;chatbot&#x2F;seq2seq-2.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;chatbot&#x2F;seq2seq-3.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;chatbot&#x2F;seq2seq-4.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Attention-01.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;chatbot&#x2F;seq2seq-7.jpeg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Attention-00.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Attention-04.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Attention-03.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Attention-05.jpg">
<meta property="og:updated_time" content="2019-10-20T04:30:35.019Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;Seq2Seq-00.jpg">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<!-- jiangting add start... @2020.08.30 -->
<!-- <div id="menu" class="duration-main" style="background-color: #e7e7e7"> -->
<!--   <p class="links-p" href="/">Blair</p> -->
<!--   <address class="icons"> -->
<!--     <a href="https://github.com/blair101" class="icon-font icon linkedin" target="_blank"></a> -->
<!--     <a href="https://cn.linkedin.com/pub/tianyu-dai/a8/818/44a" class="icon-font icon linkedin" target="_blank"></a> -->
<!--   </address> -->
<!--   <div class="hr1"></div> -->
<!--   <nav> -->
<!--     <div> -->
<!--     <a class="home" href="/">Home</a></div> -->
<!--     <p class="links-p" href="/">Home</p> -->
<!--     <nav class="tag-ath"> -->
<!--       <a class="proj" href="/categories">Category</a> -->
<!--       <a class="authors" href="/about">About</a> -->
<!--     </nav> -->
<!--   </nav> -->
<!--   <div class="hr2"></div> -->
<!--     <p class="links-p">Links</p> -->
<!--       <address class="links"> -->
<!--       <a class="proj" href="/article/Create-MyWorld">Projects</a> -->
<!--       <a class="friend">Friends</a></address><div class="hr3"> -->
<!--   </div> -->
<!--   <p class="end"></p> -->
<!--   <div id="menu-links" class="duration-main" style="top: -400px; background-color: #f5f5f5"> -->
<!--     <address> -->
<!--       <li><a target="_blank" href="http://lm7.xxxxxxxx.jp">Lm7</a></li> -->
<!--       <li><a target="_blank" href="http://www.pixiv.net/member.php?id=4933015">Domik</a></li> -->
<!--       <li><a target="_blank" href="http://hana-ui.moe">hana-ui</a></li> -->
<!--       <li><a target="_blank" href="http://fil.dtysky.moe">F-I-L</a></li> -->
<!--       <li><a target="_blank" href="http://paradise.dtysky.moe">Paradise</a></li> -->
<!--       <li><a target="_blank" href="http://moe-notes.dtysky.moe">MoeNotes</a></li> -->
<!--       <li><a target="_blank" href="http://kanata.dtysky.moe">Kanata</a></li> -->
<!--       <li><a target="_blank" href="http://blog.nekohand.moe">Nekohand</a></li> -->
<!--       <li><a target="_blank" href="http://www.jerryfu.net">JerryFu</a></li> -->
<!--       <li><a target="_blank" href="http://kawabangga.com">南史</a></li> -->
<!--       <li><a>Hide Links</a></li> -->
<!--       <li><a></a></li> -->
<!--     </address> -->
<!--   </div> -->
<!-- </div> -->
<!-- jiangting add end !  @2020.08.30 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-deeplearning/Seq2Seq-Attention" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Seq2Seq and Attention
      <small class=article-detail-date-index>&nbsp; 2019-06-17</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/17/deeplearning/Seq2Seq-Attention/" class="article-date">
  <time datetime="2019-06-17T02:00:21.000Z" itemprop="datePublished">2019-06-17</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/17/deeplearning/Seq2Seq-Attention/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/deeplearning/Seq2Seq-00.jpg" width="550" alt="Attention 和人类的选择性视觉注意力机制类似" /></p>
<a id="more"></a>
<p>我们先结合上篇文章的内容，将 language model 和 Machine translation model 做一个对比：</p>
<p><img src="/images/deeplearning/C5W3-3.png" width="600" /></p>
<p><img src="/images/deeplearning/C5W3-4.png" width="700" /></p>
<p>可以看到，机器翻译模型的后半部分其实就是语言模型，Andrew 将其称之为 “<strong>条件语言模型</strong>”.</p>
<script type="math/tex; mode=display">
P(y^{<1>},…,y^{<{T\_y}>}|x^{<1>},…,x^{<{T\_x}>})</script><h2 id="1-Encoder-Decoder"><a href="#1-Encoder-Decoder" class="headerlink" title="1. Encoder-Decoder"></a>1. Encoder-Decoder</h2><p><img src="/images/deeplearning/Seq2Seq-03.jpg" width="600" alt="Encoder-Decoder" /></p>
<p>Source 和 Target 分别由各自的单词序列构成：</p>
<script type="math/tex; mode=display">
Source = ({x}\_1, {x}\_2, ..., {x}\_m)</script><script type="math/tex; mode=display">
Target = ({y}\_1, {y}\_2, ..., {y}\_n)</script><p>Encoder 顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p>
<script type="math/tex; mode=display">
C = F({x}\_1, {x}\_2, ..., {x}\_m)</script><p>对于 Decoder 来说，其任务是根据句子 Source 的 中间语义表示 C 和 之前已经生成的历史信息</p>
<script type="math/tex; mode=display">
({y}\_1, {y}\_2, ..., {y}\_{i-1})</script><p>来生成 i时刻 要生成的单词 ${y}_{i}$</p>
<script type="math/tex; mode=display">
y\_{i} = g(C, {y}\_1, {y}\_2, ..., {y}\_{i-1})</script><blockquote>
<p>每个 $y_i$ 都依次这么产生，那么看起来就是整个系统根据输入 句子Source 生成了目标句子Target。</p>
<p>(1). 如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题；<br>(2). 如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要；<br>(3). 如果Source是一句问句，Target是一句回答，那么这是问答系统。</p>
<p>Encoder-Decoder框架 不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用.</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p>
<h3 id="1-1-encoder"><a href="#1-1-encoder" class="headerlink" title="1.1 encoder"></a>1.1 encoder</h3><p>用函数 $f$ 表达 RNN 隐藏层的变换：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}\_t = f(\boldsymbol{x}\_t, \boldsymbol{h}\_{t-1}).</script><p>然后 Encoder 通过自定义函数 $q$ 将各个时间步的隐藏状态变换为背景变量</p>
<script type="math/tex; mode=display">
\boldsymbol{c} =  q(\boldsymbol{h}\_1, \ldots, \boldsymbol{h}\_T).</script><p>例如，当选择 $q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) = \boldsymbol{h}_T$ 时，背景变量是输入序列最终时间步的隐藏状态 $\boldsymbol{h}_T$。</p>
<blockquote>
<p>以上描述的编码器是一个单向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用 Bi-RNN 构造编码器。 这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。</p>
</blockquote>
<p><img src="/images/chatbot/seq2seq-5.jpeg" width="700" /></p>
<h3 id="1-2-decoder"><a href="#1-2-decoder" class="headerlink" title="1.2 decoder"></a>1.2 decoder</h3><blockquote>
<p>上小节 Encode 编码器输出的背景变量 $c$ 编码了整个输入序列 $x_1, \ldots, x_T$ 的信息。</p>
</blockquote>
<p>给定 train sample 的 input sequence： $y_1, y_2, \ldots, y_{T’}$，对每个时间步 $t’$（符号与 input sequence 或 encoder 的时间步 $t$ 有区别）， decoder 输出 $y_{t’}$ 的条件概率将基于之前的 output sequence： $y_1,\ldots,y_{t’-1}$ 和 $c$.</p>
<p><strong>即:</strong> </p>
<script type="math/tex; mode=display">
P(y\_{t'} \mid y\_1, \ldots, y\_{t'-1}, \boldsymbol{c})</script><p>为此，我们可以使用<code>另一个RNN</code>作为解码器。 在输出序列的时间步 $t^\prime$，解码器将上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $c$ 作为输入，并将它们与上一时间步的隐藏状态 $\boldsymbol{h}_{t^\prime-1}$ 变换为当前时间步的隐藏状态 $\boldsymbol{h}_{t^\prime}$。因此，我们可以用函数 $g$ 表达解码器隐藏层的变换：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}\_{t^\prime} = g(y\_{t^\prime-1}, \boldsymbol{c}, \boldsymbol{h}\_{t^\prime-1}).</script><p>可使用自定义的 output layer 和 softmax 计算 ${P}(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})$，计算当前时间步输出 $y_{t^\prime}$ 的概率分布.</p>
<h3 id="1-3-decoder-greedy-search"><a href="#1-3-decoder-greedy-search" class="headerlink" title="1.3 decoder greedy search"></a>1.3 decoder greedy search</h3><p>在语言模型之前有一 个条件也就是被翻译的句子:</p>
<script type="math/tex; mode=display">
P(y^{<1>},…,y^{<{T\_y}>}|x^{<1>},…,x^{<{T\_x}>})</script><blockquote>
<p>但是我们知道翻译是有很多种方式的，同一句话可以翻译成很多不同的句子，那么如何判断哪一句子是最好的呢？</p>
<p>还是翻译上面那句话，有如下几种翻译结果：</p>
<ul>
<li>“Jane is visiting China in September.”</li>
<li>“Jane is going to visit China in September.”</li>
<li>“In September, Jane will visit China”</li>
<li>“Jane’s Chinese friend welcomed her in September.”</li>
<li>….</li>
</ul>
</blockquote>
<p>得到最好的翻译结果，转换成数学公式就是:</p>
<script type="math/tex; mode=display">
argmax P(y^{<1>},…,y^{<{T\_y}>}|x^{<1>},…,x^{<{T\_x}>})</script><p>那么 Greedy Search 就是每次输出的那个都必须是最好的。还是以翻译那句话为例。</p>
<blockquote>
<p>现在假设通过贪婪搜索已经确定最好的翻译的前两个单词是：”Jane is “</p>
<p>然后因为 “going” 出现频率较高和其它原因，所以根据贪婪算法得出此时第三个单词的最好结果是 “going”。</p>
<p>所以据贪婪算法最后的翻译结果可能是下图中的第二个句子，<strong>但第一句可能会更好.</strong></p>
<p><img src="/images/deeplearning/C5W3-5.png" width="600" /></p>
<p>所以 Greedy Search 的缺点是局部最优并不代表全局最优. Greedy Search 更加短视，看的不长远。</p>
</blockquote>
<h3 id="1-4-decoder-beam-search"><a href="#1-4-decoder-beam-search" class="headerlink" title="1.4 decoder beam search"></a>1.4 decoder beam search</h3><p>Beam Search 是 greedy search 的加强版本，首先要预设一个值 beam width，这里等于 <code>3</code> (如果等于 1 就是 greedy search)。然后在每一步保存最佳的 3 个结果进行下一步的选择，以此直到遇到句子的终结符.</p>
<h4 id="1-4-1-step-1"><a href="#1-4-1-step-1" class="headerlink" title="1.4.1 step 1"></a>1.4.1 step 1</h4><p>如下图示，因为beam width=3，所以根据输入的需要翻译的句子选出 3 个 $y^{<1>}$最可能的输出值。</p>
<p>即选出 $P(y^{<1>}|x)$ 最大的前3个值。 假设分别是 <strong>“in”, “jane”, “september”</strong></p>
<p><img src="/images/deeplearning/C5W3-6_1.png" width="650" /></p>
<h4 id="1-4-2-step-2"><a href="#1-4-2-step-2" class="headerlink" title="1.4.2 step 2"></a>1.4.2 step 2</h4><p>以”<strong>in</strong>“为例进行说明，其他同理.</p>
<p>如下图示，在给定被翻译句子 $x$ 和确定 $y^{<1>}$ = “<strong>in</strong>“ 的条件下，下一个输出值的条件概率是 $P(y^{<2>}|x,”in”)$。</p>
<p>此时需要从 10000 种可能中找出条件概率最高的前 3 个.</p>
<p>又由公式:</p>
<script type="math/tex; mode=display">
P(y^{<1>},y^{<2>}|x)=P(y^{<1>}|x) P(y^{<2>}|x, y^{<1>})</script><p>我们此时已经得到了给定输入数据，前两个输出值的输出概率比较大的组合了.</p>
<p><img src="/images/deeplearning/C5W3-7_1.png" width="650" /></p>
<p>另外 2 个单词也做同样的计算</p>
<p><img src="/images/deeplearning/C5W3-8_1.png" width="650" /></p>
<p>此时我们得到了 9 组 $P(y^{<1>},y^{<2>}|x)$, 此时我们再从这 9组 中选出概率值最高的前 3 个。</p>
<p>如下图示，假设是这3个：</p>
<blockquote>
<ul>
<li>“in september”</li>
<li>“jane is”</li>
<li>“jane visits”</li>
</ul>
</blockquote>
<h4 id="1-4-3-step-3"><a href="#1-4-3-step-3" class="headerlink" title="1.4.3 step 3"></a>1.4.3 step 3</h4><p>继续 step 2 的过程，根据 $P(y^{<3>}|x,y^{<1>},y^{<2>})$ 选出 $P(y^{<1>},y^{<2>},y^{<3>}|x)$ 最大的前3个组合.</p>
<p>后面重复上述步骤得出结果.</p>
<h4 id="1-4-4-summary"><a href="#1-4-4-summary" class="headerlink" title="1.4.4 summary"></a>1.4.4 summary</h4><p>总结一下上面的步骤就是：</p>
<blockquote>
<ul>
<li><p>(1). 经过 encoder 以后，decoder 给出最有可能的三个开头词依次为 “in”, “jane”, “september” </p>
<script type="math/tex; mode=display">P(y^{<1>}|x)</script></li>
<li><p>(2). 经 step 1 得到的值输入到 step 2 中，最可能的三个翻译为 “in september”, “jane is”, “jane visits” </p>
</li>
</ul>
<script type="math/tex; mode=display">P(y^{<2>}|x,y^{<1>})</script><p>(这里，september开头的句子由于概率没有其他的可能性大，已经失去了作为开头词资格)</p>
<ul>
<li>(3). 继续这个过程… </li>
</ul>
<script type="math/tex; mode=display">P(y^{<3>}|x,y^{<1>},y^{<2>})</script></blockquote>
<p><img src="/images/deeplearning/C5W3-10_1.png" width="750" /></p>
<h3 id="1-5-refinements-to-beam-search"><a href="#1-5-refinements-to-beam-search" class="headerlink" title="1.5 refinements to beam search"></a>1.5 refinements to beam search</h3><script type="math/tex; mode=display">
P(y^{<1>},….,P(y^{T\_y})|x)=P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})…P(y^{<{T\_y}>}|x,y^{<1>},…y^{<{T\_y-1}>})</script><p>所以要满足 $argmax P(y^{<1>},….,P(y^{T_y})|x)$, 也就等同于要满足</p>
<script type="math/tex; mode=display">
argmax \prod\_{t=1}^{T\_y}P(y^{<{t}>}|x,y^{<1>},…y^{<{t-1}>})</script><p>但是上面的公式存在一个问题，因为概率都是小于1的，累乘之后会越来越小，可能小到计算机无法精确存储，所以可以将其转变成 log 形式（因为 log 是单调递增的，所以对最终结果不会有影响），其公式如下：</p>
<script type="math/tex; mode=display">
argmax \sum\_{t=1}^{T\_y}logP(y^{<{t}>}|x,y^{<1>},…y^{<{t-1}>})</script><blockquote>
<p>But！！！上述公式仍然存在bug，观察可以知道，概率值都是小于1的，那么log之后都是负数，所以为了使得最后的值最大，那么只要保证翻译的句子越短，那么值就越大，所以如果使用这个公式，那么最后翻译的句子通常都是比较短的句子，这显然不行。</p>
</blockquote>
<p>所以我们可以通过归一化的方式来纠正，即保证平均到每个单词都能得到最大值。其公式如下：</p>
<script type="math/tex; mode=display">
argmax \frac{1}{T\_y}\sum\_{t=1}^{T\_y}logP(y^{<{t}>}|x,y^{<1>},…y^{<{t-1}>})</script><p>归一化的确能很好的解决上述问题，但是在实际运用中，会额外添加一个参数 $α$, 其大小介于 0 和 1 之间</p>
<script type="math/tex; mode=display">
argmax \frac{1}{T\_y^α}\sum\_{t=1}^{T\_y}logP(y^{<{t}>}|x,y^{<1>},…y^{<{t-1}>})</script><p><img src="/images/deeplearning/C5W3-11_1.png" width="700" /></p>
<blockquote>
<p>$T_y$ 为输出句子中单词的个数，$α$ 是一个超参数 (可以设置为 0.7)</p>
<p>$α$ == 1. 则代表 完全用句子长度归一化<br>$α$ == 0. 则代表 没有归一化<br>$α$ == 0~1. 则代表 在 句子长度归一化 与 没有归一化 之间的折中程度.</p>
<p>beam width = B = 3~<strong>10</strong>~100 是会有一个明显的增长，但是 B 从 1000 ~ 3000 是并没有一个明显增长的.</p>
</blockquote>
<h3 id="1-6-train-seq2seq-model"><a href="#1-6-train-seq2seq-model" class="headerlink" title="1.6 train seq2seq model"></a>1.6 train seq2seq model</h3><p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned}
{P}(y\_1, \ldots, y\_{T'} \mid x\_1, \ldots, x\_T)
&= \prod\_{t'=1}^{T'} {P}(y\_{t'} \mid y\_1, \ldots, y\_{t'-1}, x\_1, \ldots, x\_T)\\\\
&= \prod\_{t'=1}^{T'} {P}(y\_{t'} \mid y\_1, \ldots, y\_{t'-1}, \boldsymbol{c}),
\end{aligned}\end{split}</script><p>并得到该输出序列的损失</p>
<script type="math/tex; mode=display">- \log{P}(y\_1, \ldots, y\_{T'} \mid x\_1, \ldots, x\_T) = -\sum\_{t'=1}^{T'} \log {P}(y\_{t'} \mid y\_1,  \ldots, y\_{t'-1}, \boldsymbol{c}),</script><p><img src="/images/chatbot/seq2seq-6.png" width="800" /></p>
<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图中所描述的模型预测中，我们需要将decode在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列在上一个时间步的标签作为decode在当前时间步的输入。这叫做强制教学（teacher forcing）。</p>
<h3 id="1-7-summary"><a href="#1-7-summary" class="headerlink" title="1.7 summary"></a>1.7 summary</h3><ul>
<li>编码器 - 解码器（seq2seq）可以输入并输出不定长的序列。</li>
<li>编码器—解码器使用了两个 RNN。</li>
<li>在编码器—解码器的训练中，我们可以采用 teacher forcing。(这也是 Seq2Seq 2 的内容)</li>
</ul>
<h2 id="2-Seq2Seq-框架2"><a href="#2-Seq2Seq-框架2" class="headerlink" title="2. Seq2Seq 框架2"></a>2. Seq2Seq 框架2</h2><p>Seq2Seq model 来自于 “<a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a>”</p>
<p>其模型结构图如下所示：</p>
<p><img src="/images/chatbot/seq2seq-2.jpg" width="700" /></p>
<p>与上面模型最大的区别在于其source编码后的 向量$C$ 直接作为 Decoder RNN 的 init state，而不是在每次decode时都作为 RNN cell 的输入。此外，decode 时 RNN 的输入是 label，而不是前一时刻的输出。</p>
<p>Encoder 阶段：</p>
<p><img src="/images/chatbot/seq2seq-3.jpg" width="500" /></p>
<blockquote>
<p>每个词经过 RNN 都会编码为 hidden (e0,e1,e2), source序列 的编码向量e 就是 最终的 hidden state e2</p>
<p>Tips： 这里 $e_0, e_1, e_2$ 是 hidden state， 并没有经过 g 和 softmax .</p>
</blockquote>
<p>Decoder 阶段：</p>
<p><img src="/images/chatbot/seq2seq-4.jpg" width="500" /></p>
<p>e向量 仅作为 RNN 的 init state 传入decode模型，每一时刻输入都是前一时刻的正确label。直到最终输入<eos>符号截止.</p>
<h2 id="3-Seq2Seq-Attention"><a href="#3-Seq2Seq-Attention" class="headerlink" title="3. Seq2Seq Attention"></a>3. Seq2Seq Attention</h2><p>请务必要阅读： <a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">张俊林 深度学习中的注意力模型（2017版）</a></p>
<p><img src="/images/deeplearning/Attention-01.jpg" width="600" /></p>
<p><strong>decode</strong> 在各个时间步依赖相同的 <strong>背景变量 $c$</strong> 来获取输入序列信息。当 <strong>encode</strong> 为 RNN 时，<strong>背景变量$c$</strong> 来自它最终时间步的 hidden state。</p>
<blockquote>
<p>英语输入：“They”、“are”、“watching”、“.”<br>法语输出：“Ils”、“regardent”、“.”</p>
<p>翻译例子：输入为英语序列“They”、“are”、“watching”、“.”，输出为法语序列“Ils”、“regardent”、“.”。，<strong>decode</strong> 在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步 1，解码器可以主要依赖“They”、“are”的信息来生成“Ils”，在时间步 2 则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步 3 则直接映射句号“.”。这看上去就像是在 <strong>decode</strong> 的每一时间步对输入序列中不同时间步的编码信息分配不同的注意力一样。这也是注意力机制的由来 <a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="noopener">1</a>。</p>
<p>仍以 RNN 为例，Attention 通过对 Encode 所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量$c$。Decode 在每一时间步调整这些权重，即 Attention weight，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量$c$。本节我们将讨论 Attention机制 是怎么工作的。</p>
</blockquote>
<p>在“encoder-decoder（seq2seq）”, Decoder 在时间步 $t’$ 的 hidden state</p>
<script type="math/tex; mode=display">
\boldsymbol{s}\_{t'} = g(\boldsymbol{y}\_{t'-1}, \boldsymbol{c}, \boldsymbol{s}\_{t'-1})</script><p>在 Attention机制 中, Decoder 的每一时间步将使用可变的背景变量$c$</p>
<script type="math/tex; mode=display">
\boldsymbol{s}\_{t'} = g(\boldsymbol{y}\_{t'-1}, \boldsymbol{c}\_{t'}, \boldsymbol{s}\_{t'-1}).</script><p>关键是如何计算背景变量 $\boldsymbol{c}_{t’}$ 和如何利用它来更新隐藏状态 $\boldsymbol{s}_{t’}$。以下将分别描述这两个关键点。</p>
<h3 id="3-1-计算背景变量-c"><a href="#3-1-计算背景变量-c" class="headerlink" title="3.1 计算背景变量 c"></a>3.1 计算背景变量 c</h3><script type="math/tex; mode=display">
\boldsymbol{c}\_{t'} = \sum\_{t=1}^T \alpha\_{t' t} \boldsymbol{h}\_t,</script><p>其中给定 $t’$ 时，权重 $\alpha_{t’ t}$ 在 $t=1,\ldots,T$ 的值是一个概率分布。为了得到概率分布，可以使用 softmax 运算:</p>
<script type="math/tex; mode=display">
\alpha\_{t' t} = \frac{\exp(e\_{t' t})}{ \sum\_{k=1}^T \exp(e\_{t' k}) },\quad t=1,\ldots,T.</script><p>现在，我们需要定义如何计算上式中 softmax 运算的输入 $e_{t’ t}$。由于 $e_{t’ t}$ 同时取决于decode的时间步 $t’$ 和encode的时间步 $t$，我们不妨以解码器在时间步 $t’−1$ 的隐藏状态 $\boldsymbol{s}_{t’ - 1}$ 与编码器在时间步 $t$ 的隐藏状态 $h_t$ 为输入，并通过函数 $a$ 计算 $e_{t’ t}$：</p>
<script type="math/tex; mode=display">
e\_{t' t} = a(\boldsymbol{s}\_{t' - 1}, \boldsymbol{h}\_t).</script><p>这里函数 a 有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积 $a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出Attention机制的论文则将输入连结后通过含单隐藏层的多层感知机MLP 变换 </p>
<script type="math/tex; mode=display">
a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}\_s \boldsymbol{s} + \boldsymbol{W}\_h \boldsymbol{h}),</script><p>其中 $v、W_s、W_h$ 都是可以学习的模型参数。</p>
<h3 id="3-2-update-hidden-state"><a href="#3-2-update-hidden-state" class="headerlink" title="3.2 update hidden state"></a>3.2 update hidden state</h3><p>以 GRU 为例，在解码器中我们可以对 GRU 的设计稍作修改。解码器在时间步 $t’$ 的隐藏状态为</p>
<script type="math/tex; mode=display">
\boldsymbol{s}\_{t'} = \boldsymbol{z}\_{t'} \odot \boldsymbol{s}\_{t'-1}  + (1 - \boldsymbol{z}\_{t'}) \odot \tilde{\boldsymbol{s}}\_{t'},</script><p>其中的重置门、更新门和候选隐含状态分别为 :</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned}
\boldsymbol{r}\_{t'} &= \sigma(\boldsymbol{W}\_{yr} \boldsymbol{y}\_{t'-1} + \boldsymbol{W}\_{sr} \boldsymbol{s}\_{t' - 1} + \boldsymbol{W}\_{cr} \boldsymbol{c}\_{t'} + \boldsymbol{b}\_r),\\\\
\boldsymbol{z}\_{t'} &= \sigma(\boldsymbol{W}\_{yz} \boldsymbol{y}\_{t'-1} + \boldsymbol{W}\_{sz} \boldsymbol{s}\_{t' - 1} + \boldsymbol{W}\_{cz} \boldsymbol{c}\_{t'} + \boldsymbol{b}\_z),\\\\
\tilde{\boldsymbol{s}}\_{t'} &= \text{tanh}(\boldsymbol{W}\_{ys} \boldsymbol{y}\_{t'-1} + \boldsymbol{W}\_{ss} (\boldsymbol{s}\_{t' - 1} \odot \boldsymbol{r}\_{t'}) + \boldsymbol{W}\_{cs} \boldsymbol{c}\_{t'} + \boldsymbol{b}\_s),
\end{aligned}\end{split}</script><p>其中含下标的 W 和 b 分别为 GRU 的权重参数和偏差参数。</p>
<p><img src="/images/chatbot/seq2seq-7.jpeg" width="800" /></p>
<h3 id="3-3-attention-summary"><a href="#3-3-attention-summary" class="headerlink" title="3.3 attention summary"></a>3.3 attention summary</h3><ul>
<li>可以在decode的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。</li>
<li>Attention机制可以采用更为高效的矢量化计算。</li>
</ul>
<p>除此之外模型为了取得比较好的效果还是用了下面三个小技巧来改善性能：</p>
<blockquote>
<p>深层次的LSTM：作者使用了4层LSTM作为encoder和decoder模型，并且表示深层次的模型比shallow的模型效果要好（单层，神经元个数多）。</p>
<p>将source进行反序输入：输入的时候将“ABC”变成“CBA”，这样做的好处是解决了长序列的long-term依赖，使得模型可以学习到更多的对应关系，从而达到比较好的效果。</p>
</blockquote>
<p>注意力机制是一种思想，可以有多种不同的实现方式，在 Seq2Seq 模型以外的场景也有不少应用</p>
<p><img src="/images/deeplearning/Attention-00.jpg" width="700" /></p>
<h2 id="4-Attention-本质思想"><a href="#4-Attention-本质思想" class="headerlink" title="4. Attention 本质思想"></a>4. Attention 本质思想</h2><p>把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易懂:</p>
<p><img src="/images/deeplearning/Attention-04.jpg" width="600" /></p>
<h3 id="4-1-Attention-的三阶段"><a href="#4-1-Attention-的三阶段" class="headerlink" title="4.1 Attention 的三阶段"></a>4.1 Attention 的三阶段</h3><blockquote>
<ol>
<li>第一个阶段根据Query和Key计算两者的相似性或者相关性；</li>
<li>第二个阶段对第一阶段的原始分值进行归一化处理；</li>
<li>根据权重系数对Value进行加权求和。</li>
</ol>
</blockquote>
<p><img src="/images/deeplearning/Attention-03.jpg" width="600" /></p>
<h3 id="4-2-Self-Attention"><a href="#4-2-Self-Attention" class="headerlink" title="4.2 Self Attention"></a>4.2 Self Attention</h3><p>Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</p>
<p><img src="/images/deeplearning/Attention-05.jpg" width="600" /></p>
<blockquote>
<p>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p>
</blockquote>
<p>请务必要阅读： <a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">张俊林 深度学习中的注意力模型（2017版）</a></p>
<hr>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="noopener">动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32092871" target="_blank" rel="noopener">seq2seq+Attention机制模型详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习前沿笔记</a></li>
<li><a href="http://www.iterate.site/2019/04/19/05-seq2seq%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">百面 seq2seq模型</a></li>
<li><a href="http://www.iterate.site/2019/04/19/06-注意力机制/" target="_blank" rel="noopener">百面 注意力机制</a></li>
<li><a href="https://kexue.fm/archives/6736" target="_blank" rel="noopener">Bert遇上Keras</a></li>
<li><a href="/2018/08/14/deeplearning/Sequence-Models-week3/">Sequence-Models-week3</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28048246" target="_blank" rel="noopener">seq2seq中的beam search算法过程</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型（2017版）</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io" target="_blank" rel="noopener"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Encoder-Decoder"><span class="toc-text">1. Encoder-Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-encoder"><span class="toc-text">1.1 encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-decoder"><span class="toc-text">1.2 decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-decoder-greedy-search"><span class="toc-text">1.3 decoder greedy search</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-decoder-beam-search"><span class="toc-text">1.4 decoder beam search</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-1-step-1"><span class="toc-text">1.4.1 step 1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-2-step-2"><span class="toc-text">1.4.2 step 2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-3-step-3"><span class="toc-text">1.4.3 step 3</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-4-summary"><span class="toc-text">1.4.4 summary</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-refinements-to-beam-search"><span class="toc-text">1.5 refinements to beam search</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-train-seq2seq-model"><span class="toc-text">1.6 train seq2seq model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-summary"><span class="toc-text">1.7 summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Seq2Seq-框架2"><span class="toc-text">2. Seq2Seq 框架2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Seq2Seq-Attention"><span class="toc-text">3. Seq2Seq Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-计算背景变量-c"><span class="toc-text">3.1 计算背景变量 c</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-update-hidden-state"><span class="toc-text">3.2 update hidden state</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-attention-summary"><span class="toc-text">3.3 attention summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Attention-本质思想"><span class="toc-text">4. Attention 本质思想</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Attention-的三阶段"><span class="toc-text">4.1 Attention 的三阶段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Self-Attention"><span class="toc-text">4.2 Self Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/Seq2Seq/" rel="tag">Seq2Seq</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/06/20/nlp/BERT_tutorial_2/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          BERT 完全指南
        
      </div>
    </a>
  
  
    <a href="/2019/06/16/nlp/Language-Model-and-Word-Embedding/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Language Model and Perplexity&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2019/06/17/deeplearning/Seq2Seq-Attention/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
