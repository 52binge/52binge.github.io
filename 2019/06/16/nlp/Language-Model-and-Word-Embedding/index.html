<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Language Model and Perplexity - Melbourne Australia</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="PPL">
<meta property="og:type" content="article">
<meta property="og:title" content="Language Model and Perplexity">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;2019&#x2F;06&#x2F;16&#x2F;nlp&#x2F;Language-Model-and-Word-Embedding&#x2F;index.html">
<meta property="og:site_name" content="Melbourne Australia">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;LM-01.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;tensorflow&#x2F;tf-google-9.1.2_1-equation.svg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;tensorflow&#x2F;tf-google-9.1.2_3-ppl.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;tensorflow&#x2F;tf-google-9.1.2_2-equation.svg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;word2vec-nnlm.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W2-5_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C5W2-2.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;word2vec-CBOW_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;word2vec-skip-gram.svg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;word2vec-skip.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;fastText-3.webp">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;fastText-4.webp">
<meta property="og:updated_time" content="2019-10-20T04:30:35.069Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;nlp&#x2F;LM-01.jpg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/reading">Read</a>
        
          <a class="main-nav-link" href="/leetcode">LC</a>
        
          <a class="main-nav-link" href="/ai1">AI</a>
        
          <a class="main-nav-link" href="/tensorflow">Keras</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-nlp/Language-Model-and-Word-Embedding" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Language Model and Perplexity
      <small class=article-detail-date-index>&nbsp; 2019-06-16</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/16/nlp/Language-Model-and-Word-Embedding/" class="article-date">
  <time datetime="2019-06-16T03:00:21.000Z" itemprop="datePublished">2019-06-16</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/nlp/">nlp</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/16/nlp/Language-Model-and-Word-Embedding/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <img src="/images/nlp/LM-01.jpg" width="550" alt="2001 NNLM, @Yoshua bengio" />
<a id="more"></a>
<p>计算机很多事情比人类做得好，那么机器是否能懂 Natural Language?</p>
<p>一些 NLP 技术的应用:</p>
<blockquote>
<ul>
<li>简单的任务：拼写检查，关键词检索，同义词检索等</li>
<li>复杂的任务：信息提取、情感分析、文本分类等</li>
<li>更复杂任务：机器翻译、人机对话、QA系统</li>
</ul>
</blockquote>
<p>Natural Language 逐渐演变成一种 <strong>上下文信息表达</strong> 和 <strong>传递</strong> 的方式。</p>
<p>让计算机处理自然语言，一个基本的问题就是为 自然语言 这种 上下文相关的特性 建立数学模型。</p>
<h2 id="1-language-model"><a class="markdownIt-Anchor" href="#1-language-model"></a> 1. Language Model</h2>
<blockquote>
<ol>
<li>美联储主席昨天告诉媒体 7000 亿美金的救助资金将借给上百家银行、汽车公司。</li>
<li>美联储主席昨天 7000 亿美金的救助资金告诉媒体将借给上百家银行、汽车公司。</li>
<li>美联储主席昨天 告媒诉体 70 亿00美金的救助资金上百家银行将借给、汽车公司。</li>
</ol>
<p>上世纪70年代科学家们试图用规则文法判断句子是否合理。贾里尼克用统计模型解决方法更有效。</p>
</blockquote>
<p>如果 S 表示一连串特定顺序排列的词 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">w\_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.95444em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span></span></span></span>， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi mathvariant="normal">_</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">w\_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.95444em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span></span></span></span>，…， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi mathvariant="normal">_</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">w\_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.74056em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">n</span></span></span></span> ，换句话说，S 表示的是一个有意义的句子。机器对语言的识别从某种角度来说，就是想知道 S 在文本中出现的可能性，也就是数学上所说的 S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是 P(S) 可展开为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mn>2</mn><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mn>3</mn><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mi>w</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo stretchy="false">)</mo><mo>…</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>n</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mi>w</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo>…</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(S) = P(w\_1)P(w\_2|w\_1)P(w\_3| w\_1 w\_2)…P(w\_n|w\_1 w\_2…w\_{n-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">3</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">n</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span></span></span></p>
<h3 id="11-markov-assumption"><a class="markdownIt-Anchor" href="#11-markov-assumption"></a> 1.1 Markov assumption</h3>
<p>假定文本中的每个词 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">w\_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span></span></span></span> 和 前面N-1个词有关，而和更前面的词无关，这样当前词 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">w\_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span></span></span></span> 的概率值取决于前面 N-1个词 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mi>N</mi><mo>+</mo><mn>1</mn></mrow><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mi>N</mi><mo>+</mo><mn>2</mn></mrow><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w\_{i-N+1}, w\_{i-N+2}, ..., w\_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mi>N</mi><mo>+</mo><mn>1</mn></mrow><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mi>N</mi><mo>+</mo><mn>2</mn></mrow><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w\_{i}|w\_{1}, w\_{2}, ..., w\_{i-1}) = P(w\_i | w\_{i-N+1}, w\_{i-N+2}, ..., w\_{i-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord">2</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span></span></span></p>
<blockquote>
<p>N元模型， N=2 时，为二元模型。 在实际中应用最多的是 N=3 的三元模型.</p>
</blockquote>
<h3 id="12-n-gram-n2"><a class="markdownIt-Anchor" href="#12-n-gram-n2"></a> 1.2 n-gram, n=2</h3>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mn>2</mn><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mn>3</mn><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo stretchy="false">)</mo><mo>…</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo><mo>…</mo></mrow><annotation encoding="application/x-tex">P(S) = P(w\_1)P(w\_2|w\_1)P(w\_3|w\_2)…P(w\_i|w\_{i-1})…
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">3</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span></span></span></span></span></p>
<p>接下来如何估计 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P (w\_i|w\_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span></span>。只要机器数一数这对词 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mrow><mo>−</mo><mn>1</mn></mrow><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(w\_i{-1}, w\_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mord"><span class="mord">−</span><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span> 在统计的文本中出现了多少次，以及 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">w\_{i-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span></span></span> 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(w\_i|w\_{i-1}) = \frac {P(w\_{i-1}, w\_i)} {P(w\_{i-1})}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.4459999999999997em;vertical-align:-0.996em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.6999999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.996em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<h2 id="2-perplexity-ppl"><a class="markdownIt-Anchor" href="#2-perplexity-ppl"></a> 2. Perplexity, PPL</h2>
<p>语言模型效果的常用指标 perplexity， 在测试集上 perplexity 越低，说明建模效果越好.</p>
<p>计算perplexity的公式如下：</p>
<img src="/images/tensorflow/tf-google-9.1.2_1-equation.svg" width="600" />
<p><strong>perplexity</strong> 刻画的是语言模型预测一个语言样本的能力. 比如已经知道 (w1,w2,w3,…,wm) 这句话会出现在语料库之中，那么通过语言模型计算得到的这句话的概率越高，说明语言模型对这个语料库拟合得越好。</p>
<p><strong>perplexity</strong> 实际是计算每一个单词得到的概率倒数的几何平均，因此 perplexity 可以理解为平均分支系数（average branching factor），即模型预测下一个词时的平均可选择数量。</p>
<blockquote>
<p>例如，考虑一个由0~9这10个数字随机组成的长度为m的序列，由于这10个数字出现的概率是随机的，所以每个数字出现的概率是 。因此，在任意时刻，模型都有10个等概率的候选答案可以选择，于是perplexity就是10（有10个合理的答案）。</p>
<p>perplexity的计算过程如下：</p>
<img src="/images/tensorflow/tf-google-9.1.2_3-ppl.jpg" width="800" />
</blockquote>
<p>在语言模型的训练中，通常采用 perplexity 的对数表达形式：</p>
<img src="/images/tensorflow/tf-google-9.1.2_2-equation.svg" width="600" />
<blockquote>
<p>相比较乘积求平方根的方式，加法的形式可加速计算，同时避免概率乘积数值过小而导致浮点数向下溢出的问题.</p>
<p>在数学上，log perplexity 可以看作真实分布与预测分布之间的交叉熵 Cross Entropy, 交叉熵描述了两个概率分布之间的一种距离. log perplexity 和 Cross Entropy 是等价的</p>
</blockquote>
<p>在神经网络模型中，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>w</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w\_i | w\_{1}, , ..., w\_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord">1</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span></span></span></span> 分布通常是由一个 softmax层 产生的，TensorFlow 中提供了两个方便计算交叉熵的函数，可以将 logits 结果直接放入输入，来帮助计算 softmax 然后再进行计算 Cross Entropy.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y)</span><br><span class="line">cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = y)</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://www.zhihu.com/people/xi-xiang-yu-20/posts" target="_blank" rel="noopener">知乎_习翔宇</a></li>
</ul>
<h2 id="3-nnlm"><a class="markdownIt-Anchor" href="#3-nnlm"></a> 3. NNLM</h2>
<p>NNLM,直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程.</p>
<p>既然离散的表示有辣么多缺点，于是有小伙伴就尝试着用模型最优化的过程去转换词向量了.</p>
<img src="/images/nlp/word2vec-nnlm.png" width="600" />
<p>计算复杂度： (<span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \* at position 3: N \̲*̲ D + N \* D \* …'>N \* D + N \* D \* H + H \* V</span>) 相当之高, 于是有了 CBOW 和 Skip-Gram .</p>
<blockquote>
<p>NN 训练 语言模型， 会顺带产生一个 Word Embedding 矩阵.</p>
<p>词嵌矩阵 * 单词的独热编码 = 单词的词嵌</p>
<p>(300, 10000) * (10000, 1) = (300, 1)</p>
</blockquote>
<p>可以通过训练神经网络的方式构建词嵌表 <code>E</code> .</p>
<p>下图展示了预测单词的方法，即给出缺少一个单词的句子：</p>
<p>“<strong>I want a glass of orange ___</strong>”</p>
<blockquote>
<p>计算方法是将已知单词的特征向量都作为输入数据送到神经网络中去，然后经过一系列计算到达 Softmax分类层，在该例中输出节点数为 10000个。经过计算 juice 概率最高，所以预测为</p>
<p>“I want a glass of orange <code>juice</code>”</p>
</blockquote>
<img src="/images/deeplearning/C5W2-5_1.png" width="750" />
<p>在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span></span></span></p>
<blockquote>
<p>假设要预测的单词为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>，词嵌表仍然为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span></span></span>，需要注意的是训练词嵌表和预测 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span> 是两个不同的任务。</p>
<p>如果任务是预测 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>，最佳方案是使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span> 前面 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span> 个单词构建语境。</p>
<p>如果任务是训练 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span></span></span>，除了使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span> 前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 Skip Gram 算法），这些方法都能提供很好的结果。</p>
</blockquote>
<h3 id="31-word-representation"><a class="markdownIt-Anchor" href="#31-word-representation"></a> 3.1 Word Representation</h3>
<p>单词与单词之间是有很多共性的，或在某一特性上相近，比如“苹果”和“橙子”都是水果；或者在某一特性上相反，比如“父亲”在性别上是男性，“母亲”在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率，这里用一个简化的表格说明:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Man (5391)</th>
<th style="text-align:center">Woman (9853)</th>
<th style="text-align:center">Apple (456)</th>
<th style="text-align:center">Orange (6257)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">性别</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">年龄</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">-0.01</td>
</tr>
<tr>
<td style="text-align:center">食物</td>
<td style="text-align:center">0.04</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.95</td>
</tr>
<tr>
<td style="text-align:center">颜色</td>
<td style="text-align:center">0.03</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.70</td>
</tr>
</tbody>
</table>
<p>在表格中可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。括号里的数字代表这个单词在独热编码中的位置，可以用这个数字代表这个单词比如 Man = ，Man 的特性用 ，也就是那一纵列。</p>
<p>在实际的应用中，特性的数量远不止 4 种，可能有几百种，甚至更多。对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。</p>
<blockquote>
<p>压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个就是 “Word Embeddings” 的概念.</p>
</blockquote>
<img src="/images/deeplearning/C5W2-2.png" width="500" />
<blockquote>
<p>上图通过聚类将词性相类似的单词在二维空间聚为一类.</p>
</blockquote>
<h3 id="32-word-embeddings"><a class="markdownIt-Anchor" href="#32-word-embeddings"></a> 3.2 Word Embeddings</h3>
<p>先下一个非正规定义 “词嵌 - 描述了词性特征的总量，也是在高维词性空间中嵌入的位置，拥有越多共性的词，词嵌离得越近，反之则越远”。值得注意的是，表达这个“位置”，需要使用所有设定的词性特征，假如有 300 个特征（性别，颜色，…），那么词嵌的空间维度就是 300.</p>
<h3 id="33-使用词嵌三步"><a class="markdownIt-Anchor" href="#33-使用词嵌三步"></a> 3.3 使用词嵌三步</h3>
<ol>
<li>获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库</li>
<li>应用词嵌：将获得的词嵌应用在我们的训练任务中</li>
<li>可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）</li>
</ol>
<p><strong>词嵌实用场景:</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th style="text-align:center">sencentce</th>
<th style="text-align:center">replace word</th>
<th style="text-align:center">target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">Sally Johnson is an <code>orange</code> farmer.</td>
<td style="text-align:center">orange</td>
<td style="text-align:center">Sally Johnson</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">Robert Lin is an <code>apple</code> farmer.</td>
<td style="text-align:center">apple</td>
<td style="text-align:center">Robert Lin</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">Robert Lin is a <code>durian cultivator</code>.</td>
<td style="text-align:center">durian cultivator</td>
<td style="text-align:center">Robert Lin</td>
</tr>
</tbody>
</table>
<blockquote>
<p>我们继续替换，我们将 apple farmer 替换成不太常见的 durian cultivator (榴莲繁殖员)。此时词嵌入中可能并没有 durian 这个词，cultivator 也是不常用的词汇。这个时候怎么办呢？我们可以用到迁移学习。</p>
</blockquote>
<p><strong>词嵌入迁移学习步骤如下：</strong></p>
<blockquote>
<ol>
<li>学习含有大量文本语料库的词嵌入 (一般含有 10亿 到 1000亿 单词)，或者下载预训练好的词嵌入</li>
<li>将学到的词嵌入迁移到相对较小规模的训练集 (例如 10万 词汇).</li>
<li>(可选) 这一步骤就是对新的数据进行 fine-tune。</li>
</ol>
</blockquote>
<h2 id="4-word2vec"><a class="markdownIt-Anchor" href="#4-word2vec"></a> 4. word2vec</h2>
<p>word2vec 并不是一个模型， 而是一个 2013年 google 发表的工具. 该工具包含2个模型： Skip-Gram 和 CBOW. 及两种高效训练方法： negative sampling 和 hierarchicam softmax.</p>
<blockquote>
<ol>
<li>CBOW  Continous Bag of Words Model</li>
<li>Skip-Gram Model</li>
</ol>
<p>词向量（词的特征向量）既能够降低维度，又能够capture到当前词在本句子中上下文的信息.</p>
</blockquote>
<p><a href="https://blog.csdn.net/u012052268/article/details/77170517/#63%E4%B8%AA%E4%BA%BA%E5%AF%B9word-embedding%E7%9A%84%E7%90%86%E8%A7%A3" target="_blank" rel="noopener">Word2Vec</a><br />
<a href="https://blog.csdn.net/sinat_33761963/article/details/54631367" target="_blank" rel="noopener">Word2Vec词嵌入矩阵</a></p>
<h3 id="41-cbow"><a class="markdownIt-Anchor" href="#41-cbow"></a> 4.1 CBOW</h3>
<img src="/images/nlp/word2vec-CBOW_1.png" width="600" />
<blockquote>
<p>纠错 : 上图”目标函数“的第一个公式，应该是 连乘 公式，不是 连加 运算。</p>
<p>理解 : 背景词向量与 中心词向量 内积 等部分，你可考虑 softmax <span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \* at position 3: w \̲*̲ x+b'>w \* x+b</span> 中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 的关系来理解.</p>
</blockquote>
<h3 id="42-skip-gram"><a class="markdownIt-Anchor" href="#42-skip-gram"></a> 4.2 Skip-Gram</h3>
<p>跳字模型假设基于某个词来生成它在文本序列周围的词。举个例子，假设文本序列是“the”“man”“loves”“his”“son”。以“loves”作为中心词，设背景窗口大小为2。如图10.1所示，跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词“the”“man”“his”“son”的条件概率，即</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>the</mtext><mo separator="true">,</mo><mtext>man</mtext><mo separator="true">,</mo><mtext>his</mtext><mo separator="true">,</mo><mtext>son</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(\textrm{the},\textrm{man},\textrm{his},\textrm{son}\mid\textrm{loves}).
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">the</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord textrm">man</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord textrm">his</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord textrm">son</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
<p>假设给定中心词的情况下，背景词的生成是相互独立的，那么上式可以改写成</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>the</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>man</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>his</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>son</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(\textrm{the}\mid\textrm{loves})\cdot P(\textrm{man}\mid\textrm{loves})\cdot P(\textrm{his}\mid\textrm{loves})\cdot P(\textrm{son}\mid\textrm{loves}).
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">the</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">man</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">his</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">son</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
<img src="/images/nlp/word2vec-skip-gram.svg" width="300" />
<p><strong>训练 Skip-Gram</strong></p>
<p>跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msup><mo>∑</mo><mi mathvariant="normal">_</mi><mrow><mo>−</mo><mi>m</mi><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>m</mi><mo separator="true">,</mo><mtext> </mtext><mi>j</mi><mi mathvariant="normal">≠</mi><mn>0</mn></mrow><mtext>log</mtext><mtext> </mtext><mi>P</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo>∣</mo><msup><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex"> - \sum\_{t=1}^{T} \sum\_{-m \leq j \leq m,\ j \neq 0} \text{log}\, P(w^{(t+j)} \mid w^{(t)}).
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.6000100000000002em;vertical-align:-0.55001em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord">−</span><span class="mord mathdefault">m</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace"> </span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span><span class="mord text"><span class="mord">log</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
<p>如果使用随机梯度下降，那么在每一次迭代里我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，首先看到</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>o</mi><mo>∣</mo><mi>w</mi><mi mathvariant="normal">_</mi><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">u</mi><mi mathvariant="normal">_</mi><msup><mi>o</mi><mi mathvariant="normal">⊤</mi></msup><mi mathvariant="bold-italic">v</mi><mi mathvariant="normal">_</mi><mi>c</mi><mo>−</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo>∑</mo><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>∈</mo><mi mathvariant="script">V</mi></mrow><mtext>exp</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">u</mi><mi mathvariant="normal">_</mi><msup><mi>i</mi><mi mathvariant="normal">⊤</mi></msup><mi mathvariant="bold-italic">v</mi><mi mathvariant="normal">_</mi><mi>c</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log P(w\_o \mid w\_c) =
\boldsymbol{u}\_o^\top \boldsymbol{v}\_c - \log\left(\sum\_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}\_i^\top \boldsymbol{v}\_c)\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2091079999999998em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">u</span></span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">c</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.08222em;">V</span></span></span><span class="mord text"><span class="mord">exp</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">u</span></span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">c</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span></span></span></p>
<img src="/images/nlp/word2vec-skip.png" width="700" />
<p>它的计算需要词典中所有词以 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi mathvariant="normal">_</mi><mi>c</mi></mrow><annotation encoding="application/x-tex">w\_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.74056em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">c</span></span></span></span> 为中心词的条件概率。有关其他词向量的梯度同理可得。</p>
<p>训练结束后，对于词典中的任一索引为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span> 的词，我们均得到该词作为中心词和背景词的两组词向量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi><mi mathvariant="normal">_</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">v\_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>u</mi><mi mathvariant="normal">_</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">u\_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695199999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault">u</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span></span></span></span> 。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。</p>
<blockquote>
<p>两个向量越相似，他们的点乘也就越大.</p>
</blockquote>
<p><strong>小结：</strong></p>
<ol>
<li>最大似然估计 MLE</li>
<li>最小化损失函数（与第一步等价），损失函数对数联合概率的相反数</li>
<li>描述概率函数，该函数的自变量是词向量（u和v），词向量也是模型参数</li>
<li>对第二步中每一项求梯度。有了梯度就可以优化第二步中的损失函数，从而迭代学习到模型参数，也就是词向量。</li>
</ol>
<h3 id="43-高效近似训练"><a class="markdownIt-Anchor" href="#43-高效近似训练"></a> 4.3 高效近似训练</h3>
<ul>
<li>hierarchicam softmax</li>
<li>negative sampling</li>
</ul>
<h2 id="5-fasttext"><a class="markdownIt-Anchor" href="#5-fasttext"></a> 5. fastText</h2>
<p>FastText是一个快速文本分类算法，在使用标准多核CPU的情况下，在10分钟内可以对超过10亿个单词进行训练。 不需要使用预先训练好的词向量，因为FastText会自己训练词向量。</p>
<p>文本分类：</p>
<img src="/images/nlp/fastText-3.webp" width="500" />
<p>情感分类:</p>
<img src="/images/nlp/fastText-4.webp" width="500" />
<p>fastText 能够做到效果好，速度快，主要依靠两个秘密武器：</p>
<blockquote>
<ol>
<li>利用了 词内的n-gram信息 (subword n-gram information)</li>
<li>用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick.</li>
</ol>
</blockquote>
<p><strong>fastText 和 word2vec 的区别:</strong></p>
<p><strong>两者表面的不同：</strong></p>
<blockquote>
<p><strong>模型的输出层：</strong></p>
<p>word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用；</p>
<p><strong>模型的输入层：</strong></p>
<p>word2vec的输出层，是 context window 内的term；而fasttext对应的整个sentence的内容，包括term，也包括 n-gram的内容；</p>
</blockquote>
<p><strong>两者本质的不同，体现在 h-softmax 的使用：</strong></p>
<blockquote>
<p>Wordvec的目的是得到词向量，该词向量最终是在输入层得到，输出层对应的 h-softmax也会生成一系列的向量，但最终都被抛弃，不会使用。</p>
<p>fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）</p>
</blockquote>
<h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2>
<ul>
<li>《数学之美》 读书笔记</li>
<li><a href="https://whiskytina.github.io/word2vec.html" target="_blank" rel="noopener">word2vec前世今生</a></li>
<li><a href="https://whiskytina.github.io/14947653164873.html" target="_blank" rel="noopener">CS224N NLP with Deep Learning: Lecture 1 课程笔记</a></li>
<li><a href="https://blog.csdn.net/m0_37324740/article/details/79411651" target="_blank" rel="noopener">good, sklearn 中 CountVectorizer、TfidfTransformer 和 TfidfVectorizer</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_weibo_icon.png);background-size: contain;display: inline-block; width:50px; height:50px" href="https://service.weibo.com/share/share.php?url" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io" target="_blank" rel="noopener"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-language-model"><span class="toc-text"> 1. Language Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-markov-assumption"><span class="toc-text"> 1.1 Markov assumption</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-n-gram-n2"><span class="toc-text"> 1.2 n-gram, n=2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-perplexity-ppl"><span class="toc-text"> 2. Perplexity, PPL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-nnlm"><span class="toc-text"> 3. NNLM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-word-representation"><span class="toc-text"> 3.1 Word Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-word-embeddings"><span class="toc-text"> 3.2 Word Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-使用词嵌三步"><span class="toc-text"> 3.3 使用词嵌三步</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-word2vec"><span class="toc-text"> 4. word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-cbow"><span class="toc-text"> 4.1 CBOW</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-skip-gram"><span class="toc-text"> 4.2 Skip-Gram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-高效近似训练"><span class="toc-text"> 4.3 高效近似训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-fasttext"><span class="toc-text"> 5. fastText</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-text"> Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/nlp/">nlp</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/PPL/" rel="tag">PPL</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/06/17/deeplearning/Seq2Seq-Attention/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Seq2Seq and Attention
        
      </div>
    </a>
  
  
    <a href="/2019/06/14/deeplearning/RNN-LSTM-GRU/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Recurrent Neural Networks&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2019/06/16/nlp/Language-Model-and-Word-Embedding/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
