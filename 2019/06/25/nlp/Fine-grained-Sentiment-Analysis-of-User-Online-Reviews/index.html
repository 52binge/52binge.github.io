<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Fine-grained Sentiment Analysis of User Online Reviews - Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="chatbot">
<meta property="og:type" content="article">
<meta property="og:title" content="Fine-grained Sentiment Analysis of User Online Reviews">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;2019&#x2F;06&#x2F;25&#x2F;nlp&#x2F;Fine-grained-Sentiment-Analysis-of-User-Online-Reviews&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-11.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-14.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-13.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-21.webp">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-22.webp">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-16-1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-17-1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-23.webp">
<meta property="og:updated_time" content="2019-10-20T04:30:35.067Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;AI-Challenger-11.png">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<!-- jiangting add start... @2020.08.30 -->
<!-- <div id="menu" class="duration-main" style="background-color: #e7e7e7"> -->
<!--   <p class="links-p" href="/">Blair</p> -->
<!--   <address class="icons"> -->
<!--     <a href="https://github.com/blair101" class="icon-font icon linkedin" target="_blank"></a> -->
<!--     <a href="https://cn.linkedin.com/pub/tianyu-dai/a8/818/44a" class="icon-font icon linkedin" target="_blank"></a> -->
<!--   </address> -->
<!--   <div class="hr1"></div> -->
<!--   <nav> -->
<!--     <div> -->
<!--     <a class="home" href="/">Home</a></div> -->
<!--     <p class="links-p" href="/">Home</p> -->
<!--     <nav class="tag-ath"> -->
<!--       <a class="proj" href="/categories">Category</a> -->
<!--       <a class="authors" href="/about">About</a> -->
<!--     </nav> -->
<!--   </nav> -->
<!--   <div class="hr2"></div> -->
<!--     <p class="links-p">Links</p> -->
<!--       <address class="links"> -->
<!--       <a class="proj" href="/article/Create-MyWorld">Projects</a> -->
<!--       <a class="friend">Friends</a></address><div class="hr3"> -->
<!--   </div> -->
<!--   <p class="end"></p> -->
<!--   <div id="menu-links" class="duration-main" style="top: -400px; background-color: #f5f5f5"> -->
<!--     <address> -->
<!--       <li><a target="_blank" href="http://lm7.xxxxxxxx.jp">Lm7</a></li> -->
<!--       <li><a target="_blank" href="http://www.pixiv.net/member.php?id=4933015">Domik</a></li> -->
<!--       <li><a target="_blank" href="http://hana-ui.moe">hana-ui</a></li> -->
<!--       <li><a target="_blank" href="http://fil.dtysky.moe">F-I-L</a></li> -->
<!--       <li><a target="_blank" href="http://paradise.dtysky.moe">Paradise</a></li> -->
<!--       <li><a target="_blank" href="http://moe-notes.dtysky.moe">MoeNotes</a></li> -->
<!--       <li><a target="_blank" href="http://kanata.dtysky.moe">Kanata</a></li> -->
<!--       <li><a target="_blank" href="http://blog.nekohand.moe">Nekohand</a></li> -->
<!--       <li><a target="_blank" href="http://www.jerryfu.net">JerryFu</a></li> -->
<!--       <li><a target="_blank" href="http://kawabangga.com">南史</a></li> -->
<!--       <li><a>Hide Links</a></li> -->
<!--       <li><a></a></li> -->
<!--     </address> -->
<!--   </div> -->
<!-- </div> -->
<!-- jiangting add end !  @2020.08.30 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/ai">AI</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Fine-grained Sentiment Analysis of User Online Reviews
      <small class=article-detail-date-index>&nbsp; 2019-06-25</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/25/nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews/" class="article-date">
  <time datetime="2019-06-25T02:16:21.000Z" itemprop="datePublished">2019-06-25</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/chatbot/">chatbot</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/25/nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/deeplearning/AI-Challenger-11.png" width="550" alt="AI-Challenger"/></p>
<a id="more"></a>
<p><a href="https://challenger.ai/" target="_blank" rel="noopener">Challenger.AI</a></p>
<p>Online reviews have become the critical factor to make consumption decision in recent years. They not only have a profound impact on the incisive understanding of shops, users, and the implied sentiment, but also have been widely used in Internet and e-commerce industry, such as personalized recommendation, intelligent search, product feedback, and business security. In this challenge, we provide a dataset of user reviews for fine-grained sentiment analysis from the catering industry, containning 335K public user reviews from Dianping.com. The dataset builds a two-layer labeling system according to the granularity, which contains 6 categories and 20 fine-grained elements.</p>
<blockquote>
<p>Training set: 105K</p>
<p>Verification set: 15K</p>
<p>Test set A: 15K</p>
<p>Test set B: 200K</p>
</blockquote>
<p>There are four sentimental types for every fine-grained element: Positive, Neutral, Negative and Not mentioned, which are labelled as 1, 0, -1 and-2. The meaning of these four labels are listed below.</p>
<p><img src="/images/deeplearning/AI-Challenger-14.png" width="650" alt=""/></p>
<p>An example of one labelled review:</p>
<blockquote>
<p>“味道不错的面馆，性价比也相当之高，分量很足～女生吃小份，胃口小的，可能吃不完呢。环境在面馆来说算是好的，至少看上去堂子很亮，也比较干净，一般苍蝇馆子还是比不上这个卫生状况的。中午饭点的时候，人很多，人行道上也是要坐满的，隔壁的冒菜馆子，据说是一家，有时候也会开放出来坐吃面的人。“</p>
</blockquote>
<p><img src="/images/deeplearning/AI-Challenger-13.png" width="650" alt=""/></p>
<hr>
<p>「AI Challenger」是面向全球人工智能人才的开源数据集和编程竞赛平台。AI Challenger 2018 由创新工场、搜狗、美团点评、美图公司联合主办。有上万支团队参赛， 覆盖 81 个国家、1100 所高校、990 家公司。</p>
<!--<img src="/images/deeplearning/AI-Challenger-21.webp" width="850" alt=""/>
-->
<h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><ul>
<li><strong>问题建模</strong></li>
<li><strong>模型基本架构</strong></li>
<li><strong>数据处理</strong><br>。。。</li>
</ul>
<p><img src="/images/deeplearning/AI-Challenger-22.webp" width="890" alt=""/></p>
<p>有20个粒度的评价指标，每个粒度又有4种情感状态，从官方baseline来看，分别训练了20个（4标签）分类器。</p>
<blockquote>
<p>FastText（0.573）、Attention-RNN (0.637)、Attention-RCNN (0.669)、ELMO-like（0.68830）</p>
</blockquote>
<h2 id="1-FastText-baseline"><a href="#1-FastText-baseline" class="headerlink" title="1. FastText baseline"></a>1. FastText baseline</h2><blockquote>
<p>skift：scikit-learn wrappers for Python fastText.</p>
</blockquote>
<p>什么是 skift?</p>
<blockquote>
<p>skift 包括几个 scikit-learn兼容包裝器, 封裝了fasttext模型，fasttext原理类似word2vec，主要用于文本快速分类。其优势在于分类速度快，使用n-gram 特别容易获得文本句子局部信息、构造新詞。</p>
</blockquote>
<p>fasttext 缺点是随着语料的增长，內存需求也会增长。那么如果解決內存问题呢？</p>
<blockquote>
<ol>
<li>过滤掉次数出现少的词；</li>
<li>采用word粒度，而非char粒度</li>
</ol>
</blockquote>
<p>例如句子: </p>
<blockquote>
<p>我喜歡去中國， 如果採用char粒度，則使用2-gram的話，產生的特徵爲:</p>
<p>我喜 喜歡 歡中 中國</p>
</blockquote>
<p>如果採用word粒度的話，產生的特徵爲</p>
<blockquote>
<p>我喜歡 喜歡去 去中國</p>
</blockquote>
<ul>
<li><a href="https://www.twblogs.net/a/5c1215debd9eee5e40bb42fd" target="_blank" rel="noopener">利用skift實現fasttext模型</a></li>
<li><a href="https://blog.csdn.net/supinyu/article/details/81136590" target="_blank" rel="noopener">FastText文本分类以及生成词向量</a></li>
<li><a href="https://blog.csdn.net/sinat_26917383/article/details/83041424#2_fasttext_21" target="_blank" rel="noopener">极简使用︱Gensim-FastText 词向量训练以及OOV</a></li>
<li><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">fastText原理和文本分类实战，看这一篇就够了</a></li>
</ul>
<blockquote>
<p>默认配置参数训练 fasttext 多模型，f1均值 约为 0.5513 的 fasttext 多分类模型(20个）</p>
</blockquote>
<h3 id="1-1-数据情况记录"><a href="#1-1-数据情况记录" class="headerlink" title="1.1 数据情况记录"></a>1.1 数据情况记录</h3><p>fastText 时候， 词的粒度</p>
<ul>
<li>jieba 分词</li>
<li>建立词典时，过滤掉出现次数小于 2~5 的词</li>
<li>训练集、验证集 以及 测试集A组成的语料，词典大小为 66347</li>
<li>预测和训练时，词典没有出现的词 用 <code>&lt;UNK&gt;</code> 代替</li>
</ul>
<h3 id="1-2-main-code"><a href="#1-2-main-code" class="headerlink" title="1.2 main code"></a>1.2 main code</h3><p>利用训练集，来训练 <strong>20 个</strong> 4分类 分类器, 训练 15 分钟</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">sk_clf = FirstColFtClassifier(lr=learning_rate, epoch=epoch,</span><br><span class="line">                              wordNgrams=word_ngrams,</span><br><span class="line">                              minCount=min_count, verbose=<span class="number">2</span>)</span><br><span class="line">sk_clf.fit(train_data_format, train_label)</span><br></pre></td></tr></table></figure>
<!--
验证集，计算 macro F1


    for column in columns[2:]:
        true_label = np.asarray(validate_data_df[column])
        classifier = classifier_dict[column]
        pred_label = classifier.predict(validata_data_format).astype(int)
        f1_score = get_f1_score(true_label, pred_label)
        f1_score_dict[column] = f1_score

    f1_score = np.mean(list(f1_score_dict.values()))
-->
<blockquote>
<p>min_count设置为2貌似也有一些负向影响， word_ngrams 2， epoch 10 .</p>
</blockquote>
<h3 id="1-3-baseline-效果"><a href="#1-3-baseline-效果" class="headerlink" title="1.3 baseline 效果"></a>1.3 baseline 效果</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">service_wait_time:0.5247890022873511</span><br><span class="line">service_waiters_attitude:0.6781093513108542</span><br><span class="line">service_parking_convenience:0.5828932335474249</span><br><span class="line">service_serving_speed:0.6146828053320519</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">f1_score: 0.5513</span><br></pre></td></tr></table></figure>
<p>调参：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">python main_train.py -mn fasttext_model_wn2.pkl -wn <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>约跑15分钟左右，存储的模型大约在17G，验证集 macro F1值结果如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">service_wait_time:0.5247890022873511</span><br><span class="line">service_waiters_attitude:0.6881098513108542</span><br><span class="line">service_parking_convenience:0.5828935095474249</span><br><span class="line">service_serving_speed:0.6168828054420539</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">f1_score: 0.5783</span><br></pre></td></tr></table></figure>
<p>这个结果看起来还不错，我们可以基于这个fasttext多分类模型进行测试集的预测：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">python main_predict.py -mn fasttext_wn2_model.pkl</span><br></pre></td></tr></table></figure>
<p>优化方法： 去停用词和去一些标点符号，调参，learning_rate的影响是比较直接的，min_count</p>
<h3 id="1-4-fastText-速度快"><a href="#1-4-fastText-速度快" class="headerlink" title="1.4 fastText 速度快"></a>1.4 fastText 速度快</h3><p>能够做到效果好，速度快，主要依靠两个秘密武器：</p>
<blockquote>
<ol>
<li>利用了 词内的n-gram信息 (subword n-gram information)</li>
<li>用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick.</li>
</ol>
</blockquote>
<h2 id="2-Attention-RNN、RCNN"><a href="#2-Attention-RNN、RCNN" class="headerlink" title="2. Attention RNN、RCNN"></a>2. Attention RNN、RCNN</h2><h3 id="2-1-预处理-data"><a href="#2-1-预处理-data" class="headerlink" title="2.1 预处理 data"></a>2.1 预处理 data</h3><p>粗暴使用char模型，用到的停用词也不多。</p>
<ul>
<li>trainsets lines：  501132， 合法例子 ： 105000</li>
<li>validationset lines：  70935, 合法例子 ： 15000</li>
<li>testsets lines：  72028， 合法例子 ： 15000</li>
</ul>
<p>数据预处理，生成 train_char.csv、test_char.csv、test_char.csv 三个文件:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-rw-r--r--  1 blair 10:36 test_char.csv</span><br><span class="line">-rw-r--r--  1 blair 10:09 train_char.csv</span><br><span class="line">-rw-r--r--  1 blair 10:32 validation_char.csv</span><br></pre></td></tr></table></figure>
<p><strong>word2vec：</strong> 维度 100， 窗口 10， 过滤掉次数小于 1~2 的字</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3.1M chars.vector</span><br></pre></td></tr></table></figure>
<p>过滤掉低频词之后：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">word2vec/chars.vector 为 7983 * 100</span><br></pre></td></tr></table></figure>
<h3 id="2-2-Attention-RCNN"><a href="#2-2-Attention-RCNN" class="headerlink" title="2.2 Attention RCNN"></a>2.2 Attention RCNN</h3><blockquote>
<p>Attention-RNN (0.637)、Attention-RCNN (0.669)</p>
</blockquote>
<ul>
<li>Attention 参考自 Kaggle 的 <a href="https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043" target="_blank" rel="noopener">Attention Model</a></li>
</ul>
<p>Kaggle 常见文本分类结构: 2层GRU 接Attention层，然后和 avgpool、maxpool concat 接起来.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(self, embeddings_matrix, maxlen, word_index, num_class)</span>:</span></span><br></pre></td></tr></table></figure>
<p>为了之后 summary 看清楚网络结构，所以我们一些参数先写死看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># from JoinAttLayer import Attention</span></span><br><span class="line"></span><br><span class="line">maxlen=<span class="number">1200</span></span><br><span class="line"></span><br><span class="line">inp = Input(shape=(maxlen,)) <span class="comment"># 当输入序列的长度固定时，该值为其长度 1200 （一个文档doc的最大长度）</span></span><br><span class="line"></span><br><span class="line">encode = Bidirectional(CuDNNGRU(<span class="number">128</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line">encode2 = Bidirectional(CuDNNGRU(<span class="number">128</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># attention = Attention(maxlen)</span></span><br><span class="line"></span><br><span class="line">x_4 = Embedding(<span class="number">7555</span>+ <span class="number">1</span>,<span class="comment">#7983+1 # 词汇表大小， 即，最大整数 index + 1, len(word_index) + 1, # input_dim</span></span><br><span class="line">                <span class="number">100</span>, <span class="comment"># output_dim: int &gt;= 0。词向量的维度。</span></span><br><span class="line">                input_length=maxlen, <span class="comment"># maxlen=1200, 一个 doc 最大长度</span></span><br><span class="line">                trainable=<span class="literal">True</span>)(inp)</span><br></pre></td></tr></table></figure>
<p>接下来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_3 = encode(x_4)</span><br><span class="line">x_3 = encode2(x_3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入shape， 形如（samples，steps，features）的3D张量</span></span><br><span class="line"><span class="comment"># 输出shape， 形如(samples, features)的2D张量</span></span><br><span class="line">avg_pool_3 = GlobalAveragePooling1D()(x_3) <span class="comment"># GlobalAveragePooling1D 为时域信号施加全局平均值池化</span></span><br><span class="line">max_pool_3 = GlobalMaxPooling1D()(x_3) <span class="comment"># 对于时间信号的全局最大池化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># attention_3 = attention(x_3)</span></span><br><span class="line"></span><br><span class="line">x = keras.layers.concatenate([avg_pool_3, max_pool_3], name=<span class="string">"fc"</span>)</span><br><span class="line">x = Dense(<span class="number">4</span>, activation=<span class="string">"softmax"</span>)(x)</span><br><span class="line"></span><br><span class="line">adam = keras.optimizers.Adam(lr=<span class="number">0.001</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, epsilon=<span class="number">1e-08</span>, amsgrad=<span class="literal">True</span>)</span><br><span class="line">rmsprop = keras.optimizers.RMSprop(lr=<span class="number">0.001</span>, rho=<span class="number">0.9</span>, epsilon=<span class="number">1e-06</span>)</span><br><span class="line">model = Model(inputs=inp, outputs=x)</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">    optimizer=adam)</span><br><span class="line"></span><br><span class="line"><span class="comment"># categorical_crossentropy 用来做多分类问题</span></span><br><span class="line"><span class="comment"># binary_crossentropy 用来做多标签分类问题</span></span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>RNN：</p>
<p><img src="/images/deeplearning/AI-Challenger-16-1.png" width="900" alt=""/></p>
<p>RCNN：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_4 = Embedding(<span class="number">7555</span>+ <span class="number">1</span>,<span class="comment">#7983+1 # 词汇表大小， 即，最大整数 index + 1</span></span><br><span class="line">                <span class="number">100</span>,</span><br><span class="line">                input_length=maxlen,</span><br><span class="line">                trainable=<span class="literal">True</span>)(inp)</span><br><span class="line"></span><br><span class="line">x_3 = encode(x_4)</span><br><span class="line">x_3 = encode2(x_3)</span><br><span class="line"></span><br><span class="line">x_3 = Conv1D(<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">"valid"</span>, kernel_initializer=<span class="string">"glorot_uniform"</span>)(x_3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入shape， 形如（samples，steps，features）的3D张量</span></span><br><span class="line"><span class="comment"># 输出shape， 形如(samples, features)的2D张量</span></span><br><span class="line">avg_pool_3 = GlobalAveragePooling1D()(x_3) <span class="comment"># GlobalAveragePooling1D 为时域信号施加全局平均值池化</span></span><br><span class="line">max_pool_3 = GlobalMaxPooling1D()(x_3) <span class="comment"># 对于时间信号的全局最大池化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># attention_3 = attention(x_3)</span></span><br><span class="line"></span><br><span class="line">x = keras.layers.concatenate([avg_pool_3, max_pool_3], name=<span class="string">"fc"</span>)</span><br><span class="line">x = Dense(<span class="number">4</span>, activation=<span class="string">"softmax"</span>)(x)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p><img src="/images/deeplearning/AI-Challenger-17-1.png" width="900" alt=""/></p>
<blockquote>
<p>Input 一个网络层次，输入层 在 keras</p>
<p>SpatialDropout1D ，那么常规的 dropout 将无法使激活正则化，且导致有效的学习速率降低。<br>SpatialDropout1D ，在这种情况下，SpatialDropout1D 将有助于提高特征图之间的独立性，应该使用它来代替 Dropout。</p>
<p>CuDNNGRU 是 基于CuDNN的快速GRU实现，只能在GPU上运行，只能使用 tensoflow 为后端<br>CuDNNLSTM 是 基于CuDNN的快速LSTM实现，只能在GPU上运行，只能使用 tensoflow 为后端</p>
<p>attention = Attention(maxlen)</p>
<p>Embedding嵌入层将正整数（下标）转换为具有固定大小的向量，如[<a href="https://blog.csdn.net/liuchonge/article/details/77140719" target="_blank" rel="noopener">4</a>, [20]]-&gt;[[0.25, 0.1], [0.6, -0.2]]</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">keras.layers.embeddings.Embedding(</span><br><span class="line">    input_dim, </span><br><span class="line">    output_dim, </span><br><span class="line">    embeddings_initializer=<span class="string">'uniform'</span>, <span class="comment"># embeddings_regularizer=None, </span></span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,  <span class="comment"># embeddings_constraint=None,              </span></span><br><span class="line">    mask_zero=<span class="literal">False</span>, </span><br><span class="line">    input_length=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Embedding 的一些参数解释：</p>
<blockquote>
<p>Embedding层只能作为模型的第一层</p>
<p>input_dim: int &gt; 0。词汇表大小， 即，最大整数 index + 1。<br>output_dim: int &gt;= 0。词向量的维度。<br>embeddings_initializer: 嵌入矩阵的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers<br>input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。</p>
</blockquote>
<p><a href="/2018/08/21/deeplearning/Convolutional-Neural-Networks-week1/#4-1-运用-Padding-的原因">Convolutional Neural Networks (week1) - CNN , 运用 Padding</a><br><a href="https://blog.csdn.net/wuzqChom/article/details/74785643" target="_blank" rel="noopener">TensorFlow中CNN的两种padding方式“SAME”和“VALID”</a></p>
<blockquote>
<p>word2vec : 7983 100 word2vec/chars.vector 过滤掉低频词</p>
</blockquote>
<p>循环卷积神经网络(RCNN)，并将其应用于文本分类的任务。首先，我们应用一个双向的循环结构，与传统的基于窗口的神经网络相比，它可以大大减少噪声，从而最大程度地捕捉上下文信息。此外，<strong>该模型在学习文本表示时可以保留更大范围的词序</strong>。其次，我们使用了一个可以<strong>自动判断哪些特性在文本分类中扮演关键角色的池化层</strong>，以捕获文本中的关键组件。我们的模型结合了RNN的结构和最大池化层，<strong>利用了循环神经模型和卷积神经模型的优点</strong>。此外，我们的模型显示了O(n)的时间复杂度，它与文本长度的长度是线性相关的。</p>
<blockquote>
<ol>
<li>RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。</li>
<li>RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。</li>
<li>CNN 优点： 提取数据中的局部位置的特征，然后再拼接池化层。 CNN可以更好地捕捉文本的语义。是O(n)</li>
<li>CNN 优点： 一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。</li>
</ol>
<p>首先我们来理解下什么是卷积操作？卷积，你可以把它想象成一个应用在矩阵上的滑动窗口函数。</p>
<p>卷积网络也就是对输入样本进行多次卷积操作，提取数据中的局部位置的特征，然后再拼接池化层（图中的Pooling层）做进一步的降维操作</p>
<p>我们可以把CNN类比N-gram模型，N-gram也是基于词窗范围这种局部的方式对文本进行特征提取，与CNN的做法很类似</p>
</blockquote>
<ul>
<li><a href="https://plushunter.github.io/2018/03/08/自然语言处理系列（8）：RCNN/" target="_blank" rel="noopener">自然语言处理系列（8）：RCNN</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/29201491" target="_blank" rel="noopener">Keras之文本分类实现</a></li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.backend.tensorflow_backend <span class="keyword">import</span> set_session</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">set_session(tf.Session(config=config))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.seed = <span class="number">42</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> set_random_seed</span><br><span class="line">set_random_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> text, sequence</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint, Callback</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, recall_score, precision_score</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> classifier_bigru <span class="keyword">import</span> TextClassifier</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models.keyedvectors <span class="keyword">import</span> KeyedVectors</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gc</span><br></pre></td></tr></table></figure>
<h3 id="2-3-loss-function"><a href="#2-3-loss-function" class="headerlink" title="2.3 loss function"></a>2.3 loss function</h3><p><a href="https://blog.csdn.net/qinglv1/article/details/85701106" target="_blank" rel="noopener">多分类和多标签分类</a>, <a href="https://blog.csdn.net/sinat_26917383/article/details/69803018" target="_blank" rel="noopener">gensim训练word2vec及相关函数</a></p>
<blockquote>
<p>多分类：类别数目大于2个，类别之间是互斥的。比如是猫，就不能是狗、猪<br>categorical crossentropy 用来做多分类问题<br>binary crossentropy 用来做多标签分类问题</p>
</blockquote>
<p><a href="https://www.zhihu.com/question/36307214" target="_blank" rel="noopener">sigmoid,softmax,binary/categorical crossentropy的联系？</a></p>
<blockquote>
<p><strong>Binary cross-entropy</strong> 常用于二分类问题，当然也可以用于多分类问题，通常需要在网络的最后一层添加sigmoid进行配合使用 </p>
<p><strong>Categorical cross-entropy</strong> 适用于多分类问题，并使用softmax作为输出层的激活函数的情况。</p>
</blockquote>
<h3 id="2-4-Early-Stop"><a href="#2-4-Early-Stop" class="headerlink" title="2.4 Early Stop"></a>2.4 Early Stop</h3><p>需要在每个 epoch 结束之后去计算模型的 F1 值，这样可以更好的掌握模型的训练情况。</p>
<blockquote>
<p>Tips 如果我们在训练中设置 metric 的话，得到是每个 batch 的 F1 值, 是不靠谱的.</p>
</blockquote>
<p>类似这样: </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getClassification</span><span class="params">(arr)</span>:</span></span><br><span class="line">    arr = list(arr)</span><br><span class="line">    <span class="keyword">if</span> arr.index(max(arr)) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-2</span></span><br><span class="line">    <span class="keyword">elif</span> arr.index(max(arr)) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">elif</span> arr.index(max(arr)) == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Metrics</span><span class="params">(Callback)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span><span class="params">(self, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        self.val_f1s = []</span><br><span class="line">        self.val_recalls = []</span><br><span class="line">        self.val_precisions = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>early_stop，就是在训练模型的时候，当在验证集上效果不再提升的时候，就提前停止训练，节约时间。</p>
</blockquote>
<!--

### 2.5 Class Weight (类别权重)

一般而言，当数据集样本不均衡的时候，通过设置正负样本权重，可以提高一些效果，但是在这道题目里面，我对4个类别分别设置了class_weight 之后，我发现效果竟然变得更差了。

-->
<h3 id="2-6-Max-Length-padding"><a href="#2-6-Max-Length-padding" class="headerlink" title="2.6 Max Length (padding)"></a>2.6 Max Length (padding)</h3><blockquote>
<p>所有评论平均的长度是 200 左右，max_length 取 2 * 200，效果一直不给力.</p>
<p>将 max_length 改为 1200 ，macro f-score 效果明显提升</p>
<p>Tips： 多分类问题中，那些长度很长的评论可能会有部分属于那些样本数很少的类别，padding过短会导致这些长评论无法被正确划分。</p>
</blockquote>
<h2 id="3-ELMO-Like"><a href="#3-ELMO-Like" class="headerlink" title="3. ELMO-Like"></a>3. ELMO-Like</h2><!--<img src="/images/deeplearning/AI-Challenger-23.webp" width="850" alt=""/>
-->
<p>（腾讯词向量 16G， 800W * 200 = 5W * 200 + 自训词向 5W * 128 ） + BiGRU 中层语义 + BiGRU 高层语义</p>
<blockquote>
<p>10W+ 数据集，词频前5W的词, 每个评论一个 epoch 输入1次， 参数共享</p>
<p>328 + 256 + 256 近1000维度，Batch 128， 512维度的时候，Batch 256 可以放得下.</p>
<p>机器配置： 32G 内存， i9 CPU， 显卡型号 1080， 显存8G</p>
</blockquote>
<p><strong>多任务学习</strong></p>
<blockquote>
<ul>
<li><strong>分别训练20个分类模型的计算复杂度较高</strong></li>
<li><strong>20个分类模型占用存储空间</strong></li>
<li><strong>多任务学习可以通过特征共享降低过拟合风险</strong></li>
</ul>
</blockquote>
<p>epoch</p>
<blockquote>
<p>一次输入一个Batch=128条评论，20个属性都4分类成功， 1 个 epoch， 1200秒=20多分钟</p>
<p>每个评论一个 epoch 输入1次， 参数共享</p>
<p>maxLen 500 左右</p>
</blockquote>
<hr>
<blockquote>
<p>1.5W 跑一次测试集 1~2 分钟.<br>20W, 跑一次测试集 10多分钟 左右</p>
</blockquote>
<h2 id="4-Summary"><a href="#4-Summary" class="headerlink" title="4. Summary"></a>4. Summary</h2><blockquote>
<ol>
<li>RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。</li>
<li>RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。</li>
<li>CNN 优点： 提取数据中的局部位置的特征，然后再拼接池化层。 CNN可以更好地捕捉文本的语义。是O(n)</li>
<li>CNN 优点： 一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。</li>
</ol>
</blockquote>
<hr>
<p>在文本分类任务中，有哪些对性能有重要影响的tricks？</p>
<blockquote>
<ol>
<li>数据预处理时vocab的选取（前N个高频词或者过滤掉出现次数小于3的词等等）</li>
<li>词向量的选择，可以使用预训练好的词向量如谷歌、facebook开源出来的，当训练集比较大的时候也可以进行微调或者随机初始化与训练同时进行。训练集较小时就别微调了</li>
<li>结合要使用的模型，这里可以把数据处理成char、word或者都用等</li>
<li>有时将词性标注信息也加入训练数据会收到比较好的效果</li>
<li>至于PAD的话，取均值或者一个稍微比较大的数（比较大的值，费点空间，谨慎使用）</li>
<li>神经网络结构的话到没有什么要说的，加上dropout和BN可能会更好。模型这块还是要具体问题具体分析吧.</li>
<li>文本领域用过数据增强的方法，就是对文本进行随机的shuffle和drop等操作来增加数据量</li>
</ol>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://blog.csdn.net/liuchonge/article/details/77140719" target="_blank" rel="noopener">深度学习与文本分类总结第一篇—常用模型总结</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25928551" target="_blank" rel="noopener">用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践</a></li>
<li><a href="https://www.zhihu.com/question/59236897" target="_blank" rel="noopener">严重数据倾斜文本分类，比如正反比1:20～100，适合什么model</a></li>
<li><a href="https://github.com/xueyouluo/fsauor2018" target="_blank" rel="noopener">第16名解决方案</a>、 <a href="https://github.com/BigHeartC/Al_challenger_2018_sentiment_analysis" target="_blank" rel="noopener">第17名解决方案</a>、 <a href="https://github.com/brightmart/sentiment_analysis_fine_grain" target="_blank" rel="noopener">基于Bert的尝试</a></li>
</ul>
<!--

- [AI-Challenger Baseline (0.70201) 前篇 总览][niu1]
- [AI-Challenger Baseline (0.70201) 后篇 训练][niu2]
- [2019 11家互联网公司，NLP面经回馈][v1]
- [暑期实习NLP算法岗面经总结][v2]
- [呜呜哈做一个有思想的码农][v3]
- [AI Challenger 2018 进行时][w6]
- [AI Challenger 2018 细粒度用户评论情感分析 fastText Baseline][w7]

-->
<ul>
<li><a href="http://www.52nlp.cn/ai-challenger-2018-文本挖掘类竞赛相关解决方案及代码汇总" target="_blank" rel="noopener">ai-challenger-2018-文本挖掘类竞赛相关解决方案及代码汇总</a></li>
<li><a href="http://www.52nlp.cn/qa问答系统中的深度学习技术实现" target="_blank" rel="noopener">QA问答系统中的深度学习技术实现</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/42517760" target="_blank" rel="noopener">深度学习代码复现之减少随机性的影响</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io" target="_blank" rel="noopener"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#整体流程"><span class="toc-text">整体流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-FastText-baseline"><span class="toc-text">1. FastText baseline</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-数据情况记录"><span class="toc-text">1.1 数据情况记录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-main-code"><span class="toc-text">1.2 main code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-baseline-效果"><span class="toc-text">1.3 baseline 效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-fastText-速度快"><span class="toc-text">1.4 fastText 速度快</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Attention-RNN、RCNN"><span class="toc-text">2. Attention RNN、RCNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-预处理-data"><span class="toc-text">2.1 预处理 data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Attention-RCNN"><span class="toc-text">2.2 Attention RCNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-loss-function"><span class="toc-text">2.3 loss function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Early-Stop"><span class="toc-text">2.4 Early Stop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-Max-Length-padding"><span class="toc-text">2.6 Max Length (padding)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-ELMO-Like"><span class="toc-text">3. ELMO-Like</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Summary"><span class="toc-text">4. Summary</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/chatbot/">chatbot</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/chatbot/" rel="tag">chatbot</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/07/01/leetcode/leetcode/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          2019 Leetcode
        
      </div>
    </a>
  
  
    <a href="/2019/06/23/datascience/credit-score/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">User Credit Score&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2019/06/25/nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
