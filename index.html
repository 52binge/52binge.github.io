<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一、AI算法基础1.1 防止 overfiting 的 8 条
1). get more data2). Data augmentation3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验)4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；)5). Choosing Right Network Structure6).">
<meta property="og:type" content="website">
<meta property="og:title" content="Home">
<meta property="og:url" content="http://sggo.me/ai1/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="一、AI算法基础1.1 防止 overfiting 的 8 条
1). get more data2). Data augmentation3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验)4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；)5). Choosing Right Network Structure6).">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=g%28s%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-s%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-a1c3ce43528dbc274be8952c06d2b9b4_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-569009cc3ccd3e9922b77c1e4cbf4ca0_hd.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-4713a5b63da71ef5afba3fcd3a65299d_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-a384924b89b1bdd581cef7d75b56e226_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-aca3644ddd56abe1e47c0f45601587c3_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-492ac29bbed274a282eee069c0b63c93_hd.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-3ee1faa3a371c7667fdca01e960dd294_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-5c649d8fbb03dae0703a1b70413ae82d_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-947f270aaae4164a14c9093859cf0cce_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/50/v2-296b158ebb205a2b90d05f5d2074bbe9_hd.gif">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E5%99%A8%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E8%AF%A6%E8%A7%A3/BV-Tradeoff.png">
<meta property="og:updated_time" content="2019-04-15T09:28:06.272Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Home">
<meta name="twitter:description" content="一、AI算法基础1.1 防止 overfiting 的 8 条
1). get more data2). Data augmentation3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验)4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；)5). Choosing Right Network Structure6).">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/chatbot">SPO</a>
        
          <a class="main-nav-link" href="/deeplearning">DeepLearning</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://sggo.me"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="page-undefined" class="article article-type-page" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    <div class="article-meta">
      <!--<a href="/ai1/index.html" class="article-date">
  <time datetime="2019-04-15T09:28:06.287Z" itemprop="datePublished">2019-04-15</time>
</a>-->
      <!-- 
--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://sggo.me/ai1/index.html#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="一、AI算法基础"><a href="#一、AI算法基础" class="headerlink" title="一、AI算法基础"></a>一、AI算法基础</h2><h3 id="1-1-防止-overfiting-的-8-条"><a href="#1-1-防止-overfiting-的-8-条" class="headerlink" title="1.1 防止 overfiting 的 8 条"></a>1.1 防止 overfiting 的 8 条</h3><blockquote>
<p>1). get more data<br>2). Data augmentation<br>3). Regularization（权值衰减）. (L1 拉普拉斯先验, L2 高斯先验)<br>4). Dropout (类似 RF bagging 作用，最后以投票的方式降低过拟合；)<br>5). Choosing Right Network Structure<br>6). Early stopping<br>7). Model Ensumble<br>8). Batch Normalization</p>
<p>Batch Normalization 可以有效避免复杂参数对网络训练产生的影响，也可提高泛化能力.</p>
<p>神经网路的训练过程的本质是学习数据分布，如果训练数据与测试数据分布不同，将大大降低网络泛化能力， BN 是针对每一批数据，在网络的每一层输入之前增加 BN，(均值0，标准差1)。</p>
<p>Dropout 可以抑制过拟合，作用于每份小批量的训练数据，随机丢弃部分神经元机制. bagging 原理.</p>
<p><a href="https://posts.careerengine.us/p/5cae13b2d401440a7fe047af" target="_blank" rel="external">ML算法： 关于防止过拟合，整理了 8 条迭代方向</a></p>
</blockquote>
<h3 id="1-2-机器学习之类别不平衡问题"><a href="#1-2-机器学习之类别不平衡问题" class="headerlink" title="1.2 机器学习之类别不平衡问题"></a>1.2 机器学习之类别不平衡问题</h3><p>机器学习之类别不平衡问题 (1) —— 各种评估指标</p>
<blockquote>
<p>1）过采样和欠采样；（1. 随机过采样(不太使用了，重采样往往会导致严重的过拟合)， 2. <strong>Border-line SMOTE</strong>）<br>2）修改权重（修改损失函数）；<br>3）集成方法：bagging，类似随机森林、自助采样；<br>4）多任务联合学习；</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/34473430" target="_blank" rel="external">机器学习之类别不平衡问题 (1) —— 各种评估指标</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34655990" target="_blank" rel="external">机器学习之类别不平衡问题 (2) —— ROC和PR曲线</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/41237940" target="_blank" rel="external">机器学习之类别不平衡问题 (3) —— 采样方法</a></li>
</ul>
</blockquote>
<h3 id="1-3-CrossEntropy-与-最大似然？"><a href="#1-3-CrossEntropy-与-最大似然？" class="headerlink" title="1.3 CrossEntropy 与 最大似然？"></a>1.3 CrossEntropy 与 最大似然？</h3><blockquote>
<p>1）CrossEntropy lossFunction <img src="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D" alt=""></p>
<p>二分类: <img src="https://www.zhihu.com/equation?tex=g%28s%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-s%7D%7D" alt=""></p>
<p>意义：能表征 真实样本标签 和 预测概率 之间的差值</p>
<p>2）最小化交叉熵的本质就是对数似然函数的最大化；</p>
<p>3）对数似然函数的本质就是衡量在某个参数下，整体的估计和真实情况一样的概率，越大代表越相近；</p>
<p>4）损失函数的本质就是衡量预测值和真实值之间的差距，越大代表越不相近。</p>
</blockquote>
<p>Reference Article</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/38241764" target="_blank" rel="external">知乎： 简单的交叉熵损失函数，你真的懂了吗？</a></p>
<p>我们希望 log P(y|x) 越大越好，反过来，只要 log P(y|x) 的负值 -log P(y|x) 越小就行了。那我们就可以引入损失函数，且令 Loss = -log P(y|x)即可。则得到损失函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D" alt=""></p>
<p>图可以帮助我们对 CrossEntropy lossFunction 有更直观的理解。无论真实样本标签 y 是 0 还是 1，L 都表征了预测输出与 y 的差距。</p>
<p><strong>重点一提：</strong></p>
<p>预测输出与 y 差得越多，L 的值越大，也就是说对当前模型的 “ 惩罚 ” 越大，而且是非线性增大，是一种类似指数增长的级别。这是由 log 函数本身的特性所决定的。这样的好处是 模型会倾向于让预测输出更接近真实样本标签 y。</p>
<p><a href="https://zhuanlan.zhihu.com/p/26614750" target="_blank" rel="external">知乎：一文搞懂极大似然估计</a><br><a href="https://blog.csdn.net/u011508640/article/details/72815981" target="_blank" rel="external">CSDN：详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解</a></p>
<p>就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的 <font color="#c7254e"><strong>模型参数值</strong>！</font></p>
<p>对于这个函数： $P(x|θ)$</p>
<p>输入有两个：$x$ 表示某一个具体的数据；$θ$ 表示模型的参数。</p>
<p>如果 $θ$ 是已知确定的，$x$ 是变量，这个函数叫做概率函数 (probability function)，它描述对于不同的样本点 $x$，其出现概率是多少。</p>
<p>如果 $x$ 是已知确定的，$θ$ 是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现 $x$ 这个样本点的概率是多少。</p>
<p>MLE 提供了一种 <strong>给定观察数据来评估模型参数</strong> 的方法，即：“模型已定，参数未知”。</p>
<p>MLE 中 <strong>采样</strong> 需满足一个重要的假设，就是所有的采样都是 <strong>独立同分布</strong> 的.</p>
<p>一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。</p>
</blockquote>
<h3 id="1-4-SVM-和-LR-的区别与联系？"><a href="#1-4-SVM-和-LR-的区别与联系？" class="headerlink" title="1.4 SVM 和 LR 的区别与联系？"></a>1.4 SVM 和 LR 的区别与联系？</h3><blockquote>
<p>1). 对非线性表达上，LR 只能通过人工的特征组合来实现，而 SVM 可以很容易引入非线性核函数来实现非线性表达，当然也可以通过特征组合。</p>
<p>2). LR 产出的是概率值，而SVM只能产出是正类还是负类，不能产出概率。LR 的损失函数是 log loss，而 SVM 使用的是 hinge loss。</p>
<p>3). SVM 不直接依赖数据分布，而LR则依赖, SVM 主要关注的是“支持向量”，也就是和分类最相关的少数点，即关注局部关键信息；而 LR 是在全局进行优化的。这导致 SVM 天然比 LR 有<strong>更好的泛化能力</strong>，防止过拟合。</p>
<p>4). 损失函数的优化方法不同，LR 是使用 GD 来求解 <strong>对数似然函数</strong> 的最优解；SVM 使用 (Sequnential Minimal Optimal) 顺序最小优化，来求解条件约束损失函数的对偶形式。</p>
<hr>
<p>一般用线性核和高斯核，也就是Linear核与RBF核需要注意的是需要对 <strong>数据归一化处理</strong>.</p>
<p>一般情况下RBF效果是不会差于Linear但是时间上RBF会耗费更多</p>
</blockquote>
<p><strong>Andrew Ng 的见解：</strong></p>
<blockquote>
<ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ol>
</blockquote>
<p><strong>如何量化 feature number 和 sample number：</strong></p>
<blockquote>
<p>n 是feature的数量, m是样本数   </p>
<p>1). feature number &gt;&gt; sample number，则使用LR算法或者不带核函数的SVM（线性分类）<br>  &nbsp;&nbsp;&nbsp;&nbsp; feature number = 1W， sample number = 1K</p>
<p>2). <strong>fn</strong> 小， sample number <strong>一般</strong>1W，使用带有 <strong>kernel函数</strong> 的 SVM算法.  </p>
<p>3). <strong>fn</strong> 小， sample number <strong>很大</strong>5W+（n=1-1000，m=50000+）<br>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; 增加更多的 feature 然后使用LR 算法或者 not have kernel 的 SVM</p>
</blockquote>
<h3 id="1-5-ERM-SRM"><a href="#1-5-ERM-SRM" class="headerlink" title="1.5 ERM / SRM"></a>1.5 ERM / SRM</h3><p>Supervised Learning Obj</p>
<p>$$<br>w^*=argmin_w\sum_iL(y_i,f(x_i;w))+\lambda\Omega(w)<br>$$</p>
<blockquote>
<ol>
<li>第一项对应模型的训练损失函数 (Square Loss、Hinge loss、Exp loss、Log loss)</li>
<li>第二项对应模型的正则化项 （模型参数向量的范数）</li>
</ol>
<p>经验风险最小化 empirical risk minimization, 结构风险最小化 structural risk minimization</p>
</blockquote>
<p>李沐曾经说过：</p>
<blockquote>
<p>model是用离散特征还是连续特征，其实是“<strong>海量离散特征+简单模型</strong>” 同 “<strong>少量连续特征+复杂模型</strong>”的权衡。</p>
<p>既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾 <strong>feature</strong> 还是折腾 <strong>model</strong> 了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</p>
</blockquote>
<h3 id="1-6-Linear-classifier-Nonlinear-classifier-区别优劣"><a href="#1-6-Linear-classifier-Nonlinear-classifier-区别优劣" class="headerlink" title="1.6 Linear classifier / Nonlinear classifier 区别优劣?"></a>1.6 Linear classifier / Nonlinear classifier 区别优劣?</h3><blockquote>
<p>线性和非线性是针对，模型参数和输入特征来讲的；</p>
<p>比如输入x，模型 y=ax+ax^2 那么就是 nonlinear model 如果输入是x和X^2则模型是线性的。</p>
<ol>
<li>Linear classifier 可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。</li>
<li>nonlinear classifier 拟合能力较强，不足之处是数据量不足容易 <strong>overfiting</strong> 、计算复杂度高、解释性不好。</li>
</ol>
<p>Linear classifier ：LR,贝叶斯分类，单层感知机、线性回归</p>
<p>nonlinear classifier：决策树、RF、GBDT、多层感知机</p>
<p>SVM 两种都有（看线性核还是高斯核）</p>
</blockquote>
<h3 id="1-7-Random-Forest"><a href="#1-7-Random-Forest" class="headerlink" title="1.7 Random Forest"></a>1.7 Random Forest</h3><blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/34534004" target="_blank" rel="external">RF、bagging、boosting、GBDT、xgboost算法总结</a></li>
</ul>
<p>RF 是一个典型的多个决策树的组合分类器。</p>
<p>1). 数据的随机性选取<br>2). 待选特征的随机选取</p>
</blockquote>
<p>Sample Random： </p>
<p><img src="https://pic1.zhimg.com/80/v2-a1c3ce43528dbc274be8952c06d2b9b4_hd.jpg" alt=""></p>
<p>Feature Random：</p>
<blockquote>
<p>与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。</p>
</blockquote>
<p><img src="https://pic1.zhimg.com/80/v2-569009cc3ccd3e9922b77c1e4cbf4ca0_hd.jpg" alt=""></p>
<h3 id="1-8-GBDT"><a href="#1-8-GBDT" class="headerlink" title="1.8 GBDT"></a>1.8 GBDT</h3><p>GBDT 是以决策树（CART）为基学习器的 GB算法，是迭代树，而不是分类树。</p>
<p>一般 Boosting 算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。</p>
<p><img src="https://pic2.zhimg.com/80/v2-4713a5b63da71ef5afba3fcd3a65299d_hd.jpg" alt=""></p>
<p>GBDT 的核心就在于：<strong>每一棵树学的是之前所有树结论和的残差</strong>，这个残差就是一个加预测值后能得真实值的累加量。</p>
<p><img src="https://pic3.zhimg.com/80/v2-a384924b89b1bdd581cef7d75b56e226_hd.jpg" alt=""></p>
<h3 id="1-9-RF-vs-GBDT-区别"><a href="#1-9-RF-vs-GBDT-区别" class="headerlink" title="1.9 RF vs GBDT 区别"></a>1.9 RF vs GBDT 区别</h3><blockquote>
<ol>
<li>组成 RF 的树可以是分类树，也可以是回归树；而GBDT只由回归树组成 </li>
<li>组成 RF 的树可以并行生成；而GBDT只能是串行生成 </li>
<li>对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来 </li>
<li>RF 对异常值不敏感，GBDT对异常值非常敏感 </li>
<li>RF 对训练集一视同仁，GBDT是基于权值的弱分类器的集成 </li>
<li>RF 是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能</li>
</ol>
</blockquote>
<h3 id="1-10-GBDT-vs-Xgboost"><a href="#1-10-GBDT-vs-Xgboost" class="headerlink" title="1.10 GBDT vs Xgboost"></a>1.10 GBDT vs Xgboost</h3><p>Xgboost相比于GBDT来说，更加有效应用了数值优化，最重要是<strong>对损失函数</strong>（预测值和真实值的误差）<strong>变得更复杂</strong>。目标函数依然是所有树的预测值相加等于预测值。</p>
<blockquote>
<ol>
<li>二阶泰勒展开，同时用到了一阶和二阶导数</li>
<li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度</li>
<li>Shrinkage（缩减），相当于学习速率（xgboost中的eta）</li>
<li>列抽样（column subsampling）。xgboost借鉴RF做法，支持列抽样（即每次的输入特征不是全部特征)</li>
<li>并行化处理： 预先对每个特征内部进行了排序找出候选切割点.各个<strong>feature</strong>的增益计算就可以开多线程进行.</li>
</ol>
</blockquote>
<a href="!--![](https://pic2.zhimg.com/80/v2-1c0706e463f78b6036b3923048ac9149_hd.jpg)--">!--![](https://pic2.zhimg.com/80/v2-1c0706e463f78b6036b3923048ac9149_hd.jpg)--</a>
<blockquote>
<p>好的模型需要具备两个基本要素：</p>
<ol>
<li>是要有好的精度（即好的拟合程度）</li>
<li>是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定）</li>
</ol>
<p>因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项）</p>
<p>常用的误差项有平方误差和逻辑斯蒂误差，常见的惩罚项有l1，l2正则，l1正则是将模型各个元素进行求和，l2正则是对元素求平方。</p>
<p><a href="https://zhuanlan.zhihu.com/p/34534004" target="_blank" rel="external">ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结</a></p>
</blockquote>
<p><font color="#c7254e">Bagging &amp; Boosting</font> 的理念：</p>
<p>Bagging 的思想比较简单，即每一次从原始数据中根据 <strong>均匀概率分布有放回的抽取和原始数据大小相同的样本集合</strong>，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。</p>
<p>boosting 的每一次抽样的 <strong>样本分布都是不一样</strong> 的。每一次迭代，都根据上一次迭代的结果，<strong>增加被错误分类的样本的权重</strong>，使得模型能在之后的迭代中更加注意到 <strong>难以分类的样本</strong>，这是一个 <strong>不断学习的过程，也是一个不断提升</strong> 的过程，这也就是boosting思想的本质所在。 迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。</p>
<p><img src="https://pic4.zhimg.com/80/v2-aca3644ddd56abe1e47c0f45601587c3_hd.jpg" alt=""></p>
<h3 id="1-11-Evaluation-Metric-对比"><a href="#1-11-Evaluation-Metric-对比" class="headerlink" title="1.11 Evaluation Metric 对比 ?"></a>1.11 Evaluation Metric 对比 ?</h3><p>Confusion Matrix</p>
<p><strong>1). accuracy</strong> (正反比例严重失衡，则没意义，存在 accuracy paradox 现象)</p>
<p><img src="https://pic1.zhimg.com/80/v2-492ac29bbed274a282eee069c0b63c93_hd.jpg" alt=""></p>
<blockquote>
<p>accuracy 准确率 = (TP+TN)/(TP+TN+FP+FN), <strong>准确率可以判断总的正确率</strong></p>
</blockquote>
<p><strong>2). precision</strong> 查准率 (80% = 你一共预测了100个正例，80个是对的正例)</p>
<p><img src="https://pic2.zhimg.com/80/v2-3ee1faa3a371c7667fdca01e960dd294_hd.jpg" alt=""></p>
<blockquote>
<p>Precision = TP/(TP+FP)</p>
</blockquote>
<p><strong>3). recall</strong> (样本中的正例有多少被预测正确 TPR = TP/(TP+FN))</p>
<p><img src="https://pic1.zhimg.com/80/v2-5c649d8fbb03dae0703a1b70413ae82d_hd.jpg" alt=""></p>
<p><strong>4). F1-score</strong> （precision 和 recall 的 metric）</p>
<blockquote>
<p>2*precision*recall / (precision + recall)</p>
</blockquote>
<p><strong>5). P-R（precision-recall）PRC</strong></p>
<blockquote>
<p>依靠 LR 举例:</p>
<p>这条曲线是根据什么变化的？为什么是这个形状的曲线？</p>
<p>这个阈值是我们随便定义的，我们并不知道这个阈值是否符合我们的要求</p>
<p>遍历 0 到 1 之间所有的阈值, 得到了这条曲线</p>
</blockquote>
<p><strong>5). ROC curve</strong> （TPR 纵轴，FPR 横轴，TP（真正率）和 FP（假正率），设一个阈值）</p>
<blockquote>
<p>ROC（Receiver Operating Characteristic）曲线。 ROC 曲线 是基于混淆矩阵得出的。 </p>
<p>TPR = recall = 灵敏度 = P（X=1 | Y=1）<br>FPR = 特异度 = P（X=0 | Y=0）</p>
</blockquote>
<p><img src="https://pic3.zhimg.com/80/v2-947f270aaae4164a14c9093859cf0cce_hd.jpg" alt=""></p>
<p><strong>ROC曲线的阈值问题:</strong></p>
<blockquote>
<p>与前面的P-R曲线类似，ROC曲线也是通过遍历所有阈值来绘制整条曲线的。 </p>
</blockquote>
<p><img src="https://pic4.zhimg.com/50/v2-296b158ebb205a2b90d05f5d2074bbe9_hd.gif" alt=""></p>
<p><strong>ROC曲线无视样本不平衡</strong></p>
<blockquote>
<ul>
<li><a href="https://www.zhihu.com/question/30643044" target="_blank" rel="external">精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34655990" target="_blank" rel="external">机器学习之类别不平衡问题 (2) —— ROC和PR曲线</a></li>
</ul>
</blockquote>
<p><strong>6). AUC</strong>  (AUC = 0.5，跟随机猜测一样， ROC 纵轴 TPR 越大， 横轴 FPR 越小 模型越好）</p>
<blockquote>
<p>0.5 - 0.7：效果较低，但用于预测股票已经很不错了<br>0.7 - 0.85：效果一般<br>0.85 - 0.95：效果很好</p>
<p>real world data 经常会面临 class imbalance 问题，即正负样本比例失衡。</p>
<p>根据计算公式可以推知，在 testing set 出现 imbalance 时 ROC曲线 能保持不变，而 PR 则会出现大变化。</p>
</blockquote>
<p><strong>7). multi-class classification</strong> 如果非要用一个综合考量的 metric 的话，</p>
<blockquote>
<ol>
<li>macro-average（宏平均） </li>
<li>micro-average（微平均）</li>
</ol>
<p>macro-average（宏平均） 会比 micro-average（微平均）好一些哦，因为 macro 会受 minority class 影响更大，也就是说更能体现在 small class 上的 performance.</p>
</blockquote>
<p>precision &amp; recall</p>
<blockquote>
<p>precision 是相对你自己的模型预测而言<br>recall 是相对真实的答案而言</p>
</blockquote>
<h3 id="1-12-Bias-Variance-Tradeoff"><a href="#1-12-Bias-Variance-Tradeoff" class="headerlink" title="1.12 Bias-Variance Tradeoff"></a>1.12 Bias-Variance Tradeoff</h3><p><img src="https://charlesliuyx.github.io/2017/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E5%99%A8%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E8%AF%A6%E8%A7%A3/BV-Tradeoff.png" width="450"></p>
<blockquote>
<ul>
<li><a href="https://charlesliuyx.github.io" target="_blank" rel="external">Bias-Variance Tradeoff</a></li>
</ul>
</blockquote>
<h3 id="1-13-激活函数的对比？"><a href="#1-13-激活函数的对比？" class="headerlink" title="1.13 激活函数的对比？"></a>1.13 激活函数的对比？</h3><blockquote>
<ol>
<li>Sigmoid 和 Tanh 为什么会导致 Vanishing/Exploding gradients ? </li>
<li>Tanh 值域 (-1,1) Sigmoid 值域 (0,1)</li>
<li>ReLU 的优点，和局限性分别是什么? </li>
<li><a href="https://zhuanlan.zhihu.com/p/48776056" target="_blank" rel="external">谈谈激活函数 Sigmoid,Tanh,ReLu,softplus,softmax</a></li>
</ol>
</blockquote>
<h3 id="1-14-sigmoid-用作激活函数时，分类为什么要用-crossentry-loss，而不用均方损失？"><a href="#1-14-sigmoid-用作激活函数时，分类为什么要用-crossentry-loss，而不用均方损失？" class="headerlink" title="1.14 sigmoid 用作激活函数时，分类为什么要用 crossentry loss，而不用均方损失？"></a>1.14 sigmoid 用作激活函数时，分类为什么要用 crossentry loss，而不用均方损失？</h3><blockquote>
<ol>
<li>softmax函数可以看做是Sigmoid函数的一般化，可以进行多分类。</li>
<li>非常适合用于<code>分类</code>问题： <code>Cross Entropy</code> 交叉熵损失函数</li>
<li>Square error loss function 与 Cross Entropy Error Function 分别适合什么景？</li>
</ol>
</blockquote>
<h2 id="二、NLP高频问题"><a href="#二、NLP高频问题" class="headerlink" title="二、NLP高频问题"></a>二、NLP高频问题</h2><p><strong>1. word2vec 和 tf-idf 相似度计算时的区别？</strong></p>
<blockquote>
<p><strong>word2vec:</strong> </p>
<p>1). 稠密的 低维度的 </p>
<p>2). 表达出相似度； </p>
<p>3). 表达能力强；</p>
<p>4). 泛化能力强；</p>
</blockquote>
<ol>
<li><p>word2vec和NNLM对比有什么区别？（word2vec vs NNLM）</p>
</li>
<li><p>word2vec负采样有什么作用？</p>
</li>
<li><p>word2vec和fastText对比有什么区别？（word2vec vs fastText）</p>
</li>
</ol>
<p>1）都可以无监督学习词向量， fastText训练词向量时会考虑subword；</p>
<p>2）fastText还可以进行有监督学习进行文本分类，其主要特点：</p>
<p>结构与CBOW类似，但学习目标是人工标注的分类结果；<br>采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径；<br>引入N-gram，考虑词序特征；<br>引入subword来处理长词，处理未登陆词问题；</p>
<ol>
<li><p>glove和word2vec、 LSA对比有什么区别？（word2vec vs glove vs LSA）</p>
</li>
<li><p>elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）</p>
</li>
<li><p>RNN 和 LSTM 和 GRU 的区别？</p>
</li>
</ol>
<h2 id="三、其他算法问题"><a href="#三、其他算法问题" class="headerlink" title="三、其他算法问题"></a>三、其他算法问题</h2><p>1、怎么进行单个样本的学习？<br>2、 决策树 bagging boosting adaboost 区别？RF的特征随机目的是什么？<br>3、transformer各部分怎么用？Q K V怎么计算；Attention怎么用？<br>4、HMM 假设是什么？CRF解决了什么问题？CRF做过特征工程吗？HMM中的矩阵意义？5、说以一下空洞卷积？膨胀卷积怎么理解？什么是Piece-CNN？<br>6、怎么解决beam-search局部最优问题？global embedding 怎么做？<br>7、数学题：什么是半正定矩阵？机器学习中有什么应用？<br>8、卷积的物理意义是什么？傅里叶变换懂吗？<br>9、说一下Bert？<br>10、推导word2vec？<br>11、怎么理解传统的统计语言模型？现在的神经网络语言模型有什么不同？<br>12、神经网络优化的难点是什么？这个问题要展开来谈。<br>13、attention你知道哪些？<br>14、自动文章摘要抽取时，怎么对一篇文章进行分割？（从序列标注、无监督等角度思考）<br>15、在做NER任务时，lstm后面可以不用加CRF吗？<br>16、通过画图描述TextRank？<br>17、LDA和pLSA有什么区别？<br>18、Transformer在实际应用中都会有哪些做法？<br>19、讲出过拟合的解决方案？<br>20、说一下transforemr、LSTM、CNN间的区别？从多个角度进行讲解？<br>21、梯度消失的原因和解决办法有哪些？<br>22、数学题：贝叶斯计算概率？<br>23、数学题：25只兔子赛跑问题，共5个赛道，最少几次比赛可以选出前5名？<br>24、数学题：100盏灯问题？</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/57153934" target="_blank" rel="external">【NLP/AI算法面试必备-2】NLP/AI面试全记录（持续更新）</a></li>
<li><a href="https://www.zhihu.com/people/lou-jie-9/posts" target="_blank" rel="external">【NLP/AI算法面试必备-1】学习NLP/AI，必须深入理解“神经网络及其优化问题”</a></li>
<li><a href="https://www.zhihu.com/people/lou-jie-9/posts" target="_blank" rel="external">JayLouNLP算法工程师</a></li>
</ul>

      
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、AI算法基础"><span class="toc-number"></span> <span class="toc-text">一、AI算法基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-防止-overfiting-的-8-条"><span class="toc-number"></span> <span class="toc-text">1.1 防止 overfiting 的 8 条</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-机器学习之类别不平衡问题"><span class="toc-number"></span> <span class="toc-text">1.2 机器学习之类别不平衡问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-CrossEntropy-与-最大似然？"><span class="toc-number"></span> <span class="toc-text">1.3 CrossEntropy 与 最大似然？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-SVM-和-LR-的区别与联系？"><span class="toc-number"></span> <span class="toc-text">1.4 SVM 和 LR 的区别与联系？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-ERM-SRM"><span class="toc-number"></span> <span class="toc-text">1.5 ERM / SRM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-Linear-classifier-Nonlinear-classifier-区别优劣"><span class="toc-number"></span> <span class="toc-text">1.6 Linear classifier / Nonlinear classifier 区别优劣?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-Random-Forest"><span class="toc-number"></span> <span class="toc-text">1.7 Random Forest</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-8-GBDT"><span class="toc-number"></span> <span class="toc-text">1.8 GBDT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-9-RF-vs-GBDT-区别"><span class="toc-number"></span> <span class="toc-text">1.9 RF vs GBDT 区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-10-GBDT-vs-Xgboost"><span class="toc-number"></span> <span class="toc-text">1.10 GBDT vs Xgboost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-11-Evaluation-Metric-对比"><span class="toc-number"></span> <span class="toc-text">1.11 Evaluation Metric 对比 ?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-12-Bias-Variance-Tradeoff"><span class="toc-number"></span> <span class="toc-text">1.12 Bias-Variance Tradeoff</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-13-激活函数的对比？"><span class="toc-number"></span> <span class="toc-text">1.13 激活函数的对比？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-14-sigmoid-用作激活函数时，分类为什么要用-crossentry-loss，而不用均方损失？"><span class="toc-number"></span> <span class="toc-text">1.14 sigmoid 用作激活函数时，分类为什么要用 crossentry loss，而不用均方损失？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、NLP高频问题"><span class="toc-number"></span> <span class="toc-text">二、NLP高频问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三、其他算法问题"><span class="toc-number"></span> <span class="toc-text">三、其他算法问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number"></span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        

      </footer>
    
  </div>
  
    
  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://sggo.me/ai1/index.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
