<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="摘要心得1：



Sigmoid 和 Tanh 为什么会导致 Vanishing/Exploding gradients ? 
Tanh 值域 (-1,1) Sigmoid 值域 (0,1)
ReLU 的优点，和局限性分别是什么? 
谈谈激活函数 Sigmoid,Tanh,ReLu,softplus,softmax


摘要心得2：



softmax函数可以看做是Sigmoid函数的一般化，">
<meta property="og:type" content="website">
<meta property="og:title" content="Home">
<meta property="og:url" content="http://sggo.me/sggo4/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="摘要心得1：



Sigmoid 和 Tanh 为什么会导致 Vanishing/Exploding gradients ? 
Tanh 值域 (-1,1) Sigmoid 值域 (0,1)
ReLU 的优点，和局限性分别是什么? 
谈谈激活函数 Sigmoid,Tanh,ReLu,softplus,softmax


摘要心得2：



softmax函数可以看做是Sigmoid函数的一般化，">
<meta property="og:updated_time" content="2019-04-03T02:34:29.838Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Home">
<meta name="twitter:description" content="摘要心得1：



Sigmoid 和 Tanh 为什么会导致 Vanishing/Exploding gradients ? 
Tanh 值域 (-1,1) Sigmoid 值域 (0,1)
ReLU 的优点，和局限性分别是什么? 
谈谈激活函数 Sigmoid,Tanh,ReLu,softplus,softmax


摘要心得2：



softmax函数可以看做是Sigmoid函数的一般化，">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/chatbot">SPO</a>
        
          <a class="main-nav-link" href="/deeplearning">DeepLearning</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://sggo.me"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="page-undefined" class="article article-type-page" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    <div class="article-meta">
      <!--<a href="/sggo4/index.html" class="article-date">
  <time datetime="2019-04-03T02:34:29.850Z" itemprop="datePublished">2019-04-03</time>
</a>-->
      <!-- 
--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://sggo.me/sggo4/index.html#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <font color="#c7254e"><strong>摘要心得1：</strong></font>

<blockquote>
<ol>
<li>Sigmoid 和 Tanh 为什么会导致 Vanishing/Exploding gradients ? </li>
<li>Tanh 值域 (-1,1) Sigmoid 值域 (0,1)</li>
<li>ReLU 的优点，和局限性分别是什么? </li>
<li><a href="https://zhuanlan.zhihu.com/p/48776056" target="_blank" rel="external">谈谈激活函数 Sigmoid,Tanh,ReLu,softplus,softmax</a></li>
</ol>
</blockquote>
<font color="#c7254e"><strong>摘要心得2：</strong></font>

<blockquote>
<ol>
<li>softmax函数可以看做是Sigmoid函数的一般化，可以进行多分类。</li>
<li>非常适合用于<code>分类</code>问题： <code>Cross Entropy</code> 交叉熵损失函数</li>
<li>Square error loss function 与 Cross Entropy Error Function 分别适合什么景？</li>
<li><a href="https://blog.csdn.net/u012033832/article/details/78401486" target="_blank" rel="external">偏差和方差与过拟合欠拟合的关系 ?</a></li>
</ol>
</blockquote>
<font color="#c7254e"><strong>摘要心得3：</strong></font>

<blockquote>
<p>解决 overfiting 的方法:</p>
<ol>
<li>Data augmentation</li>
<li>Regularization</li>
<li>Model Ensembel</li>
<li>Dropout 是 model 集成方法中最高效常用的技巧</li>
</ol>
<p>Batch Normalization 可以有效避免复杂参数对网络训练产生的影响，也可提高泛化能力.</p>
<p>神经网路的训练过程的本质是学习数据分布，如果训练数据与测试数据分布不同，将大大降低网络泛化能力， BN 是针对每一批数据，在网络的每一层输入之前增加 BN，(均值0，标准差1)。</p>
<p>Dropout 可以抑制过拟合，作用于每份小批量的训练数据，随机丢弃部分神经元机制. bagging 原理.</p>
</blockquote>
<font color="#c7254e"><strong>摘要心得4：</strong></font>  

<blockquote>
<ol>
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/6279498" target="_blank" rel="external">十道海量数据处理面试题</a></li>
</ol>
</blockquote>
<font color="#c7254e"><strong>摘要心得5：</strong></font>  

<p><strong>RF &amp; GBDT 区别:</strong></p>
<blockquote>
<ol>
<li>组成 RF 的树可以是分类树，也可以是回归树；而GBDT只由回归树组成 </li>
<li>组成 RF 的树可以并行生成；而GBDT只能是串行生成 </li>
<li>对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来 </li>
<li>RF 对异常值不敏感，GBDT对异常值非常敏感 </li>
<li>RF 对训练集一视同仁，GBDT是基于权值的弱分类器的集成 </li>
<li>RF 是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能</li>
</ol>
</blockquote>
<font color="#c7254e"><strong>摘要心得6：</strong></font>  

<p><strong>LR 和 SVM 区别:</strong></p>
<blockquote>
<ol>
<li>LR是参数模型，SVM是非参数模型。</li>
<li>LR: logistical loss，SVM: hinge loss<br>这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</li>
<li>LR 大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</li>
<li>SVM 不直接依赖数据分布，而LR则依赖，因为SVM只与支持向量那几个点有关系，而<strong>LR和所有点都有关系</strong>。</li>
</ol>
</blockquote>
<p>一般用线性核和高斯核，也就是Linear核与RBF核需要注意的是需要对数据归一化处理.</p>
<p>一般情况下RBF效果是不会差于Linear但是时间上RBF会耗费更多</p>
<p><strong>Andrew Ng:</strong></p>
<p>n是feature的数量, m是样本数   </p>
<blockquote>
<ol>
<li><p>如果n相对于m来说很大，则使用LR算法或者不带核函数的SVM（线性分类）<br>   n远大于m，n=10000，m=10-1000.  </p>
</li>
<li><p>如果n很小，m的数量适中（n=1-1000，m=10-10000）使用带有核函数的SVM算法.  </p>
</li>
<li><p>如果n很小，m很大（n=1-1000，m=50000+）增加更多的feature然后使用LR算法或者不带核函数的SVMLR和不带核函数的SVM比较类似。</p>
</li>
</ol>
</blockquote>
<p>吴恩达的见解：</p>
<blockquote>
<ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ol>
</blockquote>
<h3 id="3-经验风险最小化和结构风险最小化SPM"><a href="#3-经验风险最小化和结构风险最小化SPM" class="headerlink" title="3. 经验风险最小化和结构风险最小化SPM"></a>3. <a href="https://blog.csdn.net/munan2017/article/details/80288090" target="_blank" rel="external">经验风险最小化和结构风险最小化SPM</a></h3><font color="#c7254e"><strong>摘要心得7：</strong></font>  

<blockquote>
<p><strong>李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</strong></p>
</blockquote>
<h4 id="28-逻辑斯特回归为什么要对特征进行离散化。机器学习-ML模型-中等"><a href="#28-逻辑斯特回归为什么要对特征进行离散化。机器学习-ML模型-中等" class="headerlink" title="28 逻辑斯特回归为什么要对特征进行离散化。机器学习 ML模型 中等"></a>28 逻辑斯特回归为什么要对特征进行离散化。机器学习 ML模型 中等</h4><p>@严林，本题解析来源：<a href="https://www.zhihu.com/question/31989952" target="_blank" rel="external">https://www.zhihu.com/question/31989952</a></p>
<p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<blockquote>
<ol>
<li>离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li>
<li>离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li>
</ol>
</blockquote>
<font color="#c7254e"><strong>摘要心得8：</strong></font> 

<h4 id="30-hash-冲突及解决办法。数据结构-算法-中等"><a href="#30-hash-冲突及解决办法。数据结构-算法-中等" class="headerlink" title="30 hash 冲突及解决办法。数据结构/算法 中等"></a>30 hash 冲突及解决办法。数据结构/算法 中等</h4><p>@Sommer_Xia，来源：<a href="http://blog.csdn.net/shymi1991/article/details/39432775" target="_blank" rel="external">http://blog.csdn.net/shymi1991/article/details/39432775</a><br>关键字值不同的元素可能会映象到哈希表的同一地址上就会发生哈希冲突。</p>
<p>解决办法：</p>
<p>1）开放定址法：当冲突发生时，使用某种探查(亦称探测)技术在散列表中形成一个探查(测)序列。沿此序列逐个单元地查找，直到找到给定 的关键字，或者碰到一个开放的地址(即该地址单元为空)为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。查找时探查到开放的 地址则表明表中无待查的关键字，即查找失败。<br>2） 再哈希法：同时构造多个不同的哈希函数。<br>3）链地址法：将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。<br>4）建立公共溢出区：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。</p>
<font color="#c7254e"><strong>摘要心得9：</strong></font> 

<p><strong>线性分类器与非线性分类器的区别以及优劣?</strong>。</p>
<blockquote>
<p>@伟祺，线性和非线性是针对，模型参数和输入特征来讲的；</p>
<p>比如输入x，模型y=ax+ax^2那么就是非线性模型 如果输入是x和X^2则模型是线性的。</p>
<ol>
<li>线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。</li>
<li>非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。</li>
</ol>
<p>常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归</p>
<p>常见的非线性分类器：决策树、RF、GBDT、多层感知机</p>
<p>SVM两种都有（看线性核还是高斯核）</p>
</blockquote>
<p><strong>请简要介绍下tensorflow的计算图，深度学习 DL框架 中</strong></p>
<blockquote>
<p>Tensorflow 通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可把计算图看做是一种有向图.</p>
<p>Tensorflow 中的每一个节点都是计算图上的一个Tensor, 也就是张量，而节点之间的边描述了计算之间的依赖关系(定义时) 和 数学操作(运算时)。</p>
</blockquote>
<font color="#c7254e"><strong>摘要心得10：</strong></font> 

<p><strong>KNN中的K如何选取的？机器学习 ML模型 易</strong></p>
<p>KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：</p>
<blockquote>
<p>1). 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；  </p>
<p>2). 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。  </p>
<p>3). K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</p>
<p>在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</p>
</blockquote>
<font color="#c7254e"><strong>摘要心得11：</strong></font> 

<blockquote>
<p><a href="https://www.cnblogs.com/pinard/p/6160412.html" target="_blank" rel="external">scikit-learn随机森林调参小结</a></p>
</blockquote>
<font color="#c7254e"><strong>摘要心得12：</strong></font> 

<p><strong>XGBoost 怎么给特征评分？</strong></p>
<blockquote>
<p>在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。</p>
</blockquote>
<p><strong>什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</strong></p>
<blockquote>
<p>Bagging方法中Bootstrap每次约有的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这的数据称为袋外数据OOB（out of bag）,它可以用于取代测试集误差估计方法。</p>
<p><strong>袋外数据(OOB)误差的计算方法如下：</strong> </p>
<p>对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类,因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=;这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。</p>
</blockquote>
<h3 id="2-机器学习interview320道"><a href="#2-机器学习interview320道" class="headerlink" title="2. 机器学习interview320道"></a>2. <a href="https://blog.csdn.net/qq_33335553/article/details/80651017" target="_blank" rel="external">机器学习interview320道</a></h3><h3 id="2-机器学习常见面试题整理"><a href="#2-机器学习常见面试题整理" class="headerlink" title="2. 机器学习常见面试题整理"></a>2. <a href="http://kubicode.me/2015/08/16/Machine%20Learning/Common-Interview/" target="_blank" rel="external">机器学习常见面试题整理</a></h3><h3 id="3-机器学习面试题总结"><a href="#3-机器学习面试题总结" class="headerlink" title="3. 机器学习面试题总结"></a>3. <a href="https://zhuanlan.zhihu.com/c_129612503" target="_blank" rel="external">机器学习面试题总结</a></h3><h3 id="5-BAT机器学习面试1000题系列"><a href="#5-BAT机器学习面试1000题系列" class="headerlink" title="5. BAT机器学习面试1000题系列"></a>5. <a href="https://blog.csdn.net/v_july_v/article/details/78121924" target="_blank" rel="external">BAT机器学习面试1000题系列</a></h3><h2 id="工程架构"><a href="#工程架构" class="headerlink" title="工程架构"></a>工程架构</h2><h3 id="1-47道机器学习常见面试题（上）"><a href="#1-47道机器学习常见面试题（上）" class="headerlink" title="1. 47道机器学习常见面试题（上）"></a>1. <a href="https://zhuanlan.zhihu.com/p/45091568" target="_blank" rel="external">47道机器学习常见面试题（上）</a></h3><h3 id="2-操作系统面试题"><a href="#2-操作系统面试题" class="headerlink" title="2. 操作系统面试题"></a>2. <a href="https://zhuanlan.zhihu.com/p/23755202" target="_blank" rel="external">操作系统面试题</a></h3><h3 id="3-网络面试题"><a href="#3-网络面试题" class="headerlink" title="3. 网络面试题"></a>3. <a href="https://zhuanlan.zhihu.com/p/24001696" target="_blank" rel="external">网络面试题</a></h3><h3 id="4-TCP-IP四层模型"><a href="#4-TCP-IP四层模型" class="headerlink" title="4. TCP/IP四层模型"></a>4. <a href="http://www.cnblogs.com/BlueTzar/articles/811160.html" target="_blank" rel="external">TCP/IP四层模型</a></h3><h3 id="5-数据库篇"><a href="#5-数据库篇" class="headerlink" title="5. 数据库篇"></a>5. <a href="https://zhuanlan.zhihu.com/p/23713529?refer=passer" target="_blank" rel="external">数据库篇</a></h3><h3 id="6-面试题"><a href="#6-面试题" class="headerlink" title="6. 面试题"></a>6. <a href="http://python.jobbole.com/85231/" target="_blank" rel="external">面试题</a></h3>
      
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-经验风险最小化和结构风险最小化SPM"><span class="toc-number"></span> <span class="toc-text">3. 经验风险最小化和结构风险最小化SPM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#28-逻辑斯特回归为什么要对特征进行离散化。机器学习-ML模型-中等"><span class="toc-number"></span> <span class="toc-text">28 逻辑斯特回归为什么要对特征进行离散化。机器学习 ML模型 中等</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#30-hash-冲突及解决办法。数据结构-算法-中等"><span class="toc-number"></span> <span class="toc-text">30 hash 冲突及解决办法。数据结构/算法 中等</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-机器学习interview320道"><span class="toc-number"></span> <span class="toc-text">2. 机器学习interview320道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-机器学习常见面试题整理"><span class="toc-number"></span> <span class="toc-text">2. 机器学习常见面试题整理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-机器学习面试题总结"><span class="toc-number"></span> <span class="toc-text">3. 机器学习面试题总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-BAT机器学习面试1000题系列"><span class="toc-number"></span> <span class="toc-text">5. BAT机器学习面试1000题系列</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#工程架构"><span class="toc-number"></span> <span class="toc-text">工程架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-47道机器学习常见面试题（上）"><span class="toc-number"></span> <span class="toc-text">1. 47道机器学习常见面试题（上）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-操作系统面试题"><span class="toc-number"></span> <span class="toc-text">2. 操作系统面试题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-网络面试题"><span class="toc-number"></span> <span class="toc-text">3. 网络面试题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-TCP-IP四层模型"><span class="toc-number"></span> <span class="toc-text">4. TCP/IP四层模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-数据库篇"><span class="toc-number"></span> <span class="toc-text">5. 数据库篇</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-面试题"><span class="toc-number"></span> <span class="toc-text">6. 面试题</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        

      </footer>
    
  </div>
  
    
  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://sggo.me/sggo4/index.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
