<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark - Data Skew Advanced - Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark - Data Skew Advanced">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;2021&#x2F;01&#x2F;27&#x2F;spark&#x2F;spark-summary-4-data-skew&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-summary-logo-1.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-data-skew-1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-data-skew-2.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-data-skew-reduce-by-key.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-data-skew-5-join.png">
<meta property="og:updated_time" content="2021-02-02T01:05:10.747Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-summary-logo-1.jpg">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<!-- jiangting add start... @2020.08.30 -->
<!-- <div id="menu" class="duration-main" style="background-color: #e7e7e7"> -->
<!--   <p class="links-p" href="/">Blair</p> -->
<!--   <address class="icons"> -->
<!--     <a href="https://github.com/blair101" class="icon-font icon linkedin" target="_blank"></a> -->
<!--     <a href="https://cn.linkedin.com/pub/tianyu-dai/a8/818/44a" class="icon-font icon linkedin" target="_blank"></a> -->
<!--   </address> -->
<!--   <div class="hr1"></div> -->
<!--   <nav> -->
<!--     <div> -->
<!--     <a class="home" href="/">Home</a></div> -->
<!--     <p class="links-p" href="/">Home</p> -->
<!--     <nav class="tag-ath"> -->
<!--       <a class="proj" href="/categories">Category</a> -->
<!--       <a class="authors" href="/about">About</a> -->
<!--     </nav> -->
<!--   </nav> -->
<!--   <div class="hr2"></div> -->
<!--     <p class="links-p">Links</p> -->
<!--       <address class="links"> -->
<!--       <a class="proj" href="/article/Create-MyWorld">Projects</a> -->
<!--       <a class="friend">Friends</a></address><div class="hr3"> -->
<!--   </div> -->
<!--   <p class="end"></p> -->
<!--   <div id="menu-links" class="duration-main" style="top: -400px; background-color: #f5f5f5"> -->
<!--     <address> -->
<!--       <li><a target="_blank" href="http://lm7.xxxxxxxx.jp">Lm7</a></li> -->
<!--       <li><a target="_blank" href="http://www.pixiv.net/member.php?id=4933015">Domik</a></li> -->
<!--       <li><a target="_blank" href="http://hana-ui.moe">hana-ui</a></li> -->
<!--       <li><a target="_blank" href="http://fil.dtysky.moe">F-I-L</a></li> -->
<!--       <li><a target="_blank" href="http://paradise.dtysky.moe">Paradise</a></li> -->
<!--       <li><a target="_blank" href="http://moe-notes.dtysky.moe">MoeNotes</a></li> -->
<!--       <li><a target="_blank" href="http://kanata.dtysky.moe">Kanata</a></li> -->
<!--       <li><a target="_blank" href="http://blog.nekohand.moe">Nekohand</a></li> -->
<!--       <li><a target="_blank" href="http://www.jerryfu.net">JerryFu</a></li> -->
<!--       <li><a target="_blank" href="http://kawabangga.com">南史</a></li> -->
<!--       <li><a>Hide Links</a></li> -->
<!--       <li><a></a></li> -->
<!--     </address> -->
<!--   </div> -->
<!-- </div> -->
<!-- jiangting add end !  @2020.08.30 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-spark/spark-summary-4-data-skew" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark - Data Skew Advanced
      <small class=article-detail-date-index>&nbsp; 2021-01-27</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2021/01/27/spark/spark-summary-4-data-skew/" class="article-date">
  <time datetime="2021-01-26T23:07:21.000Z" itemprop="datePublished">2021-01-27</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2021/01/27/spark/spark-summary-4-data-skew/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <img src="/images/spark/spark-summary-logo-1.jpg" width="500" alt="" />
<a id="more"></a>
<h2 id="1-spark-data-skew"><a class="markdownIt-Anchor" href="#1-spark-data-skew"></a> 1. Spark Data Skew</h2>
<p>For example, there are a total of 1,000 tasks, 997 tasks are executed within 1 minute, but the remaining two or three tasks take one or two hours. This situation is very common.</p>
<p><strong>Most tasks are executed very fast, but some tasks are extremely slow.</strong></p>
<blockquote>
<p>the progress of the entire Spark job is determined by the task with the longest running time.</p>
</blockquote>
<h2 id="2-the-principle-of-data-skew"><a class="markdownIt-Anchor" href="#2-the-principle-of-data-skew"></a> 2. The principle of Data Skew</h2>
<p>when performing shuffle, the <code>same key on each node</code> must be pulled to <code>a task on a node</code> for processing, such as <strong><code>aggregation or join</code></strong> operations according to the key.</p>
<p>For example, most keys correspond to 10 pieces of data, but individual keys correspond to 1 million pieces of data, so most tasks may only be assigned to 10 pieces of data, and then run out in 1 second; but individual tasks may be assigned 1 million pieces The data will run for one or two hours.</p>
<img src="/images/spark/spark-data-skew-1.png" width="700" alt="Data skew only occurs during the shuffle process." />
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th style="text-align:left">trigger shuffle operations <br> when data skew, it may be caused by using one of these operators.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td style="text-align:left">distinct</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td style="text-align:left">groupByKey</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td style="text-align:left">reduceByKey</td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td style="text-align:left">aggregateByKey</td>
</tr>
<tr>
<td style="text-align:center">5.</td>
<td style="text-align:left">join, cogroup, repartition, etc.</td>
</tr>
</tbody>
</table>
<h2 id="3-the-execution-of-a-task-slow"><a class="markdownIt-Anchor" href="#3-the-execution-of-a-task-slow"></a> 3. The execution of a task slow</h2>
<p>The first thing to look at is <strong>which stage of data skew occurs</strong> in.</p>
<ol>
<li>yarn-client submit, you can see the log locally, find which stage is currently running in the log;</li>
<li>yarn-cluster submit, Spark Web UI Run to the first few stages.</li>
</ol>
<blockquote>
<p>Whether using the yarn-client mode or the yarn-cluster mode, we can take a deep look at the amount of data allocated by <strong>each task of this stage</strong> on the Spark Web UI, so as to further determine whether the uneven data allocated by the task causes data skew.</p>
</blockquote>
<img src="/images/spark/spark-data-skew-2.png" width="880" alt="Data skew only occurs during the shuffle process." />
<p>After knowing which stage the data skew occurs, then we need to calculate which part of the code corresponds to the stage where the skew occurs based on the principle of stage division.</p>
<p><strong>solution</strong>: as long as you see a shuffle operator or Spark SQL SQL in the Spark code If there is a statement that will cause shuffle in the statement (such as a group by statement), then it can be determined that the front and the back stage are divided by that place.</p>
<p><strong>Word Count</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://..."</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"> </span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure>
<h3 id="31-stages-divided"><a class="markdownIt-Anchor" href="#31-stages-divided"></a> 3.1 stages divided</h3>
<p>the entire code, only one reduceByKey operator will shuffle, the front and back stages will be divided.<br />
* stage0, mainly to perform operations from textFile to map, and perform <strong>shuffle write operations</strong>.</p>
<h3 id="32-shuffle-write"><a class="markdownIt-Anchor" href="#32-shuffle-write"></a> 3.2 shuffle write</h3>
<p>The shuffle write operation can be simply understood as <strong>partitioning the data in the pairs RDD</strong>. In the data processed by each task, the same key will be written to the same disk file.<br />
* Stage1 is mainly to perform operations from reduceByKey to collect.</p>
<h3 id="33-shuffle-read"><a class="markdownIt-Anchor" href="#33-shuffle-read"></a> 3.3 shuffle read</h3>
<p>When <strong>each task of stage1</strong> starts to run, it will first perform shuffle read operation. The task that performs the shuffle read operation will pull those keys that <code>belong to the node where each task of stage 0 is located</code>, and then perform operations such as global aggregation or join on the same key. Here, the value of the key is accumulated.</p>
<p>After <code>stage1 executes the reduceByKey operator, it calculates the final wordCounts RDD</code>, and then executes the collect operator to pull all the data to the Driver for us to traverse and print out.</p>
<h2 id="4-data-skew-distribution-of-keys"><a class="markdownIt-Anchor" href="#4-data-skew-distribution-of-keys"></a> 4. data skew - distribution of keys</h2>
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th style="text-align:left">View the data distribution of keys that cause data skew</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td style="text-align:left">If the data skew caused by the group by and join statements in Spark SQL, then query the key distribution of the table used in SQL Happening.</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td style="text-align:left">If the data skew is caused by the shuffle operator on Spark RDD, you can view the key distribution in the Spark job, such as <code>RDD.countByKey()</code>.  Then, collect/take, see the distribution of the keys.</td>
</tr>
</tbody>
</table>
<blockquote>
<p>For example, Word Count</p>
<p>we can first sample 10% of the sample data for pairs, then use the countByKey operator to count the number of occurrences of each key, and finally traverse and print the number of occurrences of each key in the sample data on the client.</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th style="text-align:left">solutions of the data skew</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td style="text-align:left">Improve the parallelism of shuffle operations <br><br> 在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量<br><br>Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 <br><br> Experience： cannot completely solve the data skew，such as the amount of data a key is 1 million.</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td style="text-align:left">Two-stage aggregation (local aggregation + global aggregation) <br><br> <strong>disadvantages</strong>: only solve aggregate shuffle operations. If it is a shuffle operation of the <code>join</code> class, other solutions have to be used.</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td style="text-align:left">Convert reduce join to map join <br><br><strong>advantages</strong>: The effect is very good for data skew caused by the join operation, because shuffle and data skew will not happen at all. <br> <strong>disadvantages</strong>: only suitable for <code>a large table and a small table</code>. After all, we need to broadcast the small table, which <code>consumes more memory resources</code>. <br>The driver and each Executor will have a full amount of data of a small RDD in the memory. <br> If the RDD data we broadcast is relatively large, such as 10G or more, then memory overflow may occur. Therefore, it is not suitable for the situation where both are large tables.</td>
</tr>
</tbody>
</table>
<img src="/images/spark/spark-data-skew-reduce-by-key.png" width="" alt="Data skew only occurs during the shuffle process." />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> random </span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line"><span class="comment"># sc, random.randint(0,2) # 0 or 1 0r 2</span></span><br></pre></td></tr></table></figure>
<details>
<summary>WordCounts reduceByKey More Info...</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read data from text file and split each line into words</span></span><br><span class="line">input_file = sc.textFile(<span class="string">"/Users/blair/Desktop/input.txt"</span>, <span class="number">2</span>)</span><br><span class="line">words=input_file.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br><span class="line"><span class="comment"># words.collect() # ['China', 'Singapore', 'bbb', 'Singapore', 'hello', 'haha', 'hello', 'world'] </span></span><br><span class="line"><span class="comment"># words.first()</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># count the occurrence of each word</span></span><br><span class="line">wordCounts = words.map(<span class="keyword">lambda</span> word: (<span class="string">f'<span class="subst">&#123;random.randint(<span class="number">0</span>,<span class="number">2</span>)&#125;</span>_<span class="subst">&#123;word&#125;</span>'</span>, <span class="number">1</span>)).reduceByKey(add)</span><br><span class="line">wordCounts.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[('0_Singapore', 3),
 ('2_bbb', 1),
 ('0_hello', 1),
 ('2_haha', 1),
 ('0_world', 1),
 ('1_ShangHai', 1),
 ('0_China', 1),
 ('2_Singapore', 4),
 ('1_Singapore', 1),
 ('2_hello', 1)]
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words_recover = wordCounts.map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>][x[<span class="number">0</span>].find(<span class="string">'_'</span>)+<span class="number">1</span>:], x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">word_count_ret = words_recover.reduceByKey(add)</span><br><span class="line">word_count_ret.take(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
</details>
<pre><code>[('world', 1), ('ShangHai', 1), ('China', 1), ('Singapore', 8), ('bbb', 1)]
</code></pre>
<h2 id="5-convert-reduce-join-to-map-join"><a class="markdownIt-Anchor" href="#5-convert-reduce-join-to-map-join"></a> 5. Convert reduce join to map join</h2>
<img src="/images/spark/spark-data-skew-5-join.png" width="750" alt="the smaller RDD directly into the Driver memory through the collect operator, and then create a Broadcast variable" />
<p>code:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                Map&lt;Long, Row&gt; rdd1DataMap = <span class="keyword">new</span> HashMap&lt;Long, Row&gt;();</span><br><span class="line">                <span class="keyword">for</span>(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                String key = tuple._1;</span><br><span class="line">                String value = tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                Row rdd1Value = rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(key, <span class="keyword">new</span> Tuple2&lt;String, Row&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>方案实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p>
<p>Other Solution 1: Use Hive ETL to preprocess data<br />
Other Solution 2: Filter a few keys that cause skew</p>
</blockquote>
<h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/257917645" target="_blank" rel="noopener">Hive数仓建表该选用ORC还是Parquet，压缩选LZO还是Snappy？</a></li>
<li><a href="https://www.cnblogs.com/stillcoolme/p/10576563.html" target="_blank" rel="noopener">Spark实践 – 性能优化基础</a></li>
<li><a href="https://www.bilibili.com/video/BV11A411L7CK?p=184" target="_blank" rel="noopener">尚硅谷2021迎新版大数据Spark从入门到精通</a></li>
<li><a href="https://www.bilibili.com/video/BV1Hp4y1z7aZ?p=79" target="_blank" rel="noopener">尚硅谷大数据电商数仓V3.0版本教程（数据仓库项目开发实战）</a></li>
<li><a href="https://blog.csdn.net/sinat_36231857/article/details/88819553" target="_blank" rel="noopener">Spark Sql 与 MySql 使用 group by 的差别</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io" target="_blank" rel="noopener"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-spark-data-skew"><span class="toc-text"> 1. Spark Data Skew</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-the-principle-of-data-skew"><span class="toc-text"> 2. The principle of Data Skew</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-the-execution-of-a-task-slow"><span class="toc-text"> 3. The execution of a task slow</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-stages-divided"><span class="toc-text"> 3.1 stages divided</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-shuffle-write"><span class="toc-text"> 3.2 shuffle write</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-shuffle-read"><span class="toc-text"> 3.3 shuffle read</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-data-skew-distribution-of-keys"><span class="toc-text"> 4. data skew - distribution of keys</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-convert-reduce-join-to-map-join"><span class="toc-text"> 5. Convert reduce join to map join</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-text"> Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/spark/" rel="tag">spark</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2021/01/26/dataware/SQL-LC/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">SQL@Leetcode&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2021/01/27/spark/spark-summary-4-data-skew/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
