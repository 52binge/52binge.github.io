<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark Review Summary 2 - Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Review Summary 2">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;2021&#x2F;01&#x2F;08&#x2F;spark&#x2F;spark-summary-review-2&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-summary-logo-1.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;rdd-df-ds.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-rdd-split-task-partition.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-map-vs-mapPartitions.jpg">
<meta property="og:updated_time" content="2021-01-12T01:07:13.280Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-summary-logo-1.jpg">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<!-- jiangting add start... @2020.08.30 -->
<!-- <div id="menu" class="duration-main" style="background-color: #e7e7e7"> -->
<!--   <p class="links-p" href="/">Blair</p> -->
<!--   <address class="icons"> -->
<!--     <a href="https://github.com/blair101" class="icon-font icon linkedin" target="_blank"></a> -->
<!--     <a href="https://cn.linkedin.com/pub/tianyu-dai/a8/818/44a" class="icon-font icon linkedin" target="_blank"></a> -->
<!--   </address> -->
<!--   <div class="hr1"></div> -->
<!--   <nav> -->
<!--     <div> -->
<!--     <a class="home" href="/">Home</a></div> -->
<!--     <p class="links-p" href="/">Home</p> -->
<!--     <nav class="tag-ath"> -->
<!--       <a class="proj" href="/categories">Category</a> -->
<!--       <a class="authors" href="/about">About</a> -->
<!--     </nav> -->
<!--   </nav> -->
<!--   <div class="hr2"></div> -->
<!--     <p class="links-p">Links</p> -->
<!--       <address class="links"> -->
<!--       <a class="proj" href="/article/Create-MyWorld">Projects</a> -->
<!--       <a class="friend">Friends</a></address><div class="hr3"> -->
<!--   </div> -->
<!--   <p class="end"></p> -->
<!--   <div id="menu-links" class="duration-main" style="top: -400px; background-color: #f5f5f5"> -->
<!--     <address> -->
<!--       <li><a target="_blank" href="http://lm7.xxxxxxxx.jp">Lm7</a></li> -->
<!--       <li><a target="_blank" href="http://www.pixiv.net/member.php?id=4933015">Domik</a></li> -->
<!--       <li><a target="_blank" href="http://hana-ui.moe">hana-ui</a></li> -->
<!--       <li><a target="_blank" href="http://fil.dtysky.moe">F-I-L</a></li> -->
<!--       <li><a target="_blank" href="http://paradise.dtysky.moe">Paradise</a></li> -->
<!--       <li><a target="_blank" href="http://moe-notes.dtysky.moe">MoeNotes</a></li> -->
<!--       <li><a target="_blank" href="http://kanata.dtysky.moe">Kanata</a></li> -->
<!--       <li><a target="_blank" href="http://blog.nekohand.moe">Nekohand</a></li> -->
<!--       <li><a target="_blank" href="http://www.jerryfu.net">JerryFu</a></li> -->
<!--       <li><a target="_blank" href="http://kawabangga.com">南史</a></li> -->
<!--       <li><a>Hide Links</a></li> -->
<!--       <li><a></a></li> -->
<!--     </address> -->
<!--   </div> -->
<!-- </div> -->
<!-- jiangting add end !  @2020.08.30 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-spark/spark-summary-review-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark Review Summary 2
      <small class=article-detail-date-index>&nbsp; 2021-01-08</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2021/01/08/spark/spark-summary-review-2/" class="article-date">
  <time datetime="2021-01-07T23:07:21.000Z" itemprop="datePublished">2021-01-08</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2021/01/08/spark/spark-summary-review-2/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/spark/spark-summary-logo-1.jpg" width="500" alt="" /></p>
<a id="more"></a>
<blockquote>
<p><a href="https://www.jianshu.com/p/b70fe63a77a8" target="_blank" rel="noopener">good - Spark会把数据都载入到内存么？</a></p>
</blockquote>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th>Title</th>
<th>Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.</td>
<td>kaike - sparkSQL底层实现原理</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">0.</td>
<td><a href="/2019/08/25/spark/spark-aura-9.1-SparkSql/">SparkSql - 结构化数据处理 (上)</a></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">0.</td>
<td><a href="https://www.cnblogs.com/yanghaolie/p/6909640.html" target="_blank" rel="noopener">Spark Container Executor task之间的关系</a></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">0.</td>
<td><a href="https://mp.weixin.qq.com/s?__biz=Mzg3NjIyNjQwMg==&amp;mid=2247494393&amp;idx=1&amp;sn=4869549081ca75250b6dfdc84af43418&amp;chksm=cf37d8f4f84051e2d8fd073f21d18fd97f9f9ec95317ccde679fffd0349e8fee0e8bf3a8c9e6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Spark 漫画 全面解释Spark企业调优点</a></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">&nbsp;</td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">1.</td>
<td>RDD 属性？  5大属性</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td>算子分为哪几类(RDD支持哪几种类型的操作) &nbsp;&nbsp;&nbsp;&nbsp; 1. Transformation （lazy模式）2. Action</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td>创建rdd的几种方式</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td>spark运行流程</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">5.</td>
<td>Spark中coalesce与repartition的区别</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">6.</td>
<td>sortBy 和 sortByKey的区别</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">7.</td>
<td>map和mapPartitions的区别</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">8.</td>
<td>数据存入Redis  优先使用map mapPartitions  foreach  foreachPartions ? <br> def f(x): print(x) <br> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">9.</td>
<td>reduceByKey和groupBykey的区别</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">10.</td>
<td><a href="/2021/01/08/spark/spark-interview/#10-cache%E5%92%8Ccheckpoint%E7%9A%84%E6%AF%94%E8%BE%83">cache和checkPoint的比较</a> : 都是做 RDD 持久化的</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">11.</td>
<td>简述map和flatMap的区别和应用场景 <br> &nbsp;&nbsp;&nbsp;&nbsp; map是对每个元素进行操做，flatmap是对每个元素操做后并压平.</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">12.</td>
<td>计算曝光数和点击数</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">13.</td>
<td>分别列出几个常用的transformation和action算子</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">17.</td>
<td>Spark应用执行有哪些模式，其中哪几种是集群模式</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">18.</td>
<td><a href="https://www.cnblogs.com/Lee-yl/p/9777857.html" target="_blank" rel="noopener">请说明spark中广播变量的用途 ?</a>（1）broadcast (不能改)（2）accumulator, sc.accumulator(0) <br><br> 使用广播变量，每个 Executor 的内存中，只驻留一份变量副本，而不是对 每个 task 都传输一次大变量，省了很多的网络传输， 对性能提升具有很大帮助， 而且会通过高效的广播算法来减少传输代价.<br><br>mapper = <code>{&#39;dog&#39;:1, &#39;cat&#39;:12}</code>, bc=sc.broadcast(mapper), bc: <code>{&#39;dog&#39;:1, &#39;cat&#39;:2}</code> <br><br> mapper = <code>{&#39;pig&#39;: 3}</code> , bc.unpersist(), sc.broadcast(mapper).value <br><br> <a href="https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#broadcast-variables" target="_blank" rel="noopener">2.2 rdd-programming-guide.html#broadcast-variables</a></td>
<td><br><br><br><br>❎</td>
</tr>
<tr>
<td style="text-align:center">20.</td>
<td><a href="https://mp.weixin.qq.com/s/UPtxoGWZIvPE0KHoOs_tBw" target="_blank" rel="noopener">Spark高频考点</a>: 写出你用过的spark中的算子，其中哪些会产生shuffle过程 <br><br>1. reduceBykey 2. groupByKey 3. …ByKey</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">21.</td>
<td><a href="https://www.cnblogs.com/qingyunzong/p/8899715.html" target="_blank" rel="noopener">good - Spark学习之路 （三）Spark之RDD</a>  扎心了老铁</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">22.</td>
<td><a href="https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#sql" target="_blank" rel="noopener">请写出创建Dateset的几种方式</a> <br> 1. 常用的方式通过sparksession读取外部文件或者数据生成dataset <br> 2. 通过调用createDataFrame生成Dataset <br><br> df.select(<code>&quot;name&quot;</code>).show()<br>df.select(df[<code>&#39;name&#39;</code>], df[<code>&#39;age&#39;</code>] + 1).show() <br> df.filter(df[<code>&#39;age&#39;</code>] &gt; 21).show() <br> df.groupBy(<code>&quot;age&quot;</code>).count().show() <br><br> df.createOrReplaceTempView(<code>&quot;people&quot;</code>) <br> sqlDF = spark.sql(<code>&quot;SELECT * FROM people&quot;</code>) <br> sqlDF.show() <br><br> teenNames = <code>teenagers.rdd</code>.map(lambda p: “Name: “ + p.name).collect()</td>
<td><br><br><br><br>❎</td>
</tr>
<tr>
<td style="text-align:center">23.</td>
<td>描述一下 RDD，DataFrame，DataSet 的区别？ <br><br> DataSet 结合了 RDD 和 DataFrame 的优势，并带来的一个新的概念 Encoder。<br> 当序列化数据时，Encoder 产生字节码与 off-heap 进行交互，可以达到按需访问数据的效果，而不用反序列化整个对象。Spark 尚未提供自定义 Encoder 的 API，可是将来会加入 <br> <img src="/images/spark/rdd-df-ds.jpg" width="700" alt="" /> <br><br> <a href="https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#datasets-and-dataframes" target="_blank" rel="noopener">Apache spark DataFrame &amp; Dataset</a></td>
<td><br><br><br><br><br><br>❎</td>
</tr>
<tr>
<td style="text-align:center">24.</td>
<td>描述一下Spark中stage是如何划分的？描述一下shuffle的概念</td>
<td>✔️❎</td>
</tr>
<tr>
<td style="text-align:center">25.</td>
<td><a href="https://www.cnblogs.com/bigdata1024/p/12116621.html" target="_blank" rel="noopener">Spark 在yarn上运行需要做哪些关键的配置工作？</a> <br> <a href="https://blog.csdn.net/power0405hf/article/details/50457960" target="_blank" rel="noopener">如何kill -个Spark在yarn运行中Application</a>: <code>yarn application -kill &lt;appId&gt;</code> <br> 但是这样会导致端口在一段时间（24小时）内被占用</td>
<td><br>❎</td>
</tr>
<tr>
<td style="text-align:center">26.</td>
<td>通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">27.</td>
<td><a href="https://blog.csdn.net/qq_31598113/article/details/70832701" target="_blank" rel="noopener">RDD中的数据在哪？</a><br> 不可变的意思是RDD中的每个分区数据是 <strong>only-read</strong> <br> RDD要做逻辑分区（这里的分区类似hadoop中的逻辑切片split），每个分区可单独在集群节点计算</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">28.</td>
<td>如果对RDD进行cache操作后，数据在哪里？ <br><br> 1. 执行cache算子时数据会被加载到各个Executor进程的内存. <br> 2. 第二次使用 会直接从内存中读取而不会区磁盘.</td>
<td><br>❎</td>
</tr>
<tr>
<td style="text-align:center">29.</td>
<td>Spark中Partition的数量由什么决定?  答： 和MR一样，但是Spark默认最少有两个分区.</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">30.</td>
<td><del>Spark判断Shuffle的依据?</del></td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">35.</td>
<td><del>Sparkcontext的作用?</del></td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">36.</td>
<td><strong>Spark SQL</strong> 在 <strong>Spark Core</strong> 的基础上针对结构化数据处理进行很多优化和改进.</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">37.</td>
<td>简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">38.</td>
<td>数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">39.</td>
<td>简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作.</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">41.</td>
<td>有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条</td>
</tr>
<tr>
<td style="text-align:center">42.</td>
<td>现有一文件，格式如下，请用spark统计每个单词出现的次数</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">45.</td>
<td>特别大的数据，怎么发送到excutor中？ Answ： broadcast</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">46.</td>
<td>spark调优都做过哪些方面？ 要非常具体的场景</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">47.</td>
<td>spark任务为什么会被yarn kill掉？</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">48.</td>
<td>Spark on Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">49.</td>
<td><a href="https://blog.csdn.net/ljp812184246/article/details/53897613" target="_blank" rel="noopener">spark中的cache() persist() checkpoint()之间的区别</a> <br><br> 1. checkpoint 的 RDD 会被计算两次 <br> 2. rdd.persist(StorageLevel.DISK_ONLY), partition 由 blockManager 管理, blockManager stop, cache 到磁盘上 RDD 也会被清空 <br>  3. checkpoint 将 RDD 持久化到 HDFS 或本地文件夹, 可以被下一个 driver program 使用.</td>
<td><br><br> ❎</td>
</tr>
<tr>
<td style="text-align:center">50.</td>
<td><a href="https://blog.csdn.net/m0_37294838/article/details/91407028" target="_blank" rel="noopener">spark算子调优四：repartition解决SparkSQL低并行度问题</a> <br><br> 你自己通过spark.default.parallelism参数指定的并行度，只会在没有spark sql的stage中生效 <br><br> hive表，对应了一个hdfs文件，有20个block;你自己设置了spark.default.parallelish参数为100；你的第一个stage的并行度，是不受你设置的参数控制的，就只有20task</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">51.</td>
<td><a href="https://www.cnblogs.com/dflmg/p/10430181.html" target="_blank" rel="noopener">very good 多弗朗明哥 -【大数据】Spark性能优化和故障处理</a></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line">sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment"># +----+-------+</span></span><br><span class="line"><span class="comment"># | age|   name|</span></span><br><span class="line"><span class="comment"># +----+-------+</span></span><br><span class="line"><span class="comment"># |null|Michael|</span></span><br><span class="line"><span class="comment"># |  30|   Andy|</span></span><br><span class="line"><span class="comment"># |  19| Justin|</span></span><br><span class="line"><span class="comment"># +----+-------+</span></span><br></pre></td></tr></table></figure>
<h2 id="1-RDD-属性"><a href="#1-RDD-属性" class="headerlink" title="1. RDD 属性"></a>1. RDD 属性</h2><blockquote>
<ul>
<li>A list of partitions</li>
<li>A function for computing each split</li>
<li>A list of dependencies on other RDDs</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li>
<li>Optionally, a list of preferred locations to compute each split on (block locations for an HDFS file) </li>
</ul>
</blockquote>
<ul>
<li><a href="https://blog.csdn.net/qq_22473611/article/details/107822168" target="_blank" rel="noopener">very good Spark分区 partition 详解</a></li>
</ul>
<p><img src="/images/spark/spark-rdd-split-task-partition.png" width="800" alt="申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task" /></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th>Title</th>
<th>Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td><strong>一组分片（Partition）</strong>，即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">5.</td>
<td>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置</td>
<td>❎</td>
</tr>
</tbody>
</table>
</div>
<p>尽量保证每轮Stage里每个task处理的数据量&gt;128M</p>
<h2 id="2-RDD支持的操作"><a href="#2-RDD支持的操作" class="headerlink" title="2. RDD支持的操作"></a>2. RDD支持的操作</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th>Title</th>
<th>Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td><strong>Transformation</strong>： 现有的RDD通过转换生成一个新的RDD。lazy模式，延迟执行。<br><br> map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，union,join, coalesce.</td>
<td><br>❎</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td><strong>Action</strong>： 在RDD上运行计算，并返回结果给驱动程序(Driver)或写入文件系统. <br><br> reduce，collect，count，first，take，countByKey 及 foreach 等等.</td>
<td><br>❎</td>
</tr>
<tr>
<td style="text-align:center">说明</td>
<td>collect 该方法把数据收集到driver端 Array数组类型, transformation只有遇到action才能被执行.</td>
<td>❎</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>当执行action之后，数据类型不再是rdd了，数据就会存储到指定文件系统中，或者直接打印结 果或者收集起来.</p>
</blockquote>
<h2 id="3-创建rdd的几种方式"><a href="#3-创建rdd的几种方式" class="headerlink" title="3. 创建rdd的几种方式"></a>3. 创建rdd的几种方式</h2><p>1.集合并行化创建(有数据)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(arr)</span><br><span class="line"><span class="keyword">val</span> rdd =sc.makeRDD(arr)</span><br></pre></td></tr></table></figure>
<p>2.读取外部文件系统，如hdfs，或者读取本地文件(最常用的方式)(没数据)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://hdp-01:9000/words.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取本地文件</span></span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(“file:<span class="comment">///root/words.txt”)</span></span><br></pre></td></tr></table></figure>
<p>3.从父RDD转换成新的子RDD</p>
<blockquote>
<p>调用Transformation类的方法，生成新的RDD</p>
</blockquote>
<h2 id="5-coalesce-repartition区别"><a href="#5-coalesce-repartition区别" class="headerlink" title="5. coalesce, repartition区别"></a>5. coalesce, repartition区别</h2><blockquote>
<ol>
<li>repartition 底层调用的就是 coalesce 方法：<code>coalesce(numPartitions, shuffle = true)</code></li>
<li>repartition 一定会发生 shuffle，coalesce 根据传入的参数来判断是否发生 shuffle</li>
</ol>
<p>一般情况下<strong>增大 rdd 的 partition 数量</strong>使用 repartition，减少 partition 数量时使用coalesce</p>
</blockquote>
<h2 id="6-sortBy-sortByKey区别"><a href="#6-sortBy-sortByKey区别" class="headerlink" title="6. sortBy / sortByKey区别"></a>6. sortBy / sortByKey区别</h2><blockquote>
<p>sortBy既可以作用于RDD[K] ，还可以作用于RDD[(k,v)]</p>
<p>sortByKey  只能作用于 RDD[K,V] 类型上</p>
</blockquote>
<p>sortBy : <code>sortBy(lambda x:x[2],ascending = False)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#任务：有一批学生信息表格，包括name,age,score, 找出score排名前3的学生, score相同可以任取</span></span><br><span class="line">students = [(<span class="string">"LiLei"</span>,<span class="number">18</span>,<span class="number">87</span>),(<span class="string">"HanMeiMei"</span>,<span class="number">16</span>,<span class="number">77</span>),(<span class="string">"DaChui"</span>,<span class="number">16</span>,<span class="number">66</span>),(<span class="string">"Jim"</span>,<span class="number">18</span>,<span class="number">77</span>),(<span class="string">"RuHua"</span>,<span class="number">18</span>,<span class="number">50</span>)]</span><br><span class="line">rdd_students = sc.parallelize(students)</span><br><span class="line">rdd_sorted = rdd_students.sortBy(<span class="keyword">lambda</span> x:x[<span class="number">2</span>],ascending = <span class="literal">False</span>)</span><br><span class="line"><span class="comment"># [(‘LiLei’, 18, 87), (‘HanMeiMei’, 16, 77), (‘Jim’, 18, 77)]</span></span><br></pre></td></tr></table></figure>
<p>sortByKey : <code>sortByKey().map(lambda x:x[0])</code> </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#任务：按从小到大排序并返回序号, 大小相同的序号可以不同</span></span><br><span class="line">data = [<span class="number">1</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">18</span>,<span class="number">34</span>,<span class="number">9</span>,<span class="number">0</span>,<span class="number">12</span>,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">rdd_data = sc.parallelize(data)</span><br><span class="line"></span><br><span class="line">rdd_sorted = rdd_data.map(<span class="keyword">lambda</span> x:(x,<span class="number">1</span>)).sortByKey().map(<span class="keyword">lambda</span> x:x[<span class="number">0</span>]) </span><br><span class="line"><span class="comment"># [0, 1, 3, 5, 7, 8, 8, 9, 12, 18, 34]</span></span><br></pre></td></tr></table></figure>
<h2 id="7-map和mapPartitions的区别"><a href="#7-map和mapPartitions的区别" class="headerlink" title="7. map和mapPartitions的区别"></a>7. map和mapPartitions的区别</h2><p><img src="/images/spark/spark-map-vs-mapPartitions.jpg" width="500" alt="" /></p>
<h2 id="8-数据存入Redis-优先使用什么算子"><a href="#8-数据存入Redis-优先使用什么算子" class="headerlink" title="8. 数据存入Redis  优先使用什么算子?"></a>8. 数据存入Redis  优先使用什么算子?</h2><p><code>foreachPartions</code></p>
<ol>
<li>map </li>
<li>mapPartitions  </li>
<li>foreach  </li>
<li>foreachPartions</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkFiles</span><br><span class="line">path = os.path.join(tempdir, <span class="string">"test.txt"</span>)</span><br><span class="line"><span class="keyword">with</span> open(path, <span class="string">"w"</span>) <span class="keyword">as</span> testFile:</span><br><span class="line">   _ = testFile.write(<span class="string">"100"</span>)</span><br><span class="line">sc.addFile(path)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(iterator)</span>:</span></span><br><span class="line">   <span class="keyword">with</span> open(SparkFiles.get(<span class="string">"test.txt"</span>)) <span class="keyword">as</span> testFile:</span><br><span class="line">       fileVal = int(testFile.readline())</span><br><span class="line">       <span class="keyword">return</span> [x * fileVal <span class="keyword">for</span> x <span class="keyword">in</span> iterator]</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).mapPartitions(func).collect()</span><br><span class="line">[<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>, <span class="number">400</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用 foreachPartions</p>
<ol>
<li>map mapPartitions   是转换类的算子， 有返回值</li>
<li>写mysql,redis 的链接</li>
</ol>
</blockquote>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=foreach" target="_blank" rel="noopener">pyspark.html</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># foreach(f)[source]</span></span><br><span class="line"><span class="comment"># Applies a function to all elements of this RDD.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> print(x)</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).foreach(f)</span><br><span class="line">foreachPartition(f)[source]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Applies a function to each partition of this RDD.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(iterator)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> iterator:</span><br><span class="line">         print(x)</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).foreachPartition(f)</span><br></pre></td></tr></table></figure>
<h2 id="9-reduceByKey和groupBykey区别"><a href="#9-reduceByKey和groupBykey区别" class="headerlink" title="9. reduceByKey和groupBykey区别"></a>9. reduceByKey和groupBykey区别</h2><blockquote>
<p>reduceByKey会传一个聚合函数， 至关于  groupByKey + mapValues</p>
<p>reduceByKey 会有一个分区内聚合，而groupByKey没有  最核心的区别  </p>
<p>结论：reduceByKey有分区内聚合，更高效，优先选择使用reduceByKey</p>
</blockquote>
<h2 id="10-cache和checkPoint的比较"><a href="#10-cache和checkPoint的比较" class="headerlink" title="10. cache和checkPoint的比较"></a>10. cache和checkPoint的比较</h2><p>都是作 RDD 持久化的</p>
<blockquote>
<p>1.缓存，是在触发action以后，把数据写入到内存或者磁盘中。不会截断血缘关系</p>
<p>（设置缓存级别为memory_only：内存不足，只会部分缓存或者没有缓存，缓存会丢失,memory_and_disk :内存不足，会使用磁盘）</p>
<p>2.checkpoint 也是在触发action以后，执行任务。单独再启动一个job，负责写入数据到hdfs中。（把rdd中的数据，以二进制文本的方式写入到hdfs中，有几个分区，就有几个二进制文件）</p>
<p>3.某一个RDD被checkpoint以后，他的父依赖关系会被删除，血缘关系被截断，该RDD转换成了CheckPointRDD，之后再对该rdd的全部操做，都是从hdfs中的checkpoint的具体目录来读取数据。缓存以后，rdd的依赖关系仍是存在的。</p>
<p><strong>checkpoint</strong></p>
<ol>
<li>sc.setCheckpointDir(“/Users/xulijie/Documents/data/checkpoint”)</li>
<li>val pairs = sc.parallelize(data, 3)</li>
<li>pairs.checkpoint</li>
</ol>
</blockquote>
<p><strong>Cache</strong></p>
<blockquote>
<p>Cache(): 运算时间很长或运算量太大才能得到的 RDD，computing chain 过长或依赖其他 RDD 很多的 RDD.</p>
<p>df2.cache()<br>rdd2.cache()</p>
</blockquote>
<p>cache 机制是每计算出一个要 cache 的 partition 就直接将其 cache 到内存了。但 checkpoint 没有使用这种第一次计算得到就存储的方法，而是等到 job 结束后另外启动专门的 job 去完成 checkpoint 。 也就是说需要 checkpoint 的 RDD 会被计算两次。因此，在使用 rdd.checkpoint() 的时候，建议加上 rdd.cache()， 这样第二次运行的 job 就不用再去计算该 rdd 了，直接读取 cache 写磁盘。</p>
<h2 id="Hadoop-vs-Spark-区别"><a href="#Hadoop-vs-Spark-区别" class="headerlink" title="Hadoop vs Spark 区别"></a>Hadoop vs Spark 区别</h2><p>Spark比MapReduce运行速度快的原因主要有以下几点：</p>
<blockquote>
<ol>
<li>task启动时间比较快，Spark是fork出线程；而MR是启动一个新的进程；</li>
<li>更快的shuffles，Spark只有在shuffle的时候才会将数据放在磁盘，而MR却不是。</li>
<li>更快的工作流：典型的MR工作流是由很多MR作业组成的，他们之间的数据交互需要把数据持久化到磁盘才可以；而<strong>Spark支持DAG以及pipelining，在没有遇到shuffle完全可以不把数据缓存到磁盘。<br>缓存</strong>：虽然目前HDFS也支持缓存，但是一般来说，Spark的缓存功能更加高效，特别是在SparkSQL中，我们可以将数据以列式的形式储存在内存中。</li>
<li>所有的这些原因才使得Spark相比Hadoop拥有更好的性能表现；在比较短的作业确实能快上100倍，但是在真实的生产环境下，一般只会快 2.5x ~ 3x！</li>
</ol>
<p>JVM 的优化: Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，基于进程的操作。而 Spark 每次 MapReduce 操作是基于线程的，<strong><code>Spark 只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的</code></strong>。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多。</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.cnblogs.com/dflmg/p/10430181.html" target="_blank" rel="noopener">very good 多弗朗明哥 -【大数据】Spark性能优化和故障处理</a></li>
<li><a href="https://blog.csdn.net/qq_22473611/article/details/107822168" target="_blank" rel="noopener">good - Spark分区 partition 详解</a></li>
<li><a href="https://mp.weixin.qq.com/s/pwyus1xfX7QAz5MtecveZw" target="_blank" rel="noopener">good - 2020大数据/数仓/数开面试题真题总结(附答案)</a></li>
<li><a href="https://www.shangmayuan.com/a/b5776271f2194b89b6ea2a14.html" target="_blank" rel="noopener">2020大数据/数仓/数开面试题真题总结(附答案)</a></li>
<li><a href="https://blog.csdn.net/ljp812184246/article/details/53897613" target="_blank" rel="noopener">spark中的cache() persist() checkpoint()之间的区别</a></li>
<li><a href="https://www.jianshu.com/p/cbff05e3f125" target="_blank" rel="noopener">airflow的使用方法</a></li>
</ul>
<p>other:</p>
<ul>
<li><a href="https://juejin.cn/post/6844904020075610125" target="_blank" rel="noopener">剖析Spark数据分区之Hadoop分片</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io" target="_blank" rel="noopener"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark"><span class="toc-text">Spark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-RDD-属性"><span class="toc-text">1. RDD 属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-RDD支持的操作"><span class="toc-text">2. RDD支持的操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-创建rdd的几种方式"><span class="toc-text">3. 创建rdd的几种方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-coalesce-repartition区别"><span class="toc-text">5. coalesce, repartition区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-sortBy-sortByKey区别"><span class="toc-text">6. sortBy / sortByKey区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-map和mapPartitions的区别"><span class="toc-text">7. map和mapPartitions的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-数据存入Redis-优先使用什么算子"><span class="toc-text">8. 数据存入Redis  优先使用什么算子?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-reduceByKey和groupBykey区别"><span class="toc-text">9. reduceByKey和groupBykey区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-cache和checkPoint的比较"><span class="toc-text">10. cache和checkPoint的比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop-vs-Spark-区别"><span class="toc-text">Hadoop vs Spark 区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/spark/" rel="tag">spark</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/01/09/dataware/dataware-summary-review-2/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          DataWare Review Summary 2
        
      </div>
    </a>
  
  
    <a href="/2021/01/06/spark/python-spark-practice/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Spark Practice&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2021/01/08/spark/spark-summary-review-2/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
