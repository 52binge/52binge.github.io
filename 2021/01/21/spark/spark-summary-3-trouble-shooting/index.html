<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark - troubleshooting - Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark - troubleshooting">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;2021&#x2F;01&#x2F;21&#x2F;spark&#x2F;spark-summary-3-trouble-shooting&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-summary-logo-1.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-rdd-split-task-partition.png">
<meta property="og:updated_time" content="2021-04-08T10:20:01.447Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;spark-summary-logo-1.jpg">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<!-- jiangting add start... @2020.08.30 -->
<!-- <div id="menu" class="duration-main" style="background-color: #e7e7e7"> -->
<!--   <p class="links-p" href="/">Blair</p> -->
<!--   <address class="icons"> -->
<!--     <a href="https://github.com/blair101" class="icon-font icon linkedin" target="_blank"></a> -->
<!--     <a href="https://cn.linkedin.com/pub/tianyu-dai/a8/818/44a" class="icon-font icon linkedin" target="_blank"></a> -->
<!--   </address> -->
<!--   <div class="hr1"></div> -->
<!--   <nav> -->
<!--     <div> -->
<!--     <a class="home" href="/">Home</a></div> -->
<!--     <p class="links-p" href="/">Home</p> -->
<!--     <nav class="tag-ath"> -->
<!--       <a class="proj" href="/categories">Category</a> -->
<!--       <a class="authors" href="/about">About</a> -->
<!--     </nav> -->
<!--   </nav> -->
<!--   <div class="hr2"></div> -->
<!--     <p class="links-p">Links</p> -->
<!--       <address class="links"> -->
<!--       <a class="proj" href="/article/Create-MyWorld">Projects</a> -->
<!--       <a class="friend">Friends</a></address><div class="hr3"> -->
<!--   </div> -->
<!--   <p class="end"></p> -->
<!--   <div id="menu-links" class="duration-main" style="top: -400px; background-color: #f5f5f5"> -->
<!--     <address> -->
<!--       <li><a target="_blank" href="http://lm7.xxxxxxxx.jp">Lm7</a></li> -->
<!--       <li><a target="_blank" href="http://www.pixiv.net/member.php?id=4933015">Domik</a></li> -->
<!--       <li><a target="_blank" href="http://hana-ui.moe">hana-ui</a></li> -->
<!--       <li><a target="_blank" href="http://fil.dtysky.moe">F-I-L</a></li> -->
<!--       <li><a target="_blank" href="http://paradise.dtysky.moe">Paradise</a></li> -->
<!--       <li><a target="_blank" href="http://moe-notes.dtysky.moe">MoeNotes</a></li> -->
<!--       <li><a target="_blank" href="http://kanata.dtysky.moe">Kanata</a></li> -->
<!--       <li><a target="_blank" href="http://blog.nekohand.moe">Nekohand</a></li> -->
<!--       <li><a target="_blank" href="http://www.jerryfu.net">JerryFu</a></li> -->
<!--       <li><a target="_blank" href="http://kawabangga.com">南史</a></li> -->
<!--       <li><a>Hide Links</a></li> -->
<!--       <li><a></a></li> -->
<!--     </address> -->
<!--   </div> -->
<!-- </div> -->
<!-- jiangting add end !  @2020.08.30 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-spark/spark-summary-3-trouble-shooting" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark - troubleshooting
      <small class=article-detail-date-index>&nbsp; 2021-01-21</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2021/01/21/spark/spark-summary-3-trouble-shooting/" class="article-date">
  <time datetime="2021-01-20T23:07:21.000Z" itemprop="datePublished">2021-01-21</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2021/01/21/spark/spark-summary-3-trouble-shooting/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <img src="/images/spark/spark-summary-logo-1.jpg" width="500" alt="" />
<a id="more"></a>
<h2 id="spark"><a class="markdownIt-Anchor" href="#spark"></a> Spark</h2>
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th>Title</th>
<th>Desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td>coalesce</td>
<td>无论是在RDD中还是DataSet，默认情况下coalesce不会产生shuffle，此时通过coalesce创建的RDD分区数小于等于父RDD的分区数。</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td>repartition</td>
<td><strong>1）增加分区数</strong> <br>- repartition触发shuffle，shuffle的情况下可以增加分区数.<br>- coalesce默认不触发shuffle，即调用该算子增加分区数，实际情况是分区数仍是当前的分区数.</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td>union</td>
<td>val rdd4 = rdd1.union(rdd3) - res: Array[Int] = Array(1,2,3,4,5,6,7,8,9,12,14,16,18) <br> 多数情况: 通过union生成的RDD的分区数为父RDD的分区数之和</td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td>Join</td>
<td>join(otherDataset, [numTasks])是连接操作，将输入数据集(K,V)和另外一个数据集(K,W)进行Join， 得到(K, (V,W))；该操作是对于相同K的V和W集合进行笛卡尔积 操作，也即V和W的所有组合 <br><br>val rdd5 = rdd0.join(rdd0)<br> res3: Array[(Char,(Int, Int))] = Array((d,(9,8)), (c,(6,6)), (c,(6,7))) <br><br>rdd 算子： leftOuterJoin, fullOuterJoin, … <br><a href="https://blog.csdn.net/zhousishuo/article/details/73292428" target="_blank" rel="noopener">spark sql 之join等函数用法</a></td>
</tr>
<tr>
<td style="text-align:center">5.</td>
<td>cogroup</td>
<td>cogroup(otherDataset, [numTasks])是将输入数据集(K, V)和另外一个数据集(K, W)进行cogroup，得到一个格式为(K, Seq[V], Seq[W])的数据集<br><br> val rdd6 = rdd0.cogroup(rdd0)<br>res: Array[(Int, (Iterable[Int], Iterable[Int]))] = Array((1,(ArrayBuffer(1, 2, 3),ArrayBuffer(1, 2, 3))), (2,(ArrayBuffer(1, 2, 3),ArrayBuffer(1, 2, 3))))<br><br> <a href="https://blog.csdn.net/baolibin528/article/details/50319545?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;dist_request_id=0edc445c-b5ab-4005-b8cb-dafcf3725516&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control" target="_blank" rel="noopener">spark的union和join操作演示</a></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>No.</th>
<th>Title Author</th>
<th>Link &amp; Solutions</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.</td>
<td>GROUPING SETS</td>
<td>spark.sql.files.maxPartitionBytes 默认128M, 单个分区读取的最大文件大小 <br>（对于大部分的Parquet压缩表来说,注意压缩要可分割lzo，这个默认设置其实会导致性能问题）<br>可以通过设置spark.sql.files.maxPartitionBytes 来分割每个task 的输入<br><br>在Hadoop里，任务的并发默认是以hdfs block为单位的，而Spark里多了一种选择，即以RowGroup为基本单位: spark 处理parquet 文件时，一个row group 只能由一个task来处理<br><br>row group是需要调优的spark参数,重要一点,就是控制任务的并发度:<br>set parquet.block.size=16M<br>set spark.sql.files.maxPartitionBytes=16M</td>
</tr>
<tr>
<td>1.</td>
<td>较多的 DataFrame join 操作时</td>
<td>调大此参数：spark.sql.autoBroadcastJoinThreshold，默认10M，可设置为 100M</td>
</tr>
<tr>
<td>2.</td>
<td>华为开发者<br>SparkCore<br><br>知乎大数据<br>SparkSQL</td>
<td><a href="https://support-it.huawei.com/docs/zh-cn/fusioninsight-all/developer_guide/zh-cn_topic_0171822910.html" target="_blank" rel="noopener">开发者指南 &gt; 组件成功案例 &gt; Spark &gt; 案例10：Spark Core调优 &gt; 经验总结</a> <br><br><a href="https://zhuanlan.zhihu.com/p/148758337" target="_blank" rel="noopener">Spark基础：Spark SQL调优</a> <br><br> <strong>1. Cache 缓存</strong> <br>  1.1 spark.catalog.cacheTable(“t”) 或 df.cache() <br>             Spark SQL会把需要的列压缩后缓存，避免使用和GC的压力<br>  1.2 spark.sql.inMemoryColumnarStorage.compressed 默认true <br>   1.3 spark.sql.inMemoryColumnarStorage.batchSize 默认10000 <br>             控制列缓存时的数量，避免OOM风险。<br> 引申要点： 行式存储 &amp; 列式存储 优缺点 <br> <strong>2. 其他配置</strong> <br>  2.1 spark.sql.autoBroadcastJoinThreshold <br>  2.2 spark.sql.shuffle.partitions 默认200，配置join和agg的时候的分区数 <br>  2.3 spark.sql.broadcastTimeout 默认300秒，广播join时广播等待的时间 <br>  2.4 spark.sql.files.maxPartitionBytes 默认128MB，单个分区读取的最大文件大小<br>  2.5 spark.sql.files.openCostInBytes <br>parquet.block.size<br><strong>3. 广播 hash join - BHJ</strong> <br>   3.1 当系统 spark.sql.autoBroadcastJoinThreshold 判断满足条件，会自动使用BHJ <br><br><a href="https://support-it.huawei.com/docs/zh-cn/fusioninsight-all/developer_guide/zh-cn_topic_0171822912.html" target="_blank" rel="noopener">华为云Stack全景图 &gt; 开发者指南 &gt; SQL和DataFrame调优 &gt; Spark SQL join优化</a> <br><br> <details><summary>spark不会</summary> 注意spark不会确保每次选择广播表都是正确的，因为有的场景比如 full outer join 是不支持BHJ的。手动指定广播: broadcast(spark.table(“src”)).join(spark.table(“records”), “key”).show() </details></td>
</tr>
<tr>
<td><code>开发小知识</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>0.</td>
<td>NULL, AVG/NOT IN</td>
<td>select avg(amount) as a_mount from orders <br> amount (150, 150, null) avg = 150 不是 100 <br>select * from stores where tag not in (&quot;&quot;)</td>
</tr>
<tr>
<td>1.</td>
<td>NVL(expr1,expr2)</td>
<td>NVL(expr1,expr2) NVL(UnitsOnOrder,0) <br> other simlar: IFNULL(UnitsOnOrder, 0) / coalesce(null, “”) <br> NULLIF(exp1,expr2)函数的作用是如果exp1和exp2相等则返回空(NULL)</td>
</tr>
<tr>
<td>2.</td>
<td>IF( expr1 , expr2 , expr3 )</td>
<td>expr1 的值为 TRUE，则返回值为 expr2 <br> expr1 的值为FALSE，则返回值为 expr3</td>
</tr>
<tr>
<td>3.</td>
<td>IFNULL( expr1 , expr2 )</td>
<td>if expr1 not null, return  expr1</td>
</tr>
<tr>
<td><code>常见问题</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>3.</td>
<td><br>定位性能问题对应的sql</td>
<td>1. spark driver log 看 执行慢的stage（99%） <br>2. spark ui 上看 该stage 的task 执行完成比率<br>3. spark ui 上看 该stage 对应的 continer id 和 所属job<br>4. spark ui 上看 sql 的执行计划 和 执行计划图，最终定位到是哪段sql</td>
</tr>
<tr>
<td>4.</td>
<td>一道sql的题，一张表，用户id和登录日期，查找连续两天登陆的用户</td>
<td>left join tb_log b on a.uid = b.uid on a.uid = b.uid</td>
</tr>
<tr>
<td>5.</td>
<td>写sql。求一个省份下的uv最高的城市 主要考察窗口函数</td>
<td>select province,city,row_nnumber()over(partition by province order by uv desc ) rank</td>
</tr>
<tr>
<td>6.</td>
<td>数据不一致遇到过吗，是什么原因?</td>
<td></td>
</tr>
<tr>
<td>7.</td>
<td>知道什么是 whole stage codengen吗</td>
<td>面向接口编程太耗时间，主要是方法递归调用，虚函数调用 可以将一个stage的所有task整理成一个方法，并且生成动态字节码 并结合</td>
</tr>
<tr>
<td>8.</td>
<td>spark 3.0 特性</td>
<td>待学</td>
</tr>
<tr>
<td><br>9.</td>
<td><br> wordCount</td>
<td>lines=sc.textFile(path) <br>words = lines.flatMap(lambda x: x.split(’ '))<br> wco = words.map(lambda x: (x, 1))<br>word_count = wco.reduceByKey(add)</td>
</tr>
</tbody>
</table>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    factory,</span><br><span class="line">    department,</span><br><span class="line">    <span class="keyword">SUM</span>(quantity)</span><br><span class="line"><span class="keyword">FROM</span> production</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">GROUPING</span> <span class="keyword">SETS</span>(factory, department)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> factory, department</span><br><span class="line"></span><br><span class="line">HDFS:</span><br><span class="line"><span class="number">205.2</span> M  part<span class="number">-00000</span><span class="number">-30</span>ceee1e<span class="number">-2</span>ed6<span class="number">-4239</span><span class="number">-8</span>a6b<span class="number">-45</span>fc6cbf1ef6.c000</span><br><span class="line"><span class="number">205.2</span> M  part<span class="number">-00001</span><span class="number">-30</span>ceee1e<span class="number">-2</span>ed6<span class="number">-4239</span><span class="number">-8</span>a6b<span class="number">-45</span>fc6cbf1ef6.c000</span><br><span class="line"><span class="number">3.8</span> M    part<span class="number">-00002</span><span class="number">-30</span>ceee1e<span class="number">-2</span>ed6<span class="number">-4239</span><span class="number">-8</span>a6b<span class="number">-45</span>fc6cbf1ef6.c000</span><br><span class="line"></span><br><span class="line">共三个数据文件，如果设置参数 spark.sql.files.maxPartitionBytes为<span class="number">64</span>M，会把数据分<span class="number">8</span>个块:</span><br><span class="line"><span class="comment">##part-00000  四块</span></span><br><span class="line"><span class="keyword">range</span>: <span class="number">0</span><span class="number">-67108864</span>  ; </span><br><span class="line">range: 67108864-134217728;  </span><br><span class="line">range: 134217728-201326592</span><br><span class="line">range: 201326592-215189723</span><br><span class="line"><span class="comment">##part-00001  四块</span></span><br><span class="line">range: 0-67108864  ; </span><br><span class="line">range: 67108864-134217728;  </span><br><span class="line">range: 134217728-201326592</span><br><span class="line">range: 201326592-215167669</span><br><span class="line"><span class="comment">##part-00002  一块</span></span><br><span class="line">range: 0-4002630</span><br><span class="line"></span><br><span class="line">启动7个task：</span><br><span class="line"></span><br><span class="line">理论上有6个task分别负责每个64M的块数据，然后最后一个task负责part-00000，part-00001剩余的不足64M的两个块以及part-00002</span><br><span class="line">分区数确实增加了，由四个增加到了7个，但是新增的3个却没处理什么数据，大部分的数据还是4个partition在处理，所以还是很慢~~~~</span><br><span class="line">task数增加了，但是数据并没有均分到每个task，为什么呢?</span><br><span class="line"></span><br><span class="line">spark 在处理parquet 文件时，一个row group 只能由一个task 来处理，在hdfs 中一个row group 可能横跨hdfs block ，那么spark是怎么保证一个task只处理一个 row group 的呢？</span><br><span class="line">检查table_a发现，生成table_a时，parquet.block.size 用的默认值128M ，这样就导致一个row group 有128M 的大小。</span><br><span class="line">parquet.block.size 是可以依据实际使用情况来调优的，对于做多维分析表，可以设置稍小一点。</span><br><span class="line"></span><br><span class="line">最终 经过调试设置parquet.block.size 为16M ；设置spark.sql.files.maxPartitionBytes为16M</span><br><span class="line">读取hdfs文件时，并行了22个task，并且每个task处理数据均匀</span><br><span class="line"></span><br><span class="line">parquet.block.size所控制的parquet row group大小是一个需要调优的spark参数。其中重要一点，就是控制任务的并发度。</span><br><span class="line">在Hadoop里，任务的并发默认是以hdfs block为单位的，而Spark里多了一种选择，即以RowGroup为基本单位。</span><br><span class="line">在调用HiveContext.read.parquet(path)时，会触发ParquetRelation2对象生成SqlNewHadoopRDD对象，并覆盖其中getPartitions()方法</span><br><span class="line"></span><br><span class="line">60min -&gt; 3mins</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th>Title</th>
<th>Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.</td>
<td>kaike - sparkSQL底层实现原理<br><a href="https://blog.csdn.net/abc33880238/article/details/102100570" target="_blank" rel="noopener">spark.sql.shuffle.partitions和 spark.default.parallelism 的区别</a><br><a href="https://blog.csdn.net/xiaoduan_/article/details/79809262" target="_blank" rel="noopener">SparkSQL并行度参数设置方法</a></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">1.</td>
<td><a href="https://www.bilibili.com/video/BV1fE411E7Ak?p=23" target="_blank" rel="noopener">B站 我爱喝假酒 - 性能调优</a></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td><a href="https://www.cnblogs.com/jxhd1/p/6702218.html" target="_blank" rel="noopener">Spark性能调优之合理设置并行度 (稍有误)</a>， <a href="https://www.cnblogs.com/stillcoolme/p/10576563.html" target="_blank" rel="noopener">Spark实践 – 性能优化基础</a></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td>spark.defalut.parallelism 默认是没有值的，如设置值为10，是在<code>shuffle/窄依赖</code> 的过程才会起作用（val rdd2 = rdd1.reduceByKey(_+_) //rdd2的分区数就是10，rdd1的分区数不受这个参数的影响）</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td>如果读取的数据在HDFS上，增加block数，默认情况下split与block是一对一的，而split又与RDD中的partition对应，所以增加了block数，也就提高了并行度</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">5.</td>
<td>reduceByKey的算子指定partition的数量 <br> val rdd2 = rdd1.reduceByKey(_+_,10)  val rdd3 = rdd2.map.filter.reduceByKey(_+_)</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">6.</td>
<td>val rdd3 = rdd1.join（rdd2）  rdd3里面partiiton的数量是由父RDD中最多的partition数量来决定，因此使用join算子的时候，增加父RDD中partition的数量</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">7.</td>
<td>由于Spark SQL所在stage的 <strong>并行度无法手动设置</strong><br><br>如果数据量较大，并且此stage中后续的transformation操作有着复杂的业务逻辑，而Spark SQL自动设置的task数量很少，这就意味着每个task要处理为数不少的数据量，然后还要执行非常复杂的处理逻辑，这就可能表现为第一个有Spark SQL的stage速度很慢，而后续的没有Spark SQL的stage运行速度非常快。</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="rdd-属性"><a class="markdownIt-Anchor" href="#rdd-属性"></a> RDD 属性</h2>
<blockquote>
<ul>
<li>A list of partitions</li>
<li>A function for computing each split</li>
<li>A list of dependencies on other RDDs</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li>
<li>Optionally, a list of preferred locations to compute each split on (block locations for an HDFS file)</li>
</ul>
</blockquote>
<ul>
<li><a href="https://blog.csdn.net/qq_22473611/article/details/107822168" target="_blank" rel="noopener">very good Spark分区 partition 详解</a></li>
</ul>
<img src="/images/spark/spark-rdd-split-task-partition.png" width="800" alt="申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task" />
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th>Title</th>
<th>Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td><strong>一组分片（Partition）</strong>，即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">4.</td>
<td>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</td>
<td>❎</td>
</tr>
<tr>
<td style="text-align:center">5.</td>
<td>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置</td>
<td>❎</td>
</tr>
</tbody>
</table>
<p>尽量保证每轮Stage里每个task处理的数据量&gt;128M</p>
<h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2>
<ul>
<li><a href="https://jiamaoxiang.top/2020/11/26/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%845%E5%A4%A7SQL%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener">数仓开发需要了解的5大SQL分析函数</a></li>
<li><a href="https://www.huaweicloud.com/articles/82b67712cabc74252eb3efe12aff7914.html" target="_blank" rel="noopener">干货：一文读懂数据仓库设计方案</a> |</li>
<li><a href="https://zhuanlan.zhihu.com/p/148466975" target="_blank" rel="noopener">使用Hive窗口函数替换union all处理分组汇总（小计，总计）</a><br />
<br></li>
<li><a href="https://www.cnblogs.com/stillcoolme/p/10576563.html" target="_blank" rel="noopener">Spark实践 – 性能优化基础</a></li>
<li><a href="https://blog.csdn.net/Anbang713/article/details/82844499" target="_blank" rel="noopener">Spark项目实战-troubleshooting之控制shuffle reduce端缓冲大小以避免OOM</a></li>
<li><a href="https://developer.aliyun.com/article/766699" target="_blank" rel="noopener">结合源码谈谈 - 通过spark.default.parallelism谈Spark并行度</a></li>
<li><a href="https://blog.csdn.net/weixin_43179522/article/details/107942679" target="_blank" rel="noopener">谈谈spark.sql.shuffle.partitions和 spark.default.parallelism 的区别及spark并行度的理解</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io" target="_blank" rel="noopener"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#spark"><span class="toc-text"> Spark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rdd-属性"><span class="toc-text"> RDD 属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-text"> Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/spark/" rel="tag">spark</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/01/22/dataware/dataware-business-3/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          DataWare Business Review 3
        
      </div>
    </a>
  
  
    <a href="/2021/01/20/python/language/py_getattr/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">python 的 decorator &amp; getattr() 函数&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2021/01/21/spark/spark-summary-3-trouble-shooting/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
