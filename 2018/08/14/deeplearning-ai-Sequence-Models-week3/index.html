<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Sequence Models (week3) - Attention mechanism - Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
<meta property="og:type" content="article">
<meta property="og:title" content="Sequence Models (week3) - Attention mechanism">
<meta property="og:url" content="http://iequa.com/2018/08/14/deeplearning-ai-Sequence-Models-week3/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-1.jpg">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-2.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-3.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-4.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-5.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-6_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-7_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-8_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-9_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-10_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-11_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-12_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-13_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-14_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-15_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-16_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-17_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W3-18_1.png">
<meta property="og:updated_time" content="2018-08-18T06:50:14.846Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sequence Models (week3) - Attention mechanism">
<meta name="twitter:description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
<meta name="twitter:image" content="http://iequa.com/images/deeplearning/C5W3-1.jpg">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/english">English</a>
        
          <a class="main-nav-link" href="/deeplearning">DL</a>
        
          <a class="main-nav-link" href="/ai">ML</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/project_frame">Project</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-deeplearning-ai-Sequence-Models-week3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Sequence Models (week3) - Attention mechanism
      <small class=article-detail-date-index>&nbsp; 2018-08-14</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2018/08/14/deeplearning-ai-Sequence-Models-week3/" class="article-date">
  <time datetime="2018-08-14T02:00:21.000Z" itemprop="datePublished">2018-08-14</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2018/08/14/deeplearning-ai-Sequence-Models-week3/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

<ul>
<li>能够将序列模型应用到自然语言问题、音频应用 等，包括文字合成、语音识别和音乐合成。</li>
</ul>
<a id="more"></a>
<h2 id="1-Basic-models"><a href="#1-Basic-models" class="headerlink" title="1. Basic models"></a>1. Basic models</h2><p>假设需要翻译下面这句话</p>
<blockquote>
<p>“简将要在9月访问中国”</p>
</blockquote>
<p>我们希望得到的结果是</p>
<blockquote>
<p>“<strong>Jane is visiting China in September</strong>”</p>
</blockquote>
<p>在这个例子中输入的数量是10个中文汉字，输出为6个单词， $T_x$ 与 $T_y$ 数量不一致，就需要用到 Sequence to sequence model <strong>RNN</strong></p>
<p><img src="/images/deeplearning/C5W3-1.jpg" width="750"></p>
<p>类似的例子还有用机器为下面这张图片生成描述</p>
<p><img src="/images/deeplearning/C5W3-2.png" width="600"></p>
<p>只需要将encoder部分用一个CNN模型替换就可以了，比如AlexNet，就可以得到“一只（可爱的）猫躺在楼梯上”</p>
<h2 id="2-Picking-the-most-likely-sentence"><a href="#2-Picking-the-most-likely-sentence" class="headerlink" title="2. Picking the most likely sentence"></a>2. Picking the most likely sentence</h2><p>下面将之前学习的语言模型和机器翻译模型做一个对比, P为概率</p>
<p>语言模型:</p>
<p><img src="/images/deeplearning/C5W3-3.png" width="700"></p>
<p>机器翻译模型:</p>
<p><img src="/images/deeplearning/C5W3-4.png" width="750"></p>
<p>可以看到，机器翻译模型的后半部分其实就是语言模型，Andrew将其称之为“条件语言模型”，在语言模型之前有一 个条件也就是被翻译的句子:</p>
<p>$$<br>P(y^{<1>},…,y^{&lt;{T_y}&gt;}|x^{<1>},…,x^{&lt;{T_x}&gt;})<br>$$</1></1></p>
<blockquote>
<p>但是我们知道翻译是有很多种方式的，同一句话可以翻译成很多不同的句子，那么我们如何判断哪一个句子是最好的呢？</p>
<p>还是翻译上面那句话，有如下几种翻译结果：</p>
<ul>
<li>“Jane is visiting China in September.”</li>
<li>“Jane is going to visit China in September.”</li>
<li>“In September, Jane will visit China”</li>
<li>“Jane’s Chinese friend welcomed her in September.”</li>
<li>….</li>
</ul>
<p>与语言模型不同的是，机器模型在输出部分不再使用softmax随机分布的形式进行取样，因为很容易得到一个不准确的翻译，取而代之的是使用 <code>Beam Search</code> 做最优化的选择。这个方法会在后下一小节介绍，在此之前先介绍一下<strong>贪婪搜索(Greedy Search)</strong>及其弊端，这样才能更好地了解Beam Search的优点。</p>
</blockquote>
<h3 id="2-1-Greedy-Search"><a href="#2-1-Greedy-Search" class="headerlink" title="2.1 Greedy Search"></a>2.1 Greedy Search</h3><p>得到最好的翻译结果，转换成数学公式就是:</p>
<p>$$<br>argmax P(y^{<1>},…,y^{&lt;{T_y}&gt;}|x^{<1>},…,x^{&lt;{T_x}&gt;})<br>$$</1></1></p>
<p>那么贪婪搜索是什么呢？</p>
<p>通俗解释就是每次输出的那个都必须是最好的。还是以翻译那句话为例。</p>
<p>现在假设通过贪婪搜索已经确定最好的翻译的前两个单词是：”Jane is “</p>
<p>然后因为”going”这个单词出现频率较高和其它原因，所以根据贪婪算法得出此时第三个单词的最好结果是”going”。</p>
<p>所以据贪婪算法最后的翻译结果可能是下图中的第二个句子，但第一句可能会更好(不服气的话，我们就假设第一句更好).</p>
<p><img src="/images/deeplearning/C5W3-5.png" width="700"></p>
<p>所以贪婪搜索的缺点是局部最优并不代表全局最优，就好像五黑，一队都是很牛逼的，但是各个都太优秀，就显得没那么优秀了，而另一队虽然说不是每个都是最优秀，但是凑在一起就是能carry全场。</p>
<p>更形象的理解可能就是贪婪搜索更加短视，看的不长远，而且也更加耗时。假设字典中共有10000个单词，如果使用贪婪搜索，那么可能的组合有1000010种，所以还是挺恐怖的2333~~</p>
<h2 id="3-Beam-Search"><a href="#3-Beam-Search" class="headerlink" title="3. Beam Search"></a>3. Beam Search</h2><p><strong>Beam Search</strong> 是greedy search的加强版本，首先要预设一个值 beam width，这里等于 <code>3</code> (如果等于<strong>1</strong>就是<strong>greedy search</strong>)。然后在每一步保存最佳的3个结果进行下一步的选择，以此直到遇到句子的终结符.</p>
<h3 id="3-1-步骤一"><a href="#3-1-步骤一" class="headerlink" title="3.1 步骤一"></a>3.1 步骤一</h3><p>如下图示，因为beam width=3，所以根据输入的需要翻译的句子选出 3 个 $y^{<1>}$最可能的输出值，即选出$P(y^{<1>}|x)$最大的前3个值。假设分别是”in”,”jane”,”september”</1></1></p>
<p><img src="/images/deeplearning/C5W3-6_1.png" width="700"></p>
<h3 id="3-2-步骤二"><a href="#3-2-步骤二" class="headerlink" title="3.2 步骤二"></a>3.2 步骤二</h3><p>以”<strong>in</strong>“为例进行说明，其他同理.</p>
<p>如下图示，在给定被翻译句子 $x$ 和确定 $y^{<1>}$ = “<strong>in</strong>“ 的条件下，下一个输出值的条件概率是 $P(y^{<2>}|x,”in”)$。此时需要从 10000 种可能中找出条件概率最高的前 3 个.</2></1></p>
<p>又由公式 $P(y^{<1>},y^{<2>}|x)=P(y^{<1>}|x) P(y^{<2>}|x, y^{<1>})$, 我们此时已经得到了给定输入数据，前两个输出值的输出概率比较大的组合了.</1></2></1></2></1></p>
<p><img src="/images/deeplearning/C5W3-7_1.png" width="700"></p>
<p>另外 2 个单词也做同样的计算</p>
<p><img src="/images/deeplearning/C5W3-8_1.png" width="700"></p>
<p>此时我们得到了 9 组 $P(y^{<1>},y^{<2>}|x)$, 此时我们再从这 9组 中选出概率值最高的前 3 个。如下图示，假设是这3个：</2></1></p>
<ul>
<li>“in september”</li>
<li>“jane is”</li>
<li>“jane visits”</li>
</ul>
<p><img src="/images/deeplearning/C5W3-9_1.png" width="550"></p>
<h3 id="3-3-步骤三"><a href="#3-3-步骤三" class="headerlink" title="3.3 步骤三"></a>3.3 步骤三</h3><p>继续步骤2的过程，根据 $P(y^{<3>}|x,y^{<1>},y^{<2>})$ 选出 $P(y^{<1>},y^{<2>},y^{<3>}|x)$ 最大的前3个组合.</3></2></1></2></1></3></p>
<p>后面重复上述步骤得出结果.</p>
<h3 id="3-4-总结"><a href="#3-4-总结" class="headerlink" title="3.4 总结"></a>3.4 总结</h3><p>总结一下上面的步骤就是：</p>
<blockquote>
<ul>
<li><p>(1). 经过 encoder 以后，decoder 给出最有可能的三个开头词依次为 “in”, “jane”, “september”<br>$$P(y^{<1>}|x)$$</1></p>
</li>
<li><p>(2). 经过将第一步得到的值输入到第二步中，最有可能的三个翻译为 “in september”, “jane is”, “jane visits” </p>
</li>
</ul>
<p>$$P(y^{<2>}|x,y^{<1>})$$</1></2></p>
<p>(这里，september开头的句子由于概率没有其他的可能性大，已经失去了作为开头词资格)</p>
<ul>
<li>(3). 继续这个过程… </li>
</ul>
<p>$$P(y^{<3>}|x,y^{<1>},y^{<2>})$$</2></1></3></p>
</blockquote>
<p><img src="/images/deeplearning/C5W3-10_1.png" width="750"></p>
<h2 id="4-Refinements-to-beam-search"><a href="#4-Refinements-to-beam-search" class="headerlink" title="4. Refinements to beam search"></a>4. Refinements to beam search</h2><p>$$<br>P(y^{<1>},….,P(y^{T_y})|x)=P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})…P(y^{&lt;{T_y}&gt;}|x,y^{<1>},…y^{&lt;{T_y-1}&gt;})<br>$$</1></1></2></1></1></p>
<p>所以要满足 $argmax P(y^{<1>},….,P(y^{T_y})|x)$, 也就等同于要满足</1></p>
<p>$$<br>argmax \prod_{t=1}^{T_y}P(y^{&lt;{t}&gt;}|x,y^{<1>},…y^{&lt;{t-1}&gt;})<br>$$</1></p>
<p>但是上面的公式存在一个问题，因为概率都是小于1的，累乘之后会越来越小，可能小到计算机无法精确存储，所以可以将其转变成 log 形式（因为 log 是单调递增的，所以对最终结果不会有影响），其公式如下：</p>
<p>$$<br>argmax \sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{<1>},…y^{&lt;{t-1}&gt;})<br>$$</1></p>
<blockquote>
<p>But！！！上述公式仍然存在bug，观察可以知道，概率值都是小于1的，那么log之后都是负数，所以为了使得最后的值最大，那么只要保证翻译的句子越短，那么值就越大，所以如果使用这个公式，那么最后翻译的句子通常都是比较短的句子，这显然不行。</p>
</blockquote>
<p>所以我们可以通过归一化的方式来纠正，即保证平均到每个单词都能得到最大值。其公式如下：</p>
<p>$$<br>argmax \frac{1}{T_y}\sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{<1>},…y^{&lt;{t-1}&gt;})<br>$$</1></p>
<p>通过归一化的确能很好的解决上述问题，但是在实际运用中，会额外添加一个参数 $α$, 其大小介于 0 和 1 之间，公式如下:</p>
<p>$$<br>argmax \frac{1}{T_y^α}\sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{<1>},…y^{&lt;{t-1}&gt;})<br>$$</1></p>
<p><img src="/images/deeplearning/C5W3-11_1.png" width="700"></p>
<blockquote>
<p>$T_y$ 为输出句子中单词的个数，$α$ 是一个超参数 (可以设置为 0.7)</p>
<p>$α$ == 1. 则代表 完全用句子长度归一化<br>$α$ == 0. 则代表 没有归一化<br>$α$ == 0~1. 则代表 在 句子长度归一化 与 没有归一化 之间的折中程度.</p>
<p>beam width = B = 3~<strong>10</strong>~100 是会有一个明显的增长，但是 B 从 1000 ~ 3000 是并没有一个明显增长的.</p>
</blockquote>
<h2 id="5-Error-analysis-on-beam-search"><a href="#5-Error-analysis-on-beam-search" class="headerlink" title="5. Error analysis on beam search"></a>5. Error analysis on beam search</h2><p>仔细想想 <strong>beam search</strong>，我们会发现其实它是近似搜索，也就是说可能使用这种方法最终得到的结果并不是最好的。当然也有可能是因为使用的 <strong>RNN</strong> 模型有缺陷导致结果不是最好的。</p>
<p><strong>所以我们如何判断误差是出在哪个地方呢？</strong></p>
<blockquote>
<p>还是以翻译这句话为例：“<strong>简在9月访问中国</strong>”。</p>
<ul>
<li>假设按照人类的习惯翻译成英文是“Jane visits China in September.”,该结果用 $y^*$ 表示。</li>
<li>假设通过算法得出的翻译结果是：“Jane visited China in September.”,该结果用 $\hat{y}$ 表示。</li>
</ul>
<p>要判断误差出在哪，只需要比较 $P(y^*|x)$ 和 $P(\hat{y}|x)$ 的大小即可.</p>
</blockquote>
<p>下面分两种情况讨论：</p>
<p><img src="/images/deeplearning/C5W3-12_1.png" width="750"></p>
<blockquote>
<p>RNN 实际上是 encode 和 decode 的过程.</p>
</blockquote>
<p>两种情况：</p>
<p>(1). $<br>P(y^*|x)&gt;P(\hat{y}|x)<br>$</p>
<p>上面的不等式的含义是 beam search 最后选出的结果不如人类，也就是 beam search 并没有选出最好的结果，所以问题出在 beam search</p>
<p>(2). $<br>P(y^*|x)≤P(\hat{y}|x)<br>$</p>
<p>上面不等式表示 beam search 最后选出的结果要比人类的更好，也就是说 beam search 已经选出了最好的结果，但是模型对各个组合的预测概率值并不符合人类的预期，所以 RNN模型 at fault.</p>
<blockquote>
<p>上面已经介绍了误差分析的方式，但时仅凭一次误差分析就判定谁该背锅肯定也不行，所以还需要进行多次误差分析多次。</p>
<p>如下图示已经进行了多次的误差分析，每次分析之后都判定了锅该谁背，最后计算出beam search和模型背锅的比例，根据比例作出相应的调整。</p>
<p>例如:</p>
<ul>
<li>如果 beam search 更高，可以相应调整 beam width.</li>
<li>如果模型背锅比例更高，那么可以考虑增加正则化，增加数据等操作.</li>
</ul>
</blockquote>
<p><img src="/images/deeplearning/C5W3-13_1.png" width="750"></p>
<h2 id="6-Bleu-score-optional"><a href="#6-Bleu-score-optional" class="headerlink" title="6. Bleu score (optional)"></a>6. Bleu score (optional)</h2><p>主要介绍了如何给机器翻译结果打分，因为是选修内容, 所以 balabala…</p>
<h2 id="7-Attention-model-intuition"><a href="#7-Attention-model-intuition" class="headerlink" title="7. Attention model intuition"></a>7. Attention model intuition</h2><p>之前介绍的RNN翻译模型存在一个很明显的问题就是:</p>
<blockquote>
<p>机器翻译的翻译过程是首先将所有需要翻译的句子输入到 <strong>Encoder</strong> 中，之后再通过 <strong>Decoder</strong> 输出翻译语句.</p>
</blockquote>
<h3 id="7-1-Why-Attention-model"><a href="#7-1-Why-Attention-model" class="headerlink" title="7.1 Why Attention model"></a>7.1 Why Attention model</h3><p>如下图示机器算法将法语翻译成英语的模型.</p>
<p><img src="/images/deeplearning/C5W3-14_1.png" width="750"></p>
<p>机器翻译与人类的翻译过程不太相同。因为人类翻译一般是逐句翻译，或者是讲一段很长的句子分解开来进行翻译。</p>
<p>所以上述模型的翻译结果的Bleu评分与被翻译句子的长短有很大关系，句子较短时，模型可能无法捕捉到关键信息，所以翻译结果不是很高；但是当句子过长时，模型又抓不到重点等原因使得结果也不是很高。</p>
<p><img src="/images/deeplearning/C5W3-15_1.png" width="750"></p>
<blockquote>
<p>​见上图，如果机器能像人一样逐句或者每次将注意力只集中在一小部分进行翻译，那么翻译结果将不受句子长度的影响。下图中的绿色线即为使用了注意力模型后的翻译句子得分。</p>
</blockquote>
<h3 id="7-2-Attention-model-intro"><a href="#7-2-Attention-model-intro" class="headerlink" title="7.2 Attention model intro"></a>7.2 Attention model intro</h3><p>下图展示了普通的翻译模型双向 RNN 结构，该结构可根据输入 $x^{&lt;{t}&gt;}$ 直接得到输出 $y^{&lt;{t}&gt;}$.</p>
<p><img src="/images/deeplearning/C5W3-16_1.png" width="750"></p>
<p>注意力模型在此基础上做进一步处理。</p>
<p>为避免误解，使用另一个符号s来表示节点。</p>
<p>如下图示，根据下面一层的双向RNN计算结果可得到节点s<1>与其他节点权重α<1,1>,α<1,2>,…,通过这些权重可以知道该节点与其他节点的相关联程度，从而可以达到将注意力集中到部分区域的效果。</1,2></1,1></1></p>
<p><img src="/images/deeplearning/C5W3-17_1.png" width="750"></p>
<p>​其他节点同理。整个注意力模型结构如下图示</p>
<p><img src="/images/deeplearning/C5W3-18_1.png" width="750"></p>
<h2 id="8-Attention-model"><a href="#8-Attention-model" class="headerlink" title="8. Attention model"></a>8. Attention model</h2><h2 id="9-Speech-recognition"><a href="#9-Speech-recognition" class="headerlink" title="9. Speech recognition"></a>9. Speech recognition</h2><h2 id="10-Trigger-word-detection"><a href="#10-Trigger-word-detection" class="headerlink" title="10. Trigger word detection"></a>10. Trigger word detection</h2><h2 id="11-Summary-and-thank-you"><a href="#11-Summary-and-thank-you" class="headerlink" title="11. Summary and thank you"></a>11. Summary and thank you</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
<li><a href="https://www.ctolib.com/Yukong-Deeplearning-ai-Solutions.html" target="_blank" rel="external">deeplearning.ai深度学习课程字幕翻译项目</a></li>
<li><a href="https://blog.csdn.net/Jerr__y/article/details/53749693" target="_blank" rel="external">seq2seq学习笔记</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_weibo_icon.png);background-size: contain;display: inline-block; width:50px; height:50px" href="https://service.weibo.com/share/share.php?url" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/2017/11/05/support-pay-blog/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/2017/11/05/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  原创文章，转载请注明： 转载自<a href="http://www.iequa.com"> Blair Chan's Blog</a>，作者：
  <a href="http://www.iequa.com/about">Blair Chan</a> <br>
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>

 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Basic-models"><span class="toc-number"></span> <span class="toc-text">1. Basic models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Picking-the-most-likely-sentence"><span class="toc-number"></span> <span class="toc-text">2. Picking the most likely sentence</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Greedy-Search"><span class="toc-number"></span> <span class="toc-text">2.1 Greedy Search</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Beam-Search"><span class="toc-number"></span> <span class="toc-text">3. Beam Search</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-步骤一"><span class="toc-number"></span> <span class="toc-text">3.1 步骤一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-步骤二"><span class="toc-number"></span> <span class="toc-text">3.2 步骤二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-步骤三"><span class="toc-number"></span> <span class="toc-text">3.3 步骤三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-总结"><span class="toc-number"></span> <span class="toc-text">3.4 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Refinements-to-beam-search"><span class="toc-number"></span> <span class="toc-text">4. Refinements to beam search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Error-analysis-on-beam-search"><span class="toc-number"></span> <span class="toc-text">5. Error analysis on beam search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Bleu-score-optional"><span class="toc-number"></span> <span class="toc-text">6. Bleu score (optional)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Attention-model-intuition"><span class="toc-number"></span> <span class="toc-text">7. Attention model intuition</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Why-Attention-model"><span class="toc-number"></span> <span class="toc-text">7.1 Why Attention model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Attention-model-intro"><span class="toc-number"></span> <span class="toc-text">7.2 Attention model intro</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Attention-model"><span class="toc-number"></span> <span class="toc-text">8. Attention model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Speech-recognition"><span class="toc-number"></span> <span class="toc-text">9. Speech recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Trigger-word-detection"><span class="toc-number"></span> <span class="toc-text">10. Trigger word detection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-Summary-and-thank-you"><span class="toc-number"></span> <span class="toc-text">11. Summary and thank you</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number"></span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/deeplearning-ai/">deeplearning.ai</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/08/16/chatbot-july-3/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          机器学习构建聊天机器人 3
        
      </div>
    </a>
  
  
    <a href="/2018/08/02/deeplearning-ai-Sequence-Models-week2/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Sequence Models (week2) - NLP - Word Embeddings&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Libin Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2018/08/14/deeplearning-ai-Sequence-Models-week3/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
