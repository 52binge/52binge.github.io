<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>FastText 用于高效文本分类的技巧 - Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。
fastText 是 智慧与美貌并重的 文本分类 and 向量化工具 的项目，它是有两部分组成的。 
论文1链接： Bag of Tricks for Efficient Text Classification
论文2链接： Enriching Word Vectors with">
<meta property="og:type" content="article">
<meta property="og:title" content="FastText 用于高效文本分类的技巧">
<meta property="og:url" content="http://iequa.com/2018/12/19/nlp/fastText/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。
fastText 是 智慧与美貌并重的 文本分类 and 向量化工具 的项目，它是有两部分组成的。 
论文1链接： Bag of Tricks for Efficient Text Classification
论文2链接： Enriching Word Vectors with">
<meta property="og:image" content="http://iequa.com/images/nlp/fastText2.jpg">
<meta property="og:updated_time" content="2019-01-20T01:01:36.470Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FastText 用于高效文本分类的技巧">
<meta name="twitter:description" content="fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。
fastText 是 智慧与美貌并重的 文本分类 and 向量化工具 的项目，它是有两部分组成的。 
论文1链接： Bag of Tricks for Efficient Text Classification
论文2链接： Enriching Word Vectors with">
<meta name="twitter:image" content="http://iequa.com/images/nlp/fastText2.jpg">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/chatbot">Bot</a>
        
          <a class="main-nav-link" href="/deeplearning">Deep Learning</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-nlp/fastText" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      FastText 用于高效文本分类的技巧
      <small class=article-detail-date-index>&nbsp; 2018-12-19</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2018/12/19/nlp/fastText/" class="article-date">
  <time datetime="2018-12-18T23:00:21.000Z" itemprop="datePublished">2018-12-19</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/nlp/">nlp</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2018/12/19/nlp/fastText/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>fastText 是 2016年 FAIR(Facebook AIResearch) 推出的一款文本分类与向量化工具。</p>
<p>fastText 是 智慧与美貌并重的 <strong>文本分类</strong> and <strong>向量化工具</strong> 的项目，它是有两部分组成的。 </p>
<p>论文1链接： <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="external">Bag of Tricks for Efficient Text Classification</a></p>
<p>论文2链接： <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a></p>
<a id="more"></a>
<p>github链接： <a href="https://github.com/facebookresearch/fastText" target="_blank" rel="external">facebookresearch/fastText</a></p>
<p>fastText 能够做到效果好，速度快，主要依靠两个秘密武器：</p>
<blockquote>
<ol>
<li>利用了词内的n-gram信息(subword n-gram information)</li>
<li>用到了层次化Softmax回归(Hierarchical Softmax) 的训练 trick.</li>
</ol>
</blockquote>
<p><strong>fastText 背景</strong></p>
<p>英语单词通常有其内部结构和形成方式。例如我们可以从“dog”、“dogs”和“dogcatcher”的字面上推测他们的关系。这些词都有同一个词根“dog”，这个关联可以推广至其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。很多词根据场景不同有多种不同的形态。构词学（morphology）作为语言学的一个重要分支，研究的正是词的内部结构和形成方式。</p>
<p>在 word2vec 中，我们并没有直接利用构词学中的信息。无论是在 <strong>skip-gram</strong> 还是 <strong>CBOW</strong> 中，我们将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。有鉴于此，fastText 提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入 word2vec 中的 <strong>skip-gram</strong>。</p>
<p><strong>子词嵌入 subword embedding</strong></p>
<p>在 fastText 中，每个中心词被表示成子词的集合。下面我们用单词“where”作为例子来了解子词是如何产生的。首先，我们在单词的首尾分别添加特殊字符“&lt;”和“&gt;”以区分作为前后缀的子词。然后，将单词当成一个由字符构成的序列来提取 $n$ 元语法。例如当 $n=3$ 时，我们得到所有长度为 3 的子词：</p>
<p>$$&lt;.wh ， whe ， her ， ere ， re&gt;$$</p>
<p>以及特殊子词 “&lt;.where&gt;”。</p>
<p>在 fastText 中，对于一个词 $w$，将它所有长度在 3 到 6 的子词和特殊子词的并集记为 $\mathcal{G}_w$。那么词典则是所有词的子词集合的并集。假设词典中子词 $g$ 的向量为 $\boldsymbol{z}_g$，那么跳字模型中词 $w$ 的作为中心词的向量 $\boldsymbol{v}_w$ 则表示成</p>
<p>$$<br>\boldsymbol{v}_w = \sum_{g\in\mathcal{G}_w} \boldsymbol{z}_g.<br>$$</p>
<p>FastText 的其余部分同 <strong>skip-gram</strong> 一致，不在此重复。可以看到，同 <strong>skip-gram</strong> 相比，fastText 中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。但与此同时，较生僻的复杂单词，甚至是词典中没有的单词，可能会从同它结构类似的其他词那里获取更好的词向量表示。</p>
<h2 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h2><h3 id="1-1-Softmax-Regression"><a href="#1-1-Softmax-Regression" class="headerlink" title="1.1 Softmax Regression"></a>1.1 Softmax Regression</h3><p>Softmax Regression (回归) 又被称作多项LR（multinomial logistic regression），它是LR在多类别任务上的推广。</p>
<p><img src="/images/nlp/fastText2.jpg" width="850" img=""></p>
<h3 id="1-2-Hierarchical-Softmax"><a href="#1-2-Hierarchical-Softmax" class="headerlink" title="1.2 Hierarchical Softmax"></a>1.2 Hierarchical Softmax</h3><h3 id="1-3-n-gram’s-feature"><a href="#1-3-n-gram’s-feature" class="headerlink" title="1.3 n-gram’s feature"></a>1.3 n-gram’s feature</h3><p>在文本特征提取中，常常能看到 n-gram 的身影。它是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为 N 的滑动窗口操作，最终形成长度为 N 的字节片段序列。看下面的例子：</p>
<p><strong>字粒度</strong></p>
<blockquote>
<p>我来到达观数据参观</p>
<ul>
<li><p>相应的bigram特征为：我来 来到 到达 达观 观数 数据 据参 参观</p>
</li>
<li><p>相应的trigram特征为：我来到 来到达 到达观 达观数 观数据 数据参 据参观</p>
</li>
</ul>
</blockquote>
<p><strong>词粒度</strong></p>
<blockquote>
<p>我 来到 达观数据 参观</p>
<ul>
<li><p>相应的bigram特征为：我/来到 来到/达观数据 达观数据/参观</p>
</li>
<li><p>相应的trigram特征为：我/来到/达观数据 来到/达观数据/参观 </p>
</li>
</ul>
</blockquote>
<p><strong>小结：</strong></p>
<p>n-gram中的gram根据粒度不同。它可以是字粒度，也可以是词粒度的。</p>
<p>n-gram 产生的特征只是作为<strong>文本特征的候选集</strong>，你后面可能会采用信息熵、卡方统计、IDF等文本特征选择方式筛选出比较重要特征。</p>
<h2 id="2-word2vec-架构原理"><a href="#2-word2vec-架构原理" class="headerlink" title="2. word2vec 架构原理"></a>2. word2vec 架构原理</h2><h3 id="2-1-CBOW-模型架构"><a href="#2-1-CBOW-模型架构" class="headerlink" title="2.1 CBOW 模型架构"></a>2.1 CBOW 模型架构</h3><h3 id="2-2-前向传播"><a href="#2-2-前向传播" class="headerlink" title="2.2 前向传播"></a>2.2 前向传播</h3><h3 id="2-3-反向传播"><a href="#2-3-反向传播" class="headerlink" title="2.3 反向传播"></a>2.3 反向传播</h3><h2 id="3-fastText-核心思想"><a href="#3-fastText-核心思想" class="headerlink" title="3. fastText 核心思想"></a>3. fastText 核心思想</h2><h3 id="3-1-字符级-n-gram"><a href="#3-1-字符级-n-gram" class="headerlink" title="3.1 字符级 n-gram"></a>3.1 字符级 n-gram</h3><h3 id="3-2-模型架构"><a href="#3-2-模型架构" class="headerlink" title="3.2 模型架构"></a>3.2 模型架构</h3><h3 id="3-3-核心思想"><a href="#3-3-核心思想" class="headerlink" title="3.3 核心思想"></a>3.3 核心思想</h3><p>仔细观察模型的后半部分，即从隐含层输出到输出层输出，会发现它就是一个softmax线性多类别分类器，分类器的输入是一个用来表征当前文档的向量；模型的前半部分，即从输入层输入到隐含层输出部分，主要在做一件事情：生成用来表征文档的向量。那么它是如何做的呢？叠加构成这篇文档的所有词及n-gram的词向量，然后取平均。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。</p>
<p>于是fastText的核心思想就是：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。</p>
<h3 id="3-4-分类效果"><a href="#3-4-分类效果" class="headerlink" title="3.4 分类效果"></a>3.4 分类效果</h3><p>还有个问题，就是为何fastText的分类效果常常不输于传统的非线性分类器？</p>
<p><strong>假设我们有两段文本：</strong></p>
<blockquote>
<p>我 来到 达观数据</p>
<p>俺 去了 达而观信息科技</p>
</blockquote>
<p>这两段文本意思几乎一模一样，如果要分类，肯定要分到同一个类中去。但在传统的分类器中，用来表征这两段文本的向量可能差距非常大。传统的文本分类中，你需要计算出每个词的权重，比如tfidf值， “我”和“俺” 算出的tfidf值相差可能会比较大，其它词类似，于是，VSM（向量空间模型）中用来表征这两段文本的文本向量差别可能比较大。</p>
<p>但是fastText就不一样了，它是用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度，于是，在fastText模型中，这两段文本的向量应该是非常相似的，于是，它们很大概率会被分到同一个类中。</p>
<p><strong>fastText效果好的原因：</strong></p>
<blockquote>
<ol>
<li>使用词embedding而非词本身作为特征</li>
<li>字符级n-gram特征的引入对分类效果会有一些提升 </li>
</ol>
</blockquote>
<h2 id="4-fastText-keras-实战"><a href="#4-fastText-keras-实战" class="headerlink" title="4. fastText keras 实战"></a>4. fastText keras 实战</h2><h2 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a>6. 小结</h2><ul>
<li>FastText 提出了子词嵌入方法。在 word2vec <strong>skip-gram</strong> 基础上，将中心词向量表示成单词的子词向量之和。</li>
<li>子词嵌入（subword embedding）利用构词上的规律，通常可以提升生僻词表示的质量。</li>
<li>fastText 训练时复杂度 采用层次化 softmax 之后，减少为 O(hlogK) 级别, 预测时还是 O(Kh) </li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zh.gluon.ai/chapter_natural-language-processing/fasttext.html" target="_blank" rel="external">子词嵌入（fastText）</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-05-3" target="_blank" rel="external">fastText，智慧与美貌并重的文本分类及向量化工具</a></li>
<li><a href="https://blog.csdn.net/sinat_26917383/article/details/54850933" target="_blank" rel="external">NLP︱高级词向量表达（二）——FastText（简述、学习笔记）</a></li>
<li><a href="https://blog.csdn.net/sinat_26917383/article/details/83041424" target="_blank" rel="external">如何在python 非常简单训练FastText</a></li>
<li><a href="http://www.52nlp.cn/fasttext" target="_blank" rel="external">我爱自然语言处理-fastText原理及实践</a></li>
<li><a href="https://www.jianshu.com/p/2acc49549af6" target="_blank" rel="external">FastText文本分类算法学习笔记（好文）</a></li>
<li><a href="https://blog.csdn.net/fendouaini/article/details/81086575" target="_blank" rel="external">FastText的内部机制</a></li>
<li><a href="https://blog.csdn.net/joleoy/article/details/84987230" target="_blank" rel="external">利用skift实现fasttext模型</a></li>
<li><a href="http://www.52nlp.cn/ai-challenger-2018-进行时" target="_blank" rel="external">AI Challenger 2018 进行时</a></li>
<li><a href="http://www.52nlp.cn/ai-challenger-2018-细粒度用户评论情感分析-fasttext-baseline" target="_blank" rel="external">AI Challenger 2018 细粒度用户评论情感分析 fastText Baseline</a></li>
<li><a href="http://www.52nlp.cn/ai-challenger-2018-文本挖掘类竞赛相关解决方案及代码汇总" target="_blank" rel="external">ai-challenger-2018-文本挖掘类竞赛相关解决方案及代码汇总</a></li>
<li><a href="http://www.52nlp.cn/qa问答系统中的深度学习技术实现" target="_blank" rel="external">QA问答系统中的深度学习技术实现</a></li>
<li><a href="https://blog.csdn.net/sxllllwd/article/details/81914447" target="_blank" rel="external">CSDN 层次softmax</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_weibo_icon.png);background-size: contain;display: inline-block; width:50px; height:50px" href="https://service.weibo.com/share/share.php?url" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-前置知识"><span class="toc-number"></span> <span class="toc-text">1. 前置知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Softmax-Regression"><span class="toc-number"></span> <span class="toc-text">1.1 Softmax Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Hierarchical-Softmax"><span class="toc-number"></span> <span class="toc-text">1.2 Hierarchical Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-n-gram’s-feature"><span class="toc-number"></span> <span class="toc-text">1.3 n-gram’s feature</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-word2vec-架构原理"><span class="toc-number"></span> <span class="toc-text">2. word2vec 架构原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-CBOW-模型架构"><span class="toc-number"></span> <span class="toc-text">2.1 CBOW 模型架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-前向传播"><span class="toc-number"></span> <span class="toc-text">2.2 前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-反向传播"><span class="toc-number"></span> <span class="toc-text">2.3 反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-fastText-核心思想"><span class="toc-number"></span> <span class="toc-text">3. fastText 核心思想</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-字符级-n-gram"><span class="toc-number"></span> <span class="toc-text">3.1 字符级 n-gram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-模型架构"><span class="toc-number"></span> <span class="toc-text">3.2 模型架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-核心思想"><span class="toc-number"></span> <span class="toc-text">3.3 核心思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-分类效果"><span class="toc-number"></span> <span class="toc-text">3.4 分类效果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-fastText-keras-实战"><span class="toc-number"></span> <span class="toc-text">4. fastText keras 实战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-小结"><span class="toc-number"></span> <span class="toc-text">6. 小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number"></span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/nlp/">nlp</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/fastText/">fastText</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/12/23/nlp/ELMo/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          ELMo 最好用的词向量《Deep Contextualized Word Representations》
        
      </div>
    </a>
  
  
    <a href="/2018/12/16/nlp/textCNN/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">TextCNN 文本情感分类的卷积神经网络&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Libin Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2018/12/19/nlp/fastText/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
