<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Sequence Models (week1) - Recurrent Neural Networks - Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
<meta property="og:type" content="article">
<meta property="og:title" content="Sequence Models (week1) - Recurrent Neural Networks">
<meta property="og:url" content="http://iequa.com/2018/07/26/deeplearning-ai-Sequence-Models-week1/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-1_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-2_2.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-2_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-3_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-4_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-5_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-6_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-7_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-8_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-9_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-10_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-19_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-11_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-12_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-13_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-14_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-15_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-16_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-17_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-20_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-20_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-18_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-21_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-22_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-23_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-24_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-25_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-26_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-27_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-28_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-29_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-30_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-31_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-32_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C5W1-33_1.png">
<meta property="og:updated_time" content="2018-08-04T07:42:11.193Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sequence Models (week1) - Recurrent Neural Networks">
<meta name="twitter:description" content="MathJax.Hub.Config({
    extensions: [&quot;tex2jax.js&quot;],
    jax: [&quot;input/TeX&quot;],
    tex2jax: {
      inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;] ],
      displayMath: [ [&apos;$$&apos;,&apos;$$&apos;]],
      processEscapes:">
<meta name="twitter:image" content="http://iequa.com/images/deeplearning/C5W1-1_1.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/english">English</a>
        
          <a class="main-nav-link" href="/deeplearning">DL</a>
        
          <a class="main-nav-link" href="/ai">ML</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/project_frame">Project</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-deeplearning-ai-Sequence-Models-week1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Sequence Models (week1) - Recurrent Neural Networks
      <small class=article-detail-date-index>&nbsp; 2018-07-26</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2018/07/26/deeplearning-ai-Sequence-Models-week1/" class="article-date">
  <time datetime="2018-07-26T11:00:21.000Z" itemprop="datePublished">2018-07-26</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2018/07/26/deeplearning-ai-Sequence-Models-week1/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

<p>这次我们要学习专项课程中第五门课 <strong>Sequence Models</strong>， 通过这门课的学习，你将会：</p>
<blockquote>
<ul>
<li>理解如何构建并训练循环神经网络（RNN），以及一些广泛应用的变体，例如 GRU 和 LSTM</li>
<li>能够将序列模型应用到自然语言问题中，包括文字合成.</li>
<li>能够将序列模型应用到音频应用，包括语音识别和音乐合成.</li>
</ul>
</blockquote>
<p><strong>第一周:  Recurrent Neural Networks</strong> 已被证明在时间数据上表现好，它有几个变体，包括 LSTM、GRU 和双向神经网络.</p>
<a id="more"></a>
<h2 id="1-Why-sequence-models"><a href="#1-Why-sequence-models" class="headerlink" title="1. Why sequence models?"></a>1. Why sequence models?</h2><p>为什么要学习序列模型呢? 序列模型, 普遍称为RNN(递归神经网络 - Recurrent Neural Network), 做为深度学习中 非常重要的一环，有着比普通神经网络更广的宽度与更多的可能性，其应用领域包括但不限于“语音识别”， “NLP”， “DNA序列分析”，“Machine Translation”， “视频动作分析”，等等… 有这样一种说法，也许并不严谨，但有助于我们理解RNN，大意是这样的:</p>
<blockquote>
<p>普通神经网络处理的是一维的数据，CNN处理的是二维的数据，RNN处理的是三维的数据</p>
<p>最直观的理解是在CNN对图片的分析基础上，RNN可以对视频进行分析，这里也就引入了第三维“时间”的概念</p>
</blockquote>
<p><img src="/images/deeplearning/C5W1-1_1.png" width="700"></p>
<p>这一小节通过一个小例子为我们打开序列模型的大门，例子如下:</p>
<blockquote>
<p>给出这样一个句子 “Harry Potter and Herminone Granger invented a new spell.”(哈利波特与赫敏格兰杰发明了 一个新的咒语。)， 我们的任务是在这个句子中准确的定位到人名 Harry Potter 和 Herminone Granger. 用深度学习的语言来描述如下图 - 每一个单词对应一个输出0或者1，1代表着是人名，0代表不是。</p>
</blockquote>
<p><img src="/images/deeplearning/C5W1-2_2.png" width="750"></p>
<blockquote>
<p>接下来我们要解决的一个问题是如何才能代表一个单词，比如我们例子中的“Harry”，这里我们介绍一种新的编码方式， 就是用另一种方式来代表每一个单词 - 独热编码（<strong>One-Hot Encoding</strong>）。 具体流程是这样，假设我们有 10000 个常用词，为其构建一个10000*1 的矩阵(column matrix)，假如第一个词是苹果(apple), 那么对应的第一个位置为1，其他都为0，所以称之为独热。这样每个单词都有对应的矩阵进行表示，如果这个词没有出现在我们的字典中，那么我们可以给一个特殊的符号代替，常用的是 <unk> (unknown)</unk></p>
</blockquote>
<h2 id="2-Notation"><a href="#2-Notation" class="headerlink" title="2. Notation"></a>2. Notation</h2><p>为了后面方便说明，先将会用到的数学符号进行介绍. 以下图为例，假如我们需要定位一句话中人名出现的位置.</p>
<p><img src="/images/deeplearning/C5W1-2_1.png" width="750"></p>
<blockquote>
<ul>
<li>红色框中的为输入、输出值。可以看到人名输出用1表示，反之用0表示；</li>
<li><p>绿色框中的 $x^{&lt; t >}$,$y^{&lt; t >}$ 表示对应红色框中的输入输出值的数学表示，注意从1开始.</p>
</li>
<li><p>灰色框中的 $T_x,T_y$ 分别表示输入输出序列的长度，在该例中，$T_x=9,T_y=9$</p>
</li>
<li><p>黄色框中 $X^{(i)&lt; t >}$ 上的表示<strong>第$i$个输入样本的第$t$个输入值</strong>，$T_x^{ (i) }$ 则表示第i个输入样本的长度。输出y也同理.</p>
</li>
</ul>
</blockquote>
<p>输入值中每个单词使用<strong>One-Hot</strong>来表示。即首先会构建一个字典(Dictionary), 假设该例中的字典维度是10000*1(如图示)。第一个单词”Harry”的数学表示形式即为[0,0,0,……,1 (在第4075位) ,0,……,0]，其他单词同理。</p>
<p>但是如果某一个单词并没有被包含在字典中怎么办呢？此时我们可以添加一个新的标记，也就是一个叫做 Unknown Word 的伪造单词，用 &lt;<strong>UNK</strong>&gt; 表示。具体的细节会在后面介绍。</p>
<p><img src="/images/deeplearning/C5W1-3_1.png" width="750"></p>
<h2 id="3-Recurrent-Neural-Network-Model"><a href="#3-Recurrent-Neural-Network-Model" class="headerlink" title="3. Recurrent Neural Network Model"></a>3. Recurrent Neural Network Model</h2><p>在介绍RNN之前，首先解释一下为什么之前的标准网络不再适用了。因为它有两个缺点：</p>
<ul>
<li>输入和输出的长度不尽相同</li>
<li>无法共享从其他位置学来的特征<blockquote>
<p>例如上一节中的 <strong>Harry</strong> 这个词是用$x^{<1>}$表示的，网络从该位置学习了它是一个人名。但我们希望无论 <strong>Harry</strong> 在哪个位置出现网络都能识别出这是一个人名的一部分，而标准网络无法做到这一点.</1></p>
</blockquote>
</li>
</ul>
<p><img src="/images/deeplearning/C5W1-4_1.png" width="750"></p>
<blockquote>
<p>输入层，比如每个 $x^{<1>}$ 都是一个 1000 维的向量，这样输入层很庞大, 那么第一层的权重矩阵就有着巨大的参数.</1></p>
</blockquote>
<h3 id="3-1-RNN-结构"><a href="#3-1-RNN-结构" class="headerlink" title="3.1 RNN 结构"></a>3.1 RNN 结构</h3><p>还是以识别人名为例,第一个单词 $x^{<1>}$ 输入神经网络得到输出 $y^{<1>}$</1></1></p>
<p><img src="/images/deeplearning/C5W1-5_1.png" width="90"></p>
<p>同理, 由 $x^{<2>}$ 将得到 $y^{<2>}$,以此类推。但是这就是传统网络存在的问题，即单词之间没有联系</2></2></p>
<p><img src="/images/deeplearning/C5W1-6_1.png" width="90"></p>
<p>为了将单词之间关联起来，所以将前一层的结果也作为下一层的输入数据。如下图示</p>
<p><img src="/images/deeplearning/C5W1-7_1.png" width="250"></p>
<p>整体的 RNN 结构有两种表示形式，如下图示, 左边是完整的表达形式，注意第一层的 $a^{<0>}$ 一般设置为 0向量.</0></p>
<p><img src="/images/deeplearning/C5W1-8_1.png" width="750"></p>
<blockquote>
<p>要开始整个流程, 需要编造一个激活值, 这通常是 0向量, 有些研究人员会用其他方法随机初始化 $a^{<0>}=\vec{0}$. 不过使用 0向量，作为0时刻的伪激活值 是最常见的选择. 因此我们把它输入神经网络.</0></p>
<p>(右边的示意图是RNN的简写示意图)</p>
</blockquote>
<hr>
<p>介绍完结构之后，我们还需要知道网络中参数的表达方式及其含义。如下图示，$x^{&lt;{i}&gt;}$ 到网络的参数用 $W_{ax}$ 表示，$a^{&lt;{i}&gt;}$ 到网络的参数用 $W_{aa}$ 表示，$y^{&lt;{i}&gt;}$ 到网络的参数用 $W_{ya}$ 表示，具体含义将在下面进行说明.</p>
<p><img src="/images/deeplearning/C5W1-9_1.png" width="750"></p>
<blockquote>
<p>$x^{<1>}$ 通过网络可以传递到 $y^{<3>}$</3></1></p>
<p>但是这存在一个问题，即每个输出只与前面的输入有关，而与后面的无关。这个问题会在后续内容中进行改进. </p>
<p>举个🌰: He said, “Teddy Roosevelt was a great President.”</p>
<p>对于这句话，只知道 <strong>He said</strong> 前面两个词，来判断 Teddy 是否是人名是不够的，还需后面的信息.（BRNN 可处理这问题）</p>
</blockquote>
<h3 id="3-2-RNN-Forward-Propagation"><a href="#3-2-RNN-Forward-Propagation" class="headerlink" title="3.2 RNN Forward Propagation"></a>3.2 RNN Forward Propagation</h3><p>RNN 在正向传播的过程中可以看到 <code>a</code> 的值随着时间的推移被传播了出去，也就一定程度上保存了单词之间的特性:</p>
<p><img src="/images/deeplearning/C5W1-10_1.png" width="750"></p>
<blockquote>
<p>$a^{<0>}=\vec{0}$</0></p>
<p>$a^{<1>}=g_1(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$</1></0></1></p>
<p>$y^{<1>}=g_2(W_{ya}a^{<1>}+b_y)$</1></1></p>
<p>$a^{&lt;{t}&gt;}=g_1(W_{aa}a^{&lt;{t-1}&gt;}+W_{ax}x^{&lt;{t}&gt;}+b_a)$</p>
<p>$y^{&lt;{t}&gt;}=g_2(W_{ya}a^{&lt;{t}&gt;}+b_y)$</p>
<p>激活函数：<strong>$g_1$</strong> 一般为 <strong><code>tanh</code>函数</strong> (或者是 <strong><code>Relu</code>函数</strong>)，<strong>$g_2$</strong> 一般是 <strong><code>Sigmod</code>函数</strong>.</p>
<p>注意: 参数的下标是有顺序含义的，如 $W_{ax}$ 下标的第一个参数表示要计算的量的类型，即要计算 $a$ 矢量，第二个参数表示要进行乘法运算的数据类型，即需要与 $x$ 矢量做运算。如 $W_{ax} x^{t}\rightarrow{a}$</p>
</blockquote>
<p><img src="/images/deeplearning/C5W1-19_1.png" width="750"></p>
<p><strong>Tx</strong> ， <strong>Ty</strong> 是时间单位, 这里统称为“时刻”，在这例子中对应不同时刻是输入的第几个单词， <strong>x</strong> 是“输入值”，例子中是当前时刻的单词（以独热编码的形式）， <strong>y</strong> 是“输出值”<strong>0</strong>或者<strong>1</strong>， <strong>a</strong> 称为激活值用于将前一个单元的输出结果传递到下一个单元， <strong>Wax</strong> <strong>Way</strong> <strong>Waa</strong> 是不同的“权重矩阵”也就是我们神经网络update的值。每一个单元有两个输入，$a^{&lt;{T_x-1}&gt;}$ 和 <strong>x</strong> ，有两个输出 $a^{&lt;{T_x}&gt;}$ 和 <strong>y</strong> . 图中没有出现的g是“激活函数”。</p>
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">名字</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$x$</td>
<td style="text-align:center">输入值</td>
</tr>
<tr>
<td style="text-align:center">$a$</td>
<td style="text-align:center">激活值</td>
</tr>
<tr>
<td style="text-align:center">$T_x$, $T_y$</td>
<td style="text-align:center">$x$,$y$ 时刻</td>
</tr>
<tr>
<td style="text-align:center">Wax, Way, Waa</td>
<td style="text-align:center">权重矩阵</td>
</tr>
</tbody>
</table>
<h3 id="3-3-Simplified-RNN-notation"><a href="#3-3-Simplified-RNN-notation" class="headerlink" title="3.3 Simplified RNN notation"></a>3.3 Simplified RNN notation</h3><p>下面将对如下公式进行化简：</p>
<p><img src="/images/deeplearning/C5W1-11_1.png" width="400"></p>
<p><strong>1. 简化 $a^{&lt;{t}&gt;}$</strong></p>
<p>$$<br>\begin{align}<br>a^{&lt;{t}&gt;}&amp;= g(W_{aa}a^{&lt;{t-1}&gt;}+W_{ax}x^{&lt;{t}&gt;}+b_a) \notag \\<br>&amp;= g(W_a [a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^{T}+b_a) \notag<br>\end{align}<br>$$</p>
<p><img src="/images/deeplearning/C5W1-12_1.png" width="750"></p>
<blockquote>
<p>注意，公式中使用了两个矩阵进行化简，分别是 $W_a$ 和 $[a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^T$ (使用转置符号更易理解),下面分别进行说明：</p>
</blockquote>
<p>$W_a = [ W_{aa}, W_{ax} ]$, 假设 $W_{aa}$ 是 (100,100) 的矩阵，$W_{ax}$ 是 (100,10000) 的矩阵,那么 $W$ 则是 (100,10100) 的矩阵.</p>
<p><img src="/images/deeplearning/C5W1-13_1.png" width="550"></p>
<p>$[a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^T$ 是下图示意:</p>
<p><img src="/images/deeplearning/C5W1-14_1.png" width="550"></p>
<p>故 $W_a [a^{&lt;{t-1}&gt;},x^{&lt;{t}&gt;}]^{T}$ 矩阵计算如下图示:</p>
<p><img src="/images/deeplearning/C5W1-15_1.png" width="550"></p>
<p><strong>2. 简化 $y^{&lt;{t}&gt;}$</strong></p>
<p><img src="/images/deeplearning/C5W1-16_1.png" width="550"></p>
<p>该节PPT内容：</p>
<p><img src="/images/deeplearning/C5W1-17_1.png" width="750"></p>
<p>再回顾下干净的前向传播概览图:</p>
<p><img src="/images/deeplearning/C5W1-20_1.png" width="750"></p>
<h2 id="4-Backpropagation-through-time"><a href="#4-Backpropagation-through-time" class="headerlink" title="4. Backpropagation through time"></a>4. Backpropagation through time</h2><p>RNN 的反向传播通常都由类似 Tensorflow、Torch 之类的库或者框架帮你完成，不过感官上和普通神经网络类似，算梯度值然后更新权重矩阵.</p>
<p>但是下面这里依然会对<strong>反向传播</strong>进行详细的介绍，跟着下面一张一张的图片走起来 😄😄:</p>
<h3 id="4-1-整体感受"><a href="#4-1-整体感受" class="headerlink" title="4.1 整体感受"></a>4.1 整体感受</h3><p>首先再回顾一下 RNN 的整体结构:</p>
<p><img src="/images/deeplearning/C5W1-20_1.png" width="750"></p>
<p>要进行反向传播，首先需要前向传播，传播方向如蓝色箭头所示，其次再按照红色箭头进行反向传播</p>
<p><img src="/images/deeplearning/C5W1-18_1.png" width="750"></p>
<h3 id="4-2-前向传播"><a href="#4-2-前向传播" class="headerlink" title="4.2 前向传播"></a>4.2 前向传播</h3><p>首先给出所有输入数据，即从 $x^{<1>}$ 到 $x^{&lt;{T_x}&gt;}$, $T_x$ 表示输入数据的数量.</1></p>
<p><img src="/images/deeplearning/C5W1-21_1.png" width="650"></p>
<p>初始化参数 $W_a$, $b_a$，将输入数据输入网络得到对应的 $a^{&lt;{t}&gt;}$</p>
<p><img src="/images/deeplearning/C5W1-22_1.png" width="650"></p>
<p>再通过与初始化参数 $W_y$, $b_y$ 得到 $y^{&lt;{t}&gt;}$</p>
<p><img src="/images/deeplearning/C5W1-23_1.png" width="650"></p>
<h3 id="4-3-损失函数定义"><a href="#4-3-损失函数定义" class="headerlink" title="4.3 损失函数定义"></a>4.3 损失函数定义</h3><p>要进行反向传播，必须得有损失函数嘛，所以我们将损失函数定义如下：</p>
<p><strong>每个节点的损失函数:</strong></p>
<p>$$<br>L^{&lt;{t}&gt;}(\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;})=y^{&lt;{t}&gt;}log(y^{&lt;{t}&gt;})-(1-y^{&lt;{t}&gt;})log(1-\hat{y}^{&lt;{t}&gt;})<br>$$</p>
<p><strong>整个网络的损失函数:</strong></p>
<p>$$<br>L(\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;)}) = \sum_{t=1}^{T_y}L^{&lt;{t}&gt;}(\hat{y}^{&lt;{t}&gt;},y^{&lt;{t}&gt;})<br>$$</p>
<p><img src="/images/deeplearning/C5W1-24_1.png" width="750"></p>
<h3 id="4-4-反向传播"><a href="#4-4-反向传播" class="headerlink" title="4.4 反向传播"></a>4.4 反向传播</h3><p><img src="/images/deeplearning/C5W1-25_1.png" width="750"></p>
<h3 id="4-5-整个流程图"><a href="#4-5-整个流程图" class="headerlink" title="4.5 整个流程图"></a>4.5 整个流程图</h3><p><img src="/images/deeplearning/C5W1-26_1.png" width="750"></p>
<h2 id="5-Different-types-of-RNNs"><a href="#5-Different-types-of-RNNs" class="headerlink" title="5. Different types of RNNs"></a>5. Different types of RNNs</h2><p><strong>RNN的不同应用领域:</strong></p>
<p>序列模型对输入与输出的长度没有要求，在常见的例子中，机器翻译就是多个输入与多个输出，简称“多对多”， 语音识别可视为“单对多”， 它的反例是音乐生成-“单对多”。课程中介绍了多种可能的RNN模式，我们用下面一张图概括：</p>
<p><img src="/images/deeplearning/C5W1-27_1.png" width="800"></p>
<p>RNN 不同的结构给了我们更多的可能性.</p>
<h2 id="6-Language-model-and-sequence-generation"><a href="#6-Language-model-and-sequence-generation" class="headerlink" title="6. Language model and sequence generation"></a>6. Language model and sequence generation</h2><p>语言模型和序列生成</p>
<h3 id="6-1-什么是语言模型"><a href="#6-1-什么是语言模型" class="headerlink" title="6.1 什么是语言模型"></a>6.1 什么是语言模型</h3><p>凡事开头举个🌰，一切都好说：</p>
<p>假设一个语音识别系统听一句话得到了如下两种选择，作为正常人肯定会选择第二种。但是机器才如何做判断呢？</p>
<p><img src="/images/deeplearning/C5W1-28_1.png" width="600"></p>
<p>此时就需要通过语言模型来预测每句话的概率：</p>
<p><img src="/images/deeplearning/C5W1-29_1.png" width="600"></p>
<h3 id="6-2-如何使用-RNN构建语言模型"><a href="#6-2-如何使用-RNN构建语言模型" class="headerlink" title="6.2 如何使用 RNN构建语言模型"></a>6.2 如何使用 RNN构建语言模型</h3><ol>
<li>首先我们需要一个很大的语料库(<strong>Corpus</strong>)</li>
<li>将每个单词字符化(<strong>Tokenize</strong>，<strong>即使用One-shot编码</strong>)得到词典,，假设有10000个单词</li>
<li>还需要添加两个特殊的单词<blockquote>
<ul>
<li>end of sentence. 终止符，表示句子结束.<br><img src="/images/deeplearning/C5W1-30_1.png" width="600"></li>
<li>UNknown, 之前的笔记已介绍过<br><img src="/images/deeplearning/C5W1-31_1.png" width="600"></li>
</ul>
</blockquote>
</li>
</ol>
<h3 id="6-3-构建语言模型示例"><a href="#6-3-构建语言模型示例" class="headerlink" title="6.3 构建语言模型示例"></a>6.3 构建语言模型示例</h3><p>假设要对这句话进行建模：<strong>Cats average 15 hours of sleep a day. <eos></eos></strong></p>
<p><strong>1.初始化</strong></p>
<blockquote>
<p>这一步比较特殊，即 $x^{<1>}$ 和 $a^{<0>}$ 都需要初始化为 $\vec{0}$ .<br>此时 $\hat{y}^{<1>}$ 将会对第一个字可能出现的每一个可能进行概率的判断,即 $\hat{y}^{<1>}=[p(a),…,p(cats),…]$.</1></1></0></1></p>
<p>当然在最开始的时候没有任何的依据，可能得到的是完全不相干的字，因为只是根据初始的值和激活函数做出的取样</p>
<p><img src="/images/deeplearning/C5W1-32_1.png" width="600"></p>
</blockquote>
<p><strong>2.将真实值作为输入值:</strong></p>
<blockquote>
<p>之所以将真实值作为输入值很好理解,如果我们一直传错误的值，将永远也无法得到字与字之间的关系</p>
</blockquote>
<p>如下图示，将 $y^{<1>}$ 所表示的真实值Cats作为输入，即 $x^{<2>}=y^{<1>}$ 得到 $\hat{y}^{<2>}$</2></1></2></1></p>
<p>此时的 $\hat{y}^{<2>}=[p(a|cats),…,p(average|cats),…]$</2></p>
<p>同理有 $\hat{y}^{<3>}=[p(a|cats\, average),…,p(average|cats\,average),…]$</3></p>
<p>另外输入值满足： $x^{&lt;{t}&gt;}=y^{&lt;{t-1}&gt;}$</p>
<p><img src="/images/deeplearning/C5W1-33_1.png" width="700"></p>
<p><strong>3.计算出损失值:</strong></p>
<h2 id="7-Sampling-novel-sequences"><a href="#7-Sampling-novel-sequences" class="headerlink" title="7. Sampling novel sequences"></a>7. Sampling novel sequences</h2><p>对新序列采样</p>
<h2 id="8-Vanishing-gradients-with-RNNs"><a href="#8-Vanishing-gradients-with-RNNs" class="headerlink" title="8. Vanishing gradients with RNNs"></a>8. Vanishing gradients with RNNs</h2><p>循环神经网络的梯度消失</p>
<h2 id="9-GRU-Gated-Recurrent-Unit"><a href="#9-GRU-Gated-Recurrent-Unit" class="headerlink" title="9. GRU - Gated Recurrent Unit"></a>9. GRU - Gated Recurrent Unit</h2><h2 id="10-LSTM（long-short-term-memory）unit"><a href="#10-LSTM（long-short-term-memory）unit" class="headerlink" title="10. LSTM（long short term memory）unit"></a>10. LSTM（long short term memory）unit</h2><h2 id="11-Bidirectional-RNN"><a href="#11-Bidirectional-RNN" class="headerlink" title="11. Bidirectional RNN"></a>11. Bidirectional RNN</h2><h2 id="12-Deep-RNNs"><a href="#12-Deep-RNNs" class="headerlink" title="12. Deep RNNs"></a>12. Deep RNNs</h2><h2 id="13-Reference"><a href="#13-Reference" class="headerlink" title="13. Reference"></a>13. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/tag/DeepLearning/" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
<li><a href="https://github.com/theBigDataDigest/Andrew-Ng-deeplearning-part-5-Course-notes-in-Chinese/blob/master/Andrew-Ng-deeplearning.ai-part-5-Course%20notes.pdf" target="_blank" rel="external">大数据文摘 DeepLearning.ai学习笔记</a></li>
<li><a href="https://kulbear.github.io/pdf/sequence-models.pdf" target="_blank" rel="external">Sequence Models 英文版笔记</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_weibo_icon.png);background-size: contain;display: inline-block; width:50px; height:50px" href="https://service.weibo.com/share/share.php?url" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/2017/11/05/support-pay-blog/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/2017/11/05/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  原创文章，转载请注明： 转载自<a href="http://www.iequa.com"> Blair Chan's Blog</a>，作者：
  <a href="http://www.iequa.com/about">Blair Chan</a> <br>
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>

 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Why-sequence-models"><span class="toc-number"></span> <span class="toc-text">1. Why sequence models?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Notation"><span class="toc-number"></span> <span class="toc-text">2. Notation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Recurrent-Neural-Network-Model"><span class="toc-number"></span> <span class="toc-text">3. Recurrent Neural Network Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-RNN-结构"><span class="toc-number"></span> <span class="toc-text">3.1 RNN 结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-RNN-Forward-Propagation"><span class="toc-number"></span> <span class="toc-text">3.2 RNN Forward Propagation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Simplified-RNN-notation"><span class="toc-number"></span> <span class="toc-text">3.3 Simplified RNN notation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Backpropagation-through-time"><span class="toc-number"></span> <span class="toc-text">4. Backpropagation through time</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-整体感受"><span class="toc-number"></span> <span class="toc-text">4.1 整体感受</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-前向传播"><span class="toc-number"></span> <span class="toc-text">4.2 前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-损失函数定义"><span class="toc-number"></span> <span class="toc-text">4.3 损失函数定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-反向传播"><span class="toc-number"></span> <span class="toc-text">4.4 反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-整个流程图"><span class="toc-number"></span> <span class="toc-text">4.5 整个流程图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Different-types-of-RNNs"><span class="toc-number"></span> <span class="toc-text">5. Different types of RNNs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Language-model-and-sequence-generation"><span class="toc-number"></span> <span class="toc-text">6. Language model and sequence generation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-什么是语言模型"><span class="toc-number"></span> <span class="toc-text">6.1 什么是语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-如何使用-RNN构建语言模型"><span class="toc-number"></span> <span class="toc-text">6.2 如何使用 RNN构建语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-构建语言模型示例"><span class="toc-number"></span> <span class="toc-text">6.3 构建语言模型示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Sampling-novel-sequences"><span class="toc-number"></span> <span class="toc-text">7. Sampling novel sequences</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Vanishing-gradients-with-RNNs"><span class="toc-number"></span> <span class="toc-text">8. Vanishing gradients with RNNs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-GRU-Gated-Recurrent-Unit"><span class="toc-number"></span> <span class="toc-text">9. GRU - Gated Recurrent Unit</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-LSTM（long-short-term-memory）unit"><span class="toc-number"></span> <span class="toc-text">10. LSTM（long short term memory）unit</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-Bidirectional-RNN"><span class="toc-number"></span> <span class="toc-text">11. Bidirectional RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-Deep-RNNs"><span class="toc-number"></span> <span class="toc-text">12. Deep RNNs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-Reference"><span class="toc-number"></span> <span class="toc-text">13. Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/deeplearning-ai/">deeplearning.ai</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/08/02/deeplearning-ai-Sequence-Models-week2/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Sequence Models (week2) - Natural Language Processing - Word Embeddings
        
      </div>
    </a>
  
  
    <a href="/2018/07/25/deeplearning-ai-Structured-Machine-Learning-Projects-week2/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Structured Machine Learning Projects (week2) - ML Strategy 2&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Libin Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2018/07/26/deeplearning-ai-Sequence-Models-week1/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
