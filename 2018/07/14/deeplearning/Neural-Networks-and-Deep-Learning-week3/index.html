<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Neural Networks and Deep Learning (week3) - Shallow Neural Networks - Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。
在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Networks and Deep Learning (week3) - Shallow Neural Networks">
<meta property="og:url" content="http://shopee.ai/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:description" content="正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。
在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-1_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-2_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-3_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-4_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-5_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-6_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-7_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-8_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-9_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-10_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-11_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-12_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-13_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-14.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-15_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-16_1.png">
<meta property="og:image" content="http://shopee.ai/images/deeplearning/C1W3-17_1.png">
<meta property="og:updated_time" content="2019-01-20T01:01:36.454Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Neural Networks and Deep Learning (week3) - Shallow Neural Networks">
<meta name="twitter:description" content="正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。
在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。">
<meta name="twitter:image" content="http://shopee.ai/images/deeplearning/C1W3-1_1.png">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/ai1">AI</a>
        
          <a class="main-nav-link" href="/tensorflow">TF/Keras</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shopee.ai"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-deeplearning/Neural-Networks-and-Deep-Learning-week3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Neural Networks and Deep Learning (week3) - Shallow Neural Networks
      <small class=article-detail-date-index>&nbsp; 2018-07-14</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/" class="article-date">
  <time datetime="2018-07-14T06:55:21.000Z" itemprop="datePublished">2018-07-14</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://shopee.ai/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。</p>
<p>在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。</p>
<a id="more"></a>
<h2 id="1-常用符号与基本概念"><a href="#1-常用符号与基本概念" class="headerlink" title="1. 常用符号与基本概念"></a>1. 常用符号与基本概念</h2><p><img src="/images/deeplearning/C1W3-1_1.png" width="750"></p>
<blockquote>
<p>该神经网络完全可以使用上一周所讲的计算图来表示, 和 $LR$ 计算图的区别仅仅在于多了一个 $z$ 和 $a$ 的计算而已. </p>
<p>如果你已经完全掌握了上一周的内容, 那么其实你已经知道了神经网络的前向传播, 反向传播(梯度计算)等等.</p>
<p>要注意的是各种参数, 中间变量 $(a, z)$ 的维度问题. 关于神经网络的基本概念, 这里就不赘述了. 见下图回顾一下:</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-2_1.png" width="750"></p>
<h2 id="2-神经网络中的前向传播"><a href="#2-神经网络中的前向传播" class="headerlink" title="2. 神经网络中的前向传播"></a>2. 神经网络中的前向传播</h2><blockquote>
<p>我们先以一个训练样本来看神经网络中的前向传播.<br>我们只看这个神经网络中的输入层和隐藏层的第一个激活单元(如下图右边所示). 其实这就是一个Logistic Regression. </p>
<ol>
<li>神经网络中输入层和隐藏层 (不看输出层), 这就不就是四个LR放在一起吗? </li>
<li>在 LR 中 $z$ 和 $a$ 的计算我们已经掌握了, 那么在神经网络中 $z$ 和 $a$ 又是什么呢? </li>
</ol>
<p><strong>我们记隐藏层第一个 $z$ 为 $z_1$, 第二个 $z$ 记为 $z_2$ 以此类推</strong>.<br>只要将这四个 $z$ 纵向叠加在一起称为一个<strong><code>列向量</code> 即可得到神经网络中这一层的 $z$</strong> ($a$同理).</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-3_1.png" width="750"></p>
<p>那么这一层的 $w, b$ 又是如何得到的? 别忘了, 对于参数 $w$ 来说, 它本身就是一个列项量, 那么它是如何做纵向叠加的呢? 我们只需要将其转置变成一个横向量, 再纵向叠加即可.</p>
<p><img src="/images/deeplearning/C1W3-4_1.png" width="750"></p>
<p>得到隐藏层的 $a$ 之后, 我们可以将其视为输入, 现只看神经网络的隐藏层和输出层, 我们发现这不就是个 $LR$ 嘛.</p>
<p><img src="/images/deeplearning/C1W3-5_1.png" width="750"></p>
<p>这里总结一下各种变量的维度 (注意: 这里是针对一个训练样本来说的, $n_L$ 代表的 $L$ 层的节点个数):</p>
<ul>
<li>$w.shape : (n_L, n_{(L-1)})$</li>
<li>$b.shape : (n_L, 1)$</li>
<li>$z.shape : (n_L, 1)$</li>
<li>$a.shape : (n_L, 1)$</li>
</ul>
<p>那么如果有 $m$ 个训练样本这些变量的维度又是怎样的呢. 我们思考哪些变量的维度会随着样本数的变化而变化. $w$ 是参数显然它的维度是不会变的. 而输入每一个样本都会有一个 $z$ 和 $a$, 还记得 $X$ 的形式吗? 同样地, $Z$ 就是将每个样本算出来的 $z$ 横向叠加(A同理). 具体计算过程如下图:</p>
<p><img src="/images/deeplearning/C1W3-6_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W3-7_1.png" width="750"></p>
<p><img src="/images/deeplearning/C1W3-8_1.png" width="750"></p>
<h2 id="3-神经网络中的激活函数"><a href="#3-神经网络中的激活函数" class="headerlink" title="3. 神经网络中的激活函数"></a>3. 神经网络中的激活函数</h2><p>四种常用的激活函数: Sigmoid, Tanh, ReLU, Leaky ReLU.</p>
<p>其中 sigmoid 我们已经见过了, 它的输出可以看成一个概率值, 往往用在输出层. <strong>对于中间层来说, 往往是<code>ReLU</code>的效果最好.</strong></p>
<blockquote>
<p>Tanh 数据平均值为 0，具有数据中心化的效果，几乎在任何场合都优于 Sigmoid</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-9_1.png" width="750"></p>
<p>以上激活函数的导数请自行在草稿纸上推导.</p>
<p><img src="/images/deeplearning/C1W3-10_1.png" width="750"></p>
<blockquote>
<p>derivative of <strong><code>sigmoid</code></strong> activation function</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-11_1.png" width="750"></p>
<blockquote>
<p>derivative of <strong><code>tanh</code></strong> activation function</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-12_1.png" width="750"></p>
<blockquote>
<p>derivative of <strong><code>ReLU and Leaky ReLU</code></strong> activation function</p>
</blockquote>
<p>为什么需要激活函数? 如果没有激活函数, 那么不论多少层的神经网络都只相当于一个LR. 证明如下:</p>
<blockquote>
<p><strong>it turns out that if you use a linear activation function or alternatively if you don’t have an activation function, then no matter how many layers your neural network has, always doing just computing a linear activation function, so you might as well not have any hidden layers.</strong></p>
<p>so unless you throw a non-linearity in there, then you’re not computing more interesting functions.</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-13_1.png" width="750"></p>
<blockquote>
<p>你可以在隐藏层用 tanh，输出层用 sigmoid，说明不同层的激活函数可以不一样。</p>
<p>现实情况是 : <strong>the tanh is pretty much stricly superior. never use sigmoid</strong></p>
</blockquote>
<p><strong>ReLU</strong> (rectified linear unit 矫正线性单元)</p>
<blockquote>
<p>tanh 和 sigmoid 都有一个缺点，就是 z 非常大或者非常小，函数的斜率(导数梯度)就会非常小, 梯度下降很慢.</p>
<p><strong>the slope of the function you know ends up being close to zero, and so this can slow down gradient descent</strong></p>
<p><strong>ReLU (rectified linear unit) is well</strong>, z = 0 的时候，你可以给导数赋值为 0 or 1，虽然这个点是不可微的. 但<strong>实现</strong>没有影响.</p>
<p>虽然 z &lt; 0, 的时候，斜率为0， 但在实践中，有足够多的隐藏单元 令 z &gt; 0, 对大多数训练样本来说是很快的.</p>
</blockquote>
<p>Notes:</p>
<blockquote>
<p>so the one place you might use as linear activation function, others usually in the output layer.</p>
</blockquote>
<h2 id="4-神经网络中的反向传播-back-propagation"><a href="#4-神经网络中的反向传播-back-propagation" class="headerlink" title="4. 神经网络中的反向传播 back propagation"></a>4. 神经网络中的反向传播 back propagation</h2><blockquote>
<p>反向传播最主要的就是计算梯度, 在上一周的内容中, 我们已经知道了LR梯度的计算. </p>
<p>同样的方式, 我们使用<strong>计算图</strong>来计算<strong>神经网络中的各种梯度</strong>.</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-14.png" width="750"></p>
<p>$$<br>dz^{[2]} = \frac{dL}{dz}= \frac{dL}{da^{[2]}}\frac{da^{[2]}}{dz^{[2]}}=a^{[2]}-y<br>$$</p>
<p>$$<br>dW^{[2]}=\frac{dL}{dW^{[2]}}=\frac{dL}{dz^{[2]}}\frac{dz^{[2]}}{dW^{[2]}}=dz^{[2]}a^{[1]}<br>$$</p>
<p>$$<br>db^{[2]}=\frac{dL}{db^{[2]}}=\frac{dL}{dz^{[2]}}\frac{dz^{[2]}}{db^{[2]}}=dz^{[2]}<br>$$</p>
<blockquote>
<p><strong>backward propagation :</strong></p>
</blockquote>
<p>$$<br>dz^{[1]} = \frac{dL}{dz^{[2]}}\frac{dz^{[2]}}{da^{[1]}}\frac{da^{[1]}}{dz^{[1]}}=W^{[2]T}dz^{[2]}*g^{[1]’}(z^{[1]})<br>$$</p>
<p>$$<br>dW^{[1]}=\frac{dL}{dW^{[1]}}=\frac{dL}{dz^{[1]}}\frac{dz^{[1]}}{dW^{[1]}}=dz^{[1]}x^T<br>$$</p>
<p>$$<br>db^{[1]}=\frac{dL}{db^{[1]}}=\frac{dL}{dz^{[1]}}\frac{dz^{[1]}}{db^{[1]}}=dz^{[1]}<br>$$</p>
<blockquote>
<p>Notes: $\frac{dL}{dz^{[2]}} = dz^{[2]}$ ， $\frac{dz^{[2]}}{da^{[1]}} = W^{[2]}$ ， $\frac{da^{[1]}}{dz^{[1]}}=g^{[1]’}(z^{[1]})$</p>
</blockquote>
<p>下图右边为在$m$个训练样本上的向量化表达:</p>
<p><img src="/images/deeplearning/C1W3-15_1.png" width="750"></p>
<blockquote>
<p>Notes: </p>
<ul>
<li>$n^[0]$ = input features</li>
<li>$n^[1]$ = hidden units</li>
<li>$n^[2]$ = output units</li>
</ul>
</blockquote>
<h2 id="5-神经网络中的参数初始化"><a href="#5-神经网络中的参数初始化" class="headerlink" title="5. 神经网络中的参数初始化"></a>5. 神经网络中的参数初始化</h2><p>在 LR 中我们的参数 $w$ 初始化为 0, 如果在神经网络中也是用相同的初始化, 那么一个隐藏层的每个节点都是相同的, 不论迭代多少次. 这显然是不合理的, 所以我们应该<font color="red"> <strong>随机地初始化</strong></font> $w$ 从而解决这个 sysmmetry breaking problem. 破坏对称问题</p>
<p><img src="/images/deeplearning/C1W3-16_1.png" width="750"></p>
<blockquote>
<p>具体初始化代码可参见下图, 其中 <strong>乘以 0.01</strong> 是为了让参数 $w$ 较小, 加速梯度下降 </p>
<p>如激活函数为 tanh 时, 若参数较大则 $z$ 也较大, 此时的梯度接近于 0, 更新缓慢. 如不是 tanh or sigmoid 则问题不大.</p>
<p>this is a relatively shallow neural network without too many hidden layers, so 0.01 maybe work ok.</p>
<p>finally it turns out that sometimes there can be better constants than 0.01.</p>
</blockquote>
<p><img src="/images/deeplearning/C1W3-17_1.png" width="750"></p>
<blockquote>
<p>$b$ 并没有这个 sysmmetry breaking problem, 所以可以 $np.zeros((2, 1))$</p>
</blockquote>
<h2 id="6-用Python搭建简单神经网络"><a href="#6-用Python搭建简单神经网络" class="headerlink" title="6. 用Python搭建简单神经网络"></a>6. 用Python搭建简单神经网络</h2><p>使用Python+Numpy实现一个简单的神经网络. 以下为参考代码</p>
<p>SimpleNeuralNetwork.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNeuralNetwork</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># simple neural network with one hidden layer</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_layer_size)</span>:</span></span><br><span class="line">        self.paramters = self.__parameter_initailizer(input_size, hidden_layer_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__parameter_initailizer</span><span class="params">(self, n_x, n_h)</span>:</span></span><br><span class="line">        <span class="comment"># W cannot be initialized with zeros</span></span><br><span class="line">        W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">        b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">        W2 = np.random.randn(<span class="number">1</span>, n_h) * <span class="number">0.01</span></span><br><span class="line">        b2 = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'W1'</span>: W1,<span class="string">'b1'</span>: b1,<span class="string">'W2'</span>: W2,<span class="string">'b2'</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__forward_propagation</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        W1 = self.paramters[<span class="string">'W1'</span>]</span><br><span class="line">        b1 = self.paramters[<span class="string">'b1'</span>]</span><br><span class="line">        W2 = self.paramters[<span class="string">'W2'</span>]</span><br><span class="line">        b2 = self.paramters[<span class="string">'b2'</span>]</span><br><span class="line">        <span class="comment"># forward propagation</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.tanh(Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = sigmoid(Z2)</span><br><span class="line">        cache = &#123;<span class="string">'Z1'</span>: Z1,<span class="string">'A1'</span>: A1,<span class="string">'Z2'</span>: Z2,<span class="string">'A2'</span>: A2&#125;</span><br><span class="line">        <span class="keyword">return</span> A2, cache</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__compute_cost</span><span class="params">(self, A2, Y)</span>:</span></span><br><span class="line">        m = A2.shape[<span class="number">1</span>]</span><br><span class="line">        cost = -np.sum(Y*np.log(A2) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A2)) / m</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="comment"># use the result from forward propagation and the label Y to compute cost</span></span><br><span class="line">        A2, cache = self.__forward_propagation(X)</span><br><span class="line">        cost = self.__compute_cost(A2, Y)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__backward_propagation</span><span class="params">(self, cache, Y)</span>:</span></span><br><span class="line">        A1, A2 = cache[<span class="string">'A1'</span>], cache[<span class="string">'A2'</span>]</span><br><span class="line">        W2 = self.paramters[<span class="string">'W2'</span>]</span><br><span class="line">        m = X.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># backward propagation computes gradients</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">        db2 = np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) / m</span><br><span class="line">        dZ1 = np.dot(W2.T, dZ2) * (<span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">        dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">        db1 = np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) / m</span><br><span class="line">        grads = &#123;<span class="string">'dW1'</span>: dW1,<span class="string">'db1'</span>: db1,<span class="string">'dW2'</span>: dW2,<span class="string">'db2'</span>: db2&#125;</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__update_parameters</span><span class="params">(self, grads, learning_rate)</span>:</span></span><br><span class="line">        self.paramters[<span class="string">'W1'</span>] -= learning_rate * grads[<span class="string">'dW1'</span>]</span><br><span class="line">        self.paramters[<span class="string">'b1'</span>] -= learning_rate * grads[<span class="string">'db1'</span>]</span><br><span class="line">        self.paramters[<span class="string">'W2'</span>] -= learning_rate * grads[<span class="string">'dW2'</span>]</span><br><span class="line">        self.paramters[<span class="string">'b2'</span>] -= learning_rate * grads[<span class="string">'db2'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, num_iterations, learning_rate, print_cost=False, print_num=<span class="number">100</span>)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">            <span class="comment"># forward propagation</span></span><br><span class="line">            A2, cache = self.__forward_propagation(X)</span><br><span class="line">            <span class="comment"># compute cost</span></span><br><span class="line">            cost = self.cost_function(X, Y)</span><br><span class="line">            <span class="comment"># backward propagation</span></span><br><span class="line">            grads = self.__backward_propagation(cache, Y)</span><br><span class="line">            <span class="comment"># update parameters</span></span><br><span class="line">            self.__update_parameters(grads, learning_rate)</span><br><span class="line">            <span class="comment"># print cost</span></span><br><span class="line">            <span class="keyword">if</span> i % print_num == <span class="number">0</span> <span class="keyword">and</span> print_cost:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_prob</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># result of forward_propagation is the probability</span></span><br><span class="line">        A2, _ = self.__forward_propagation(X)</span><br><span class="line">        <span class="keyword">return</span> A2</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        pred_prob = self.predict_prob(X)</span><br><span class="line">        threshold_func = np.vectorize(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        Y_prediction = threshold_func(pred_prob)</span><br><span class="line">        <span class="keyword">return</span> Y_prediction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        pred = self.predict(X)</span><br><span class="line">        <span class="keyword">return</span> len(Y[pred == Y]) / Y.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>main.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br><span class="line">X, Y = load_planar_dataset()</span><br><span class="line"><span class="comment"># Please note that the above code is from the programming assignment</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> SimpleNeuralNetwork</span><br><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line">num_iter = <span class="number">10001</span></span><br><span class="line">learning_rate = <span class="number">1.2</span></span><br><span class="line">input_size = X.shape[<span class="number">0</span>]</span><br><span class="line">hidden_layer_size = <span class="number">4</span></span><br><span class="line">clf = SimpleNeuralNetwork(input_size=input_size,</span><br><span class="line">                          hidden_layer_size=hidden_layer_size)\</span><br><span class="line">        .fit(X, Y, num_iter, learning_rate, <span class="keyword">True</span>, <span class="number">1000</span>)</span><br><span class="line">train_acc = clf.accuracy_score(X, Y)</span><br><span class="line">print(<span class="string">'training accuracy: &#123;&#125;%'</span>.format(train_acc*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># Cost after iteration 0: 0.693162</span></span><br><span class="line"><span class="comment"># Cost after iteration 1000: 0.258625</span></span><br><span class="line"><span class="comment"># Cost after iteration 2000: 0.239334</span></span><br><span class="line"><span class="comment"># Cost after iteration 3000: 0.230802</span></span><br><span class="line"><span class="comment"># Cost after iteration 4000: 0.225528</span></span><br><span class="line"><span class="comment"># Cost after iteration 5000: 0.221845</span></span><br><span class="line"><span class="comment"># Cost after iteration 6000: 0.219094</span></span><br><span class="line"><span class="comment"># Cost after iteration 7000: 0.220628</span></span><br><span class="line"><span class="comment"># Cost after iteration 8000: 0.219400</span></span><br><span class="line"><span class="comment"># Cost after iteration 9000: 0.218482</span></span><br><span class="line"><span class="comment"># Cost after iteration 10000: 0.217738</span></span><br><span class="line"><span class="comment"># training accuracy: 90.5%</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> hidden_layer_size <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]:</span><br><span class="line">    clf = SimpleNeuralNetwork(input_size=input_size,</span><br><span class="line">                               hidden_layer_size=hidden_layer_size)\</span><br><span class="line">            .fit(X, Y, num_iter, learning_rate, <span class="keyword">False</span>)</span><br><span class="line">    print(<span class="string">'&#123;&#125; hidden units, cost: &#123;&#125;, accuracy: &#123;&#125;%'</span></span><br><span class="line">           .format(hidden_layer_size,</span><br><span class="line">                   clf.cost_function(X, Y),</span><br><span class="line">                   clf.accuracy_score(X, Y)))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># 1 hidden units, cost: 0.6315593779798304, accuracy: 67.5%</span></span><br><span class="line"><span class="comment"># 2 hidden units, cost: 0.5727606525435293, accuracy: 67.25%</span></span><br><span class="line"><span class="comment"># 3 hidden units, cost: 0.2521014374551156, accuracy: 91.0%</span></span><br><span class="line"><span class="comment"># 4 hidden units, cost: 0.24703039056643344, accuracy: 91.25%</span></span><br><span class="line"><span class="comment"># 5 hidden units, cost: 0.17206481441467936, accuracy: 91.5%</span></span><br><span class="line"><span class="comment"># 20 hidden units, cost: 0.16003869681611513, accuracy: 92.25%</span></span><br><span class="line"><span class="comment"># 50 hidden units, cost: 0.16000569403994763, accuracy: 92.5%</span></span><br></pre></td></tr></table></figure>
<h2 id="7-本周内容回顾"><a href="#7-本周内容回顾" class="headerlink" title="7. 本周内容回顾"></a>7. 本周内容回顾</h2><ul>
<li>学习了神经网络的基本概念</li>
<li>掌握了神经网络中各种变量的维度</li>
<li>掌握了神经网络中的前向传播与反向传播</li>
<li>了解了神经网络中的激活函数</li>
<li>学习了神经网络中参数初始化的重要性</li>
<li>掌握了使用Python实现简单的神经网络</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="http://daniellaah.github.io/2017/deeplearning-ai-Neural-Networks-and-Deep-Learning-week3.html" target="_blank" rel="external">deeplearning.ai 专项课程一第三周</a></li>
<li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="external">Coursera - Deep Learning Specialization</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-常用符号与基本概念"><span class="toc-text">1. 常用符号与基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-神经网络中的前向传播"><span class="toc-text">2. 神经网络中的前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-神经网络中的激活函数"><span class="toc-text">3. 神经网络中的激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-神经网络中的反向传播-back-propagation"><span class="toc-text">4. 神经网络中的反向传播 back propagation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-神经网络中的参数初始化"><span class="toc-text">5. 神经网络中的参数初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-用Python搭建简单神经网络"><span class="toc-text">6. 用Python搭建简单神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-本周内容回顾"><span class="toc-text">7. 本周内容回顾</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/deeplearning-ai/">deeplearning.ai</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/07/15/deeplearning/Neural-Networks-and-Deep-Learning-week4/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Neural Networks and Deep Learning (week4) - Deep Neural Networks
        
      </div>
    </a>
  
  
    <a href="/2018/07/07/deeplearning/Neural-Networks-and-Deep-Learning-week2/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Neural Networks and Deep Learning (week2) - Neural Networks Basics&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://shopee.ai/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
