<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Neural Networks and Deep Learning (week3) - Shallow Neural Networks - Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。 在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。">
<meta name="keywords" content="deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Networks and Deep Learning (week3) - Shallow Neural Networks">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;07&#x2F;14&#x2F;deeplearning&#x2F;Neural-Networks-and-Deep-Learning-week3&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:description" content="正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。 在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-1_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-2_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-3_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-4_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-5_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-6_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-7_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-8_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-9_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-10_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-11_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-12_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-13_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-14.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-15_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-16_1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-17_1.png">
<meta property="og:updated_time" content="2019-10-20T04:30:35.022Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;deeplearning&#x2F;C1W3-1_1.png">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<!-- jiangting add start... @2020.08.30 -->
<!-- <div id="menu" class="duration-main" style="background-color: #e7e7e7"> -->
<!--   <p class="links-p" href="/">Blair</p> -->
<!--   <address class="icons"> -->
<!--     <a href="https://github.com/blair101" class="icon-font icon linkedin" target="_blank"></a> -->
<!--     <a href="https://cn.linkedin.com/pub/tianyu-dai/a8/818/44a" class="icon-font icon linkedin" target="_blank"></a> -->
<!--   </address> -->
<!--   <div class="hr1"></div> -->
<!--   <nav> -->
<!--     <div> -->
<!--     <a class="home" href="/">Home</a></div> -->
<!--     <p class="links-p" href="/">Home</p> -->
<!--     <nav class="tag-ath"> -->
<!--       <a class="proj" href="/categories">Category</a> -->
<!--       <a class="authors" href="/about">About</a> -->
<!--     </nav> -->
<!--   </nav> -->
<!--   <div class="hr2"></div> -->
<!--     <p class="links-p">Links</p> -->
<!--       <address class="links"> -->
<!--       <a class="proj" href="/article/Create-MyWorld">Projects</a> -->
<!--       <a class="friend">Friends</a></address><div class="hr3"> -->
<!--   </div> -->
<!--   <p class="end"></p> -->
<!--   <div id="menu-links" class="duration-main" style="top: -400px; background-color: #f5f5f5"> -->
<!--     <address> -->
<!--       <li><a target="_blank" href="http://lm7.xxxxxxxx.jp">Lm7</a></li> -->
<!--       <li><a target="_blank" href="http://www.pixiv.net/member.php?id=4933015">Domik</a></li> -->
<!--       <li><a target="_blank" href="http://hana-ui.moe">hana-ui</a></li> -->
<!--       <li><a target="_blank" href="http://fil.dtysky.moe">F-I-L</a></li> -->
<!--       <li><a target="_blank" href="http://paradise.dtysky.moe">Paradise</a></li> -->
<!--       <li><a target="_blank" href="http://moe-notes.dtysky.moe">MoeNotes</a></li> -->
<!--       <li><a target="_blank" href="http://kanata.dtysky.moe">Kanata</a></li> -->
<!--       <li><a target="_blank" href="http://blog.nekohand.moe">Nekohand</a></li> -->
<!--       <li><a target="_blank" href="http://www.jerryfu.net">JerryFu</a></li> -->
<!--       <li><a target="_blank" href="http://kawabangga.com">南史</a></li> -->
<!--       <li><a>Hide Links</a></li> -->
<!--       <li><a></a></li> -->
<!--     </address> -->
<!--   </div> -->
<!-- </div> -->
<!-- jiangting add end !  @2020.08.30 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/ai1">AI</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-deeplearning/Neural-Networks-and-Deep-Learning-week3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Neural Networks and Deep Learning (week3) - Shallow Neural Networks
      <small class=article-detail-date-index>&nbsp; 2018-07-14</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/" class="article-date">
  <time datetime="2018-07-14T06:55:21.000Z" itemprop="datePublished">2018-07-14</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>正式进入神经网络的学习. 当然, 我们先从简单的只有一个隐藏层的神经网络开始。</p>
<p>在学习完本周内容之后, 我们将会使用 Python 实现一个单个隐藏层的神经网络。</p>
<a id="more"></a>
<h2 id="1-常用符号与基本概念"><a class="markdownIt-Anchor" href="#1-常用符号与基本概念"></a> 1. 常用符号与基本概念</h2>
<img src="/images/deeplearning/C1W3-1_1.png" width="750" />
<blockquote>
<p>该神经网络完全可以使用上一周所讲的计算图来表示, 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LR</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span></span></span> 计算图的区别仅仅在于多了一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span> 的计算而已.</p>
<p>如果你已经完全掌握了上一周的内容, 那么其实你已经知道了神经网络的前向传播, 反向传播(梯度计算)等等.</p>
<p>要注意的是各种参数, 中间变量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(a, z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span> 的维度问题. 关于神经网络的基本概念, 这里就不赘述了. 见下图回顾一下:</p>
</blockquote>
<img src="/images/deeplearning/C1W3-2_1.png" width="750" />
<h2 id="2-神经网络中的前向传播"><a class="markdownIt-Anchor" href="#2-神经网络中的前向传播"></a> 2. 神经网络中的前向传播</h2>
<blockquote>
<p>我们先以一个训练样本来看神经网络中的前向传播.<br />
我们只看这个神经网络中的输入层和隐藏层的第一个激活单元(如下图右边所示). 其实这就是一个Logistic Regression.</p>
<ol>
<li>神经网络中输入层和隐藏层 (不看输出层), 这就不就是四个LR放在一起吗?</li>
<li>在 LR 中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span> 的计算我们已经掌握了, 那么在神经网络中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span> 又是什么呢?</li>
</ol>
<p><strong>我们记隐藏层第一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mi mathvariant="normal">_</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">z\_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.95444em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">1</span></span></span></span>, 第二个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 记为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mi mathvariant="normal">_</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">z\_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.95444em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">2</span></span></span></span> 以此类推</strong>.<br />
只要将这四个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 纵向叠加在一起称为一个**<code>列向量</code> 即可得到神经网络中这一层的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span>** (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span>同理).</p>
</blockquote>
<img src="/images/deeplearning/C1W3-3_1.png" width="750" />
<p>那么这一层的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo separator="true">,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">w, b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span></span></span></span> 又是如何得到的? 别忘了, 对于参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 来说, 它本身就是一个列项量, 那么它是如何做纵向叠加的呢? 我们只需要将其转置变成一个横向量, 再纵向叠加即可.</p>
<img src="/images/deeplearning/C1W3-4_1.png" width="750" />
<p>得到隐藏层的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span> 之后, 我们可以将其视为输入, 现只看神经网络的隐藏层和输出层, 我们发现这不就是个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LR</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span></span></span> 嘛.</p>
<img src="/images/deeplearning/C1W3-5_1.png" width="750" />
<p>这里总结一下各种变量的维度 (注意: 这里是针对一个训练样本来说的, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mi mathvariant="normal">_</mi><mi>L</mi></mrow><annotation encoding="application/x-tex">n\_L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.99333em;vertical-align:-0.31em;"></span><span class="mord mathdefault">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">L</span></span></span></span> 代表的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span></span></span></span> 层的节点个数):</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo>:</mo><mo stretchy="false">(</mo><mi>n</mi><mi mathvariant="normal">_</mi><mi>L</mi><mo separator="true">,</mo><mi>n</mi><mi mathvariant="normal">_</mi><mrow><mo stretchy="false">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w.shape : (n\_L, n\_{(L-1)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord">.</span><span class="mord mathdefault">s</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">p</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mopen">(</span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span><span class="mclose">)</span></span><span class="mclose">)</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo>:</mo><mo stretchy="false">(</mo><mi>n</mi><mi mathvariant="normal">_</mi><mi>L</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">b.shape : (n\_L, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">b</span><span class="mord">.</span><span class="mord mathdefault">s</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">p</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo>:</mo><mo stretchy="false">(</mo><mi>n</mi><mi mathvariant="normal">_</mi><mi>L</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z.shape : (n\_L, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mord">.</span><span class="mord mathdefault">s</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">p</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo>:</mo><mo stretchy="false">(</mo><mi>n</mi><mi mathvariant="normal">_</mi><mi>L</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a.shape : (n\_L, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">a</span><span class="mord">.</span><span class="mord mathdefault">s</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">p</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></li>
</ul>
<p>那么如果有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">m</span></span></span></span> 个训练样本这些变量的维度又是怎样的呢. 我们思考哪些变量的维度会随着样本数的变化而变化. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 是参数显然它的维度是不会变的. 而输入每一个样本都会有一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span>, 还记得 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> 的形式吗? 同样地, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">Z</span></span></span></span> 就是将每个样本算出来的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 横向叠加(A同理). 具体计算过程如下图:</p>
<img src="/images/deeplearning/C1W3-6_1.png" width="750" />
<img src="/images/deeplearning/C1W3-7_1.png" width="750" />
<img src="/images/deeplearning/C1W3-8_1.png" width="750" />
<h2 id="3-神经网络中的激活函数"><a class="markdownIt-Anchor" href="#3-神经网络中的激活函数"></a> 3. 神经网络中的激活函数</h2>
<p>四种常用的激活函数: Sigmoid, Tanh, ReLU, Leaky ReLU.</p>
<p>其中 sigmoid 我们已经见过了, 它的输出可以看成一个概率值, 往往用在输出层. <strong>对于中间层来说, 往往是<code>ReLU</code>的效果最好.</strong></p>
<blockquote>
<p>Tanh 数据平均值为 0，具有数据中心化的效果，几乎在任何场合都优于 Sigmoid</p>
</blockquote>
<img src="/images/deeplearning/C1W3-9_1.png" width="750" />
<p>以上激活函数的导数请自行在草稿纸上推导.</p>
<img src="/images/deeplearning/C1W3-10_1.png" width="750" />
<blockquote>
<p>derivative of <strong><code>sigmoid</code></strong> activation function</p>
</blockquote>
<img src="/images/deeplearning/C1W3-11_1.png" width="750" />
<blockquote>
<p>derivative of <strong><code>tanh</code></strong> activation function</p>
</blockquote>
<img src="/images/deeplearning/C1W3-12_1.png" width="750" />
<blockquote>
<p>derivative of <strong><code>ReLU and Leaky ReLU</code></strong> activation function</p>
</blockquote>
<p>为什么需要激活函数? 如果没有激活函数, 那么不论多少层的神经网络都只相当于一个LR. 证明如下:</p>
<blockquote>
<p><strong>it turns out that if you use a linear activation function or alternatively if you don’t have an activation function, then no matter how many layers your neural network has, always doing just computing a linear activation function, so you might as well not have any hidden layers.</strong></p>
<p>so unless you throw a non-linearity in there, then you’re not computing more interesting functions.</p>
</blockquote>
<img src="/images/deeplearning/C1W3-13_1.png" width="750" />
<blockquote>
<p>你可以在隐藏层用 tanh，输出层用 sigmoid，说明不同层的激活函数可以不一样。</p>
<p>现实情况是 : <strong>the tanh is pretty much stricly superior. never use sigmoid</strong></p>
</blockquote>
<p><strong>ReLU</strong> (rectified linear unit 矫正线性单元)</p>
<blockquote>
<p>tanh 和 sigmoid 都有一个缺点，就是 z 非常大或者非常小，函数的斜率(导数梯度)就会非常小, 梯度下降很慢.</p>
<p><strong>the slope of the function you know ends up being close to zero, and so this can slow down gradient descent</strong></p>
<p><strong>ReLU (rectified linear unit) is well</strong>, z = 0 的时候，你可以给导数赋值为 0 or 1，虽然这个点是不可微的. 但<strong>实现</strong>没有影响.</p>
<p>虽然 z &lt; 0, 的时候，斜率为0， 但在实践中，有足够多的隐藏单元 令 z &gt; 0, 对大多数训练样本来说是很快的.</p>
</blockquote>
<p>Notes:</p>
<blockquote>
<p>so the one place you might use as linear activation function, others usually in the output layer.</p>
</blockquote>
<h2 id="4-神经网络中的反向传播-back-propagation"><a class="markdownIt-Anchor" href="#4-神经网络中的反向传播-back-propagation"></a> 4. 神经网络中的反向传播 back propagation</h2>
<blockquote>
<p>反向传播最主要的就是计算梯度, 在上一周的内容中, 我们已经知道了LR梯度的计算.</p>
<p>同样的方式, 我们使用<strong>计算图</strong>来计算<strong>神经网络中的各种梯度</strong>.</p>
</blockquote>
<img src="/images/deeplearning/C1W3-14.png" width="750" />
<p class='katex-block katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 5: dz^{\̲[̲2\]} = \frac{dL…'>dz^{\[2\]} = \frac{dL}{dz}= \frac{dL}{da^{\[2\]}}\frac{da^{\[2\]}}{dz^{\[2\]}}=a^{\[2\]}-y
</p>
<p class='katex-block katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 5: dW^{\̲[̲2\]}=\frac{dL}{…'>dW^{\[2\]}=\frac{dL}{dW^{\[2\]}}=\frac{dL}{dz^{\[2\]}}\frac{dz^{\[2\]}}{dW^{\[2\]}}=dz^{\[2\]}a^{\[1\]}
</p>
<p class='katex-block katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 5: db^{\̲[̲2\]}=\frac{dL}{…'>db^{\[2\]}=\frac{dL}{db^{\[2\]}}=\frac{dL}{dz^{\[2\]}}\frac{dz^{\[2\]}}{db^{\[2\]}}=dz^{\[2\]}
</p>
<blockquote>
<p><strong>backward propagation :</strong></p>
</blockquote>
<p class='katex-block katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 5: dz^{\̲[̲1\]} = \frac{dL…'>dz^{\[1\]} = \frac{dL}{dz^{\[2\]}}\frac{dz^{\[2\]}}{da^{\[1\]}}\frac{da^{\[1\]}}{dz^{\[1\]}}=W^{\[2\]T}dz^{\[2\]}*g^{\[1\]’}(z^{\[1\]})
</p>
<p class='katex-block katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 5: dW^{\̲[̲1\]}=\frac{dL}{…'>dW^{\[1\]}=\frac{dL}{dW^{\[1\]}}=\frac{dL}{dz^{\[1\]}}\frac{dz^{\[1\]}}{dW^{\[1\]}}=dz^{\[1\]}x^T
</p>
<p class='katex-block katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 5: db^{\̲[̲1\]}=\frac{dL}{…'>db^{\[1\]}=\frac{dL}{db^{\[1\]}}=\frac{dL}{dz^{\[1\]}}\frac{dz^{\[1\]}}{db^{\[1\]}}=dz^{\[1\]}
</p>
<blockquote>
<p>Notes: <span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 15: \frac{dL}{dz^{\̲[̲2\]}} = dz^{\[2…'>\frac{dL}{dz^{\[2\]}} = dz^{\[2\]}</span> ， <span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 11: \frac{dz^{\̲[̲2\]}}{da^{\[1\]…'>\frac{dz^{\[2\]}}{da^{\[1\]}} = W^{\[2\]}</span> ， <span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 11: \frac{da^{\̲[̲1\]}}{dz^{\[1\]…'>\frac{da^{\[1\]}}{dz^{\[1\]}}=g^{\[1\]’}(z^{\[1\]})</span></p>
</blockquote>
<p>下图右边为在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">m</span></span></span></span>个训练样本上的向量化表达:</p>
<img src="/images/deeplearning/C1W3-15_1.png" width="750" />
<blockquote>
<p>Notes:</p>
<ul>
<li><span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 3: n^\̲[̲0\]'>n^\[0\]</span> = input features</li>
<li><span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 3: n^\̲[̲1\]'>n^\[1\]</span> = hidden units</li>
<li><span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \[ at position 3: n^\̲[̲2\]'>n^\[2\]</span> = output units</li>
</ul>
</blockquote>
<h2 id="5-神经网络中的参数初始化"><a class="markdownIt-Anchor" href="#5-神经网络中的参数初始化"></a> 5. 神经网络中的参数初始化</h2>
<p>在 LR 中我们的参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 初始化为 0, 如果在神经网络中也是用相同的初始化, 那么一个隐藏层的每个节点都是相同的, 不论迭代多少次. 这显然是不合理的, 所以我们应该<font color="red"> <strong>随机地初始化</strong></font> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 从而解决这个 sysmmetry breaking problem. 破坏对称问题</p>
<img src="/images/deeplearning/C1W3-16_1.png" width="750" />
<blockquote>
<p>具体初始化代码可参见下图, 其中 <strong>乘以 0.01</strong> 是为了让参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 较小, 加速梯度下降</p>
<p>如激活函数为 tanh 时, 若参数较大则 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 也较大, 此时的梯度接近于 0, 更新缓慢. 如不是 tanh or sigmoid 则问题不大.</p>
<p>this is a relatively shallow neural network without too many hidden layers, so 0.01 maybe work ok.</p>
<p>finally it turns out that sometimes there can be better constants than 0.01.</p>
</blockquote>
<img src="/images/deeplearning/C1W3-17_1.png" width="750" />
<blockquote>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span> 并没有这个 sysmmetry breaking problem, 所以可以 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mn>2</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">np.zeros((2, 1))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">p</span><span class="mord">.</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mopen">(</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></p>
</blockquote>
<h2 id="6-用python搭建简单神经网络"><a class="markdownIt-Anchor" href="#6-用python搭建简单神经网络"></a> 6. 用Python搭建简单神经网络</h2>
<p>使用Python+Numpy实现一个简单的神经网络. 以下为参考代码</p>
<p><a href="http://SimpleNeuralNetwork.py" target="_blank" rel="noopener">SimpleNeuralNetwork.py</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNeuralNetwork</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># simple neural network with one hidden layer</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_layer_size)</span>:</span></span><br><span class="line">        self.paramters = self.__parameter_initailizer(input_size, hidden_layer_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__parameter_initailizer</span><span class="params">(self, n_x, n_h)</span>:</span></span><br><span class="line">        <span class="comment"># W cannot be initialized with zeros</span></span><br><span class="line">        W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">        b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">        W2 = np.random.randn(<span class="number">1</span>, n_h) * <span class="number">0.01</span></span><br><span class="line">        b2 = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'W1'</span>: W1,<span class="string">'b1'</span>: b1,<span class="string">'W2'</span>: W2,<span class="string">'b2'</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__forward_propagation</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        W1 = self.paramters[<span class="string">'W1'</span>]</span><br><span class="line">        b1 = self.paramters[<span class="string">'b1'</span>]</span><br><span class="line">        W2 = self.paramters[<span class="string">'W2'</span>]</span><br><span class="line">        b2 = self.paramters[<span class="string">'b2'</span>]</span><br><span class="line">        <span class="comment"># forward propagation</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.tanh(Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = sigmoid(Z2)</span><br><span class="line">        cache = &#123;<span class="string">'Z1'</span>: Z1,<span class="string">'A1'</span>: A1,<span class="string">'Z2'</span>: Z2,<span class="string">'A2'</span>: A2&#125;</span><br><span class="line">        <span class="keyword">return</span> A2, cache</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__compute_cost</span><span class="params">(self, A2, Y)</span>:</span></span><br><span class="line">        m = A2.shape[<span class="number">1</span>]</span><br><span class="line">        cost = -np.sum(Y*np.log(A2) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A2)) / m</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="comment"># use the result from forward propagation and the label Y to compute cost</span></span><br><span class="line">        A2, cache = self.__forward_propagation(X)</span><br><span class="line">        cost = self.__compute_cost(A2, Y)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__backward_propagation</span><span class="params">(self, cache, Y)</span>:</span></span><br><span class="line">        A1, A2 = cache[<span class="string">'A1'</span>], cache[<span class="string">'A2'</span>]</span><br><span class="line">        W2 = self.paramters[<span class="string">'W2'</span>]</span><br><span class="line">        m = X.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># backward propagation computes gradients</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">        db2 = np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br><span class="line">        dZ1 = np.dot(W2.T, dZ2) * (<span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">        dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">        db1 = np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br><span class="line">        grads = &#123;<span class="string">'dW1'</span>: dW1,<span class="string">'db1'</span>: db1,<span class="string">'dW2'</span>: dW2,<span class="string">'db2'</span>: db2&#125;</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__update_parameters</span><span class="params">(self, grads, learning_rate)</span>:</span></span><br><span class="line">        self.paramters[<span class="string">'W1'</span>] -= learning_rate * grads[<span class="string">'dW1'</span>]</span><br><span class="line">        self.paramters[<span class="string">'b1'</span>] -= learning_rate * grads[<span class="string">'db1'</span>]</span><br><span class="line">        self.paramters[<span class="string">'W2'</span>] -= learning_rate * grads[<span class="string">'dW2'</span>]</span><br><span class="line">        self.paramters[<span class="string">'b2'</span>] -= learning_rate * grads[<span class="string">'db2'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, num_iterations, learning_rate, print_cost=False, print_num=<span class="number">100</span>)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">            <span class="comment"># forward propagation</span></span><br><span class="line">            A2, cache = self.__forward_propagation(X)</span><br><span class="line">            <span class="comment"># compute cost</span></span><br><span class="line">            cost = self.cost_function(X, Y)</span><br><span class="line">            <span class="comment"># backward propagation</span></span><br><span class="line">            grads = self.__backward_propagation(cache, Y)</span><br><span class="line">            <span class="comment"># update parameters</span></span><br><span class="line">            self.__update_parameters(grads, learning_rate)</span><br><span class="line">            <span class="comment"># print cost</span></span><br><span class="line">            <span class="keyword">if</span> i % print_num == <span class="number">0</span> <span class="keyword">and</span> print_cost:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_prob</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># result of forward_propagation is the probability</span></span><br><span class="line">        A2, _ = self.__forward_propagation(X)</span><br><span class="line">        <span class="keyword">return</span> A2</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        pred_prob = self.predict_prob(X)</span><br><span class="line">        threshold_func = np.vectorize(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        Y_prediction = threshold_func(pred_prob)</span><br><span class="line">        <span class="keyword">return</span> Y_prediction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        pred = self.predict(X)</span><br><span class="line">        <span class="keyword">return</span> len(Y[pred == Y]) / Y.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p><a href="http://main.py" target="_blank" rel="noopener">main.py</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br><span class="line">X, Y = load_planar_dataset()</span><br><span class="line"><span class="comment"># Please note that the above code is from the programming assignment</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> SimpleNeuralNetwork</span><br><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line">num_iter = <span class="number">10001</span></span><br><span class="line">learning_rate = <span class="number">1.2</span></span><br><span class="line">input_size = X.shape[<span class="number">0</span>]</span><br><span class="line">hidden_layer_size = <span class="number">4</span></span><br><span class="line">clf = SimpleNeuralNetwork(input_size=input_size,</span><br><span class="line">                          hidden_layer_size=hidden_layer_size)\</span><br><span class="line">        .fit(X, Y, num_iter, learning_rate, <span class="literal">True</span>, <span class="number">1000</span>)</span><br><span class="line">train_acc = clf.accuracy_score(X, Y)</span><br><span class="line">print(<span class="string">'training accuracy: &#123;&#125;%'</span>.format(train_acc*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># Cost after iteration 0: 0.693162</span></span><br><span class="line"><span class="comment"># Cost after iteration 1000: 0.258625</span></span><br><span class="line"><span class="comment"># Cost after iteration 2000: 0.239334</span></span><br><span class="line"><span class="comment"># Cost after iteration 3000: 0.230802</span></span><br><span class="line"><span class="comment"># Cost after iteration 4000: 0.225528</span></span><br><span class="line"><span class="comment"># Cost after iteration 5000: 0.221845</span></span><br><span class="line"><span class="comment"># Cost after iteration 6000: 0.219094</span></span><br><span class="line"><span class="comment"># Cost after iteration 7000: 0.220628</span></span><br><span class="line"><span class="comment"># Cost after iteration 8000: 0.219400</span></span><br><span class="line"><span class="comment"># Cost after iteration 9000: 0.218482</span></span><br><span class="line"><span class="comment"># Cost after iteration 10000: 0.217738</span></span><br><span class="line"><span class="comment"># training accuracy: 90.5%</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> hidden_layer_size <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]:</span><br><span class="line">    clf = SimpleNeuralNetwork(input_size=input_size,</span><br><span class="line">                               hidden_layer_size=hidden_layer_size)\</span><br><span class="line">            .fit(X, Y, num_iter, learning_rate, <span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">'&#123;&#125; hidden units, cost: &#123;&#125;, accuracy: &#123;&#125;%'</span></span><br><span class="line">           .format(hidden_layer_size,</span><br><span class="line">                   clf.cost_function(X, Y),</span><br><span class="line">                   clf.accuracy_score(X, Y)))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># 1 hidden units, cost: 0.6315593779798304, accuracy: 67.5%</span></span><br><span class="line"><span class="comment"># 2 hidden units, cost: 0.5727606525435293, accuracy: 67.25%</span></span><br><span class="line"><span class="comment"># 3 hidden units, cost: 0.2521014374551156, accuracy: 91.0%</span></span><br><span class="line"><span class="comment"># 4 hidden units, cost: 0.24703039056643344, accuracy: 91.25%</span></span><br><span class="line"><span class="comment"># 5 hidden units, cost: 0.17206481441467936, accuracy: 91.5%</span></span><br><span class="line"><span class="comment"># 20 hidden units, cost: 0.16003869681611513, accuracy: 92.25%</span></span><br><span class="line"><span class="comment"># 50 hidden units, cost: 0.16000569403994763, accuracy: 92.5%</span></span><br></pre></td></tr></table></figure>
<h2 id="7-本周内容回顾"><a class="markdownIt-Anchor" href="#7-本周内容回顾"></a> 7. 本周内容回顾</h2>
<ul>
<li>学习了神经网络的基本概念</li>
<li>掌握了神经网络中各种变量的维度</li>
<li>掌握了神经网络中的前向传播与反向传播</li>
<li>了解了神经网络中的激活函数</li>
<li>学习了神经网络中参数初始化的重要性</li>
<li>掌握了使用Python实现简单的神经网络</li>
</ul>
<h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2>
<ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="noopener">网易云课堂 - deeplearning</a></li>
<li><a href="http://daniellaah.github.io/2017/deeplearning-ai-Neural-Networks-and-Deep-Learning-week3.html" target="_blank" rel="noopener">deeplearning.ai 专项课程一第三周</a></li>
<li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Coursera - Deep Learning Specialization</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io" target="_blank" rel="noopener"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-常用符号与基本概念"><span class="toc-text"> 1. 常用符号与基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-神经网络中的前向传播"><span class="toc-text"> 2. 神经网络中的前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-神经网络中的激活函数"><span class="toc-text"> 3. 神经网络中的激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-神经网络中的反向传播-back-propagation"><span class="toc-text"> 4. 神经网络中的反向传播 back propagation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-神经网络中的参数初始化"><span class="toc-text"> 5. 神经网络中的参数初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-用python搭建简单神经网络"><span class="toc-text"> 6. 用Python搭建简单神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-本周内容回顾"><span class="toc-text"> 7. 本周内容回顾</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-text"> Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/deeplearning-ai/" rel="tag">deeplearning.ai</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/07/15/deeplearning/Neural-Networks-and-Deep-Learning-week4/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Neural Networks and Deep Learning (week4) - Deep Neural Networks
        
      </div>
    </a>
  
  
    <a href="/2018/07/07/deeplearning/Neural-Networks-and-Deep-Learning-week2/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Neural Networks and Deep Learning (week2) - Neural Networks Basics&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2018/07/14/deeplearning/Neural-Networks-and-Deep-Learning-week3/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
