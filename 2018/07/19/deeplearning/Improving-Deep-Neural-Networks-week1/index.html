<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Improving Deep Neural Networks (week1) - 深度学习的实用层面 - Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这次我们要学习专项课程中第二门课 Improving Deep Neural Networks
学完这门课之后，你将会:


能够高效地使用神经网络通用的技巧，包括 初始化、L2和dropout正则化、Batch归一化、梯度检验。
能够实现并应用各种优化算法，例如 Mini-batch、Momentum、RMSprop、Adam，并检查它们的收敛程度。
理解深度学习时代关于如何 构建训练/开发/测">
<meta property="og:type" content="article">
<meta property="og:title" content="Improving Deep Neural Networks (week1) - 深度学习的实用层面">
<meta property="og:url" content="http://iequa.com/2018/07/19/deeplearning/Improving-Deep-Neural-Networks-week1/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="这次我们要学习专项课程中第二门课 Improving Deep Neural Networks
学完这门课之后，你将会:


能够高效地使用神经网络通用的技巧，包括 初始化、L2和dropout正则化、Batch归一化、梯度检验。
能够实现并应用各种优化算法，例如 Mini-batch、Momentum、RMSprop、Adam，并检查它们的收敛程度。
理解深度学习时代关于如何 构建训练/开发/测">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-1_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-2_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-3_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-4_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-5_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-6_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-7_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-8_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-9_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-10_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-11_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-11_2.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-12_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-13_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-14_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-15_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-16_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-16_2.jpg">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-17_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-18_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-19_1.png">
<meta property="og:image" content="http://iequa.com/images/deeplearning/C2W1-20_1.png">
<meta property="og:updated_time" content="2019-01-20T01:01:36.452Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving Deep Neural Networks (week1) - 深度学习的实用层面">
<meta name="twitter:description" content="这次我们要学习专项课程中第二门课 Improving Deep Neural Networks
学完这门课之后，你将会:


能够高效地使用神经网络通用的技巧，包括 初始化、L2和dropout正则化、Batch归一化、梯度检验。
能够实现并应用各种优化算法，例如 Mini-batch、Momentum、RMSprop、Adam，并检查它们的收敛程度。
理解深度学习时代关于如何 构建训练/开发/测">
<meta name="twitter:image" content="http://iequa.com/images/deeplearning/C2W1-1_1.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/chatbot">Bot</a>
        
          <a class="main-nav-link" href="/deeplearning">Deep Learning</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-deeplearning/Improving-Deep-Neural-Networks-week1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Improving Deep Neural Networks (week1) - 深度学习的实用层面
      <small class=article-detail-date-index>&nbsp; 2018-07-19</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2018/07/19/deeplearning/Improving-Deep-Neural-Networks-week1/" class="article-date">
  <time datetime="2018-07-19T12:00:21.000Z" itemprop="datePublished">2018-07-19</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2018/07/19/deeplearning/Improving-Deep-Neural-Networks-week1/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>这次我们要学习专项课程中第二门课 Improving Deep Neural Networks</p>
<p>学完这门课之后，你将会:</p>
<blockquote>
<ul>
<li>能够高效地使用神经网络<strong>通用</strong>的技巧，包括 <code>初始化、L2和dropout正则化、Batch归一化、梯度检验</code>。</li>
<li>能够实现并应用各种<strong>优化</strong>算法，例如 <code>Mini-batch、Momentum、RMSprop、Adam，并检查它们的收敛程度</code>。</li>
<li>理解深度学习时代关于如何 <strong>构建训练/开发/测试集</strong> 以及 <strong>偏差/方差分析</strong> 最新最有效的方法.</li>
<li>能够用TensorFlow实现一个神经网络</li>
</ul>
</blockquote>
<p>这门课将会详尽地介绍深度学习的基本原理，而不仅仅只进行理论概述.</p>
<a id="more"></a>
<p>本周主要内容包括:</p>
<blockquote>
<ol>
<li>Data set partition</li>
<li>Bias / Variance</li>
<li>Regularization</li>
<li>Normalization</li>
<li>Gradient Checking</li>
</ol>
</blockquote>
<h2 id="1-Train-dev-test"><a href="#1-Train-dev-test" class="headerlink" title="1. Train/dev/test"></a>1. Train/dev/test</h2><p>在上一周的内容中, 介绍了神经网络中的常用符号以及各种变量的维度. 不清楚的可以回顾上周的笔记内容.</p>
<h3 id="1-1-Data-set-partition"><a href="#1-1-Data-set-partition" class="headerlink" title="1.1 Data set partition"></a>1.1 Data set partition</h3><p>在训练完一个模型时, 我们需要知道这个模型预测的效果. 此时就需要一个额外的数据集, 我们称为 dev/hold out/validation set, 这里我们就统一称之为<code>验证集</code>. </p>
<blockquote>
<p>如果我们需要知道模型最终效果的无偏估计, 那么我们还需要一个测试集. </p>
<p>在以往传统的机器学习中, 我们通常按照 70/30 来数据集分为 <code>Train set</code>/<code>Validation set</code>, 或者按照 60/20/20 的比例分为 <code>Train/Validation/Test</code>. </p>
<p>但在今天机器学习问题中, 我们可用的<strong>数据集的量级非常大</strong> (例如有 100W 个样本). 这时我们就<strong>不需要给验证集和测试集太大的比例, 例如 98/1/1</strong>.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-1_1.png" width="750"></p>
<h3 id="1-2-Data-src-distribution"><a href="#1-2-Data-src-distribution" class="headerlink" title="1.2 Data src distribution"></a>1.2 Data src distribution</h3><p>在划分数据集中, 有一个比较常见的错误就是不小心使得在<code>训练集</code>中的数据和<code>验证</code>或<code>测试</code>集中的数据来自于不同的分布. 例如我们想要做一个猫的分类器, 在划分数据的时候发现<code>训练集</code>中的图片全都是来自于网页, 而<code>验证集</code>和<code>测试集</code>中的数据全都来自于用户. 这是一种完全错误的做法, 在实际中一定要杜绝.</p>
<p><img src="/images/deeplearning/C2W1-2_1.png" width="750"></p>
<h2 id="2-Bias-Variance"><a href="#2-Bias-Variance" class="headerlink" title="2. Bias / Variance"></a>2. Bias / Variance</h2><p>关于 Bias / Variance 相比大家都很熟悉了, 在机器学习的课程中也已经学习到. 下面祭出 Andrew Ng 经典的图例解释:</p>
<p><img src="/images/deeplearning/C2W1-3_1.png" width="750"></p>
<p>我们该如何定位模型所处的问题? 如下图所示, 这里举了四中情况下的训练集和验证集误差.</p>
<ul>
<li>当 训练误差很小, 验证误差很大时 为 High Variance</li>
<li>当 训练误差 和 验证误差 接近 且 都很大 时为 High Bias</li>
<li>当 训练误差很大, 验证误差更大时为 High Variance &amp;&amp; High Bias</li>
<li>当 训练误差 和 验证误差接近且都很小时为 Low Variance &amp;&amp; Low Bias</li>
</ul>
<p><img src="/images/deeplearning/C2W1-4_1.png" width="750"></p>
<p>关于高方差高偏差可能是第一次听过, 如下图所示, 整体上模型处于高偏差, 但是对于一些噪声又拟合地很好. 此时就处于高偏差高方差的状态.</p>
<p><img src="/images/deeplearning/C2W1-5_1.png" width="750"></p>
<p>当我们学会定位模型的问题后, 那么该怎样解决对应的问题呢? 见下图:</p>
<p><img src="/images/deeplearning/C2W1-6_1.png" width="750"></p>
<blockquote>
<p>若 <strong>High bias</strong>, 我们可以增加模型的复杂度<strong>例如使用一个”更大”的网络结构或者训练更久一点</strong>.<br>如 <strong>High variance</strong>, 我们可以想办法 <strong>get more data</strong>, 或者使用接下来我们要讲的 <code>Regularization</code>.</p>
</blockquote>
<h2 id="3-Regularization"><a href="#3-Regularization" class="headerlink" title="3. Regularization"></a>3. Regularization</h2><p>为什么正则化没有加 $\frac{\lambda}{2m} b^2$:</p>
<blockquote>
<p>因为 $w$ 通常是一个高维参数矢量, 已经可以表达 <strong>High bias</strong> 的问题, $w$ 可能含有很多参数，我们不可能拟合所有参数, 而 $b$ 只是单个数字, 所以 $w$ 几乎覆盖了所有参数，而不是 $b$, 如果加了 $b$ 也没有影响，因为 $b$ 只是众多参数中的一个.</p>
</blockquote>
<p>关于 L1 regularization :</p>
<blockquote>
<p>如果用的是 L1 regularization, then $w$ will end up being sprase 稀疏的, 也就是说 $w$ 向量中有很多 0. 有人说这样有利于压缩模型，但是我觉得不是很合适. 越来越多的人使用 L2.</p>
<p>Notes: 不称为:矩阵 L2 范数， 按照惯例我们称为: <strong>Frobenius norm of a matrix</strong>, 其实就是 : 矩阵 L2 范数。</p>
</blockquote>
<h3 id="3-1-L2-regularization"><a href="#3-1-L2-regularization" class="headerlink" title="3.1 L2 regularization"></a>3.1 L2 regularization</h3><blockquote>
<p>L2 regularization 下的 Cost Function 如下所示, 只需要添加正则项 <strong>$\frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2_F$</strong>, 其中 F 代表 Frobenius Norm. 在添加了正则项之后, 相应的梯度也要变化, 所以在更新参数的时候需要加上对应的项. 这里注意一点, 我们只对参数 $w$ 正则, 而不对 $b$. 因为对于每一层来说, $w$ 有很高的维度, 而 $b$ 只是一个标量. $w$ 对整个模型的影响远大于 $b$.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-7_1.png" width="750"></p>
<p>下面给出添加 regularization 为什么能防止过拟合给出直观的解释. 如下图所示:</p>
<blockquote>
<p>当我们的 λ 比较大的时候, 模型就会加大对 w 的惩罚, 这样有些 w 就会变得很小 (L2 Regularization 也叫权重衰减, <strong>weights decay</strong>). 从下图左边的神经网络来看, 效果就是整个神经网络变得简单了(一些隐藏层甚至 $w$ 趋向于 0), 从而降低了过拟合的风险.</p>
<p>那些 隐藏层 并没有被消除，只是影响变得更小了，神经网络变得简单了.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-8_1.png" width="750"></p>
<blockquote>
<p>从另一个角度来看. 以 tanh激活函数 为例, 当 $λ$ 增加时, $w$ 会偏小, 这样 $z = wa +b$ 也会偏小, 此时的激活函数大致是线性的. 这样模型的复杂度也就降低了, 即降低了过拟合的风险.</p>
<p>如果神经网络每层都是线性的，其实整个还是一个线性的, 即使是一个很深的网络，因为线性激活函数的特征，最终我们只能计算线性函数.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-9_1.png" width="750"></p>
<h3 id="3-2-Dropout"><a href="#3-2-Dropout" class="headerlink" title="3.2 Dropout"></a>3.2 Dropout</h3><p>dropout 也是一种正则化的手段, 在训练时以 1-keep_prob 随机地”丢弃”一些节点. 如下图所示.</p>
<p><img src="/images/deeplearning/C2W1-10_1.png" width="600"></p>
<blockquote>
<p>具体可参考如下实现方式, 在前向传播时将 $a$ 中的某些值置为0, 为了保证大概的大小不受添加 dropout 影响, 再将处理后的 $a$ 除以 keep_prob.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-11_1.png" width="750"></p>
<blockquote>
<p>dropout 将产生收缩权重的平方范数的效果, 和 L2 类似，实施 dropout 的结果是它会压缩权重，并完成一些预防过拟合的外层正则化，事实证明 dropout 被正式地作为一种正则化的替代形式</p>
<p>L2 对不同权重的衰减是不同的，它取决于倍增的激活函数的大小.</p>
<p>dropout 的功能类似于 L2 正则化. 甚至 dropout 更适用于不同的输入范围.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-11_2.png" width="700"></p>
<blockquote>
<p>Notes: 每一层的 keep_prob 可能是不同的, keep_prob 取 1， 则是该层保留所有单元. </p>
<p>输出层的 keep_prob 经常设置为 1，有时候也可以设置为 1.9 (&gt;1). &lt; 1 通常在输出层是不太可能的. </p>
<p>输入层的 keep_prob 经常设置为 1，有时候也可以设置为 0.9， 如果是 0.5 消减一半，通常是不可能的.</p>
<p>其他 : 计算机视觉的人员非常钟情 dropout 函数.</p>
<p>Notes: dropout 的一大缺点就是 J 不会被明确定义. 每次迭代都会被随机删除一些节点. 如果再三检查梯度下降的性能，实际上是很难复查的.</p>
<p>定义明确的代价函数，每次迭代都会下降. 因为 dropout 使得 J 没有被明确定义，或者在某种程度上很难计算. 所以我们失去了调试工具，我通常会关闭 dropout. keep_prob 设置为 1， 运行代码，确保 J 函数单调递减, 然后在打开 dropout, 在 dropout 的过程中，代码并未引入bug.</p>
</blockquote>
<p>实现代码(未完成)</p>
<h3 id="3-3-Other-Regularization"><a href="#3-3-Other-Regularization" class="headerlink" title="3.3 Other Regularization"></a>3.3 Other Regularization</h3><ul>
<li>Data augmentation</li>
</ul>
<p><img src="/images/deeplearning/C2W1-12_1.png" width="600"></p>
<ul>
<li>Early stopping</li>
</ul>
<p><img src="/images/deeplearning/C2W1-13_1.png" width="600"></p>
<blockquote>
<p>W 开始是变小的，之后会随着迭代越来越大. early stopping 就是在中间点停止迭代过程.</p>
<p>Notes: </p>
<ol>
<li>early stopping  缺点是 提早停止，w 是防止了过拟合，但是 J 没有被继续下降.</li>
<li>L2 正则化 的缺点是，要用大量精力搜索合适的 λ .</li>
</ol>
<p>我个人也是更倾向于使用 L2，如果你可以负担大量的计算代价.</p>
</blockquote>
<h2 id="4-Normalization"><a href="#4-Normalization" class="headerlink" title="4. Normalization"></a>4. Normalization</h2><p><img src="/images/deeplearning/C2W1-14_1.png" width="600"></p>
<blockquote>
<ol>
<li>0 均值化 </li>
<li>归一化 方差</li>
</ol>
<p>上图2， 特征 x1 的方差 比 特征 x2 的方差 大很多<br>上图3， 特征 x1 和 特征 x2 的 方差 都是 1</p>
<p>注意: 不论 训练集 和 测试集，都是通过相同的 $\mu$ 和 ${\sigma}^2$ 定义的相同数据转换, 其中 $\mu$ 和 ${\sigma}^2$ 是由训练数据计算而来.</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-15_1.png" width="700"></p>
<h2 id="5-Vanishing-Exploding-gradients"><a href="#5-Vanishing-Exploding-gradients" class="headerlink" title="5. Vanishing/Exploding gradients"></a>5. Vanishing/Exploding gradients</h2><p>Vanishing/Exploding gradients 指的是随着前向传播不断地进行, 激活单元的值会逐层指数级地增加或减小, 从而导致梯度无限增大或者趋近于零, 这样会严重影响神经网络的训练. 如下图.</p>
<p><img src="/images/deeplearning/C2W1-16_1.png" width="750"></p>
<p>为了直观理解梯度消失和梯度爆炸，我们假设所有激活函数为线性激活函数，即 $g(z)=z$。 并假设前 L−1 个权重矩阵都相等, 即为 $W_{linear}$，所以可以得到 $y_{hat}=W_{linear}^{L-1}W_{L}X$</p>
<p>假设 $W_{linear}$ 都等于这个: <img src="/images/deeplearning/C2W1-16_2.jpg" alt=""></p>
<p>那么则有 $y_{hat}=1.5^{L-1}W_LX$，很显然当 L 很大时则会出现梯度爆炸。</p>
<p>同理若将权重的值设置为小于1，那么则会出现梯度消失。</p>
<blockquote>
<p>一个可以减小这种情况发生的方法, 就是用有效的参数初始化 (该方法并不能完全解决这个问题). 但是也是有意义的</p>
</blockquote>
<p><img src="/images/deeplearning/C2W1-17_1.png" width="750"></p>
<blockquote>
<p>设置合理的权重，希望你设置的权重矩阵，既不会增长过快，也不会下降过快到 0.</p>
<p>想更加了解如何初始化权重可以看下这篇文章 <a href="http://www.cnblogs.com/marsggbo/p/7462682.html" target="_blank" rel="external">神经网络权重初始化问题</a>，其中很详细的介绍了权重初始化问题。</p>
</blockquote>
<h2 id="6-Gradient-checking-implementation"><a href="#6-Gradient-checking-implementation" class="headerlink" title="6. Gradient checking implementation"></a>6. Gradient checking implementation</h2><p><img src="/images/deeplearning/C2W1-18_1.png" width="700"></p>
<p><img src="/images/deeplearning/C2W1-19_1.png" width="700"></p>
<p><img src="/images/deeplearning/C2W1-20_1.png" width="700"></p>
<blockquote>
<p>很难用梯度检验来双重检验 dropout 的计算， 所以我不同时使用梯度检验和 dropout，除非 dropout keep.prob 设置为 1.</p>
<p>我建议关闭 dropout 用梯度检验进行双重检查.</p>
<p>在没有 dropout 的情况下，确保你的算法是正确的，然后再打开 dropout.</p>
<p>现实中 几乎不会出现, 当 w 和 b 接近 0 时，梯度下降的实施是正确的.</p>
</blockquote>
<h2 id="8-Reference"><a href="#8-Reference" class="headerlink" title="8. Reference"></a>8. Reference</h2><ul>
<li><a href="https://study.163.com/my#/smarts" target="_blank" rel="external">网易云课堂 - deeplearning</a></li>
<li><a href="https://daniellaah.github.io/2017/deeplearning-ai-Improving-Deep-Neural-Networks-week1.html" target="_blank" rel="external">deeplearning.ai 专项课程二第一周</a></li>
<li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="external">Coursera - Deep Learning Specialization</a></li>
<li><a href="http://www.cnblogs.com/marsggbo/p/7470989.html" target="_blank" rel="external">DeepLearning.ai学习笔记汇总</a></li>
</ul>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_weibo_icon.png);background-size: contain;display: inline-block; width:50px; height:50px" href="https://service.weibo.com/share/share.php?url" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/tech-logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Train-dev-test"><span class="toc-number"></span> <span class="toc-text">1. Train/dev/test</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Data-set-partition"><span class="toc-number"></span> <span class="toc-text">1.1 Data set partition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Data-src-distribution"><span class="toc-number"></span> <span class="toc-text">1.2 Data src distribution</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Bias-Variance"><span class="toc-number"></span> <span class="toc-text">2. Bias / Variance</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Regularization"><span class="toc-number"></span> <span class="toc-text">3. Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-L2-regularization"><span class="toc-number"></span> <span class="toc-text">3.1 L2 regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Dropout"><span class="toc-number"></span> <span class="toc-text">3.2 Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Other-Regularization"><span class="toc-number"></span> <span class="toc-text">3.3 Other Regularization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Normalization"><span class="toc-number"></span> <span class="toc-text">4. Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Vanishing-Exploding-gradients"><span class="toc-number"></span> <span class="toc-text">5. Vanishing/Exploding gradients</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Gradient-checking-implementation"><span class="toc-number"></span> <span class="toc-text">6. Gradient checking implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Reference"><span class="toc-number"></span> <span class="toc-text">8. Reference</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/deeplearning-ai/">deeplearning.ai</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/07/21/deeplearning/Improving-Deep-Neural-Networks-week2/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Improving Deep Neural Networks (week2) - Optimization Algorithm
        
      </div>
    </a>
  
  
    <a href="/2018/07/15/deeplearning/Neural-Networks-and-Deep-Learning-week4/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Neural Networks and Deep Learning (week4) - Deep Neural Networks&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2018/07/19/deeplearning/Improving-Deep-Neural-Networks-week1/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
