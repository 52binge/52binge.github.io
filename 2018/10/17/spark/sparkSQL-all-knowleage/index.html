<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SparkSQL 底层实现原理 - Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="sparkSQL">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL 底层实现原理">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;SparkSql-logo-2.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;kaikeba&#x2F;sparkSQL-1&#x2F;1569468946521.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;kaikeba&#x2F;sparkSQL-1&#x2F;1569469087993.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;kaikeba&#x2F;sparkSQL-1&#x2F;1569469225309.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;kaikeba&#x2F;sparkSQL-2.assets&#x2F;1569469413038.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;kaikeba&#x2F;sparkSQL-2.assets&#x2F;1569469446641.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1569492382924.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1569492571159.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1569492595941.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1569492618490.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1569492637053.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;sparkSQL.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;Untitled%20Diagram.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;Untitled%20Diagram-1591500000169.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;unresolved%20Logical%20Plan.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;spark_batch_ruleexecutor-iteblog.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;unresolved%20Logical%20Plan-1594954815185.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;Analyzer%20Logical%20Plan.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;Analyzer%20Logical%20Plan-1591504429944.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8-1591504632746.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8-1591504632746-1594954838864.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;optimized.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;spark_constantpropagation-iteblog.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;spark_constantfolding-iteblog.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1591506889458.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;logical%20Plan.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;%E5%8E%9F%E5%A7%8B%E8%A7%A3%E6%9E%90sql%E6%96%B9%E5%BC%8F.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1591525895593.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;%E5%85%A8%E9%98%B6%E6%AE%B5%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1591534793978.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1591537781641.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1591607427157.png">
<meta property="og:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;2018&#x2F;10&#x2F;17&#x2F;spark&#x2F;sparkSQL-all-knowleage&#x2F;1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets&#x2F;1591607516396.png">
<meta property="og:updated_time" content="2020-11-30T22:59:13.609Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;iequa.com&#x2F;images&#x2F;spark&#x2F;SparkSql-logo-2.png">
  
  
    <link rel="icon" href="/css/images/favicon-Tiktok.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<!-- jiangting add start... @2020.08.30 -->
<!-- <div id="menu" class="duration-main" style="background-color: #e7e7e7"> -->
<!--   <p class="links-p" href="/">Blair</p> -->
<!--   <address class="icons"> -->
<!--     <a href="https://github.com/blair101" class="icon-font icon linkedin" target="_blank"></a> -->
<!--     <a href="https://cn.linkedin.com/pub/tianyu-dai/a8/818/44a" class="icon-font icon linkedin" target="_blank"></a> -->
<!--   </address> -->
<!--   <div class="hr1"></div> -->
<!--   <nav> -->
<!--     <div> -->
<!--     <a class="home" href="/">Home</a></div> -->
<!--     <p class="links-p" href="/">Home</p> -->
<!--     <nav class="tag-ath"> -->
<!--       <a class="proj" href="/categories">Category</a> -->
<!--       <a class="authors" href="/about">About</a> -->
<!--     </nav> -->
<!--   </nav> -->
<!--   <div class="hr2"></div> -->
<!--     <p class="links-p">Links</p> -->
<!--       <address class="links"> -->
<!--       <a class="proj" href="/article/Create-MyWorld">Projects</a> -->
<!--       <a class="friend">Friends</a></address><div class="hr3"> -->
<!--   </div> -->
<!--   <p class="end"></p> -->
<!--   <div id="menu-links" class="duration-main" style="top: -400px; background-color: #f5f5f5"> -->
<!--     <address> -->
<!--       <li><a target="_blank" href="http://lm7.xxxxxxxx.jp">Lm7</a></li> -->
<!--       <li><a target="_blank" href="http://www.pixiv.net/member.php?id=4933015">Domik</a></li> -->
<!--       <li><a target="_blank" href="http://hana-ui.moe">hana-ui</a></li> -->
<!--       <li><a target="_blank" href="http://fil.dtysky.moe">F-I-L</a></li> -->
<!--       <li><a target="_blank" href="http://paradise.dtysky.moe">Paradise</a></li> -->
<!--       <li><a target="_blank" href="http://moe-notes.dtysky.moe">MoeNotes</a></li> -->
<!--       <li><a target="_blank" href="http://kanata.dtysky.moe">Kanata</a></li> -->
<!--       <li><a target="_blank" href="http://blog.nekohand.moe">Nekohand</a></li> -->
<!--       <li><a target="_blank" href="http://www.jerryfu.net">JerryFu</a></li> -->
<!--       <li><a target="_blank" href="http://kawabangga.com">南史</a></li> -->
<!--       <li><a>Hide Links</a></li> -->
<!--       <li><a></a></li> -->
<!--     </address> -->
<!--   </div> -->
<!-- </div> -->
<!-- jiangting add end !  @2020.08.30 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/ai">AI</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-spark/sparkSQL-all-knowleage" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      SparkSQL 底层实现原理
      <small class=article-detail-date-index>&nbsp; 2018-10-17</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2018/10/17/spark/sparkSQL-all-knowleage/" class="article-date">
  <time datetime="2018-10-17T07:28:21.000Z" itemprop="datePublished">2018-10-17</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2018/10/17/spark/sparkSQL-all-knowleage/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <img src="/images/spark/SparkSql-logo-2.png" width="500" alt="" />
<a id="more"></a>
<h1 id="1sparksql概述"><a class="markdownIt-Anchor" href="#1sparksql概述"></a> 1.sparksql概述</h1>
<h2 id="11-sparksql的前世今生"><a class="markdownIt-Anchor" href="#11-sparksql的前世今生"></a> 1.1 sparksql的前世今生</h2>
<ul>
<li><mark>Shark是专门针对于spark的构建大规模数据仓库系统的一个框架</mark></li>
<li>Shark与Hive兼容、同时也依赖于Spark版本</li>
<li>Hivesql底层把sql解析成了mapreduce程序，Shark是把sql语句解析成了Spark任务</li>
<li>随着性能优化的上限，以及集成SQL的一些复杂的分析功能，发现Hive的MapReduce思想限制了Shark的发展。</li>
<li>最后Databricks公司终止对Shark的开发
<ul>
<li>决定单独开发一个框架，不在依赖hive，把重点转移到了<mark>sparksql</mark>这个框架上。</li>
</ul>
</li>
</ul>
<h2 id="12-什么是sparksql"><a class="markdownIt-Anchor" href="#12-什么是sparksql"></a> 1.2 什么是sparksql</h2>
<p><img src="/images/kaikeba/sparkSQL-1/1569468946521.png" alt="1569468946521" /></p>
<ul>
<li><strong>Spark SQL</strong> is Apache Spark’s module for working with structured data.</li>
<li>SparkSQL是apache Spark用来处理结构化数据的一个模块</li>
</ul>
<h1 id="2-sparksql的四大特性"><a class="markdownIt-Anchor" href="#2-sparksql的四大特性"></a> 2. sparksql的四大特性</h1>
<ul>
<li>
<p><mark>1、易整合</mark></p>
<p><img src="/images/kaikeba/sparkSQL-1/1569469087993.png" alt="1569469087993" /></p>
</li>
<li>
<p>将SQL查询与Spark程序无缝混合</p>
</li>
<li>
<p>可以使用不同的语言进行代码开发</p>
<ul>
<li>java</li>
<li>scala</li>
<li>python</li>
<li>R</li>
</ul>
</li>
<li>
<p><mark>2、统一的数据源访问</mark></p>
<p><img src="/images/kaikeba/sparkSQL-1/1569469225309.png" alt="1569469225309" /></p>
<ul>
<li>
<p>以相同的方式连接到任何数据源</p>
<ul>
<li>sparksql后期可以采用一种统一的方式去对接任意的外部数据源</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span>  dataFrame = sparkSession.read.文件格式的方法名(<span class="string">"该文件格式的路径"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p><mark>3、兼容hive</mark></p>
<p><img src="/images/kaikeba/sparkSQL-2.assets/1569469413038.png" alt="1569469413038" /></p>
<ul>
<li>sparksql可以支持hivesql这种语法  sparksql兼容hivesql</li>
</ul>
</li>
<li>
<p><mark>4、支持标准的数据库连接</mark></p>
<p><img src="/images/kaikeba/sparkSQL-2.assets/1569469446641.png" alt="1569469446641" /></p>
<ul>
<li>sparksql支持标准的数据库连接JDBC或者ODBC</li>
</ul>
</li>
</ul>
<p>spark-core-----&gt;去操作RDD----&gt;封装了数据</p>
<p>spark-sql------&gt;编程抽象DataFrame</p>
<h1 id="3-dataframe概述"><a class="markdownIt-Anchor" href="#3-dataframe概述"></a> 3. DataFrame概述</h1>
<h2 id="31-dataframe发展"><a class="markdownIt-Anchor" href="#31-dataframe发展"></a> 3.1 DataFrame发展</h2>
<ul>
<li>DataFrame前身是schemaRDD,这个schemaRDD是直接继承自RDD，它是RDD的一个实现类</li>
<li>在spark1.3.0之后把schemaRDD改名为DataFrame,它不在继承自RDD，而是自己实现RDD上的一些功能</li>
<li>也可以把dataFrame转换成一个rdd，调用rdd这个方法
<ul>
<li>例如 val rdd1=dataFrame.rdd</li>
</ul>
</li>
</ul>
<h2 id="32-dataframe是什么"><a class="markdownIt-Anchor" href="#32-dataframe是什么"></a> 3.2 DataFrame是什么</h2>
<ul>
<li>在Spark中，DataFrame是一种<mark>以RDD为基础的分布式数据集</mark>，类似于<mark>传统数据库的二维表格</mark></li>
<li>DataFrame带有<mark>Schema元信息</mark>，即DataFrame所表示的二维表数据集的每一列都带有名称和类型，但底层做了更多的优化</li>
<li>DataFrame可以从很多数据源构建
<ul>
<li>比如：已经存在的RDD、结构化文件、外部数据库、Hive表。</li>
</ul>
</li>
<li>RDD可以把它内部元素看成是一个java对象</li>
<li>DataFrame可以把内部是一个Row对象，它表示一行一行的数据</li>
</ul>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1569492382924.png" alt="1569492382924" /></p>
<ul>
<li>可以把DataFrame这样去理解
<ul>
<li>RDD+schema元信息</li>
</ul>
</li>
<li>dataFrame相比于rdd来说，多了对数据的描述信息（schema元信息）</li>
</ul>
<h2 id="33-dataframe和rdd的优缺点"><a class="markdownIt-Anchor" href="#33-dataframe和rdd的优缺点"></a> 3.3 DataFrame和RDD的优缺点</h2>
<ul>
<li>
<p><mark>1、RDD</mark></p>
<ul>
<li>
<p><mark>优点</mark></p>
<ul>
<li>1、编译时类型安全
<ul>
<li>开发会进行类型检查，在编译的时候及时发现错误</li>
</ul>
</li>
<li>2、具有面向对象编程的风格</li>
</ul>
</li>
<li>
<p><mark>缺点</mark></p>
<ul>
<li>
<p>1、构建大量的java对象占用了大量heap堆空间，导致频繁的GC</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">由于数据集RDD它的数据量比较大，后期都需要存储在heap堆中，这里有heap堆中的内存空间有限，出现频繁的垃圾回收（GC），程序在进行垃圾回收的过程中，所有的任务都是暂停。影响程序执行的效率</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>2、数据的序列化和反序列性能开销很大</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在分布式程序中，对象(对象的内容和结构)是先进行序列化，发送到其他服务器，进行大量的网络传输，然后接受到这些序列化的数据之后，再进行反序列化来恢复该对象</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><mark>2、DataFrame</mark></p>
<ul>
<li><mark>DataFrame引入了schema元信息和off-heap(堆外)</mark></li>
<li><mark>优点</mark>
<ul>
<li>1、DataFrame引入off-heap，大量的对象构建直接使用操作系统层面上的内存，不在使用heap堆中的内存，这样一来heap堆中的内存空间就比较充足，不会导致频繁GC，程序的运行效率比较高，它是解决了RDD构建大量的java对象占用了大量heap堆空间，导致频繁的GC这个缺点。</li>
<li>2、DataFrame引入了schema元信息—就是数据结构的描述信息，后期spark程序中的大量对象在进行网络传输的时候，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。这样一来数据网络传输的数据量是有所减少，数据的序列化和反序列性能开销就不是很大了。它是解决了RDD数据的序列化和反序列性能开销很大这个缺点</li>
<li><mark>缺点</mark>
<ul>
<li>DataFrame引入了schema元信息和off-heap(堆外)它是分别解决了RDD的缺点，同时它也丢失了RDD的优点
<ul>
<li>1、编译时类型不安全
<ul>
<li>编译时不会进行类型的检查，这里也就意味着前期是无法在编译的时候发现错误，只有在运行的时候才会发现</li>
</ul>
</li>
<li>2、不在具有面向对象编程的风格</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="4-读取文件构建dataframe"><a class="markdownIt-Anchor" href="#4-读取文件构建dataframe"></a> 4. 读取文件构建DataFrame</h1>
<h2 id="41-读取文本文件创建dataframe"><a class="markdownIt-Anchor" href="#41-读取文本文件创建dataframe"></a> 4.1 读取文本文件创建DataFrame</h2>
<ul>
<li>第一种方式</li>
<li>将数据person.txt上传到node01的/kkb/install/sparkdatas本地路径下</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">node01执行以下命令进入spark-shell</span><br><span class="line"></span><br><span class="line">cd /kkb/install/spark<span class="number">-2.3</span><span class="number">.3</span>-bin-hadoop2<span class="number">.7</span>/</span><br><span class="line">bin/spark-shell  --master local[<span class="number">2</span>] --jars /kkb/install/hadoop<span class="number">-2.6</span><span class="number">.0</span>-cdh5<span class="number">.14</span><span class="number">.2</span>/share/hadoop/common/hadoop-lzo<span class="number">-0.4</span><span class="number">.20</span>.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> personDF=spark.read.text(<span class="string">"file:///kkb/install/sparkdatas/person.txt"</span>)</span><br><span class="line"><span class="comment">//org.apache.spark.sql.DataFrame = [value: string]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//打印schema信息</span></span><br><span class="line">personDF.printSchema</span><br><span class="line"></span><br><span class="line"><span class="comment">//展示数据</span></span><br><span class="line">personDF.show</span><br></pre></td></tr></table></figure>
<ul>
<li>第二种方式</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//加载数据</span></span><br><span class="line"><span class="keyword">val</span> rdd1=sc.textFile(<span class="string">"file:///kkb/install/sparkdatas/person.txt"</span>).map(x=&gt;x.split(<span class="string">" "</span>))</span><br><span class="line"><span class="comment">//定义一个样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">id:<span class="type">String</span>,name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//把rdd与样例类进行关联</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">personRDD=rdd1</span>.<span class="title">map</span>(<span class="params">x=&gt;<span class="type">Person</span>(x(0</span>),<span class="title">x</span>(<span class="params">1</span>),<span class="title">x</span>(<span class="params">2</span>).<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">//把rdd转换成DataFrame</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">personDF=personRDD</span>.<span class="title">toDF</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//打印schema信息</span></span></span><br><span class="line"><span class="class"><span class="title">personDF</span>.<span class="title">printSchema</span></span></span><br><span class="line"><span class="class"><span class="title">//展示数据</span></span></span><br><span class="line"><span class="class"><span class="title">personDF</span>.<span class="title">show</span></span></span><br></pre></td></tr></table></figure>
<h2 id="42-读取json文件创建dataframe"><a class="markdownIt-Anchor" href="#42-读取json文件创建dataframe"></a> 4.2 读取json文件创建DataFrame</h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDF=spark.read.json(<span class="string">"file:///kkb/install/spark-2.3.3-bin-hadoop2.7/examples/src/main/resources/people.json"</span>)</span><br><span class="line"><span class="comment">//打印schema信息</span></span><br><span class="line">peopleDF.printSchema</span><br><span class="line"></span><br><span class="line"><span class="comment">//展示数据</span></span><br><span class="line">peopleDF.show</span><br></pre></td></tr></table></figure>
<h2 id="43-读取parquet文件创建dataframe"><a class="markdownIt-Anchor" href="#43-读取parquet文件创建dataframe"></a> 4.3 读取parquet文件创建DataFrame</h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> usersDF=spark.read.parquet(<span class="string">"file:////kkb/install/spark-2.3.3-bin-hadoop2.7/examples/src/main/resources/users.parquet"</span>)</span><br><span class="line"><span class="comment">//打印schema信息</span></span><br><span class="line">usersDF.printSchema</span><br><span class="line"></span><br><span class="line"><span class="comment">//展示数据</span></span><br><span class="line">usersDF.show</span><br></pre></td></tr></table></figure>
<h1 id="5-dataframe常用操作"><a class="markdownIt-Anchor" href="#5-dataframe常用操作"></a> 5. DataFrame常用操作</h1>
<h2 id="51-dsl风格语法"><a class="markdownIt-Anchor" href="#51-dsl风格语法"></a> 5.1 DSL风格语法</h2>
<p>创建maven工程，导入jar包</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.avro<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>avro-mapred<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-lang3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0-mr1-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line">                               <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-dependencyfile<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                               <span class="tag">&lt;<span class="name">arg</span>&gt;</span>$&#123;project.build.directory&#125;/.scala_dependencies<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                               <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                   <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                   <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                       <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                       <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                       <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                   <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                               <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                               <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                   <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span><span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                               <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>就是sparksql中的DataFrame自身提供了一套自己的Api，可以去使用这套api来做相应的处理</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//定义一个样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">id:<span class="type">String</span>,name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SparkDSL</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"sparkDSL"</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkSession: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = sparkSession.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="comment">//加载数据</span></span><br><span class="line">    <span class="keyword">val</span> rdd1=sc.textFile(<span class="string">"file:///D:\\开课吧课程资料\\15、scala与spark课程资料\\2、spark课程\\spark_day05\\数据/person.txt"</span>).map(x=&gt;x.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把rdd与样例类进行关联</span></span><br><span class="line">    <span class="keyword">val</span> personRDD=rdd1.map(x=&gt;<span class="type">Person</span>(x(<span class="number">0</span>),x(<span class="number">1</span>),x(<span class="number">2</span>).toInt))</span><br><span class="line">    <span class="comment">//把rdd转换成DataFrame</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> sparkSession.implicits._  <span class="comment">// 隐式转换</span></span><br><span class="line">    <span class="keyword">val</span> personDF=personRDD.toDF</span><br><span class="line">    <span class="comment">//打印schema信息</span></span><br><span class="line">    personDF.printSchema</span><br><span class="line">    <span class="comment">//展示数据</span></span><br><span class="line">    personDF.show</span><br><span class="line"></span><br><span class="line">    <span class="comment">//查询指定的字段</span></span><br><span class="line">    personDF.select(<span class="string">"name"</span>).show</span><br><span class="line">    personDF.select($<span class="string">"name"</span>).show</span><br><span class="line">      <span class="comment">//实现age+1</span></span><br><span class="line">    personDF.select($<span class="string">"name"</span>,$<span class="string">"age"</span>,$<span class="string">"age"</span>+<span class="number">1</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//实现age大于30过滤</span></span><br><span class="line">    personDF.filter($<span class="string">"age"</span> &gt; <span class="number">30</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">//按照age分组统计次数</span></span><br><span class="line">    personDF.groupBy(<span class="string">"age"</span>).count.show</span><br><span class="line"></span><br><span class="line">    <span class="comment">//按照age分组统计次数降序</span></span><br><span class="line">    personDF.groupBy(<span class="string">"age"</span>).count().sort($<span class="string">"count"</span>.desc).show</span><br><span class="line">    sparkSession.stop()</span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="52-sql风格语法"><a class="markdownIt-Anchor" href="#52-sql风格语法"></a> 5.2 SQL风格语法</h2>
<ul>
<li>可以把DataFrame注册成一张表，然后通过==sparkSession.sql(sql语句)==操作</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//DataFrame注册成表</span></span><br><span class="line">personDF.createTempView(<span class="string">"person"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用SparkSession调用sql方法统计查询</span></span><br><span class="line">spark.sql(<span class="string">"select * from person"</span>).show</span><br><span class="line">spark.sql(<span class="string">"select name from person"</span>).show</span><br><span class="line">spark.sql(<span class="string">"select name,age from person"</span>).show</span><br><span class="line">spark.sql(<span class="string">"select * from person where age &gt;30"</span>).show</span><br><span class="line">spark.sql(<span class="string">"select count(*) from person where age &gt;30"</span>).show</span><br><span class="line">spark.sql(<span class="string">"select age,count(*) from person group by age"</span>).show</span><br><span class="line">spark.sql(<span class="string">"select age,count(*) as count from person group by age"</span>).show</span><br><span class="line">spark.sql(<span class="string">"select * from person order by age desc"</span>).show</span><br></pre></td></tr></table></figure>
<h1 id="6-dataset概述"><a class="markdownIt-Anchor" href="#6-dataset概述"></a> 6. DataSet概述</h1>
<h2 id="61-dataset是什么"><a class="markdownIt-Anchor" href="#61-dataset是什么"></a> 6.1 DataSet是什么</h2>
<ul>
<li>DataSet是分布式的数据集合，Dataset提供了<mark>强类型支持</mark>，也是在RDD的每行数据加了类型约束。</li>
<li>DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型和可以用强大lambda函数）以及使用了Spark SQL优化的执行引擎。</li>
</ul>
<h2 id="62-dataframe-dataset区别"><a class="markdownIt-Anchor" href="#62-dataframe-dataset区别"></a> 6.2 DataFrame、DataSet区别</h2>
<ul>
<li>假设RDD中的两行数据长这样</li>
</ul>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1569492571159.png" alt="1569492571159" /></p>
<ul>
<li>那么DataFrame中的数据长这样</li>
</ul>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1569492595941.png" alt="1569492595941" /></p>
<ul>
<li>
<p>Dataset中的数据长这样</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1569492618490.png" alt="1569492618490" /></p>
<ul>
<li>或者长这样（每行数据是个Object）</li>
</ul>
<p>​	<img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1569492637053.png" alt="1569492637053" /></p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DataSet包含了DataFrame的功能，Spark2.0中两者统一，DataFrame表示为DataSet[Row]，即DataSet的子集。</span><br><span class="line">（1）DataSet可以在编译时检查类型</span><br><span class="line">（2）并且是面向对象的编程接口</span><br></pre></td></tr></table></figure>
<h2 id="63-dataframe与dataset互转"><a class="markdownIt-Anchor" href="#63-dataframe与dataset互转"></a> 6.3 DataFrame与DataSet互转</h2>
<ul>
<li>
<p>1、把一个DataFrame转换成DataSet</p>
<ul>
<li>val <a href="http://dataSet=dataFrame.as" target="_blank" rel="noopener">dataSet=dataFrame.as</a>[强类型]</li>
</ul>
</li>
<li>
<p>2、把一个DataSet转换成DataFrame</p>
<ul>
<li>val dataFrame=dataSet.toDF</li>
</ul>
</li>
<li>
<p><strong>补充说明</strong></p>
<ul>
<li>可以从dataFrame和dataSet获取得到rdd
<ul>
<li>val rdd1=dataFrame.rdd</li>
<li>val rdd2=dataSet.rdd</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="64-构建dataset"><a class="markdownIt-Anchor" href="#64-构建dataset"></a> 6.4 构建DataSet</h2>
<ul>
<li>
<p>1、 通过sparkSession调用createDataset方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ds=spark.createDataset(<span class="number">1</span> to <span class="number">10</span>) <span class="comment">//scala集合</span></span><br><span class="line"><span class="keyword">val</span> ds=spark.createDataset(sc.textFile(<span class="string">"/person.txt"</span>))  <span class="comment">//rdd</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>2、使用scala集合和rdd调用toDS方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">"/person.txt"</span>).toDS</span><br><span class="line"><span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).toDS</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>3、把一个DataFrame转换成DataSet</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val dataSet=dataFrame.as[强类型]</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>4、通过一个DataSet转换生成一个新的DataSet</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).toDS.map(x=&gt;x*<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="65-rdd以及dataframe以及dataset的关系"><a class="markdownIt-Anchor" href="#65-rdd以及dataframe以及dataset的关系"></a> 6.5 RDD以及DataFrame以及DataSet的关系</h2>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/sparkSQL.png" alt="sparkSQL" /></p>
<p>首先，Spark RDD、DataFrame和DataSet是Spark的三类API，下图是他们的发展过程：</p>
<p>DataFrame是spark1.3.0版本提出来的，spark1.6.0版本又引入了DateSet的，但是在spark2.0版本中，DataFrame和DataSet合并为DataSet。</p>
<p>那么你可能会问了：那么，在2.0以后的版本里，RDD是不是不需要了呢？</p>
<p>答案是：NO！首先，DataFrame和DataSet是基于RDD的，而且这三者之间可以通过简单的API调用进行无缝切换。</p>
<p>下面，依次介绍这三类API的特点<br />
一、RDD</p>
<p>RDD的优点：<br />
1.相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。<br />
2.面向对象编程，直接存储的java对象，类型转化也安全</p>
<p>RDD的缺点：<br />
1.由于它基本和hadoop一样万能的，因此没有针对特殊场景的优化，比如对于结构化数据处理相对于sql来比非常麻烦<br />
2.默认采用的是java序列号方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁</p>
<p>二、DataFrame<br />
<em>DataFrame<strong>的优点</strong>：</em><br />
1.结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表</p>
<p>2.有针对性的优化，如采用Kryo序列化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数,所以运行更快。</p>
<p>3.hive兼容，支持hql、udf等</p>
<p>DataFrame的缺点：<br />
1.编译时不能类型转化安全检查，运行时才能确定是否有问题<br />
2.对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象</p>
<p>三、DateSet</p>
<p><em>DateSet<strong>的优点</strong>：</em><br />
1.DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据<br />
2.和RDD一样，支持自定义对象存储<br />
3.和DataFrame一样，支持结构化数据的sql查询<br />
4.采用堆外内存存储，gc友好</p>
<p>5.类型转化安全，代码友好</p>
<h1 id="7-通过idea开发程序实现把rdd转换dataframe"><a class="markdownIt-Anchor" href="#7-通过idea开发程序实现把rdd转换dataframe"></a> 7. 	通过IDEA开发程序实现把RDD转换DataFrame</h1>
<h2 id="71-利用反射机制"><a class="markdownIt-Anchor" href="#71-利用反射机制"></a> 7.1 利用反射机制</h2>
<ul>
<li>
<p>定义一个样例类，后期直接映射成DataFrame的schema信息</p>
<ul>
<li>
<p>应用场景</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在开发代码之前，是可以先确定好DataFrame的schema元信息</span><br><span class="line"></span><br><span class="line">case class Person(id:String,name:String,age:Int)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>代码开发</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Column</span>, <span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//todo:利用反射机制实现把rdd转成dataFrame</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">id:<span class="type">String</span>,name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">CaseClassSchema</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1、构建SparkSession对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().appName(<span class="string">"CaseClassSchema"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2、获取sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"warn"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3、读取文件数据</span></span><br><span class="line">    <span class="keyword">val</span> data: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = sc.textFile(<span class="string">"file:///D:\\开课吧课程资料\\15、scala与spark课程资料\\2、spark课程\\spark_day05\\数据"</span>).map(x=&gt;x.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4、定义一个样例类</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//5、将rdd与样例类进行关联</span></span><br><span class="line">    <span class="keyword">val</span> personRDD: <span class="type">RDD</span>[<span class="type">Person</span>] = data.map(x=&gt;<span class="type">Person</span>(x(<span class="number">0</span>),x(<span class="number">1</span>),x(<span class="number">2</span>).toInt))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6、将rdd转换成dataFrame</span></span><br><span class="line">    <span class="comment">//需要手动导入隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> personDF: <span class="type">DataFrame</span> = personRDD.toDF</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7、对dataFrame进行相应的语法操作</span></span><br><span class="line">    <span class="comment">//todo：----------------- DSL风格语法-----------------start</span></span><br><span class="line">    <span class="comment">//打印schema</span></span><br><span class="line">    personDF.printSchema()</span><br><span class="line">    <span class="comment">//展示数据</span></span><br><span class="line">    personDF.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取第一行数据</span></span><br><span class="line">    <span class="keyword">val</span> first: <span class="type">Row</span> = personDF.first()</span><br><span class="line">    println(<span class="string">"first:"</span>+first)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//取出前3位数据</span></span><br><span class="line">    <span class="keyword">val</span> top3: <span class="type">Array</span>[<span class="type">Row</span>] = personDF.head(<span class="number">3</span>)</span><br><span class="line">    top3.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取name字段</span></span><br><span class="line">    personDF.select(<span class="string">"name"</span>).show()</span><br><span class="line">    personDF.select($<span class="string">"name"</span>).show()</span><br><span class="line">    personDF.select(<span class="keyword">new</span> <span class="type">Column</span>(<span class="string">"name"</span>)).show()</span><br><span class="line">    personDF.select(<span class="string">"name"</span>,<span class="string">"age"</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//实现age +1</span></span><br><span class="line">    personDF.select($<span class="string">"name"</span>,$<span class="string">"age"</span>,$<span class="string">"age"</span>+<span class="number">1</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//按照age过滤</span></span><br><span class="line">    personDF.filter($<span class="string">"age"</span> &gt;<span class="number">30</span>).show()</span><br><span class="line">    <span class="keyword">val</span> count: <span class="type">Long</span> = personDF.filter($<span class="string">"age"</span> &gt;<span class="number">30</span>).count()</span><br><span class="line">    println(<span class="string">"count:"</span>+count)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分组</span></span><br><span class="line">    personDF.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"></span><br><span class="line">    personDF.show()</span><br><span class="line">    personDF.foreach(row =&gt; println(row))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用foreach获取每一个row对象中的name字段</span></span><br><span class="line">    personDF.foreach(row =&gt;println(row.getAs[<span class="type">String</span>](<span class="string">"name"</span>)))</span><br><span class="line">    personDF.foreach(row =&gt;println(row.get(<span class="number">1</span>)))</span><br><span class="line">    personDF.foreach(row =&gt;println(row.getString(<span class="number">1</span>)))</span><br><span class="line">    personDF.foreach(row =&gt;println(row.getAs[<span class="type">String</span>](<span class="number">1</span>)))</span><br><span class="line">    <span class="comment">//todo：----------------- DSL风格语法--------------------end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//todo：----------------- SQL风格语法-----------------start</span></span><br><span class="line">    personDF.createTempView(<span class="string">"person"</span>)</span><br><span class="line">    <span class="comment">//使用SparkSession调用sql方法统计查询</span></span><br><span class="line">    spark.sql(<span class="string">"select * from person"</span>).show</span><br><span class="line">    spark.sql(<span class="string">"select name from person"</span>).show</span><br><span class="line">    spark.sql(<span class="string">"select name,age from person"</span>).show</span><br><span class="line">    spark.sql(<span class="string">"select * from person where age &gt;30"</span>).show</span><br><span class="line">    spark.sql(<span class="string">"select count(*) from person where age &gt;30"</span>).show</span><br><span class="line">    spark.sql(<span class="string">"select age,count(*) from person group by age"</span>).show</span><br><span class="line">    spark.sql(<span class="string">"select age,count(*) as count from person group by age"</span>).show</span><br><span class="line">    spark.sql(<span class="string">"select * from person order by age desc"</span>).show</span><br><span class="line">    <span class="comment">//todo：----------------- SQL风格语法----------------------end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭sparkSession对象</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="72-通过structtype动态指定schema"><a class="markdownIt-Anchor" href="#72-通过structtype动态指定schema"></a> 7.2 通过StructType动态指定Schema</h2>
<ul>
<li>
<p>应用场景</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在开发代码之前，是无法确定需要的DataFrame对应的schema元信息。需要在开发代码的过程中动态指定。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>代码开发</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//todo；通过动态指定dataFrame对应的schema信息将rdd转换成dataFrame</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StructTypeSchema</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1、构建SparkSession对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().appName(<span class="string">"StructTypeSchema"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2、获取sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"warn"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3、读取文件数据</span></span><br><span class="line">    <span class="keyword">val</span> data: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = sc.textFile(<span class="string">"file:///D:\\开课吧课程资料\\15、scala与spark课程资料\\2、spark课程\\spark_day05\\数据"</span>).map(x=&gt;x.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4、将rdd与Row对象进行关联</span></span><br><span class="line">    <span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = data.map(x=&gt;<span class="type">Row</span>(x(<span class="number">0</span>),x(<span class="number">1</span>),x(<span class="number">2</span>).toInt))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5、指定dataFrame的schema信息</span></span><br><span class="line">    <span class="comment">//这里指定的字段个数和类型必须要跟Row对象保持一致</span></span><br><span class="line">    <span class="keyword">val</span> schema=<span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"id"</span>,<span class="type">StringType</span>)::</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"name"</span>,<span class="type">StringType</span>)::</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"age"</span>,<span class="type">IntegerType</span>)::<span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataFrame: <span class="type">DataFrame</span> = spark.createDataFrame(rowRDD,schema)</span><br><span class="line">    dataFrame.printSchema()</span><br><span class="line">    dataFrame.show()</span><br><span class="line"></span><br><span class="line">    dataFrame.createTempView(<span class="string">"user"</span>)</span><br><span class="line">    spark.sql(<span class="string">"select * from user"</span>).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="8-sparksql读取sql数据"><a class="markdownIt-Anchor" href="#8-sparksql读取sql数据"></a> 8、sparkSQL读取sql数据</h1>
<p>spark sql可以通过 JDBC 从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中</p>
<ul>
<li>添加mysql连接驱动jar包</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>代码开发</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//todo:利用sparksql加载mysql表中的数据</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFromMysql</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1、创建SparkConf对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"DataFromMysql"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2、创建SparkSession对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3、读取mysql表的数据</span></span><br><span class="line">    <span class="comment">//3.1 指定mysql连接地址</span></span><br><span class="line">    <span class="keyword">val</span> url=<span class="string">"jdbc:mysql://localhost:3306/mydb?characterEncoding=UTF-8"</span></span><br><span class="line">    <span class="comment">//3.2 指定要加载的表名</span></span><br><span class="line">    <span class="keyword">val</span> tableName=<span class="string">"jobdetail"</span></span><br><span class="line">    <span class="comment">// 3.3 配置连接数据库的相关属性</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//用户名</span></span><br><span class="line">    properties.setProperty(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">    <span class="comment">//密码</span></span><br><span class="line">    properties.setProperty(<span class="string">"password"</span>,<span class="string">"123456"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mysqlDF: <span class="type">DataFrame</span> = spark.read.jdbc(url,tableName,properties)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打印schema信息</span></span><br><span class="line">    mysqlDF.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//展示数据</span></span><br><span class="line">    mysqlDF.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把dataFrame注册成表</span></span><br><span class="line">    mysqlDF.createTempView(<span class="string">"job_detail"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"select * from job_detail where city = '广东' "</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="9-sparksql操作csv文件并将结果写入mysql"><a class="markdownIt-Anchor" href="#9-sparksql操作csv文件并将结果写入mysql"></a> 9、sparkSQL操作CSV文件并将结果写入mysql</h1>
<p>使用spark程序读取CSV文件，然后将读取到的数据内容，保存到mysql里面去，注意csv文件的换行问题。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CSVOperate</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[8]"</span>).setAppName(<span class="string">"sparkCSV"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> session: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    session.sparkContext.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="keyword">val</span> frame: <span class="type">DataFrame</span> = session</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"timestampFormat"</span>, <span class="string">"yyyy/MM/dd HH:mm:ss ZZ"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"multiLine"</span>, <span class="literal">true</span>)</span><br><span class="line">      .load(<span class="string">"file:///D:\\开课吧课程资料\\15、scala与spark课程资料\\2、spark课程\\spark_day05\\数据\\招聘数据"</span>)</span><br><span class="line"></span><br><span class="line">    frame.createOrReplaceTempView(<span class="string">"job_detail"</span>)</span><br><span class="line">    <span class="comment">//session.sql("select job_name,job_url,job_location,job_salary,job_company,job_experience,job_class,job_given,job_detail,company_type,company_person,search_key,city from job_detail where job_company = '北京无极慧通科技有限公司'  ").show(80)</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">    prop.put(<span class="string">"password"</span>, <span class="string">"123456"</span>)</span><br><span class="line"></span><br><span class="line">    frame.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).jdbc(<span class="string">"jdbc:mysql://localhost:3306/mydb?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"mydb.jobdetail_copy"</span>, prop)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="10-spark-on-hive-与hive-on-spark"><a class="markdownIt-Anchor" href="#10-spark-on-hive-与hive-on-spark"></a> 10、spark  on  hive  与hive on   spark</h1>
<p><strong>Spark on hive 与 Hive on Spark 的区别</strong></p>
<ul>
<li>Spark on hive</li>
</ul>
<p>Spark通过Spark-SQL使用hive 语句,操作hive,底层运行的还是 spark rdd。</p>
<p>（1）就是通过sparksql，加载hive的配置文件，获取到hive的元数据信息</p>
<p>（2）spark sql获取到hive的元数据信息之后就可以拿到hive的所有表的数据</p>
<p>（3）接下来就可以通过spark sql来操作hive表中的数据</p>
<ul>
<li>Hive on Spark</li>
</ul>
<p>是把hive查询从mapreduce 的mr (Hadoop计算引擎)操作替换为spark rdd（spark 执行引擎） 操作. 相对于spark on hive,这个要实现起来则麻烦很多, 必须重新编译你的spark和导入jar包，不过目前大部分使用的是spark on hive。</p>
<p><a href="/images/kaikeba/sparkSQL-1%5Cspark%E4%B8%8Ehive.pptx">spark与hive.pptx</a></p>
<h2 id="1-spark_sql与hive进行整合"><a class="markdownIt-Anchor" href="#1-spark_sql与hive进行整合"></a> 1、spark_sql与hive进行整合</h2>
<h3 id="第一步拷贝hive-sitexml配置文件"><a class="markdownIt-Anchor" href="#第一步拷贝hive-sitexml配置文件"></a> 第一步：拷贝hive-site.xml配置文件</h3>
<p>将node03服务器安装的hive家目录下的conf文件夹下面的hive-site.xml拷贝到spark安装的各个机器节点，node03执行以下命令进行拷贝</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /kkb/install/hive-1.1.0-cdh5.14.2/conf</span><br><span class="line">scp hive-site.xml  node01:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/</span><br><span class="line">scp hive-site.xml  node02:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/</span><br><span class="line">scp hive-site.xml  node03:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/</span><br></pre></td></tr></table></figure>
<h3 id="第二步拷贝mysql连接驱动包"><a class="markdownIt-Anchor" href="#第二步拷贝mysql连接驱动包"></a> 第二步：拷贝mysql连接驱动包</h3>
<p>将hive当中mysql的连接驱动包拷贝到spark安装家目录下的lib目录下，node03执行下命令拷贝mysql的lib驱动包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /kkb/install/hive-1.1.0-cdh5.14.2/lib/</span><br><span class="line">scp mysql-connector-java-5.1.38.jar  node01:/kkb/install/spark-2.3.3-bin-hadoop2.7/jars/</span><br><span class="line">scp mysql-connector-java-5.1.38.jar  node02:/kkb/install/spark-2.3.3-bin-hadoop2.7/jars/</span><br><span class="line">scp mysql-connector-java-5.1.38.jar  node03:/kkb/install/spark-2.3.3-bin-hadoop2.7/jars/</span><br></pre></td></tr></table></figure>
<h3 id="第三步进入spark-sql直接操作hive数据库当中的数据"><a class="markdownIt-Anchor" href="#第三步进入spark-sql直接操作hive数据库当中的数据"></a> 第三步：进入spark-sql直接操作hive数据库当中的数据</h3>
<p>在spark2.0版本后由于出现了sparkSession，在初始化sqlContext的时候，会设置默认的<mark>spark.sql.warehouse.dir=spark-warehouse</mark>,</p>
<p>此时将hive与sparksql整合完成之后，在通过spark-sql脚本启动的时候，还是会在哪里启动spark-sql脚本，就会在当前目录下创建一个spark.sql.warehouse.dir为spark-warehouse的目录，存放由spark-sql创建数据库和创建表的数据信息，与之前hive的数据息不是放在同一个路径下（可以互相访问）。但是此时spark-sql中表的数据在本地，不利于操作，也不安全。</p>
<p>所有在启动的时候需要加上这样一个参数：</p>
<p>–conf  spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse</p>
<p>保证spark-sql启动时不在产生新的存放数据的目录，sparksql与hive最终使用的是hive同一存放数据的目录。</p>
<p>node01直接执行以下命令，进入spark-sql交互界面，然后操作hive当中的数据，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /kkb/install/spark-2.3.3-bin-hadoop2.7/</span><br><span class="line"></span><br><span class="line">bin/spark-sql  --master local[2] \</span><br><span class="line">--executor-memory 512m --total-executor-cores 3 \</span><br><span class="line">--conf  spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \</span><br><span class="line">--jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar</span><br></pre></td></tr></table></figure>
<p>使用sparkSQL有hive进行整合之后，就可以通过sparkSQL语句来操作hive表数据了</p>
<ul>
<li>应用场景</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash">定义sparksql提交脚本的头信息</span></span><br><span class="line">SUBMITINFO="spark-sql --master spark://node01:7077 --executor-memory 1g --total-executor-cores 4 --conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse" </span><br><span class="line"><span class="meta">#</span><span class="bash">定义一个sql语句</span></span><br><span class="line">SQL="select * from default.hive_source;" </span><br><span class="line"><span class="meta">#</span><span class="bash">执行sql语句   类似于 hive -e sql语句</span></span><br><span class="line">echo "$SUBMITINFO" </span><br><span class="line">echo "$SQL"</span><br><span class="line"><span class="meta">$</span><span class="bash">SUBMITINFO -e <span class="string">"<span class="variable">$SQL</span>"</span></span></span><br></pre></td></tr></table></figure>
<h2 id="2-启用spark的thrift-server与hive进行远程交互"><a class="markdownIt-Anchor" href="#2-启用spark的thrift-server与hive进行远程交互"></a> 2、启用spark的thrift  server与hive进行远程交互</h2>
<p>除了可以通过spark-shell来与hive进行整合之外，我们也可以通过spark的thrift服务来远程与hive进行交互</p>
<h3 id="第一步修改hive-sitexml的配置"><a class="markdownIt-Anchor" href="#第一步修改hive-sitexml的配置"></a> 第一步：修改hive-site.xml的配置</h3>
<p>node03执行以下命令修改hive-site.xml的配置属性，添加以下几个配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">cd /kkb/install/hive-1.1.0-cdh5.14.2/conf</span><br><span class="line">vim hive-site.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node03:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Thrift URI for the remote metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.min.worker.threads<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.max.worker.threads<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>500<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="第二步修改完的配置文件分发到其他机器"><a class="markdownIt-Anchor" href="#第二步修改完的配置文件分发到其他机器"></a> 第二步：修改完的配置文件分发到其他机器</h3>
<p>node03执行以下命令分发hive配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /kkb/install/hive-1.1.0-cdh5.14.2/conf</span><br><span class="line">scp hive-site.xml  node01:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/</span><br><span class="line">scp hive-site.xml  node02:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/</span><br><span class="line">scp hive-site.xml  node03:/kkb/install/spark-2.3.3-bin-hadoop2.7/conf/</span><br></pre></td></tr></table></figure>
<h3 id="第三步node03启动metastore服务"><a class="markdownIt-Anchor" href="#第三步node03启动metastore服务"></a> 第三步：node03启动metastore服务</h3>
<p>node03执行以下命令启动metastore服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /kkb/install/hive-1.1.0-cdh5.14.2/</span><br><span class="line">bin/hive --service metastore</span><br></pre></td></tr></table></figure>
<h3 id="第四步node03执行以下命令启动spark的thrift-server"><a class="markdownIt-Anchor" href="#第四步node03执行以下命令启动spark的thrift-server"></a> 第四步：node03执行以下命令启动spark的thrift  server</h3>
<p><mark>注意：hive安装在哪一台，就在哪一台服务器启动spark的thrift  server</mark></p>
<p>我的hive安装在node03服务器，所以我在node03服务器上面启动spark的thrift  server服务</p>
<p>node03执行以下命令启动thrift  server服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /kkb/install/spark-2.3.3-bin-hadoop2.7</span><br><span class="line"></span><br><span class="line">sbin/start-thriftserver.sh --master local[2]  --executor-memory 5g --total-executor-cores 5 --jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar</span><br></pre></td></tr></table></figure>
<h3 id="第五步直接使用beeline来连接"><a class="markdownIt-Anchor" href="#第五步直接使用beeline来连接"></a> 第五步：直接使用beeline来连接</h3>
<p>直接在node03服务器上面使用beeline来进行连接spark-sql</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /kkb/install/spark-2.3.3-bin-hadoop2.7</span><br><span class="line">bin/beeline </span><br><span class="line"></span><br><span class="line">beeline&gt; !connect jdbc:hive2://node03:10000</span><br><span class="line">Connecting to jdbc:hive2://node03:10000</span><br><span class="line">Enter username for jdbc:hive2://node03:10000: hadoop</span><br><span class="line">Enter password for jdbc:hive2://node03:10000: ******</span><br></pre></td></tr></table></figure>
<h1 id="11-sparksql自定义函数"><a class="markdownIt-Anchor" href="#11-sparksql自定义函数"></a> 11、sparkSQL自定义函数</h1>
<p>用户自定义函数类别分为以下三种：</p>
<p>1).UDF：输入一行，返回一个结果(一对一)，在上篇案例 使用SparkSQL实现根据ip地址计算归属地二 中实现的自定义函数就是UDF，输入一个十进制的ip地址，返回一个省份</p>
<p>2).UDTF：输入一行，返回多行(一对多)，在SparkSQL中没有，因为Spark中使用flatMap即可实现这个功能</p>
<p>3).UDAF：输入多行，返回一行，这里的A是aggregate，聚合的意思，如果业务复杂，需要自己实现聚合函数</p>
<h2 id="1-自定义udf函数"><a class="markdownIt-Anchor" href="#1-自定义udf函数"></a> 1、自定义UDF函数</h2>
<p>读取深圳二手房成交数据，对房子的年份进行自定义函数处理，如果没有年份，那么就给默认值1990</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.regex.&#123;<span class="type">Matcher</span>, <span class="type">Pattern</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.api.java.<span class="type">UDF1</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">DataTypes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[8]"</span>).setAppName(<span class="string">"sparkCSV"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> session: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    session.sparkContext.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="keyword">val</span> frame: <span class="type">DataFrame</span> = session</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"timestampFormat"</span>, <span class="string">"yyyy/MM/dd HH:mm:ss ZZ"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"multiLine"</span>, <span class="literal">true</span>)</span><br><span class="line">      .load(<span class="string">"file:///D:\\开课吧课程资料\\15、scala与spark课程资料\\2、spark课程\\spark_day05\\数据\\深圳链家二手房成交明细"</span>)</span><br><span class="line"></span><br><span class="line">    frame.createOrReplaceTempView(<span class="string">"house_sale"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    session.udf.register(<span class="string">"house_udf"</span>,<span class="keyword">new</span> <span class="type">UDF1</span>[<span class="type">String</span>,<span class="type">String</span>] &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> pattern: <span class="type">Pattern</span> = <span class="type">Pattern</span>.compile(<span class="string">"^[0-9]*$"</span>)</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">call</span></span>(input: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> matcher: <span class="type">Matcher</span> = pattern.matcher(input)</span><br><span class="line">        <span class="keyword">if</span>(matcher.matches())&#123;</span><br><span class="line">          input</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          <span class="string">"1990"</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,<span class="type">DataTypes</span>.<span class="type">StringType</span>)</span><br><span class="line"></span><br><span class="line">    session.sql(<span class="string">"select house_udf(house_age) from house_sale  limit 200"</span>).show()</span><br><span class="line">    session.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-自定义udaf函数"><a class="markdownIt-Anchor" href="#2-自定义udaf函数"></a> 2、自定义UDAF函数</h2>
<p>需求：自定义UDAF函数，读取深圳二手房数据，然后按照楼层进行分组，求取每个楼层的平均成交金额</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"floor"</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合缓冲区中值得数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>, <span class="type">DoubleType</span>) :: <span class="type">StructField</span>(<span class="string">"count"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回值的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对于相同的输入是否一直返回相同的输出。</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 用于存储不同类型的楼房的总成交额</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>D</span><br><span class="line">    <span class="comment">// 用于存储不同类型的楼房的总个数</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 相同Execute间的数据合并。</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getDouble(<span class="number">0</span>) + input.getDouble(<span class="number">0</span>)</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 不同Execute间的数据合并</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getDouble(<span class="number">0</span>) + buffer2.getDouble(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getDouble(<span class="number">0</span>) / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[8]"</span>).setAppName(<span class="string">"sparkCSV"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> session: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    session.sparkContext.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="keyword">val</span> frame: <span class="type">DataFrame</span> = session</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"timestampFormat"</span>, <span class="string">"yyyy/MM/dd HH:mm:ss ZZ"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"multiLine"</span>, <span class="literal">true</span>)</span><br><span class="line">      .load(<span class="string">"file:///D:\\开课吧课程资料\\15、scala与spark课程资料\\2、spark课程\\spark_day05\\数据\\深圳链家二手房成交明细"</span>)</span><br><span class="line">    frame.createOrReplaceTempView(<span class="string">"house_sale"</span>)</span><br><span class="line">    session.sql(<span class="string">"select floor from house_sale limit 30"</span>).show()</span><br><span class="line">    session.udf.register(<span class="string">"udaf"</span>,<span class="keyword">new</span> <span class="type">MyAverage</span>)</span><br><span class="line">    session.sql(<span class="string">"select floor, udaf(house_sale_money) from house_sale group by floor"</span>).show()</span><br><span class="line">    frame.printSchema()</span><br><span class="line">    session.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="12-sparksql架构设计"><a class="markdownIt-Anchor" href="#12-sparksql架构设计"></a> 12、sparkSQL架构设计</h1>
<p>sparkSQL是spark技术栈当中又一非常出彩的模块，通过引入SQL的支持，大大降低了开发人员和学习人员的使用成本，让我们开发人员直接使用SQL的方式就能够实现大数据的开发，它同时支持DSL以及SQL的语法风格，目前在spark的整个架构设计当中，所有的spark模块，例如SQL，SparkML，sparkGrahpx以及Structed Streaming等都是基于 Catalyst Optimization &amp; Tungsten Execution模块之上运行，如下图所示就显示了spark的整体架构模块设计</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/Untitled%20Diagram.png" alt="Untitled Diagram" /></p>
<h2 id="1-sparksql的架构设计实现"><a class="markdownIt-Anchor" href="#1-sparksql的架构设计实现"></a> 1、sparkSQL的架构设计实现</h2>
<p>sparkSQL 执行先会经过 SQL Parser 解析 SQL，然后经过 Catalyst 优化器处理，最后到 Spark 执行。而 Catalyst 的过程又分为很多个过程，其中包括：</p>
<ul>
<li>Analysis：主要利用 Catalog 信息将 Unresolved Logical Plan 解析成 Analyzed logical plan；</li>
<li>Logical Optimizations：利用一些 Rule （规则）将 Analyzed logical plan 解析成 Optimized Logical Plan；</li>
<li>Physical Planning：前面的 logical plan 不能被 Spark 执行，而这个过程是把 logical plan 转换成多个 physical plans，然后利用代价模型（cost model）选择最佳的 physical plan；</li>
<li>Code Generation：这个过程会把 SQL 查询生成 Java 字 节码。</li>
</ul>
<p><a href="/images/kaikeba/sparkSQL-1%5CsparkSQL%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1.pptx">sparkSQL架构设计.pptx</a></p>
<p>例如执行以下SQL语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> temp1.class,<span class="keyword">sum</span>(temp1.degree),<span class="keyword">avg</span>(temp1.degree)  <span class="keyword">from</span> (<span class="keyword">SELECT</span>  students.sno <span class="keyword">AS</span> ssno,students.sname,students.ssex,students.sbirthday,students.class, scores.sno,scores.degree,scores.cno  <span class="keyword">FROM</span> students <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> scores <span class="keyword">ON</span> students.sno =  scores.sno ) temp1 <span class="keyword">group</span> <span class="keyword">by</span> temp1.class</span><br></pre></td></tr></table></figure>
<p>代码实现过程如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kkb.sparksql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//todo:利用sparksql加载mysql表中的数据</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFromMysqlPlan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1、创建SparkConf对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"DataFromMysql"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//sparkConf.set("spark.sql.codegen.wholeStage","true")</span></span><br><span class="line">    <span class="comment">//2、创建SparkSession对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3、读取mysql表的数据</span></span><br><span class="line">    <span class="comment">//3.1 指定mysql连接地址</span></span><br><span class="line">    <span class="keyword">val</span> url=<span class="string">"jdbc:mysql://localhost:3306/mydb?characterEncoding=UTF-8"</span></span><br><span class="line">    <span class="comment">//3.2 指定要加载的表名</span></span><br><span class="line">    <span class="keyword">val</span> student=<span class="string">"students"</span></span><br><span class="line">    <span class="keyword">val</span> score=<span class="string">"scores"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.3 配置连接数据库的相关属性</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//用户名</span></span><br><span class="line">    properties.setProperty(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">    <span class="comment">//密码</span></span><br><span class="line">    properties.setProperty(<span class="string">"password"</span>,<span class="string">"123456"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> studentFrame: <span class="type">DataFrame</span> = spark.read.jdbc(url,student,properties)</span><br><span class="line">    <span class="keyword">val</span> scoreFrame: <span class="type">DataFrame</span> = spark.read.jdbc(url,score,properties)</span><br><span class="line">    <span class="comment">//把dataFrame注册成表</span></span><br><span class="line">    studentFrame.createTempView(<span class="string">"students"</span>)</span><br><span class="line">    scoreFrame.createOrReplaceTempView(<span class="string">"scores"</span>)</span><br><span class="line">    <span class="comment">//spark.sql("SELECT temp1.class,SUM(temp1.degree),AVG(temp1.degree) FROM (SELECT  students.sno AS ssno,students.sname,students.ssex,students.sbirthday,students.class, scores.sno,scores.degree,scores.cno  FROM students LEFT JOIN scores ON students.sno =  scores.sno ) temp1  GROUP BY temp1.class; ").show()</span></span><br><span class="line">    <span class="keyword">val</span> resultFrame: <span class="type">DataFrame</span> = spark.sql(<span class="string">"SELECT temp1.class,SUM(temp1.degree),AVG(temp1.degree)  FROM (SELECT  students.sno AS ssno,students.sname,students.ssex,students.sbirthday,students.class, scores.sno,scores.degree,scores.cno  FROM students LEFT JOIN scores ON students.sno =  scores.sno  WHERE degree &gt; 60 AND sbirthday &gt; '1973-01-01 00:00:00' ) temp1 GROUP BY temp1.class"</span>)</span><br><span class="line">    resultFrame.explain(<span class="literal">true</span>)</span><br><span class="line">    resultFrame.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过explain方法来查看sql的执行计划，得到以下信息</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">== Parsed Logical Plan ==</span><br><span class="line">'Aggregate ['temp1.class], ['temp1.class, unresolvedalias('SUM('temp1.degree), None), unresolvedalias('AVG('temp1.degree), None)]</span><br><span class="line">+- 'SubqueryAlias temp1</span><br><span class="line">   +- 'Project ['students.sno AS ssno<span class="comment">#16, 'students.sname, 'students.ssex, 'students.sbirthday, 'students.class, 'scores.sno, 'scores.degree, 'scores.cno]</span></span><br><span class="line">      +- 'Filter (('degree &gt; 60) &amp;&amp; ('sbirthday &gt; 1973-01-01 00:00:00))</span><br><span class="line">         +- 'Join LeftOuter, ('students.sno = 'scores.sno)</span><br><span class="line">            :- 'UnresolvedRelation `students`</span><br><span class="line">            +- 'UnresolvedRelation `scores`</span><br><span class="line"></span><br><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">class: string, sum(degree): decimal(20,1), avg(degree): decimal(14,5)</span><br><span class="line">Aggregate [class<span class="comment">#4], [class#4, sum(degree#12) AS sum(degree)#27, avg(degree#12) AS avg(degree)#28]</span></span><br><span class="line">+- SubqueryAlias temp1</span><br><span class="line">   +- Project [sno<span class="comment">#0 AS ssno#16, sname#1, ssex#2, sbirthday#3, class#4, sno#10, degree#12, cno#11]</span></span><br><span class="line">      +- Filter ((cast(degree<span class="comment">#12 as decimal(10,1)) &gt; cast(cast(60 as decimal(2,0)) as decimal(10,1))) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00))</span></span><br><span class="line">         +- Join LeftOuter, (sno<span class="comment">#0 = sno#10)</span></span><br><span class="line">            :- SubqueryAlias students</span><br><span class="line">            :  +- Relation[sno<span class="comment">#0,sname#1,ssex#2,sbirthday#3,class#4] JDBCRelation(students) [numPartitions=1]</span></span><br><span class="line">            +- SubqueryAlias scores</span><br><span class="line">               +- Relation[sno<span class="comment">#10,cno#11,degree#12] JDBCRelation(scores) [numPartitions=1]</span></span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Aggregate [class<span class="comment">#4], [class#4, sum(degree#12) AS sum(degree)#27, cast((avg(UnscaledValue(degree#12)) / 10.0) as decimal(14,5)) AS avg(degree)#28]</span></span><br><span class="line">+- Project [class<span class="comment">#4, degree#12]</span></span><br><span class="line">   +- Join Inner, (sno<span class="comment">#0 = sno#10)</span></span><br><span class="line">      :- Project [sno<span class="comment">#0, class#4]</span></span><br><span class="line">      :  +- Filter ((isnotnull(sbirthday<span class="comment">#3) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)) &amp;&amp; isnotnull(sno#0))</span></span><br><span class="line">      :     +- Relation[sno<span class="comment">#0,sname#1,ssex#2,sbirthday#3,class#4] JDBCRelation(students) [numPartitions=1]</span></span><br><span class="line">      +- Project [sno<span class="comment">#10, degree#12]</span></span><br><span class="line">         +- Filter ((isnotnull(degree<span class="comment">#12) &amp;&amp; (degree#12 &gt; 60.0)) &amp;&amp; isnotnull(sno#10))</span></span><br><span class="line">            +- Relation[sno<span class="comment">#10,cno#11,degree#12] JDBCRelation(scores) [numPartitions=1]</span></span><br><span class="line"></span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(6) HashAggregate(keys=[class#4], functions=[sum(degree#12), avg(UnscaledValue(degree#12))], output=[class#4, sum(degree)#27, avg(degree)#28])</span><br><span class="line">+- Exchange hashpartitioning(class<span class="comment">#4, 200)</span></span><br><span class="line">   +- *(5) HashAggregate(keys=[class#4], functions=[partial_sum(degree#12), partial_avg(UnscaledValue(degree#12))], output=[class#4, sum#32, sum#33, count#34L])</span><br><span class="line">      +- *(5) Project [class#4, degree#12]</span><br><span class="line">         +- *(5) SortMergeJoin [sno#0], [sno#10], Inner</span><br><span class="line">            :- *(2) Sort [sno#0 ASC NULLS FIRST], false, 0</span><br><span class="line">            :  +- Exchange hashpartitioning(sno<span class="comment">#0, 200)</span></span><br><span class="line">            :     +- *(1) Project [sno#0, class#4]</span><br><span class="line">            :        +- *(1) Filter (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)</span><br><span class="line">            :           +- *(1) Scan JDBCRelation(students) [numPartitions=1] [sno#0,class#4,sbirthday#3] PushedFilters: [*IsNotNull(sbirthday), *IsNotNull(sno)], ReadSchema: struct&lt;sno:string,class:string,sbirthday:timestamp&gt;</span><br><span class="line">            +- *(4) Sort [sno#10 ASC NULLS FIRST], false, 0</span><br><span class="line">               +- Exchange hashpartitioning(sno<span class="comment">#10, 200)</span></span><br><span class="line">                  +- *(3) Scan JDBCRelation(scores) [numPartitions=1] [sno#10,degree#12] PushedFilters: [*IsNotNull(degree), *GreaterThan(degree,60.0), *IsNotNull(sno)], ReadSchema: struct&lt;sno:string,degree:decimal(10,1)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="2-catalyst执行过程"><a class="markdownIt-Anchor" href="#2-catalyst执行过程"></a> 2、 Catalyst执行过程</h2>
<p>从上面的查询计划我们可以看得出来，我们编写的sql语句，经过多次转换，最终进行编译成为字节码文件进行执行，这一整个过程经过了好多个步骤，其中包括以下几个重要步骤</p>
<ul>
<li>sql解析阶段 parse</li>
<li>生成逻辑计划  Analyzer</li>
<li>sql语句调优阶段  Optimizer</li>
<li>生成物理查询计划  planner</li>
</ul>
<h3 id="1-sql解析阶段-parser"><a class="markdownIt-Anchor" href="#1-sql解析阶段-parser"></a> 1、sql解析阶段 Parser</h3>
<p>在spark2.x的版本当中，为了解析sparkSQL的sql语句，引入了<mark>Antlr</mark>。Antlr 是一款强大的语法生成器工具，可用于读取、处理、执行和翻译结构化的文本或二进制文件，是当前 Java 语言中使用最为广泛的语法生成器工具，我们常见的大数据 SQL 解析都用到了这个工具，包括 Hive、Cassandra、Phoenix、Pig 以及 presto 等。目前最新版本的 Spark 使用的是<mark>ANTLR4</mark>，通过这个对 SQL 进行词法分析并构建语法树。</p>
<p>我们可以通过github去查看spark的源码，具体路径如下：</p>
<p><a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4</a></p>
<p>查看得到sparkSQL支持的SQL语法，所有sparkSQL支持的语法都定义在了这个文件当中。如果我们需要重构sparkSQL的语法，那么我们只需要重新定义好相关语法，然后使用Antlr4对SqlBase.g4进行语法解析，生成相关的java类，其中就包含重要的词法解析器SqlBaseLexer.java和语法解析器SqlBaseParser.java。在我们运行上面的java的时候，第一步就是通过SqlBaseLexer来解析关键词以及各种标识符，然后使用SqlBaseParser来构建语法树。</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/Untitled%20Diagram-1591500000169.png" alt="Untitled Diagram" /></p>
<p>最终通过Lexer以及parse解析之后，生成语法树，生成语法树之后，使用AstBuilder将语法树转换成为LogicalPlan，这个LogicalPlan也被称为Unresolved  LogicalPlan。解析之后的逻辑计划如下，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">== Parsed Logical Plan ==</span><br><span class="line">&apos;Aggregate [&apos;temp1.class], [&apos;temp1.class, unresolvedalias(&apos;SUM(&apos;temp1.degree), None), unresolvedalias(&apos;AVG(&apos;temp1.degree), None)]</span><br><span class="line">+- &apos;SubqueryAlias temp1</span><br><span class="line">   +- &apos;Project [&apos;students.sno AS ssno#16, &apos;students.sname, &apos;students.ssex, &apos;students.sbirthday, &apos;students.class, &apos;scores.sno, &apos;scores.degree, &apos;scores.cno]</span><br><span class="line">      +- &apos;Filter ((&apos;degree &gt; 60) &amp;&amp; (&apos;sbirthday &gt; 1973-01-01 00:00:00))</span><br><span class="line">         +- &apos;Join LeftOuter, (&apos;students.sno = &apos;scores.sno)</span><br><span class="line">            :- &apos;UnresolvedRelation `students`</span><br><span class="line">            +- &apos;UnresolvedRelation `scores`</span><br></pre></td></tr></table></figure>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/unresolved%20Logical%20Plan.png" alt="unresolved Logical Plan" /></p>
<p>从上图可以看得到，两个表被join之后生成了UnresolvedRelation，选择的列以及聚合的字段都有了，sql解析的第一个阶段就已经完成，接着准备进入到第二个阶段</p>
<h3 id="2-绑定逻辑计划analyzer"><a class="markdownIt-Anchor" href="#2-绑定逻辑计划analyzer"></a> 2、绑定逻辑计划Analyzer</h3>
<p>在sql解析parse阶段，生成了很多的unresolvedalias ， UnresolvedRelation等很多未解析出来的有些关键字，这些都是属于 Unresolved LogicalPlan解析的部分。 Unresolved LogicalPlan仅仅是一种数据结构，不包含任何数据信息，例如不知道数据源，数据类型，不同的列来自哪张表等等。。Analyzer 阶段会使用事先定义好的 Rule 以及 SessionCatalog 等信息对 Unresolved LogicalPlan 进行 transform。SessionCatalog 主要用于各种<mark>函数资源信息和元数据信息</mark>（数据库、数据表、数据视图、数据分区与函数等）的统一管理。而Rule 是定义在 Analyzer 里面的，具体的类的路径如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">org.apache.spark.sql.catalyst.analysis.<span class="type">Analyzer</span></span><br><span class="line"></span><br><span class="line">具体的rule规则定义如下：</span><br><span class="line"> <span class="keyword">lazy</span> <span class="keyword">val</span> batches: <span class="type">Seq</span>[<span class="type">Batch</span>] = <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"Hints"</span>, fixedPoint,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ResolveHints</span>.<span class="type">ResolveBroadcastHints</span>(conf),</span><br><span class="line">      <span class="type">ResolveHints</span>.<span class="type">RemoveAllHints</span>),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"Simple Sanity Check"</span>, <span class="type">Once</span>,</span><br><span class="line">      <span class="type">LookupFunctions</span>),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"Substitution"</span>, fixedPoint,</span><br><span class="line">      <span class="type">CTESubstitution</span>,</span><br><span class="line">      <span class="type">WindowsSubstitution</span>,</span><br><span class="line">      <span class="type">EliminateUnions</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SubstituteUnresolvedOrdinals</span>(conf)),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"Resolution"</span>, fixedPoint,</span><br><span class="line">      <span class="type">ResolveTableValuedFunctions</span> ::</span><br><span class="line">      <span class="type">ResolveRelations</span> ::</span><br><span class="line">      <span class="type">ResolveReferences</span> ::</span><br><span class="line">      <span class="type">ResolveCreateNamedStruct</span> ::</span><br><span class="line">      <span class="type">ResolveDeserializer</span> ::</span><br><span class="line">      <span class="type">ResolveNewInstance</span> ::</span><br><span class="line">      <span class="type">ResolveUpCast</span> ::</span><br><span class="line">      <span class="type">ResolveGroupingAnalytics</span> ::</span><br><span class="line">      <span class="type">ResolvePivot</span> ::</span><br><span class="line">      <span class="type">ResolveOrdinalInOrderByAndGroupBy</span> ::</span><br><span class="line">      <span class="type">ResolveAggAliasInGroupBy</span> ::</span><br><span class="line">      <span class="type">ResolveMissingReferences</span> ::</span><br><span class="line">      <span class="type">ExtractGenerator</span> ::</span><br><span class="line">      <span class="type">ResolveGenerate</span> ::</span><br><span class="line">      <span class="type">ResolveFunctions</span> ::</span><br><span class="line">      <span class="type">ResolveAliases</span> ::</span><br><span class="line">      <span class="type">ResolveSubquery</span> ::</span><br><span class="line">      <span class="type">ResolveSubqueryColumnAliases</span> ::</span><br><span class="line">      <span class="type">ResolveWindowOrder</span> ::</span><br><span class="line">      <span class="type">ResolveWindowFrame</span> ::</span><br><span class="line">      <span class="type">ResolveNaturalAndUsingJoin</span> ::</span><br><span class="line">      <span class="type">ExtractWindowExpressions</span> ::</span><br><span class="line">      <span class="type">GlobalAggregates</span> ::</span><br><span class="line">      <span class="type">ResolveAggregateFunctions</span> ::</span><br><span class="line">      <span class="type">TimeWindowing</span> ::</span><br><span class="line">      <span class="type">ResolveInlineTables</span>(conf) ::</span><br><span class="line">      <span class="type">ResolveTimeZone</span>(conf) ::</span><br><span class="line">      <span class="type">ResolvedUuidExpressions</span> ::</span><br><span class="line">      <span class="type">TypeCoercion</span>.typeCoercionRules(conf) ++</span><br><span class="line">      extendedResolutionRules : _*),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"Post-Hoc Resolution"</span>, <span class="type">Once</span>, postHocResolutionRules: _*),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"View"</span>, <span class="type">Once</span>,</span><br><span class="line">      <span class="type">AliasViewChild</span>(conf)),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"Nondeterministic"</span>, <span class="type">Once</span>,</span><br><span class="line">      <span class="type">PullOutNondeterministic</span>),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"UDF"</span>, <span class="type">Once</span>,</span><br><span class="line">      <span class="type">HandleNullInputsForUDF</span>),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"FixNullability"</span>, <span class="type">Once</span>,</span><br><span class="line">      <span class="type">FixNullability</span>),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"Subquery"</span>, <span class="type">Once</span>,</span><br><span class="line">      <span class="type">UpdateOuterReferences</span>),</span><br><span class="line">    <span class="type">Batch</span>(<span class="string">"Cleanup"</span>, fixedPoint,</span><br><span class="line">      <span class="type">CleanupAliases</span>)</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>
<p>从上面代码可以看出，多个性质类似的 Rule 组成一个 Batch，比如上面名为 Hints 的 Batch就是由很多个 Hints Rule 组成；而多个 Batch 构成一个 batches。这些 batches 会由 RuleExecutor 执行，先按一个一个 Batch 顺序执行，然后对 Batch 里面的每个 Rule 顺序执行。每个 Batch 会执行一次（Once）或多次（FixedPoint，由<br />
<code>spark.sql.optimizer.maxIterations</code> 参数决定），执行过程如下：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/spark_batch_ruleexecutor-iteblog.jpg" alt="spark_batch_ruleexecutor-iteblog" /></p>
<p>所以上面的 SQL 经过这个阶段生成的 Analyzed Logical Plan 如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">class: string, sum(degree): decimal(20,1), avg(degree): decimal(14,5)</span><br><span class="line">Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, avg(degree#12) AS avg(degree)#28]</span><br><span class="line">+- SubqueryAlias temp1</span><br><span class="line">   +- Project [sno#0 AS ssno#16, sname#1, ssex#2, sbirthday#3, class#4, sno#10, degree#12, cno#11]</span><br><span class="line">      +- Filter ((cast(degree#12 as decimal(10,1)) &gt; cast(cast(60 as decimal(2,0)) as decimal(10,1))) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00))</span><br><span class="line">         +- Join LeftOuter, (sno#0 = sno#10)</span><br><span class="line">            :- SubqueryAlias students</span><br><span class="line">            :  +- Relation[sno#0,sname#1,ssex#2,sbirthday#3,class#4] JDBCRelation(students) [numPartitions=1]</span><br><span class="line">            +- SubqueryAlias scores</span><br><span class="line">               +- Relation[sno#10,cno#11,degree#12] JDBCRelation(scores) [numPartitions=1]</span><br></pre></td></tr></table></figure>
<p>从上面的解析过程来看，students和scores表已经被解析成为了带有sno#0 AS ssno#16, sname#1, ssex#2, sbirthday#3, class#4, sno#10, degree#12, cno#11这么具体的字段，其中还有聚合函数</p>
<p>Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, avg(degree#12) AS avg(degree)#28]，并且最终返回的四个字段的类型也已经确定了class: string, sum(degree): decimal(20,1), avg(degree): decimal(14,5)，而且也已经知道了数据来源是JDBCRelation(students)表和 JDBCRelation(scores)表。总结来看Analyzed Logical Plan主要就是干了一些这些事情</p>
<p>1、确定最终返回字段名称以及返回类型：</p>
<ul>
<li>​	class: string, sum(degree): decimal(20,1), avg(degree): decimal(14,5)</li>
</ul>
<p>2、确定聚合函数</p>
<ul>
<li>Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, avg(degree#12) AS avg(degree)#28]</li>
</ul>
<p>3、确定表当中获取的查询字段</p>
<ul>
<li>​	Project [sno#0 AS ssno#16, sname#1, ssex#2, sbirthday#3, class#4, sno#10, degree#12, cno#11]</li>
</ul>
<p>4、确定过滤条件</p>
<p>Filter ((cast(degree#12 as decimal(10,1)) &gt; cast(cast(60 as decimal(2,0)) as decimal(10,1))) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00))</p>
<p>5、确定join方式</p>
<p>Join LeftOuter, (sno#0 = sno#10)</p>
<p>6、确定表当中的数据来源以及分区个数</p>
<ul>
<li>
<p>JDBCRelation(students) [numPartitions=1]</p>
</li>
<li>
<p>JDBCRelation(scores) [numPartitions=1]</p>
</li>
</ul>
<p>至此Analyzed Logical  Plan已经完成。对比Unresolved   Logical Plan到Analyzed Logical Plan 过程如下图</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/unresolved%20Logical%20Plan-1594954815185.png" alt="unresolved Logical Plan" /></p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/Analyzer%20Logical%20Plan.png" alt="Analyzer Logical Plan" /></p>
<p>到这里， Analyzed LogicalPlan 就完全生成了</p>
<h3 id="3-逻辑优化阶段optimizer"><a class="markdownIt-Anchor" href="#3-逻辑优化阶段optimizer"></a> 3、逻辑优化阶段Optimizer</h3>
<p>在前文的绑定逻辑计划阶段对 Unresolved LogicalPlan 进行相关 transform 操作得到了 Analyzed Logical Plan，这个 Analyzed Logical Plan 是可以直接转换成 Physical Plan 然后在 [Spark] 中执行。但是如果直接这么弄的话，得到的 Physical Plan 很可能不是最优的，因为在实际应用中，很多低效的写法会带来执行效率的问题，需要进一步对Analyzed Logical Plan 进行处理，得到更优的逻辑算子树。于是， 针对 SQL 逻辑算子树的优化器 Optimizer 应运而生。</p>
<p>这个阶段的优化器主要是基于规则的（Rule-based Optimizer，简称 <mark>RBO</mark>），而绝大部分的规则都是启发式规则，也就是基于直观或经验而得出的规则，比如<mark>列裁剪</mark>（过滤掉查询不需要使用到的列）、<mark>谓词下推</mark>（将过滤尽可能地下沉到数据源端）、常量累加（比如 1 + 2 这种事先计算好） 以及常量替换（比如 SELECT * FROM table WHERE i = 5 AND j = i + 3 可以转换成 SELECT * FROM table WHERE i = 5 AND j = 8）等等。</p>
<p>与前文介绍绑定逻辑计划阶段类似，这个阶段所有的规则也是实现 Rule 抽象类，多个规则组成一个 Batch，多个 Batch 组成一个 batches，同样也是在 RuleExecutor 中进行执行</p>
<p>这里按照 Rule 执行顺序一一进行说明。</p>
<h4 id="谓词下推"><a class="markdownIt-Anchor" href="#谓词下推"></a> 谓词下推</h4>
<p>谓词下推在 SparkQL 是由 <code>PushDownPredicate</code> 实现的，这个过程主要将过滤条件尽可能地下推到底层，最好是数据源。所以针对我们上面介绍的 SQL，使用谓词下推优化得到的逻辑计划如下：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/Analyzer%20Logical%20Plan-1591504429944.png" alt="Analyzer Logical Plan" /></p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8-1591504632746.png" alt="谓词下推" /></p>
<p>从上图可以看出，谓词下推将 Filter 算子直接下推到 Join 之前了（注意，上图是从下往上看的）</p>
<p>。也就是在扫描 student表的时候使用条件过滤条件过滤出满足条件的数据；同时在扫描 t2 表的时候会先使用 isnotnull(id#8) &amp;&amp; (id#8 &gt; 50000) 过滤条件过滤出满足条件的数据。经过这样的操作，可以大大减少 Join 算子处理的数据量，从而加快计算速度。</p>
<h4 id="列裁剪"><a class="markdownIt-Anchor" href="#列裁剪"></a> 列裁剪</h4>
<p>列裁剪在 Spark SQL 是由 <code>ColumnPruning</code> 实现的。因为我们查询的表可能有很多个字段，但是每次查询我们很大可能不需要扫描出所有的字段，这个时候利用列裁剪可以把那些查询不需要的字段过滤掉，使得扫描的数据量减少。所以针对我们上面介绍的 SQL，使用列裁剪优化得到的逻辑计划如下：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8-1591504632746-1594954838864.png" alt="谓词下推" /></p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/optimized.png" alt="optimized" /></p>
<p>从上图可以看出，经过列裁剪后，students 表只需要查询 sno和 class 两个字段；scores 表只需要查询 sno,degree 字段。这样减少了数据的传输，而且如果底层的文件格式为列存（比如 Parquet），可以大大提高数据的扫描速度的。</p>
<h4 id="常量替换"><a class="markdownIt-Anchor" href="#常量替换"></a> 常量替换</h4>
<p>常量替换在 Spark SQL 是由 <code>ConstantPropagation</code> 实现的。也就是将变量替换成常量，比如 SELECT * FROM table WHERE i = 5 AND j = i + 3 可以转换成 SELECT * FROM table WHERE i = 5 AND j = 8。这个看起来好像没什么的，但是如果扫描的行数非常多可以减少很多的计算时间的开销的。经过这个优化，得到的逻辑计划如下</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/spark_constantpropagation-iteblog.jpg" alt="spark_constantpropagation-iteblog" /></p>
<p>我们的查询中有 <code>t1.cid = 1 AND t1.did = t1.cid + 1</code> 查询语句，从里面可以看出 t1.cid 其实已经是确定的值了，所以我们完全可以使用它计算出 t1.did。</p>
<h4 id="常量累加"><a class="markdownIt-Anchor" href="#常量累加"></a> 常量累加</h4>
<p>常量累加在 Spark SQL 是由 <code>ConstantFolding</code> 实现的。这个和常量替换类似，也是在这个阶段把一些常量表达式事先计算好。这个看起来改动的不大，但是在数据量非常大的时候可以减少大量的计算，减少 CPU 等资源的使用。经过这个优化，得到的逻辑计划如下：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/spark_constantfolding-iteblog.jpg" alt="spark_constantfolding-iteblog" /></p>
<p>所以经过上面四个步骤的优化之后，得到的优化之后的逻辑计划为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Aggregate [class#4], [class#4, sum(degree#12) AS sum(degree)#27, cast((avg(UnscaledValue(degree#12)) / 10.0) as decimal(14,5)) AS avg(degree)#28]</span><br><span class="line">+- Project [class#4, degree#12]</span><br><span class="line">   +- Join Inner, (sno#0 = sno#10)</span><br><span class="line">      :- Project [sno#0, class#4]</span><br><span class="line">      :  +- Filter ((isnotnull(sbirthday#3) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)) &amp;&amp; isnotnull(sno#0))</span><br><span class="line">      :     +- Relation[sno#0,sname#1,ssex#2,sbirthday#3,class#4] JDBCRelation(students) [numPartitions=1]</span><br><span class="line">      +- Project [sno#10, degree#12]</span><br><span class="line">         +- Filter ((isnotnull(degree#12) &amp;&amp; (degree#12 &gt; 60.0)) &amp;&amp; isnotnull(sno#10))</span><br><span class="line">            +- Relation[sno#10,cno#11,degree#12] JDBCRelation(scores) [numPartitions=1]</span><br></pre></td></tr></table></figure>
<p>到此为止，优化逻辑阶段基本完成，另外更多的其他优化，参见spark源码：</p>
<p><a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L59" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L59</a></p>
<h3 id="4-生成可执行的物理计划阶段physical-plan"><a class="markdownIt-Anchor" href="#4-生成可执行的物理计划阶段physical-plan"></a> 4、生成可执行的物理计划阶段Physical Plan</h3>
<p>经过前面多个步骤，包括parse，analyzer以及Optimizer等多个阶段，得到经过优化之后的sql语句，但是这个sql语句仍然不能执行，为了能够执行这个sql，最终必须得要翻译成为可以被执行的物理计划，到这个阶段spark就知道该如何执行这个sql了，和前面逻辑计划绑定和优化不一样，这个阶段使用的是策略strategy，而且经过前面介绍的逻辑计划绑定和 Transformations 动作之后，树的类型并没有改变，也就是说：Expression 经过 Transformations 之后得到的还是 Transformations ；Logical Plan 经过 Transformations 之后得到的还是 Logical Plan。而到了这个阶段，经过 Transformations 动作之后，树的类型改变了，由 Logical Plan 转换成 Physical Plan 了。</p>
<p>一个逻辑计划（Logical Plan）经过一系列的策略处理之后，得到多个物理计划（Physical Plans），物理计划在 Spark 是由 SparkPlan 实现的。多个物理计划再经过代价模型（Cost Model）得到选择后的物理计划（Selected Physical Plan），整个过程如下所示：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1591506889458.png" alt="1591506889458" /></p>
<p>Cost Model 对应的就是基于代价的优化（Cost-based Optimizations，CBO，主要由华为的大佬们实现的，详见 <a href="https://issues.apache.org/jira/browse/SPARK-16026" target="_blank" rel="noopener">SPARK-16026</a> ），核心思想是计算每个物理计划的代价，然后得到最优的物理计划。但是在目前最新版的 Spark 2.4.3，这一部分并没有实现，直接返回多个物理计划列表的第一个作为最优的物理计划，如下</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> sparkPlan: <span class="type">SparkPlan</span> = &#123;</span><br><span class="line">    <span class="type">SparkSession</span>.setActiveSession(sparkSession)</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> We use next(), i.e. take the first plan returned by the planner, here for now,</span></span><br><span class="line">    <span class="comment">//       but we will implement to choose the best plan.</span></span><br><span class="line">    planner.plan(<span class="type">ReturnAnswer</span>(optimizedPlan)).next()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而 <a href="https://issues.apache.org/jira/browse/SPARK-16026" target="_blank" rel="noopener">SPARK-16026</a> 引入的 CBO 优化主要是在前面介绍的<strong>优化逻辑计划阶段 - Optimizer</strong> 阶段进行的，对应的 Rule 为 <code>CostBasedJoinReorder</code>，并且默认是关闭的，需要通过 <code>spark.sql.cbo.enabled</code> 或 <code>spark.sql.cbo.joinReorder.enabled</code> 参数开启。<br />
所以到了这个节点，最后得到的物理计划如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">== Physical Plan ==</span><br><span class="line">*(6) HashAggregate(keys=[class#4], functions=[sum(degree#12), avg(UnscaledValue(degree#12))], output=[class#4, sum(degree)#27, avg(degree)#28])</span><br><span class="line">+- Exchange hashpartitioning(class#4, 200)</span><br><span class="line">   +- *(5) HashAggregate(keys=[class#4], functions=[partial_sum(degree#12), partial_avg(UnscaledValue(degree#12))], output=[class#4, sum#32, sum#33, count#34L])</span><br><span class="line">      +- *(5) Project [class#4, degree#12]</span><br><span class="line">         +- *(5) SortMergeJoin [sno#0], [sno#10], Inner</span><br><span class="line">            :- *(2) Sort [sno#0 ASC NULLS FIRST], false, 0</span><br><span class="line">            :  +- Exchange hashpartitioning(sno#0, 200)</span><br><span class="line">            :     +- *(1) Project [sno#0, class#4]</span><br><span class="line">            :        +- *(1) Filter (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)</span><br><span class="line">            :           +- *(1) Scan JDBCRelation(students) [numPartitions=1] [sno#0,class#4,sbirthday#3] PushedFilters: [*IsNotNull(sbirthday), *IsNotNull(sno)], ReadSchema: struct&lt;sno:string,class:string,sbirthday:timestamp&gt;</span><br><span class="line">            +- *(4) Sort [sno#10 ASC NULLS FIRST], false, 0</span><br><span class="line">               +- Exchange hashpartitioning(sno#10, 200)</span><br><span class="line">                  +- *(3) Scan JDBCRelation(scores) [numPartitions=1] [sno#10,degree#12] PushedFilters: [*IsNotNull(degree), *GreaterThan(degree,60.0), *IsNotNull(sno)], ReadSchema: struct&lt;sno:string,degree:decimal(10,1)&gt;</span><br></pre></td></tr></table></figure>
<p>从上面的结果可以看出，物理计划阶段已经知道数据源是从 JDBC里面读取了，也知道文件的路径，数据类型等。而且在读取文件的时候，直接将过滤条件（PushedFilters）加进去了</p>
<p>同时，这个 Join 变成了 SortMergeJoin，</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/logical%20Plan.png" alt="logical Plan" /></p>
<p>到这里， Physical Plan 就完全生成了</p>
<h3 id="5-代码生成阶段"><a class="markdownIt-Anchor" href="#5-代码生成阶段"></a> 5、代码生成阶段</h3>
<p>从以上多个过程执行完成之后，例如parser，analyzer，Optimizer，physicalPlan等，最终我们得到的物理执行计划，这个物理执行计划标明了整个的代码执行过程当中我们代码层面的执行过程，以及最终要得到的数据字段以及字段类型，也包含了我们对应的数据源的位置，虽然得到了物理执行计划，但是这个物理执行计划想要被执行，最终还是得要生成完整的代码，底层还是基于sparkRDD去进行处理的，spark最后也还会有一些Rule对生成的物理执行计划进行处理，这个处理过程就是prepareForExecution，这些rule规则定义在</p>
<p>org.apache.spark.sql.execution.QueryExecution 这个类当中的这个方法里面</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareForExecution</span></span>(plan: <span class="type">SparkPlan</span>): <span class="type">SparkPlan</span> = &#123;</span><br><span class="line">   preparations.foldLeft(plan) &#123; <span class="keyword">case</span> (sp, rule) =&gt; rule.apply(sp) &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/** A sequence of rules that will be applied in order to the physical plan before execution. */</span></span><br><span class="line"> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">preparations</span></span>: <span class="type">Seq</span>[<span class="type">Rule</span>[<span class="type">SparkPlan</span>]] = <span class="type">Seq</span>(</span><br><span class="line">   python.<span class="type">ExtractPythonUDFs</span>,  <span class="comment">//抽取python的自定义函数</span></span><br><span class="line">   <span class="type">PlanSubqueries</span>(sparkSession),  <span class="comment">//子查询物理计划处理</span></span><br><span class="line">   <span class="type">EnsureRequirements</span>(sparkSession.sessionState.conf),  <span class="comment">//确保执行计划分区排序正确</span></span><br><span class="line">   <span class="type">CollapseCodegenStages</span>(sparkSession.sessionState.conf), <span class="comment">//收集生成代码</span></span><br><span class="line">   <span class="type">ReuseExchange</span>(sparkSession.sessionState.conf),  <span class="comment">//节点重用</span></span><br><span class="line">   <span class="type">ReuseSubquery</span>(sparkSession.sessionState.conf))  <span class="comment">//子查询重用</span></span><br></pre></td></tr></table></figure>
<p>上面的 Rule 中 <code>CollapseCodegenStages</code> 是重头戏，这就是大家熟知的全代码阶段生成，Catalyst 全阶段代码生成的入口就是这个规则。当然，如果需要 Spark 进行全阶段代码生成，需要将 <code>spark.sql.codegen.wholeStage</code> 设置为 true（默认）。</p>
<h4 id="生成代码与sql解析引擎的区别"><a class="markdownIt-Anchor" href="#生成代码与sql解析引擎的区别"></a> 生成代码与sql解析引擎的区别</h4>
<p>在sparkSQL当中，通过生成代码，来实现sql语句的最终生成，说白了最后底层执行的还是代码，那么为什么要这么麻烦，使用代码的方式来执行我们的sql语句，难道没有sql的解析引擎直接执行sql语句嘛？当然是有的，在spark2.0版本之前使用的都是基于Volcano Iterator Model（参见 <a href="http://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf" target="_blank" rel="noopener">《Volcano-An Extensible and Parallel Query Evaluation System》</a>） 来实现sql的解析的，这个是由 Goetz Graefe 在 1993 年提出的，当今绝大多数数据库系统处理 SQL 在底层都是基于这个模型的。这个模型的执行可以概括为：首先数据库引擎会将 SQL 翻译成一系列的关系代数算子或表达式，然后依赖这些关系代数算子逐条处理输入数据并产生结果。每个算子在底层都实现同样的接口，比如都实现了 next() 方法，然后最顶层的算子 next() 调用子算子的 next()，子算子的 next() 在调用孙算子的 next()，直到最底层的 next()，具体过程如下图表示：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/%E5%8E%9F%E5%A7%8B%E8%A7%A3%E6%9E%90sql%E6%96%B9%E5%BC%8F.png" alt="原始解析sql方式" /></p>
<p>Volcano Iterator Model 的优点是抽象起来很简单，很容易实现，而且可以通过任意组合算子来表达复杂的查询。但是缺点也很明显，存在大量的<mark>虚函数调用</mark>，会引起 CPU 的中断，最终影响了执行效率。<a href="https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html" target="_blank" rel="noopener">databricks的官方博客</a>对比过使用 Volcano Iterator Model 和手写代码的执行效率，结果发现<mark>手写的代码执行效率要高出十倍</mark>！</p>
<p>所以总结起来就是将sql解析成为代码，比sql引擎直接解析sql语句效率要快，所以spark2.0最终选择使用代码生成的方式来执行sql语句</p>
<p>基于上面的发现，从 Apache Spark 2.0 开始，社区开始引入了 Whole-stage Code Generation，参见 <a href="https://issues.apache.org/jira/browse/SPARK-12795" target="_blank" rel="noopener">SPARK-12795</a>，主要就是想通过这个来模拟手写代码，从而提升 Spark SQL 的执行效率。Whole-stage Code Generation 来自于2011年 Thomas Neumann 发表的 <a href="http://www.vldb.org/pvldb/vol4/p539-neumann.pdf" target="_blank" rel="noopener">Efficiently Compiling Efficient Query Plans for Modern Hardware</a>论文，这个也是 Tungsten 计划的一部分。</p>
<p>Tungsten 代码生成分为三部分：</p>
<ul>
<li>表达式代码生成（expression codegen）</li>
<li>全阶段代码生成（Whole-stage Code Generation）</li>
<li>加速序列化和反序列化（speed up serialization/deserialization）</li>
</ul>
<h4 id="表达式代码生成expression-codegen"><a class="markdownIt-Anchor" href="#表达式代码生成expression-codegen"></a> 表达式代码生成（expression codegen）</h4>
<p>这个其实在 Spark 1.x 就有了。表达式代码生成的基类是 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator，其下有七个子类：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1591525895593.png" alt="1591525895593" /></p>
<p>我们前文的 SQL 生成的逻辑计划中的 <code>(isnotnull(sbirthday#3) &amp;&amp; (cast(sbirthday#3 as string) &gt; 1973-01-01 00:00:00)</code> 就是最基本的表达式。它也是一种 Predicate，所以会调用 org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate 来生成表达式的代码。</p>
<h4 id="全阶段代码生成whole-stage-code-generation"><a class="markdownIt-Anchor" href="#全阶段代码生成whole-stage-code-generation"></a> 全阶段代码生成（Whole-stage Code Generation）</h4>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/%E5%85%A8%E9%98%B6%E6%AE%B5%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90.png" alt="全阶段代码生成" /></p>
<p>全阶段代码生成（Whole-stage Code Generation），用来将多个处理逻辑整合到单个代码模块中，其中也会用到上面的表达式代码生成。和前面介绍的表达式代码生成不一样，这个是对整个 SQL 过程进行代码生成，前面的表达式代码生成仅对于表达式的。全阶段代码生成都是继承自 org.apache.spark.sql.execution.BufferedRowIterator 的，生成的代码需要实现 processNext() 方法，这个方法会在 org.apache.spark.sql.execution.WholeStageCodegenExec 里面的 doExecute 方法里面被调用。而这个方法里面的 rdd 会将数据传进生成的代码里面 ，比如我们上文 SQL 这个例子的数据源是 JDBC文件，底层使用 org.apache.spark.sql.execution.RowDataSourceScanExec这个类读取文件，然后生成 inputRDD，这个 rdd 在 WholeStageCodegenExec 类中的 doExecute 方法里面调用生成的代码，然后执行我们各种判断得到最后的结果。WholeStageCodegenExec 类中的 doExecute 方法部分代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * WholeStageCodegen compiles a subtree of plans that support codegen together into single Java</span></span><br><span class="line"><span class="comment"> * function.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Here is the call graph of to generate Java source (plan A supports codegen, but plan B does not):</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   WholeStageCodegen       Plan A               FakeInput        Plan B</span></span><br><span class="line"><span class="comment"> * =========================================================================</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * -&gt; execute()</span></span><br><span class="line"><span class="comment"> *     |</span></span><br><span class="line"><span class="comment"> *  doExecute() ---------&gt;   inputRDDs() -------&gt; inputRDDs() ------&gt; execute()</span></span><br><span class="line"><span class="comment"> *     |</span></span><br><span class="line"><span class="comment"> *     +-----------------&gt;   produce()</span></span><br><span class="line"><span class="comment"> *                             |</span></span><br><span class="line"><span class="comment"> *                          doProduce()  -------&gt; produce()</span></span><br><span class="line"><span class="comment"> *                                                   |</span></span><br><span class="line"><span class="comment"> *                                                doProduce()</span></span><br><span class="line"><span class="comment"> *                                                   |</span></span><br><span class="line"><span class="comment"> *                         doConsume() &lt;--------- consume()</span></span><br><span class="line"><span class="comment"> *                             |</span></span><br><span class="line"><span class="comment"> *  doConsume()  &lt;--------  consume()</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * SparkPlan A should override `doProduce()` and `doConsume()`.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * `doCodeGen()` will create a `CodeGenContext`, which will hold a list of variables for input,</span></span><br><span class="line"><span class="comment"> * used to generated code for [[BoundReference]].</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doExecute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> (ctx, cleanedSource) = doCodeGen()</span><br><span class="line">    <span class="comment">// try to compile and fallback if it failed</span></span><br><span class="line">    <span class="keyword">val</span> (_, maxCodeSize) = <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">CodeGenerator</span>.compile(cleanedSource)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">Exception</span> <span class="keyword">if</span> !<span class="type">Utils</span>.isTesting &amp;&amp; sqlContext.conf.codegenFallback =&gt;</span><br><span class="line">        <span class="comment">// We should already saw the error message</span></span><br><span class="line">        logWarning(<span class="string">s"Whole-stage codegen disabled for plan (id=<span class="subst">$codegenStageId</span>):\n <span class="subst">$treeString</span>"</span>)</span><br><span class="line">        <span class="keyword">return</span> child.execute()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Check if compiled code has a too large function</span></span><br><span class="line">    <span class="keyword">if</span> (maxCodeSize &gt; sqlContext.conf.hugeMethodLimit) &#123;</span><br><span class="line">      logInfo(<span class="string">s"Found too long generated codes and JIT optimization might not work: "</span> +</span><br><span class="line">        <span class="string">s"the bytecode size (<span class="subst">$maxCodeSize</span>) is above the limit "</span> +</span><br><span class="line">        <span class="string">s"<span class="subst">$&#123;sqlContext.conf.hugeMethodLimit&#125;</span>, and the whole-stage codegen was disabled "</span> +</span><br><span class="line">        <span class="string">s"for this plan (id=<span class="subst">$codegenStageId</span>). To avoid this, you can raise the limit "</span> +</span><br><span class="line">        <span class="string">s"`<span class="subst">$&#123;SQLConf.WHOLESTAGE_HUGE_METHOD_LIMIT.key&#125;</span>`:\n<span class="subst">$treeString</span>"</span>)</span><br><span class="line">      child <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">// The fallback solution of batch file source scan still uses WholeStageCodegenExec</span></span><br><span class="line">        <span class="keyword">case</span> f: <span class="type">FileSourceScanExec</span> <span class="keyword">if</span> f.supportsBatch =&gt; <span class="comment">// do nothing</span></span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="keyword">return</span> child.execute()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> references = ctx.references.toArray</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> durationMs = longMetric(<span class="string">"pipelineTime"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdds = child.asInstanceOf[<span class="type">CodegenSupport</span>].inputRDDs()</span><br><span class="line">    assert(rdds.size &lt;= <span class="number">2</span>, <span class="string">"Up to two input RDDs can be supported"</span>)</span><br><span class="line">    <span class="keyword">if</span> (rdds.length == <span class="number">1</span>) &#123;</span><br><span class="line">      rdds.head.mapPartitionsWithIndex &#123; (index, iter) =&gt;</span><br><span class="line">        <span class="keyword">val</span> (clazz, _) = <span class="type">CodeGenerator</span>.compile(cleanedSource)</span><br><span class="line">        <span class="keyword">val</span> buffer = clazz.generate(references).asInstanceOf[<span class="type">BufferedRowIterator</span>]</span><br><span class="line">        buffer.init(index, <span class="type">Array</span>(iter))</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">InternalRow</span>] &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> v = buffer.hasNext</span><br><span class="line">            <span class="keyword">if</span> (!v) durationMs += buffer.durationMs()</span><br><span class="line">            v</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>: <span class="type">InternalRow</span> = buffer.next()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Right now, we support up to two input RDDs.</span></span><br><span class="line">      rdds.head.zipPartitions(rdds(<span class="number">1</span>)) &#123; (leftIter, rightIter) =&gt;</span><br><span class="line">        <span class="type">Iterator</span>((leftIter, rightIter))</span><br><span class="line">        <span class="comment">// a small hack to obtain the correct partition index</span></span><br><span class="line">      &#125;.mapPartitionsWithIndex &#123; (index, zippedIter) =&gt;</span><br><span class="line">        <span class="keyword">val</span> (leftIter, rightIter) = zippedIter.next()</span><br><span class="line">        <span class="keyword">val</span> (clazz, _) = <span class="type">CodeGenerator</span>.compile(cleanedSource)</span><br><span class="line">        <span class="keyword">val</span> buffer = clazz.generate(references).asInstanceOf[<span class="type">BufferedRowIterator</span>]</span><br><span class="line">        buffer.init(index, <span class="type">Array</span>(leftIter, rightIter))</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">InternalRow</span>] &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> v = buffer.hasNext</span><br><span class="line">            <span class="keyword">if</span> (!v) durationMs += buffer.durationMs()</span><br><span class="line">            v</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>: <span class="type">InternalRow</span> = buffer.next()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>在WholeStageCodegenExec 这个类的注释当中也说明了，最终生成的代码过程如下</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * WholeStageCodegen compiles a subtree of plans that support codegen together into single Java</span></span><br><span class="line"><span class="comment"> * function.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Here is the call graph of to generate Java source (plan A supports codegen, but plan B does not):</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   WholeStageCodegen       Plan A               FakeInput        Plan B</span></span><br><span class="line"><span class="comment"> * =========================================================================</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * -&gt; execute()</span></span><br><span class="line"><span class="comment"> *     |</span></span><br><span class="line"><span class="comment"> *  doExecute() ---------&gt;   inputRDDs() -------&gt; inputRDDs() ------&gt; execute()</span></span><br><span class="line"><span class="comment"> *     |</span></span><br><span class="line"><span class="comment"> *     +-----------------&gt;   produce()</span></span><br><span class="line"><span class="comment"> *                             |</span></span><br><span class="line"><span class="comment"> *                          doProduce()  -------&gt; produce()</span></span><br><span class="line"><span class="comment"> *                                                   |</span></span><br><span class="line"><span class="comment"> *                                                doProduce()</span></span><br><span class="line"><span class="comment"> *                                                   |</span></span><br><span class="line"><span class="comment"> *                         doConsume() &lt;--------- consume()</span></span><br><span class="line"><span class="comment"> *                             |</span></span><br><span class="line"><span class="comment"> *  doConsume()  &lt;--------  consume()</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * SparkPlan A should override `doProduce()` and `doConsume()`.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * `doCodeGen()` will create a `CodeGenContext`, which will hold a list of variables for input,</span></span><br><span class="line"><span class="comment"> * used to generated code for [[BoundReference]].</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
<p>相比 Volcano Iterator Model，全阶段代码生成的执行过程如下：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1591534793978.png" alt="1591534793978" /></p>
<p>通过引入全阶段代码生成，大大减少了虚函数的调用，减少了 CPU 的调用，使得 SQL 的执行速度有很大提升。</p>
<h4 id="代码编译"><a class="markdownIt-Anchor" href="#代码编译"></a> 代码编译</h4>
<p>生成代码之后需要解决的另一个问题是如何将生成的代码进行编译然后加载到同一个 JVM 中去。在早期 Spark 版本是使用 Scala 的 Reflection 和 Quasiquotes 机制来实现代码生成的。Quasiquotes 是一个简洁的符号，可以让我们轻松操作 Scala 语法树，具体参见 <a href="https://docs.scala-lang.org/overviews/quasiquotes/intro.html" target="_blank" rel="noopener">这里</a>。虽然 Quasiquotes 可以很好的为我们解决代码生成等相关的问题，但是带来的新问题是编译代码时间比较长（大约 50ms - 500ms）！所以社区不得不默认关闭表达式代码生成。</p>
<p>为了解决这个问题，Spark 引入了 Janino 项目，参见 <a href="https://issues.apache.org/jira/browse/SPARK-7956" target="_blank" rel="noopener">SPARK-7956</a>。Janino 是一个超级小但又超级快的 Java™ 编译器. 它不仅能像 javac 工具那样将一组源文件编译成字节码文件，还可以对一些 Java 表达式，代码块，类中的文本(class body)或者内存中源文件进行编译，并把编译后的字节码直接加载到同一个 JVM 中运行。Janino 不是一个开发工具, 而是作为运行时的嵌入式编译器，比如作为表达式求值的翻译器或类似于 JSP 的服务端页面引擎，关于 Janino 的更多知识请参见<a href="https://janino-compiler.github.io/janino/" target="_blank" rel="noopener">这里</a>。通过引入了 Janino 来编译生成的代码，结果显示 SQL 表达式的编译时间减少到 5ms。在 Spark 中使用了 <code>ClassBodyEvaluator</code> 来编译生成之后的代码，参见 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator。</p>
<p><strong>需要主要的是，代码生成是在 Driver 端进行的，而代码编译是在 Executor 端进行的。</strong></p>
<h4 id="sql执行"><a class="markdownIt-Anchor" href="#sql执行"></a> SQL执行</h4>
<p>终于到了 SQL 真正执行的地方了。这个时候 Spark 会执行上阶段生成的代码，然后得到最终的结果，DAG 执行图如下：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1591537781641.png" alt="1591537781641" /></p>
<h2 id="3-sparksql执行过程深度总结"><a class="markdownIt-Anchor" href="#3-sparksql执行过程深度总结"></a> 3、sparkSQL执行过程深度总结</h2>
<p><a href="/images/kaikeba/sparkSQL-1%5CsparkSQL%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93.pptx">sparkSQL执行过程总结.pptx</a></p>
<p>从上面可以看得出来，sparkSQL的执行主要经过了这么几个大的步骤</p>
<p>1、输入sql，dataFrame或者dataSet</p>
<p>2、经过Catalyst过程，生成最终我们得到的最优的物理执行计划</p>
<p>​	1、parser阶段</p>
<p>​		主要是通过Antlr4解析SqlBase.g4 ，所有spark’支持的语法方式都是定义在sqlBase.g4里面了，如果需要扩展sparkSQL的语法，我们只需要扩展sqlBase.g4即可，通过antlr4解析sqlBase.g4文件，生成了我们的语法解析器SqlBaseLexer.java和词法解析器SqlBaseParser.java</p>
<p>​		parse阶段  ==》  	antlr4  ==》解析  ==》 SqlBase.g4  ==》得到  ==》 语法解析器SqlBaseLexer.java + 词法解析器SqlBaseParser.java</p>
<p>​	2、analyzer阶段</p>
<p>​		使用基于Rule的规则解析以及Session Catalog来实现函数资源信息和元数据管理信息</p>
<p>​		Analyzer 阶段   ==》 使用   ==》  Rule  +  Session Catalog  ==》多个rule  ==》 组成一个batch</p>
<p>​				session CataLog  ==》 保存函数资源信息以及元数据信息等</p>
<p>​	3、optimizer阶段</p>
<p>​			optimizer调优阶段  ==》 基于规则的RBO优化rule-based optimizer  ==&gt;  谓词下推 + 列剪枝  + 常量替换  + 常量累加</p>
<p>​	4、planner阶段</p>
<p>​		通过analyzer生成多个物理计划  ==》 经过Cost  Model进行最优选择  ==》基于代价的CBO优化    ==》 最终选定得到的最优物理执行计划</p>
<p>​	5、选定最终的物理计划，准备执行</p>
<p>​		最终选定的最优物理执行计划  ==》  准备生成代码去开始执行</p>
<p>3、将最终得到的物理执行计划进行代码生成，提交代码去执行我们的最终任务</p>
<h1 id="13-sparksql调优"><a class="markdownIt-Anchor" href="#13-sparksql调优"></a> 13、sparkSQL调优</h1>
<h2 id="1-数据缓存"><a class="markdownIt-Anchor" href="#1-数据缓存"></a> 1、数据缓存</h2>
<p>性能调优主要是将数据放入内存中操作，spark缓存注册表的方法</p>
<p>缓存spark表：</p>
<p>spark.catalog.cacheTable(“tableName”)缓存表</p>
<p>释放缓存表：</p>
<p>spark.catalog.uncacheTable(“tableName”)解除缓存</p>
<h2 id="2-性能优化相关参数"><a class="markdownIt-Anchor" href="#2-性能优化相关参数"></a> 2、性能优化相关参数</h2>
<p>Sparksql仅仅会缓存必要的列，并且自动调整压缩算法来减少内存和GC压力。</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.sql.inMemoryColumnarStorage.compressed</td>
<td>true</td>
<td>Spark SQL 将会基于统计信息自动地为每一列选择一种压缩编码方式。</td>
</tr>
<tr>
<td>spark.sql.inMemoryColumnarStorage.batchSize</td>
<td>10000</td>
<td>缓存批处理大小。缓存数据时, 较大的批处理大小可以提高内存利用率和压缩率，但同时也会带来 OOM（Out Of Memory）的风险。</td>
</tr>
<tr>
<td>spark.sql.files.maxPartitionBytes</td>
<td>128 MB</td>
<td>读取文件时单个分区可容纳的最大字节数（不过不推荐手动修改，可能在后续版本自动的自适应修改）</td>
</tr>
<tr>
<td>spark.sql.files.openCostInBytes</td>
<td>4M</td>
<td>打开文件的估算成本, 按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度)。</td>
</tr>
</tbody>
</table>
<h2 id="3-表数据广播"><a class="markdownIt-Anchor" href="#3-表数据广播"></a> 3、表数据广播</h2>
<p>在进行表join的时候，将小表广播可以提高性能，spark2.+中可以调整以下参数、</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.sql.broadcastTimeout</td>
<td>300</td>
<td>广播等待超时时间，单位秒</td>
</tr>
<tr>
<td>spark.sql.autoBroadcastJoinThreshold</td>
<td>10M</td>
<td>用于配置一个表在执行 join 操作时能够广播给所有 worker 节点的最大字节大小。通过将这个值设置为 -1 可以禁用广播。注意，当前数据统计仅支持已经运行了 ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan 命令的 Hive Metastore 表。</td>
</tr>
</tbody>
</table>
<h2 id="4-分区数的控制"><a class="markdownIt-Anchor" href="#4-分区数的控制"></a> 4、分区数的控制</h2>
<p>spark任务并行度的设置中，spark有两个参数可以设置</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.sql.shuffle.partitions</td>
<td>200</td>
<td>用于配置 join 或aggregate  shuffle数据时使用的分区数。</td>
</tr>
<tr>
<td>spark.default.parallelism</td>
<td>对于分布式shuffle操作像reduceByKey和join，父RDD中分区的最大数目。对于无父RDD的并行化等操作，它取决于群集管理器：-本地模式：本地计算机上的核心数-Mesos fine grained mode：8-其他：所有执行节点上的核心总数或2，以较大者为准</td>
<td>分布式shuffle操作的分区数</td>
</tr>
</tbody>
</table>
<p>看起来它们的定义似乎也很相似，但在实际测试中，</p>
<ul>
<li>spark.default.parallelism只有在处理RDD时才会起作用，对Spark SQL的无效。</li>
<li>spark.sql.shuffle.partitions则是对sparks SQL专用的设置</li>
</ul>
<h2 id="5-文件与分区"><a class="markdownIt-Anchor" href="#5-文件与分区"></a> 5. 文件与分区</h2>
<p>这个总共有两个参数可以调整：</p>
<ul>
<li>读取文件的时候一个分区接受多少数据；</li>
<li>文件打开的开销，通俗理解就是小文件合并的阈值。</li>
</ul>
<p>文件打开是有开销的，开销的衡量，Spark 采用了一个比较好的方式就是打开文件的开销用，相同时间能扫描的数据的字节数来衡量。</p>
<p>参数介绍如下：</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.sql.files.maxPartitionBytes</td>
<td>134217728 (128 MB)</td>
<td>打包传入一个分区的最大字节，在读取文件的时候</td>
</tr>
<tr>
<td>spark.sql.files.openCostInBytes</td>
<td>4194304 (4 MB)</td>
<td>用相同时间内可以扫描的数据的大小来衡量打开一个文件的开销。当将多个文件写入同一个分区的时候该参数有用。该值设置大一点有好处，有小文件的分区会比大文件分区处理速度更快（优先调度）</td>
</tr>
</tbody>
</table>
<p>spark.sql.files.maxPartitionBytes该值的调整要结合你想要的并发度及内存的大小来进行。</p>
<p>spark.sql.files.openCostInBytes说直白一些这个参数就是合并小文件的阈值，小于这个阈值的文件将会合并</p>
<h2 id="6-数据的本地性"><a class="markdownIt-Anchor" href="#6-数据的本地性"></a> 6、数据的本地性</h2>
<p>分布式计算系统的精粹在于移动计算而非移动数据，但是在实际的计算过程中，总存在着移动数据的情况，除非是在集群的所有节点上都保存数据的副本。移动数据，将数据从一个节点移动到另一个节点进行计算，不但消耗了网络IO，也消耗了磁盘IO，降低了整个计算的效率。为了提高数据的本地性，除了优化算法（也就是修改spark内存，难度有点高），就是合理设置数据的副本。设置数据的副本，这需要通过配置参数并长期观察运行状态才能获取的一个经验值。</p>
<p>先来看看一个 stage 里所有 task 运行的一些性能指标，其中的一些说明：</p>
<ul>
<li><code>Scheduler Delay</code> : spark 分配 task 所花费的时间</li>
<li><code>Executor Computing Time</code> : executor 执行 task 所花费的时间</li>
<li><code>Getting Result Time</code> : 获取 task 执行结果所花费的时间</li>
<li><code>Result Serialization Time</code> : task 执行结果序列化时间</li>
<li><code>Task Deserialization Time</code> : task 反序列化时间</li>
<li><code>Shuffle Write Time</code> : shuffle 写数据时间</li>
<li><code>Shuffle Read Time</code> : shuffle 读数据所花费时间</li>
</ul>
<p>​      下面是spark webUI监控Stage的一个图：</p>
<ul>
<li>PROCESS_LOCAL是指读取缓存在本地节点的数据</li>
<li>NODE_LOCAL是指读取本地节点硬盘数据</li>
<li>ANY是指读取非本地节点数据</li>
<li>通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关。</li>
</ul>
<h2 id="7-sparksql参数调优总结"><a class="markdownIt-Anchor" href="#7-sparksql参数调优总结"></a> 7、sparkSQL参数调优总结</h2>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">//1.下列Hive参数对Spark同样起作用。</span></span><br><span class="line"><span class="attr">set</span> <span class="string">hive.exec.dynamic.partition=true; // 是否允许动态生成分区</span></span><br><span class="line"><span class="attr">set</span> <span class="string">hive.exec.dynamic.partition.mode=nonstrict; // 是否容忍指定分区全部动态生成</span></span><br><span class="line"><span class="attr">set</span> <span class="string">hive.exec.max.dynamic.partitions = 100; // 动态生成的最多分区数</span></span><br><span class="line"></span><br><span class="line"><span class="attr">//2.运行行为</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.sql.autoBroadcastJoinThreshold; // 大表 JOIN 小表，小表做广播的阈值</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.dynamicAllocation.enabled; // 开启动态资源分配</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.dynamicAllocation.maxExecutors; //开启动态资源分配后，最多可分配的Executor数</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.dynamicAllocation.minExecutors; //开启动态资源分配后，最少可分配的Executor数</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.sql.shuffle.partitions; // 需要shuffle是mapper端写出的partition个数</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.sql.adaptive.enabled; // 是否开启调整partition功能，如果开启，spark.sql.shuffle.partitions设置的partition可能会被合并到一个reducer里运行</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.sql.adaptive.shuffle.targetPostShuffleInputSize; //开启spark.sql.adaptive.enabled后，两个partition的和低于该阈值会合并到一个reducer</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.sql.adaptive.minNumPostShufflePartitions; // 开启spark.sql.adaptive.enabled后，最小的分区数</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.hadoop.mapreduce.input.fileinputformat.split.maxsize; //当几个stripe的大小大于该值时，会合并到一个task中处理</span></span><br><span class="line"></span><br><span class="line"><span class="attr">//3.executor能力</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.executor.memory; // executor用于缓存数据、代码执行的堆内存以及JVM运行时需要的内存</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.yarn.executor.memoryOverhead; //Spark运行还需要一些堆外内存，直接向系统申请，如数据传输时的netty等。</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.sql.windowExec.buffer.spill.threshold; //当用户的SQL中包含窗口函数时，并不会把一个窗口中的所有数据全部读进内存，而是维护一个缓存池，当池中的数据条数大于该参数表示的阈值时，spark将数据写到磁盘</span></span><br><span class="line"><span class="attr">set</span> <span class="string">spark.executor.cores; //单个executor上可以同时运行的task数</span></span><br></pre></td></tr></table></figure>
<h1 id="14-spark的动态资源划分"><a class="markdownIt-Anchor" href="#14-spark的动态资源划分"></a> 14、spark的动态资源划分</h1>
<p>动态资源划分，主要是spark当中用于对计算的时候资源如果不够或者资源剩余的情况下进行动态的资源划分，以求资源的利用率达到最大</p>
<p><a href="http://spark.apache.org/docs/2.3.3/configuration.html#dynamic-allocation" target="_blank" rel="noopener">http://spark.apache.org/docs/2.3.3/configuration.html#dynamic-allocation</a></p>
<p>Spark中，所谓资源单位一般指的是executors，和Yarn中的Containers一样，在Spark On Yarn模式下，通常使用–num-executors来指定Application使用的executors数量，而–executor-memory和–executor-cores分别用来指定每个executor所使用的内存和虚拟CPU核数</p>
<p>假设有这样的场景，如果使用Hive，多个用户同时使用hive-cli做数据开发和分析，只有当用户提交执行了Hive SQL时候，才会向YARN申请资源，执行任务，如果不提交执行，无非就是停留在Hive-cli命令行，也就是个JVM而已，并不会浪费YARN的资源。现在想用Spark-SQL代替Hive来做数据开发和分析，也是多用户同时使用，如果按照之前的方式，以yarn-client模式运行spark-sql命令行，在启动时候指定–num-executors 10，那么每个用户启动时候都使用了10个YARN的资源（Container），这10个资源就会一直被占用着，只有当用户退出spark-sql命令行时才会释放。例如通过以下这种方式使用spark-sql</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">直接通过-e来执行任务，执行完成任务之后，回收资源</span><br><span class="line">cd /kkb/install/spark-2.3.3-bin-hadoop2.7</span><br><span class="line">bin/spark-sql  --master yarn-client  \</span><br><span class="line">--executor-memory 512m –num-executors 10 \</span><br><span class="line">--conf  spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \</span><br><span class="line">--jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar  \</span><br><span class="line">-e  "select count(*) from game_center.ods_task_log;"</span><br><span class="line"></span><br><span class="line">进入spark-sql客户端，但是不执行任务，一直持续占有资源</span><br><span class="line">cd /kkb/install/spark-2.3.3-bin-hadoop2.7</span><br><span class="line">bin/spark-sql  --master yarn-client  \</span><br><span class="line">--executor-memory 512m –num-executors 10 \</span><br><span class="line">--conf  spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \</span><br><span class="line">--jars /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/common/hadoop-lzo-0.4.20.jar  </span><br><span class="line"></span><br><span class="line">在这种模式下，就算你不提交资源，申请的资源也会一直常驻，这样就明显不合理了</span><br></pre></td></tr></table></figure>
<p>spark-sql On Yarn，能不能像Hive一样，执行SQL的时候才去申请资源，不执行的时候就释放掉资源呢，其实从Spark1.2之后，对于On Yarn模式，已经支持动态资源分配（Dynamic Resource Allocation），这样，就可以根据Application的负载（Task情况），动态的增加和减少executors，这种策略非常适合在YARN上使用spark-sql做数据开发和分析，以及将spark-sql作为长服务来使用的场景。</p>
<p>spark当中支持通过动态资源划分的方式来实现动态资源的配置，尽量减少内存的持久占用，但是动态资源划分又会产生进一步的问题例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">executor动态调整的范围？无限减少？无限制增加？</span><br><span class="line">executor动态调整速率？线性增减？指数增减？</span><br><span class="line">何时移除Executor？</span><br><span class="line">何时新增Executor了？只要由新提交的Task就新增Executor吗？</span><br><span class="line">Spark中的executor不仅仅提供计算能力，还可能存储持久化数据，这些数据在宿主executor被kill后，该如何访问？</span><br></pre></td></tr></table></figure>
<p>通过spark-shell当中最简单的wordcount为例来查看spark当中的资源划分</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"># 以yarn模式执行，并指定executor个数为<span class="number">1</span></span><br><span class="line">$ spark-shell --master=yarn --num-executors=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"># 提交<span class="type">Job1</span> wordcount</span><br><span class="line">scala&gt; sc.textFile(<span class="string">"file:///etc/hosts"</span>).flatMap(line =&gt; line.split(<span class="string">" "</span>)).map(word =&gt; (word,<span class="number">1</span>)).reduceByKey(_ + _).count();</span><br><span class="line"></span><br><span class="line"># 提交<span class="type">Job2</span> wordcount</span><br><span class="line">scala&gt; sc.textFile(<span class="string">"file:///etc/profile"</span>).flatMap(line =&gt; line.split(<span class="string">" "</span>)).map(word =&gt; (word,<span class="number">1</span>)).reduceByKey(_ + _).count();</span><br><span class="line"></span><br><span class="line"># <span class="type">Ctrl</span>+<span class="type">C</span> <span class="type">Kill</span> <span class="type">JVM</span></span><br></pre></td></tr></table></figure>
<p>上述的Spark应用中，以yarn模式启动spark-shell，并顺序执行两次wordcount，最后Ctrl+C退出spark-shell。此例中Executor的生命周期如下图：</p>
<p><img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1591607427157.png" alt="1591607427157" /></p>
<p>从上图可以看出，Executor在整个应用执行过程中，其状态一直处于Busy（执行Task）或Idle（空等）。处于Idle状态的Executor造成资源浪费这个问题已经在上面提到。下面重点看下开启Spark动态资源分配功能后，Executor如何运作。<br />
<img src="1%E3%80%81sparkSQL%E7%AC%AC%E4%BA%8C%E6%AC%A1%E8%AF%BE.assets/1591607516396.png" alt="1591607516396" /></p>
<p>下面分析下上图中各个步骤：</p>
<ol>
<li>spark-shell Start：启动spark-shell应用，并通过–num-executor指定了1个执行器。</li>
<li>Executor1 Start：启动执行器Executor1。注意：Executor启动前存在一个AM向ResourceManager申请资源的过程，所以启动时机略微滞后与Driver。</li>
<li>Job1 Start：提交第一个wordcount作业，此时，Executor1处于Busy状态。</li>
<li>Job1 End：作业1结束，Executor1又处于Idle状态。</li>
<li>Executor1 timeout：Executor1空闲一段时间后，超时被Kill。</li>
<li>Job2 Submit：提交第二个wordcount，此时，没有Active的Executor可用。Job2处于Pending状态。</li>
<li>Executor2 Start：检测到有Pending的任务，此时Spark会启动Executor2。</li>
<li>Job2 Start：此时，已经有Active的执行器，Job2会被分配到Executor2上执行。</li>
<li>Job2 End：Job2结束。</li>
<li>Executor2 End：Ctrl+C 杀死Driver，Executor2也会被RM杀死。</li>
</ol>
<p>上述流程中需要重点关注的几个问题：</p>
<ul>
<li>Executor超时：当Executor不执行任何任务时，会被标记为Idle状态。空闲一段时间后即被认为超时，会被kill。该空闲时间由<strong>spark.dynamicAllocation.executorIdleTimeout</strong>决定，默认值60s。对应上图中：Job1 End到Executor1 timeout之间的时间。</li>
<li>资源不足时，何时新增Executor：当有Task处于pending状态，意味着资源不足，此时需要增加Executor。这段时间由<strong>spark.dynamicAllocation.schedulerBacklogTimeout</strong>控制，默认1s。对应上述step6和step7之间的时间。</li>
<li>该新增多少Executor：新增Executor的个数主要依据是当前负载情况，即running和pending任务数以及当前Executor个数决定。用maxNumExecutorsNeeded代表当前实际需要的最大Executor个数，maxNumExecutorsNeeded和当前Executor个数的差值即是<strong>潜在的</strong>新增Executor的个数。注意：之所以说<strong>潜在的个数</strong>，是因为最终新增的Executor个数还有别的因素需要考虑，后面会有分析。下面是maxNumExecutorsNeeded计算方法：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maxNumExecutorsNeeded</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> numRunningOrPendingTasks = listener.totalPendingTasks + listener.totalRunningTasks</span><br><span class="line">  math.ceil(numRunningOrPendingTasks * executorAllocationRatio /</span><br><span class="line">            tasksPerExecutorForFullParallelism)</span><br><span class="line">    .toInt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中numRunningOrPendingTasks为当前running和pending任务数之和。</p>
<p>executorAllocationRatio：最理想的情况下，有多少待执行的任务，那么我们就新增多少个Executor，从而达到最大的任务并发度。但是这也有副作用，如果当前任务都是小任务，那么这一策略就会造成资源浪费。可能最后申请的Executor还没启动，这些小任务已经被执行完了。该值是一个系数值，范围[0~1]。默认1.</p>
<p>tasksPerExecutorForFullParallelism：每个Executor的最大并发数，简单理解为：cpu核心数（spark.executor.cores）/ 每个任务占用的核心数（spark.task.cpus）。</p>
<h2 id="问题1executor动态调整的范围无限减少无限制增加调整速率"><a class="markdownIt-Anchor" href="#问题1executor动态调整的范围无限减少无限制增加调整速率"></a> 问题1：executor动态调整的范围？无限减少？无限制增加？调整速率？</h2>
<p>要实现资源的动态调整，那么限定调整范围是最先考虑的事情，Spark通过下面几个参数实现：</p>
<ul>
<li>spark.dynamicAllocation.minExecutors：Executor调整下限。（默认值：0）</li>
<li>spark.dynamicAllocation.maxExecutors：Executor调整上限。（默认值：Integer.MAX_VALUE）</li>
<li>spark.dynamicAllocation.initialExecutors：Executor初始数量（默认值：minExecutors）。</li>
</ul>
<p>三者的关系必须满足：minExecutors &lt;= initialExecutors &lt;= maxExecutors</p>
<blockquote>
<p>注意：如果显示指定了num-executors参数，那么initialExecutors就是num-executor指定的值。</p>
</blockquote>
<h2 id="问题2spark中的executor既提供计算能力也提供存储能力-这些因超时被杀死的executor中持久化的数据如何处理"><a class="markdownIt-Anchor" href="#问题2spark中的executor既提供计算能力也提供存储能力-这些因超时被杀死的executor中持久化的数据如何处理"></a> 问题2：Spark中的Executor既提供计算能力，也提供存储能力。这些因超时被杀死的Executor中持久化的数据如何处理？</h2>
<p>如果Executor中缓存了数据，那么该Executor的Idle-timeout时间就不是由<strong>executorIdleTimeout</strong>决定，而是用<strong>spark.dynamicAllocation.cachedExecutorIdleTimeout</strong>控制，默认值：Integer.MAX_VALUE。如果手动设置了该值，当这些缓存数据的Executor被kill后，我们可以通过NodeManannger的External Shuffle Server来访问这些数据。这就要求NodeManager中<strong>spark.shuffle.service.enabled</strong>必须开启。</p>
<h2 id="如何配置spark的动态资源划分"><a class="markdownIt-Anchor" href="#如何配置spark的动态资源划分"></a> 如何配置spark的动态资源划分</h2>
<h3 id="第一步修改yarn-sitexml配置文件"><a class="markdownIt-Anchor" href="#第一步修改yarn-sitexml配置文件"></a> 第一步：修改yarn-site.xml配置文件</h3>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.shuffle.service.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>7337<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="第二步配置spark的配置文件"><a class="markdownIt-Anchor" href="#第二步配置spark的配置文件"></a> 第二步：配置spark的配置文件</h3>
<p>修改spark-conf的配置选项，开启动态资源划分，或者直接修改spark-defaults.conf，增加以下参数：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">spark.shuffle.service.enabled</span> <span class="string">true   //启用External shuffle Service服务</span></span><br><span class="line"><span class="meta">spark.shuffle.service.port</span> <span class="string">7337 //Shuffle Service服务端口，必须和yarn-site中的一致</span></span><br><span class="line"><span class="meta">spark.dynamicAllocation.enabled</span> <span class="string">true  //开启动态资源分配</span></span><br><span class="line"><span class="meta">spark.dynamicAllocation.minExecutors</span> <span class="string">1  //每个Application最小分配的executor数</span></span><br><span class="line"><span class="meta">spark.dynamicAllocation.maxExecutors</span> <span class="string">30  //每个Application最大并发分配的executor数</span></span><br><span class="line"><span class="meta">spark.dynamicAllocation.schedulerBacklogTimeout</span> <span class="string">1s </span></span><br><span class="line"><span class="meta">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout</span> <span class="string">5s</span></span><br></pre></td></tr></table></figure>
<p>动态资源分配策略：</p>
<p>开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout(默认1s)时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout(默认1s)时间申请一次，直到申请到足够的资源。每次申请的资源量是指数增长的，即1,2,4,8等。<br />
之所以采用指数增长，出于两方面考虑：其一，开始申请的少是考虑到可能application会马上得到满足；其次要成倍增加，是为了防止application需要很多资源，而该方式可以在很少次数的申请之后得到满足。</p>
<p>动态资源回收策略：</p>
<p>当application的executor空闲时间超过spark.dynamicAllocation.executorIdleTimeout（默认60s）后，就会被回收。</p>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<div id="bottom-donation-section">
<span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span>
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
</div>
<div class="well">
  <!--
  原创文章，转载请注明： 转载自<a href="http://52binge.github.io" target="_blank" rel="noopener"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1sparksql概述"><span class="toc-text"> 1.sparksql概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-sparksql的前世今生"><span class="toc-text"> 1.1 sparksql的前世今生</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-什么是sparksql"><span class="toc-text"> 1.2 什么是sparksql</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-sparksql的四大特性"><span class="toc-text"> 2. sparksql的四大特性</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-dataframe概述"><span class="toc-text"> 3. DataFrame概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-dataframe发展"><span class="toc-text"> 3.1 DataFrame发展</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-dataframe是什么"><span class="toc-text"> 3.2 DataFrame是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#33-dataframe和rdd的优缺点"><span class="toc-text"> 3.3 DataFrame和RDD的优缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-读取文件构建dataframe"><span class="toc-text"> 4. 读取文件构建DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#41-读取文本文件创建dataframe"><span class="toc-text"> 4.1 读取文本文件创建DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#42-读取json文件创建dataframe"><span class="toc-text"> 4.2 读取json文件创建DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#43-读取parquet文件创建dataframe"><span class="toc-text"> 4.3 读取parquet文件创建DataFrame</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-dataframe常用操作"><span class="toc-text"> 5. DataFrame常用操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#51-dsl风格语法"><span class="toc-text"> 5.1 DSL风格语法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#52-sql风格语法"><span class="toc-text"> 5.2 SQL风格语法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-dataset概述"><span class="toc-text"> 6. DataSet概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#61-dataset是什么"><span class="toc-text"> 6.1 DataSet是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#62-dataframe-dataset区别"><span class="toc-text"> 6.2 DataFrame、DataSet区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#63-dataframe与dataset互转"><span class="toc-text"> 6.3 DataFrame与DataSet互转</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#64-构建dataset"><span class="toc-text"> 6.4 构建DataSet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#65-rdd以及dataframe以及dataset的关系"><span class="toc-text"> 6.5 RDD以及DataFrame以及DataSet的关系</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-通过idea开发程序实现把rdd转换dataframe"><span class="toc-text"> 7. 	通过IDEA开发程序实现把RDD转换DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#71-利用反射机制"><span class="toc-text"> 7.1 利用反射机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#72-通过structtype动态指定schema"><span class="toc-text"> 7.2 通过StructType动态指定Schema</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-sparksql读取sql数据"><span class="toc-text"> 8、sparkSQL读取sql数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-sparksql操作csv文件并将结果写入mysql"><span class="toc-text"> 9、sparkSQL操作CSV文件并将结果写入mysql</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-spark-on-hive-与hive-on-spark"><span class="toc-text"> 10、spark  on  hive  与hive on   spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-spark_sql与hive进行整合"><span class="toc-text"> 1、spark_sql与hive进行整合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#第一步拷贝hive-sitexml配置文件"><span class="toc-text"> 第一步：拷贝hive-site.xml配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第二步拷贝mysql连接驱动包"><span class="toc-text"> 第二步：拷贝mysql连接驱动包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第三步进入spark-sql直接操作hive数据库当中的数据"><span class="toc-text"> 第三步：进入spark-sql直接操作hive数据库当中的数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-启用spark的thrift-server与hive进行远程交互"><span class="toc-text"> 2、启用spark的thrift  server与hive进行远程交互</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#第一步修改hive-sitexml的配置"><span class="toc-text"> 第一步：修改hive-site.xml的配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第二步修改完的配置文件分发到其他机器"><span class="toc-text"> 第二步：修改完的配置文件分发到其他机器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第三步node03启动metastore服务"><span class="toc-text"> 第三步：node03启动metastore服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第四步node03执行以下命令启动spark的thrift-server"><span class="toc-text"> 第四步：node03执行以下命令启动spark的thrift  server</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第五步直接使用beeline来连接"><span class="toc-text"> 第五步：直接使用beeline来连接</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-sparksql自定义函数"><span class="toc-text"> 11、sparkSQL自定义函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-自定义udf函数"><span class="toc-text"> 1、自定义UDF函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-自定义udaf函数"><span class="toc-text"> 2、自定义UDAF函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-sparksql架构设计"><span class="toc-text"> 12、sparkSQL架构设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-sparksql的架构设计实现"><span class="toc-text"> 1、sparkSQL的架构设计实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-catalyst执行过程"><span class="toc-text"> 2、 Catalyst执行过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-sql解析阶段-parser"><span class="toc-text"> 1、sql解析阶段 Parser</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-绑定逻辑计划analyzer"><span class="toc-text"> 2、绑定逻辑计划Analyzer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-逻辑优化阶段optimizer"><span class="toc-text"> 3、逻辑优化阶段Optimizer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#谓词下推"><span class="toc-text"> 谓词下推</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#列裁剪"><span class="toc-text"> 列裁剪</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#常量替换"><span class="toc-text"> 常量替换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#常量累加"><span class="toc-text"> 常量累加</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-生成可执行的物理计划阶段physical-plan"><span class="toc-text"> 4、生成可执行的物理计划阶段Physical Plan</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-代码生成阶段"><span class="toc-text"> 5、代码生成阶段</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#生成代码与sql解析引擎的区别"><span class="toc-text"> 生成代码与sql解析引擎的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#表达式代码生成expression-codegen"><span class="toc-text"> 表达式代码生成（expression codegen）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#全阶段代码生成whole-stage-code-generation"><span class="toc-text"> 全阶段代码生成（Whole-stage Code Generation）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代码编译"><span class="toc-text"> 代码编译</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sql执行"><span class="toc-text"> SQL执行</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-sparksql执行过程深度总结"><span class="toc-text"> 3、sparkSQL执行过程深度总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-sparksql调优"><span class="toc-text"> 13、sparkSQL调优</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-数据缓存"><span class="toc-text"> 1、数据缓存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-性能优化相关参数"><span class="toc-text"> 2、性能优化相关参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-表数据广播"><span class="toc-text"> 3、表数据广播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-分区数的控制"><span class="toc-text"> 4、分区数的控制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-文件与分区"><span class="toc-text"> 5. 文件与分区</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-数据的本地性"><span class="toc-text"> 6、数据的本地性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-sparksql参数调优总结"><span class="toc-text"> 7、sparkSQL参数调优总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-spark的动态资源划分"><span class="toc-text"> 14、spark的动态资源划分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#问题1executor动态调整的范围无限减少无限制增加调整速率"><span class="toc-text"> 问题1：executor动态调整的范围？无限减少？无限制增加？调整速率？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#问题2spark中的executor既提供计算能力也提供存储能力-这些因超时被杀死的executor中持久化的数据如何处理"><span class="toc-text"> 问题2：Spark中的Executor既提供计算能力，也提供存储能力。这些因超时被杀死的Executor中持久化的数据如何处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何配置spark的动态资源划分"><span class="toc-text"> 如何配置spark的动态资源划分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#第一步修改yarn-sitexml配置文件"><span class="toc-text"> 第一步：修改yarn-site.xml配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第二步配置spark的配置文件"><span class="toc-text"> 第二步：配置spark的配置文件</span></a></li></ol></li></ol></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/sparkSQL/" rel="tag">sparkSQL</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/10/23/tensorflow/tf-app_run_&_tf_flags/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Tensorflow tf.app.run()与命令行参数解析
        
      </div>
    </a>
  
  
    <a href="/2018/10/15/tensorflow/tf-seq2seq/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">从 Encoder 到 Decoder 实现 Seq2Seq 模型&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2018/10/17/spark/sparkSQL-all-knowleage/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
