<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Auckland New Zealand</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Everyone should not forget his dream">
<meta property="og:type" content="website">
<meta property="og:title" content="Auckland New Zealand">
<meta property="og:url" content="http:&#x2F;&#x2F;iequa.com&#x2F;page&#x2F;2&#x2F;index.html">
<meta property="og:site_name" content="Auckland New Zealand">
<meta property="og:description" content="Everyone should not forget his dream">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;" target="_blank" rel="noopener"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/ai1">AI</a>
        
          <a class="main-nav-link" href="/tensorflow">TF/Keras</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer">
      <article id="post-tensorflow/keras-1.4-CNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/08/22/tensorflow/keras-1.4-CNN/"><strong>CNN classifier in Keras</strong></a>
      <small class=article-date-index>&nbsp; 2019-08-22</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/08/22/tensorflow/keras-1.4-CNN/" class="article-date">
  <time datetime="2019-08-22T07:17:21.000Z" itemprop="datePublished">2019-08-22</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/tensorflow/">tensorflow</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/08/22/tensorflow/keras-1.4-CNN/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>Convolutional Neural Networks，CNN 也是一种前馈神经网络，其特点是每层的神经元节点只响应前一层局部区域范围内的神经元（全连接网络中每个神经元节点响应前一层的全部节点）</p>
<p>&lt;!-- more --&gt;</p>
<p>&lt;img src=&quot;/images/tensorflow/keras-cnn4.png&quot; width=&quot;550&quot; alt=&quot;Convolutional Neural Network in Keras&quot;/&gt;</p>
<p><strong>pooling</strong></p>
<p>&lt;img src=&quot;/images/tensorflow/keras-cnn5.png&quot; width=&quot;470&quot; alt=&quot;Convolutional Neural Network in Keras&quot;/&gt;</p>
<blockquote>
<p>研究发现, 在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, pooling 就可以很好地解决这一问题. 而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析.</p>
<p>同时也减轻了神经网络的计算负担. 也就是说在卷集的时候, 我们不压缩长宽, 尽量保留更多信息, 压缩的工作就交给池化了,这样的一项附加工作能够很有效的提高准确性. 有了这些技术,我们就可以搭建一个 CNN.</p>
</blockquote>
<p>&lt;img src=&quot;/images/tensorflow/keras-cnn2.png&quot; width=&quot;550&quot; alt=&quot;Convolutional Neural Network in Keras&quot;/&gt;</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Convolution2D, MaxPooling2D, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br></pre></td></tr></table></figure></p>
<p>数据集 MNIST</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># download the mnist to the path '~/.keras/datasets/' if it is the first time to be called</span></span><br><span class="line"><span class="comment"># training X shape (60000, 28x28), Y shape (60000, ). test X shape (10000, 28x28), Y shape (10000, )</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span><br></pre></td></tr></table></figure></p>
<h2>1. data pre-processing</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train = X_train.reshape(<span class="number">-1</span>, <span class="number">1</span>,<span class="number">28</span>, <span class="number">28</span>)/<span class="number">255.</span></span><br><span class="line">X_test = X_test.reshape(<span class="number">-1</span>, <span class="number">1</span>,<span class="number">28</span>, <span class="number">28</span>)/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line">y_train = np_utils.to_categorical(y_train, num_classes=<span class="number">10</span>)</span><br><span class="line">y_test = np_utils.to_categorical(y_test, num_classes=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<h2>2. build model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Another way to build your CNN</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Conv layer 1 output shape (32, 28, 28)</span></span><br><span class="line">model.add(Convolution2D(</span><br><span class="line">    batch_input_shape=(<span class="literal">None</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>),</span><br><span class="line">    filters=<span class="number">32</span>,</span><br><span class="line">    kernel_size=<span class="number">5</span>,</span><br><span class="line">    strides=<span class="number">1</span>,</span><br><span class="line">    padding=<span class="string">'same'</span>,     <span class="comment"># Padding method</span></span><br><span class="line">    data_format=<span class="string">'channels_first'</span>,</span><br><span class="line">))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pooling layer 1 (max pooling) output shape (32, 14, 14)</span></span><br><span class="line">model.add(MaxPooling2D(</span><br><span class="line">    pool_size=<span class="number">2</span>,</span><br><span class="line">    strides=<span class="number">2</span>,</span><br><span class="line">    padding=<span class="string">'same'</span>,    <span class="comment"># Padding method</span></span><br><span class="line">    data_format=<span class="string">'channels_first'</span>,</span><br><span class="line">))</span><br></pre></td></tr></table></figure></p>
<p>再添加第二, 卷积层和池化层</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Conv layer 2 output shape (64, 14, 14)</span></span><br><span class="line">model.add(Convolution2D(<span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>, data_format=<span class="string">'channels_first'</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pooling layer 2 (max pooling) output shape (64, 7, 7)</span></span><br><span class="line">model.add(MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>, <span class="string">'same'</span>, data_format=<span class="string">'channels_first'</span>))</span><br></pre></td></tr></table></figure></p>
<p>Fully connected layer 1 input shape (64 * 7 * 7) = (3136), output shape (1024)</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1024</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br></pre></td></tr></table></figure></p>
<p>Fully connected layer 2 to shape (10) for 10 classes</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.add(Dense(<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure></p>
<p>define your optimizer</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Another way to define your optimizer</span></span><br><span class="line">adam = Adam(lr=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure></p>
<p><a href="https://keras.io/zh/layers/convolutional/" target="_blank" rel="noopener">keras.layers.Conv1D 1D 卷积层 (例如时序卷积)</a>
<a href="https://blog.csdn.net/qq_19707521/article/details/78486185" target="_blank" rel="noopener">Keras Convolution1D与Convolution2D区别</a></p>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_3 (Conv2D)            (None, 32, 28, 28)        832       
_________________________________________________________________
activation_5 (Activation)    (None, 32, 28, 28)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 32, 14, 14)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 64, 14, 14)        51264     
_________________________________________________________________
activation_6 (Activation)    (None, 64, 14, 14)        0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 64, 7, 7)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 3136)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 1024)              3212288   
_________________________________________________________________
activation_7 (Activation)    (None, 1024)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 10)                10250     
_________________________________________________________________
activation_8 (Activation)    (None, 10)                0         
=================================================================
Total params: 3,274,634
Trainable params: 3,274,634
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<h2>3. compile model</h2>
<p>设置adam优化方法，loss函数, metrics方法来观察输出结果</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We add metrics to get more results you want to see</span></span><br><span class="line">model.compile(optimizer=adam,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></p>
<h2>4. train model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Training ------------'</span>)</span><br><span class="line"><span class="comment"># Another way to train the model</span></span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">1</span>, batch_size=<span class="number">64</span>,)</span><br></pre></td></tr></table></figure></p>
<h2>5. evaluate model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line"><span class="comment"># Evaluate the model with the metrics we defined earlier</span></span><br><span class="line">loss, accuracy = model.evaluate(X_test, y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntest loss: '</span>, loss)</span><br><span class="line">print(<span class="string">'\ntest accuracy: '</span>, accuracy)</span><br></pre></td></tr></table></figure></p>
<h2>Reference</h2>
<ul>
<li><a href="https://keras-cn.readthedocs.io/en/latest/backend/" target="_blank" rel="noopener">keras-cn</a>、 <a href="https://keras.io/" target="_blank" rel="noopener">keras.io</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/keras/" target="_blank" rel="noopener">莫烦 Keras</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-tensorflow/keras-1.3-classifier" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/08/22/tensorflow/keras-1.3-classifier/"><strong>Classifier in Keras</strong></a>
      <small class=article-date-index>&nbsp; 2019-08-22</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/08/22/tensorflow/keras-1.3-classifier/" class="article-date">
  <time datetime="2019-08-22T06:17:21.000Z" itemprop="datePublished">2019-08-22</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/tensorflow/">tensorflow</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/08/22/tensorflow/keras-1.3-classifier/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;img src=&quot;/images/tensorflow/keras-Classifier.png&quot; width=&quot;550&quot; alt=&quot;Classifier in Keras&quot;/&gt;</p>
<p>&lt;!-- more --&gt;</p>
<p><strong>data preprocessing</strong></p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"></span><br><span class="line"><span class="comment"># download the mnist to the path '~/.keras/datasets/' if it is the first time to be called</span></span><br><span class="line"><span class="comment"># X shape (60,000 28x28), y shape (10,000, )</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># data pre-processing</span></span><br><span class="line">X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], <span class="number">-1</span>) / <span class="number">255.</span>   <span class="comment"># normalize</span></span><br><span class="line">X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], <span class="number">-1</span>) / <span class="number">255.</span>      <span class="comment"># normalize</span></span><br><span class="line"></span><br><span class="line">y_train = np_utils.to_categorical(y_train, num_classes=<span class="number">10</span>)</span><br><span class="line">y_test = np_utils.to_categorical(y_test, num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">print(X_train[<span class="number">1</span>].shape)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(784,)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print(y_train[:3])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]</span></span><br><span class="line"><span class="string"> [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]</span></span><br><span class="line"><span class="string"> [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></p>
<h2>1. build model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br></pre></td></tr></table></figure></p>
<p>Another way to build your neural net</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Another way to build your neural net</span></span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(<span class="number">32</span>, input_dim=<span class="number">784</span>),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    Dense(<span class="number">10</span>),</span><br><span class="line">    Activation(<span class="string">'softmax'</span>),</span><br><span class="line">])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure></p>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_5 (Dense)              (None, 32)                25120     
_________________________________________________________________
activation_5 (Activation)    (None, 32)                0         
_________________________________________________________________
dense_6 (Dense)              (None, 10)                330       
_________________________________________________________________
activation_6 (Activation)    (None, 10)                0         
=================================================================
Total params: 25,450
Trainable params: 25,450
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Another way to define your optimizer</span></span><br><span class="line">rmsprop = RMSprop(lr=<span class="number">0.001</span>, rho=<span class="number">0.9</span>, epsilon=<span class="number">1e-08</span>, decay=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure></p>
<h2>2. compile model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We add metrics to get more results you want to see</span></span><br><span class="line">model.compile(optimizer=rmsprop,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></p>
<h2>3. train model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Training ------------'</span>)</span><br><span class="line"><span class="comment"># Another way to train the model</span></span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">2</span>, batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># """</span></span><br><span class="line"><span class="comment"># Training ------------</span></span><br><span class="line"><span class="comment"># Epoch 1/2</span></span><br><span class="line"><span class="comment"># 60000/60000 [==============================] - 2s - loss: 0.3506 - acc: 0.9025     </span></span><br><span class="line"><span class="comment"># Epoch 2/2</span></span><br><span class="line"><span class="comment"># 60000/60000 [==============================] - 2s - loss: 0.1995 - acc: 0.9421   </span></span><br><span class="line"><span class="comment"># """</span></span><br></pre></td></tr></table></figure></p>
<h2>4. evaluate model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line"><span class="comment"># Evaluate the model with the metrics we defined earlier</span></span><br><span class="line">loss, accuracy = model.evaluate(X_test, y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'test loss: '</span>, loss)</span><br><span class="line">print(<span class="string">'test accuracy: '</span>, accuracy)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Testing ------------</span></span><br><span class="line"><span class="string"> 9760/10000 [============================&gt;.] - ETA: 0s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">test loss:  0.1724540345</span></span><br><span class="line"><span class="string">test accuracy:  0.9489</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></p>
<h2>Reference</h2>
<ul>
<li><a href="https://keras-cn.readthedocs.io/en/latest/backend/" target="_blank" rel="noopener">keras-cn</a>、 <a href="https://keras.io/" target="_blank" rel="noopener">keras.io</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/keras/" target="_blank" rel="noopener">莫烦 Keras</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-tensorflow/keras-1.2-Regressor" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/08/21/tensorflow/keras-1.2-Regressor/"><strong>Regressor in Keras</strong></a>
      <small class=article-date-index>&nbsp; 2019-08-21</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/08/21/tensorflow/keras-1.2-Regressor/" class="article-date">
  <time datetime="2019-08-21T05:17:21.000Z" itemprop="datePublished">2019-08-21</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/tensorflow/">tensorflow</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/08/21/tensorflow/keras-1.2-Regressor/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>NN 可用来模拟 regression，给一组数据，用一条线对数据进行拟合，并可预测新输入 x 的输出值。</p>
<p>&lt;!-- more --&gt;</p>
<p>&lt;img src=&quot;/images/tensorflow/keras-regressor-1.2-1.png&quot; width=&quot;450&quot; alt=&quot;Regressor in Keras&quot;/&gt;</p>
<p><strong>创建数据</strong></p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 可视化模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create some data</span></span><br><span class="line">X = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">np.random.shuffle(X)    <span class="comment"># randomize the data</span></span><br><span class="line"></span><br><span class="line">Y = <span class="number">0.5</span> * X + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, (<span class="number">200</span>, ))</span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plot data</span></span><br><span class="line">plt.scatter(X, Y)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">X_train, Y_train = X[:<span class="number">160</span>], Y[:<span class="number">160</span>]     <span class="comment"># train 前 160 data points</span></span><br><span class="line">X_test, Y_test = X[<span class="number">160</span>:], Y[<span class="number">160</span>:]       <span class="comment"># test 后 40 data points</span></span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;/images/tensorflow/keras-regressor-1.2-2.png&quot; width=&quot;450&quot; /&gt;</p>
<h2>1. build model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(output_dim=<span class="number">1</span>, input_dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>用 Sequential 建立 model， 再用 model.add 添加神经层，添加的是 Dense FC 层</p>
<p>参数有两个，一个是输入数据和输出数据的维度，本代码的例子中 x 和 y 是一维的。</p>
<p>如果需要添加下一个神经层的时候，不用再定义输入的纬度，因为它默认就把前一层的输出作为当前层的输入。在这个例子里，只需要一层就够了。</p>
</blockquote>
<h2>2. compile model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># choose loss function and optimizing method</span></span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'sgd'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mse 均方误差； optimizer sgd 随机梯度下降法.</span></span><br></pre></td></tr></table></figure></p>
<h2>3. train model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># training</span></span><br><span class="line">print(<span class="string">'Training -----------'</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">301</span>):</span><br><span class="line">    cost = model.train_on_batch(X_train, Y_train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'train cost: '</span>, cost)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Training -----------</span></span><br><span class="line"><span class="string">train cost:  4.111329555511475</span></span><br><span class="line"><span class="string">train cost:  0.08777070790529251</span></span><br><span class="line"><span class="string">train cost:  0.007415373809635639</span></span><br><span class="line"><span class="string">train cost:  0.003544030711054802</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>训练的时候用 <code>model.train_on_batch</code> 一批一批的训练 <code>X_train</code>, <code>Y_train</code>。默认的返回值是 <code>cost</code>.</p>
</blockquote>
<h2>4. evaluate model</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line">cost = model.evaluate(X_test, Y_test, batch_size=<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'test cost:'</span>, cost)</span><br><span class="line">W, b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line">print(<span class="string">'Weights='</span>, W, <span class="string">'\nbiases='</span>, b)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Testing ------------</span></span><br><span class="line"><span class="string">40/40 [==============================] - 0s</span></span><br><span class="line"><span class="string">test cost: 0.004269329831</span></span><br><span class="line"><span class="string">Weights= [[ 0.54246825]] </span></span><br><span class="line"><span class="string">biases= [ 2.00056005]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>用到的函数是 <code>model.evaluate</code>，输入测试集的<code>x</code>和<code>y</code>， 输出 <code>cost</code>，<code>weights</code> 和 <code>biases</code>。其中 <code>weights</code> 和 <code>biases</code> 是取在模型的第一层 <code>model.layers[0]</code> 学习到的参数。</p>
<p>从学习到的结果可以看到, weights 比较接近0.5，bias 接近 2。</p>
</blockquote>
<h2>5. Visualization</h2>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plotting the prediction</span></span><br><span class="line">Y_pred = model.predict(X_test)</span><br><span class="line">plt.scatter(X_test, Y_test)</span><br><span class="line">plt.plot(X_test, Y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;/images/tensorflow/keras-regressor-1.2-3.png&quot; width=&quot;450&quot; /&gt;</p>
<h2>6. Gaussian Distribution</h2>
<p>先看伟大的高斯分布（Gaussian Distribution）的概率密度函数（probability density function）</p>
<p>$$
f(x)=\frac1{\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$</p>
<p>对应于numpy中：</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numpy.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<h2>Reference</h2>
<ul>
<li><a href="https://keras-cn.readthedocs.io/en/latest/backend/" target="_blank" rel="noopener">keras-cn</a>、 <a href="https://keras.io/" target="_blank" rel="noopener">keras.io</a></li>
<li><a href="https://blog.csdn.net/lanchunhui/article/details/50163669" target="_blank" rel="noopener">从np.random.normal()到正态分布的拟合</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/keras/" target="_blank" rel="noopener">莫烦 Keras</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/keras/3-1-save/" target="_blank" rel="noopener">莫烦 Keras Model, Save &amp; reload</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-tensorflow/keras-1.1-intro" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/08/19/tensorflow/keras-1.1-intro/"><strong>Keras Introduce</strong></a>
      <small class=article-date-index>&nbsp; 2019-08-19</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/08/19/tensorflow/keras-1.1-intro/" class="article-date">
  <time datetime="2019-08-19T05:17:21.000Z" itemprop="datePublished">2019-08-19</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/tensorflow/">tensorflow</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/08/19/tensorflow/keras-1.1-intro/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;!--&lt;img src=&quot;/images/tensorflow/keras-5.jpeg&quot; width=&quot;550&quot; alt=&quot;Keras&quot;/&gt;
--&gt;</p>
<p>Keras 开发重点是支持快速的实验。能够以最小的时延把你的想法转换为实验结果，是做好研究的关键。</p>
<p>Keras 并不处理如张量乘法、卷积等底层操作。这些操作依赖于某种特定的、优化良好的张量操作库。</p>
<p>&lt;!-- more --&gt;</p>
<p>&lt;img src=&quot;/images/tensorflow/keras-4.png&quot; width=&quot;550&quot; alt=&quot;Keras + Tensorflow&quot;/&gt;</p>
<h2>1. Keras install</h2>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install --upgrade pip</span><br><span class="line">pip3 install keras</span><br><span class="line">pip3 install ipython</span><br><span class="line">pip3 install notebook</span><br><span class="line">pip3 install tensorflow</span><br><span class="line">pip3 install --upgrade tensorflow</span><br><span class="line">pip3 install astkit</span><br><span class="line">pip3 install pandas</span><br><span class="line">pip3 install matplotlib</span><br></pre></td></tr></table></figure></p>
<h2>2. Basic concepts</h2>
<p><a href="https://keras-cn.readthedocs.io/en/latest/for_beginners/concepts/" target="_blank" rel="noopener">keras-cn.readthedocs.io 一些基本概念</a></p>
<ol>
<li>符号计算</li>
<li>tensor 张量</li>
<li>data_format</li>
<li>functional model API</li>
<li>batch</li>
<li>epochs</li>
</ol>
<blockquote>
<p>规模最小的张量是0阶张量，即标量，也就是一个数。</p>
<p>Keras 模型有一种叫 Sequential，也就是单输入单输出，一条路通到底，跨层连接统统没有。</p>
<p>Keras 中用的优化器SGD是stochastic gradient descent的缩写，不是一样本更新，还是基于mini-batch的.</p>
<p><a href="http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000" target="_blank" rel="noopener">廖雪峰的Python教程</a></p>
</blockquote>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 2</span></span><br><span class="line"><span class="comment"># 3 4</span></span><br><span class="line"></span><br><span class="line">sum0 = np.sum(a, axis=<span class="number">0</span>)</span><br><span class="line">sum1 = np.sum(a, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(sum0)</span><br><span class="line">print(sum1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [4 6]</span></span><br><span class="line"><span class="comment"># [3 7]</span></span><br></pre></td></tr></table></figure></p>
<h2>3. Quickstart in 30s</h2>
<p><a href="https://keras-cn.readthedocs.io/en/latest/" target="_blank" rel="noopener">30s上手Keras</a></p>
<p>Sequential模型如下</p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br></pre></td></tr></table></figure></p>
<p>将一些网络层通过<code>.add()</code>堆叠起来，就构成了一个模型：</p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation</span><br><span class="line"></span><br><span class="line">model.add(Dense(units=<span class="number">64</span>, input_dim=<span class="number">100</span>))</span><br><span class="line">model.add(Activation(<span class="string">"relu"</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">"softmax"</span>))</span><br></pre></td></tr></table></figure></p>
<p>完成模型的搭建后，我们需要使用<code>.compile()</code>方法来编译模型：</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'sgd'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># from keras.optimizers import SGD</span></span><br><span class="line"><span class="comment"># model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))</span></span><br></pre></td></tr></table></figure></p>
<p>完成模型编译后，我们在训练数据上按batch进行一定次数的迭代来训练网络</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(x_train, y_train, epochs=<span class="number">5</span>, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure></p>
<p>也可以手动将一个个batch的数据送入网络中训练，这时候需要使用：</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.train_on_batch(x_batch, y_batch)</span><br></pre></td></tr></table></figure></p>
<p>随后，我们可以使用一行代码对我们的模型进行评估，看看模型的指标是否满足我们的要求：</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss_and_metrics = model.evaluate(x_test, y_test, batch_size=<span class="number">128</span>)</span><br></pre></td></tr></table></figure></p>
<p>使用模型，对新的数据进行预测：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">classes = model.predict(x_test, batch_size=128)</span><br></pre></td></tr></table></figure></p>
<h2>4. Functional model</h2>
<p><a href="https://keras-cn.readthedocs.io/en/latest/getting_started/functional_API/" target="_blank" rel="noopener">快速开始函数式（Functional）模型</a></p>
<blockquote>
<ol>
<li>第一个模型：全连接网络</li>
<li>多输入和多输出模型</li>
<li>共享层, 层“节点”的概念</li>
</ol>
</blockquote>
<h2>5. Sequential model</h2>
<p><a href="https://keras-cn.readthedocs.io/en/latest/getting_started/sequential_model/" target="_blank" rel="noopener">Sequential model</a> 是多个网络层的线性堆叠，也就是“一条路走到黑”。</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">32</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br></pre></td></tr></table></figure></p>
<h3>5.1 input data shape</h3>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">32</span>, input_dim=<span class="number">784</span>))</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">32</span>, input_shape=(<span class="number">784</span>,)))</span><br></pre></td></tr></table></figure></p>
<h3>5.2 compile and train (fit)</h3>
<p><a href="https://keras-cn.readthedocs.io/en/latest/getting_started/sequential_model/" target="_blank" rel="noopener">Sequential model methods_cn</a>、<a href="https://keras.io/models/sequential/" target="_blank" rel="noopener">Sequential model methods_en</a></p>
<p><strong>compile</strong> Arguments</p>
<blockquote>
<ul>
<li>optimizer</li>
<li>loss</li>
<li>metrics</li>
</ul>
</blockquote>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For a multi-class classification problem</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></p>
<p>Keras 以 Numpy数组 作为 <strong><code>input_data</code></strong> 和 <strong><code>label</code></strong> 的数据类型。训练模型一般使用**<code>fit函数</code>**.</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For a single-input model with 2 classes (binary classification):</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>, input_dim=<span class="number">100</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate dummy data</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = np.random.random((<span class="number">1000</span>, <span class="number">100</span>))</span><br><span class="line">labels = np.random.randint(<span class="number">2</span>, size=(<span class="number">1000</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model, iterating on the data in batches of 32 samples</span></span><br><span class="line">model.fit(data, labels, epochs=<span class="number">10</span>, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure></p>
<h2>6. Pre-knowledge</h2>
<h3>6.1 python language</h3>
<ul>
<li>
<p>Object-oriented,  class, object, encapsulation, polymorphism, inheritance, scope, etc.</p>
</li>
<li>
<p>Python 的科学计算包有一定了解，numpy, scipy, scikit-learn, pandas...</p>
</li>
<li>
<p>generator，以及如何编写 generator。什么是匿名函数（lambda）</p>
</li>
</ul>
<h3>6.2 deep learning</h3>
<blockquote>
<p>Supervised Learning, Unsupervised Learning, Classification, Clustering, Regression</p>
<p>Neuron model, multilayer perceptron，BP algorithm</p>
<p>loss function，activation function，Gradient descent</p>
<p>Fully Connected NN、CNN、RNN、LSTM</p>
<p>Training set, test set, cross validation, under-fitting, over-fitting</p>
</blockquote>
<h2>Reference</h2>
<ul>
<li><a href="https://keras-cn.readthedocs.io/en/latest/backend/" target="_blank" rel="noopener">keras-cn</a></li>
<li><a href="https://keras.io/" target="_blank" rel="noopener">keras.io</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-nlp/info-NER" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/08/15/nlp/info-NER/"><strong>Named Entity Recognition，NER</strong></a>
      <small class=article-date-index>&nbsp; 2019-08-15</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/08/15/nlp/info-NER/" class="article-date">
  <time datetime="2019-08-15T03:00:21.000Z" itemprop="datePublished">2019-08-15</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/nlp/">nlp</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/08/15/nlp/info-NER/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;!--&lt;img src=&quot;/images/nlp/info-NER-1.png&quot; width=&quot;550&quot; alt=&quot;Information Extraction, Named Entity Recognition&quot;/&gt;--&gt;</p>
<p>&lt;img src=&quot;/images/nlp/info-NER-2.png&quot; width=&quot;600&quot; /&gt;</p>
<p>&lt;!-- more --&gt;</p>
<h2>1. NER Introduce</h2>
<p>NER系统 就是从非结构化的输入文本中抽取出上述实体，并且可以按照业务需求识别出更多类别的实体.</p>
<p>&lt;!--&lt;img src=&quot;/images/nlp/info-NER-2.png&quot; width=&quot;700&quot; alt=&quot;Information Extraction, NER&quot;/&gt;--&gt;</p>
<blockquote>
<p>命名实体识别是未登录词中数量最多、识别难度最大、对分词效果影响最大的问题，同时它也是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。</p>
</blockquote>
<h2>2. Deep Learning in NER</h2>
<p>&lt;img src=&quot;/images/nlp/info-NER-3.jpg&quot; width=&quot;650&quot; alt=&quot;NER发展趋势&quot;/&gt;</p>
<p>** Embedding+BiLSTM+CRF** 是一个非常强的 baseline 模型，是目前基于深度学习的 NER 方法中最主流的模型。</p>
<h3>2.1  BiLSTM-CRF</h3>
<p>LongShort Term Memory 网络一般叫做LSTM，是RNN的一种特殊类型，可以学习长距离依赖信息。</p>
<p>&lt;img src=&quot;/images/nlp/info-NER-5.png&quot; width=&quot;500&quot; alt=&quot;Information Extraction, NER&quot;/&gt;</p>
<p>LSTM 同样是这样的结构，但是重复的单元拥有一个不同的结构。不同于普通RNN单元，这里是有四个，以一种非常特殊的方式进行交互。</p>
<p>&lt;img src=&quot;/images/nlp/info-NER-6.png&quot; width=&quot;500&quot; alt=&quot;Information Extraction, NER&quot;/&gt;</p>
<p>LSTM 通过三个门结构（输入门，遗忘门，输出门），选择性地遗忘部分历史信息，加入部分当前输入信息，最终整合到当前状态并产生输出状态。</p>
<p>&lt;img src=&quot;/images/nlp/info-NER-7.png&quot; width=&quot;600&quot; alt=&quot;Information Extraction, NER&quot;/&gt;</p>
<p>应用于NER中的 biLSTM-CRF 模型主要由 Embedding层（主要有词向量，字向量等特征）。</p>
<p>无需特征工程，使用词向量以及字符向量就可以达到很好的效果，如果有高质量的词典特征，能进一步提高。</p>
<p>&lt;br&gt;
&lt;img src=&quot;/images/nlp/info-NER-8.png&quot; width=&quot;550&quot; alt=&quot;Information Extraction, NER&quot;/&gt;</p>
<h3>2.2 IDCNN-CRF</h3>
<h2>3. 实战应用</h2>
<h3>3.1 语料准备</h3>
<h3>3.2 数据增强</h3>
<h2>4. ELMO / GPT / Bert</h2>
<p><strong>ELMO / GPT / Bert</strong></p>
<ol>
<li>ELMO 是基于双向 LSTM 的语言模型.</li>
<li>GPT 是单向 Transformer 语言模型.</li>
<li>Bert 是双向 Transformer 语言模型.</li>
</ol>
<p><strong>NLP 领域已经开始从单一任务学习</strong>，发展为**<code>多任务两阶段学习</code>**：</p>
<ol>
<li>第一阶段利用语言模型进行预训练；</li>
<li>第二个阶段在下游任务上 finetune。</li>
</ol>
<p>这些语言模型在 NER 都达到了非常好的效果。</p>
<blockquote>
<p>姜兴华，浙江大学计算机硕士 ，研究方向机器学习，自然语言处理，在 ACM-multimedia、IJCAI 会议上发表过多篇文章。在 ByteCup2018 比赛中获得第一名。</p>
<p>崔德盛，北京邮电大学模式识别实验室 ，主要的研究方向是自然语言处理和广告推荐，曾获 2017 知乎看山杯挑战赛亚军，2017 摩拜算法挑战赛季军，2019 搜狐算法大赛冠军。</p>
</blockquote>
<h2>5. Summary</h2>
<p>最后进行一下总结，将神经网络与CRF模型相结合的CNN/RNN-CRF成为了目前NER的主流模型。</p>
<p>对于CNN与RNN，各有各的优点。由于RNN有天然的序列结构，所以RNN-CRF使用更为广泛。</p>
<h2>Reference</h2>
<ul>
<li>摘要心得: <strong><code>baeline</code></strong> 是非常重要的.</li>
<li><a href="https://zhuanlan.zhihu.com/p/75342886" target="_blank" rel="noopener">达观数据高翔详解文本抽取（附“达观杯”参赛方式）</a></li>
<li><a href="https://www.jishuwen.com/d/2TEc#tuit" target="_blank" rel="noopener">第三届“达观杯”文本智能算法大赛参赛指南</a></li>
<li><a href="https://easyai.tech/ai-definition/ner/" target="_blank" rel="noopener">Named-entity recognition | NER</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-08-31-2" target="_blank" rel="noopener">一文详解深度学习在命名实体识别(NER)中的应用</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-nlp/info-spo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/07/31/nlp/info-spo/"><strong>Information Extraction - SPO</strong></a>
      <small class=article-date-index>&nbsp; 2019-07-31</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/07/31/nlp/info-spo/" class="article-date">
  <time datetime="2019-07-31T01:00:21.000Z" itemprop="datePublished">2019-07-31</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/nlp/">nlp</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/07/31/nlp/info-spo/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;!--&lt;a href=&quot;/2019/06/30/nlp/BERT/&quot; target=&quot;_self&quot;&gt;&lt;img src=&quot;/images/nlp/Bert-Ernie-logo.jpg&quot; width=&quot;550&quot; alt=&quot;Bert-Ernie&quot; /&gt;
&lt;/a&gt;--&gt;</p>
<p>CNN + Attention</p>
<p>&lt;!-- more --&gt;</p>
<h2>DGCNN</h2>
<p>一个在 GTX1060 上都可以几个小时训练完成的模型！</p>
<h2>Reference</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/35755367?group_id=969333287374770176" target="_blank" rel="noopener">基于CNN的阅读理解式问答模型：DGCNN</a></li>
<li><a href="https://kexue.fm/archives/6671" target="_blank" rel="noopener">基于DGCNN和概率图的轻量级信息抽取模型</a></li>
<li><a href="https://www.cnblogs.com/vipyoumay/p/ner-chinese-keras.html" target="_blank" rel="noopener">基于keras的BiLstm与CRF实现命名实体标注</a></li>
<li><a href="https://www.zhihu.com/question/288927992" target="_blank" rel="noopener">用keras BILSTM+CRF层中文命名实体识别问题？</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-acm/leetcode" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/07/01/acm/leetcode/"><strong>2019 Leetcode</strong></a>
      <small class=article-date-index>&nbsp; 2019-07-01</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/07/01/acm/leetcode/" class="article-date">
  <time datetime="2019-07-01T02:54:16.000Z" itemprop="datePublished">2019-07-01</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/icpc/">icpc</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/07/01/acm/leetcode/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;img src=&quot;/images/icpc/Array-In-C.png&quot; width=&quot;580&quot; /&gt;</p>
<p>&lt;!-- more --&gt;</p>
<p>1、七种常见的数组排序算法整理(C语言版本)
2、2019 算法面试相关(leetcode)--数组和链表
3、2019 算法面试相关(leetcode)--字符串
4、2019 算法面试相关(leetcode)--栈和队列
5、2019 算法面试相关(leetcode)--优先队列
6、2019 算法面试相关(leetcode)--哈希表
7、2019 算法面试相关(leetcode)--树、二叉树、二叉搜索树
8、2019 算法面试相关(leetcode)--递归与分治
9、2019 算法面试相关(leetcode)--贪心算法
10、2019 算法面试相关(leetcode)--动态规划(Dynamic Programming)</p>
<p><strong>Hot100</strong></p>
<h3>hot easy</h3>
<blockquote>
<ol>
<li>两数之和， ✔️</li>
<li>反转链表， 回文链表(1.快慢p拆，2.翻转 3. 判断)， <a href="https://blog.csdn.net/qq_17550379/article/details/85680899" target="_blank" rel="noopener">相交链表</a>（f1: set() f2: A+B和B+A f3:快慢p）    ✔️</li>
<li><a href="https://blog.csdn.net/Jaster_wisdom/article/details/80662037" target="_blank" rel="noopener">最大子序和</a> （动态规划/分治法）， ✔️</li>
<li>爬楼梯， 打家劫舍  &amp; 环形链表 , 合并2个有序链表， ✔️</li>
<li>有效的括号， 最小栈 ✔️</li>
<li><a href="https://blog.csdn.net/cumt_cx/article/details/48015735" target="_blank" rel="noopener">买卖股票的最佳时机-3, 两阶段</a>， ✔️</li>
<li><a href="https://www.cnblogs.com/grandyang/p/4295761.html" target="_blank" rel="noopener">买卖股票的最佳时机-4, 超难</a></li>
<li>翻转二叉树 , 把二叉搜索树转换为累加树  ， ✔️</li>
<li>只出现一次的数字， 汉明距离 x&amp;(x-1)来统计二进制数x中1的个数 ✔️</li>
<li>二叉树的直径 &amp; 二叉树的最大深度 &amp; 对称二叉树, 合并二叉树 ， ✔️</li>
<li>求众数 ，移动零 ，找到所有数组中消失的数字</li>
<li>最短无序连续子数组,找到字符串中所有字母异位词 , 路径总和 III</li>
</ol>
</blockquote>
<h3>hot 中等</h3>
<p>Page1</p>
<blockquote>
<ol start="2">
<li>两数之和（方法：一遍哈希表）  Map &lt; Integer, Integer &gt; ， ✔️</li>
<li>最长回文子串</li>
<li>反转链表    ， ✔️</li>
<li>LRU缓存机制</li>
<li>编辑距离</li>
<li>无重复字符的最长子串</li>
<li>戳气球</li>
<li>接雨水</li>
<li>两数相加 (链表+进位carry)， ✔️</li>
<li>寻找两个有序数组的中位数</li>
<li><a href="https://blog.csdn.net/qq_17550379/article/details/80614597" target="_blank" rel="noopener">三数之和</a> （2方法： hash &amp; 对撞指针） ， ✔️</li>
<li><a href="https://blog.csdn.net/Jaster_wisdom/article/details/80662037" target="_blank" rel="noopener">最大子序和</a> （动态规划/分治法）， ✔️</li>
<li>字符串解码</li>
<li><a href="https://www.cnblogs.com/grandyang/p/4944875.html" target="_blank" rel="noopener">删除无效的括号</a></li>
<li><a href="https://www.cnblogs.com/grandyang/p/4322667.html" target="_blank" rel="noopener">最大矩形</a> 有难度 nok</li>
<li><a href="https://my.oschina.net/u/3744313/blog/1923933" target="_blank" rel="noopener">全排列 (交换法，类比字符串思路)</a>， ✔️</li>
<li>合并两个有序链表  ， ✔️</li>
<li>正则表达式匹配</li>
<li>零钱兑换 ， ✔️</li>
</ol>
</blockquote>
<p>Page3</p>
<blockquote>
<ol>
<li><a href="https://blog.csdn.net/weixin_41958153/article/details/81735032" target="_blank" rel="noopener">单词搜索</a> (DFS vector &lt; vector &lt; bool &gt; &gt; <strong>visit(row, vector &lt; bool &gt; (col, false))</strong>)</li>
<li><a href="https://www.cnblogs.com/grandyang/p/5275096.html" target="_blank" rel="noopener">打家劫舍之三</a> (DFS)</li>
<li>最小路径和</li>
<li>电话号码的字母组合</li>
<li>每日温度</li>
<li>回文子串</li>
<li>验证二叉搜索树</li>
<li>单词拆分</li>
<li>颜色分类</li>
<li>在排序数组中查找元素的第一个和最后一个位置</li>
<li>字母异位词分组</li>
<li>完全平方数</li>
<li>环形链表 II</li>
<li>除法求值</li>
<li>课程表</li>
<li>根据身高重建队列</li>
<li>除自身以外数组的乘积</li>
<li>目标和</li>
<li>比特位计数</li>
</ol>
</blockquote>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        HashMap&lt;Integer, Integer&gt; m = <span class="keyword">new</span> HashMap&lt;Integer, Integer&gt;();</span><br><span class="line">        <span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">2</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (m.containsKey(target - nums[i])) &#123;</span><br><span class="line">                res[<span class="number">0</span>] = i;</span><br><span class="line">                res[<span class="number">1</span>] = m.get(target - nums[i]);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            m.put(nums[i], i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<ol start="16">
<li>爬楼梯</li>
</ol>
</blockquote>
<h2>1. Array</h2>
<p><strong>1.1 easy</strong></p>
<blockquote>
<ol>
<li><strong>二维数组中的查找</strong> bool Find(int b[][4], int rows, int cols, int value)， ✔️</li>
<li>替换空格 char* replace(char* str, int len) ‘ ’-&gt;%20 在源数组总长度，从后向前，逐个赋值， ✔️</li>
<li>数字在排序数组中出现的次数 biSea(arr,k+0.5)-biSea(arr,k-0.5); / bina(*a, len, num, isLeft)， ✔️</li>
<li>旋转数组的最小元素 while(low &lt; high) { if(a[m] &gt; a[high]) min[m+1,high], else [low,m]} ✔️</li>
<li>调整数组位数使奇数位于前面 void odds(int[] arr) ， ✔️</li>
<li>次数超过一半的次数 * int core(int *a, int len)， ✔️</li>
<li><strong>丑数</strong>, 只包含质因子2、3和5的数称作丑数, 1, 2, 3, 5, 6, ... ， ✔️</li>
<li>和为S的两个数字(双指针思想) ， ✔️</li>
<li>扑克牌顺子 (排序后，统计大小王数量 + 间隔)， ✔️</li>
<li>构建乘积数组 (A数组，从前向后，再从后向前j-2,构造 B)， ✔️</li>
<li>求1+2+3+…+n (判断 &amp;&amp; 递归)， ✔️</li>
</ol>
</blockquote>
<p><strong>1.2 medium</strong></p>
<blockquote>
<ol>
<li><strong><code>八皇后</code></strong>， void dfs(int n) for(int i = 0; i &lt; 8; i++) { pos[n] = i; ， ✔️</li>
<li><a href="https://www.weiweiblog.cn/findcontinuoussequence/" target="_blank" rel="noopener">和S连续正数序</a> (3fun，mid = (1+sum)/2; while(start&lt;mid), Sum(int start, int end), 双vector)， ✔️</li>
<li>约瑟夫环 LinkedList; (index = (index + m) %link.size();link.remove(index--);) link.get(0); ， ✔️</li>
<li><a href="https://www.weiweiblog.cn/printminnumber/" target="_blank" rel="noopener">数组排成最小的数</a> Arrays.sort(str, new Comparator&lt;String&gt;(){ public int compare(String s1, String s2)，✔️</li>
<li>数组中只出现一次的数字 , 划分2数组，num &amp; (-num);二者与后得到的数，将num最右边的1保留下来，✔️</li>
</ol>
</blockquote>
<p><strong>1.3 important</strong></p>
<blockquote>
<ol>
<li>数组中的逆序对(归并排序).  void mergeSort(int a[], int l, int r) ， ✔️</li>
<li>最小的K个数(堆排序)， <a href="https://www.weiweiblog.cn/getmedian/" target="_blank" rel="noopener">数据流中的中位数 （2 PriorityQueue）</a>.</li>
<li><strong>最小的K个数</strong> （2fun）快排思想 part return l, 外else return left; void set_k(int* input, n, k) ， ✔️</li>
<li><strong>quickSort</strong>(a[],left,right), while(1) { (双while, if(l &gt;= r) break; swap) } swap(a[left], a[l]);， ✔️</li>
<li><a href="https://www.cnblogs.com/biyeymyhjob/archive/2012/07/31/2615833.html" target="_blank" rel="noopener">最短路Floyd</a>， ✔️</li>
</ol>
</blockquote>
<blockquote>
<p>quickSort (1 while + [2while + break + swap] + swap + 2 quickSort)</p>
<p>mergeSort (2 mergeSort + new *arr + 3 while + 1 for)</p>
</blockquote>
<p>1.1 八皇后</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">105</span>;</span><br><span class="line"><span class="keyword">int</span> pos[N];</span><br><span class="line"><span class="keyword">int</span> num = <span class="number">8</span>, cnt = <span class="number">0</span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ok</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(pos[i] == pos[n]) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">abs</span>(pos[n] - pos[i]) == n-i) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n == num) &#123;</span><br><span class="line">        cnt++;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num; i++) &#123;</span><br><span class="line">        pos[n] = i;</span><br><span class="line">        <span class="keyword">if</span>(ok(n)) &#123;</span><br><span class="line">            dfs(n+<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    res = dfs(<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; res &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.2 二维数组中的查找</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[][<span class="number">4</span>] = &#123;</span><br><span class="line">    &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span> &#125;,</span><br><span class="line">    &#123; <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span> &#125;,</span><br><span class="line">    &#123; <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>&#125;,</span><br><span class="line">    &#123; <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>&#125;,</span><br><span class="line">    &#123; <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Find</span><span class="params">(<span class="keyword">int</span> b[][<span class="number">4</span>], <span class="keyword">int</span> rows, <span class="keyword">int</span> cols, <span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> row = <span class="number">0</span>, col = cols - <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (b != <span class="literal">NULL</span> &amp;&amp; row &lt; rows &amp;&amp; col &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (b[row][col] == value) &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"row: "</span> &lt;&lt; row &lt;&lt; <span class="string">" col: "</span> &lt;&lt; col &lt;&lt; end;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (b[row][col] &gt; value) &#123;</span><br><span class="line">            col--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            row++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.3 替换空格</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">replace</span><span class="params">(<span class="keyword">char</span>* str, <span class="keyword">int</span> len)</span> </span>&#123;    </span><br><span class="line">    <span class="keyword">while</span>(str[len] != <span class="string">'\0'</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(str[len] == <span class="string">' '</span>)&#123;</span><br><span class="line">            konglen++;</span><br><span class="line">        &#125;</span><br><span class="line">        len++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'\0'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.4 旋转数组的最小元素</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(left &lt; right) &#123; <span class="comment">// exit. good idea!</span></span><br><span class="line">        <span class="keyword">int</span> l = left, r = right, x = a[l];</span><br><span class="line">        <span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[r] &gt;= x) r--;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[l] &lt;= x) l++;</span><br><span class="line">            <span class="keyword">if</span>(l &gt;= r) <span class="keyword">break</span>;</span><br><span class="line">            swap(a[r], a[l]);</span><br><span class="line">        &#125;</span><br><span class="line">        swap(a[left], a[l]);</span><br><span class="line">        quickSort(a, left, l<span class="number">-1</span>);</span><br><span class="line">        quickSort(a, l+<span class="number">1</span>, right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.5 调整数组位数使奇数位于前面</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">reorderOddEven</span><span class="params">(<span class="keyword">int</span>[] arr, len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (arr == <span class="literal">NULL</span> || len &lt;= <span class="number">1</span> ) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">0</span>, r = len - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r &amp;&amp; arr[r] % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">            r--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r &amp;&amp; arr[l] % <span class="number">2</span> == <span class="number">1</span>) &#123;</span><br><span class="line">            l++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (l &lt; r) &#123;</span><br><span class="line">            swap(arr[l], arr[r]);</span><br><span class="line">            l++, r--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.6 出现次数超过一半的次数</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">core</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (arr == <span class="literal">NULL</span> || len &lt;= <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> target = a[<span class="number">0</span>], cnt = <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a[i] == target) &#123;</span><br><span class="line">            cnt++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            cnt--;</span><br><span class="line">            <span class="keyword">if</span> (cnt == <span class="number">0</span>) &#123;</span><br><span class="line">                target = a[i];</span><br><span class="line">                cnt = <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> target;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.7 最小的K个数  part 快排思想</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">105</span>;</span><br><span class="line"><span class="keyword">int</span> a[N] = &#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">8</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">part</span> <span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(left &lt; right) &#123;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">int</span> x = a[<span class="number">0</span>], l = left, r = right;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">while</span>(l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[r] &gt;= x) r--;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r &amp;&amp; a[l] &lt;= x) l++;</span><br><span class="line">            <span class="keyword">if</span>(l &gt;= r) <span class="keyword">break</span>;</span><br><span class="line">            swap(a[l], a[r]);</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        swap(x, a[l]);</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> left;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_k</span><span class="params">(<span class="keyword">int</span> *input, <span class="keyword">int</span> n, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">if</span>(input == <span class="literal">NULL</span> || k &gt; n || k &lt;= <span class="number">0</span> || n &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> start = <span class="number">0</span>, end = n - <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> index = part(input, start, end);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(index != k<span class="number">-1</span>) &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(index &gt; k<span class="number">-1</span>) &#123;</span><br><span class="line">            end = index - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(index &lt; k - <span class="number">1</span>) &#123;</span><br><span class="line">            start = index + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        index = part(input, start, end);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.8 数组中的逆序对 &amp; 归并排序</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123; <span class="comment">//  8, 5, 4, 9, 2, 3, 6</span></span><br><span class="line">    <span class="keyword">if</span>(l &gt;= r) <span class="keyword">return</span>;   <span class="comment">// exit.</span></span><br><span class="line">    <span class="keyword">int</span> mid = (l+r) / <span class="number">2</span>; <span class="comment">// overflow  &lt;-&gt;  l + (r-l)/2</span></span><br><span class="line">    mergeSort(a, l, mid);</span><br><span class="line">    mergeSort(a, mid+<span class="number">1</span>, r);  </span><br><span class="line">    <span class="keyword">int</span> *arr = <span class="keyword">new</span> <span class="keyword">int</span>[r-l+<span class="number">1</span>];  </span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> i = l, j = mid + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid &amp;&amp; j &lt;= r) &#123;  </span><br><span class="line">        <span class="keyword">if</span>(a[i] &lt;= a[j]) &#123;</span><br><span class="line">            arr[k++] = a[i++]; </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            arr[k++] = a[j++]; <span class="comment">// ans += (mid-i+1);  </span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid) arr[k++] = a[i++];</span><br><span class="line">    <span class="keyword">while</span>(j &lt;= r) arr[k++] = a[j++];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = l; i &lt;= r; i++) &#123;</span><br><span class="line">        a[i] = arr[i-l];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> []arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.15 约瑟夫环</p>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">LastRemaining_Solution</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(n &lt; <span class="number">1</span> || m &lt; <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        LinkedList&lt;Integer&gt; link = <span class="keyword">new</span> LinkedList&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">            link.add(i);</span><br><span class="line">        <span class="keyword">int</span> index = -<span class="number">1</span>;   <span class="comment">//起步是 -1 不是 0</span></span><br><span class="line">        <span class="keyword">while</span>(link.size() &gt; <span class="number">1</span>)&#123;</span><br><span class="line">            index = (index + m) % link.size();  <span class="comment">//对 link的长度求余不是对 n</span></span><br><span class="line">            link.remove(index);</span><br><span class="line">            index --;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> link.get(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1.17 排序数组中某数字出现的次数</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bina</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> len, <span class="keyword">int</span> data)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(a == <span class="literal">NULL</span> || len &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">0</span>, r = len - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(l &lt;= r) &#123;</span><br><span class="line">        <span class="keyword">int</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span>(a[mid] == data) <span class="keyword">return</span> mid;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(data &lt; a[mid]) &#123;</span><br><span class="line">            r = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> l = mid+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<h2>2. LinkedList</h2>
<p><a href="https://www.weiweiblog.cn/linkedlist_summary/" target="_blank" rel="noopener">linkedlist_summary</a></p>
<p><strong>2.1 easy:</strong></p>
<blockquote>
<ol>
<li>在 O(1) 时间删除链表节点， ✔️</li>
<li>删除单链表倒数第 n 个节点， ✔️</li>
<li>求单链表的中间节点， ✔️</li>
<li>判断单链表是否存在环， ✔️</li>
<li>从尾到头打印链表, 递归 ok.， ✔️</li>
<li>链表中倒数第k个结点 ok.， ✔️</li>
<li>判断两个无环单链表是否相交， ✔️</li>
<li>两个链表相交扩展：求两个无环单链表的第一个相交点， ✔️</li>
<li>两个链表的第一个公共结点  ， ✔️</li>
<li>旋转单链表</li>
</ol>
<p>题目描述：给定一个单链表，设计一个算法实现链表向右旋转 K 个位置。
举例： 给定 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;NULL, K=3
则4-&gt;5-&gt;6-&gt;1-&gt;2-&gt;3-&gt;NULL</p>
</blockquote>
<p>环路的入口点</p>
<blockquote>
<p>在第 4 题两个指针相遇后，让其中一个指针回到链表的头部，另一个指针在原地，同时往前每次走一步，当它们再次相遇时，就是在环路的入口点。</p>
</blockquote>
<p><strong>2.2 medium:</strong></p>
<blockquote>
<ol start="2">
<li><a href="https://www.jianshu.com/p/bd6a64d36916" target="_blank" rel="noopener">反转链表</a> next=head-&gt;next, head-&gt;next=pre, pre=head, head=next; 4步 ok， ✔️</li>
<li>翻转部分单链表 举例：1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;null, from = 2, to = 4 结果：1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;null</li>
<li><a href="https://zhuanlan.zhihu.com/p/38888164" target="_blank" rel="noopener">复杂链表的复制</a> ok， ✔️</li>
<li>链表划分 （题目描述： 给定一个单链表和数值x，划分链表使得小于x的节点排在大于等于x的节点之前）</li>
<li>单链表排序</li>
<li>合并两个或k个有序链表  ok， 递归 (三元运算符).</li>
<li>删除链表重复结点  链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5. first-&gt;next=head, last, p 三针， ✔️</li>
<li>链表中环的入口结点， ✔️</li>
</ol>
</blockquote>
<p>单链表排序 or 合并两个或k个有序链表</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span> Node *next; <span class="keyword">int</span> value; &#125;; </span><br><span class="line"><span class="comment">/* 你一定要相信递归是一个强大的思想 */</span></span><br><span class="line"><span class="function">Node* <span class="title">mergeList</span><span class="params">(Node* head1, Node* head2)</span> </span>&#123; <span class="comment">// 有序链表的合并</span></span><br><span class="line">    <span class="keyword">if</span>(head1 == <span class="literal">NULL</span>) <span class="keyword">return</span> head2;</span><br><span class="line">    <span class="keyword">if</span>(head2 == <span class="literal">NULL</span>) <span class="keyword">return</span> head1;</span><br><span class="line">    Node* tmp;</span><br><span class="line">    <span class="keyword">if</span>(head1-&gt;value &lt; head2-&gt;value) &#123;</span><br><span class="line">        tmp = head1;</span><br><span class="line">        head1 = head1-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        tmp = head2;</span><br><span class="line">        head2 = head2-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    tmp-&gt;next = mergeList(head1, head2);</span><br><span class="line">    <span class="keyword">return</span> tmp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">Node* <span class="title">mergeSort</span><span class="params">(Node* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(head == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    Node* r_head = head;</span><br><span class="line">    Node* head1 = head;</span><br><span class="line">    Node* head2 = head;</span><br><span class="line">    <span class="keyword">while</span>(head2-&gt;next != <span class="literal">NULL</span> &amp;&amp; head2-&gt;next-&gt;next != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        head1 = head1-&gt;next;</span><br><span class="line">        head2 = head2-&gt;next-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(head1-&gt;next == <span class="literal">NULL</span>) <span class="keyword">return</span> r_head; <span class="comment">// 只有一个节点</span></span><br><span class="line">    head2 = head1-&gt;next;</span><br><span class="line">    head1 = head;</span><br><span class="line">    r_head = mergeList(mergeSort(head1), mergeSort(head2));</span><br><span class="line">    <span class="keyword">return</span> r_head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>2.3 difficul:</strong></p>
<ol>
<li>链表求和</li>
</ol>
<p><strong>---</strong></p>
<p><strong>反转链表:</strong></p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ListNode <span class="title">reverseList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">    ListNode next = null;</span><br><span class="line">    ListNode pre = null</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (head != null) &#123;</span><br><span class="line">        next = head.next; (保存当前头结点的下个节点)</span><br><span class="line">        head.next = pre;  (将当前头结点的下一个节点指向“上一个节点”，这一步是实现了反转)</span><br><span class="line">        pre = head;       (将当前头结点设置为“上一个节点”)</span><br><span class="line">        head = next;      (将保存的下一个节点设置为头结点)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pre;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>复杂链表的复制:</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-995312b12e59f14c77f3572e9c94d4c5_hd.jpg" alt=""></p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">public class RandomListNode &#123;</span></span><br><span class="line"><span class="comment">    int label;</span></span><br><span class="line"><span class="comment">    RandomListNode next = null;</span></span><br><span class="line"><span class="comment">    RandomListNode random = null;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    RandomListNode(int label) &#123;</span></span><br><span class="line"><span class="comment">        this.label = label;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RandomListNode <span class="title">Clone</span><span class="params">(RandomListNode pHead)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pHead == null)</span><br><span class="line">            <span class="keyword">return</span> null;</span><br><span class="line">        <span class="comment">//复制节点 A-&gt;B-&gt;C 变成 A-&gt;A'-&gt;B-&gt;B'-&gt;C-&gt;C'</span></span><br><span class="line">        RandomListNode head = pHead;</span><br><span class="line">        <span class="keyword">while</span>(head != null)&#123;</span><br><span class="line">            RandomListNode node = <span class="keyword">new</span> RandomListNode(head.label);</span><br><span class="line">            node.next = head.next;</span><br><span class="line">            head.next = node;</span><br><span class="line">            head = node.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//复制random</span></span><br><span class="line">        head = pHead;</span><br><span class="line">        <span class="keyword">while</span>(head != null)&#123;</span><br><span class="line">            head.next.random = head.random == null ? null : head.random.next;</span><br><span class="line">            head = head.next.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//折分</span></span><br><span class="line">        head = pHead;</span><br><span class="line">        RandomListNode chead = head.next;</span><br><span class="line">        <span class="keyword">while</span>(head != null)&#123;</span><br><span class="line">            RandomListNode node = head.next;</span><br><span class="line">            head.next = node.next;</span><br><span class="line">            node.next = node.next == null ? null : node.next.next;</span><br><span class="line">            head = head.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> chead;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>链表划分：</strong></p>
<p>给定一个单链表和数值x，划分链表使得所有小于x的节点排在大于等于x的节点之前。</p>
<p>你应该保留两部分内链表节点原有的相对顺序。</p>
<p>样例
给定链表 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;2-&gt;null，并且 x=3
返回** 1-&gt;2-&gt;2-&gt;4-&gt;3-&gt;5-&gt;null**</p>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">partition</span><span class="params">(ListNode head, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write your code here</span></span><br><span class="line">        <span class="keyword">if</span>(head == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        ListNode leftDummy = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">        ListNode rightDummy = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">        ListNode left = leftDummy, right = rightDummy;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (head != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (head.val &lt; x) &#123;</span><br><span class="line">                left.next = head;</span><br><span class="line">                left = head;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                right.next = head;</span><br><span class="line">                right = head;</span><br><span class="line">            &#125;</span><br><span class="line">            head = head.next;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        right.next = <span class="keyword">null</span>;</span><br><span class="line">        left.next = rightDummy.next;</span><br><span class="line">        <span class="keyword">return</span> leftDummy.next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2>3. String</h2>
<p><a href="https://www.weiweiblog.cn/13string/" target="_blank" rel="noopener">13 道题搞定 BAT 面试——字符串</a></p>
<p>字符串</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> x;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; x;</span><br><span class="line">    sort(x.begin(), x.end());</span><br><span class="line">    reverse(x.begin(), x.end());</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>3.1 easy:</strong></p>
<blockquote>
<ol>
<li>替换空格， ✔️</li>
<li>反转字符串 abcd -&gt; dcba， ✔️</li>
<li>翻转单词顺序列， I am a student. -&gt; student. a am I， ✔️</li>
<li>旋转字符串， abcde --2位--&gt; cdeab, 若干次旋转操作之后，A 能变成B，那么返回True， ✔️</li>
<li>左旋转字符串 string LeftRotateString(string str, int n),string Reverse(string str)， ✔️</li>
</ol>
</blockquote>
<blockquote>
<p>反转字串单词 string ReverseSentence(string str), reverse(str.begin(), str.end()); in lib algorithm</p>
</blockquote>
<p><strong>3.2 medium</strong></p>
<blockquote>
<ol>
<li><a href="https://www.weiweiblog.cn/firstappearingonce/" target="_blank" rel="noopener">字符流中第一个不重复的字符</a> (哈希来存每个字符及其出现的次数，另用一字符串 s 来保存字符流中字符顺序)</li>
<li><a href="https://www.weiweiblog.cn/firstnotrepeatingchar/" target="_blank" rel="noopener">第一个只出现一次的字符</a></li>
<li>字符串全排列 void res(char *str, char *pStr), scanf(&quot;%s&quot;, str); #include &lt; utility&gt;</li>
<li>字符串转整型 int StrToInt(char* str)       ok</li>
<li>字符串的排列 (给定两个字符串 s1 和 s2，第一个字符串的排列之一是第二个字符串的子串)</li>
</ol>
</blockquote>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    HashMap&lt;Character, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;Character, Integer&gt;();</span><br><span class="line">    StringBuffer s = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">    <span class="comment">//Insert one char from stringstream</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Insert</span><span class="params">(<span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        s.append(ch);</span><br><span class="line">        <span class="keyword">if</span>(map.containsKey(ch))&#123;</span><br><span class="line">            map.put(ch, map.get(ch)+<span class="number">1</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            map.put(ch, <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">//return the first appearence once char in current stringstream</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">char</span> <span class="title">FirstAppearingOnce</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(map.get(s.charAt(i)) == <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">return</span> s.charAt(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'#'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>3.2 difficult</strong></p>
<blockquote>
<ol>
<li>KMP 算法</li>
<li>最长公共前缀</li>
<li>最长回文串 (3.1) 验证回文串 (3.2) 最长回文子串 (3.3) 最长回文子序列</li>
<li>表示数值的字符串</li>
</ol>
</blockquote>
<blockquote>
<p>剑指offer: 表示数值的字符</p>
<p>请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100″,”5e2″,”-123″,”3.1416″和”-1E-16″都表示数值。 但是”12e”,”1a3.14″,”1.2.3″,”+-5″和”12e+4.3″都不是。</p>
</blockquote>
<p><strong>3.1 easy code</strong></p>
<p>3.1.1 替换空格</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=tail; i&gt;len &amp;&amp; i&gt;=<span class="number">0</span>; i--)&#123;</span><br><span class="line">    <span class="keyword">if</span>(str[len] == <span class="string">' '</span>) &#123;</span><br><span class="line">        str[i--] = <span class="string">'0'</span>;</span><br><span class="line">        str[i--] = <span class="string">'2'</span>;</span><br><span class="line">        str[i] = <span class="string">'%'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        str[i] = str[len];</span><br><span class="line">    &#125;</span><br><span class="line">    len--;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>3.1.4 旋转字符串</p>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">rotateString</span><span class="params">(String A, String B)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> A.length() == B.length() &amp;&amp; (A+A).contains(B);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>3.1.5 左旋转字符串</p>
<p>字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！</p>
<p>在第 n 个字符后面将切一刀，将字符串分为两部分，再重新并接起来即可。注意字符串长度为 0 的情况。</p>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">LeftRotateString</span><span class="params">(String str,<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = str.length();</span><br><span class="line">        <span class="keyword">if</span>(len == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        n = n % len;</span><br><span class="line">        String s1 = str.substring(n, len);</span><br><span class="line">        String s2 = str.substring(<span class="number">0</span>, n);</span><br><span class="line">        <span class="keyword">return</span> s1+s2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>3.2 medium code</strong></p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">res</span><span class="params">(<span class="keyword">char</span> *str, <span class="keyword">char</span> *pStr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(*pStr == <span class="string">'\0'</span>) <span class="built_in">cout</span> &lt;&lt; str &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">char</span> *p = pStr; *p != <span class="string">'\0'</span>; ++p) &#123;</span><br><span class="line">        <span class="keyword">char</span> tmp = *p;</span><br><span class="line">        *p = *pStr;</span><br><span class="line">        *pStr = tmp;</span><br><span class="line">        res(str, pStr+<span class="number">1</span>);</span><br><span class="line">        tmp = *p;</span><br><span class="line">        *p = *pStr;</span><br><span class="line">        *pStr = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>字符流中第一个不重复的字符 C++</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">/<span class="class"><span class="keyword">class</span> <span class="title">Solution</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">//Insert one char from stringstream</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// vector用来记录字符流</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; vec;</span><br><span class="line">    <span class="comment">// map用来统计字符的个数</span></span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; table;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Insert</span><span class="params">(<span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        table[ch]++;</span><br><span class="line">        vec.push_back(ch);</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">//return the first appearence once char in current stringstream</span></span><br><span class="line">    <span class="function"><span class="keyword">char</span> <span class="title">FirstAppearingOnce</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// 遍历字符流，找到第一个为1的字符</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">char</span> c : vec) &#123;</span><br><span class="line">            <span class="keyword">if</span> (table[c] == <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">return</span> c;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">'#'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>字符流中第一个不重复的字符 Java</p>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    HashMap&lt;Character, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;Character, Integer&gt;();</span><br><span class="line">    StringBuffer s = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">    <span class="comment">//Insert one char from stringstream    </span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Insert</span><span class="params">(<span class="keyword">char</span> ch)</span>    </span>&#123;</span><br><span class="line">        s.append(ch);</span><br><span class="line">        <span class="keyword">if</span>(map.containsKey(ch))&#123;</span><br><span class="line">            map.put(ch, map.get(ch)+<span class="number">1</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            map.put(ch, <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">//return the first appearence once char in current stringstream    </span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">char</span> <span class="title">FirstAppearingOnce</span><span class="params">()</span>    </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(map.get(s.charAt(i)) == <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">return</span> s.charAt(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'#'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2>4. Binary Tree</h2>
<ul>
<li><a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">算法&amp;数据结构 ， 20Tree</a></li>
</ul>
<p>&lt;img src=&quot;/images/icpc/BinaryTree-1.png&quot; width=&quot;450&quot; /&gt;</p>
<p><strong>4.1 easy</strong></p>
<blockquote>
<ol>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">求二叉树中的节点个数</a> ， <strong>✔️</strong></li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">求二叉树的最大层数(最大深度) &amp; (最小深度)</a> 最小深度特殊情况：left || right==0 ， ✔️</li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">求二叉树第K层的节点个数</a> get_k(root.left, k-1) + get_k(root.right, k-1); good ， ✔️</li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">求二叉树第K层的叶子节点个数</a> if(k==1 and root.left and root.right is null) return 1; ， ✔️</li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">二叉树先序遍历/前序遍历</a>  (fIno(Node* root) { while(1) {if else}</li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">判断两棵二叉树是否结构相同</a> ， ✔️</li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">求二叉树的镜像（反转二叉树）</a> ， （左右递归交换）✔️</li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">对称二叉树</a> （双函数，承接上题二叉树的镜像， good） ， ✔️</li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">求二叉树中两个节点的最低公共祖先节点 good</a> ， ✔️</li>
<li>递归： <a href="https://www.weiweiblog.cn/20tree/" target="_blank" rel="noopener">求二叉搜索树的最近公共祖先 good</a> ， ✔️</li>
<li>递归： 根据前序和中序重建二叉树 ， ✔️</li>
</ol>
</blockquote>
<p>双函数，递归</p>
<blockquote>
<ol>
<li>树的子结构,遍历+判断, bool f5(Node* root1, Node* root2), bool son(Node* p1, Node* p2)  ， ✔️</li>
<li>判断二叉树是不是平衡二叉树 bool isBalance(Node* root)， int maxHigh(Node* root)  ， ✔️</li>
<li>求二叉树的直径 （直径长度是任意两个结点路径长度中的最大值）， ✔️</li>
</ol>
</blockquote>
<p><strong>4.2 medium</strong></p>
<blockquote>
<ol>
<li>分层遍历 (判断二叉树是不是完全二叉树) （遍历到了NULL结点，如后续还有非NULL结点）， ✔️</li>
<li>分层遍历 (自下而上分层遍历) bfs + vector&lt; vector &lt; int &gt; &gt;， ✔️</li>
<li>分层遍历 (按之字形顺序打印二叉树)， ✔️</li>
</ol>
</blockquote>
<p><strong>4.3 difficult:</strong></p>
<blockquote>
<ol>
<li>二叉树中和为某一值的路径 void f4(Node* root, int exSum, int curSum, vecotr&lt; int &gt;&amp; path)， ✔️</li>
<li><a href="https://blog.csdn.net/libin1105/article/details/48422299" target="_blank" rel="noopener">二叉树下一结点:3情况</a>  (1.有right.child 2.没有right.child,父left.child 3.没有right.child,父right.child)✔️</li>
<li>序列化二叉树， String serialize(TreeNode root), TreeNode deserialize(String data) Queue&lt;String&gt; queue = new LinkedList&lt;&gt;(); ✔️</li>
<li>二叉搜索树的后序遍历序列 bool f6(int* sec, int len)， ✔️</li>
<li>二叉搜索树与双向链表 void convert(Node* root, Node*&amp; pLast) ， ✔️</li>
<li>二叉搜索树的第k个结点 ， ✔️</li>
<li><a href="https://blog.csdn.net/xiaoxiaoxuanao/article/details/61918125" target="_blank" rel="noopener">二叉查找树节点的删除</a>.  重要</li>
</ol>
</blockquote>
<p><strong>4.1 easy code</strong></p>
<p>4.1.9 二叉树两节点的最低公共祖先</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">    Node* lchild;</span><br><span class="line">    Node* rchild;</span><br><span class="line">    <span class="keyword">int</span> value;</span><br><span class="line">    Node(): value(<span class="number">0</span>), lchild(<span class="literal">NULL</span>), rchild(<span class="literal">NULL</span>) &#123;&#125;</span><br><span class="line">    </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;Node*&gt; vec1;</span><br><span class="line"><span class="built_in">vector</span>&lt;Node*&gt; vec2;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  1. 找到目标节点1，寻找路径，存放在 vec1 中.</span></span><br><span class="line"><span class="comment">  2. 找到目标节点2，寻找路径，存放在 vec2 中.</span></span><br><span class="line"><span class="comment">  3. 同时遍历 2 个 </span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">getNodePath</span><span class="params">(Node* root, Node* target, <span class="built_in">vector</span>&lt;Node*&gt;&amp; vec)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == NULl) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    vec.push_back(root);</span><br><span class="line">    <span class="keyword">if</span> (root == target) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line">    flag = getNodePath(root-&gt;lchild, target, vec)</span><br><span class="line">    <span class="keyword">if</span> (!flag &amp;&amp; root-&gt;rchild) &#123;</span><br><span class="line">        flag = getNodePath(root-&gt;rchild, target, vec)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!flag) &#123;</span><br><span class="line">        vec.pop_back();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> flag;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Node* <span class="title">getCom</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Node*&gt;&amp; v1,  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Node*&gt;&amp; v2)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;Node*&gt;::const_iterator it1 = v1.begin();</span><br><span class="line">    <span class="built_in">vector</span>&lt;Node*&gt;::const_iterator it2 = v2.begin();</span><br><span class="line">    </span><br><span class="line">    Node* pLast = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (it1 != v1.end() &amp;&amp; it2 != v2.end()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (*it1 != *it2) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        pLast = *it1;</span><br><span class="line">        it1++;</span><br><span class="line">        it2++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pLast;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    retunr <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.1.10 求二叉搜索树的最近公共祖先</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Node* <span class="title">lowestCommonAncestor</span><span class="params">(Node* root, Node* p, Node* q)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">NULL</span> || p == <span class="literal">NULL</span> || q == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">if</span> (root-&gt;value &gt; p-&gt;value &amp;&amp; root &gt; q-&gt;value) &#123;</span><br><span class="line">        <span class="keyword">return</span> lowestCommonAncestor(root-&gt;lchild, p, q);</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">if</span> (root-&gt;value &lt; p-&gt;value &amp;&amp; root &lt; q-&gt;value) &#123;</span><br><span class="line">        <span class="keyword">return</span> lowestCommonAncestor(root-&gt;rchild, p, q);</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.1.11 前序中序重建二叉树</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Node* <span class="title">f3</span><span class="params">(<span class="keyword">int</span>* pre, <span class="keyword">int</span>* ino, <span class="keyword">int</span> len)</span> </span>&#123; <span class="comment">// pre : 1, 2, 4, 7, 3, 5, 6, 8  ino : 4, 7, 2, 1, 5, 3, 8, 6</span></span><br><span class="line">    <span class="keyword">if</span>(pre == <span class="literal">NULL</span> || ino == <span class="literal">NULL</span> || len &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">int</span> r_v = pre[<span class="number">0</span>];</span><br><span class="line">    Node* root = <span class="keyword">new</span> Node();</span><br><span class="line">    root-&gt;value = r_v;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; ; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(ino[i] == r_v) <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    root-&gt;lchild = f3(pre+<span class="number">1</span>, ino, i);</span><br><span class="line">    root-&gt;rchild = f3(pre+i+<span class="number">1</span>, ino+i+<span class="number">1</span>, len<span class="number">-1</span>-i);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>双函数 + 递归</p>
<p>4.1.12 树2是否是树1的子结构</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">son</span><span class="params">(Node* p1, Node* p2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p2 == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (p1 == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (p1-&gt;value == p2-&gt;value) &#123;</span><br><span class="line">        <span class="keyword">return</span> son(p1-&gt;lchild, p2-lchild);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">son_tree</span><span class="params">(Node* root1, Node* root2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root2 == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (root1 == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (root1-&gt;value == root2-&gt;child) &#123;</span><br><span class="line">        <span class="keyword">return</span> son(root1, root2);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">bool</span> flag = <span class="literal">false</span></span><br><span class="line">    flag = son_tree(root1-&gt;lchild, root2);</span><br><span class="line">    <span class="keyword">if</span> (!flag) &#123;</span><br><span class="line">        <span class="keyword">return</span> son_tree(root1-&gt;rchild, root2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.1.13 平衡二叉树</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isBalanced</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">-1</span> &lt;= (maxHigh(root-&gt;lchild) - maxHigh(root-&gt;rchild)) &lt;= <span class="number">1</span>)</span><br><span class="line">        &amp;&amp; isBalanced(root-&gt;lchild) &amp;&amp; isBalanced(root-&gt;lchild);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxHigh</span><span class="params">(Node* root)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> max(maxHigh(root-&gt;lchild), maxHigh(root-&gt;rchild))+<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.1.14 求二叉树的直径</p>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">diamHelper</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> left = diamHelper(root.left);</span><br><span class="line">    <span class="keyword">int</span> right = diamHelper(root.right);</span><br><span class="line">    path = Math.max(path, left + right);</span><br><span class="line">    <span class="keyword">return</span> Math.max(left, right) + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>4.2 medium code</strong></p>
<p>4.2.1 判断二叉树是不是完全二叉树</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">checkCompleteTree</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> flag = <span class="literal">true</span>;</span><br><span class="line">    <span class="built_in">queue</span>&lt;Node*&gt; q;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (root == null)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    q.push(root);</span><br><span class="line"></span><br><span class="line">     <span class="keyword">while</span>(!q.empty())&#123;  </span><br><span class="line">         <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; q.size(); ++i) &#123;  </span><br><span class="line">             Node* tmp = q.front();</span><br><span class="line">             q.pop();</span><br><span class="line">            </span><br><span class="line">             <span class="keyword">if</span> (tmp-&gt;lchild == <span class="literal">NULL</span> &amp;&amp; tmp-&gt;rchild != <span class="literal">NULL</span>)&#123;</span><br><span class="line">                 flag = <span class="literal">false</span>;</span><br><span class="line">                 <span class="keyword">break</span>;</span><br><span class="line">             &#125;</span><br><span class="line">             <span class="keyword">if</span> (tmp-&gt;left != <span class="literal">NULL</span>)</span><br><span class="line">                 que.push(tmp-&gt;left);</span><br><span class="line">             <span class="keyword">if</span> (tmp-&gt;right != <span class="literal">NULL</span>)</span><br><span class="line">                 que.push(tmp-&gt;right);</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> flag;    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.2.2 分层遍历 (自下而上分层遍历)</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; bfs(Node* root) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span> &lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; ans;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">queue</span>&lt;Node*&gt; q;</span><br><span class="line">    </span><br><span class="line">    q.push(root);</span><br><span class="line"></span><br><span class="line">     <span class="keyword">while</span>(!q.empty()) &#123;  </span><br><span class="line">     </span><br><span class="line">         <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tv;</span><br><span class="line">         </span><br><span class="line">         <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; q.size(); ++i) &#123;  </span><br><span class="line">             Node* tmp = q.front();</span><br><span class="line">             q.pop();</span><br><span class="line">             <span class="keyword">if</span> (tmp-&gt;lchild == <span class="literal">NULL</span> &amp;&amp; tmp-&gt;rchild != <span class="literal">NULL</span>)&#123;</span><br><span class="line">                 flag = <span class="literal">false</span>;</span><br><span class="line">                 <span class="keyword">break</span>;</span><br><span class="line">             &#125;</span><br><span class="line">             <span class="keyword">if</span> (tmp-&gt;left != <span class="literal">NULL</span>)</span><br><span class="line">                 que.push(tmp-&gt;left);</span><br><span class="line">             <span class="keyword">if</span> (tmp-&gt;right != <span class="literal">NULL</span>)</span><br><span class="line">                 que.push(tmp-&gt;right);</span><br><span class="line">             </span><br><span class="line">             tv.push_back(tmp-&gt;value);</span><br><span class="line">         &#125;</span><br><span class="line">         ans.push_back(tv)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> flag;    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// reverse(res[i].begin(), res[i].end());</span></span><br></pre></td></tr></table></figure></p>
<p><strong>4.3 difficult code</strong></p>
<p>4.3.1 二叉树中和为某一值的路径</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f4</span><span class="params">(Node*, <span class="keyword">int</span>, <span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f4</span><span class="params">(Node* root, <span class="keyword">int</span> exSum)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; V;</span><br><span class="line">    <span class="keyword">int</span> curSum = <span class="number">0</span>;</span><br><span class="line">    f4(root, exSum, curSum, V);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f4</span><span class="params">(Node* root, <span class="keyword">int</span> exSum, <span class="keyword">int</span> curSum, vecotr&lt;<span class="keyword">int</span>&gt;&amp; path)</span> </span>&#123;</span><br><span class="line">    curSum += root-&gt;value;</span><br><span class="line">    path.push_back(root-&gt;value);</span><br><span class="line">    <span class="keyword">if</span>(curSum == exSum &amp;&amp; root-&gt;lchild == <span class="literal">NULL</span> &amp;&amp; root-&gt;rchild == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="comment">//; 打印vector中的路径</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(root-&gt;lchild) f4(root-&gt;lchild, exSum, curSum, path);</span><br><span class="line">    <span class="keyword">if</span>(root-&gt;rchild) f4(root-&gt;rchild, exSum, curSum, path);</span><br><span class="line">    curSum -= root-&gt;value;</span><br><span class="line">    path.pop_back();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.3.3 序列化二叉树</p>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">serialize</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"#,"</span>;</span><br><span class="line">    StringBuffer res = <span class="keyword">new</span> StringBuffer(root.val + <span class="string">","</span>);</span><br><span class="line">    res.append(serialize(root.left));</span><br><span class="line">    res.append(serialize(root.right));</span><br><span class="line">    <span class="keyword">return</span> res.toString();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Decodes your encoded data to tree.</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">deserialize</span><span class="params">(String data)</span> </span>&#123;</span><br><span class="line">    String [] d = data.split(<span class="string">","</span>);</span><br><span class="line">    Queue&lt;String&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; d.length; i++)&#123;</span><br><span class="line">        queue.offer(d[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pre(queue);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">pre</span><span class="params">(Queue&lt;String&gt; queue)</span></span>&#123;</span><br><span class="line">    String val = queue.poll();</span><br><span class="line">    <span class="keyword">if</span>(val.equals(<span class="string">"#"</span>))</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    TreeNode node = <span class="keyword">new</span> TreeNode(Integer.parseInt(val));</span><br><span class="line">    node.left = pre(queue);</span><br><span class="line">    node.right = pre(queue);</span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.3.4 二叉搜索树后序遍历的结果</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">f6</span><span class="params">(<span class="keyword">int</span>* sec, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(sec == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(len &lt;= <span class="number">1</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">int</span> i, rv = sec[len<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len<span class="number">-1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(sec[i] &gt; rv) <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = i; j &lt; len<span class="number">-1</span>; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(sec[j] &lt; rv) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> f6(sec, i) &amp;&amp; f6(sec+i, len-i<span class="number">-1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.3.5 二叉搜索树与双向链表</p>
<p><img src="https://pic4.zhimg.com/80/v2-dfed873e672f0cb9aa0f6cd729fc19df_hd.jpg" alt=""></p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">convert</span><span class="params">(Node* root, Node*&amp; pLast)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">if</span>(root-&gt;lchild) convert(root-&gt;lchild, pLast);</span><br><span class="line">    </span><br><span class="line">    Node* pCur = root;</span><br><span class="line">    pCur-&gt;lchild = pLast;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(pLast) pLast-&gt;rchild = pCur;</span><br><span class="line">    pLast = pCur;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(root-&gt;rchild) convert(root-&gt;rchild, pLast);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4.3.6 二叉搜索树的第k个结点</p>
<p>给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8）中，按结点数值大小顺序第三小结点的值为4。</p>
<p>因为二叉搜索树按照中序遍历的顺序打印出来就是排好序的，所以，我们按照中序遍历找到第k个结点就是题目所求的结点。</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">kthSmallest</span><span class="params">(TreeNode root, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == null)</span><br><span class="line">             <span class="keyword">return</span> Integer.MIN_VALUE;</span><br><span class="line">        Stack&lt;TreeNode&gt; <span class="built_in">stack</span> = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        TreeNode p = root;</span><br><span class="line">        <span class="keyword">while</span>(p != null || !<span class="built_in">stack</span>.isEmpty())&#123;</span><br><span class="line">            <span class="keyword">if</span>(p != null)&#123;</span><br><span class="line">                <span class="built_in">stack</span>.push(p);</span><br><span class="line">                p = p.left;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                TreeNode node = <span class="built_in">stack</span>.pop();</span><br><span class="line">                count ++;</span><br><span class="line">                <span class="keyword">if</span>(count == k)</span><br><span class="line">                    <span class="keyword">return</span> node.val;</span><br><span class="line">                p = node.right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Integer.MIN_VALUE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2>5. 递归 / 回溯</h2>
<h3>5.1 斐波拉契</h3>
<blockquote>
<ol>
<li>斐波拉契数列 &amp; 跳台阶 &amp; 变态跳台阶  2 * Fib(n-1).  ， ✔️</li>
<li>矩形覆盖  ， ✔️</li>
</ol>
</blockquote>
<h3>5.2 回溯</h3>
<blockquote>
<ol>
<li><a href="https://www.weiweiblog.cn/haspath/" target="_blank" rel="noopener">矩阵中的路径(BFS)</a></li>
<li><a href="https://www.weiweiblog.cn/movingcount/" target="_blank" rel="noopener">机器人的运动范围(DFS)</a></li>
</ol>
</blockquote>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> threshold, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols, <span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">int</span>[][] flag)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">0</span> || j&lt;<span class="number">0</span> || i&gt;=rows || j&gt;=cols || sum(i)+sum(j) &gt; threshold || flag[i][j] == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    flag[i][j] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> + count(threshold, rows, cols, i - <span class="number">1</span>, j, flag) + </span><br><span class="line">        count(threshold, rows, cols, i + <span class="number">1</span>, j, flag) +</span><br><span class="line">        count(threshold, rows, cols, i, j - <span class="number">1</span>, flag) +</span><br><span class="line">        count(threshold, rows, cols, i, j + <span class="number">1</span>, flag);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3>5.3 位运算</h3>
<blockquote>
<ol>
<li>二进制中1的个数  n &amp; n-1.， ✔️</li>
<li>数值的整数次方 dp.   ， ✔️</li>
<li>数组中只出现一次的数字 ok.  ， ✔️</li>
</ol>
</blockquote>
<h2>6. Stack &amp; Queue &amp; heap</h2>
<p><a href="https://www.weiweiblog.cn/ispoporder/" target="_blank" rel="noopener">Stack &amp; Queue</a></p>
<blockquote>
<ol>
<li><strong><code>用两个栈实现队列</code></strong> ， ✔️</li>
<li><strong><code>包含min函数的栈</code></strong> ， ✔️</li>
<li><a href="https://www.weiweiblog.cn/ispoporder/" target="_blank" rel="noopener">栈的压入、弹出序列</a></li>
</ol>
</blockquote>
<h3>6.1 用两个栈实现队列</h3>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyQueue</span> </span>&#123;</span><br><span class="line">    Stack&lt;Integer&gt; input = <span class="keyword">new</span> Stack&lt;Integer&gt;();</span><br><span class="line">    Stack&lt;Integer&gt; output = <span class="keyword">new</span> Stack&lt;Integer&gt;();</span><br><span class="line">    <span class="comment">/** Push element x to the back of queue. */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        input.push(x);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/** Removes the element from in front of queue and returns that element. */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        peek();</span><br><span class="line">        <span class="keyword">return</span> output.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/** Get the front element. */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">peek</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(output.isEmpty())&#123;</span><br><span class="line">            <span class="keyword">while</span>(!input.isEmpty())</span><br><span class="line">                output.push(input.pop());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> output.peek();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/** Returns whether the queue is empty. */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">empty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> input.isEmpty() &amp;&amp; output.isEmpty();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3>6.2 包含min函数的栈</h3>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MinStack</span> </span>&#123;</span><br><span class="line">    Stack&lt;Integer&gt; stack = <span class="keyword">new</span> Stack&lt;Integer&gt;();</span><br><span class="line">    Stack&lt;Integer&gt; temp = <span class="keyword">new</span> Stack&lt;Integer&gt;();</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        stack.push(x);</span><br><span class="line">        <span class="keyword">if</span>(temp.isEmpty() || temp.peek() &gt;= x)</span><br><span class="line">            temp.push(x);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> x = stack.pop();</span><br><span class="line">        <span class="keyword">int</span> min = temp.peek();</span><br><span class="line">        <span class="keyword">if</span>(x == min)</span><br><span class="line">            temp.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> stack.peek();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getMin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> temp.peek();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3>6.3 栈的 push pop 序列</h3>
<p>栈的 push pop 序列
1 2 3 4 5
4 3 5 1 2</p>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Stack;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">IsPopOrder</span><span class="params">(<span class="keyword">int</span> [] pushA, <span class="keyword">int</span> [] popA)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pushA.length != popA.length || </span><br><span class="line">               pushA.length == <span class="number">0</span> ||</span><br><span class="line">               popA.length == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        Stack&lt;Integer&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; pushA.length; i++)&#123;</span><br><span class="line">            stack.push(pushA[i]);</span><br><span class="line">            <span class="keyword">while</span>(!stack.empty() &amp;&amp; stack.peek() == popA[index])&#123;</span><br><span class="line">                stack.pop();</span><br><span class="line">                index++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> stack.empty();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2>7. PriorityQueue</h2>
<blockquote>
<ol>
<li><a href="https://www.weiweiblog.cn/getleastnumbers_solution/" target="_blank" rel="noopener">最小的K个数</a></li>
<li>数据流中的第K大元素</li>
<li>滑动窗口最大值</li>
<li>前K个高频单词</li>
</ol>
</blockquote>
<ul>
<li><a href="https://www.jianshu.com/p/1bedaee726da" target="_blank" rel="noopener">优先队列</a></li>
</ul>
<h2>8. Dynamic Programming</h2>
<blockquote>
<ol>
<li>爬楼梯 ， ✔️</li>
<li>不同路径 II ， ✔️</li>
<li>编辑距离 ， ✔️</li>
</ol>
</blockquote>
<p>不同路径 II</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1782258-9dbebab909d4a555.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400/format/webp" alt=""></p>
<blockquote>
<p>如果当前没有障碍物，dp[m][n] = dp[m - 1][n] + dp[m][n - 1]
如果有障碍物，则dp[m][n] = 0</p>
</blockquote>
<p>编辑距离</p>
<blockquote>
<p>如果单词1第i+1个字符和单词2第j+1个字符相同，那么就不用操作，则DP[i + 1][j + 1] = DP[i][j];</p>
<p>如果不相同,则有三种可能操作，即增，删，替换。则取这三种操作的最优值，即dp[i + 1][j + 1] = 1 + Math.min(dp[i][j], Math.min(dp[i][j + 1], dp[i + 1][j]));</p>
</blockquote>
<h3>8.1 一维DP</h3>
<blockquote>
<ol>
<li><strong>连续子数组的最大和</strong>   // dp: F[i] = max(a[i], F[i-1]+a[i]);</li>
</ol>
</blockquote>
<h3>8.2 二维DP</h3>
<p><strong>布尔数组</strong></p>
<blockquote>
<ol>
<li>Longest Palindromic Substring/最长回文子串 给出一个字符串S，找到一个最长的连续回文串。</li>
</ol>
</blockquote>
<ol start="2">
<li>Interleaving String/交错字符串 输入三个字符串s1、s2和s3，判断第三个字符串s3是否由前两个字符串s1和s2交替而成且不改变s1和s2中各个字符原有的相对顺序。</li>
</ol>
<p><strong>数字数组</strong></p>
<blockquote>
<ol>
<li><a href="https://blog.csdn.net/yuanliang861/article/details/83514372" target="_blank" rel="noopener">Unique Paths II/不同路径</a> (初始化很重要) ， 起点到终点有多少条不同路径，向右或向下走。</li>
</ol>
</blockquote>
<ol start="2">
<li><a href="https://www.cnblogs.com/grandyang/p/4353255.html" target="_blank" rel="noopener">Minimum Path Sum</a> 矩阵左上角出发到右下角，只能向右或向下走，找出哪一条路径上的数字之和最小。</li>
<li>Edit Distance/编辑距离 求两个字符串之间的最短编辑距离，即原来的字符串至少要经过多少次操作才能够变成目标字符串，操作包括删除一个字符、插入一个字符、更新一个字符。</li>
<li>Distinct Subsequences/不同子序列 给定S和T两个字符串，问把通过删除S中的某些字符，把S变为T有几种方法？</li>
</ol>
<blockquote>
<p>补充：京东2019实习编程题-删除0或部分字符使其成为回文串 见笔试整理总结</p>
</blockquote>
<h3>8.3 三维DP</h3>
<h2>9. 剑指offer</h2>
<p><a href="https://www.weiweiblog.cn/category/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/page/5/" target="_blank" rel="noopener">算法学习5</a></p>
<ol>
<li>数据流中的中位数</li>
<li>滑动窗口的最大值 (双向队列)</li>
<li><a href="https://www.weiweiblog.cn/match/" target="_blank" rel="noopener">正则表达式匹配</a></li>
<li><a href="https://www.weiweiblog.cn/power/" target="_blank" rel="noopener">数值的整数次方</a></li>
<li><a href="https://www.weiweiblog.cn/findfirstcommonnode/" target="_blank" rel="noopener">两个链表的第一个公共节点（2个stack or 长短链表相减）</a></li>
</ol>
<p><strong>diffcult</strong></p>
<ol start="14">
<li><a href="https://github.com/WordZzzz/Note/blob/master/AtOffer/%E3%80%8A%E5%89%91%E6%8C%87offer%E3%80%8B%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0%EF%BC%88%E6%97%B6%E9%97%B4%E6%95%88%E7%8E%87%EF%BC%89%EF%BC%9A%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%88%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%89.md" target="_blank" rel="noopener">整数中1出现的次数</a> (判断每一位， 比如百位分别为 1, 0, 2~9, 后2种情况可合并)</li>
<li><a href="https://blog.csdn.net/JackZhang_123/article/details/78015549" target="_blank" rel="noopener">LRU Cache 需要深入学习Java的Map的内部实现</a></li>
</ol>
<blockquote>
<ol>
<li>vector: vector&lt;int&gt;::iterator, Modifiers (push_back, pop_back, insert)</li>
<li>array : len = sizeof(arr)/sizeof(int)</li>
</ol>
</blockquote>
<p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ArrayList&lt;Integer&gt; <span class="title">maxInWindows</span><span class="params">(<span class="keyword">int</span> [] num, <span class="keyword">int</span> size)</span></span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line">        LinkedList&lt;Integer&gt; deque = <span class="keyword">new</span> LinkedList&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">if</span>(num.length == <span class="number">0</span> || size == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num.length; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(!deque.isEmpty() &amp;&amp; deque.peekFirst() &lt;= i - size)</span><br><span class="line">                deque.poll();</span><br><span class="line">            <span class="keyword">while</span>(!deque.isEmpty() &amp;&amp; num[deque.peekLast()] &lt; num[i])</span><br><span class="line">                deque.removeLast();</span><br><span class="line">            deque.offerLast(i);</span><br><span class="line">            <span class="keyword">if</span>(i + <span class="number">1</span> &gt;= size)</span><br><span class="line">                res.add(num[deque.peekFirst()]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2>10. shopee &amp; sina</h2>
<p>shopee</p>
<blockquote>
<ol>
<li><a href="https://blog.csdn.net/zhangzhetaojj/article/details/80837232" target="_blank" rel="noopener">下一个更大元素</a> (Stack &lt; Integer &gt; (), Map &lt; Int, Int &gt;  map, map.getOrDefault(nums1[i], -1); 序列为 9 2 1 4 借助栈实现，判断栈顶 和 下一个元素的大小 ， ✔️</li>
<li><a href="https://www.cnblogs.com/grandyang/p/11048142.html" target="_blank" rel="noopener">鸡蛋掉落</a> (DP问题，难)</li>
<li>google 扔鸡蛋，原题是 100 层楼，鸡蛋无限，答案 14 次。， ✔️</li>
<li><a href="https://cxyxiaowu.com/articles/2019/05/02/1556786653527.html" target="_blank" rel="noopener">二叉树的右视图 (层次遍历)</a> res.push_back(q.back()-&gt;val);， ✔️</li>
<li>复杂链表指针  ， ✔️</li>
<li><a href="https://blog.csdn.net/qq_17550379/article/details/80696835" target="_blank" rel="noopener">K 个一组翻转链表</a> 1-&gt;2-&gt;3-&gt;4-&gt;5， 当 k = 2 时，应返: 2-&gt;1-&gt;4-&gt;3-&gt;5 ， ✔️</li>
<li><a href="https://blog.csdn.net/shinanhualiu/article/details/50225093" target="_blank" rel="noopener">不同的二叉搜索树</a>  ， ✔️</li>
<li><a href="https://blog.csdn.net/zw159357/article/details/82664026" target="_blank" rel="noopener">零钱兑换</a> 完全背包问题 ，i=coins[j];i&lt;=amount;，dp[i]=Math.min(dp[i],dp[i-coins[j]]+1); ✔️</li>
<li>相交链表</li>
<li><a href="https://blog.csdn.net/qq_17550379/article/details/80723003" target="_blank" rel="noopener">有效的括号 （Stack来解决）</a> ， ✔️</li>
<li>两数相加</li>
</ol>
</blockquote>
<p>[LeetCode] 887. Super Egg Drop 超级鸡蛋掉落 ， ✔️</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">dp[i][j] = min(dp[i][j], max(dp[i - <span class="number">1</span>][k - <span class="number">1</span>], dp[i][j - k]) + <span class="number">1</span>) ( <span class="number">1</span> &lt;= k &lt;= j )</span><br></pre></td></tr></table></figure></p>
<p>之后可以再优化.</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValid</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type s: str</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        stack = list()</span><br><span class="line">        match = &#123;<span class="string">'&#123;'</span>:<span class="string">'&#125;'</span>, <span class="string">'['</span>:<span class="string">']'</span>, <span class="string">'('</span>:<span class="string">')'</span>&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> i == <span class="string">'&#123;'</span> <span class="keyword">or</span> i == <span class="string">'('</span> <span class="keyword">or</span> i == <span class="string">'['</span>:</span><br><span class="line">                stack.append(i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> len(stack) == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">                top = stack.pop()</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> match[top] != i:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(stack) != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure></p>
<p>sina</p>
<blockquote>
<ol>
<li>两数之和 (链表 carry) ， ✔️</li>
<li>最大子序和 ， ✔️ 分治？</li>
<li><a href="https://www.cnblogs.com/grandyang/p/5310649.html" target="_blank" rel="noopener">搜寻名人</a> (if (knows(res, i)) res = i;) ， ✔️</li>
<li>连续出现的数字  ， ✔️</li>
<li>搜索二维矩阵 ， ✔️</li>
<li>排序链表</li>
<li>翻转二叉树 ， ✔️</li>
<li>买卖股票的最佳时机 系列</li>
<li>字符串转整型</li>
<li>无重复字符的最长子串 ， ✔️ （借助hashmap）</li>
</ol>
</blockquote>
<p>Tencent</p>
<blockquote>
<p><a href="https://www.cnblogs.com/grandyang/p/7076026.html" target="_blank" rel="noopener">Minimum Factorization 最小因数分解</a>
[LeetCode 最小基因变化（广度优先搜索）]</p>
</blockquote>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">addTwoNumbers</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> carry = <span class="number">0</span>;</span><br><span class="line">        ListNode* fakeHead = <span class="keyword">new</span> ListNode(<span class="number">-1</span>);</span><br><span class="line">        ListNode* curr = fakeHead;</span><br><span class="line">        <span class="keyword">while</span> (l1 != <span class="literal">NULL</span> || l2 != <span class="literal">NULL</span> | carry != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (l1 != <span class="literal">NULL</span>) &#123;</span><br><span class="line">                carry += l1-&gt;val;</span><br><span class="line">                l1 = l1-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (l2 != <span class="literal">NULL</span>) &#123;</span><br><span class="line">                carry += l2-&gt;val;</span><br><span class="line">                l2 = l2-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            ListNode* node = <span class="keyword">new</span> ListNode(carry % <span class="number">10</span>);</span><br><span class="line">            carry /= <span class="number">10</span>;</span><br><span class="line">            curr-&gt;next = node;</span><br><span class="line">            curr = node;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> fakeHead-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h2>11. 海量数据</h2>
<p>m_15 二叉树非递归的 先根遍历和中序遍历   (必会数据结构之一)</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fPre</span><span class="params">(Node* root)</span> </span>&#123; <span class="comment">// 先根遍历  根-&gt;左-&gt;右</span></span><br><span class="line">    Node* p = root;</span><br><span class="line">    <span class="built_in">stack</span>&lt;Node*&gt;  S;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; p-&gt;value &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">            S.push(p);</span><br><span class="line">            p = p-&gt;lchild;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(S.empty()) <span class="keyword">return</span>;</span><br><span class="line">            p = S.top(); S.pop();</span><br><span class="line">            p = p-&gt;rchild;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fIno</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">    Node* p = root;</span><br><span class="line">    <span class="built_in">stack</span>&lt;Node*&gt; S;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            S.push(p);</span><br><span class="line">            p = p-&gt;lchild;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(S.empty()) <span class="keyword">return</span>;</span><br><span class="line">            p = S.top(); S.pop();</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; p-&gt;value &lt;&lt;  <span class="string">' '</span>;</span><br><span class="line">            p = p-&gt;rchild;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><a href="https://leetcode-cn.com/interview" target="_blank" rel="noopener">leetcode CN</a></li>
<li><a href="https://leetcode.com/problemset/all/" target="_blank" rel="noopener">leetcode EN</a></li>
</ul>
<h2>Reference</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/35707293" target="_blank" rel="noopener">知乎： [Leetcode][动态规划]相关题目汇总/分析/总结</a></li>
<li><a href="https://www.jianshu.com/p/af880bbba792" target="_blank" rel="noopener">简书： 2019 算法面试相关(leetcode)--动态规划(Dynamic Programming)</a></li>
<li><a href="https://blog.csdn.net/EbowTang/article/details/50791500" target="_blank" rel="noopener">CSDN leetcode DP</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/06/25/nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews/"><strong>Fine-grained Sentiment Analysis of User Online Reviews</strong></a>
      <small class=article-date-index>&nbsp; 2019-06-25</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/25/nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews/" class="article-date">
  <time datetime="2019-06-25T02:16:21.000Z" itemprop="datePublished">2019-06-25</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/chatbot/">chatbot</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/25/nlp/Fine-grained-Sentiment-Analysis-of-User-Online-Reviews/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;img src=&quot;/images/deeplearning/AI-Challenger-11.png&quot; width=&quot;550&quot; alt=&quot;AI-Challenger&quot;/&gt;</p>
<p>&lt;!-- more --&gt;</p>
<p><a href="https://challenger.ai/" target="_blank" rel="noopener">Challenger.AI</a></p>
<p>Online reviews have become the critical factor to make consumption decision in recent years. They not only have a profound impact on the incisive understanding of shops, users, and the implied sentiment, but also have been widely used in Internet and e-commerce industry, such as personalized recommendation, intelligent search, product feedback, and business security. In this challenge, we provide a dataset of user reviews for fine-grained sentiment analysis from the catering industry, containning 335K public user reviews from Dianping.com. The dataset builds a two-layer labeling system according to the granularity, which contains 6 categories and 20 fine-grained elements.</p>
<blockquote>
<p>Training set: 105K</p>
<p>Verification set: 15K</p>
<p>Test set A: 15K</p>
<p>Test set B: 200K</p>
</blockquote>
<p>There are four sentimental types for every fine-grained element: Positive, Neutral, Negative and Not mentioned, which are labelled as 1, 0, -1 and-2. The meaning of these four labels are listed below.</p>
<p>&lt;img src=&quot;/images/deeplearning/AI-Challenger-14.png&quot; width=&quot;650&quot; alt=&quot;&quot;/&gt;</p>
<p>An example of one labelled review:</p>
<blockquote>
<p>“味道不错的面馆，性价比也相当之高，分量很足～女生吃小份，胃口小的，可能吃不完呢。环境在面馆来说算是好的，至少看上去堂子很亮，也比较干净，一般苍蝇馆子还是比不上这个卫生状况的。中午饭点的时候，人很多，人行道上也是要坐满的，隔壁的冒菜馆子，据说是一家，有时候也会开放出来坐吃面的人。“</p>
</blockquote>
<p>&lt;img src=&quot;/images/deeplearning/AI-Challenger-13.png&quot; width=&quot;650&quot; alt=&quot;&quot;/&gt;</p>
<hr>
<p>「AI Challenger」是面向全球人工智能人才的开源数据集和编程竞赛平台。AI Challenger 2018 由创新工场、搜狗、美团点评、美图公司联合主办。有上万支团队参赛， 覆盖 81 个国家、1100 所高校、990 家公司。</p>
<p>&lt;!--&lt;img src=&quot;/images/deeplearning/AI-Challenger-21.webp&quot; width=&quot;850&quot; alt=&quot;&quot;/&gt;
--&gt;</p>
<h2>整体流程</h2>
<ul>
<li><strong>问题建模</strong></li>
<li><strong>模型基本架构</strong></li>
<li><strong>数据处理</strong>
。。。</li>
</ul>
<p>&lt;img src=&quot;/images/deeplearning/AI-Challenger-22.webp&quot; width=&quot;890&quot; alt=&quot;&quot;/&gt;</p>
<p>有20个粒度的评价指标，每个粒度又有4种情感状态，从官方baseline来看，分别训练了20个（4标签）分类器。</p>
<blockquote>
<p>FastText（0.573）、Attention-RNN (0.637)、Attention-RCNN (0.669)、ELMO-like（0.68830）</p>
</blockquote>
<h2>1. FastText baseline</h2>
<blockquote>
<p>skift：scikit-learn wrappers for Python fastText.</p>
</blockquote>
<p>什么是 skift?</p>
<blockquote>
<p>skift 包括几个 scikit-learn兼容包裝器, 封裝了fasttext模型，fasttext原理类似word2vec，主要用于文本快速分类。其优势在于分类速度快，使用n-gram 特别容易获得文本句子局部信息、构造新詞。</p>
</blockquote>
<p>fasttext 缺点是随着语料的增长，內存需求也会增长。那么如果解決內存问题呢？</p>
<blockquote>
<ol>
<li>过滤掉次数出现少的词；</li>
<li>采用word粒度，而非char粒度</li>
</ol>
</blockquote>
<p>例如句子:</p>
<blockquote>
<p>我喜歡去中國， 如果採用char粒度，則使用2-gram的話，產生的特徵爲:</p>
<p>我喜 喜歡 歡中 中國</p>
</blockquote>
<p>如果採用word粒度的話，產生的特徵爲</p>
<blockquote>
<p>我喜歡 喜歡去 去中國</p>
</blockquote>
<ul>
<li><a href="https://www.twblogs.net/a/5c1215debd9eee5e40bb42fd" target="_blank" rel="noopener">利用skift實現fasttext模型</a></li>
<li><a href="https://blog.csdn.net/supinyu/article/details/81136590" target="_blank" rel="noopener">FastText文本分类以及生成词向量</a></li>
<li><a href="https://blog.csdn.net/sinat_26917383/article/details/83041424#2_fasttext_21" target="_blank" rel="noopener">极简使用︱Gensim-FastText 词向量训练以及OOV</a></li>
<li><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">fastText原理和文本分类实战，看这一篇就够了</a></li>
</ul>
<blockquote>
<p>默认配置参数训练 fasttext 多模型，f1均值 约为 0.5513 的 fasttext 多分类模型(20个）</p>
</blockquote>
<h3>1.1 数据情况记录</h3>
<p>fastText 时候， 词的粒度</p>
<ul>
<li>jieba 分词</li>
<li>建立词典时，过滤掉出现次数小于 2~5 的词</li>
<li>训练集、验证集 以及 测试集A组成的语料，词典大小为 66347</li>
<li>预测和训练时，词典没有出现的词 用 <code>&lt;UNK&gt;</code> 代替</li>
</ul>
<h3>1.2 main code</h3>
<p>利用训练集，来训练 <strong>20 个</strong> 4分类 分类器, 训练 15 分钟</p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">sk_clf = FirstColFtClassifier(lr=learning_rate, epoch=epoch,</span><br><span class="line">                              wordNgrams=word_ngrams,</span><br><span class="line">                              minCount=min_count, verbose=<span class="number">2</span>)</span><br><span class="line">sk_clf.fit(train_data_format, train_label)</span><br></pre></td></tr></table></figure></p>
<p>&lt;!--
验证集，计算 macro F1</p>
<pre><code>for column in columns[2:]:
    true_label = np.asarray(validate_data_df[column])
    classifier = classifier_dict[column]
    pred_label = classifier.predict(validata_data_format).astype(int)
    f1_score = get_f1_score(true_label, pred_label)
    f1_score_dict[column] = f1_score

f1_score = np.mean(list(f1_score_dict.values()))
</code></pre>
<p>--&gt;</p>
<blockquote>
<p>min_count设置为2貌似也有一些负向影响， word_ngrams 2， epoch 10 .</p>
</blockquote>
<h3>1.3 baseline 效果</h3>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">service_wait_time:0.5247890022873511</span><br><span class="line">service_waiters_attitude:0.6781093513108542</span><br><span class="line">service_parking_convenience:0.5828932335474249</span><br><span class="line">service_serving_speed:0.6146828053320519</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">f1_score: 0.5513</span><br></pre></td></tr></table></figure></p>
<p>调参：</p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">python main_train.py -mn fasttext_model_wn2.pkl -wn <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>约跑15分钟左右，存储的模型大约在17G，验证集 macro F1值结果如下：</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">service_wait_time:0.5247890022873511</span><br><span class="line">service_waiters_attitude:0.6881098513108542</span><br><span class="line">service_parking_convenience:0.5828935095474249</span><br><span class="line">service_serving_speed:0.6168828054420539</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">f1_score: 0.5783</span><br></pre></td></tr></table></figure></p>
<p>这个结果看起来还不错，我们可以基于这个fasttext多分类模型进行测试集的预测：</p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">python main_predict.py -mn fasttext_wn2_model.pkl</span><br></pre></td></tr></table></figure></p>
<p>优化方法： 去停用词和去一些标点符号，调参，learning_rate的影响是比较直接的，min_count</p>
<h3>1.4 fastText 速度快</h3>
<p>能够做到效果好，速度快，主要依靠两个秘密武器：</p>
<blockquote>
<ol>
<li>利用了 词内的n-gram信息 (subword n-gram information)</li>
<li>用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick.</li>
</ol>
</blockquote>
<h2>2. Attention RNN、RCNN</h2>
<h3>2.1 预处理 data</h3>
<p>粗暴使用char模型，用到的停用词也不多。</p>
<ul>
<li>trainsets lines：  501132， 合法例子 ： 105000</li>
<li>validationset lines：  70935, 合法例子 ： 15000</li>
<li>testsets lines：  72028， 合法例子 ： 15000</li>
</ul>
<p>数据预处理，生成 train_char.csv、test_char.csv、test_char.csv 三个文件:</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-rw-r--r--  1 blair 10:36 test_char.csv</span><br><span class="line">-rw-r--r--  1 blair 10:09 train_char.csv</span><br><span class="line">-rw-r--r--  1 blair 10:32 validation_char.csv</span><br></pre></td></tr></table></figure></p>
<p><strong>word2vec：</strong> 维度 100， 窗口 10， 过滤掉次数小于 1~2 的字</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3.1M chars.vector</span><br></pre></td></tr></table></figure></p>
<p>过滤掉低频词之后：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">word2vec/chars.vector 为 7983 * 100</span><br></pre></td></tr></table></figure></p>
<h3>2.2 Attention RCNN</h3>
<blockquote>
<p>Attention-RNN (0.637)、Attention-RCNN (0.669)</p>
</blockquote>
<ul>
<li>Attention 参考自 Kaggle 的 <a href="https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043" target="_blank" rel="noopener">Attention Model</a></li>
</ul>
<p>Kaggle 常见文本分类结构: 2层GRU 接Attention层，然后和 avgpool、maxpool concat 接起来.</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(self, embeddings_matrix, maxlen, word_index, num_class)</span>:</span></span><br></pre></td></tr></table></figure></p>
<p>为了之后 summary 看清楚网络结构，所以我们一些参数先写死看一下：</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># from JoinAttLayer import Attention</span></span><br><span class="line"></span><br><span class="line">maxlen=<span class="number">1200</span></span><br><span class="line"></span><br><span class="line">inp = Input(shape=(maxlen,)) <span class="comment"># 当输入序列的长度固定时，该值为其长度 1200 （一个文档doc的最大长度）</span></span><br><span class="line"></span><br><span class="line">encode = Bidirectional(CuDNNGRU(<span class="number">128</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line">encode2 = Bidirectional(CuDNNGRU(<span class="number">128</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># attention = Attention(maxlen)</span></span><br><span class="line"></span><br><span class="line">x_4 = Embedding(<span class="number">7555</span>+ <span class="number">1</span>,<span class="comment">#7983+1 # 词汇表大小， 即，最大整数 index + 1, len(word_index) + 1, # input_dim</span></span><br><span class="line">                <span class="number">100</span>, <span class="comment"># output_dim: int &gt;= 0。词向量的维度。</span></span><br><span class="line">                input_length=maxlen, <span class="comment"># maxlen=1200, 一个 doc 最大长度</span></span><br><span class="line">                trainable=<span class="literal">True</span>)(inp)</span><br></pre></td></tr></table></figure></p>
<p>接下来：</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_3 = encode(x_4)</span><br><span class="line">x_3 = encode2(x_3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入shape， 形如（samples，steps，features）的3D张量</span></span><br><span class="line"><span class="comment"># 输出shape， 形如(samples, features)的2D张量</span></span><br><span class="line">avg_pool_3 = GlobalAveragePooling1D()(x_3) <span class="comment"># GlobalAveragePooling1D 为时域信号施加全局平均值池化</span></span><br><span class="line">max_pool_3 = GlobalMaxPooling1D()(x_3) <span class="comment"># 对于时间信号的全局最大池化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># attention_3 = attention(x_3)</span></span><br><span class="line"></span><br><span class="line">x = keras.layers.concatenate([avg_pool_3, max_pool_3], name=<span class="string">"fc"</span>)</span><br><span class="line">x = Dense(<span class="number">4</span>, activation=<span class="string">"softmax"</span>)(x)</span><br><span class="line"></span><br><span class="line">adam = keras.optimizers.Adam(lr=<span class="number">0.001</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, epsilon=<span class="number">1e-08</span>, amsgrad=<span class="literal">True</span>)</span><br><span class="line">rmsprop = keras.optimizers.RMSprop(lr=<span class="number">0.001</span>, rho=<span class="number">0.9</span>, epsilon=<span class="number">1e-06</span>)</span><br><span class="line">model = Model(inputs=inp, outputs=x)</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">    loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">    optimizer=adam)</span><br><span class="line"></span><br><span class="line"><span class="comment"># categorical_crossentropy 用来做多分类问题</span></span><br><span class="line"><span class="comment"># binary_crossentropy 用来做多标签分类问题</span></span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure></p>
<p>RNN：</p>
<p>&lt;img src=&quot;/images/deeplearning/AI-Challenger-16-1.png&quot; width=&quot;900&quot; alt=&quot;&quot;/&gt;</p>
<p>RCNN：</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_4 = Embedding(<span class="number">7555</span>+ <span class="number">1</span>,<span class="comment">#7983+1 # 词汇表大小， 即，最大整数 index + 1</span></span><br><span class="line">                <span class="number">100</span>,</span><br><span class="line">                input_length=maxlen,</span><br><span class="line">                trainable=<span class="literal">True</span>)(inp)</span><br><span class="line"></span><br><span class="line">x_3 = encode(x_4)</span><br><span class="line">x_3 = encode2(x_3)</span><br><span class="line"></span><br><span class="line">x_3 = Conv1D(<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">"valid"</span>, kernel_initializer=<span class="string">"glorot_uniform"</span>)(x_3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入shape， 形如（samples，steps，features）的3D张量</span></span><br><span class="line"><span class="comment"># 输出shape， 形如(samples, features)的2D张量</span></span><br><span class="line">avg_pool_3 = GlobalAveragePooling1D()(x_3) <span class="comment"># GlobalAveragePooling1D 为时域信号施加全局平均值池化</span></span><br><span class="line">max_pool_3 = GlobalMaxPooling1D()(x_3) <span class="comment"># 对于时间信号的全局最大池化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># attention_3 = attention(x_3)</span></span><br><span class="line"></span><br><span class="line">x = keras.layers.concatenate([avg_pool_3, max_pool_3], name=<span class="string">"fc"</span>)</span><br><span class="line">x = Dense(<span class="number">4</span>, activation=<span class="string">"softmax"</span>)(x)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;/images/deeplearning/AI-Challenger-17-1.png&quot; width=&quot;900&quot; alt=&quot;&quot;/&gt;</p>
<blockquote>
<p>Input 一个网络层次，输入层 在 keras</p>
<p>SpatialDropout1D ，那么常规的 dropout 将无法使激活正则化，且导致有效的学习速率降低。
SpatialDropout1D ，在这种情况下，SpatialDropout1D 将有助于提高特征图之间的独立性，应该使用它来代替 Dropout。</p>
<p>CuDNNGRU 是 基于CuDNN的快速GRU实现，只能在GPU上运行，只能使用 tensoflow 为后端
CuDNNLSTM 是 基于CuDNN的快速LSTM实现，只能在GPU上运行，只能使用 tensoflow 为后端</p>
<p>attention = Attention(maxlen)</p>
<p>Embedding嵌入层将正整数（下标）转换为具有固定大小的向量，如[<a href="https://blog.csdn.net/liuchonge/article/details/77140719" target="_blank" rel="noopener">4</a>, [20]]-&gt;[[0.25, 0.1], [0.6, -0.2]]</p>
</blockquote>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">keras.layers.embeddings.Embedding(</span><br><span class="line">    input_dim, </span><br><span class="line">    output_dim, </span><br><span class="line">    embeddings_initializer=<span class="string">'uniform'</span>, <span class="comment"># embeddings_regularizer=None, </span></span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,  <span class="comment"># embeddings_constraint=None,              </span></span><br><span class="line">    mask_zero=<span class="literal">False</span>, </span><br><span class="line">    input_length=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>Embedding 的一些参数解释：</p>
<blockquote>
<p>Embedding层只能作为模型的第一层</p>
<p>input_dim: int &gt; 0。词汇表大小， 即，最大整数 index + 1。
output_dim: int &gt;= 0。词向量的维度。
embeddings_initializer: 嵌入矩阵的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers
input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。</p>
</blockquote>
<p><a href="/2018/08/21/deeplearning/Convolutional-Neural-Networks-week1/#4-1-%E8%BF%90%E7%94%A8-Padding-%E7%9A%84%E5%8E%9F%E5%9B%A0">Convolutional Neural Networks (week1) - CNN , 运用 Padding</a>
<a href="https://blog.csdn.net/wuzqChom/article/details/74785643" target="_blank" rel="noopener">TensorFlow中CNN的两种padding方式“SAME”和“VALID”</a></p>
<blockquote>
<p>word2vec : 7983 100 word2vec/chars.vector 过滤掉低频词</p>
</blockquote>
<p>循环卷积神经网络(RCNN)，并将其应用于文本分类的任务。首先，我们应用一个双向的循环结构，与传统的基于窗口的神经网络相比，它可以大大减少噪声，从而最大程度地捕捉上下文信息。此外，<strong>该模型在学习文本表示时可以保留更大范围的词序</strong>。其次，我们使用了一个可以<strong>自动判断哪些特性在文本分类中扮演关键角色的池化层</strong>，以捕获文本中的关键组件。我们的模型结合了RNN的结构和最大池化层，<strong>利用了循环神经模型和卷积神经模型的优点</strong>。此外，我们的模型显示了O(n)的时间复杂度，它与文本长度的长度是线性相关的。</p>
<blockquote>
<ol>
<li>RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。</li>
<li>RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。</li>
<li>CNN 优点： 提取数据中的局部位置的特征，然后再拼接池化层。 CNN可以更好地捕捉文本的语义。是O(n)</li>
<li>CNN 优点： 一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。</li>
</ol>
</blockquote>
<blockquote>
<p>首先我们来理解下什么是卷积操作？卷积，你可以把它想象成一个应用在矩阵上的滑动窗口函数。</p>
<p>卷积网络也就是对输入样本进行多次卷积操作，提取数据中的局部位置的特征，然后再拼接池化层（图中的Pooling层）做进一步的降维操作</p>
<p>我们可以把CNN类比N-gram模型，N-gram也是基于词窗范围这种局部的方式对文本进行特征提取，与CNN的做法很类似</p>
</blockquote>
<ul>
<li><a href="https://plushunter.github.io/2018/03/08/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9ARCNN/" target="_blank" rel="noopener">自然语言处理系列（8）：RCNN</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/29201491" target="_blank" rel="noopener">Keras之文本分类实现</a></li>
</ul>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.backend.tensorflow_backend <span class="keyword">import</span> set_session</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">set_session(tf.Session(config=config))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.seed = <span class="number">42</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> set_random_seed</span><br><span class="line">set_random_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> text, sequence</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint, Callback</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, recall_score, precision_score</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> classifier_bigru <span class="keyword">import</span> TextClassifier</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models.keyedvectors <span class="keyword">import</span> KeyedVectors</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gc</span><br></pre></td></tr></table></figure></p>
<h3>2.3 loss function</h3>
<p><a href="https://blog.csdn.net/qinglv1/article/details/85701106" target="_blank" rel="noopener">多分类和多标签分类</a>, <a href="https://blog.csdn.net/sinat_26917383/article/details/69803018" target="_blank" rel="noopener">gensim训练word2vec及相关函数</a></p>
<blockquote>
<p>多分类：类别数目大于2个，类别之间是互斥的。比如是猫，就不能是狗、猪
categorical crossentropy 用来做多分类问题
binary crossentropy 用来做多标签分类问题</p>
</blockquote>
<p><a href="https://www.zhihu.com/question/36307214" target="_blank" rel="noopener">sigmoid,softmax,binary/categorical crossentropy的联系？</a></p>
<blockquote>
<p><strong>Binary cross-entropy</strong> 常用于二分类问题，当然也可以用于多分类问题，通常需要在网络的最后一层添加sigmoid进行配合使用</p>
<p><strong>Categorical cross-entropy</strong> 适用于多分类问题，并使用softmax作为输出层的激活函数的情况。</p>
</blockquote>
<h3>2.4 Early Stop</h3>
<p>需要在每个 epoch 结束之后去计算模型的 F1 值，这样可以更好的掌握模型的训练情况。</p>
<blockquote>
<p>Tips 如果我们在训练中设置 metric 的话，得到是每个 batch 的 F1 值, 是不靠谱的.</p>
</blockquote>
<p>类似这样:</p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getClassification</span><span class="params">(arr)</span>:</span></span><br><span class="line">    arr = list(arr)</span><br><span class="line">    <span class="keyword">if</span> arr.index(max(arr)) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-2</span></span><br><span class="line">    <span class="keyword">elif</span> arr.index(max(arr)) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">elif</span> arr.index(max(arr)) == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Metrics</span><span class="params">(Callback)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span><span class="params">(self, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        self.val_f1s = []</span><br><span class="line">        self.val_recalls = []</span><br><span class="line">        self.val_precisions = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>early_stop，就是在训练模型的时候，当在验证集上效果不再提升的时候，就提前停止训练，节约时间。</p>
</blockquote>
<p>&lt;!--</p>
<h3>2.5 Class Weight (类别权重)</h3>
<p>一般而言，当数据集样本不均衡的时候，通过设置正负样本权重，可以提高一些效果，但是在这道题目里面，我对4个类别分别设置了class_weight 之后，我发现效果竟然变得更差了。</p>
<p>--&gt;</p>
<h3>2.6 Max Length (padding)</h3>
<blockquote>
<p>所有评论平均的长度是 200 左右，max_length 取 2 * 200，效果一直不给力.</p>
<p>将 max_length 改为 1200 ，macro f-score 效果明显提升</p>
<p>Tips： 多分类问题中，那些长度很长的评论可能会有部分属于那些样本数很少的类别，padding过短会导致这些长评论无法被正确划分。</p>
</blockquote>
<h2>3. ELMO-Like</h2>
<p>&lt;!--&lt;img src=&quot;/images/deeplearning/AI-Challenger-23.webp&quot; width=&quot;850&quot; alt=&quot;&quot;/&gt;
--&gt;</p>
<p>（腾讯词向量 16G， 800W * 200 = 5W * 200 + 自训词向 5W * 128 ） + BiGRU 中层语义 + BiGRU 高层语义</p>
<blockquote>
<p>10W+ 数据集，词频前5W的词, 每个评论一个 epoch 输入1次， 参数共享</p>
<p>328 + 256 + 256 近1000维度，Batch 128， 512维度的时候，Batch 256 可以放得下.</p>
<p>机器配置： 32G 内存， i9 CPU， 显卡型号 1080， 显存8G</p>
</blockquote>
<p><strong>多任务学习</strong></p>
<blockquote>
<ul>
<li><strong>分别训练20个分类模型的计算复杂度较高</strong></li>
<li><strong>20个分类模型占用存储空间</strong></li>
<li><strong>多任务学习可以通过特征共享降低过拟合风险</strong></li>
</ul>
</blockquote>
<p>epoch</p>
<blockquote>
<p>一次输入一个Batch=128条评论，20个属性都4分类成功， 1 个 epoch， 1200秒=20多分钟</p>
<p>每个评论一个 epoch 输入1次， 参数共享</p>
<p>maxLen 500 左右</p>
</blockquote>
<hr>
<blockquote>
<p>1.5W 跑一次测试集 1~2 分钟.
20W, 跑一次测试集 10多分钟 左右</p>
</blockquote>
<h2>4. Summary</h2>
<blockquote>
<ol>
<li>RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。</li>
<li>RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。</li>
<li>CNN 优点： 提取数据中的局部位置的特征，然后再拼接池化层。 CNN可以更好地捕捉文本的语义。是O(n)</li>
<li>CNN 优点： 一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。</li>
</ol>
</blockquote>
<hr>
<p>在文本分类任务中，有哪些对性能有重要影响的tricks？</p>
<blockquote>
<ol>
<li>数据预处理时vocab的选取（前N个高频词或者过滤掉出现次数小于3的词等等）</li>
<li>词向量的选择，可以使用预训练好的词向量如谷歌、facebook开源出来的，当训练集比较大的时候也可以进行微调或者随机初始化与训练同时进行。训练集较小时就别微调了</li>
<li>结合要使用的模型，这里可以把数据处理成char、word或者都用等</li>
<li>有时将词性标注信息也加入训练数据会收到比较好的效果</li>
<li>至于PAD的话，取均值或者一个稍微比较大的数（比较大的值，费点空间，谨慎使用）</li>
<li>神经网络结构的话到没有什么要说的，加上dropout和BN可能会更好。模型这块还是要具体问题具体分析吧.</li>
<li>文本领域用过数据增强的方法，就是对文本进行随机的shuffle和drop等操作来增加数据量</li>
</ol>
</blockquote>
<h2>Reference</h2>
<ul>
<li><a href="https://blog.csdn.net/liuchonge/article/details/77140719" target="_blank" rel="noopener">深度学习与文本分类总结第一篇--常用模型总结</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25928551" target="_blank" rel="noopener">用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践</a></li>
<li><a href="https://www.zhihu.com/question/59236897" target="_blank" rel="noopener">严重数据倾斜文本分类，比如正反比1:20～100，适合什么model</a></li>
<li><a href="https://github.com/xueyouluo/fsauor2018" target="_blank" rel="noopener">第16名解决方案</a>、 <a href="https://github.com/BigHeartC/Al_challenger_2018_sentiment_analysis" target="_blank" rel="noopener">第17名解决方案</a>、 <a href="https://github.com/brightmart/sentiment_analysis_fine_grain" target="_blank" rel="noopener">基于Bert的尝试</a></li>
</ul>
<p>&lt;!--</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/47207009" target="_blank" rel="noopener">AI-Challenger Baseline (0.70201) 前篇 总览</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/47207009" target="_blank" rel="noopener">AI-Challenger Baseline (0.70201) 后篇 训练</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/46999592" target="_blank" rel="noopener">2019 11家互联网公司，NLP面经回馈</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36387348" target="_blank" rel="noopener">暑期实习NLP算法岗面经总结</a></li>
<li><a href="https://www.zhihu.com/people/liu-he-he-44/posts" target="_blank" rel="noopener">呜呜哈做一个有思想的码农</a></li>
<li><a href="http://www.52nlp.cn/ai-challenger-2018-%E8%BF%9B%E8%A1%8C%E6%97%B6" target="_blank" rel="noopener">AI Challenger 2018 进行时</a></li>
<li><a href="http://www.52nlp.cn/ai-challenger-2018-%E7%BB%86%E7%B2%92%E5%BA%A6%E7%94%A8%E6%88%B7%E8%AF%84%E8%AE%BA%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90-fasttext-baseline" target="_blank" rel="noopener">AI Challenger 2018 细粒度用户评论情感分析 fastText Baseline</a></li>
</ul>
<p>--&gt;</p>
<ul>
<li><a href="http://www.52nlp.cn/ai-challenger-2018-%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E7%B1%BB%E7%AB%9E%E8%B5%9B%E7%9B%B8%E5%85%B3%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E5%8F%8A%E4%BB%A3%E7%A0%81%E6%B1%87%E6%80%BB" target="_blank" rel="noopener">ai-challenger-2018-文本挖掘类竞赛相关解决方案及代码汇总</a></li>
<li><a href="http://www.52nlp.cn/qa%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0" target="_blank" rel="noopener">QA问答系统中的深度学习技术实现</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/42517760" target="_blank" rel="noopener">深度学习代码复现之减少随机性的影响</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-datascience/credit-score" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/06/23/datascience/credit-score/"><strong>User Credit Score</strong></a>
      <small class=article-date-index>&nbsp; 2019-06-23</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/23/datascience/credit-score/" class="article-date">
  <time datetime="2019-06-23T05:28:21.000Z" itemprop="datePublished">2019-06-23</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/datascience/">datascience</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/23/datascience/credit-score/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;img src=&quot;/images/datascience/credit-score-02.jpg&quot; width=&quot;550&quot; alt=&quot;Credit Score Card&quot;/&gt;</p>
<p>&lt;!-- more --&gt;</p>
<p>信用评分卡模型是最常见的金融风控手段之一，它是指根据客户的各种属性和行为数据，利用一定的信用评分模型，对客户进行信用评分，据此决定是否给予授信以及授信的额度和利率，从而识别和减少在金融交易中存在的交易风险。</p>
<p>按照借贷用户的借贷时间，评分卡模型可以划分为以下三种：</p>
<ul>
<li>贷前： Application score card， 又称为A卡</li>
<li>贷中： Behavior score card， 又称为B卡</li>
<li>贷后： Collection score card， 又称为C卡</li>
</ul>
<p>申请评分卡要求最为严格，也最为重要，可解释性也要求最强，一般主流模型为 LR.</p>
<blockquote>
<p>如需了解更多关于 互联网金融风控 的相关背景知识，请参阅：</p>
<ul>
<li>
<p><a href="/2018/04/20/datascience/internet-finance-1/">Internet Financial Risk Control (part1) ： The Fraud Risk of Financial Technology Enterprises</a></p>
</li>
<li>
<p><a href="/2018/04/21/datascience/internet-finance-2/">Internet Financial Risk Control (part2) ： model strategy</a></p>
</li>
<li>
<p><a href="/2018/04/23/datascience/internet-finance-3/">Internet Financial Risk Control (part3) ： Lending Club Data dev</a></p>
</li>
</ul>
</blockquote>
<h2>1. 数据获取</h2>
<ul>
<li>金融机构自身字段： 年龄，户籍，性别，收入，...；</li>
<li>第三方机构的数据： 消费行为...</li>
</ul>
<blockquote>
<p><strong>数据情况：</strong></p>
<p>23W+ 去掉一些灰用户，剩余</p>
<p>M1标准： 好/坏: 13W+ 坏 4K</p>
<p>30~50 : 1 都是正常的</p>
</blockquote>
<h2>2. EDA</h2>
<p>Exploratory Data Analysis</p>
<blockquote>
<p>每个字段的缺失值情况、异常值情况、平均值、中位数、最大值、最小值、分布情况等</p>
</blockquote>
<ul>
<li><a href="https://www.zhihu.com/question/22320408/answer/141973314" target="_blank" rel="noopener">牛顿插值的几何解释是怎么样的？</a></li>
<li><a href="https://www.zhihu.com/question/58333118" target="_blank" rel="noopener">如何直观地理解拉格朗日插值法？</a></li>
<li><a href="https://www.matongxue.com/madocs/126.html" target="_blank" rel="noopener">从牛顿插值法到泰勒公式</a></li>
<li><a href="https://www.zhihu.com/question/21149770/answer/111173412" target="_blank" rel="noopener">泰勒公式一句话描述：就是用多项式函数去逼近光滑函数</a></li>
</ul>
<blockquote>
<p>缺失值的处理，还可以用 RF 去拟合.</p>
</blockquote>
<h2>3. 数据预处理</h2>
<p>(1). 数据清洗</p>
<p>(2). 变量分箱</p>
<p>(3). WOE 编码</p>
<blockquote>
<ol>
<li>缺失值太多</li>
<li>非数值变量多 (emp_title..)</li>
<li>id, member_id 等</li>
<li>loan_amnt != df.funded_amnt</li>
<li>空值填充为 0</li>
<li>带 % 的浮点，去掉 %</li>
</ol>
</blockquote>
<blockquote>
<p>好坏比是 34:1 是非常难以处理的样本了.</p>
</blockquote>
<h3>3.1 数据清洗</h3>
<p>缺失值太多</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 处理对象类型的缺失，unique</span></span><br><span class="line">df.select_dtypes(include=[<span class="string">'O'</span>]).describe().T.\</span><br><span class="line">assign(missing_pct=df.apply(<span class="keyword">lambda</span> x : (len(x)-x.count())/float(len(x))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 缺失值特别高的可以删除掉</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以对 非数值型 变量，做一个循环，发现 zip_code 有 873 个 unique 的值，那么先不处理</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.select_dtypes(include=[<span class="string">'object'</span>]).columns:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Column &#123;&#125; has &#123;&#125; unique instances"</span>.format( col, len(df[col].unique())) )</span><br></pre></td></tr></table></figure></p>
<h3>3.2 变量分箱</h3>
<ul>
<li>对连续变量进行分段离散化；</li>
<li>将多状态的离散变量进行合并，减少离散变量的状态数。</li>
</ul>
<p>先可以粗分箱，之后在合并。 比如 年龄取值数有 30个，那么分为 30 箱，最后合为 5 箱。
其他变量可能粗分箱为 100 箱，之后合并.</p>
<blockquote>
<p>分箱方法很多，最常见的方法之一： Merge分箱中的 Chimerge 分箱.</p>
<p>Chimerge 其基本思想是如果两个相邻的区间具有类似的类分布，则这两个区间合并；
否则，它们应保持分开。Chimerge通常采用<strong>卡方值</strong>来衡量两相邻区间的类分布情况。</p>
<ul>
<li>连续值按升序排列，离散值先转化为坏客户的比率，然后再按升序排列；</li>
<li>为了减少计算量，对于状态数大于某一阈值 (建议为100) 的变量，利用等频分箱进行粗分箱。</li>
<li>若有缺失值，则缺失值单独作为一个分箱。</li>
</ul>
</blockquote>
<p><strong>分箱的最大区间数 &amp; 分箱初始化</strong></p>
<blockquote>
<p>这里我取： n = 5</p>
</blockquote>
<blockquote>
<ol>
<li>连续值按升序排列，离散值先转化为坏客户的比率，然后再按升序排列；</li>
<li>为了减少计算量，对于状态数大于某一阈值 (建议为100) 的变量，利用等频分箱进行粗分箱。</li>
<li>若有缺失值，则缺失值单独作为一个分箱。</li>
</ol>
</blockquote>
<p><strong>合并区间：</strong></p>
<blockquote>
<ol>
<li>将卡方值最小的一对区间合并</li>
</ol>
</blockquote>
<p><strong>分箱后处理：</strong></p>
<blockquote>
<ol>
<li>对于坏客户比例为 0 或 1 的分箱进行合并 (一个分箱内不能全为好客户或者全为坏客户)。</li>
<li>对于分箱后某一箱样本占比超过 95% 的箱子进行删除。</li>
<li>检查缺失分箱的坏客户比例是否和非缺失分箱相等，如果相等，进行合并。</li>
</ol>
</blockquote>
<p><strong>总结一下特征分箱的优势</strong>：</p>
<blockquote>
<ol>
<li>特征分箱可以有效处理特征中的缺失值和异常值。</li>
<li>特征分箱后，数据和模型会更稳定。</li>
<li>特征分箱可以简化逻辑回归模型，降低模型过拟合的风险，提高模型的泛化能力。</li>
<li>将所有特征统一变换为类别型变量。</li>
<li>分箱后变量才可以使用标准的评分卡格式，即对不同的分段进行评分。</li>
</ol>
<p>列表内容特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</p>
</blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/52312186" target="_blank" rel="noopener">变量分箱实践</a></li>
<li><a href="http://www.jiehuozhe.com/article/3" target="_blank" rel="noopener">One-Hot编码与哑变量</a></li>
<li><a href="https://blog.csdn.net/zengxiantao1994/article/details/77855644" target="_blank" rel="noopener">方差、标准差和均方根误差的区别总结</a></li>
<li><a href="https://www.cnblogs.com/wzdLY/p/9649101.html" target="_blank" rel="noopener">基于卡方分箱的评分卡建模</a></li>
</ul>
<hr>
<p><a href="https://www.cnblogs.com/wzdLY/p/9649101.html" target="_blank" rel="noopener">good 基于卡方分箱的评分卡建模 , 卡方分布—chi-square distribution, χ2-distribution</a></p>
<p><strong>卡方值的意义,举个例子：</strong></p>
<p>某医院对某种病症的患者使用了 $A$，$B$ 两种不同的疗法，结果如表1，问两种疗法有无差别？</p>
<table>
<thead>
<tr>
<th style="text-align:center">组别</th>
<th style="text-align:center">有效</th>
<th style="text-align:center">无效</th>
<th style="text-align:center">合计</th>
<th style="text-align:center">有效率（%）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">A组</td>
<td style="text-align:center">19</td>
<td style="text-align:center">24</td>
<td style="text-align:center">43</td>
<td style="text-align:center">44.2</td>
</tr>
<tr>
<td style="text-align:center">B组</td>
<td style="text-align:center">34</td>
<td style="text-align:center">10</td>
<td style="text-align:center">44</td>
<td style="text-align:center">77.3</td>
</tr>
<tr>
<td style="text-align:center">合计</td>
<td style="text-align:center">53</td>
<td style="text-align:center">34</td>
<td style="text-align:center">87</td>
<td style="text-align:center">60.9</td>
</tr>
</tbody>
</table>
<p>可以计算出各格内的期望频数：</p>
<p>第1行1列： 43×53/87=26.2 ,</p>
<p>第1行2列： 43×34/87=16.8 ,</p>
<p>第2行1列： 44×53/87=26.8 ,</p>
<p>第2行2列： 44×34/87=17.2</p>
<p>A箱</p>
<p>(26.2-19)*(26.2-19) / 26.2  -&gt; <strong>(26.2 是 正类的期望频数， 19 是真实频数)</strong></p>
<p>(16.8-24)*(16.8-24) / 16.8 (16.8 是 负类的期望频数)</p>
<p>B箱</p>
<p>(26.8-34)*(26.8-34) / 26.8 (26.8 是 正类的期望频数)</p>
<p>(17.2-10)*(17.2-10) / 17.2 (17.2 是 负类的期望频数)</p>
<p>先建立原假设：A、B两种疗法没有区别。根据卡方值的计算公式，计算：卡方值=10.01。得到卡方值以后，接下来需要查询卡方分布表来判断p值，从而做出接受或拒绝原假设的决定。</p>
<p>&lt;details&gt;
&lt;summary&gt; 点击时的区域标题： 卡方分布—chi-square distribution, χ2-distribution &lt;/summary&gt;</p>
<p><strong>χ2-distribution:</strong></p>
<p>&lt;strong&gt;
若 $k$ 个独立的随机变量 $Z_1, Z_2, ..., Z_k$ 满足标准正态分布 $N(0,1)$ , 则这 $k$ 个随机变量的平方和：</p>
<p>&lt;img src=&quot;/images/datascience/credit-score-01.jpeg&quot; width=&quot;150&quot; alt=&quot;&quot;/&gt;</p>
<p>卡方检验—χ2检验是以χ2分布为基础的一种假设检验方法，主要用于分类变量之间的独立性检验</p>
<p>思想是根据样本数据推断 <strong><code>总体分布与期望分布 是否有显著性差异</code></strong>，或者推断两个分类变量是否相关或者独立。</p>
<p>一般可以设原假设为 ：观察频数与期望频数没有差异，或者两个变量相互独立不相关。</p>
<p>实际应用中，我们先假设原假设成立，计算出卡方值，卡方表示观察值与理论值间的偏离程度。</p>
<p>&lt;img src=&quot;/images/datascience/credit-score-03.png&quot; width=&quot;500&quot; alt=&quot;&quot;/&gt;</p>
<p>卡方值只是一个中间过程，通过卡方值计算出p值，p值才是我们最重要需要的。p小于0.05意味着存在显著差异。</p>
<p>卡方值是非参数检验中的一个统计量，主要用于非参数统计分析中。它的作用是检验数据的相关性。如果卡方值的显著性（即SIG.）小于0.05，说明两个变量是显著相关的。
&lt;/strong&gt;
&lt;/details&gt;</p>
<hr>
<h3>3.3 WOE (weight of evidence)</h3>
<blockquote>
<p>WOE 称为证据权重(weight of evidence) , 将离散变量转化为连续变量。</p>
</blockquote>
<p>WOE 将预测类别的集中度的属性作为编码的数值。对于自变量第 $i$ 箱的WOE值为：</p>
<p>&lt;img src=&quot;/images/datascience/credit-score-04.svg&quot; width=&quot;600&quot; alt=&quot;&quot;/&gt;</p>
<blockquote>
<p>$p_{i1}$ 是第 $i$ 箱中坏客户占所有坏客户比例</p>
<p>$p_{i0}$ 是第 $i$ 箱中好客户占所有好客户比例</p>
<p><strong><code>WOE: 当前分箱中坏客户和好客户的比值，和所有样本中这个比值的差异</code></strong></p>
</blockquote>
<blockquote>
<p>当分箱中坏客户和好客户的比例等于随机坏客户和好客户的比值时，说明这个分箱没有预测能力，即WOE=0。 (WOE为0，说明该箱出的特征对结果没有区分度)</p>
</blockquote>
<p>实际上WOE编码相当于把分箱后的特征从非线性可分映射到近似线性可分的空间内:</p>
<p>&lt;img src=&quot;/images/datascience/credit-score-05.svg&quot; width=&quot;600&quot; alt=&quot;&quot;/&gt;</p>
<p>总结一下WOE编码的优势：</p>
<blockquote>
<ol>
<li>可提升模型的预测效果</li>
<li>将自变量规范到同一尺度上</li>
<li>WOE能反映自变量取值的贡献情况</li>
<li>有利于对变量的每个分箱进行评分</li>
<li>转化为连续变量之后，便于分析变量与变量之间的相关性</li>
<li>与独热向量编码相比，可以保证变量的完整性，同时避免稀疏矩阵和维度灾难</li>
</ol>
</blockquote>
<hr>
<h2>4. 变量筛选</h2>
<p>主要衡量标准: 变量的预测能力和变量的线性相关性。</p>
<p>挑选入模变量:</p>
<blockquote>
<ol>
<li>变量的预测能力</li>
<li>变量之间的线性相关性</li>
<li>变量在业务上的可解释性</li>
</ol>
</blockquote>
<p>变量两两相关性分析，变量的多重共线性分析。</p>
<h3>4.1 单变量筛选</h3>
<p>IV 信息价值(information value)，自变量的IV值越大，表示自变量的预测能力越强。 类似指标还有信息增益等。</p>
<p>&lt;img src=&quot;/images/datascience/credit-score-06.jpg&quot; width=&quot;850&quot; alt=&quot;&quot;/&gt;</p>
<p><img src="https://www.zhihu.com/equation?tex=IV_i+%3D%28%5Cfrac%7B%5C%23B_i%7D%7B%5C%23B_T%7D-%5Cfrac%7B%5C%23G_i%7D%7B%5C%23G_T%7D%29+%2A+log+%28%5Cfrac%7B%5C%23B_i%2F%5C%23B_T%7D%7B%5C%23G_i%2F%5C%23G_T%7D%29%3D%28%5Cfrac%7B%5C%23B_i%7D%7B%5C%23B_T%7D-%5Cfrac%7B%5C%23G_i%7D%7B%5C%23G_T%7D%29+%2A+WOE_i+%5C%5C" alt=""></p>
<p>变量对应的IV值为所有分箱对应的 IV 值之和：</p>
<p><img src="https://www.zhihu.com/equation?tex=IV+%3D+%5Csum%5Climits_i%5En+IV_i+%5C%5C" alt=""></p>
<p>IV 值实际上式变量各个分箱的加权求和。且和决策树中的交叉熵有异曲同工之妙。以下为交叉熵公式：</p>
<p>&lt;img src=&quot;/images/datascience/credit-score-07.svg&quot; width=&quot;600&quot; alt=&quot;&quot;/&gt;</p>
<p>IV值的具体的计算流程如下：</p>
<p>&lt;img src=&quot;/images/datascience/credit-score-08.jpg&quot; width=&quot;800&quot; alt=&quot;&quot;/&gt;</p>
<p>IV排序后，选择IV&gt;0.02的变量，共58个变量IV&gt;0.02</p>
<h3>4.2 多变量分析</h3>
<p>保留相关性低于阈值0.6的变量，剩余27个变量</p>
<p><strong>为什么要进行相关性分析？</strong></p>
<blockquote>
<p>理想状态下，系数权重会有无数种取法，使系数权重变得无法解释，导致变量的每个分段的得分也有无数种取法（后面我们会发现变量中不同分段的评分会用到变量的系数）</p>
</blockquote>
<p><strong>总结一下变量筛选的意义：</strong></p>
<ol>
<li>剔除跟目标变量不太相关的特征</li>
<li>消除由于线性相关的变量，避免特征冗余</li>
<li>减轻后期验证、部署、监控的负担</li>
<li>保证变量的可解释性</li>
</ol>
<p>特征相关度筛选</p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">cor = df.corr()</span><br><span class="line">cor.loc[:,:] = np.tril(cor, k=<span class="number">-1</span>) <span class="comment"># below main lower triangle of an array</span></span><br><span class="line">cor = cor.stack()</span><br><span class="line">cor[(cor &gt; <span class="number">0.55</span>) | (cor &lt; <span class="number">-0.55</span>)] <span class="comment"># 特征相关度筛选</span></span><br></pre></td></tr></table></figure></p>
<h3>4.3 显著性分析</h3>
<p>删除P值不显著的变量，剩余12个变量了。</p>
<p><strong>GBDT, GBRT, Xgboost, RF grid search</strong></p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">'learning_rate'</span>: [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.02</span>, <span class="number">0.01</span>],</span><br><span class="line">    <span class="string">'max_depth'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="string">'min_samples_split'</span>: [<span class="number">50</span>,<span class="number">100</span>,<span class="number">200</span>,<span class="number">400</span>],</span><br><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">100</span>,<span class="number">200</span>,<span class="number">400</span>,<span class="number">800</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>特征重要度：</strong></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Top Ten， GBRT top 10 的参数有哪些，可以作为参考， GBRT 不仅是一个分类器，还能帮你筛选变量</span><br><span class="line"># 比如 feature_importance = 0 的话，那么下一次 这个特征，你就去掉就可以了</span><br><span class="line"># 重新用模型GBRT重新跑，或者用其他模型LR跑</span><br><span class="line"># 或者其他的 RF 也可以帮助你做特征筛选</span><br></pre></td></tr></table></figure></p>
<p>最后选择出 84 个变量， 然后在放到不同的模型中做训练，在 ensemble 应该效果还是不错的。</p>
<p><strong>特征不稳定的，不可以作为入模变量：</strong></p>
<blockquote>
<p>挑选变量的时候，开始每个月我一直在看它的均值和方差的变化是否在容忍的范围内，超过50%舍</p>
</blockquote>
<h2>5. 转化为评分卡</h2>
<p>将 odds 带入可得：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7Blog%7D%28+%5Ctext%7Bodds%7D%29+%3D+%5Ctheta%5ETx+%5C%5C" alt=""></p>
<p>评分卡的分值可以定义为比率对数的线性表达来，即：</p>
<p><img src="https://www.zhihu.com/equation?tex=Score+%3D+A+-B+%5Ctimes+%5Ctext%7Blog%7D%28+%5Ctext%7Bodds%7D%29+%5C%5C" alt=""></p>
<p>最终得到评分卡模型：</p>
<p><img src="https://pic2.zhimg.com/80/v2-fec98ff9de65d835a5be217f01f678a5_hd.jpg" alt=""></p>
<p>需要设定两个假设：</p>
<ul>
<li>某个特定的违约概率下的预期评分，即比率 即比率 $\text{odds}$ 为 $θ_0$ 时的分数为 $P_0$</li>
<li>该odds为$2θ_0$情况下评分的减少量（PDO）</li>
</ul>
<h2>6. KS 评估</h2>
<p>K-S曲线其实数据来源和本质和ROC曲线是一致的，只是ROC曲线是把真正率和假正率当作横纵轴，而K-S曲线是把真正率和假正率都当作是纵轴，横轴则由选定的阈值来充当。</p>
<blockquote>
<p>由于ks值能找出模型中差异最大的一个分段，因此适合用于cut_off，像评分卡这种就很适合用ks值来评估。但是ks值只能反映出哪个分段是区分最大的，而不能总体反映出所有分段的效果，因果AUC值更能胜任。</p>
<p>KS值表示了模型将+和-区分开来的能力。值越大，模型的预测准确性越好。</p>
</blockquote>
<p>KS 值表示了模型区分好坏客户的能力。</p>
<p>等分 10 份，两条洛伦兹曲线， TPR 与 FPR 的差值. 好坏客户的区程度.</p>
<p>其实质是 $TPR - FPR$ 随好坏客户阈值变化的最大值。KS 值越大，模型的预测准确性越好。一般，KS &gt; 0.4 即认为模型有很好的预测性能。</p>
<h2>Reference</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/34370741" target="_blank" rel="noopener">Python三大评分卡之行为评分卡</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36539125" target="_blank" rel="noopener">玩转逻辑回归之金融评分卡模型</a></li>
<li><a href="http://www.sfinst.com/?p=1389" target="_blank" rel="noopener">拍拍贷教你如何用GBDT做评分卡</a></li>
<li><a href="https://blog.csdn.net/zs15321583801/article/details/88217839" target="_blank" rel="noopener">模型区分度指标 KS 值</a></li>
<li><a href="https://blog.csdn.net/Orange_Spotty_Cat/article/details/82425113" target="_blank" rel="noopener">分类模型评判指标 ROC，AUC，GINI，KS</a></li>
<li><a href="https://www.cnblogs.com/gczr/p/10354646.html" target="_blank" rel="noopener">深入理解KS</a></li>
<li><a href="http://rosen.xyz/2018/02/01/AUC%E5%92%8CKS%E6%8C%87%E6%A0%87" target="_blank" rel="noopener">AUC和KS指标</a></li>
<li><a href="https://blog.csdn.net/sscc_learning/article/details/86707005" target="_blank" rel="noopener">一文完全理解模型ks指标含义并画出ks曲线</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-nlp/BERT-Simple" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/06/21/nlp/BERT-Simple/"><strong>Bert 最简单的打开姿势</strong></a>
      <small class=article-date-index>&nbsp; 2019-06-21</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/21/nlp/BERT-Simple/" class="article-date">
  <time datetime="2019-06-21T03:00:21.000Z" itemprop="datePublished">2019-06-21</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/nlp/">nlp</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/21/nlp/BERT-Simple/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>2018.10 google 发布 BERT 模型. 引爆整个AI圈的 NLP 模型. 在 NLP领域 刷新 11 项记录.</p>
<p>BERT 创新点在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的 NLP 任务.</p>
<p>&lt;!-- more --&gt;</p>
<p>&lt;img src=&quot;/images/nlp/bert-sample-1.png&quot; width=&quot;700&quot; alt=&quot;Bert的预训练和微调（图片来自Bert的原论文）&quot; /&gt;</p>
<h2>当Bert遇上Keras</h2>
<p>在Keras下对Bert最好的封装是：</p>
<p>keras-bert：https://github.com/CyberZHG/keras-bert</p>
<p>这里简单解释一下Tokenizer的输出结果。首先，默认情况下，分词后句子首位会分别加上[CLS]和[SEP]标记，其中[CLS]位置对应的输出向量是能代表整句的句向量（反正Bert是这样设计的），而[SEP]则是句间的分隔符，其余部分则是单字输出（对于中文来说）</p>
<h2>Reference</h2>
<ul>
<li><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li><a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-chatbot/chatbot-project" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/06/20/chatbot/chatbot-project/"><strong>Chatbot Project</strong></a>
      <small class=article-date-index>&nbsp; 2019-06-20</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/20/chatbot/chatbot-project/" class="article-date">
  <time datetime="2019-06-20T11:16:21.000Z" itemprop="datePublished">2019-06-20</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/chatbot/">chatbot</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/20/chatbot/chatbot-project/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;img src=&quot;/images/chatbot/chatbot-logo-03.png&quot; width=&quot;550&quot; alt=&quot;chatbot&quot;/&gt;</p>
<p>&lt;!-- more --&gt;</p>
<p>����������������聊天机器人（chatbot），也被称为会话代理或对话系统，现已成为了一个热门话题。微软在聊天机器人上押上了重注，Facebook（M）、苹果（Siri）、谷歌 和 Slack 等公司也是如此。 新一波创业者们正在尝试改变消费者与服务的交互方式。</p>
<ul>
<li>
<p><a href="/2017/08/11/chatbot/chatbot-research1/">Chatbot Research 1 - 聊天机器人的行业综述</a></p>
</li>
<li>
<p><a href="/2017/08/12/chatbot/chatbot-research2/">Chatbot Research 2 - NLP 的基础知识回顾</a></p>
</li>
<li>
<p><a href="/2017/08/13/chatbot/chatbot-research3/">Chatbot Research 3 - 机器学习构建 chatbot</a></p>
</li>
<li>
<p><a href="/2017/08/14/chatbot/chatbot-research4/">Chatbot Research 4 - 深度学习知识回顾</a></p>
</li>
<li>
<p><a href="/2017/08/15/chatbot/chatbot-research5/">Chatbot Research 5 - 基于深度学习的检索聊天机器人</a></p>
</li>
<li>
<p><a href="/2017/08/16/chatbot/chatbot-research6/">Chatbot Research 6 - 更多论文 (感谢 PaperWeekly)</a></p>
</li>
<li>
<p><a href="/2017/09/26/chatbot/chatbot-research7/">Chatbot Research 7 - Dialog_Corpus 常用数据集</a></p>
</li>
<li>
<p><a href="/2017/11/17/chatbot/chatbot-research8/">Chatbot Research 8 - 理论 seq2seq+Attention 机制模型详解</a></p>
</li>
<li>
<p><a href="/2017/11/29/chatbot/chatbot-research11/">Chatbot Research 11 - 第二个版本 (新版实现)</a></p>
</li>
<li>
<p><a href="/2018/12/01/chatbot/chatbot-research12/">Chatbot Research 12 - 理论篇： 评价指标介绍</a></p>
</li>
<li>
<p><a href="/2018/12/05/chatbot/chatbot-research13/">Chatbot Research 13 - 理论篇： MMI 模型理论</a></p>
</li>
<li>
<p><a href="/2018/12/01/chatbot/chatbot-common-links/">Chatbot Useful Links</a></p>
</li>
</ul>
<p><strong>预备知识</strong></p>
<ul>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.1 词嵌入（word2vec）</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.2 近似训练</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.3 Word2vec 的实现</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.4 子词嵌入（fastText）</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.5 全局向量的词嵌入（GloVe）</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.6 求近义词和类比词</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.7 文本情感分类：使用 RNN</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.8 文本情感分类：使用 CNN（textCNN）</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.9 编码器—解码器（seq2seq）</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.10 束搜索 beam-search</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.11 Attention机制</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/55600911" target="_blank" rel="noopener">1.12 tensorflow模型线上部署</a></p>
</li>
</ul>
<h2>2. TensorFlow</h2>
<p>TensorFlow 用于机器学习和神经网络方面的研究，采用<strong>数据流图</strong>来进行数值计算的开源软件库.</p>
<p><a href="https://keras.io/zh/models/about-keras-models/" target="_blank" rel="noopener">Keras</a> 开发重点是支持快速的实验。能够以最小的时延把你的想法转换为实验结果，是做好研究的关键。</p>
<h3>2.1 TensorFlow 简介</h3>
<ul>
<li>
<p><a href="/2017/08/22/tensorflow/tf-1.1-why/">1.1 TensorFlow Why ?</a></p>
</li>
<li>
<p><a href="/2017/10/23/tensorflow/tf-doc/">1.2 TensorFlow 快速学习 &amp; 文档</a></p>
</li>
</ul>
<h3>2.2 Tensorflow 基础构架</h3>
<ul>
<li>
<p><a href="/2017/08/25/tensorflow/tf-2.1-structure/">2.1 处理结构: 计算图</a></p>
</li>
<li>
<p><a href="/2017/08/27/tensorflow/tf-2.2-example/">2.2 完整步骤 例子2 🌰（创建数据、搭建模型、计算误差、传播误差、训练）</a></p>
</li>
<li>
<p><a href="/2017/08/28/tensorflow/tf-2.3-session/">2.3 Session 会话控制</a></p>
</li>
<li>
<p><a href="/2017/08/29/tensorflow/tf-2.4-variable/">2.4 Variable 变量</a></p>
</li>
<li>
<p><a href="/2017/08/30/tensorflow/tf-2.5-placeholde/">2.5 Placeholder 传入值</a></p>
</li>
<li>
<p><a href="/2017/09/07/tensorflow/tf-2.6-A-activation-function/">2.6 什么是激励函数 (Activation Function)</a></p>
</li>
<li>
<p><a href="/2017/09/07/tensorflow/tf-2.6-B-activation-function/">2.7 激励函数 Activation Function</a></p>
</li>
<li>
<p><a href="/2017/09/08/tensorflow/tf-2.8-tensorflow-basic-summary/">2.8 TensorFlow 基本用法总结 🌰🌰🌰</a></p>
</li>
</ul>
<h3>2.3 建造我们第一个神经网络</h3>
<ul>
<li>
<p><a href="/2017/09/09/tensorflow/tf-3.1-add-layer/">3.1 添加层 def add_layer()</a></p>
</li>
<li>
<p><a href="/2017/09/11/tensorflow/tf-3.2-create-NN/">3.2 建造神经网络 🌰🌰🌰</a></p>
</li>
<li>
<p><a href="/2017/09/12/tensorflow/tf-3.3-A-speed-up-learning/">3.3 Speed Up Training &amp; Optimizer (转载自莫烦)</a></p>
</li>
</ul>
<h3>2.4 Tensorboard</h3>
<ul>
<li><a href="/2017/09/12/tensorflow/tf-4.1-tensorboard1/">4.1 Tensorboard 可视化好帮手 1</a></li>
</ul>
<h3>2.5 Estimator</h3>
<ul>
<li>
<p><a href="/2018/10/31/tensorflow/tf-5.1-contrib-learn-Quickstart/">5.1 tf.contrib.learn 快速入门</a></p>
</li>
<li>
<p><a href="/2018/11/01/tensorflow/tf-5.2-contrib-learn-Input-fn/">5.2 tf.contrib.learn 构建输入函数</a></p>
</li>
<li>
<p><a href="/2018/11/04/tensorflow/tf-5.3-contrib-learn-MonitorAPI/">5.3 tf.contrib.learn 基础的记录和监控教程</a></p>
</li>
<li>
<p><a href="/2018/11/04/tensorflow/tf-5.4-contrib-learn-Estimator/">5.4 tf.contrib.learn 创建 Estimator</a></p>
</li>
<li>
<p><a href="https://www.jianshu.com/p/8850127ed25d" target="_blank" rel="noopener">5.5 TF 保存和加载模型 - 简书</a></p>
</li>
</ul>
<h3>2.6 Language model 介绍</h3>
<p>语言模型是自然语言处理问题中一类最基本的问题，它有着非常广泛的应用。</p>
<ul>
<li>
<p><a href="/2018/11/08/tensorflow/tf-google-8-rnn-1/">1.1 RNN 循环神经网络 简介</a></p>
</li>
<li>
<p><a href="/2018/11/10/tensorflow/tf-google-8-rnn-2/">1.2 LSTM &amp; Bi-RNN &amp; Deep RNN</a></p>
</li>
<li>
<p><a href="/2018/12/06/chatbot/chatbot-index/#1-Chatbot">1.3 Language model 介绍 / 评价方法 perplexity (not finish)</a></p>
</li>
</ul>
<h3>2.7 NNLM (神经语言模型)</h3>
<ul>
<li>
<p><a href="/2018/10/01/tensorflow/tf-nlp-9.2.2/">2.2 PTB 数据的 batching 方法</a></p>
</li>
<li>
<p><a href="/2018/10/02/tensorflow/tf-nlp-9.2.3/">2.3 RNN 的语言模型 TensorFlow 实现</a></p>
</li>
</ul>
<h3>2.8 MNIST 数字识别问题</h3>
<ul>
<li>
<p><a href="/2018/10/04/tensorflow/tf-mnist-1-beginners/">3.1 简单前馈网络实现 mnist 分类</a></p>
</li>
<li>
<p><a href="/2017/10/05/tensorflow/tf-4.3-name-variable_scope/">3.3 name / variable_scope</a></p>
</li>
<li>
<p><a href="/2017/10/07/tensorflow/tf-simple-lstms/">3.4 多层 LSTM 通俗易懂版</a></p>
</li>
</ul>
<h2>4. Python</h2>
<p>Python 哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码.</p>
<p>Python 数据分析模块: Numpy &amp; Pandas, 及强大的画图工具 Matplotlib</p>
<ul>
<li>
<p><a href="/python_language">Python</a></p>
</li>
<li>
<p><a href="/python_numpy_pandas">Numpy &amp; Pandas</a></p>
</li>
<li>
<p><a href="/python_matplotlib">Matplotlib</a></p>
</li>
</ul>
<h2>5. Scikit-Learn</h2>
<p>Sklearn 机器学习领域当中最知名的 Python 模块之一 <a href="/2018/01/03/python/py-sklearn-0-why/">why</a></p>
<ul>
<li>
<p><a href="/2018/01/03/python/py-sklearn-1-choosing-estimator/">1.1 : Sklearn Choosing The Right Estimator</a></p>
</li>
<li>
<p><a href="/2018/01/05/python/py-sklearn-2-general-learning-model/">1.2 : Sklearn General Learning Model</a></p>
</li>
<li>
<p><a href="/2018/01/03/python/py-sklearn-3-database/">1.3 : Sklearn DataSets</a></p>
</li>
<li>
<p><a href="/2018/01/05/python/py-sklearn-4-common-attributes/">1.4 : Sklearn Common Attributes and Functions</a></p>
</li>
<li>
<p><a href="/2018/01/06/python/py-sklearn-5-normalization/">1.5 : Normalization</a></p>
</li>
<li>
<p><a href="/2018/01/08/python/py-sklearn-6-cross-validation-1/">1.6 : Cross-validation 1</a></p>
</li>
<li>
<p><a href="/2018/01/09/python/py-sklearn-6-cross-validation-2/">1.7 : Cross-validation 2</a></p>
</li>
<li>
<p><a href="/2018/01/09/python/py-sklearn-6-cross-validation-3/">1.8 : Cross-validation 3</a></p>
</li>
<li>
<p><a href="/2018/01/10/python/py-sklearn-7-save-model/">1.9 : Sklearn Save Model</a></p>
</li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-nlp/BERT" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/06/20/nlp/BERT/"><strong>BERT 完全指南</strong></a>
      <small class=article-date-index>&nbsp; 2019-06-20</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/20/nlp/BERT/" class="article-date">
  <time datetime="2019-06-20T03:00:21.000Z" itemprop="datePublished">2019-06-20</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/nlp/">nlp</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/20/nlp/BERT/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;a href=&quot;/2019/06/20/nlp/BERT/&quot; target=&quot;_self&quot;&gt;&lt;img src=&quot;/images/nlp/Bert-Ernie-logo.jpg&quot; width=&quot;550&quot; alt=&quot;Bert-Ernie&quot; /&gt;
&lt;/a&gt;</p>
<p>&lt;!--&lt;a href=&quot;/2019/06/30/nlp/BERT/&quot; target=&quot;_self&quot; style=&quot;display:block; margin:0 auto; background:url('/images/nlp/Bert-Ernie-logo.jpg') no-repeat 0 0 / contain; height:323px; width:550px;&quot;&gt;&lt;/a&gt;--&gt;</p>
<p>&lt;!-- more --&gt;</p>
<p><strong>2018.10</strong> google 发布 <strong>BERT</strong> 模型. 引爆整个AI圈的 NLP 模型. 在 NLP领域 刷新 11 项记录.</p>
<p><strong>BERT</strong> 其实是 language_encoder，把输入的 sentence 或 paragraph 转成 feature_vector（embedding）.</p>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
<p><strong>BERT</strong> 创新点在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的 NLP 任务.</p>
<p>&lt;!-- more --&gt;</p>
<hr>
<p>&lt;!--<img src="/images/nlp/language-mode-l.jpg" alt="">
--&gt;</p>
<h2>1. NLP 的发展</h2>
<p><a href="https://www.infoq.cn/article/66vicQt*GTIFy33B4mu9" target="_blank" rel="noopener">NLP 神经网络发展历史中最重要的 8 个里程碑</a></p>
<blockquote>
<ol>
<li>
<p>Language Model (语言模型就是要看到上文预测下文, So NNLM)</p>
</li>
<li>
<p>n-gram model（n元模型）（基于 马尔可夫假设 思想）<strong>上下文相关的特性 建立数学模型</strong>。</p>
</li>
<li>
<p>2001 - <strong>NNLM</strong> , @Bengio , 火于 2013 年， 沉寂十年终时来运转。 但很快又被NLP工作者祭入神殿。</p>
</li>
<li>
<p>2008 - Multi-task learning</p>
</li>
<li>
<p>2013 - Word2Vec (Word Embedding的工具word2vec : CBOW 和 Skip-gram)</p>
</li>
<li>
<p>2014 - sequence-to-sequence</p>
</li>
<li>
<p>2015 - Attention</p>
</li>
<li>
<p>2015 - Memory-based networks</p>
</li>
<li>
<p>2018 - Pretrained language models</p>
</li>
</ol>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">good 张俊林: 深度学习中的注意力模型（2017版）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">good 张俊林: 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></p>
<p><strong>NNLM vs Word2Vec</strong></p>
<blockquote>
<ol>
<li>NNLM 目标： 训练语言模型， 语言模型就是要看上文预测下文， word embedding 只是无心的一个副产品。</li>
<li>Word2Vec目标： 它单纯就是要 word embedding 的，这是主产品。</li>
</ol>
<p>2018 年之前的 Word Embedding 有个缺点就是无法处理 <strong>多义词</strong> 的问题, 静态词嵌.</p>
</blockquote>
<p><strong>ELMO: Embedding from Language Models</strong></p>
<blockquote>
<p>ELMO的论文题目：“Deep contextualized word representation”</p>
<p>NAACL 2018 最佳论文 - ELMO： Deep contextualized word representation</p>
<p>ELMO 本身是个根据当前上下文对Word Embedding动态调整的思路。</p>
<p><strong>ELMO 有什么缺点？</strong></p>
<ol>
<li>LSTM 抽取特征能力远弱于 Transformer</li>
<li>拼接方式双向融合特征能力偏弱</li>
</ol>
</blockquote>
<p>**GPT (Generative Pre-Training) **</p>
<blockquote>
<ol>
<li>第一个阶段是利用 language 进行 Pre-Training.</li>
<li>第二阶段通过 Fine-tuning 的模式解决下游任务。</li>
</ol>
<p><strong>GPT: 有什么缺点？</strong></p>
<ol>
<li>要是把 language model 改造成双向就好了</li>
<li>不太会炒作，GPT 也是非常重要的工作.</li>
</ol>
</blockquote>
<p><strong>Bert 亮点 : 效果好 和 普适性强</strong></p>
<blockquote>
<ol>
<li>Transformer 特征抽取器</li>
<li>Language Model 作为训练任务 (双向)</li>
</ol>
<p>Bert 采用和 GPT 完全相同的 <strong>两阶段</strong> 模型：</p>
<ol>
<li>Pre-Train Language Model；</li>
<li>Fine-&gt; Tuning模式解决下游任务。</li>
</ol>
</blockquote>
<p><strong>NLP 的 4大任务</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">4 NLP task</th>
<th style="text-align:center">description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">序列标注</td>
<td style="text-align:center">特点是句子中<strong>每个单词</strong>要求模型根据上下文都要给出一个 分类<strong>label</strong>；</td>
</tr>
<tr>
<td style="text-align:center">分类任务</td>
<td style="text-align:center">特点是不管文章有多长，总体给出一个分类<strong>label</strong> 即可；</td>
</tr>
<tr>
<td style="text-align:center">句子关系判断</td>
<td style="text-align:center">特点是给定两个句子，模型<strong>判断出两个句子</strong> 是否具备某种语义关系；</td>
</tr>
<tr>
<td style="text-align:center">生成式任务</td>
<td style="text-align:center">特点是输入文本内容后，需要自主生成另外一段文字。</td>
</tr>
</tbody>
</table>
<hr>
<p><img src="https://pic3.zhimg.com/80/v2-0245d07d9e227d1cb1091d96bf499032_hd.jpg" alt=""></p>
<p>&lt;!--</p>
<h2>2. Word Representation</h2>
<p>要处理 NLP 问题，首先要解决 <strong>Word Representation 文本表示</strong> 问题。虽然我们人去看文本，能够清楚明白文本中的符号表达什么含义，但是计算机只能做数学计算，需要将文本表示成计算机可以处理的形式。</p>
<p><img src="https://pic3.zhimg.com/80/v2-597b011ddd148eb53b5a90730b6090ae_hd.jpg" alt=""></p>
<p>后来出现了词向量，word embedding，用一个低维稠密的向量去表示一个词。通常这个向量的维度在几百到上千之间，词向量可以通过一些无监督的方法学习得到，比如 CBOW 或 Skip-Gram 等.</p>
<blockquote>
<p>更多 Word Embeddings 请参见： <a href="/2018/08/02/deeplearning/Sequence-Models-week2/">Sequence Models (week2) - NLP - Word Embeddings </a></p>
<p>在图像中就不存在表示方法的困扰，因为图像本身就是数值矩阵，计算机可以直接处理。</p>
</blockquote>
<p>NLP 领域经常引入一种做法，在非常大的语料库上进行 pre-training，然后在特定任务上进行 fine-tuning.
BERT 就是用了一个已有的模型结构，提出了一整套的 pre-training 方法和 fine-tuning 方法.--&gt;</p>
<p><a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">good 张俊林: 放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></p>
<h2>2. Feature Extraction</h2>
<blockquote>
<ol>
<li>RNN 人老珠黄，已经基本完成它的历史使命，将来会逐步退出历史舞台；</li>
<li>CNN 如果改造成功并超出期望，那么还有一丝可能继续生存壮大；</li>
<li>Transformer 明显会很快成为 NLP里 担当大任的最主流的特征抽取器。</li>
</ol>
<p>特征抽取器能否具备长距离特征捕获能力这一点对于解决NLP任务来说也是很关键的。</p>
<p>一个特征抽取器是否适配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使得它更匹配领域问题的特性。</p>
</blockquote>
<p>Three Feature Extraction:</p>
<blockquote>
<p>“Transformer考上了北京大学；CNN进了中等技术学校，希望有一天能够考研考进北京大学；RNN在百货公司当售货员：我们都有看似光明的前途。”</p>
</blockquote>
<blockquote>
<p><strong>1. 进退维谷的 RNN</strong></p>
<ol>
<li>RNN (包括LSTM、GRU + Attention) 效果与 Transformer 差距很明显</li>
<li>RNN 很难并行计算。 由于 RNN 特点 ： 线形序列收集前面的信息。</li>
</ol>
<p>一个严重阻碍RNN将来继续走红的问题是：RNN本身的序列依赖结构对于大规模并行计算来说相当之不友好。通俗点说，就是RNN很难具备高效的并行计算能力，这个乍一看好像不是太大的问题，其实问题很严重。</p>
<p>对于小数据集 RNN 可能速度更快些， Transformer 慢些， 但是可以改进 Transformer 缓解：</p>
<ol>
<li>可把Block数目降低，减少参数量；</li>
<li>引入Bert两阶段训练模型，那么对于小数据集合来说会极大缓解效果问题。</li>
</ol>
<p><strong>2. 一希尚存的 CNN</strong></p>
<ol>
<li>CNN 天生自带的高并行计算能力</li>
<li>一些深度网络的优化trick，CNN在NLP领域里的深度逐步能做起来了。dilated CNN</li>
</ol>
<p>早期CNN做不好NLP的一个很大原因是网络深度做不起来。 原生的CNN在很多方面仍然是比不过Transformer的，典型的还是长距离特征捕获能力方面，而这点在NLP界算是比较严重的缺陷。</p>
<p>对于远距离特征，单层怀旧版CNN是无法捕获到的，如果滑动窗口k最大为2，而如果有个远距离特征距离是5，那么无论上多少个卷积核，都无法覆盖到长度为5的距离的输入，所以它是无法捕获长距离特征的</p>
<p>滑动窗口从左到右滑动，捕获到的特征也是如此顺序排列，所以它在结构上已经记录了相对位置信息了。但是如果卷积层后面立即接上Pooling层的话，Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。所以在NLP领域里，目前CNN的一个发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度。</p>
<p>怀旧版 CNN模型 一直处于被 RNN模型 压制到抑郁症早期的尴尬局面。</p>
<p><strong>CNN的进化</strong>：物竞天择的模型斗兽场</p>
<p>摩登CNN（使用Skip Connection来辅助优化）、Dilated CNN</p>
<p>想方设法把CNN的深度做起来，随着深度的增加，很多看似无关的问题就随之解决了。</p>
<p><strong>3. Transformer</strong></p>
<p>Transformer作为新模型，并不是完美无缺的。它也有明显的缺点：首先，对于长输入的任务，典型的比如篇章级别的任务（例如文本摘要），因为任务的输入太长，Transformer会有巨大的计算复杂度，导致速度会急剧变慢。</p>
<p>做语义特征抽取能力比较时，结论是对于距离远与13的长距离特征，Transformer性能弱于RNN，比较出乎意料，因为Transformer通过Self attention使得远距离特征直接发生关系，按理说距离不应该成为它的问题，但是效果竟然不如RNN，这背后的原因是什么呢？这也是很有价值的一个探索点。</p>
</blockquote>
<p><strong>华山论剑：三大特征抽取器比较</strong></p>
<blockquote>
<p>总而言之，关于三者速度对比方面，目前的主流经验结论基本如上所述：Transformer Base最快，CNN次之，再次Transformer Big，最慢的是RNN。RNN比前两者慢了3倍到几十倍之间。</p>
</blockquote>
<blockquote>
<p>单从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显。这两者再综合起来，如果我给的排序结果是Transformer&gt;CNN&gt;RNN</p>
<p>从速度和效果折衷的角度看，对于工业界应用，特征抽取选择方面配置<strong>Transformer base</strong>是个较好的选择。</p>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">good 张俊林: 放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></p>
<p><strong>NLP 4 大任务：</strong></p>
<ol>
<li>序列标注 (分词、POS Tag、NER、语义标注)</li>
<li>分类任务</li>
<li>句子关系判断 （Entailment、QA、语义改写）</li>
<li>生成式任务 （机器翻译、文本摘要、写诗造句、看图说话）</li>
</ol>
<blockquote>
<p>解决这些不同的任务，从模型角度来讲什么最重要？是特征抽取器的能力。尤其是深度学习流行开来后，这一点更凸显出来。因为深度学习最大的优点是“端到端（end to end）”，当然这里不是指的从客户端到云端，意思是以前研发人员得考虑设计抽取哪些特征，而端到端时代后，这些你完全不用管，把原始输入扔给好的特征抽取器，它自己会把有用的特征抽取出来。</p>
</blockquote>
<h2>3. Attention</h2>
<p><img src="/images/nlp/attention.jpg" alt=""></p>
<blockquote>
<p>Attention model 物理含义: 看作是输出 Target句子 中某个单词和输入 Source句子 每个单词的 <strong>对齐模型</strong>.</p>
<p>Self Attention 可以认为是一种特殊的 Source == Target 的情况.</p>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">good 张俊林: 深度学习中的 Attention Model（2017版）</a></p>
<h2>4. Transformer</h2>
<p>BERT 所采用的算法来自于 <strong>2017.12 google Transformer</strong>: <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attenion Is All You Need</a></p>
<p>入门 Transformer 的可以参考以下三篇文章：</p>
<blockquote>
<p>第一篇是 Jay Alammar's Blog</p>
<p><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">Transformer</a></p>
<p><a href="https://blog.csdn.net/yujianmin1990/article/details/85221271" target="_blank" rel="noopener">The Illustrated Transformer【译】</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">good The Illustrated Transformer 中文版</a></p>
<p>&lt;!--<img src="/images/nlp/bert-4.jpg" alt="">--&gt;</p>
<p><strong>Q、K、V 它们都是有助于计算和理解注意力机制</strong>的抽象概念</p>
<p><img src="/images/nlp/bert-6.jpg" alt=""></p>
<p>第二篇是 Calvo's Blog</p>
<p><a href="https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3" target="_blank" rel="noopener">Dissecting BERT Part 1: The Encoder</a></p>
<p>第三篇是 哈佛大学NLP研究组</p>
<p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/39034683" target="_blank" rel="noopener">good Transformer 知乎模型笔记，阅读心得</a></p>
</blockquote>
<p>&lt;!--<img src="/images/nlp/bert-2.jpg" alt="">--&gt;</p>
<blockquote>
<p>本文所说的Transformer特征抽取器并非原始论文所指。因为Encoder部分目的比较单纯，就是从原始句子中提取特征，而Decoder部分则功能相对比较多，除了特征提取功能外，还包含语言模型功能，以及用attention机制表达的翻译模型功能。</p>
</blockquote>
<p><img src="/images/nlp/bert-3.jpg" alt=""></p>
<ul>
<li>
<p><a href="https://blog.csdn.net/qq_42208267/article/details/84967446" target="_blank" rel="noopener">完全图解自然语言处理中的Transformer——BERT基础（入门长文）</a></p>
</li>
<li>
<p><a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">tensor2tensor 助于理解</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/58009338" target="_blank" rel="noopener">Transformer 知识点理解</a></p>
</li>
</ul>
<h3>4.1 step</h3>
<p><a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">务必阅读： The Illustrated Transformer 中文版</a></p>
<blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-f5e99be76f0727be85df0d8f4ab88057_hd.jpg" alt=""></p>
</blockquote>
<p>所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。</p>
<blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-fbb5dbc286b9f9cec2ddbc5eae2bf5a9_hd.jpg" alt=""></p>
</blockquote>
<blockquote>
<ol>
<li>从编码器输入的句子首先会经过一个自注意力（self-attention）层</li>
<li>这层帮助编码器在对<code>每个单词编码时关注输入句子的其他单词</code>。我们将在稍后的文章中更深入地研究自注意力。</li>
</ol>
</blockquote>
<blockquote>
<p>自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。</p>
</blockquote>
<blockquote>
<p>解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。</p>
</blockquote>
<blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-1cfd35f0ff43407e25da3ab25631f82d_hd.jpg" alt=""></p>
</blockquote>
<p><strong>将张量引入图景</strong></p>
<blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-d7b0bb93c9f7e7185d690b6df83d8859_hd.jpg" alt=""></p>
</blockquote>
<p><strong>现在我们开始“编码”</strong></p>
<blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-7173f8fa4d601a5255af46d48e9d370d_hd.jpg" alt=""></p>
</blockquote>
<p>这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。显然，已经在这个位置上的单词将获得最高的softmax分数，但有时关注另一个与当前单词相关的单词也会有帮助。</p>
<p>第五步是将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。</p>
<p>第六步是对加权值向量求和（译注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。），然后即得到自注意力层在该位置的输出(在我们的例子中是对于第一个单词)。</p>
<blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-609de8f8f8e628e6a9ca918230c70d67_hd.jpg" alt=""></p>
</blockquote>
<blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-f8c32c325da61ee587abcd2426854b33_hd.jpg" alt=""></p>
</blockquote>
<blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-12e11c0fea79bc485a6d9f4a2cb12f7f_hd.jpg" alt=""></p>
</blockquote>
<h2>5. BERT</h2>
<p><strong>Action:</strong></p>
<ul>
<li><a href="https://github.com/hanxiao/bert-as-service/blob/master/README.md" target="_blank" rel="noopener">hanxiao大佬开源出来的bert-as-service框架很适合初学者</a></li>
<li><a href="https://netycc.com/2018/12/05/%E5%88%A9%E7%94%A8bert%E6%9E%84%E5%BB%BA%E5%8F%A5%E5%90%91%E9%87%8F%E5%B9%B6%E8%AE%A1%E7%AE%97%E7%9B%B8%E4%BC%BC%E5%BA%A6/" target="_blank" rel="noopener">Netycc's blog 利用Bert构建句向量并计算相似度</a></li>
<li><a href="https://juejin.im/post/5c6d65a56fb9a04a0f65c45d" target="_blank" rel="noopener">BERT使用详解(实战)</a></li>
<li><a href="https://terrifyzhao.github.io/2018/11/29/%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97.html" target="_blank" rel="noopener">BERT中文文本相似度计算与文本分类</a></li>
</ul>
<p><strong>Bert完全指南</strong></p>
<ul>
<li><a href="https://terrifyzhao.github.io/2019/01/17/BERT%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97.html" target="_blank" rel="noopener">BERT完全指南</a></li>
</ul>
<h2>Reference</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/51934140" target="_blank" rel="noopener">张俊林: 天空之城：拉马努金式思维训练法</a></li>
<li><a href="https://www.zhihu.com/question/20584585/answer/15559213" target="_blank" rel="noopener">互联网人到了 30 岁，大部分都去干什么了？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/50717786" target="_blank" rel="noopener">AINLP BERT相关论文、文章和代码资源汇总</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/53099098" target="_blank" rel="noopener">自然语言处理中的Transformer和BERT</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36331888" target="_blank" rel="noopener">RNN和LSTM弱！爆！了！注意力模型才是王道</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/46997268" target="_blank" rel="noopener">NLP突破性成果 BERT 模型详细解读</a></li>
<li><a href="https://mp.weixin.qq.com/s/H4at_BDLwZWqlBHLjMZWRQ" target="_blank" rel="noopener">一步步理解BERT</a></li>
<li><a href="https://kexue.fm/archives/6736" target="_blank" rel="noopener">当Bert遇上Keras：这可能是Bert最简单的打开姿势</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-deeplearning/Seq2Seq-Attention" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/06/17/deeplearning/Seq2Seq-Attention/"><strong>Seq2Seq and Attention</strong></a>
      <small class=article-date-index>&nbsp; 2019-06-17</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/17/deeplearning/Seq2Seq-Attention/" class="article-date">
  <time datetime="2019-06-17T02:00:21.000Z" itemprop="datePublished">2019-06-17</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/17/deeplearning/Seq2Seq-Attention/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;img src=&quot;/images/deeplearning/Seq2Seq-00.jpg&quot; width=&quot;550&quot; alt=&quot;Attention 和人类的选择性视觉注意力机制类似&quot; /&gt;</p>
<p>&lt;!-- more --&gt;</p>
<p>我们先结合上篇文章的内容，将 language model 和 Machine translation model 做一个对比：</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W3-3.png&quot; width=&quot;600&quot; /&gt;</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W3-4.png&quot; width=&quot;700&quot; /&gt;</p>
<p>可以看到，机器翻译模型的后半部分其实就是语言模型，Andrew 将其称之为 “<strong>条件语言模型</strong>”.</p>
<p>$$
P(y^{&lt;1&gt;},…,y^{&lt;{T_y}&gt;}|x^{&lt;1&gt;},…,x^{&lt;{T_x}&gt;})
$$</p>
<h2>1. Encoder-Decoder</h2>
<p>&lt;img src=&quot;/images/deeplearning/Seq2Seq-03.jpg&quot; width=&quot;600&quot; alt=&quot;Encoder-Decoder&quot; /&gt;</p>
<p>Source 和 Target 分别由各自的单词序列构成：</p>
<p>$$
Source = ({x}_1, {x}_2, ..., {x}_m)
$$</p>
<p>$$
Target = ({y}_1, {y}_2, ..., {y}_n)
$$</p>
<p>Encoder 顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p>
<p>$$
C = F({x}_1, {x}_2, ..., {x}_m)
$$</p>
<p>对于 Decoder 来说，其任务是根据句子 Source 的 中间语义表示 C 和 之前已经生成的历史信息</p>
<p>$$
({y}_1, {y}_2, ..., {y}_{i-1})
$$</p>
<p>来生成 i时刻 要生成的单词 ${y}_{i}$</p>
<p>$$
y_{i} = g(C, {y}_1, {y}_2, ..., {y}_{i-1})
$$</p>
<blockquote>
<p>每个 $y_i$ 都依次这么产生，那么看起来就是整个系统根据输入 句子Source 生成了目标句子Target。</p>
<p>(1). 如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题；
(2). 如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要；
(3). 如果Source是一句问句，Target是一句回答，那么这是问答系统。</p>
</blockquote>
<blockquote>
<p>Encoder-Decoder框架 不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用.</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p>
<h3>1.1 encoder</h3>
<p>用函数 $f$ 表达 RNN 隐藏层的变换：</p>
<p>$$
\boldsymbol{h}_t = f(\boldsymbol{x}_t, \boldsymbol{h}_{t-1}).
$$</p>
<p>然后 Encoder 通过自定义函数 $q$ 将各个时间步的隐藏状态变换为背景变量</p>
<p>$$
\boldsymbol{c} =  q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T).
$$</p>
<p>例如，当选择 $q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) = \boldsymbol{h}_T$ 时，背景变量是输入序列最终时间步的隐藏状态 $\boldsymbol{h}_T$。</p>
<blockquote>
<p>以上描述的编码器是一个单向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用 Bi-RNN 构造编码器。 这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。</p>
</blockquote>
<p>&lt;img src=&quot;/images/chatbot/seq2seq-5.jpeg&quot; width=&quot;700&quot; /&gt;</p>
<h3>1.2 decoder</h3>
<blockquote>
<p>上小节 Encode 编码器输出的背景变量 $c$ 编码了整个输入序列 $x_1, \ldots, x_T$ 的信息。</p>
</blockquote>
<p>给定 train sample 的 input sequence： $y_1, y_2, \ldots, y_{T'}$，对每个时间步 $t'$（符号与 input sequence 或 encoder 的时间步 $t$ 有区别）， decoder 输出 $y_{t'}$ 的条件概率将基于之前的 output sequence： $y_1,\ldots,y_{t'-1}$ 和 $c$.</p>
<p><strong>即:</strong></p>
<p>$$
P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})
$$</p>
<p>为此，我们可以使用<code>另一个RNN</code>作为解码器。 在输出序列的时间步 $t^\prime$，解码器将上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $c$ 作为输入，并将它们与上一时间步的隐藏状态 $\boldsymbol{h}_{t^\prime-1}$ 变换为当前时间步的隐藏状态 $\boldsymbol{h}_{t^\prime}$。因此，我们可以用函数 $g$ 表达解码器隐藏层的变换：</p>
<p>$$
\boldsymbol{h}_{t^\prime} = g(y_{t^\prime-1}, \boldsymbol{c}, \boldsymbol{h}_{t^\prime-1}).
$$</p>
<p>可使用自定义的 output layer 和 softmax 计算 ${P}(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})$，计算当前时间步输出 $y_{t^\prime}$ 的概率分布.</p>
<h3>1.3 decoder greedy search</h3>
<p>在语言模型之前有一 个条件也就是被翻译的句子:</p>
<p>$$
P(y^{&lt;1&gt;},…,y^{&lt;{T_y}&gt;}|x^{&lt;1&gt;},…,x^{&lt;{T_x}&gt;})
$$</p>
<blockquote>
<p>但是我们知道翻译是有很多种方式的，同一句话可以翻译成很多不同的句子，那么如何判断哪一句子是最好的呢？</p>
<p>还是翻译上面那句话，有如下几种翻译结果：</p>
<ul>
<li>&quot;Jane is visiting China in September.&quot;</li>
<li>&quot;Jane is going to visit China in September.&quot;</li>
<li>&quot;In September, Jane will visit China&quot;</li>
<li>&quot;Jane's Chinese friend welcomed her in September.&quot;</li>
<li>....</li>
</ul>
</blockquote>
<p>得到最好的翻译结果，转换成数学公式就是:</p>
<p>$$
argmax P(y^{&lt;1&gt;},…,y^{&lt;{T_y}&gt;}|x^{&lt;1&gt;},…,x^{&lt;{T_x}&gt;})
$$</p>
<p>那么 Greedy Search 就是每次输出的那个都必须是最好的。还是以翻译那句话为例。</p>
<blockquote>
<p>现在假设通过贪婪搜索已经确定最好的翻译的前两个单词是：&quot;Jane is &quot;</p>
<p>然后因为 &quot;going&quot; 出现频率较高和其它原因，所以根据贪婪算法得出此时第三个单词的最好结果是 &quot;going&quot;。</p>
<p>所以据贪婪算法最后的翻译结果可能是下图中的第二个句子，<strong>但第一句可能会更好.</strong></p>
<p>&lt;img src=&quot;/images/deeplearning/C5W3-5.png&quot; width=&quot;600&quot; /&gt;</p>
<p>所以 Greedy Search 的缺点是局部最优并不代表全局最优. Greedy Search 更加短视，看的不长远。</p>
</blockquote>
<h3>1.4 decoder beam search</h3>
<p>Beam Search 是 greedy search 的加强版本，首先要预设一个值 beam width，这里等于 <code>3</code> (如果等于 1 就是 greedy search)。然后在每一步保存最佳的 3 个结果进行下一步的选择，以此直到遇到句子的终结符.</p>
<h4>1.4.1 step 1</h4>
<p>如下图示，因为beam width=3，所以根据输入的需要翻译的句子选出 3 个 $y^{&lt;1&gt;}$最可能的输出值。</p>
<p>即选出 $P(y^{&lt;1&gt;}|x)$ 最大的前3个值。 假设分别是 <strong>&quot;in&quot;, &quot;jane&quot;, &quot;september&quot;</strong></p>
<p>&lt;img src=&quot;/images/deeplearning/C5W3-6_1.png&quot; width=&quot;650&quot; /&gt;</p>
<h4>1.4.2 step 2</h4>
<p>以&quot;<strong>in</strong>&quot;为例进行说明，其他同理.</p>
<p>如下图示，在给定被翻译句子 $x$ 和确定 $y^{&lt;1&gt;}$ = &quot;<strong>in</strong>&quot; 的条件下，下一个输出值的条件概率是 $P(y^{&lt;2&gt;}|x,&quot;in&quot;)$。</p>
<p>此时需要从 10000 种可能中找出条件概率最高的前 3 个.</p>
<p>又由公式:</p>
<p>$$
P(y^{&lt;1&gt;},y^{&lt;2&gt;}|x)=P(y^{&lt;1&gt;}|x) P(y^{&lt;2&gt;}|x, y^{&lt;1&gt;})
$$</p>
<p>我们此时已经得到了给定输入数据，前两个输出值的输出概率比较大的组合了.</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W3-7_1.png&quot; width=&quot;650&quot; /&gt;</p>
<p>另外 2 个单词也做同样的计算</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W3-8_1.png&quot; width=&quot;650&quot; /&gt;</p>
<p>此时我们得到了 9 组 $P(y^{&lt;1&gt;},y^{&lt;2&gt;}|x)$, 此时我们再从这 9组 中选出概率值最高的前 3 个。</p>
<p>如下图示，假设是这3个：</p>
<blockquote>
<ul>
<li>&quot;in september&quot;</li>
<li>&quot;jane is&quot;</li>
<li>&quot;jane visits&quot;</li>
</ul>
</blockquote>
<h4>1.4.3 step 3</h4>
<p>继续 step 2 的过程，根据 $P(y^{&lt;3&gt;}|x,y^{&lt;1&gt;},y^{&lt;2&gt;})$ 选出 $P(y^{&lt;1&gt;},y^{&lt;2&gt;},y^{&lt;3&gt;}|x)$ 最大的前3个组合.</p>
<p>后面重复上述步骤得出结果.</p>
<h4>1.4.4 summary</h4>
<p>总结一下上面的步骤就是：</p>
<blockquote>
<ul>
<li>(1). 经过 encoder 以后，decoder 给出最有可能的三个开头词依次为 “in”, &quot;jane&quot;, &quot;september&quot;
$$P(y^{&lt;1&gt;}|x)$$</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>(2). 经 step 1 得到的值输入到 step 2 中，最可能的三个翻译为 “in september”, &quot;jane is&quot;, &quot;jane visits&quot;</li>
</ul>
<p>$$P(y^{&lt;2&gt;}|x,y^{&lt;1&gt;})$$</p>
<p>(这里，september开头的句子由于概率没有其他的可能性大，已经失去了作为开头词资格)</p>
</blockquote>
<blockquote>
<ul>
<li>(3). 继续这个过程...</li>
</ul>
<p>$$P(y^{&lt;3&gt;}|x,y^{&lt;1&gt;},y^{&lt;2&gt;})$$</p>
</blockquote>
<p>&lt;img src=&quot;/images/deeplearning/C5W3-10_1.png&quot; width=&quot;750&quot; /&gt;</p>
<h3>1.5 refinements to beam search</h3>
<p>$$
P(y^{&lt;1&gt;},….,P(y^{T_y})|x)=P(y^{&lt;1&gt;}|x)P(y^{&lt;2&gt;}|x,y^{&lt;1&gt;})…P(y^{&lt;{T_y}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{T_y-1}&gt;})
$$</p>
<p>所以要满足 $argmax P(y^{&lt;1&gt;},….,P(y^{T_y})|x)$, 也就等同于要满足</p>
<p>$$
argmax \prod_{t=1}^{T_y}P(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;})
$$</p>
<p>但是上面的公式存在一个问题，因为概率都是小于1的，累乘之后会越来越小，可能小到计算机无法精确存储，所以可以将其转变成 log 形式（因为 log 是单调递增的，所以对最终结果不会有影响），其公式如下：</p>
<p>$$
argmax \sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;})
$$</p>
<blockquote>
<p>But！！！上述公式仍然存在bug，观察可以知道，概率值都是小于1的，那么log之后都是负数，所以为了使得最后的值最大，那么只要保证翻译的句子越短，那么值就越大，所以如果使用这个公式，那么最后翻译的句子通常都是比较短的句子，这显然不行。</p>
</blockquote>
<p>所以我们可以通过归一化的方式来纠正，即保证平均到每个单词都能得到最大值。其公式如下：</p>
<p>$$
argmax \frac{1}{T_y}\sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;})
$$</p>
<p>归一化的确能很好的解决上述问题，但是在实际运用中，会额外添加一个参数 $α$, 其大小介于 0 和 1 之间</p>
<p>$$
argmax \frac{1}{T_y^α}\sum_{t=1}^{T_y}logP(y^{&lt;{t}&gt;}|x,y^{&lt;1&gt;},…y^{&lt;{t-1}&gt;})
$$</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W3-11_1.png&quot; width=&quot;700&quot; /&gt;</p>
<blockquote>
<p>$T_y$ 为输出句子中单词的个数，$α$ 是一个超参数 (可以设置为 0.7)</p>
<p>$α$ == 1. 则代表 完全用句子长度归一化
$α$ == 0. 则代表 没有归一化
$α$ == 0~1. 则代表 在 句子长度归一化 与 没有归一化 之间的折中程度.</p>
<p>beam width = B = 3~<strong>10</strong>~100 是会有一个明显的增长，但是 B 从 1000 ~ 3000 是并没有一个明显增长的.</p>
</blockquote>
<h3>1.6 train seq2seq model</h3>
<p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>
<p>$$
\begin{split}\begin{aligned}
{P}(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T)
&amp;= \prod_{t'=1}^{T'} {P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, x_1, \ldots, x_T)\\
&amp;= \prod_{t'=1}^{T'} {P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c}),
\end{aligned}\end{split}
$$</p>
<p>并得到该输出序列的损失</p>
<p>$$ - \log{P}(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T) = -\sum_{t'=1}^{T'} \log {P}(y_{t'} \mid y_1,  \ldots, y_{t'-1}, \boldsymbol{c}),
$$</p>
<p>&lt;img src=&quot;/images/chatbot/seq2seq-6.png&quot; width=&quot;800&quot; /&gt;</p>
<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图中所描述的模型预测中，我们需要将decode在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列在上一个时间步的标签作为decode在当前时间步的输入。这叫做强制教学（teacher forcing）。</p>
<h3>1.7 summary</h3>
<ul>
<li>编码器 - 解码器（seq2seq）可以输入并输出不定长的序列。</li>
<li>编码器—解码器使用了两个 RNN。</li>
<li>在编码器—解码器的训练中，我们可以采用 teacher forcing。(这也是 Seq2Seq 2 的内容)</li>
</ul>
<h2>2. Seq2Seq 框架2</h2>
<p>Seq2Seq model 来自于 “<a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a>”</p>
<p>其模型结构图如下所示：</p>
<p>&lt;img src=&quot;/images/chatbot/seq2seq-2.jpg&quot; width=&quot;700&quot; /&gt;</p>
<p>与上面模型最大的区别在于其source编码后的 向量$C$ 直接作为 Decoder RNN 的 init state，而不是在每次decode时都作为 RNN cell 的输入。此外，decode 时 RNN 的输入是 label，而不是前一时刻的输出。</p>
<p>Encoder 阶段：</p>
<p>&lt;img src=&quot;/images/chatbot/seq2seq-3.jpg&quot; width=&quot;500&quot; /&gt;</p>
<blockquote>
<p>每个词经过 RNN 都会编码为 hidden (e0,e1,e2), source序列 的编码向量e 就是 最终的 hidden state e2</p>
<p>Tips： 这里 $e_0, e_1, e_2$ 是 hidden state， 并没有经过 g 和 softmax .</p>
</blockquote>
<p>Decoder 阶段：</p>
<p>&lt;img src=&quot;/images/chatbot/seq2seq-4.jpg&quot; width=&quot;500&quot; /&gt;</p>
<p>e向量 仅作为 RNN 的 init state 传入decode模型，每一时刻输入都是前一时刻的正确label。直到最终输入&lt;eos&gt;符号截止.</p>
<h2>3. Seq2Seq Attention</h2>
<p>请务必要阅读： <a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">张俊林 深度学习中的注意力模型（2017版）</a></p>
<p>&lt;img src=&quot;/images/deeplearning/Attention-01.jpg&quot; width=&quot;600&quot; /&gt;</p>
<p><strong>decode</strong> 在各个时间步依赖相同的 <strong>背景变量 $c$</strong> 来获取输入序列信息。当 <strong>encode</strong> 为 RNN 时，<strong>背景变量$c$</strong> 来自它最终时间步的 hidden state。</p>
<blockquote>
<p>英语输入：“They”、“are”、“watching”、“.”
法语输出：“Ils”、“regardent”、“.”</p>
<p>翻译例子：输入为英语序列“They”、“are”、“watching”、“.”，输出为法语序列“Ils”、“regardent”、“.”。，<strong>decode</strong> 在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步 1，解码器可以主要依赖“They”、“are”的信息来生成“Ils”，在时间步 2 则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步 3 则直接映射句号“.”。这看上去就像是在 <strong>decode</strong> 的每一时间步对输入序列中不同时间步的编码信息分配不同的注意力一样。这也是注意力机制的由来 <a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="noopener">1</a>。</p>
<p>仍以 RNN 为例，Attention 通过对 Encode 所有时间步的隐藏状态做<strong>加权平均</strong>来得到背景变量$c$。Decode 在每一时间步调整这些权重，即 Attention weight，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量$c$。本节我们将讨论 Attention机制 是怎么工作的。</p>
</blockquote>
<p>在“encoder-decoder（seq2seq）”, Decoder 在时间步 $t'$ 的 hidden state</p>
<p>$$
\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c}, \boldsymbol{s}_{t'-1})
$$</p>
<p>在 Attention机制 中, Decoder 的每一时间步将使用可变的背景变量$c$</p>
<p>$$
\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c}_{t'}, \boldsymbol{s}_{t'-1}).
$$</p>
<p>关键是如何计算背景变量 $\boldsymbol{c}_{t'}$ 和如何利用它来更新隐藏状态 $\boldsymbol{s}_{t'}$。以下将分别描述这两个关键点。</p>
<h3>3.1 计算背景变量 c</h3>
<p>$$
\boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t,
$$</p>
<p>其中给定 $t'$ 时，权重 $\alpha_{t' t}$ 在 $t=1,\ldots,T$ 的值是一个概率分布。为了得到概率分布，可以使用 softmax 运算:</p>
<p>$$
\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) },\quad t=1,\ldots,T.
$$</p>
<p>现在，我们需要定义如何计算上式中 softmax 运算的输入 $e_{t' t}$。由于 $e_{t' t}$ 同时取决于decode的时间步 $t'$ 和encode的时间步 $t$，我们不妨以解码器在时间步 $t'−1$ 的隐藏状态 $\boldsymbol{s}_{t' - 1}$ 与编码器在时间步 $t$ 的隐藏状态 $h_t$ 为输入，并通过函数 $a$ 计算 $e_{t' t}$：</p>
<p>$$
e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t).
$$</p>
<p>这里函数 a 有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积 $a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出Attention机制的论文则将输入连结后通过含单隐藏层的多层感知机MLP 变换</p>
<p>$$
a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),
$$</p>
<p>其中 $v、W_s、W_h$ 都是可以学习的模型参数。</p>
<h3>3.2 update hidden state</h3>
<p>以 GRU 为例，在解码器中我们可以对 GRU 的设计稍作修改。解码器在时间步 $t'$ 的隐藏状态为</p>
<p>$$
\boldsymbol{s}_{t'} = \boldsymbol{z}_{t'} \odot \boldsymbol{s}_{t'-1}  + (1 - \boldsymbol{z}_{t'}) \odot \tilde{\boldsymbol{s}}_{t'},
$$</p>
<p>其中的重置门、更新门和候选隐含状态分别为 :</p>
<p>$$
\begin{split}\begin{aligned}
\boldsymbol{r}_{t'} &amp;= \sigma(\boldsymbol{W}_{yr} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sr} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cr} \boldsymbol{c}_{t'} + \boldsymbol{b}_r),\\
\boldsymbol{z}_{t'} &amp;= \sigma(\boldsymbol{W}_{yz} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sz} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cz} \boldsymbol{c}_{t'} + \boldsymbol{b}_z),\\
\tilde{\boldsymbol{s}}_{t'} &amp;= \text{tanh}(\boldsymbol{W}_{ys} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{ss} (\boldsymbol{s}_{t' - 1} \odot \boldsymbol{r}_{t'}) + \boldsymbol{W}_{cs} \boldsymbol{c}_{t'} + \boldsymbol{b}_s),
\end{aligned}\end{split}
$$</p>
<p>其中含下标的 W 和 b 分别为 GRU 的权重参数和偏差参数。</p>
<p>&lt;img src=&quot;/images/chatbot/seq2seq-7.jpeg&quot; width=&quot;800&quot; /&gt;</p>
<h3>3.3 attention summary</h3>
<ul>
<li>可以在decode的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。</li>
<li>Attention机制可以采用更为高效的矢量化计算。</li>
</ul>
<p>除此之外模型为了取得比较好的效果还是用了下面三个小技巧来改善性能：</p>
<blockquote>
<p>深层次的LSTM：作者使用了4层LSTM作为encoder和decoder模型，并且表示深层次的模型比shallow的模型效果要好（单层，神经元个数多）。</p>
<p>将source进行反序输入：输入的时候将“ABC”变成“CBA”，这样做的好处是解决了长序列的long-term依赖，使得模型可以学习到更多的对应关系，从而达到比较好的效果。</p>
</blockquote>
<p>注意力机制是一种思想，可以有多种不同的实现方式，在 Seq2Seq 模型以外的场景也有不少应用</p>
<p>&lt;img src=&quot;/images/deeplearning/Attention-00.jpg&quot; width=&quot;700&quot; /&gt;</p>
<h2>4. Attention 本质思想</h2>
<p>把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易懂:</p>
<p>&lt;img src=&quot;/images/deeplearning/Attention-04.jpg&quot; width=&quot;600&quot; /&gt;</p>
<h3>4.1 Attention 的三阶段</h3>
<blockquote>
<ol>
<li>第一个阶段根据Query和Key计算两者的相似性或者相关性；</li>
<li>第二个阶段对第一阶段的原始分值进行归一化处理；</li>
<li>根据权重系数对Value进行加权求和。</li>
</ol>
</blockquote>
<p>&lt;img src=&quot;/images/deeplearning/Attention-03.jpg&quot; width=&quot;600&quot; /&gt;</p>
<h3>4.2 Self Attention</h3>
<p>Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</p>
<p>&lt;img src=&quot;/images/deeplearning/Attention-05.jpg&quot; width=&quot;600&quot; /&gt;</p>
<blockquote>
<p>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p>
</blockquote>
<p>请务必要阅读： <a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">张俊林 深度学习中的注意力模型（2017版）</a></p>
<hr>
<h2>Reference</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y&amp;index=18&amp;list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax" target="_blank" rel="noopener">动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32092871" target="_blank" rel="noopener">seq2seq+Attention机制模型详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习前沿笔记</a></li>
<li><a href="http://www.iterate.site/2019/04/19/05-seq2seq%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">百面 seq2seq模型</a></li>
<li><a href="http://www.iterate.site/2019/04/19/06-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" target="_blank" rel="noopener">百面 注意力机制</a></li>
<li><a href="https://kexue.fm/archives/6736" target="_blank" rel="noopener">Bert遇上Keras</a></li>
<li><a href="/2018/08/14/deeplearning/Sequence-Models-week3/">Sequence-Models-week3</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28048246" target="_blank" rel="noopener">seq2seq中的beam search算法过程</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型（2017版）</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-nlp/Language-Model-and-Word-Embedding" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/06/16/nlp/Language-Model-and-Word-Embedding/"><strong>Language Model and Perplexity</strong></a>
      <small class=article-date-index>&nbsp; 2019-06-16</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/16/nlp/Language-Model-and-Word-Embedding/" class="article-date">
  <time datetime="2019-06-16T03:00:21.000Z" itemprop="datePublished">2019-06-16</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/nlp/">nlp</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/16/nlp/Language-Model-and-Word-Embedding/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;img src=&quot;/images/nlp/LM-01.jpg&quot; width=&quot;550&quot; alt=&quot;2001 NNLM, @Yoshua bengio&quot; /&gt;</p>
<p>&lt;!-- more --&gt;</p>
<p>计算机很多事情比人类做得好，那么机器是否能懂 Natural Language?</p>
<p>一些 NLP 技术的应用:</p>
<blockquote>
<ul>
<li>简单的任务：拼写检查，关键词检索，同义词检索等</li>
<li>复杂的任务：信息提取、情感分析、文本分类等</li>
<li>更复杂任务：机器翻译、人机对话、QA系统</li>
</ul>
</blockquote>
<p>Natural Language 逐渐演变成一种 <strong>上下文信息表达</strong> 和 <strong>传递</strong> 的方式。</p>
<p>让计算机处理自然语言，一个基本的问题就是为 自然语言 这种 上下文相关的特性 建立数学模型。</p>
<h2>1. Language Model</h2>
<blockquote>
<ol>
<li>美联储主席昨天告诉媒体 7000 亿美金的救助资金将借给上百家银行、汽车公司。</li>
<li>美联储主席昨天 7000 亿美金的救助资金告诉媒体将借给上百家银行、汽车公司。</li>
<li>美联储主席昨天 告媒诉体 70 亿00美金的救助资金上百家银行将借给、汽车公司。</li>
</ol>
<p>上世纪70年代科学家们试图用规则文法判断句子是否合理。贾里尼克用统计模型解决方法更有效。</p>
</blockquote>
<p>如果 S 表示一连串特定顺序排列的词 $w_1$， $w_2$，…， $w_n$ ，换句话说，S 表示的是一个有意义的句子。机器对语言的识别从某种角度来说，就是想知道 S 在文本中出现的可能性，也就是数学上所说的 S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是 P(S) 可展开为：</p>
<p>$$
P(S) = P(w_1)P(w_2|w_1)P(w_3| w_1 w_2)…P(w_n|w_1 w_2…w_{n-1})
$$</p>
<h3>1.1 Markov assumption</h3>
<p>假定文本中的每个词 $w_i$ 和 前面N-1个词有关，而和更前面的词无关，这样当前词 $w_i$ 的概率值取决于前面 N-1个词 $P(w_{i-N+1}, w_{i-N+2}, ..., w_{i-1})$</p>
<p>$$
P(w_{i}|w_{1}, w_{2}, ..., w_{i-1}) = P(w_i | w_{i-N+1}, w_{i-N+2}, ..., w_{i-1})
$$</p>
<blockquote>
<p>N元模型， N=2 时，为二元模型。 在实际中应用最多的是 N=3 的三元模型.</p>
</blockquote>
<h3>1.2 n-gram, n=2</h3>
<p>$$
P(S) = P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_i|w_{i-1})…
$$</p>
<p>接下来如何估计 $P (w_i|w_{i-1})$。只要机器数一数这对词 $(w_i{-1}, w_i)$ 在统计的文本中出现了多少次，以及 $w_{i-1}$ 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,</p>
<p>$$
P(w_i|w_{i-1}) = \frac {P(w_{i-1}, w_i)} {P(w_{i-1})}
$$</p>
<h2>2. Perplexity, PPL</h2>
<p>语言模型效果的常用指标 perplexity， 在测试集上 perplexity 越低，说明建模效果越好.</p>
<p>计算perplexity的公式如下：</p>
<p>&lt;img src=&quot;/images/tensorflow/tf-google-9.1.2_1-equation.svg&quot; width=&quot;600&quot; /&gt;</p>
<p><strong>perplexity</strong> 刻画的是语言模型预测一个语言样本的能力. 比如已经知道 (w1,w2,w3,…,wm) 这句话会出现在语料库之中，那么通过语言模型计算得到的这句话的概率越高，说明语言模型对这个语料库拟合得越好。</p>
<p><strong>perplexity</strong> 实际是计算每一个单词得到的概率倒数的几何平均，因此 perplexity 可以理解为平均分支系数（average branching factor），即模型预测下一个词时的平均可选择数量。</p>
<blockquote>
<p>例如，考虑一个由0~9这10个数字随机组成的长度为m的序列，由于这10个数字出现的概率是随机的，所以每个数字出现的概率是 。因此，在任意时刻，模型都有10个等概率的候选答案可以选择，于是perplexity就是10（有10个合理的答案）。</p>
<p>perplexity的计算过程如下：</p>
<p>&lt;img src=&quot;/images/tensorflow/tf-google-9.1.2_3-ppl.jpg&quot; width=&quot;800&quot; /&gt;</p>
</blockquote>
<p>在语言模型的训练中，通常采用 perplexity 的对数表达形式：</p>
<p>&lt;img src=&quot;/images/tensorflow/tf-google-9.1.2_2-equation.svg&quot; width=&quot;600&quot; /&gt;</p>
<blockquote>
<p>相比较乘积求平方根的方式，加法的形式可加速计算，同时避免概率乘积数值过小而导致浮点数向下溢出的问题.</p>
<p>在数学上，log perplexity 可以看作真实分布与预测分布之间的交叉熵 Cross Entropy, 交叉熵描述了两个概率分布之间的一种距离. log perplexity 和 Cross Entropy 是等价的</p>
</blockquote>
<p>在神经网络模型中，$P(w_i | w_{1}, , ..., w_{i-1})$ 分布通常是由一个 softmax层 产生的，TensorFlow 中提供了两个方便计算交叉熵的函数，可以将 logits 结果直接放入输入，来帮助计算 softmax 然后再进行计算 Cross Entropy.</p>
<p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y)</span><br><span class="line">cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = y)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><a href="https://www.zhihu.com/people/xi-xiang-yu-20/posts" target="_blank" rel="noopener">知乎_习翔宇</a></li>
</ul>
<h2>3. NNLM</h2>
<p>NNLM,直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程.</p>
<p>既然离散的表示有辣么多缺点，于是有小伙伴就尝试着用模型最优化的过程去转换词向量了.</p>
<p>&lt;img src=&quot;/images/nlp/word2vec-nnlm.png&quot; width=&quot;600&quot; /&gt;</p>
<p>计算复杂度： ($N * D + N * D * H + H * V$) 相当之高, 于是有了 CBOW 和 Skip-Gram .</p>
<blockquote>
<p>NN 训练 语言模型， 会顺带产生一个 Word Embedding 矩阵.</p>
<p>词嵌矩阵 * 单词的独热编码 = 单词的词嵌</p>
<p>(300, 10000) * (10000, 1) = (300, 1)</p>
</blockquote>
<p>可以通过训练神经网络的方式构建词嵌表 <code>E</code> .</p>
<p>下图展示了预测单词的方法，即给出缺少一个单词的句子：</p>
<p>“<strong>I want a glass of orange ___</strong>”</p>
<blockquote>
<p>计算方法是将已知单词的特征向量都作为输入数据送到神经网络中去，然后经过一系列计算到达 Softmax分类层，在该例中输出节点数为 10000个。经过计算 juice 概率最高，所以预测为</p>
<p>“I want a glass of orange <code>juice</code>”</p>
</blockquote>
<p>&lt;img src=&quot;/images/deeplearning/C5W2-5_1.png&quot; width=&quot;750&quot; /&gt;</p>
<p>在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表 $E$</p>
<blockquote>
<p>假设要预测的单词为 $W$，词嵌表仍然为 $E$，需要注意的是训练词嵌表和预测 $W$ 是两个不同的任务。</p>
<p>如果任务是预测 $W$，最佳方案是使用 $W$ 前面 $n$ 个单词构建语境。</p>
<p>如果任务是训练 $E$，除了使用 $W$ 前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 Skip Gram 算法），这些方法都能提供很好的结果。</p>
</blockquote>
<h3>3.1 Word Representation</h3>
<p>单词与单词之间是有很多共性的，或在某一特性上相近，比如“苹果”和“橙子”都是水果；或者在某一特性上相反，比如“父亲”在性别上是男性，“母亲”在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率，这里用一个简化的表格说明:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Man (5391)</th>
<th style="text-align:center">Woman (9853)</th>
<th style="text-align:center">Apple (456)</th>
<th style="text-align:center">Orange (6257)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">性别</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">年龄</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">-0.01</td>
</tr>
<tr>
<td style="text-align:center">食物</td>
<td style="text-align:center">0.04</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.95</td>
</tr>
<tr>
<td style="text-align:center">颜色</td>
<td style="text-align:center">0.03</td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.70</td>
</tr>
</tbody>
</table>
<p>在表格中可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。括号里的数字代表这个单词在独热编码中的位置，可以用这个数字代表这个单词比如 Man = ，Man 的特性用 ，也就是那一纵列。</p>
<p>在实际的应用中，特性的数量远不止 4 种，可能有几百种，甚至更多。对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。</p>
<blockquote>
<p>压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个就是 “Word Embeddings” 的概念.</p>
</blockquote>
<p>&lt;img src=&quot;/images/deeplearning/C5W2-2.png&quot; width=&quot;500&quot; /&gt;</p>
<blockquote>
<p>上图通过聚类将词性相类似的单词在二维空间聚为一类.</p>
</blockquote>
<h3>3.2 Word Embeddings</h3>
<p>先下一个非正规定义 “词嵌 - 描述了词性特征的总量，也是在高维词性空间中嵌入的位置，拥有越多共性的词，词嵌离得越近，反之则越远”。值得注意的是，表达这个“位置”，需要使用所有设定的词性特征，假如有 300 个特征（性别，颜色，...），那么词嵌的空间维度就是 300.</p>
<h3>3.3 使用词嵌三步</h3>
<ol>
<li>获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库</li>
<li>应用词嵌：将获得的词嵌应用在我们的训练任务中</li>
<li>可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）</li>
</ol>
<p><strong>词嵌实用场景:</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">No.</th>
<th style="text-align:center">sencentce</th>
<th style="text-align:center">replace word</th>
<th style="text-align:center">target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">Sally Johnson is an <code>orange</code> farmer.</td>
<td style="text-align:center">orange</td>
<td style="text-align:center">Sally Johnson</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">Robert Lin is an <code>apple</code> farmer.</td>
<td style="text-align:center">apple</td>
<td style="text-align:center">Robert Lin</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">Robert Lin is a <code>durian cultivator</code>.</td>
<td style="text-align:center">durian cultivator</td>
<td style="text-align:center">Robert Lin</td>
</tr>
</tbody>
</table>
<blockquote>
<p>我们继续替换，我们将 apple farmer 替换成不太常见的 durian cultivator (榴莲繁殖员)。此时词嵌入中可能并没有 durian 这个词，cultivator 也是不常用的词汇。这个时候怎么办呢？我们可以用到迁移学习。</p>
</blockquote>
<p><strong>词嵌入迁移学习步骤如下：</strong></p>
<blockquote>
<ol>
<li>学习含有大量文本语料库的词嵌入 (一般含有 10亿 到 1000亿 单词)，或者下载预训练好的词嵌入</li>
<li>将学到的词嵌入迁移到相对较小规模的训练集 (例如 10万 词汇).</li>
<li>(可选) 这一步骤就是对新的数据进行 fine-tune。</li>
</ol>
</blockquote>
<h2>4. word2vec</h2>
<p>word2vec 并不是一个模型， 而是一个 2013年 google 发表的工具. 该工具包含2个模型： Skip-Gram 和 CBOW. 及两种高效训练方法： negative sampling 和 hierarchicam softmax.</p>
<blockquote>
<ol>
<li>CBOW  Continous Bag of Words Model</li>
<li>Skip-Gram Model</li>
</ol>
<p>词向量（词的特征向量）既能够降低维度，又能够capture到当前词在本句子中上下文的信息.</p>
</blockquote>
<p><a href="https://blog.csdn.net/u012052268/article/details/77170517/#63%E4%B8%AA%E4%BA%BA%E5%AF%B9word-embedding%E7%9A%84%E7%90%86%E8%A7%A3" target="_blank" rel="noopener">Word2Vec</a>
<a href="https://blog.csdn.net/sinat_33761963/article/details/54631367" target="_blank" rel="noopener">Word2Vec词嵌入矩阵</a></p>
<h3>4.1 CBOW</h3>
<p>&lt;img src=&quot;/images/nlp/word2vec-CBOW_1.png&quot; width=&quot;600&quot; /&gt;</p>
<blockquote>
<p>纠错 : 上图”目标函数“的第一个公式，应该是 连乘 公式，不是 连加 运算。</p>
<p>理解 : 背景词向量与 中心词向量 内积 等部分，你可考虑 softmax $w * x+b$ 中 $x$ 和 $w$ 的关系来理解.</p>
</blockquote>
<h3>4.2 Skip-Gram</h3>
<p>跳字模型假设基于某个词来生成它在文本序列周围的词。举个例子，假设文本序列是“the”“man”“loves”“his”“son”。以“loves”作为中心词，设背景窗口大小为2。如图10.1所示，跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词“the”“man”“his”“son”的条件概率，即</p>
<p>$$
P(\textrm{the},\textrm{man},\textrm{his},\textrm{son}\mid\textrm{loves}).
$$</p>
<p>假设给定中心词的情况下，背景词的生成是相互独立的，那么上式可以改写成</p>
<p>$$
P(\textrm{the}\mid\textrm{loves})\cdot P(\textrm{man}\mid\textrm{loves})\cdot P(\textrm{his}\mid\textrm{loves})\cdot P(\textrm{son}\mid\textrm{loves}).
$$</p>
<p>&lt;img src=&quot;/images/nlp/word2vec-skip-gram.svg&quot; width=&quot;300&quot; /&gt;</p>
<p><strong>训练 Skip-Gram</strong></p>
<p>跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：</p>
<p>$$ - \sum_{t=1}^{T} \sum_{-m \leq j \leq m,\ j \neq 0} \text{log}, P(w^{(t+j)} \mid w^{(t)}).
$$</p>
<p>如果使用随机梯度下降，那么在每一次迭代里我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，首先看到</p>
<p>$$
\log P(w_o \mid w_c) =
\boldsymbol{u}_o^\top \boldsymbol{v}_c - \log\left(\sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)\right)
$$</p>
<p>&lt;img src=&quot;/images/nlp/word2vec-skip.png&quot; width=&quot;700&quot; /&gt;</p>
<p>它的计算需要词典中所有词以 $w_c$ 为中心词的条件概率。有关其他词向量的梯度同理可得。</p>
<p>训练结束后，对于词典中的任一索引为 $i$ 的词，我们均得到该词作为中心词和背景词的两组词向量 $v_i$ 和 $u_i$ 。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。</p>
<blockquote>
<p>两个向量越相似，他们的点乘也就越大.</p>
</blockquote>
<p><strong>小结：</strong></p>
<ol>
<li>最大似然估计 MLE</li>
<li>最小化损失函数（与第一步等价），损失函数对数联合概率的相反数</li>
<li>描述概率函数，该函数的自变量是词向量（u和v），词向量也是模型参数</li>
<li>对第二步中每一项求梯度。有了梯度就可以优化第二步中的损失函数，从而迭代学习到模型参数，也就是词向量。</li>
</ol>
<h3>4.3 高效近似训练</h3>
<ul>
<li>hierarchicam softmax</li>
<li>negative sampling</li>
</ul>
<h2>5. fastText</h2>
<p>FastText是一个快速文本分类算法，在使用标准多核CPU的情况下，在10分钟内可以对超过10亿个单词进行训练。 不需要使用预先训练好的词向量，因为FastText会自己训练词向量。</p>
<p>文本分类：</p>
<p>&lt;img src=&quot;/images/nlp/fastText-3.webp&quot; width=&quot;500&quot; /&gt;</p>
<p>情感分类:</p>
<p>&lt;img src=&quot;/images/nlp/fastText-4.webp&quot; width=&quot;500&quot; /&gt;</p>
<p>fastText 能够做到效果好，速度快，主要依靠两个秘密武器：</p>
<blockquote>
<ol>
<li>利用了 词内的n-gram信息 (subword n-gram information)</li>
<li>用到了 层次化Softmax回归 (Hierarchical Softmax) 的训练 trick.</li>
</ol>
</blockquote>
<p><strong>fastText 和 word2vec 的区别:</strong></p>
<p><strong>两者表面的不同：</strong></p>
<blockquote>
<p><strong>模型的输出层：</strong></p>
<p>word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用；</p>
<p><strong>模型的输入层：</strong></p>
<p>word2vec的输出层，是 context window 内的term；而fasttext对应的整个sentence的内容，包括term，也包括 n-gram的内容；</p>
</blockquote>
<p><strong>两者本质的不同，体现在 h-softmax 的使用：</strong></p>
<blockquote>
<p>Wordvec的目的是得到词向量，该词向量最终是在输入层得到，输出层对应的 h-softmax也会生成一系列的向量，但最终都被抛弃，不会使用。</p>
<p>fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）</p>
</blockquote>
<h2>Reference</h2>
<ul>
<li>《数学之美》 读书笔记</li>
<li><a href="https://whiskytina.github.io/word2vec.html" target="_blank" rel="noopener">word2vec前世今生</a></li>
<li><a href="https://whiskytina.github.io/14947653164873.html" target="_blank" rel="noopener">CS224N NLP with Deep Learning: Lecture 1 课程笔记</a></li>
<li><a href="https://blog.csdn.net/m0_37324740/article/details/79411651" target="_blank" rel="noopener">good, sklearn 中 CountVectorizer、TfidfTransformer 和 TfidfVectorizer</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-deeplearning/RNN-LSTM-GRU" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2019/06/14/deeplearning/RNN-LSTM-GRU/"><strong>Recurrent Neural Networks</strong></a>
      <small class=article-date-index>&nbsp; 2019-06-14</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
      <br>
      <br> <!-- blair add -->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2019/06/14/deeplearning/RNN-LSTM-GRU/" class="article-date">
  <time datetime="2019-06-14T02:06:16.000Z" itemprop="datePublished">2019-06-14</time>
</a>-->
      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2019/06/14/deeplearning/RNN-LSTM-GRU/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
	
    <div class="article-entry" itemprop="articleBody">
      
        <p>&lt;img src=&quot;/images/deeplearning/RNN-01.png&quot; width=&quot;500&quot; /&gt;</p>
<p>&lt;!-- more --&gt;</p>
<ul>
<li>作为生物体，我们的视觉和听觉不断地获得带有序列的声音和图像信号，并交由大脑理解；</li>
<li>互联网数据中，很多是以序列形式存在的，例如文本、语音、视频、点击流等等。</li>
</ul>
<p><a href="/2018/07/26/deeplearning/Sequence-Models-week1/">RNN 基础知识，详参本博： Sequence Models</a></p>
<h2>1. RNN Basic</h2>
<p>在介绍 RNN 之前，首先解释一下为什么之前的标准网络不再适用了。因为它有两个缺点：</p>
<ul>
<li><strong><code>输入和输出的长度不尽相同</code></strong></li>
<li><strong><code>无法共享从其他位置学来的特征</code></strong></li>
</ul>
<blockquote>
<p>传统方法文本分类： 用一篇文章的 TF-IDF 向量作为输入，其中 TF-IDF 向量是词汇表大小.</p>
</blockquote>
<p><strong>Typical RNN Structure:</strong></p>
<p>在 $h_T$ 后面直接接一个 Softmax 层，输出文本所属类别的预测概率 $y$，就可以实现文本分类.</p>
<p>&lt;img src=&quot;/images/deeplearning/RNN-02.png&quot; width=&quot;650&quot; /&gt;</p>
<p>可应用于多种具体任务：</p>
<p>$$
net_{t}=U x_{t}+W h_{t-1}
$$</p>
<p>$$
h_{t}=f\left(\text {net}_{t}\right)
$$</p>
<p>$$
y=g\left(V h_{T}\right)
$$</p>
<p>其中 $f$ 和 $g$ 为激活函数，$U$ 为输入层到隐含层的权重矩阵，$W$ 为隐含层从上一时刻到下一时刻状态转移的权重矩阵。在文本分类任务中，$f$ 可以选取 Tanh 函数或者 ReLU 函数，$g$ 可以采用 Softmax 函数。</p>
<h3>1.1 TensorFlow RNN</h3>
<p>&lt;img src=&quot;/images/tensorflow/tf-google-8-1.jpg&quot; width=&quot;700&quot; alt=&quot;Forward Propagation&quot; /&gt;</p>
<blockquote>
<p><a href="/2018/11/08/tensorflow/tf-google-8-rnn-1/">更多详情参见本博： TensorFlow：第8章 Recurrent Neural Networks 1</a></p>
</blockquote>
<h3>1.2 Forward Propagation</h3>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-10_1.png&quot; width=&quot;700&quot; /&gt;</p>
<blockquote>
<p>$a^{&lt;0&gt;}=\vec{0}$</p>
<p>$a^{&lt;1&gt;}=g_1(W_{aa}a^{&lt;0&gt;}+W_{ax}x^{&lt;1&gt;}+b_a)$</p>
<p>$y^{&lt;1&gt;}=g_2(W_{ya}a^{&lt;1&gt;}+b_y)$</p>
<p>$a^{&lt;{t}&gt;}=g_1(W_{aa}a^{&lt;{t-1}&gt;}+W_{ax}x^{&lt;{t}&gt;}+b_a)$</p>
<p>$y^{&lt;{t}&gt;}=g_2(W_{ya}a^{&lt;{t}&gt;}+b_y)$</p>
<p>激活函数：<strong>$g_1$</strong> 一般为 <strong><code>tanh</code>函数</strong> (or <strong><code>Relu</code>函数</strong>)，<strong>$g_2$</strong> 一般是 <strong><code>Sigmod</code> or <code>softmax</code> 函数</strong>.</p>
</blockquote>
<h3>1.3 RNN vs CNN</h3>
<blockquote>
<ol>
<li>RNN 优点： 最大程度捕捉上下文信息，这可能有利于捕获长文本的语义。</li>
<li>RNN 缺点： 是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。</li>
<li>CNN 优点： 提取数据中的局部位置的特征，然后再拼接池化层。 CNN可以更好地捕捉文本的语义。是O(n)</li>
<li>CNN 优点： 一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。</li>
</ol>
</blockquote>
<h2>2. Language model</h2>
<p>此时就需要通过语言模型来预测每句话的概率：</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-29_1.png&quot; width=&quot;600&quot; /&gt;</p>
<h3>2.1 RNN Language Model</h3>
<ol>
<li>首先我们需要一个很大的语料库 (<strong>Corpus</strong>)</li>
<li>将每个单词字符化 (<strong>Tokenize</strong>，<strong>即使用One-shot编码</strong>) 得到词典,，假设有 10000 个单词</li>
<li>还需要添加两个特殊的单词</li>
</ol>
<blockquote>
<ul>
<li>end of sentence. 终止符，表示句子结束.
&lt;img src=&quot;/images/deeplearning/C5W1-30_1.png&quot; width=&quot;600&quot; /&gt;</li>
<li>UNknown, 之前的笔记已介绍过
&lt;img src=&quot;/images/deeplearning/C5W1-31_1.png&quot; width=&quot;600&quot; /&gt;</li>
</ul>
</blockquote>
<h3>2.2 Language Model Example</h3>
<p>假设要对这句话进行建模：<strong>Cats average 15 hours of sleep a day. &lt;EOS&gt;</strong></p>
<p><strong>1. 初始化</strong></p>
<blockquote>
<p>这一步比较特殊，即 $x^{&lt;1&gt;}$ 和 $a^{&lt;0&gt;}$ 都需要初始化为 $\vec{0}$ .
此时 $\hat{y}^{&lt;1&gt;}$ 将会对第一个字可能出现的每一个可能进行概率的判断,即 $\hat{y}^{&lt;1&gt;}=[p(a),…,p(cats),…]$.</p>
<p>当然在最开始的时候没有任何的依据，可能得到的是完全不相干的字，因为只是根据初始的值和激活函数做出的取样。</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-32_1.png&quot; width=&quot;500&quot; /&gt;</p>
</blockquote>
<p><strong>2. 将真实值作为输入值:</strong></p>
<blockquote>
<p>之所以将真实值作为输入值很好理解，如果我们一直传错误的值，将永远也无法得到字与字之间的关系</p>
</blockquote>
<p>如下图示，将 $y^{&lt;1&gt;}$ 所表示的真实值 Cats 作为输入，即 $x^{&lt;2&gt;}=y^{&lt;1&gt;}$ 得到 $\hat{y}^{&lt;2&gt;}$</p>
<p>此时的 $\hat{y}^{&lt;2&gt;}=[p(a|cats),…,p(average|cats),…]$</p>
<p>同理有 $\hat{y}^{&lt;3&gt;}=[p(a|cats, average),…,p(average|cats,average),…]$</p>
<p>另外输入值满足： $x^{&lt;{t}&gt;}=y^{&lt;{t-1}&gt;}$</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-33_1.png&quot; width=&quot;600&quot; /&gt;</p>
<p><strong>3. 计算出损失值:</strong></p>
<p>下图给出了构建模型的过程以及损失值计算公式:</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-34_1.png&quot; width=&quot;700&quot; /&gt;</p>
<blockquote>
<p>随着训练的次数的增多，或者常用词出现的频率的增多，语言模型便慢慢的会开始掌握简单的词语比如“平均”，“每天”，“小时”。一个完善的语言模型看到类似“ 10 个小”的时候，应该就能准确的判定下一个字是“时”。</p>
<p>（当然也许实际情况是“ 10 个小朋友”，所以通常会有更多的判断因素，这里只是一个例子）</p>
</blockquote>
<h2>3. Vanishing gradients with RNNs</h2>
<blockquote>
<p>目前这种基本的 RNN 也不擅长捕获这种长期依赖效应.</p>
<p>梯度爆炸可以用梯度消减解决、梯度消失就有点麻烦了，需要用 GRU 来解决.</p>
</blockquote>
<p><strong>gradient value</strong> 在 RNN 中也可能因为反向传播的层次太多导致 <strong>过小</strong> 或 <strong>过大</strong></p>
<blockquote>
<ul>
<li>当梯度值过小的时候，网络无法有效调整自己权重矩阵致训练效果不佳，称为 <strong>gradient vanishing</strong>；</li>
<li>过大时直接影响到程序的运作因为程序已无法存储那么大的值，会返回 NaN ，称为 <strong>gradient exploding</strong>.</li>
</ul>
</blockquote>
<p>当 <strong>gradient</strong> 过大时, 可以每次将返回的梯度值进行检查，超出预定范围，则手动设为范围的边界值：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (gradient &gt; max) &#123;</span><br><span class="line">    gradient = max</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但梯度值过小的解决方案要稍微复杂一点，比如下面两句话：</p>
<blockquote>
<p>“The <strong>cat</strong>，which already ate apple，yogurt，banana，..., <strong>was</strong> full.”
“The <strong>cats</strong>，which already ate apple，yogurt，banana，..., <strong>were</strong> full.”</p>
<p>重点标出的 <strong>cat(s)</strong> 和 be 动词（<strong>was, were</strong>） 是有重要关联的，但是中间隔了一个 which 引导的定语从句，对于前面所介绍的基础的 RNN网络 很难学习到这个信息，尤其是当出现梯度消失时，而且这种情况很容易发生.</p>
</blockquote>
<p>神经网络层次很多时，反向传播很难影响前面层次的参数。解决 <strong>gradient vanishing</strong>，提出了 <strong>GRU</strong> 单元.</p>
<blockquote>
<p>将在接下来的两个章节介绍两种方法来解决 <strong>梯度过小</strong> 问题，目标是当一些重要的单词离得很远的时候，比如例子中的 “<strong>cat</strong>” 和 “<strong>was</strong>”，能让语言模型准确的输出单数人称过去时的 “<strong>was</strong>”，而不是 “<strong>is</strong>” 或者 “<strong>were</strong>”. 两个方法都将引入“记忆”的概念，也就是为 RNN 赋予一个记忆的功能.</p>
</blockquote>
<h2>4. GRU - Gated Recurrent Unit</h2>
<p>GRU 是一种用来解决梯度值过小的方法，首先来看下在一个时刻下的 RNN单元，激活函数为 tanh</p>
<h3>4.1 回顾 RNN</h3>
<p>输入数据为 $a^{&lt;{t-1}&gt;}$ 和 $x^{&lt;{t}&gt;}$, 与参数 $W_a$ 进行线性运算后再使用 $tanh$ 函数 转化得到 $a^{&lt;{t}&gt;}$.</p>
<p>当然 $a^{&lt;{t}&gt;}$, 再使用 softmax 函数处理可以得到预测值.</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-37_1.png&quot; width=&quot;750&quot; /&gt;</p>
<h3>4.2 GRU结构</h3>
<p>在 GRU中 会用到 “记忆细胞(<strong>Memory cell</strong>)” 这个概念, 我们用变量<code>C</code>表示。这个记忆细胞提供了记忆功能，例如它能够帮助记住 cat 对应 was, cats 对应 were.</p>
<p>而在 $t$ 时刻，记忆细胞所包含的值其实就是 Activation function 值，即 $c^{&lt;{t}&gt;}=a^{&lt;{t}&gt;}$</p>
<blockquote>
<p>注意：在这里两个变量的值虽然一样，但是含义不同。</p>
<p>另外在下节将介绍的 LSTM 中，二者值的大小有可能是不一样的，所以有必要使用这两种变量进行区分.</p>
</blockquote>
<p>为了更新 <strong>memory_cell</strong> 的值，我们引入 $\tilde{c}$ 来作为候选值从而来更新 $c^{&lt;{t}&gt;}$，其公式为：</p>
<p>$$
\tilde{c}=tanh(W_c [c^{&lt;{t-1}&gt;}, x^{&lt;{t}&gt;}]+b_c)
$$</p>
<p><strong>更新门 (update gate):</strong></p>
<p>更新门是 GRU 的核心概念，它的作用是用于判断是否需要进行更新.</p>
<p>更新门用 $\Gamma_u$ 表示，其公式为：</p>
<p>$$
\Gamma_u=σ(W_u [c^{&lt;{t-1}&gt;}, x^{&lt;{t}&gt;}]+b_u)
$$</p>
<blockquote>
<p>如上图示，$\Gamma_u$ 值的大小大多分布在 0 或者 1，所以可以将其值的大小粗略的视为 0 或者 1。
这就是为什么我们就可以将其理解为一扇门，如果 $\Gamma_u=1$ , 就表示此时需要更新值，反之不用.</p>
</blockquote>
<p><strong>$t$ 时刻记忆细胞:</strong></p>
<p>有了更新门公式后，我们则可以给出 $t$ 时刻 <strong>memory_cell</strong> 的值的计算公式:</p>
<p>$$
c^{&lt;{t}&gt;} =  \Gamma_u * \tilde{c} + (1-\Gamma_u) * c^{&lt;{t-1}&gt;}
$$</p>
<blockquote>
<p>公式很好理解，如果 $\Gamma_u=1$，那么 $t$ 时刻 记忆细胞的值就等于候选值 $\tilde{c}$, 反之等于前一时刻记忆细胞的值.</p>
<p><strong>注</strong>：上面公式中的 * 表示元素之间进行乘法运算，而其他公式是 矩阵运算.</p>
</blockquote>
<p>下图给出了该公式很直观的解释：</p>
<blockquote>
<p>在读到 “cat” 时候，其他时候一直为 0，知道要输出 “was” 的时刻，我们知道 “cat” 的存在，也就知道它为单数</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-39_1.png&quot; width=&quot;550&quot; /&gt;</p>
</blockquote>
<p><strong>GRU 结构示意图</strong></p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-40_1.png&quot; width=&quot;550&quot; /&gt;</p>
<h3>4.3 完整版 GRU</h3>
<p>上面简化了 GRU，在完整版中还存在另一个符号 ，这符号的意义是控制 $\tilde{c}$ 和 $c^{&lt;{t-1}&gt;}$ 之间的联系强弱，完整版如下：</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-41_1.png&quot; width=&quot;550&quot; /&gt;</p>
<blockquote>
<p>注意，完整公式中多出了一个 $\Gamma_r$, 这个符号的作用是控制 $\tilde{c}^{&lt;{t}&gt;}$ 和 $c^{&lt;{t}&gt;}$ 之间联系的强弱.</p>
</blockquote>
<h2>5. LSTM - Long Short Term</h2>
<p>介绍完 GRU 后，再介绍 LSTM 会更加容易理解。</p>
<h3>5.1 GRU and LSTM</h3>
<p>GRU 只有两个门，而 LSTM 有三个门，分别是更新门 $\Gamma_u$ (是否需要更新为 $\tilde{c}^{&lt;{t}&gt;}$)，遗忘门 $\Gamma_f$ (是否需要丢弃上一个时刻的值)，输出门 $\Gamma_o$ (是否需要输出本时刻的值)</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-42_1.png&quot; width=&quot;650&quot; /&gt;</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-43_1.png&quot; width=&quot;650&quot; /&gt;</p>
<p>下图是 LSTM 的结构示意图：</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-44_1.png&quot; width=&quot;700&quot; /&gt;</p>
<h3>5.2 LSTM Structure</h3>
<p>&lt;img src=&quot;/images/deeplearning/RNN-03.png&quot; width=&quot;700&quot; alt=&quot;1997年, Sepp Hochreiter 和 Jürgen Schmidhuber&quot; /&gt;</p>
<p>LSTM 仍是 $x_t$ 和 $h_{t−1}$ 来计算 $h_t$，但对内部的结构进行了更加精心的设计，加入 <strong>3 Gate</strong> 和 <strong>1 memory_cell</strong>.</p>
<blockquote>
<ul>
<li>
<p>输入门 $\Gamma_u$： 控制 <strong>当前计算的新状态</strong> 以多大程度更新到记忆单元中 （也叫更新门）;</p>
</li>
<li>
<p>遗忘门 $\Gamma_f$： 控制 <strong>前一步记忆单元</strong> 中的信息有多大程度被遗忘掉;</p>
</li>
<li>
<p>输出门 $\Gamma_o$： 控制当前的输出有多大程度上取决于 <strong>当前的记忆单元</strong>;</p>
</li>
<li>
<p>记忆单元 memory cell $c_t$.
$$ h_{t}=o_{t} \odot \operatorname{Tanh}\left(c_{t}\right) $$</p>
</li>
</ul>
</blockquote>
<h3>5.3 Activation function</h3>
<p>在 LSTM 中, 关于 activation function 的选取：</p>
<ul>
<li>$\Gamma_f$、$\Gamma_i{u}$ 和 $\Gamma_o$ 使用 Sigmoid 函数作为激活函数；</li>
<li>在生成候选记忆时，使用 Tanh 作为激活函数。</li>
</ul>
<blockquote>
<p><strong>注</strong>： 这两个激活函数都是饱和的，也就是说在输入达到一定值的情况下，输出就不会发生明显变化了。如果是用非饱和的激活函数，例如 ReLU，那么将难以实现门控的效果。</p>
</blockquote>
<p>使用这个激活函数的原因如下：</p>
<blockquote>
<p>(1). Sigmoid 输出在 0～1 之间。且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关。</p>
<p>(2). 在生成候选记忆时，使用 Tanh 函数，是因为其输出在 −1~1 之间，这与大多数场景下特征分布是 0 中心的吻合。此外，Tanh 函数在输入为 0 附近相比 Sigmoid 函数有更大的梯度，通常使模型收敛更快。</p>
</blockquote>
<p>总而言之，LSTM 经历了 20 年的发展，其核心思想一脉相承，但各个组件都发生了很多演化。</p>
<p><strong>GRU vs LSTM</strong></p>
<ul>
<li>GRU 和 LSTM 的效果在很多任务上不分伯仲。</li>
<li>GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达效果更好，但计算量更大。</li>
</ul>
<blockquote>
<p>从结构上来说：</p>
<ul>
<li>GRU 只有两个门（update和reset），LSTM 有三个门（forget，input，output）</li>
<li>GRU 直接将 hidden state 传给下一个单元，而 LSTM 则用 memory cell 把hidden state 包装起来。</li>
</ul>
</blockquote>
<h2>6. Bidirectional RNN</h2>
<p>前面介绍的都是单向的 RNN 结构，在处理某些问题上得到的效果不尽人意</p>
<p>如下面两句话，我们要从中标出人名：</p>
<blockquote>
<p><code>He</code> said, &quot;Teddy Roosevelt was a great President&quot;.
<code>He</code> said, &quot;Teddy bears are on sale&quot;.</p>
</blockquote>
<ol>
<li>第一句中的 Teddy Roosevelt 是人名</li>
<li>第二句中的 Teddy bears 是泰迪熊，同样都是单词 <strong>Teddy</strong> 对应的输出在第一句中应该是 1，第二句中应该是 0</li>
</ol>
<p>像这样的例子如果想让我们的序列模型明白就需要借助不同的结构比如 - 双向递归神经网络(Bidirectional RNN).
该神经网络首先从正面理解一遍这句话，再从反方向理解一遍.</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-45_1.png&quot; width=&quot;750&quot; /&gt;</p>
<p>下图摘自大数据文摘整理</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-46_1.png&quot; width=&quot;750&quot; /&gt;</p>
<h2>7. Deep RNNs</h2>
<p>深层，顾名思义就是层次增加。如下图是深层循环神经网络的示意图</p>
<p>横向表示时间展开，纵向则是层次展开。</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-47_1.png&quot; width=&quot;750&quot; /&gt;</p>
<p>注意激活值的表达形式有所改变，以 $a^{[1]&lt;0&gt;}$ 为例进行解释：</p>
<ul>
<li>[1] 表示第一层</li>
<li>&lt;0&gt; 表示第一个激活值</li>
</ul>
<p>另外各个激活值的计算公式也略有不同，以 $a^{[2]&lt;3&gt;}$ 为例，其计算公式如下：</p>
<p>&lt;img src=&quot;/images/deeplearning/C5W1-48_1.png&quot; width=&quot;550&quot; /&gt;</p>
<h2>Reference</h2>
<ul>
<li><a href="https://book.douban.com/subject/30285146/" target="_blank" rel="noopener">《百面机器学习》</a></li>
<li><a href="http://www.iterate.site/2019/04/14/01-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">RNN and CNN</a></li>
<li><a href="http://www.iterate.site/2019/04/19/04-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">LSTM 长短期记忆网络</a></li>
</ul>

      
    </div>
	
    <!--
	
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
      </nav>
    

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Blair Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos" target="_blank" rel="noopener">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
