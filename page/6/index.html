<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Everyone should not forget his dream">
<meta property="og:type" content="website">
<meta property="og:title" content="Home">
<meta property="og:url" content="http://selfboot.org/page/6/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="Everyone should not forget his dream">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Home">
<meta name="twitter:description" content="Everyone should not forget his dream">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/ml">Learning</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://selfboot.org"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <br>
    <section id="main" class="outer">
      <article id="post-spark-machine-learning-01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/04/25/spark-machine-learning-01/"><strong>Spark Machine Learning 01 - Spark编程入门</strong></a>
      <small class=article-date-index>&nbsp; 2016-04-25</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/04/25/spark-machine-learning-01/" class="article-date">
  <time datetime="2016-04-25T02:07:21.000Z" itemprop="datePublished">2016-04-25</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      介绍 Spark 的环境搭建与运行, 接触了 RDD 与 SparkContext, 启动 Spark-Shell 以及如何使用 Scala、Python 编写 Spark 程序. <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/machine-learning/">machine-learning</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/04/25/spark-machine-learning-01/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>Apache Spark</strong></p>
<p>简化了海量数据的存储(HDFS) 和 计算 (MR–在集群多个节点进行并行计算的框架) 流程。MapReduce 缺点, 如: 启动任务时的高开销、对中间数据 和 计算结果 写入磁盘的依赖。这使得 Hadoop 不适合 <strong>迭代式</strong> 或 <strong>低延迟</strong> 的任务。</p>
<p>机器学习 算法并非为并行架构而设计。 机器学习模型一般具有迭代式的特性, 这与 Spark 的设计目标一致。并行计算框架 很少能 兼顾 速度、扩展性、内存处理、容错性的同时，还提供灵活、表达力丰富的 API。<code>Spark</code> 全新的分布式计算框架, 注重 : 低延迟任务的优化、并将中间数据和结果保存在内存中。Spark 提供函数式API，并兼容 Hadoop Ecosystem</p>
<p>Spark 提供了对 Scala、Java、Python 语言的原生 API。</p>
<p>Spark 框架对 资源调度、任务提交、执行、跟踪， 节点间通信以及数据并行处理的内在底层操作都进行了抽象。它提供了高级别 API 用于处理分布式数据。</p>
<p><strong>Spark 的四种运行模式</strong></p>
<ol>
<li>本地单机模式 – Spark 进程 all run in One JVM</li>
<li>集群单机模式 – 使用 Spark 自己内置的 任务调度框架</li>
<li>基于 Mesos</li>
<li>基于 YARN</li>
</ol>
<h2 id="1-Spark运行"><a href="#1-Spark运行" class="headerlink" title="1. Spark运行"></a>1. Spark运行</h2><p>运行示例程序来测试是否一切正常：</p>
<blockquote>
<p>./bin/run-example org.apache.spark.examples.SparkPi</p>
</blockquote>
<p>该命令将在本地单机模式下执行SparkPi这个示例。在该模式下，所有的Spark进程均运行于同一个JVM中，而并行处理则通过多线程来实现。默认情况下，该示例会启用与本地系统的CPU核心数目相同的线程。</p>
<p>要在本地模式下设置并行的级别，以local[N]的格式来指定一个master变量即可。比如只使用两个线程时，可输入如下命令：</p>
<blockquote>
<p>MASTER=local[2] ./bin/run-example org.apache.spark.examples.SparkPi</p>
</blockquote>
<h2 id="2-Spark集群"><a href="#2-Spark集群" class="headerlink" title="2. Spark集群"></a>2. Spark集群</h2><p>Spark集群由两类程序构成：一个驱动程序和多个执行程序。本地模式时所有的处理都运行在同一个JVM内，而在集群模式时它们通常运行在不同的节点上。</p>
<p>举例来说，一个采用单机模式的Spark集群包括：</p>
<ol>
<li>一个运行Spark单机主进程和驱动程序的主节点；</li>
<li>各自运行一个执行程序进程的多个工作节点。</li>
</ol>
<p>比如在一个Spark单机集群上运行，只需传入主节点的URL即可：</p>
<blockquote>
<p>MASTER=spark://IP:PORT ./bin/run-example org.apache.spark.examples.SparkPi<br>其中的IP和PORT分别是主节点IP地址和端口号。这是告诉Spark让示例程序运行在主节点所对应的集群上</p>
</blockquote>
<p>(？ 貌似和单机效果一样)</p>
<h2 id="3-Spark编程模型"><a href="#3-Spark编程模型" class="headerlink" title="3. Spark编程模型"></a>3. Spark编程模型</h2><h3 id="3-1-SparkContext类"><a href="#3-1-SparkContext类" class="headerlink" title="3.1 SparkContext类"></a>3.1 SparkContext类</h3><p><strong>SparkContext类与SparkConf类</strong></p>
<p>任何Spark程序的编写都是从SparkContext开始的。SparkContext的初始化需要一个SparkConf对象，后者包含了Spark集群配置的各种参数（比如主节点的URL）。</p>
<p>初始化后，我们便可用SparkContext对象所包含的各种方法来创建和操作RDD。Spark shell（在Scala和Python下可以，但不支持Java）能自动完成上述初始化。若要用Scala代码来实现的话，可参照下面的代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Test Spark App"</span>).setMaster(<span class="string">"local[4]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>
<p>这段代码会创建一个4线程的SparkContext对象，并将其相应的任务命名为Test Spark APP。我们也可通过如下方式调用SparkContext的简单构造函数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local[4]"</span>, <span class="string">"Test Spark App"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-Spark-shell"><a href="#3-2-Spark-shell" class="headerlink" title="3.2 Spark shell"></a>3.2 Spark shell</h3><p>Spark支持 用 Scala or Python REPL（Read-Eval-Print-Loop，即交互式shell）来进行交互式的程序编写。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/spark-shell</span><br></pre></td></tr></table></figure>
<p>会启动Scala shell 并初始化一个SparkContext对象。我们可以通过 sc这个Scala值来调用这个对象</p>
<h3 id="3-3-RDD"><a href="#3-3-RDD" class="headerlink" title="3.3 RDD"></a>3.3 RDD</h3><p>一个 RDD 代表一系列的“记录”（严格来说，某种类型的对象）。<br>这些记录被分配或分区到一个集群的多个节点上（在本地模式下，可以类似地理解为单个进程里的多个线程上）。</p>
<p>Spark中的RDD具备容错性，即当某个节点或任务失败时（因非用户代码原因而引起，如硬件故障、网络不通等），RDD会在余下的节点上自动重建，以便任务能最终完成。</p>
<p><strong>1. 创建RDD</strong></p>
<p>RDD可从现有的集合创建 ：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> collection = <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>)</span><br><span class="line"><span class="keyword">val</span> rddFromCollection = sc.parallelize(collection)</span><br></pre></td></tr></table></figure>
<p>RDD也可以基于Hadoop的输入源创建，比如本地文件系统、HDFS。基于Hadoop的RDD可以使用任何实现了Hadoop InputFormat接口的输入格式，包括文本文件、其他Hadoop标准格式、HBase等。以下举例说明如何用一个本地文件系统里的文件创建RDD：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val rddFromTextFile = sc.textFile(&quot;LICENSE&quot;)</span><br></pre></td></tr></table></figure>
<p>上述代码中的textFile函数（方法）会返回一个RDD对象。该对象的每一条记录都是一个表示文本文件中某一行文字的String（字符串）对象。</p>
<p><strong>2. Spark操作</strong></p>
<p>在Spark编程模式下，所有的操作被分为 <code>transformation</code> 和 <code>action</code> 两种。</p>
<p><strong>transformation</strong> 操作是对一个数据集里的所有记录执行某种函数，从而使记录发生改变；</p>
<p><strong>action</strong> 通常是运行某些计算或聚合操作，并将结果返回运行 SparkContext 的那个驱动程序。</p>
<p>Spark 的操作通常采用<code>函数式</code>风格。</p>
<p>Spark程序中最常用的转换操作便是map操作。该操作对一个RDD里的每一条记录都执行某个函数，从而将输入映射成为新的输出。</p>
<p>比如，下面这段代码便对一个从本地文本文件创建的RDD进行操作。它对该RDD中的每一条记录都执行size函数。<br>创建一个这样的由若干String构成的RDD对象。通过map函数，我们将每一个字符串都转换为一个整数，从而返回一个由若干Int构成的RDD对象。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; rddFromTextFile.count</span><br><span class="line">res2: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> intsFromStringsRDD = rddFromTextFile.map(line =&gt; line.size)</span><br><span class="line">intsFromStringsRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; intsFromStringsRDD.count</span><br><span class="line">res3: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sumOfRecords = intsFromStringsRDD.sum</span><br><span class="line">sumOfRecords: <span class="type">Double</span> = <span class="number">17062.0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> numRecords = intsFromStringsRDD.count</span><br><span class="line">numRecords: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> aveLengthOfRecord = sumOfRecords / numRecords</span><br><span class="line">aveLengthOfRecord: <span class="type">Double</span> = <span class="number">58.034013605442176</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价于</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> aveLengthOfRecordChained = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</span><br></pre></td></tr></table></figure>
<blockquote>
<p>示例中 <strong>=&gt;</strong> 是Scala下表示匿名函数的语法。语法 <strong>line =&gt; line.size</strong> 表示以 <strong>=&gt;</strong> 操作符左边的部分作为输入，对其执行一个函数，并以 <strong>=&gt;</strong> 操作符右边代码的执行结果为输出。在这个例子中，输入为line，输出则是 <strong>line.size</strong> 函数的执行结果。在Scala语言中，这种将一个String对象映射为一个Int的函数被表示为String =&gt; Int。</p>
</blockquote>
<p>Spark的大多数操作都会返回一个新RDD，但多数的Action操作则是返回计算的结果</p>
<blockquote>
<p>注 : Spark 中的转换操作是延后的。也就是说，在RDD上调用一个转换操作并不会立即触发相应的计算。 只有必要时才计算结果并将其返回给驱动程序，从而提高了Spark的效率。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> transformedRDD = rddFromTextFile.map(line =&gt; line.size).</span><br><span class="line">     | filter(size =&gt; size &gt; <span class="number">10</span>).map(size =&gt; size * <span class="number">2</span>)</span><br><span class="line">transformedRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at map at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<p>没有触发任何计算，也没有结果被返回。<br>如果我们现在在新的RDD上调用一个执行操作，比如sum，该计算将会被触发：</p>
<p><strong><em>触发计算</em></strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> computation = transformedRDD.sum</span><br><span class="line">computation: <span class="type">Double</span> = <span class="number">34106.0</span></span><br></pre></td></tr></table></figure>
<p><strong>3. RDD缓存策略</strong></p>
<p>Spark最为强大的功能之一便是能够把数据缓存在集群的内存里。这通过调用RDD的cache函数来实现：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; rddFromTextFile.cache</span><br><span class="line">res4: rddFromTextFile.<span class="keyword">type</span> = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> aveLengthOfRecordChainedFromCached = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</span><br><span class="line">aveLengthOfRecordChainedFromCached: <span class="type">Double</span> = <span class="number">58.034013605442176</span></span><br></pre></td></tr></table></figure>
<p>在RDD首次调用一个执行操作时，这个操作对应的计算会立即执行，数据会从数据源里读出并保存到内存。因此，首次调用cache函数所需要的时间会部分取决于Spark从输入源读取数据所需要的时间。但是，当下一次访问该数据集的时候，数据可以直接从内存中读出从而减少低效的I/O操作，加快计算。多数情况下，这会取得数倍的速度提升。</p>
<blockquote>
<p>Spark支持更为细化的缓存策略。通过persist函数可以指定Spark的数据缓存策略。关于RDD缓存的更多信息可参见：<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence。</a></p>
</blockquote>
<h3 id="3-4-广播变量和累加器"><a href="#3-4-广播变量和累加器" class="headerlink" title="3.4 广播变量和累加器"></a>3.4 广播变量和累加器</h3><p>Spark的另一个核心功能是能创建两种特殊类型的变量：<strong>广播变量</strong> 和 累加器。</p>
<p>广播变量（broadcast variable）为只读变量，它由运行SparkContext的驱动程序创建后发送给会参与计算的节点。对那些需要让各工作节点高效地访问相同数据的应用场景，比如机器学习，这非常有用。</p>
<p>Spark下创建广播变量只需在SparkContext上调用一个方法即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val broadcastAList = sc.broadcast(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;))</span><br><span class="line">broadcastAList: org.apache.spark.broadcast.Broadcast[List[String]] = Broadcast(11)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<p><strong>广播变量</strong> 也可以被非驱动程序所在的节点（即工作节点）访问，访问的方法是调用该变量的<code>value</code>方法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastAList = sc.broadcast(<span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>))</span><br><span class="line">broadcastAList: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">List</span>[<span class="type">String</span>]] = <span class="type">Broadcast</span>(<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(<span class="type">List</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)).map(x =&gt; broadcastAList.value ++ x).collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">List</span>[<span class="type">Any</span>]] = <span class="type">Array</span>(<span class="type">List</span>(a, b, c, d, e, <span class="number">1</span>), <span class="type">List</span>(a, b, c, d, e, <span class="number">2</span>), <span class="type">List</span>(a, b, c, d, e, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，collect 函数一般仅在的确需要将整个结果集返回驱动程序并进行后续处理时才有必要调用。如果在一个非常大的数据集上调用该函数，可能耗尽驱动程序的可用内存，进而导致程序崩溃。</p>
</blockquote>
<p>高负荷的处理应尽可能地在整个集群上进行，从而避免驱动程序成为系统瓶颈。然而在不少情况下，将结果收集到驱动程序的确是有必要的。很多机器学习算法的迭代过程便属于这类情况。</p>
<p><strong>累加器</strong>（accumulator）也是一种被广播到工作节点的变量。累加器与广播变量的关键不同，是后者只能读取而前者却可累加。</p>
<blockquote>
<p>关于累加器的更多信息，可参见《Spark编程指南》：<a href="http://spark.apache.org/docs/latest/programming-guide.html#shared-variables。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#shared-variables。</a></p>
</blockquote>
<h2 id="4-Spark-Scala-编程入门"><a href="#4-Spark-Scala-编程入门" class="headerlink" title="4. Spark Scala 编程入门"></a>4. Spark Scala 编程入门</h2><p><a href="https://github.com/blair1/spark/tree/master/Spark-Machine-Learning_8519OSCode/Chapter%2001/scala-spark-app" target="_blank" rel="external">scala-spark-app</a></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * A simple Spark app in Scala</span><br><span class="line"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local[2]"</span>, <span class="string">"First Spark App"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)</span></span><br><span class="line">    <span class="keyword">val</span> data = sc.textFile(<span class="string">"data/UserPurchaseHistory.csv"</span>)</span><br><span class="line">      .map(line =&gt; line.split(<span class="string">","</span>))</span><br><span class="line">      .map(purchaseRecord =&gt; (purchaseRecord(<span class="number">0</span>), purchaseRecord(<span class="number">1</span>), purchaseRecord(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's count the number of purchases</span></span><br><span class="line">    <span class="keyword">val</span> numPurchases = data.count()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's count how many unique users made purchases</span></span><br><span class="line">    <span class="keyword">val</span> uniqueUsers = data.map &#123; <span class="keyword">case</span> (user, product, price) =&gt; user &#125;.distinct().count()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's sum up our total revenue</span></span><br><span class="line">    <span class="keyword">val</span> totalRevenue = data.map &#123; <span class="keyword">case</span> (user, product, price) =&gt; price.toDouble &#125;.sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's find our most popular product</span></span><br><span class="line">    <span class="keyword">val</span> productsByPopularity = data</span><br><span class="line">      .map &#123; <span class="keyword">case</span> (user, product, price) =&gt; (product, <span class="number">1</span>) &#125;</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">      .collect()</span><br><span class="line">      .sortBy(-_._2)</span><br><span class="line">    <span class="keyword">val</span> mostPopular = productsByPopularity(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// finally, print everything out</span></span><br><span class="line">    println(<span class="string">"Total purchases: "</span> + numPurchases)</span><br><span class="line">    println(<span class="string">"Unique users: "</span> + uniqueUsers)</span><br><span class="line">    println(<span class="string">"Total revenue: "</span> + totalRevenue)</span><br><span class="line">    println(<span class="string">"Most popular product: %s with %d purchases"</span>.format(mostPopular._1, mostPopular._2))</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-Spark-Java-编程入门"><a href="#5-Spark-Java-编程入门" class="headerlink" title="5. Spark Java 编程入门"></a>5. Spark Java 编程入门</h2><p>Java API与Scala API本质上很相似。Scala代码可以很方便地调用Java代码，但某些Scala代码却无法在Java里调用，特别是那些使用了隐式类型转换、默认参数和采用了某些Scala反射机制的代码。</p>
<p>SparkContext有了对应的Java版本JavaSparkContext，而RDD则对应JavaRDD。<br>Spark提供对Java 8匿名函数（lambda）语法的支持。</p>
<p>用Scala编写时，键/值对记录的RDD能支持一些特别的操作（比如reduceByKey和saveAsSequenceFile）。这些操作可以通过隐式类型转换而自动被调用。用Java编写时，则需要特别类型的JavaRDD来支持这些操作。它们包括用于键/值对的JavaPairRDD，以及用于数值记录的JavaDoubleRDD。</p>
<p>Java 8 RDD以及Java 8 lambda表达式更多信息可参见《Spark编程指南》：<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations。</a></p>
<h2 id="6-Spark-Python-编程入门"><a href="#6-Spark-Python-编程入门" class="headerlink" title="6. Spark Python 编程入门"></a>6. Spark Python 编程入门</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="string">"""用Python编写的一个简单Spark应用"""</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line">sc = SparkContext(<span class="string">"local[2]"</span>, <span class="string">"First Spark App"</span>)</span><br><span class="line"><span class="comment"># 将CSV格式的原始数据转化为(user,product,price)格式的记录集</span></span><br><span class="line">data = sc.textFile(<span class="string">"data/UserPurchaseHistory.csv"</span>).map(<span class="keyword">lambda</span> line:</span><br><span class="line">line.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> record: (record[<span class="number">0</span>], record[<span class="number">1</span>], record[<span class="number">2</span>]))</span><br><span class="line"><span class="comment"># 求总购买次数</span></span><br><span class="line">numPurchases = data.count()</span><br><span class="line"><span class="comment"># 求有多少不同客户购买过商品</span></span><br><span class="line">uniqueUsers = data.map(<span class="keyword">lambda</span> record: record[<span class="number">0</span>]).distinct().count()</span><br><span class="line"><span class="comment"># 求和得出总收入</span></span><br><span class="line">totalRevenue = data.map(<span class="keyword">lambda</span> record: float(record[<span class="number">2</span>])).sum()</span><br><span class="line"><span class="comment"># 求最畅销的产品是什么</span></span><br><span class="line">products = data.map(<span class="keyword">lambda</span> record: (record[<span class="number">1</span>], <span class="number">1.0</span>)).</span><br><span class="line">reduceByKey(<span class="keyword">lambda</span> a, b: a + b).collect()</span><br><span class="line">mostPopular = sorted(products, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Total purchases: %d"</span> % numPurchases</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Unique users: %d"</span> % uniqueUsers</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Total revenue: %2.2f"</span> % totalRevenue</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Most popular product: %s with %d purchases"</span> % (mostPopular[<span class="number">0</span>], mostPopular[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>匿名函数在Python语言中亦称lambda函数，lambda也是语法表达上的关键字。</p>
<p>用Scala编写时，一个将输入x映射为输出y的匿名函数表示为x =&gt; y，而在Python中则是lambda x : y。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  python-spark-app git:(master) ✗ <span class="built_in">pwd</span></span><br><span class="line">/Users/hp/ghome/hadoop-spark/spark/Spark-Machine-Learning_8519OSCode/Chapter01/python-spark-app</span><br><span class="line">➜  python-spark-app git:(master) ✗ <span class="variable">$SPARK_HOME</span>/bin/spark-submit pythonapp.py</span><br><span class="line">Using Spark<span class="string">'s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">16/08/26 15:56:02 INFO SparkContext: Running Spark version 1.5.2</span><br><span class="line">...</span><br><span class="line">Total purchases: 5</span><br><span class="line">Unique users: 4</span><br><span class="line">Total revenue: 39.91</span><br><span class="line">Most popular product: iPhone Cover with 2 purchases</span><br><span class="line">16/08/26 15:56:07 INFO SparkUI: Stopped Spark web UI at http://192.168.143.84:4040</span><br><span class="line">...</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Spark的Python API几乎覆盖了所有Scala API所能提供的功能. 但的确有些特性，比如Spark Streaming和个别的API方法，暂不支持。<br><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">具体参见《Spark编程指南》的Python部分</a></p>
</blockquote>
<h2 id="7-小结"><a href="#7-小结" class="headerlink" title="7. 小结"></a>7. 小结</h2><p>体会了 函数式 编程的威力， scala、python 都可以。java 不适合写 spark 程序</p>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-centos7-install-common-software" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/04/19/centos7-install-common-software/"><strong>Centos 7.1 install Common software</strong></a>
      <small class=article-date-index>&nbsp; 2016-04-19</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/04/19/centos7-install-common-software/" class="article-date">
  <time datetime="2016-04-18T23:54:16.000Z" itemprop="datePublished">2016-04-19</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      install ifconfig、vim、wget、git、netcat ... <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/devops/">devops</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/04/19/centos7-install-common-software/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-install-ifconfig"><a href="#1-install-ifconfig" class="headerlink" title="1. install ifconfig"></a>1. install ifconfig</h2> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum search ifconfig</span><br><span class="line">yum install net-tools.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="2-install-vim"><a href="#2-install-vim" class="headerlink" title="2. install vim"></a>2. install vim</h2> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum search vim</span><br><span class="line">yum install vim-enhanced</span><br></pre></td></tr></table></figure>
<h2 id="3-install-wget"><a href="#3-install-wget" class="headerlink" title="3. install wget"></a>3. install wget</h2> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> [libin@centos-linux-1 x]$ yum search wget</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * base: mirrors.skyshe.cn</span><br><span class="line"> * extras: mirrors.163.com</span><br><span class="line"> * updates: mirrors.163.com</span><br><span class="line">============================================================================================ N/S matched: wget =============================================================================================</span><br><span class="line">wget.x86_64 : A utility <span class="keyword">for</span> retrieving files using the HTTP or FTP protocols</span><br><span class="line"></span><br><span class="line">  Name and summary matches only, use <span class="string">"search all"</span> <span class="keyword">for</span> everything.</span><br><span class="line"> </span><br><span class="line"> [libin@centos-linux-1 x]$ yum install wget.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="4-install-git"><a href="#4-install-git" class="headerlink" title="4. install git"></a>4. install git</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum search git</span><br><span class="line">yum install git.x86_64</span><br></pre></td></tr></table></figure>
<p>default，git havn’t color, you can use under cmd give git add color</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git config --global color.status auto </span><br><span class="line">$ git config --global color.diff auto </span><br><span class="line">$ git config --global color.branch auto </span><br><span class="line">$ git config --global color.interactive auto</span><br></pre></td></tr></table></figure>
<h2 id="5-install-netcat"><a href="#5-install-netcat" class="headerlink" title="5. install netcat"></a>5. install netcat</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum search netcat</span><br><span class="line">yum install nmap-ncat.x86_64</span><br></pre></td></tr></table></figure>
      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-ops-centos-and-redhat" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/04/19/ops-centos-and-redhat/"><strong>CentOS and RedHat Linux</strong></a>
      <small class=article-date-index>&nbsp; 2016-04-19</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/04/19/ops-centos-and-redhat/" class="article-date">
  <time datetime="2016-04-18T23:54:16.000Z" itemprop="datePublished">2016-04-19</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      CentOS and RedHat Linux <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/devops/">devops</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/04/19/ops-centos-and-redhat/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>CentOS 是 Community ENTerprise Operating System 的简称，我们有很多人叫它社区企业操作系统，不管你怎么叫它，它都是 Linux操作系统的一个发行版本。CentOS 与 RHEL 有什么区别呢。</p>
</blockquote>
<h2 id="1-CentOS-简介"><a href="#1-CentOS-简介" class="headerlink" title="1. CentOS 简介"></a>1. CentOS 简介</h2><p>CentOS是Community ENTerprise Operating System的简称，我们有很多人叫它社区企业操作系统，不管你怎么叫它，它都是Linux操作系统的一个发行版本。</p>
<p>CentOS并不是全新的Linux发行版。在Red Hat家族中有企业版的产品，它是Red Hat Enterprise Linux（以下称之为RHEL），CentOS正是这个RHEL的克隆版本。</p>
<p>RHEL是很多企业采用的Linux发行版本，需要向Red Hat付费才可以使用，并能得到付过费用的服务和技术支持和版本升级。</p>
<p>CentOS可以像RHEL一样的构筑Linux系统环境，但不需要向Red Hat付任何的产品和服务费用，同时也得不到任何有偿技术支持和升级服务。</p>
<p>在构成RHEL的大多数软件包中，都是基于GPL协议发布的，也就是我们常说的开源软件。正因为是这样，Red Hat公司也遵循这个协议，将构成RHEL的软件包公开发布，只要是遵循GPL协议，任何人都可以在原有的软件构成的基础上再开发和发布。<strong>CentOS</strong> 就是这样在 RHEL 发布的基础上将RHEL的构成克隆再现的一个Linux发行版本。</p>
<p>虽然说是RHEL的克隆，但并不是一模一样，所说的克隆是具有100%的互换性（真的么？）。但并不保障对应RHEL的软件在CentOS上面也能够100%的正常工作。并且安全漏洞的修正和软件包的升级对应RHEL的有偿服务和技术支持来说，数日数星期数个月的延迟情况也有（其实也没看出来多慢）。</p>
<h2 id="2-CentOS-特点"><a href="#2-CentOS-特点" class="headerlink" title="2. CentOS 特点"></a>2. CentOS 特点</h2><p>在CentOS的全称里面我们可以看到Enterprise OS，也就是说企业系统，这个企业系统并不是企业级别的系统，而是它可以提供企业级应用所需要的要素。<br>例如：</p>
<ol>
<li>稳定的环境</li>
<li>长期的升级更新支持</li>
<li>保守性强</li>
<li>大规模的系统也能够发挥很好的性能</li>
</ol>
<h2 id="3-CentOS-与-RHEL-的区别"><a href="#3-CentOS-与-RHEL-的区别" class="headerlink" title="3. CentOS 与 RHEL 的区别"></a>3. CentOS 与 RHEL 的区别</h2><p>其实为什么有 CentOS？ CentOS 与 RHEL 有什么关系？</p>
<p>RHEL 在发行的时候，有两种方式。一种是二进制的发行方式，另外一种是源代码的发行方式。</p>
<p>无论是哪一种发行方式，你都可以免费获得（例如从网上下载），并再次发布。但如果你使用了他们的在线升级（包括补丁）或咨询服务，就必须要付费。</p>
<p>RHEL 一直都提供源代码的发行方式，CentOS 就是将 RHEL 发行的源代码从新编译一次，形成一个可使用的二进制版本。由于 LINUX 的源代码是 GNU，所以从获得 RHEL 的源代码到编译成新的二进制，都是合法。只是 REDHAT 是商标，所以必须在新的发行版里将 REDHAT 的商标去掉。</p>
<p>REDHAT 对这种发行版的态度是：“我们其实并不反对这种发行版，真正向我们付费的用户，他们重视的并不是系统本身，而是我们所提供的商业服务。”</p>
<p>一句话，选用 CentOS 还是 RHEL，取决于你所在公司是否拥有相应的技术力量。</p>
<h2 id="4-Refence-article"><a href="#4-Refence-article" class="headerlink" title="4. Refence article"></a>4. Refence article</h2><p>本文整理自网络文章</p>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-sbt-hello" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/03/16/sbt-hello/"><strong>SBT Hello</strong></a>
      <small class=article-date-index>&nbsp; 2016-03-16</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/03/16/sbt-hello/" class="article-date">
  <time datetime="2016-03-15T23:54:16.000Z" itemprop="datePublished">2016-03-16</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      This is a brief introduction about SBT and how to use it. <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/devops/">devops</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/03/16/sbt-hello/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>focus on :</strong></p>
<ol>
<li>什么是 SBT ?</li>
<li>SBT 项目工程目录</li>
<li>SBT 编译打包 Scala HelloWorld</li>
</ol>
<h2 id="1-SBT-What"><a href="#1-SBT-What" class="headerlink" title="1. SBT, What?"></a>1. SBT, What?</h2><p>SBT 是 Simple Build Tool 的简称. SBT 可以认为是 Scala 世界的 maven。</p>
<p>SBT的着迷特性，比如：</p>
<ol>
<li>DSL build构建, 并可混合构建 Java 和 Scala 项目；</li>
<li>通过触发执行 (trigger execution) 特性支持持续的编译与测试；</li>
<li>可以重用 Maven 或者 ivy的repository 进行依赖管理；</li>
<li>增量编译、并行任务等等…</li>
</ol>
<h2 id="2-Hello-SBT"><a href="#2-Hello-SBT" class="headerlink" title="2. Hello, SBT"></a>2. Hello, SBT</h2><p>一个极致简单的 Scala项目 （hello simple project）</p>
<p>hello/HelloWorld.scala</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        println(<span class="string">"Hello, SBT"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>sbt run</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  hello git:(master) ✗ sbt</span><br><span class="line">[info] Set current project to hello (in build file:/Users/hp/ghome/Spark-Scala/hello/)</span><br><span class="line">&gt; run</span><br><span class="line">[info] Updating &#123;file:/Users/hp/ghome/Spark-Scala/hello/&#125;hello...</span><br><span class="line">[info] Resolving org.fusesource.jansi#jansi;1.4 ...</span><br><span class="line">[info] Done updating.</span><br><span class="line">[info] Compiling 1 Scala source to /Users/hp/ghome/Spark-Scala/hello/target/scala-2.10/classes...</span><br><span class="line">[info] Running HelloWorld</span><br><span class="line">Hello, SBT</span><br><span class="line">[success] Total time: 3 s, completed 2016-3-17 9:38:44</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
<h2 id="3-SBT-项目工程结构详解"><a href="#3-SBT-项目工程结构详解" class="headerlink" title="3. SBT 项目工程结构详解"></a>3. SBT 项目工程结构详解</h2><p>一个典型的SBT项目工程结构如下图所示：</p>
<p><img src="https://segmentfault.com/img/bVtyRb" alt="图片描述"></p>
<p><strong> build.sbt 详解 </strong></p>
<p>build.sbt 相当于 maven-pom.xml，它是build定义文件。 </p>
<p>SBT 运行 使用 2 种形式 的 build 定义文件，</p>
<ol>
<li>one, put your project’s base directory，– build.sbt， a simple build definition； </li>
<li>other one, put project directory，can Use Scala language, more expressive。</li>
</ol>
<p>一个简单的build.sbt文件内容如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">name := <span class="string">"hello"</span>      <span class="comment">// 项目名称</span></span><br><span class="line"></span><br><span class="line">organization := <span class="string">"xxx.xxx.xxx"</span>  <span class="comment">// 组织名称</span></span><br><span class="line"></span><br><span class="line">version := <span class="string">"0.0.1-SNAPSHOT"</span>  <span class="comment">// 版本号</span></span><br><span class="line"></span><br><span class="line">scalaVersion := <span class="string">"2.9.2"</span>   <span class="comment">// 使用的Scala版本号</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其它build定义</span></span><br></pre></td></tr></table></figure>
<p> name 和 version的定义是必须的，因为如果想生成jar包的话，这两个属性的值将作为jar包名称的一部分, 各行之间以空行分隔。<br>除了定义以上项目相关信息，我们还可以在build.sbt中添加项目依赖：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 添加源代码编译或者运行期间使用的依赖</span><br><span class="line">libraryDependencies += &quot;ch.qos.logback&quot; % &quot;logback-core&quot; % &quot;1.0.0&quot;</span><br><span class="line"></span><br><span class="line">libraryDependencies += &quot;ch.qos.logback&quot; % &quot;logback-classic&quot; % &quot;1.0.0&quot;</span><br><span class="line"></span><br><span class="line">// 或者</span><br><span class="line"></span><br><span class="line">libraryDependencies ++= Seq(</span><br><span class="line">                            &quot;ch.qos.logback&quot; % &quot;logback-core&quot; % &quot;1.0.0&quot;,</span><br><span class="line">                            &quot;ch.qos.logback&quot; % &quot;logback-classic&quot; % &quot;1.0.0&quot;,</span><br><span class="line">                            ...</span><br><span class="line">                            )</span><br><span class="line"></span><br><span class="line">// 添加测试代码编译或者运行期间使用的依赖</span><br><span class="line">libraryDependencies ++= Seq(&quot;org.scalatest&quot; %% &quot;scalatest&quot; % &quot;1.8&quot; % &quot;test&quot;)</span><br></pre></td></tr></table></figure>
<p>当然， build.sbt文件中还可以定义很多东西，比如添加插件，声明额外的repository，声明各种编译参数等等</p>
<p><strong> project目录即相关文件介绍 </strong></p>
<p>project目录下的几个文件可以根据情况添加。</p>
<p>build.properties 文件声明使用的要使用哪个版本的SBT来编译当前项目， 最新的sbt boot launcher可以能够兼容编译所有0.10.x版本的SBT构建项目，比如如果我使用的是0.12版本的sbt，但却想用0.11.3版本的sbt来编译当前项目，则可以在build.properties文件中添加sbt.version=0.11.3来指定。</p>
<p>plugins.sbt 文件用来声明当前项目希望使用哪些插件来增强当前项目使用的sbt的功能，比如像assembly功能，清理ivy local cache功能，都有相应的sbt插件供使用， 要使用这些插件只需要在 plugins.sbt 中声明即可.</p>
<p>为了能够成功加载这些sbt插件，我们将他们的查找位置添加到resolovers当中.</p>
<p><strong> 其他 </strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ touch build.sbt</span><br><span class="line">$ mkdir src</span><br><span class="line">$ mkdir src/main</span><br><span class="line">$ mkdir src/main/java</span><br><span class="line">$ mkdir src/main/resources</span><br><span class="line">$ mkdir src/main/scala</span><br><span class="line">$ mkdir src/test</span><br><span class="line">$ mkdir src/test/java</span><br><span class="line">$ mkdir src/test/resources</span><br><span class="line">$ mkdir src/test/scala</span><br><span class="line">$ mkdir project</span><br><span class="line">$ ...</span><br></pre></td></tr></table></figure>
<p>可以使用giter8来自动化以上步骤.<br>giter8的更多信息可参考<a href="https://github.com//giter8" target="_blank" rel="external">https://github.com//giter8</a>.</p>
<h2 id="4-SBT-Cmd"><a href="#4-SBT-Cmd" class="headerlink" title="4. SBT Cmd"></a>4. SBT Cmd</h2><ol>
<li>actions – 显示对当前工程可用的命令</li>
<li>update – 下载依赖</li>
<li>compile – 编译代码</li>
<li>test – 运行测试代码</li>
<li>package – 创建一个可发布的jar包</li>
<li>publish-local – 把构建出来的jar包安装到本地的ivy缓存</li>
<li>publish – 把jar包发布到远程仓库（如果配置了的话)</li>
</ol>
<p>more cmd</p>
<ol>
<li>test-failed – 运行失败的spec</li>
<li>test-quick – 运行所有失败的以及/或者是由依赖更新的spec</li>
<li>clean-cache – 清除所有的sbt缓存。类似于sbt的clean命令</li>
<li>clean-lib – 删除lib_managed下的所有内容</li>
</ol>
<h2 id="5-Scala-HelloWorld"><a href="#5-Scala-HelloWorld" class="headerlink" title="5. Scala HelloWorld"></a>5. Scala HelloWorld</h2><p>SBT Scala HelloWorld 具体请看 : <a href="https://github.com/blair1/language/tree/master/scala/ScalaWorld" target="_blank" rel="external">Scala-Projects/HelloWorld</a></p>
<p>➜  HelloWorld&gt; sbt package</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[info] Loading project definition from /Users/hp/spark/HelloWorld/project</span><br><span class="line">[info] Set current project to HelloWorld (in build file:/Users/hp/spark/HelloWorld/)</span><br><span class="line">[info] Packaging /Users/hp/spark/HelloWorld/target/scala-2.11/helloworld_2.11-0.0.1-SNAPSHOT.jar ...</span><br><span class="line">[info] Done packaging.</span><br><span class="line">[success] Total time: 1 s, completed 2016-3-17 9:05:44</span><br></pre></td></tr></table></figure>
<p>➜  HelloWorld&gt; sbt run</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[info] Loading project definition from /Users/hp/spark/HelloWorld/project</span><br><span class="line">[info] Set current project to HelloWorld (in build file:/Users/hp/spark/HelloWorld/)</span><br><span class="line">[info] Running Hi</span><br><span class="line">Hi!</span><br><span class="line">[success] Total time: 1 s, completed 2016-3-17 9:07:43</span><br></pre></td></tr></table></figure>
<h2 id="6-Spark-HelloWorld"><a href="#6-Spark-HelloWorld" class="headerlink" title="6. Spark HelloWorld"></a>6. Spark HelloWorld</h2><p>Spark HelloWorld 具体请看 : <a href="https://github.com/blair1/hadoop-spark/tree/master/spark/HelloWorld" target="_blank" rel="external">Spark-Projects/HelloWorld</a></p>
<p>➜  HelloWorld&gt; sbt compile<br>➜  HelloWorld&gt; sbt package</p>
<pre><code class="shell">$SPARK_HOME/bin/spark-submit \
  --class &quot;HelloWorld&quot; \
    target/scala-2.11/helloworld_2.11-1.0.jar
</code></pre>
<h2 id="7-Referenced-article"><a href="#7-Referenced-article" class="headerlink" title="7. Referenced article"></a>7. Referenced article</h2><p>参考 : <a href="http://www.scala-sbt.org/0.13/docs/zh-cn/Getting-Started.html" target="_blank" rel="external">scala-sbt.org/0.13/docs/zh-cn/Getting-Started.html</a><br>参考 : <a href="https://github.com/CSUG/real_world_scala/blob/master/02_sbt.markdown" target="_blank" rel="external">CSUG/real_world_scala/blob/master/02_sbt.markdown</a><br>参考 : <a href="http://www.scala-sbt.org/0.13.1/docs/Getting-Started/Hello.html" target="_blank" rel="external">scala-sbt.org/0.13.1/docs/Getting-Started</a><br>参考 : <a href="http://article.yeeyan.org/view/442873/404261" target="_blank" rel="external">译言网</a></p>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-cdh-install-online" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/03/14/cdh-install-online/"><strong>大数据平台CDH集群在线安装</strong></a>
      <small class=article-date-index>&nbsp; 2016-03-14</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/03/14/cdh-install-online/" class="article-date">
  <time datetime="2016-03-14T07:54:16.000Z" itemprop="datePublished">2016-03-14</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      介绍了 CDH 集群的搭建与安装，其中 Server 安装步骤非常准确, Agent 需要进一步验证. <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/hadoop/">hadoop</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/03/14/cdh-install-online/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <p>标签： Cloudera-Manager CDH Hadoop 部署 集群</p>
<blockquote>
<p>摘要：管理、部署Hadoop集群需要工具，Cloudera Manager便是其一。本文详细记录了以在线方式部署CDH集群&gt;的步骤。</p>
</blockquote>
<p>以Apache Hadoop为主导的大数据技术的出现，使得中小型公司对于大数据的存储与处理也拥有了武器。</p>
<p>目前Hadoop比较流行的主要有2个版本，Apache和Cloudera版本。</p>
<p>Apache Hadoop：维护人员比较多，更新频率比较快，但是稳定性比较差。<br>Cloudera Hadoop（CDH）：CDH：Cloudera公司的发行版本，基于Apache Hadoop的二次开发，优化了组件兼容和交互接口、简化安装配置、增加Cloudera兼容特性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">大数据平台CDH集群 cdh-5.70-rpm_install 详细过程</span><br></pre></td></tr></table></figure>
<h1 id="Part-1-install-cdh-server"><a href="#Part-1-install-cdh-server" class="headerlink" title="Part 1 install cdh server"></a>Part 1 install cdh server</h1><h2 id="1-1-Ready-install-resources"><a href="#1-1-Ready-install-resources" class="headerlink" title="1.1 Ready install resources"></a>1.1 Ready install resources</h2><ol>
<li>CentOS Linux release 7.1.1503 (Core) cm-5.7.0 </li>
<li>cloudera-manager-installer.bin</li>
<li>adduser deploy</li>
</ol>
<p>centos7.1 在安装过程时，网络配置，设置静态IP</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>
<p>设置静态ip，以及指定ip地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DEVICE=&quot;eth0&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">IPADDR=192.168.1.110</span><br><span class="line">NM_CONTROLLED=&quot;yes&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">DNS1=8.8.8.8</span><br><span class="line">DNS2=8.8.4.4</span><br><span class="line">GATEWAY=192.168.1.1</span><br></pre></td></tr></table></figure>
<h2 id="1-2-网络配置（所有节点）"><a href="#1-2-网络配置（所有节点）" class="headerlink" title="1.2 网络配置（所有节点）"></a>1.2 网络配置（所有节点）</h2><p><strong>修改hostname为 cdh-server7</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">　　RedHat 的 hostname，就修改 /etc/sysconfig/network文件，将里面的 HOSTNAME 这一行修改成 HOSTNAME=NEWNAME，其中 NEWNAME 就是你要设置的 hostname。</span><br><span class="line"></span><br><span class="line">　　Debian发行版的 hostname 的配置文件是 /etc/hostname</span><br></pre></td></tr></table></figure>
<p><strong>修改ip与主机名的对应关系</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# vi /etc/hosts #修改ip与主机名的对应关系:</span><br><span class="line">192.168.181.190 node190</span><br><span class="line">192.168.181.198 node198</span><br><span class="line">192.168.181.196 node196</span><br></pre></td></tr></table></figure>
<p><strong>重启网络服务生效</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# service network restart</span><br></pre></td></tr></table></figure>
<p><strong>关闭SELINUX</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看SELINUX状态</span><br><span class="line"></span><br><span class="line">[root@cdh-server7 ~]#getenforce</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">若 SELINUX 没有关闭，按照下述方式关闭</span><br><span class="line"></span><br><span class="line">vi /etc/selinux/config</span><br><span class="line">修改SELinux=disabled。重启生效，可以等后面都设置完了重启主机</span><br><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"># SELINUX= can take one of these three values:</span><br><span class="line">#       enforcing - SELinux security policy is enforced.</span><br><span class="line">#       permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line">#       disabled - SELinux is fully disabled.</span><br><span class="line">SELINUX=disabled</span><br><span class="line"># SELINUXTYPE= type of policy in use. Possible values are:</span><br><span class="line">#       targeted - Only targeted network daemons are protected.</span><br><span class="line">#       strict - Full SELinux protection.</span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# ping www.baidu.com</span><br></pre></td></tr></table></figure>
<p>以上步骤执行完毕后，重启主机</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<p>重启后再次检查下以上几点，确保环境配置正确。</p>
<h2 id="1-3-卸载-openjdk-所有节点"><a href="#1-3-卸载-openjdk-所有节点" class="headerlink" title="1.3 卸载 openjdk (所有节点)"></a>1.3 卸载 openjdk (所有节点)</h2><blockquote>
<p>注意 : 如果没有openjdk, 则不需要卸载，默认 centos7 没有</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep java</span><br><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep jdk</span><br><span class="line"></span><br><span class="line"># if exist java or jdk, uninstall, erase it.  example under this...</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="1-4-卸载-centOS7-默认mysql"><a href="#1-4-卸载-centOS7-默认mysql" class="headerlink" title="1.4 卸载 centOS7 默认mysql"></a>1.4 卸载 centOS7 默认mysql</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep mariadb</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps mariadb-libs-5.5.41-2.el7_0.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="1-5-Cloudera-Manager安装"><a href="#1-5-Cloudera-Manager安装" class="headerlink" title="1.5 Cloudera Manager安装"></a>1.5 Cloudera Manager安装</h2><p>下载资源文件<a href="https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo" target="_blank" rel="external">https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo</a></p>
<p>将cloudera-manager.repo文件拷贝到所有节点的/etc/yum.repos.d/文件夹下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node196 ]# cd /home/deploy/cdh</span><br><span class="line">[root@node196 cdh]# wget https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo</span><br><span class="line">[root@cdh-server7 cdh]# mv cloudera-manager.repo /etc/yum.repos.d/</span><br></pre></td></tr></table></figure>
<p>验证repo文件是否起效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum list|grep cloudera</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# yum list | grep cloudera</span><br><span class="line">cloudera-manager-agent.x86_64           5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">cloudera-manager-daemons.x86_64         5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">cloudera-manager-server.x86_64          5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">cloudera-manager-server-db-2.x86_64     5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">enterprise-debuginfo.x86_64             5.7.0-1.cm560.p0.54.el7        cloudera-manager</span><br><span class="line">oracle-j2sdk1.7.x86_64                  1.7.0+update67-1               cloudera-manager</span><br></pre></td></tr></table></figure>
<p>如果列出的不是你安装的版本，执行下面命令重试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum clean all </span><br><span class="line">yum list | grep cloudera</span><br></pre></td></tr></table></figure>
<p>上传下列 <strong>rpm 包</strong> 到 [root@cdh-server7] 的 /home/deploy/cdh/cloudera-rpms (任意目录)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /home/deploy/cdh/cloudera-rpms</span><br><span class="line">cloudera-manager-agent-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">cloudera-manager-daemons-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">cloudera-manager-server-5.7.0-1.cm560.p0.54.el7.x86_64.rpm   ## agent not use</span><br><span class="line">cloudera-manager-server-db-2-5.7.0-1.cm560.p0.54.el7.x86_64.rpm  ## agent not use</span><br><span class="line">enterprise-debuginfo-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm</span><br></pre></td></tr></table></figure>
<blockquote>
<p>说明 : 可从<a href="https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5/RPMS/x86_64/" target="_blank" rel="external">https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5/RPMS/x86_64/</a> 下载相关rpm包</p>
</blockquote>
<p>切换到rpms目录下，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cd /home/deploy/cdh/cloudera-rpms/</span><br><span class="line">[root@cdh-server7 cloudera-rpms]# yum -y install *.rpm</span><br></pre></td></tr></table></figure>
<h2 id="1-6-拷贝资源包到目标目录"><a href="#1-6-拷贝资源包到目标目录" class="headerlink" title="1.6 拷贝资源包到目标目录"></a>1.6 拷贝资源包到目标目录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从 http://archive.cloudera.com/cdh5/parcels/5.7.0/ 下载资源包</span><br></pre></td></tr></table></figure>
<p>将之前下载的Parcel那3个文件拷贝到/opt/cloudera/parcel-repo目录下（如果没有该目录，请自行创建）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cp CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel /opt/cloudera/parcel-repo/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel</span><br><span class="line">[root@cdh-server7 cdh]# cp CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha</span><br><span class="line">[root@cdh-server7 cdh]# cp manifest.json /opt/cloudera/parcel-repo/manifest.json</span><br></pre></td></tr></table></figure>
<h2 id="1-7-配置-java-环境变量"><a href="#1-7-配置-java-环境变量" class="headerlink" title="1.7 配置 java 环境变量"></a>1.7 配置 java 环境变量</h2><p>设置JAVA_HOME</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]#vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera/</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@cdh-server7 cdh]#source /etc/profile</span><br></pre></td></tr></table></figure>
<p>关闭防火墙</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]#systemctl stop firewalld.service  #centos7,关闭防火墙</span><br></pre></td></tr></table></figure>
<p>以上步骤执行完毕后，重启主机</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<h2 id="1-8-安装CM-只在主节点"><a href="#1-8-安装CM-只在主节点" class="headerlink" title="1.8 安装CM (只在主节点)"></a>1.8 安装CM (只在主节点)</h2><p><strong>以下两步骤请只在主节点上执行 :</strong></p>
<ul>
<li><p>进入该目录，给bin文件赋予可执行权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# chmod a+x ./cloudera-manager-installer.bin</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装CM (该步骤, 可能是不需要的)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# ./cloudera-manager-installer.bin</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>开始启动server端</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cd /etc/init.d/</span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-server-db start</span><br><span class="line"></span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-server start</span><br><span class="line">Starting cloudera-scm-server:                              [  OK  ]</span><br><span class="line">[root@cdh-server7 init.d]# tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意 :<br> 机器重启之后，默认启动会导致异常<br> 需要按照该先启动cloudera-scm-server-db，再启动cloudera-scm-server的顺序执行</p>
</blockquote>
<h2 id="1-9-浏览器访问验证-主节点"><a href="#1-9-浏览器访问验证-主节点" class="headerlink" title="1.9 浏览器访问验证(主节点)"></a>1.9 浏览器访问验证(主节点)</h2><p>CM安装成功后浏览器输入<a href="http://ip:7180" target="_blank" rel="external">http://ip:7180</a>, 用户名和密码都输入admin，进入web管理界面。</p>
<p>通过浏览器访问验证</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://192.168.181.190:7180/</span><br></pre></td></tr></table></figure>
<p>如果打不开改网页，等待2分钟后。这个服务启动是需要一定时间的。</p>
<p>选择部署的版本，这里我们选择免费版的就可以了。</p>
<blockquote>
<p>如果不会设置，那么请参考 最靠谱的安装指南 <a href="http://www.jianshu.com/p/57179e03795f" target="_blank" rel="external">http://www.jianshu.com/p/57179e03795f</a></p>
</blockquote>
<p>安装服务时，数据库选择默认的嵌入式数据库</p>
<h1 id="Part-2-安装-agent"><a href="#Part-2-安装-agent" class="headerlink" title="Part 2 安装 agent"></a>Part 2 安装 agent</h1><blockquote>
<p>this step is similar， but I can’t be sure, exactly right. </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">安装 agent ，可以在单独的机器，主节点，可以只当做主，随意你</span><br></pre></td></tr></table></figure>
<blockquote>
<p>为agent做配置,启动agent (所有节点)<br>agent 不需要装server，其他绝大部分步骤和 安装 server 相同。</p>
</blockquote>
<h2 id="2-1-网络配置"><a href="#2-1-网络配置" class="headerlink" title="2.1 网络配置"></a>2.1 网络配置</h2><p><strong>修改ip与主机名的对应关系</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-agent1 ~]# vi /etc/hosts #修改ip与主机名的对应关系:</span><br><span class="line">192.168.181.190 cdh-server7(node190)</span><br><span class="line">192.168.181.198 cdh-agent1(node198)</span><br><span class="line">192.168.181.196 cdh-agent2(node196)</span><br></pre></td></tr></table></figure>
<p><strong>重启网络服务生效</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# service network restart</span><br></pre></td></tr></table></figure>
<p><strong>关闭SELINUX</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看SELINUX状态</span><br><span class="line"></span><br><span class="line">[root@cdh-server7 ~]#getenforce</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">若 SELINUX 没有关闭，按照下述方式关闭</span><br><span class="line"></span><br><span class="line">vi /etc/selinux/config</span><br><span class="line">修改SELinux=disabled。重启生效，可以等后面都设置完了重启主机</span><br><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"># SELINUX= can take one of these three values:</span><br><span class="line">#       enforcing - SELinux security policy is enforced.</span><br><span class="line">#       permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line">#       disabled - SELinux is fully disabled.</span><br><span class="line">SELINUX=disabled</span><br><span class="line"># SELINUXTYPE= type of policy in use. Possible values are:</span><br><span class="line">#       targeted - Only targeted network daemons are protected.</span><br><span class="line">#       strict - Full SELinux protection.</span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 ~]# ping www.baidu.com</span><br></pre></td></tr></table></figure>
<h2 id="2-2-卸载-openjdk-所有节点"><a href="#2-2-卸载-openjdk-所有节点" class="headerlink" title="2.2 卸载 openjdk (所有节点)"></a>2.2 卸载 openjdk (所有节点)</h2><blockquote>
<p>注意 : 如果没有openjdk, 则不需要卸载，默认 centos7 没有</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep java</span><br><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep jdk</span><br><span class="line"></span><br><span class="line"># if exist java or jdk, uninstall, erase it.  example under this...</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="2-3-卸载centOS7默认的mysql"><a href="#2-3-卸载centOS7默认的mysql" class="headerlink" title="2.3 卸载centOS7默认的mysql"></a>2.3 卸载centOS7默认的mysql</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]# rpm -qa | grep mariadb</span><br><span class="line">[root@cdh-server7 deploy]# rpm -e --nodeps mariadb-libs-5.5.41-2.el7_0.x86_64</span><br></pre></td></tr></table></figure>
<h2 id="2-4-cloudera-manager-repo"><a href="#2-4-cloudera-manager-repo" class="headerlink" title="2.4 cloudera-manager.repo"></a>2.4 cloudera-manager.repo</h2><blockquote>
<p>上传cloudera-manager.repo 到 cdh-agent1</p>
</blockquote>
<p>[root@cdh-agent1 cdh]# cp cloudera-manager.repo /etc/yum.repos.d/</p>
<p><strong>transparent_hugepage</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
<p><strong>vi /etc/rc.local 在文件尾放入 如下两条语句</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x /etc/rc.local</span><br></pre></td></tr></table></figure>
<p><strong>调整swappiness</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo 10 &gt; /proc/sys/vm/swappiness</span><br><span class="line"># vi /etc/sysctl.conf</span><br><span class="line">vm.swappiness = 10</span><br></pre></td></tr></table></figure>
<h2 id="2-5-cdh-cloudera-rpms"><a href="#2-5-cdh-cloudera-rpms" class="headerlink" title="2.5 ~/cdh/cloudera-rpms"></a>2.5 ~/cdh/cloudera-rpms</h2><blockquote>
<p>上传下列rpm包到cdh-agent1的/home/deploy/cdh/cloudera-rpms</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cloudera-manager-agent-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">cloudera-manager-daemons-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">enterprise-debuginfo-5.7.0-1.cm560.p0.54.el7.x86_64.rpm</span><br><span class="line">oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@cdh-agent1 init.d]# cd /home/deploy/cdh/cloudera-rpms/</span><br><span class="line">[root@cdh-agent1 init.d]# yum -y install *.rpm</span><br></pre></td></tr></table></figure>
<p><strong>设置JAVA_HOME</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]#vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera/</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@cdh-server7 cdh]#source /etc/profile</span><br></pre></td></tr></table></figure>
<p>关闭防火墙</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 deploy]#systemctl stop firewalld.service  #centos7,关闭防火墙</span><br></pre></td></tr></table></figure>
<p>以上步骤执行完毕后，重启主机</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-agent1 init.d]# vi /etc/cloudera-scm-agent/config.ini</span><br><span class="line"></span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</span><br><span class="line"># Hostname of the CM server.</span><br><span class="line">#server_host=localhost</span><br><span class="line">server_host=cdh-server7(node190)</span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cd /etc/init.d/</span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-agent start</span><br><span class="line">Starting cloudera-scm-agent:                               [  OK  ]</span><br><span class="line">[root@cdh-server deploy]# tail -f /var/log//cloudera-scm-agent/cloudera-scm-agent.log</span><br></pre></td></tr></table></figure>
<hr>
<p>注意 : </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">安装YARN NodeManager失败时，需要删除 /yarn /var/lib/hadoop-yarn 目录再重新添加</span><br></pre></td></tr></table></figure>
<hr>
<p>CDH最靠谱的安装指南 : <a href="http://www.jianshu.com/p/57179e03795f" target="_blank" rel="external">http://www.jianshu.com/p/57179e03795f</a></p>
<h1 id="Part-3-恢复启动-Our-集群"><a href="#Part-3-恢复启动-Our-集群" class="headerlink" title="Part 3 恢复启动 Our 集群"></a>Part 3 恢复启动 Our 集群</h1><h2 id="3-1-确定-firewalld-close"><a href="#3-1-确定-firewalld-close" class="headerlink" title="3.1 确定 firewalld close"></a>3.1 确定 firewalld close</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start firewalld.service#启动firewall</span><br><span class="line">systemctl stop firewalld.service#停止firewall</span><br><span class="line">systemctl disable firewalld.service#禁止firewall开机启动</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意 : 操作之前确定 firewalld 是关闭的</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node19x flag]$ vim /etc/rc.local (/etc/rc.local 对应貌似相对dir /ect/init.d)</span><br><span class="line"></span><br><span class="line">  1 #!/bin/bash</span><br><span class="line">  2 # THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES</span><br><span class="line">  3 #</span><br><span class="line">  4 # It is highly advisable to create own systemd services or udev rules</span><br><span class="line">  5 # to run scripts during boot instead of using this file.</span><br><span class="line">  6 #</span><br><span class="line">  7 # In contrast to previous versions due to parallel execution during boot</span><br><span class="line">  8 # this script will NOT be run after all other services.</span><br><span class="line">  9 #</span><br><span class="line"> 10 # Please note that you must run &apos;chmod +x /etc/rc.d/rc.local&apos; to ensure</span><br><span class="line"> 11 # that this script will be executed during boot.</span><br><span class="line"> 12</span><br><span class="line"> 13 touch /var/lock/subsys/local</span><br><span class="line"> 14 echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"> 15 echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line"> 16 service ntpd start</span><br><span class="line"> 17 service elasticsearch start</span><br></pre></td></tr></table></figure>
<h2 id="3-2-启动server端、cm"><a href="#3-2-启动server端、cm" class="headerlink" title="3.2 启动server端、cm"></a>3.2 启动server端、cm</h2><p>only at server node</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh-server7 cdh]# cd /etc/init.d/</span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-server-db start</span><br><span class="line"></span><br><span class="line">[root@cdh-server7 init.d]# ./cloudera-scm-server start</span><br><span class="line">Starting cloudera-scm-server:                              [  OK  ]</span><br><span class="line">[root@cdh-server7 init.d]# tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</span><br><span class="line"></span><br><span class="line">// 等待日志 7180 启动成功， 访问 : http://node190:7180/cmf/home</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意 :<br>机器重启之后，默认启动会导致异常<br>需要按照该先启动cloudera-scm-server-db，再启动cloudera-scm-server的顺序执行</p>
</blockquote>
<p>一般以下 agent 是自动启动的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node190 init.d]# ./cloudera-scm-agent start</span><br><span class="line">cloudera-scm-agent is already running</span><br><span class="line">node190:./cloudera-scm-agent start</span><br><span class="line">node19x:./cloudera-scm-agent start</span><br><span class="line">node19x:./cloudera-scm-agent start</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="3-3-CM页面上启动各服务"><a href="#3-3-CM页面上启动各服务" class="headerlink" title="3.3 CM页面上启动各服务"></a>3.3 CM页面上启动各服务</h2><ol>
<li>CM 页面上重启 service monitor</li>
<li>CM 页面上重启 host monitor</li>
<li>CM 页面上启动各项服务 (如 : ZK, Flume, YARN, HDFS, Hive, Sqoop, Spark etc..)</li>
</ol>
<hr>
<h2 id="3-4-各个节点启动-ES"><a href="#3-4-各个节点启动-ES" class="headerlink" title="3.4 各个节点启动 ES"></a>3.4 各个节点启动 ES</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[deploy@node190 init.d]# ll</span><br><span class="line">total 44</span><br><span class="line">-rwxr-xr-x  1 root root  8671 Apr  2 04:52 cloudera-scm-agent</span><br><span class="line">lrwxrwxrwx. 1 root root    58 Apr 18 16:55 elasticsearch -&gt; /home/deploy/elasticsearch-1.7.1/bin/service/elasticsearch</span><br><span class="line">-rw-r--r--. 1 root root 13948 Sep 16  2015 functions</span><br><span class="line">-rwxr-xr-x. 1 root root  2989 Sep 16  2015 netconsole</span><br><span class="line">-rwxr-xr-x. 1 root root  6630 Sep 16  2015 network</span><br><span class="line">-rw-r--r--. 1 root root  1160 Apr  1 00:45 README</span><br></pre></td></tr></table></figure>
<p><strong>deploy</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[deploy@node190 init.d]# ./elasticsearch start</span><br><span class="line">[deploy@node19x init.d]# ./elasticsearch start</span><br><span class="line">[deploy@node19x init.d]# ./elasticsearch start</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://node190:9200/_plugin/bigdesk/#cluster</span><br></pre></td></tr></table></figure>
<blockquote>
<p>等待同步数据完成，一般会很快，等待 Status 从 RED 变为 green 状态</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://node190:9200/_plugin/head/</span><br></pre></td></tr></table></figure>
<h2 id="3-5-启动-kibana"><a href="#3-5-启动-kibana" class="headerlink" title="3.5 启动 kibana"></a>3.5 启动 kibana</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[deploy@node196 ~]#</span><br><span class="line">cd /home/deploy/kibana-4.1.1-linux-x64</span><br><span class="line">    ./bin/kibana &gt; kibana.log 2&gt;&amp;1 &amp;              --@deploy</span><br></pre></td></tr></table></figure>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-etl-intro" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/03/07/etl-intro/"><strong>BI Dev -- ETL introduce</strong></a>
      <small class=article-date-index>&nbsp; 2016-03-07</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/03/07/etl-intro/" class="article-date">
  <time datetime="2016-03-07T07:34:16.000Z" itemprop="datePublished">2016-03-07</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      ETL 是数据抽取（Extract）、清洗（Cleaning）、转换（Transform）、装载（Load）的过程. <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/pentaho/">pentaho</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/03/07/etl-intro/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>focus on :</strong></p>
<ol>
<li>Know What’s ETL? </li>
<li>Know ETL 在 BI 开发中注意的细节</li>
</ol>
<p><strong>ETL 简介</strong></p>
<ol>
<li>ETL 是数据抽取（Extract）、清洗（Cleaning）、转换（Transform）、装载（Load）的过程。</li>
<li>ETL 是构建 <strong>DW</strong> 的重要一环，用户从数据源抽取出数据，经 数据清洗,按照预定义好的 DW模型，将数据加载到 DW 中去。</li>
<li>ETL 是将业务系统的数据经过抽取、清洗转换之后加载到 DW 的过程，目的是将企业中的分散零乱、标准不统一的数据到一起，为企业的决策提供分析依据。</li>
<li>ETL 是 <strong>BI</strong> 项目中一个重要环节。</li>
</ol>
<p><strong>ETL的设计分三个部分：</strong></p>
<ol>
<li>数据抽取</li>
<li>数据的清洗转换</li>
<li>数据的加载</li>
</ol>
<p>下面看流程图：<br><img src="/images/etl01.png" alt="ETL Flow"></p>
<h2 id="1-数据抽取"><a href="#1-数据抽取" class="headerlink" title="1. 数据抽取"></a>1. 数据抽取</h2><p>首先要搞清楚数据是从几个业务系统中来，各个业务系统的数据库服务器运行的是何种DBMS，是否存在手工数据， 非结构化数据等。</p>
<h2 id="2-数据清洗与转换"><a href="#2-数据清洗与转换" class="headerlink" title="2. 数据清洗与转换"></a>2. 数据清洗与转换</h2><p>数据仓库分为ODS,DW连部分。通常的做法是从业务系统到ODS做清洗，将脏数据和不完整的数据过滤掉，在ODS到过程中转换，进行一些业务规则的计算和聚合。</p>
<p> <strong>1. 数据清洗</strong> </p>
<p>主要是过滤那些不符合要求的数据。</p>
<ol>
<li>不完整的数据</li>
<li>错误的数据</li>
<li>重复的数据</li>
</ol>
<p><strong> 2. 数据转换</strong></p>
<p> 数据转换的任务主要进行不一致的数据转换、数据粒度的转换</p>
<h2 id="3-数据的加载"><a href="#3-数据的加载" class="headerlink" title="3. 数据的加载"></a>3. 数据的加载</h2><p>一般在数据清洗完了之后直接写入DW</p>
<h2 id="4-Refence-article"><a href="#4-Refence-article" class="headerlink" title="4. Refence article"></a>4. Refence article</h2><p>本文整理自网络</p>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-sqoop-learn-use01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/02/16/sqoop-learn-use01/"><strong>Sqoop introduce</strong></a>
      <small class=article-date-index>&nbsp; 2016-02-16</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/02/16/sqoop-learn-use01/" class="article-date">
  <time datetime="2016-02-16T07:54:16.000Z" itemprop="datePublished">2016-02-16</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      Sqoop 即 SQL to Hadoop, 是一款方便的在传统关系数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输. <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/hadoop/">hadoop</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/02/16/sqoop-learn-use01/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-Sqoop-what"><a href="#1-Sqoop-what" class="headerlink" title="1. Sqoop what ?"></a>1. Sqoop what ?</h2><p>sqoop 即 SQL to Hadoop ，是一款方便的在传统关系数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输，发展至今主要演化了二大版本，sqoop1和sqoop2。 </p>
<p>sqoop : clouder 公司开发</p>
<p><strong>生产背景</strong></p>
<ol>
<li>mysql  导入 Hadoop </li>
<li>Hadoop 导入 mysql</li>
</ol>
<p>注 : 以上 Hadoop 指 Hive、HBase、HDFS 等</p>
<h2 id="2-Sqoop-特点"><a href="#2-Sqoop-特点" class="headerlink" title="2. Sqoop 特点"></a>2. Sqoop 特点</h2><p>sqoop架构非常简单，其整合了Hive、Hbase和Oozie，通过map-reduce任务来传输数据，从而提供并发特性和容错。</p>
<p>   Sqoop 由两部分组成：客户端(client)和服务端(server)。需要在集群的其中某个节点上安装server，该节点的服务端可以作为其他 Sqoop 客户端的入口点。</p>
<p>   在 server 端的节点上必须安装有 Hadoop。client 可以安装在任意数量的机子上。在装有客户端的机子上不需要安装 Hadoop。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop 官网 : https://sqoop.apache.org</span><br><span class="line"></span><br><span class="line">1.4.5官方文档 : https://sqoop.apache.org/docs/1.4.5/</span><br><span class="line"></span><br><span class="line">sqoop2不推荐的原因 : http://blog.csdn.net/robbyo/article/details/50737356</span><br></pre></td></tr></table></figure>
<h2 id="3-Sqoop-优缺点"><a href="#3-Sqoop-优缺点" class="headerlink" title="3. Sqoop 优缺点"></a>3. Sqoop 优缺点</h2><p><strong>优点</strong></p>
<ol>
<li>高效可控的利用资源，任务并行度，超时时间。</li>
<li>数据类型映射与转化，可自动进行，用户也可自定义 .</li>
<li>支持多种主流数据库，MySQL,Oracle，SQL Server，DB2等等 。</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>基于命令行的操作方式，易出错，且不安全。</li>
<li>数据传输和数据格式是紧耦合的，这使得connector无法支持所有的数据格式</li>
<li>用户名和密码暴漏出来</li>
</ol>
<h2 id="4-Sqoop-原理"><a href="#4-Sqoop-原理" class="headerlink" title="4. Sqoop 原理"></a>4. Sqoop 原理</h2><h3 id="4-1-Sqoop的import原理"><a href="#4-1-Sqoop的import原理" class="headerlink" title="4.1 Sqoop的import原理"></a>4.1 Sqoop的import原理</h3><p>Sqoop 在 import 时，需要制定 split-by 参数。</p>
<p>Sqoop 根据不同的 split-by参数值 来进行切分, 然后将切分出来的区域分配到不同 map 中。每个map中再处理数据库中获取的一行一行的值，写入到 HDFS 中。同时split-by 根据不同的参数类型有不同的切分方法，如比较简单的int型，Sqoop会取最大和最小split-by字段值，然后根据传入的 num-mappers来确定划分几个区域。 </p>
<p>比如 select max(split_by),min(split-by) from 得到的 max(split-by)和 min(split-by) 分别为 1000 和 1, 而 num-mappers 为 2 的话，则会分成两个区域 (1,500) 和 (501-100), 同时也会分成 2个sql 给 2个map 去进行导入操作，分别为 select XXX from table where split-by&gt;=1 and split-by<500 和="" select="" xxx="" from="" table="" where="" split-by="">=501 and split-by&lt;=1000。最后每个map各自获取各自SQL中的数据进行导入工作。</500></p>
<h3 id="4-2-Sqoop的export原理"><a href="#4-2-Sqoop的export原理" class="headerlink" title="4.2. Sqoop的export原理"></a>4.2. Sqoop的export原理</h3><p>根据 mysql 表名称，生成一个以表名称命名的 Java类，该类继承了 sqoopRecord的，是一个只有 Map 的 MR，且自定义了输出字段。</p>
<p>sqoop export –connect jdbc:mysql://$url:3306/$3?characterEncoding=utf8 –username $username –password $password –table $1 –export-dir $2 –input-fields-terminated-by ‘|’ –null-non-string ‘0’ –null-string ‘0’;</p>
<h2 id="5-Sqoop-使用实例"><a href="#5-Sqoop-使用实例" class="headerlink" title="5. Sqoop 使用实例"></a>5. Sqoop 使用实例</h2><p><strong>环境</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop: sqoop-1.4.5+cdh5.3.6+78</span><br><span class="line">hive : hive-0.13.1+cdh5.3.6+397</span><br><span class="line">hbase: hbase-0.98.6+cdh5.3.6+115</span><br></pre></td></tr></table></figure>
<h3 id="5-1-Mysql-to-Hadoop"><a href="#5-1-Mysql-to-Hadoop" class="headerlink" title="5.1. Mysql to Hadoop"></a>5.1. Mysql to Hadoop</h3><ul>
<li>Mysql to Hdfs</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">  --connect $&#123;jdbc_url&#125; --username $&#123;jdbc_username&#125; --password  $&#123;jdbc_passwd&#125; \</span><br><span class="line">  --query &quot;$&#123;exec_sql&#125;&quot; \</span><br><span class="line">  --split-by $&#123;id&#125; -m 10 \</span><br><span class="line">  --target-dir $&#123;target_dir&#125; \</span><br><span class="line">  --fields-terminated-by &quot;\001&quot; --lines-terminated-by &quot;\n&quot; \</span><br><span class="line">  --hive-drop-import-delims \</span><br><span class="line">  --null-string &apos;\\N&apos; --null-non-string &apos;\\N&apos;</span><br></pre></td></tr></table></figure>
<ul>
<li>Mysql To Hive</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">  --connect $&#123;jdbc_url&#125; \</span><br><span class="line">  --username $&#123;jdbc_username&#125; --password  $&#123;jdbc_passwd&#125; \</span><br><span class="line">  --table $&#123;jdbc_table&#125; --fields-terminated-by &quot;\001&quot; --lines-terminated-by &quot;\n&quot; \</span><br><span class="line">  --hive-import --hive-overwrite --hive-table $&#123;hive_table&#125; \</span><br><span class="line">  --null-string &apos;\\N&apos; --null-non-string &apos;\\N&apos;</span><br></pre></td></tr></table></figure>
<ul>
<li>Mysql To HBase</li>
</ul>
<h3 id="5-2-Hadoop-to-Mysql"><a href="#5-2-Hadoop-to-Mysql" class="headerlink" title="5.2 Hadoop to Mysql"></a>5.2 Hadoop to Mysql</h3><ul>
<li>Hdfs To Mysql</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">  --connect $&#123;jdbc_url&#125; --username $&#123;jdbc_username&#125; --password  $&#123;jdbc_passwd&#125; \ </span><br><span class="line">  --table category \</span><br><span class="line">  --export-dir /dc_ext/xbd/dm/tmp/ods_dm_category_tmp \</span><br><span class="line">  --input-fields-terminated-by &apos;\001&apos; \</span><br><span class="line">  --input-null-non-string &apos;\\N&apos; \</span><br><span class="line">  --input-null-string &apos;\\N&apos;;</span><br></pre></td></tr></table></figure>
<p><strong>refence article</strong></p>
<p><a href="http://www.zihou.me/html/2014/01/28/9114.html" target="_blank" rel="external">Sqoop中文文档</a><br><a href="http://www.aboutyun.com/thread-12684-1-1.html" target="_blank" rel="external">Hive to Mysql 常遇九大问题总结</a> </p>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-hive-brief" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/02/15/hive-brief/"><strong>Hive Introduce 1</strong></a>
      <small class=article-date-index>&nbsp; 2016-02-15</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/02/15/hive-brief/" class="article-date">
  <time datetime="2016-02-15T07:07:21.000Z" itemprop="datePublished">2016-02-15</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      关于 Hive 的介绍, 简单介绍 Hive 的工作流、架构组件 等。 <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/hadoop/">hadoop</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/02/15/hive-brief/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>focus on :</strong></p>
<ol>
<li>初步了解 Hadoop 生态圈</li>
<li>初步了解 Hive 架构图</li>
</ol>
<h2 id="1-Hive-Introduce"><a href="#1-Hive-Introduce" class="headerlink" title="1. Hive Introduce"></a>1. Hive Introduce</h2><h3 id="1-1-Hive-Preface"><a href="#1-1-Hive-Preface" class="headerlink" title="1.1 Hive Preface"></a>1.1 Hive Preface</h3><p><strong>Hadoop</strong></p>
<ol>
<li>Hadoop 生态系统 是 处理大数据集而产生的解决方案。</li>
<li>Hadoop 实现计算模型 MapReduce, 可将计算任务分割成多个处理单元，这个计算模型下面是一个 HDFS。</li>
</ol>
<p><strong>Hive</strong></p>
<ol>
<li>Hive 提供了一个 Hive查询语言 HiveQL, 查询转换为 MapReduce job</li>
<li>Hive 适合做数据仓库，可离线维护海量数据，可对数据进行挖掘, 形成报告等</li>
<li>Hadoop、HDFS 设计本身限制了 Hive 所能胜任的工作, Hive 不支持记录级别的更新、插入 或者 删除 操作。</li>
</ol>
<p><strong>Hive 运行架构</strong></p>
<ol>
<li>使用 HQL 作为查询接口；</li>
<li>使用 MapReduce 作为执行层；</li>
<li>使用 HDFS 作为存储层；</li>
</ol>
<h3 id="1-2-Hadoop-Mapreduce"><a href="#1-2-Hadoop-Mapreduce" class="headerlink" title="1.2 Hadoop / Mapreduce"></a>1.2 Hadoop / Mapreduce</h3><p><code>Input -&gt; Mappers -&gt; Sort,Shuffle -&gt; Reducers -&gt; Output</code></p>
<h3 id="1-3-Hive-系统架构"><a href="#1-3-Hive-系统架构" class="headerlink" title="1.3 Hive 系统架构"></a>1.3 Hive 系统架构</h3><p><img src="/images/hive02.png" alt="Hive 系统架构"></p>
<h2 id="2-Hive-架构组件分析"><a href="#2-Hive-架构组件分析" class="headerlink" title="2. Hive 架构组件分析"></a>2. Hive 架构组件分析</h2><p><strong>本章重点 :</strong></p>
<ol>
<li>初步了解 Hive 的工作流</li>
<li>初步了解 hive 的工作组件</li>
</ol>
<h3 id="2-1-元数据存储Metastore"><a href="#2-1-元数据存储Metastore" class="headerlink" title="2.1 元数据存储Metastore"></a>2.1 元数据存储Metastore</h3><ul>
<li><p>Hive的数据由两部分组成：数据文件 和 元数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">元数据存储，Derby只能用于一个Hive连接，一般存储在MySQL。</span><br><span class="line"></span><br><span class="line">元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-2-驱动-Driver"><a href="#2-2-驱动-Driver" class="headerlink" title="2.2 驱动 (Driver)"></a>2.2 驱动 (Driver)</h3><ul>
<li>编译器</li>
<li>优化器</li>
<li>执行器</li>
</ul>
<p>用户通过下面的接口提交Hive给Driver，由Driver进行HQL语句解析，此时从Metastore中获取表的信息，先生成逻辑计划，再生成物理计划，再由Executor生成Job交给Hadoop运行，然后由Driver将结果返回给用户。</p>
<p>编译器（Hive的核心）：1，语义解析器（ParseDriver），将查询字符串转换成解析树表达式；2，语法解析器（SemanticAnalyzer），将解析树转换成基于语句块的内部查询表达式；3，逻辑计划生成器（Logical Plan Generator），将内部查询表达式转换为逻辑计划，这些计划由逻辑操作树组成，操作符是Hive的最小处理单元，每个操作符处理代表一道HDFS操作或者是MR作业；4，查询计划生成器（QueryPlan Generator），将逻辑计划转化成物理计划（MR Job）。</p>
<p>优化器：优化器是一个演化组件，当前它的规则是：列修剪，谓词下压。</p>
<p>执行器：编译器将操作树切分成一个Job链（DAG），执行器会顺序执行其中所有的Job；如果Task链不存在依赖关系，可以采用并发执行的方式进行Job的执行。</p>
<h3 id="2-3-接口"><a href="#2-3-接口" class="headerlink" title="2.3 接口"></a>2.3 接口</h3><p><strong>CLI、HWI、ThriftServer</strong></p>
<ol>
<li><p>CLI：为命令行工具，默认服务。bin/hive或bin/hive–service cli；</p>
</li>
<li><p>HWI：为Web接口，可以用过浏览器访问Hive，默认端口9999，启动方式为bin/hive –service hwi;</p>
</li>
<li><p>ThriftServer：通过Thrift对外提供服务，默认端口是10000，启动方式为bin/hive –service hiveserver;</p>
</li>
</ol>
<p><strong> 连接hive-metastore(如mysql)的三种方式 </strong></p>
<ol>
<li>单用户模式。此模式连到数据库Derby，一般用于Unit Test。<br><img src="/images/hive-longdis-model.jpeg" alt="单用户模式"></li>
<li>多用户模式。通过网络连接到一个数据库中，是最经常使用到的模式。<br><img src="/images/hive-more-user-model.jpeg" alt="多用户模式"></li>
<li>远程服务器模式。用于非Java客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer访问元数据库。<br><img src="/images/hive-longdis-model.jpeg" alt="远程服务器模式"></li>
</ol>
<h3 id="2-4-其他服务"><a href="#2-4-其他服务" class="headerlink" title="2.4 其他服务"></a>2.4 其他服务</h3><p><strong>bin/hive –service -help</strong></p>
<ol>
<li><p>metastore   (bin/hive –service metastore)</p>
</li>
<li><p>hiveserver2（bin/hive –service hiveserver2）</p>
</li>
</ol>
<p><strong>HiveServer2</strong></p>
<ol>
<li><p>HiveServer2是HieServer改进版本，它提供给新的ThriftAPI来处理JDBC或者ODBC客户端，进行Kerberos身份验证，多个客户端并发</p>
</li>
<li><p>HS2还提供了新的CLI：BeeLine，是Hive 0.11引入的新的交互式CLI，基于SQLLine，可以作为Hive JDBC Client 端访问HievServer2，启动一个beeline就是维护了一个session.</p>
</li>
</ol>
<p><strong>Hive下载地址</strong></p>
<ol>
<li><p>cdh-hive : <a href="https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/hive/hive-exec/0.13.1-cdh5.3.6/" target="_blank" rel="external">hive0.13.1-cdh5.3.6 jar 包</a> (没用)</p>
</li>
<li><p>apache-hive : <a href="http://archive.apache.org/dist/hive/" target="_blank" rel="external">Apache-Hive</a></p>
</li>
</ol>
<p><strong>Hive-Beeline 试验成功</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">下载 apache-hive-0.13.1-bin, apache-hadoop2.5，配置 HADOOP_HOME, 启动 </span><br><span class="line"></span><br><span class="line">➜  ./apache-hive-0.13.1-bin/bin/beeline</span><br><span class="line">Beeline version 0.13.1 by Apache Hive</span><br><span class="line">beeline&gt; !connect jdbc:hive2://node190:10000 hdfs 1</span><br><span class="line">scan complete in 3ms</span><br><span class="line">Connecting to jdbc:hive2://node190:10000</span><br><span class="line">Connected to: Apache Hive (version 0.13.1-cdh5.3.6)</span><br><span class="line">Driver: Hive JDBC (version 0.13.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://node190:10000&gt; select count(*) from ods_dm_shop_tmp;</span><br><span class="line">+-------+</span><br><span class="line">|  _c0  |</span><br><span class="line">+-------+</span><br><span class="line">| 1091  |</span><br><span class="line">+-------+</span><br><span class="line">1 row selected (24.815 seconds)</span><br><span class="line">0: jdbc:hive2://node190:10000&gt;</span><br><span class="line"></span><br><span class="line">说明 : beeline 可以成功，用代码 jdbc 就可以成功</span><br><span class="line"></span><br><span class="line">安装 hadoop 参考了 《Spark大数据处理》高彦杰@著, 不用配置直接绿色简单版</span><br></pre></td></tr></table></figure>
<p><strong>Hive table</strong></p>
<p>  table 中的一个 Partition 对应表下的一个子目录<br>  每一个 Bucket 对应一个文件；<br>  Hive的默认数据仓库目录是/user/hive/warehouse<br>  在hive-site.xml中由hive.metastore.warehouse.dir项定义；</p>
<h2 id="reference-article"><a href="#reference-article" class="headerlink" title="reference article"></a>reference article</h2><p>参考 : <a href="http://blog.csdn.net/lalaguozhe/article/details/11776055" target="_blank" rel="external">CSDN - Hive Server 2 调研，安装和部署</a><br>参考 : <a href="http://www.geedoo.info/beeline-abnormal-connection-hiveserver2.html" target="_blank" rel="external">极豆技术博客 - Beeline连接hiveserver2异常</a><br>参考 : <a href="http://blog.csdn.net/skywalker_only/article/details/38366347" target="_blank" rel="external">Hive学习之HiveServer2 JDBC客户端</a><br>参考 : <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline" target="_blank" rel="external">HiveServer2 Clients beeline</a><br>参考 : <a href="http://www.aboutyun.com/blog-6-1855.html" target="_blank" rel="external">Beeline连接hiveserver2异常</a><br>参考 : <a href="http://blog.csdn.net/skywalker_only/article/details/38335235" target="_blank" rel="external">Hive学习之HiveServer2服务端配置与启动</a></p>
<p><strong>other tmp</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## Chap 7 HiveQL 视图 ##</span><br><span class="line">## Chap 8 HiveQL 索引 ##</span><br><span class="line">## Chap 9 模式设计 ##</span><br><span class="line">## Chap 10 调优 ##</span><br><span class="line">## Chap 11 其他文件格式和压缩方法 ##</span><br><span class="line">## Chap 12 开发 ##</span><br><span class="line">## Chap 13 函数 ##</span><br><span class="line">## Chap 14 Streaming ##</span><br><span class="line">## Chap 15 自定义Hive文件和记录格式 ##</span><br><span class="line">## Chap 16 Hive 的 Thrift 服务 ##</span><br><span class="line">## Chap 11 其他文件格式和压缩方法 ##</span><br></pre></td></tr></table></figure>
<hr>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-spark-introduce-and-install" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2016/02/01/spark-introduce-and-install/"><strong>Spark Introduce and Install</strong></a>
      <small class=article-date-index>&nbsp; 2016-02-01</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/02/01/spark-introduce-and-install/" class="article-date">
  <time datetime="2016-02-01T02:07:21.000Z" itemprop="datePublished">2016-02-01</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      介绍 Spark 的历史，介绍 Spark 的安装与部署，介绍 Spark 的代码架构 等 <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/spark/">spark</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2016/02/01/spark-introduce-and-install/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <p>Spark 发源于 美国加州大学伯克利分校 AMPLap 大数据分析平台<br>Spark 立足于内存计算、从多迭代批量处理出发<br>Spark 兼顾数据仓库、流处理、图计算 等多种计算范式，大数据系统领域全栈计算平台  </p>
<p> <a href="http://spark.apache.org" target="_blank" rel="external"> Spark官网 </a> </p>
<h2 id="1-Spark-的历史与发展"><a href="#1-Spark-的历史与发展" class="headerlink" title="1. Spark 的历史与发展"></a>1. Spark 的历史与发展</h2><ul>
<li>2009 年 : Spark 诞生于 AMPLab  </li>
<li>2014-02 : Apache 顶级项目  </li>
<li>2014-05 : Spark 1.0.0 发布</li>
</ul>
<h2 id="2-Spark-之于-Hadoop"><a href="#2-Spark-之于-Hadoop" class="headerlink" title="2. Spark 之于 Hadoop"></a>2. Spark 之于 Hadoop</h2><p> Spark 是 MapReduce 的替代方案, 且兼容 HDFS、Hive 等分布式存储层。</p>
<p> Spark 相比 Hadoop MapReduce 的优势如下 :</p>
<ol>
<li>中间结果输出</li>
<li>数据格式和内存布局</li>
<li>执行策略  </li>
<li>任务调度的开销<br>(Spark用事件驱动类库AKKA来启动任务, 通过线程池复用线程避免进线程启动切换开销)</li>
</ol>
<h2 id="3-Spark-能带来什么"><a href="#3-Spark-能带来什么" class="headerlink" title="3. Spark 能带来什么 ?"></a>3. Spark 能带来什么 ?</h2><ol>
<li>打造全栈多计算范式的搞笑数据流水线 (高效)</li>
<li>轻量级快速处理</li>
<li>易于使用， Spark 支持多语言</li>
<li>与 HDFS 等 存储层 兼容</li>
<li>社区活跃度高</li>
</ol>
<h2 id="4-Spark-安装与部署"><a href="#4-Spark-安装与部署" class="headerlink" title="4. Spark 安装与部署"></a>4. Spark 安装与部署</h2><p>Spark 主要使用 HDFS 充当持久化层，所以完整的安装 Spark 需要先安装 Hadoop. (当然你测试，就不需要安装Hadoop)</p>
<p>Spark 是计算框架, 它主要使用 HDFS 充当持久化层。如 hive etc..</p>
<p><strong>Linux 集群安装 Spark</strong></p>
<ol>
<li>安装 JDK</li>
<li>安装 Scala</li>
<li>配置 SSH 免密码登陆 (可选)</li>
<li>安装 Hadoop</li>
<li>安装 Spark</li>
<li>启动 Spark 集群</li>
</ol>
<p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">Spark官网下载</a></p>
<h3 id="4-1-安装-Spark"><a href="#4-1-安装-Spark" class="headerlink" title="4.1 安装 Spark"></a>4.1 安装 Spark</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1). download  spark-1.5.2-bin-hadoop2.6.tgz</span><br><span class="line"></span><br><span class="line">(2). tar -xzvf spark-1.5.2-bin-hadoop2.6.tgz</span><br><span class="line"></span><br><span class="line">(3). 配置 conf/spark-env.sh</span><br><span class="line">    1) 详细复杂参数配置参见 官网 Configuration</span><br><span class="line">    2) vim conf/spark-env.sh</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home</span><br><span class="line">    <span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/Cellar/scala/2.11.5</span><br><span class="line">    <span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/xSoft/spark</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_IP=ip</span><br><span class="line">    <span class="built_in">export</span> MASTER=spark://ip:7077</span><br><span class="line"></span><br><span class="line">    <span class="built_in">export</span> SPARK_EXECUTOR_INSTANCES=2</span><br><span class="line">    <span class="built_in">export</span> SPARK_EXECUTOR_CORES=1</span><br><span class="line"></span><br><span class="line">    <span class="built_in">export</span> SPARK_WORKER_MEMORY=1000m</span><br><span class="line">    <span class="built_in">export</span> SPARK_EXECUTOR_MEMORY=300m</span><br><span class="line"></span><br><span class="line">    <span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$&#123;SPARK_HOME&#125;</span>/lib</span><br><span class="line"></span><br><span class="line">(4). 配置 conf/slaves (测试可选)</span><br><span class="line">(5). 一般需要 startup ssh server.</span><br></pre></td></tr></table></figure>
<h3 id="4-2-启动-Spark-集群"><a href="#4-2-启动-Spark-集群" class="headerlink" title="4.2 启动 Spark 集群"></a>4.2 启动 Spark 集群</h3><p>在 Spark 根目录启动 Spark</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/start-all.sh</span><br><span class="line">./sbin/stop-all.sh</span><br></pre></td></tr></table></figure>
<p>启动后 jps 查看 会有 Master 进程存在</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  spark-1.5.2-bin-hadoop2.6  jps</span><br><span class="line">11262 Jps</span><br><span class="line">11101 Master</span><br><span class="line">11221 Worker</span><br></pre></td></tr></table></figure>
<h3 id="4-3-Spark-集群初试"><a href="#4-3-Spark-集群初试" class="headerlink" title="4.3 Spark 集群初试"></a>4.3 Spark 集群初试</h3><p>可以通过两种方式运行 Spark 样例 :</p>
<ul>
<li>以 ./run-example 的方式执行</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/xSoft/spark</span><br><span class="line">➜  spark ./sbin/start-all.sh</span><br><span class="line">➜  spark ./bin/run-example org.apache.spark.examples.SparkPi</span><br></pre></td></tr></table></figure>
<ul>
<li>以 ./Spark Shell 方式执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; import org.apache.spark._</span><br><span class="line">import org.apache.spark._</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; object SparkPi &#123;</span><br><span class="line">     |</span><br><span class="line">     |   def main(args: Array[String]) &#123;</span><br><span class="line">     |</span><br><span class="line">     |     val slices = 2</span><br><span class="line">     |     val n = 100000 * slices</span><br><span class="line">     |</span><br><span class="line">     |     val count = sc.parallelize(1 to n, slices).map &#123; i =&gt;</span><br><span class="line">     |</span><br><span class="line">     |       val x = math.random * 2 - 1</span><br><span class="line">     |       val y = math.random * 2 - 1</span><br><span class="line">     |</span><br><span class="line">     |       if (x * x + y * y &lt; 1) 1 else 0</span><br><span class="line">     |</span><br><span class="line">     |     &#125;.reduce(_ + _)</span><br><span class="line">     |</span><br><span class="line">     |     println(&quot;Pi is rounghly &quot; + 4.0 * count / n)</span><br><span class="line">     |</span><br><span class="line">     |   &#125;</span><br><span class="line">     | &#125;</span><br><span class="line">defined module SparkPi</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br><span class="line"></span><br><span class="line">// Spark Shell 已默认将 SparkContext 类初始化为对象 sc, 用户代码可直接使用。</span><br><span class="line"></span><br><span class="line">// Spark 自带的交互式的 Shell 程序，方便进行交互式编程。</span><br></pre></td></tr></table></figure>
<ul>
<li><p>通过 Web UI 查看集群状态</p>
<pre><code>http：//masterIp:8080
</code></pre></li>
</ul>
<p><img src="/images/spark-08.png" width="840" height="500" img=""></p>
<h3 id="4-4-Spark-quick-start"><a href="#4-4-Spark-quick-start" class="headerlink" title="4.4 Spark quick start"></a>4.4 Spark quick start</h3><p>quick-start : <a href="https://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="external">https://spark.apache.org/docs/latest/quick-start.html</a></p>
<p>./bin/spark-shell</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val textFile = sc.textFile(&quot;README.md&quot;)</span><br><span class="line">textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3</span><br><span class="line">RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let’s start with a few actions:</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.count() // Number of items in this RDD</span><br><span class="line">res0: Long = 126</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.first() // First item in this RDD</span><br><span class="line">res1: String = # Apache Spark</span><br></pre></td></tr></table></figure>
<h2 id="5-Spark-生态-BDAS"><a href="#5-Spark-生态-BDAS" class="headerlink" title="5. Spark 生态 BDAS"></a>5. Spark 生态 BDAS</h2><ul>
<li>Spark 框架、架构、计算模型、数据管理策略</li>
<li>Spark BDAS 项目及其子项目进行了简要介绍</li>
<li>Spark 生态系统包含的多个子项目 : SparkSql、Spark Streaming、GraphX、MLlib</li>
</ul>
<p><img src="/images/spark-01.png" alt="Spark EcoSystem = BDAS = 伯克利数据分析栈"></p>
<ul>
<li>Spark 是 BDAS 核心, 是一 大数据分布式编程框架</li>
</ul>
<h2 id="6-Spark-架构"><a href="#6-Spark-架构" class="headerlink" title="6. Spark 架构"></a>6. Spark 架构</h2><ul>
<li>Spark 的代码结构</li>
<li>Spark 的架构</li>
<li>Spark 运行逻辑</li>
</ul>
<h3 id="6-1-Spark-的代码结构"><a href="#6-1-Spark-的代码结构" class="headerlink" title="6.1 Spark 的代码结构"></a>6.1 Spark 的代码结构</h3><p><img src="/images/spark-02.jpeg" alt="spark code"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scheduler：文件夹中含有负责整体的Spark应用、任务调度的代码。</span><br><span class="line">broadcast：含有Broadcast（广播变量）的实现代码，API中是Java和Python API的实现。</span><br><span class="line"></span><br><span class="line">deploy：含有Spark部署与启动运行的代码。</span><br><span class="line">common：不是一个文件夹，而是代表Spark通用的类和逻辑实现，有5000行代码。</span><br><span class="line"></span><br><span class="line">metrics：是运行时状态监控逻辑代码，Executor中含有Worker节点负责计算的逻辑代码。</span><br><span class="line">partial：含有近似评估代码。</span><br></pre></td></tr></table></figure>
<h3 id="6-2-Spark-的架构"><a href="#6-2-Spark-的架构" class="headerlink" title="6.2 Spark 的架构"></a>6.2 Spark 的架构</h3><p>Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行</p>
<p><img src="/images/spark-03.jpeg" alt="spark"></p>
<p>Spark 分别启动 Master进程 和 Worker进程，对整个集群进行控制。在一个 Spark应用 的执行过程中，Driver 和 Worker是两个重要角色。</p>
<p>Driver 程序是应用逻辑执行的起点，负责作业的调度，即Task任务的分发。<br>Worker用来管理计算节点和创建Executor并行处理任务。</p>
<p>在执行阶段，Driver 会将 Task 和 Task所依赖的file 和 jar 序列化后传递给对应的 Worker机器，同时 Executor对相应数据分区的任务进行处理。</p>
<p>Spark 架构中的基本组件 :</p>
<ul>
<li><p>ClusterManager : Standalone模式中为 Master, 控制整个集群，监控Worker。</p>
</li>
<li><p>Worker : 从节点，负责控制计算节点，启动Executor 或 Driver。在 YARN 模式中为 NodeManager，负责计算节点的控制。</p>
</li>
<li><p>Driver：运行Application的main()函数并创建SparkContext。</p>
</li>
<li><p>Executor：执行器，在worker node上执行任务的组件、用于启动线程池运行任务。每个Application拥有独立的一组Executors。</p>
</li>
<li><p>SparkContext：整个应用的上下文，控制应用的生命周期。</p>
</li>
<li><p>RDD：Spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph。</p>
</li>
<li><p>DAG Scheduler：根据作业（Job）构建基于Stage的DAG，并提交Stage给TaskScheduler。</p>
</li>
<li><p>TaskScheduler：将任务（Task）分发给Executor执行。</p>
</li>
<li><p>SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。SparkEnv内创建并包含如下一些重要组件的引用。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MapOutPutTracker：负责Shuffle元信息的存储。</span><br><span class="line">BroadcastManager：负责广播变量的控制与元信息的存储。</span><br><span class="line"></span><br><span class="line">BlockManager：负责存储管理、创建和查找块。</span><br><span class="line">MetricsSystem：监控运行时性能指标信息。</span><br><span class="line">SparkConf：负责存储配置信息。</span><br></pre></td></tr></table></figure>
<p>Spark 的整体流程为 : Client 提交应用，Master找到一个 Worker 启动 Driver，Driver 向 Master 或者 资源管理器申请资源，之后将应用转化为 RDD Graph，再由 DAGScheduler 将 RDD Graph 转化为 Stage的有向无环图 提交给TaskScheduler，由TaskScheduler提交任务给Executor执行。在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行。</p>
<h3 id="6-3-Spark-运行逻辑"><a href="#6-3-Spark-运行逻辑" class="headerlink" title="6.3 Spark 运行逻辑"></a>6.3 Spark 运行逻辑</h3><p>Spark应用，整个执行流程在逻辑上会形成有向无环图（DAG）。</p>
<p>Action 算子触发之后，将所有累积的算子形成一个有向无环图，然后由调度器调度该图上的任务进行运算。</p>
<p>Spark 的调度方式与 MapReduce 有所不同。Spark 根据 RDD 之间不同的依赖关系切分形成不同的阶段（Stage），一个阶段包含一系列函数执行流水线。图中的A、B、C、D、E、F 分别代表不同的 RDD，RDD内的方框代表分区。数据从 HDFS输入Spark，形成RDD A 和 RDD C，RDD C上执行 map操作，转换为RDD D， RDD B 和 RDD E 执行join 操作，转换为F，而在 B 和 E 连接转化为 F 的过程中又会 执行Shuffle，最后RDD F 通过 函数saveAsSequenceFile输出 并保存到 HDFS 中。</p>
<p><img src="/images/spark-04.jpeg" alt="spark rdd"></p>
<h2 id="7-小结"><a href="#7-小结" class="headerlink" title="7. 小结"></a>7. 小结</h2><p>由于 Spark 主要使用 HDFS 充当持久化层，所以完整的使用 Spark 需要预先安装 Hadoop.</p>
<p>Spark 将分布式的内存数据抽象为弹性分布式数据集 (RDD), 并在其上实现了丰富的算子，从而对 RDD 进行计算，最后将 算子序列 转化为 DAG 进行执行和调度。</p>
<blockquote>
<p>Spark的Python API几乎覆盖了所有Scala API所能提供的功能. 但的确有些特性，比如Spark Streaming和个别的API方法，暂不支持。<br><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">具体参见《Spark编程指南》的Python部分</a></p>
</blockquote>
<p>体会了 函数式 编程的威力， scala、python 都可以。java 不适合写 spark 程序</p>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-java-logback-indoor" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2015/12/27/java-logback-indoor/"><strong>Logback 入门初步</strong></a>
      <small class=article-date-index>&nbsp; 2015-12-27</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2015/12/27/java-logback-indoor/" class="article-date">
  <time datetime="2015-12-27T07:54:16.000Z" itemprop="datePublished">2015-12-27</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      Logback 一个开源日志组件, SLF4J 这个简单的日志前端接口（Façade）来替代 Jakarta Commons-Logging 。 <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/java/">java</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2015/12/27/java-logback-indoor/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <p>Logback 一个开源日志组件。<br>Logback 当前分成三个模块：logback-core  logback- classic  和  logback-access。</p>
<h2 id="1-logback-简介"><a href="#1-logback-简介" class="headerlink" title="1. logback 简介"></a>1. logback 简介</h2><p>Ceki在Java日志领域世界知名。他创造了Log4J ，这个最早的Java日志框架即便在JRE内置日志功能的竞争下仍然非常流行。随后他又着手实现SLF4J 这个“简单的日志前端接口（Façade）”来替代Jakarta Commons-Logging 。</p>
<p>Logback，一个“可靠、通用、快速而又灵活的Java日志框架”。</p>
<p><strong>官网网址 :</strong> <a href="http://logback.qos.ch/" target="_blank" rel="external">http://logback.qos.ch/</a></p>
<h2 id="2-工程使用需要的-jar"><a href="#2-工程使用需要的-jar" class="headerlink" title="2. 工程使用需要的 jar"></a>2. 工程使用需要的 jar</h2><p>要在工程里面使用 logback , 只需要以下jar文件：</p>
<pre><code>(1). slf4j-api.jar       
(2). logback-access.jar
(3). logback-classic.jar
(4). logback-core.jar

logback-core    是其它两个模块的基础模块。   
logback-classic 是 log4j 的一个 改良版本。   
logback-access  与Servlet容器集成提供通过Http来访问日志功能
</code></pre><p>logback-classic 完整实现 SLF4J API 使你可以很方便地更换成其它日志系统如 log4j 或 JDK Logging。</p>
<h2 id="3-logback-常用配置详解"><a href="#3-logback-常用配置详解" class="headerlink" title="3. logback 常用配置详解"></a>3. logback 常用配置详解</h2><h3 id="3-1-根节点-lt-configuration-gt"><a href="#3-1-根节点-lt-configuration-gt" class="headerlink" title="3.1 根节点&lt; configuration &gt;"></a>3.1 根节点&lt; configuration &gt;</h3><table>
<thead>
<tr>
<th>configuration</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>scan</td>
<td>当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。</td>
</tr>
<tr>
<td>scanPeriod</td>
<td>设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。</td>
</tr>
<tr>
<td>debug</td>
<td>当此属性设置为true时，将打印出 logback 内部日志信息，实时查看logback运行状态。默认值为false。</td>
</tr>
</tbody>
</table>
<h2 id="4-logback-配置示例"><a href="#4-logback-配置示例" class="headerlink" title="4. logback 配置示例"></a>4. logback 配置示例</h2><h3 id="4-1-Myself-resources-logback-xml-example"><a href="#4-1-Myself-resources-logback-xml-example" class="headerlink" title="4.1 Myself resources/logback.xml example"></a>4.1 Myself resources/logback.xml example</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"3600 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"AppName"</span> <span class="attr">value</span>=<span class="string">"your_app_name"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LogParentDir"</span> <span class="attr">value</span>=<span class="string">"/home/www/logs/"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$&#123;AppName&#125;<span class="tag">&lt;/<span class="name">contextName</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"infoAppender"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LogParentDir&#125;/$&#123;AppName&#125;/infoLogFile.%d&#123;yyyy-MM-dd&#125;.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"errorAppender"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LogParentDir&#125;/$&#123;AppName&#125;/errorLogFile.%d&#123;yyyy-MM-dd&#125;.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--其中appender的配置表示打印到控制台--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.x.dmt"</span> <span class="attr">level</span>=<span class="string">"ERROR"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"errorAppender"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--设置addtivity为false，将此loger的打印信息不向上级传递；--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.x.dmt.service"</span> <span class="attr">level</span>=<span class="string">"INFO"</span> <span class="attr">additivity</span>=<span class="string">"fasle"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"infoAppender"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 注意: logger 同名情况, 级别低的,需要放在下面,否则级别高的会覆盖级别低的权限,早晨级别低的打印不出来日志 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<hr>
<p><a href="http://aub.iteye.com/blog/1101260" target="_blank" rel="external">更多参见 iteye1101260</a></p>
<p><a href="http://logback.qos.ch/" target="_blank" rel="external">官方网址</a></p>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <article id="post-java-se-introduce" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <myh11 itemprop="name">
      <a class="article-title-index" href="/2013/02/02/java-se-introduce/"><strong>Java SE Basic Introduce</strong></a>
      <small class=article-date-index>&nbsp; 2013-02-02</small>
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11>
  


      </header>
    
    <div class="article-meta">
      <!--<a href="/2013/02/02/java-se-introduce/" class="article-date">
  <time datetime="2013-02-01T23:54:16.000Z" itemprop="datePublished">2013-02-02</time>
</a>-->
      
  
    <myh11 class="desc-index"> <!--by blair 160805 -->
      This is a introduction about Java SE and how to use it. <!-- by blair 160805 bak-->
      <!--<a class="</strong></a>-->
      <!--<a>
      <div = post.date  </div>
      </a>-->
    </myh11> <!--by blair 160805-->
  


      <!--
  <div class="article-category-index">
    <a class="article-category-index-link" href="/categories/java/">java</a>
  </div>

-->
      <!--
      
        <div class="article-comment-link-wrap">
          <a href="http://selfboot.org/2013/02/02/java-se-introduce/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <!--
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Java data type</li>
<li>OO</li>
<li>Exception</li>
<li>Java Array  </li>
<li>Java 常用类 </li>
<li>Java 容器类</li>
<li>Collection / Generic</li>
<li>Java I/O Stream</li>
<li>Java Thread</li>
<li>Java TCP/UDP, socket</li>
</ul>
<h2 id="1-Java-概述"><a href="#1-Java-概述" class="headerlink" title="1. Java 概述"></a>1. Java 概述</h2><ul>
<li>Java 运行机制</li>
<li>JDK &amp; JRE</li>
<li>Java env install</li>
<li>Java Basic Content</li>
</ul>
<blockquote>
<p>conclude : 计算机语言朝着人类易于理解的方向发展  </p>
</blockquote>
<h2 id="2-Java-特点"><a href="#2-Java-特点" class="headerlink" title="2. Java 特点"></a>2. Java 特点</h2><ul>
<li>一种 OO 语言  </li>
<li>一种平台无关的语言, 提供程序运行的解释环境  </li>
<li>一种健壮的语言, 吸收了C/C++语言的优点， 但去掉了其影响程序健壮性的部分(如: 指针， 内存的申请与释放等)。  </li>
</ul>
<h2 id="3-Java程序运行机制"><a href="#3-Java程序运行机制" class="headerlink" title="3. Java程序运行机制"></a>3. Java程序运行机制</h2><p><strong>Java 2种核心机制</strong></p>
<ul>
<li>Java Virtual Machine</li>
<li>Garbage collection</li>
</ul>
<blockquote>
<p>JVM 可理解成一个以字节码为机器指令的CPU<br>JVM 机制屏蔽了底层运行平台的差别, 实现了”一次编译, 随处运行”。</p>
<p>x.java –编译–&gt; x.class –执行–&gt; JVM</p>
<p>Java语言消除了程序员回收无用内存空间的责任;<br>它提供一种系统级线程跟踪存储空间的分配情况，并在JVM的空闲时, 检查并释放那些可被释放的存储器空间。  </p>
</blockquote>
<h2 id="4-JDK-amp-JRE-amp-env-install"><a href="#4-JDK-amp-JRE-amp-env-install" class="headerlink" title="4. JDK &amp; JRE &amp; env install"></a>4. JDK &amp; JRE &amp; env install</h2><ul>
<li>Software Development Kit (软件开发包)  开发需要 JDK  </li>
<li>Java Runtime Environment  用户只需 JRE  </li>
</ul>
<p><code>/etc/profile</code> or  <code>.zshrc</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">### JAVA ###</span><br><span class="line">JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home</span><br><span class="line">JAVA_BIN=$JAVA_HOME/bin</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jar</span><br><span class="line">export JAVA_HOME JAVA_BIN PATH CLASSPATH</span><br></pre></td></tr></table></figure>
<blockquote>
<p>classpath : java在编译和运行时要找的class所在的路径<br>建议你的 JDK 装在不带空格的目录里面</p>
</blockquote>
<h2 id="5-命名规则"><a href="#5-命名规则" class="headerlink" title="5. 命名规则"></a>5. 命名规则</h2><ol>
<li>类名首字母大写  </li>
<li>变量名和方法名的首字母小写  </li>
<li>运用驼峰标识   </li>
</ol>
<p>HelloWorld.java</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class HelloWorld &#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    System.out.println(&quot;Hello Java.&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line">  * 这里是注释</span><br><span class="line">  */</span><br></pre></td></tr></table></figure>
<blockquote>
<p>一个源文件中最多只能有一个public类. 其它类的个数不限，如果源文件 文件包含一个public class 它必需按该 class-name 命名  </p>
</blockquote>
<h2 id="6-Java-程序设计"><a href="#6-Java-程序设计" class="headerlink" title="6. Java 程序设计"></a>6. Java 程序设计</h2><p><strong>data type</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                                      -- 整数类型 (byte, short, int, long)  </span><br><span class="line">                          -- 数值型 --     </span><br><span class="line">                         |            -- 浮点类型 (float, double)  </span><br><span class="line">           --基本数据类型  -- 字符型 (char)  </span><br><span class="line">          |              |  </span><br><span class="line">          |               -- 布尔型 (boolean)  </span><br><span class="line">数据类型 --                           </span><br><span class="line">          |               -- 类 (class)  </span><br><span class="line">          |              |  </span><br><span class="line">           --引用数据类型  -- 接口 (interface)  </span><br><span class="line">                         |  </span><br><span class="line">                          -- 数组 (array)</span><br></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>java 中定义了 <strong>4类 8种</strong> 基本数据类型<br>boolean 类型只允许取值 true / false , 不可以用 0 或 非0 替代。<br>char 采用 Unicode 编码 (全球语言统一编码), 每个字符占两个字节  </p>
</blockquote>
<h2 id="7-Array-amp-Method"><a href="#7-Array-amp-Method" class="headerlink" title="7. Array &amp; Method"></a>7. Array &amp; Method</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class Test &#123;  </span><br><span class="line">    public static void main(String[] args) &#123;  </span><br><span class="line">        Date[] days;  </span><br><span class="line">        days = new Date[3];  </span><br><span class="line">        for (int i = 0; i &lt; 3; i++) &#123;  </span><br><span class="line">            days[i] = new Date(2004, 4, i+1);  </span><br><span class="line">        &#125;</span><br><span class="line">        // </span><br><span class="line">        int[] a = &#123;1, 2, 3, 4, 5, 6, 7&#125;;  </span><br><span class="line">        for (int i = 0; i &lt; a.length; i++) &#123;  </span><br><span class="line">            System.out.print(a[i] + &quot; &quot;);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">class Date &#123;  </span><br><span class="line">    int year;  </span><br><span class="line">    int month;  </span><br><span class="line">    int day;  </span><br><span class="line">    Date(int y, int m, int d) &#123;  </span><br><span class="line">        year = y;  </span><br><span class="line">        month = m;  </span><br><span class="line">        day = d;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="http://blog.csdn.net/robbyo/article/details/16942921" target="_blank" rel="external">面向过程-约瑟夫环</a></p>
<p><a href="http://blog.csdn.net/robbyo/article/details/16967715" target="_blank" rel="external">面向对象-约瑟夫环</a></p>

      
    </div>
    -->
    <!--
    
    -->
    <!--
    
      <footer class="article-footer">
      </footer>
    
    -->
  </div>
  
</article>



    
      <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/page/5/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span>
      </nav>
    

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Libin Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/blair1">blairos</a>
    </div>
  </div>
</footer>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
