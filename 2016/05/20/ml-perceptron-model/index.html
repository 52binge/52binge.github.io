<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Perceptron Model - Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Perceptron model learning for classification">
<meta property="og:type" content="article">
<meta property="og:title" content="Perceptron Model">
<meta property="og:url" content="http://iequa.com/2016/05/20/ml-perceptron-model/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="Perceptron model learning for classification">
<meta property="og:image" content="http://images.cnitblog.com/blog/414721/201304/11142837-31e4844d63c2478e8f978af1ebd59512.png">
<meta property="og:image" content="http://images.cnitblog.com/blog/414721/201304/11153222-be953a21074145b880ace08984b4f788.png">
<meta property="og:image" content="http://images.cnitblog.com/blog/414721/201304/12175006-26817264c38a47f5b93d2f56aee2f9d1.png">
<meta property="og:updated_time" content="2017-04-13T23:45:41.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Perceptron Model">
<meta name="twitter:description" content="Perceptron model learning for classification">
<meta name="twitter:image" content="http://images.cnitblog.com/blog/414721/201304/11142837-31e4844d63c2478e8f978af1ebd59512.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <br>
    <section id="main" class="outer"><article id="post-ml-perceptron-model" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Perceptron Model
      <small class=article-detail-date-index>&nbsp; 2016-05-20</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/05/20/ml-perceptron-model/" class="article-date">
  <time datetime="2016-05-20T10:01:21.000Z" itemprop="datePublished">2016-05-20</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/machine-learning/">machine-learning</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2016/05/20/ml-perceptron-model/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$']],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

<p>perceptron model learning</p>
<a id="more"></a>
<p><strong>Written Before</strong></p>
<ul>
<li>豆瓣链接 : <a href="https://book.douban.com/subject/10590856/" target="_blank" rel="external">《统计学习方法》</a></li>
<li>李航微博 : <a href="http://weibo.com/u/2060750830" target="_blank" rel="external">@李航博士</a></li>
<li>维基百科 : <a href="https://en.wikipedia.org/wiki/Perceptron" target="_blank" rel="external">perceptron</a></li>
</ul>
<p><strong>Content List</strong></p>
<ul>
<li>1.感知机模型</li>
<li>2.感知机学习策略</li>
<li>3.感知机学习算法<ul>
<li>3.1. 原始形式  </li>
<li>3.2. 算法收敛性</li>
<li>3.3. 对偶形式</li>
</ul>
</li>
</ul>
<h2 id="1-Perceptron-model"><a href="#1-Perceptron-model" class="headerlink" title="1. Perceptron model"></a>1. Perceptron model</h2><p><strong>model applicable premise</strong></p>
<p>perceptron 能够解决的问题首先要求 <code>feature_space</code> 线性可分，再者是二类分类，即将 sample 分为 {+1, -1} 两类</p>
<p>由<code>input_space</code> to <code>output_space</code> 的函数：</p>
<p>$$<br>f(x) = sign (w \bullet x + b)<br>$$</p>
<p>$w$ 和 $b$ 为 model 参数，$w$ 为权值（weight），$b$ 为偏置（bias）</p>
<p>$$<br>    sign(x) = \begin{Bmatrix}<br>        +1, &amp;\mbox{$w \bullet x + b &gt; 0;$} \\<br>        -1, &amp;\mbox{$w \bullet x + b &lt; 0;$}<br>    \end{Bmatrix}<br>$$</p>
<p>感知机模型的 <font color="#DC143C" face="STCAIYUN">hypothesis_space</font> 是定义在<code>feature_space</code>中的所有线性分类模型，即函数集合 $$ {f|f(x) = w·x + b} $$</p>
<p>感知机的定义中，线性方程 $w·x + b = 0$ 对应于问题空间中的一个超平面S，位于这个超平面两侧的样本分别被归为两类，例如下图，红色作为一类，蓝色作为另一类，它们的 feature 很简单，就是它们的坐标</p>
<p></p><p align="center"><img src="http://images.cnitblog.com/blog/414721/201304/11142837-31e4844d63c2478e8f978af1ebd59512.png" alt="图1"></p>
<p>As a method of supervised learning，Perceptron learning from the training set to obtain the perceptron model，即求得模型参数 $w，b$，这里 $x$ 和 $y$ 分别是 feature_vector 和 类别（也称为目标）。基于此，Perceptron model 可以对新的 input样本 进行分类。</p>
<h2 id="2-Perceptron-learning-strategy"><a href="#2-Perceptron-learning-strategy" class="headerlink" title="2. Perceptron learning strategy"></a>2. Perceptron learning strategy</h2><p>perceptron model is a linear classification model for two kinds of classification。 Linear classification model is required that our sample is linearly separable. But, What kind of sample is linearly separable？</p>
<p>举例来说，在二维平面中，可以用一条直线将 +1类 和 -1类 完美分开，那么这个样本空间就是线性可分的。如图1就是线性可分的，图2中的样本就是线性不可分的，感知机就不能处理这种情况。因此，在本章中的所有问题都基于一个前提，就是问题 空间线性可分。</p>
<p></p><p align="center"><img src="http://images.cnitblog.com/blog/414721/201304/11153222-be953a21074145b880ace08984b4f788.png" alt="图2"><2></2></p>
<p>为说明问题，假设数据集 </p>
<p>$$<br>T = { (x_1, y_1), (x_2, y_2), … , (x_N, y_N) }<br>$$ </p>
<p>对所有 </p>
<p>$y_i = +1$ 的实例 $i$ 有 $w \bullet x + b &gt; 0$ ,<br>$y_i = -1$ 的实例 $i$ 有 $w \bullet x + b &lt; 0$</p>
<p>这里先给出 <code>input_space</code>  $R^n$ 中任一点 $x_0$ 到超平面 $S$ 的距离：</p>
<p>$$<br>\frac{1}{||w||}  |w \bullet x_0 + b|<br>$$</p>
<blockquote>
<p>这里 $||w||$ 是 $w$ 的 $L_2$ 范数</p>
<p><a href="https://segmentfault.com/a/1190000005138706" target="_blank" rel="external">more<em>info</em>点到平面的距离</a></p>
</blockquote>
<p>对于误分类的数据  $(x_i, y_i)$ ，根据我们之前的假设，有</p>
<p>$$<br>-y_i (w \bullet x_i + b) &gt; 0<br>$$</p>
<p>因此误分类点到超平面 $S$ 的距离可以写作 :</p>
<p>$$<br>-\frac{1}{||w||} y_i (w \bullet x_i + b)<br>$$</p>
<p>假设超平面 $S$ 的误分类点集合为 $M$，那么所有误分类点到超平面S的总距离为 :</p>
<p>$$<br>-\frac{1}{||w||}\sum_{x_i \in M } y_i (w \bullet x_i + b)<br>$$</p>
<blockquote>
<p>$\frac{1}{||w||}$ 值是固定的。&lt;?&gt;</p>
</blockquote>
<p>这样就得到了感知机学习的 <strong><code>loss_function</code></strong>。根据我们的定义，loss_function自然是越小越好，因为这样就代表着 误分类点 越少、误分类点 距离超平面 $S$ 的距离越近，即我们的分类越正确。显然，这个 <strong><code>loss_function</code></strong> 是非负的，若所有的样本都分类正确，那么我们的<strong><code>loss_function</code></strong> 值为0。一个特定的样本集 $T$ 的损失函数：在误分类时是参数  $w$ 、 $b$ 的线性函数。也就是说，为求得正确的参数 $w，b$，我们的目标函数为</p>
<p>$$<br>\min \limits<em>{w, b} L(w, b) = -\sum</em>{x_i \in M } y_i (w \bullet x_i + b)<br>$$</p>
<p><code>loss_function</code> $L(w, b)$ 是  $w, b$ 的连续可导函数. </p>
<blockquote>
<p>?</p>
</blockquote>
<p>The learning strategy of the perceptron model is to select the model parameters $w, b$ for minimize loss function in the hypothesis space.</p>
<h2 id="3-Perceptron-learning-algorithm"><a href="#3-Perceptron-learning-algorithm" class="headerlink" title="3. Perceptron learning algorithm"></a>3. Perceptron learning algorithm</h2><p>根据感知机学习的策略，我们将寻找超平面 $S$ 的问题转化为求解  </p>
<p>$$<br>\min \limits<em>{w, b} L(w, b) = -\sum</em>{x_i \in M } y_i (w \bullet x_i + b)<br>$$</p>
<p>的最优化问题，最优化的方法是随机梯度下降法.</p>
<p><strong>随机梯度下降法两种形式 :</strong></p>
<ul>
<li>原始形式</li>
<li>对偶形式</li>
</ul>
<p>并证明了在 train_sets 线性可分时 算法的收敛性。</p>
<h3 id="3-1-原始形式"><a href="#3-1-原始形式" class="headerlink" title="3.1 原始形式"></a>3.1 原始形式</h3><p>所谓原始形式，就是我们用梯度下降的方法，对 $w$ 和 $b$ 进行不断的迭代更新。具体来说，就是先任意选取一个超平面 $S_0$ ，对应的参数分别为 $w_0$ 和 $x_0$ ，当然现在是可以任意赋值的，比如说选取 $w_0$ 为全为 0 的向量， $b$ 的值为0,  然后用梯度下降不断地极小化 损失函数。由于随机梯度下降（stochastic[stə’kæstɪk] gradient descent）的效率要高于批量梯度下降（batch gradient descent）（ <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">Andrew Ng，在Part 1的LMS algorithm部分讲义</a>），所以这里采用随机梯度下降的方法，每次随机选取一个误分类点对 $w$ 和 $b$ 进行更新。</p>
<p>设误分类点集合  $M$ 是固定的，为求 <em>loss_function</em> 的最小值，我们需要知道往哪个方向下降速率最快，这是可由对损失函数 $L(w, b)$ 求梯度得到，$L(w, b)$ 的梯度为</p>
<p>$$<br>\nabla<em>w L(w, b) = -\sum</em>{x_i \in M } y_i x_i<br>$$</p>
<p>$$<br>\nabla<em>b L(w, b) = -\sum</em>{x_i \in M } y_i<br>$$</p>
<blockquote>
<p>?</p>
</blockquote>
<p>接下来随机选取一个 误分类点 $(x_i, y_i)$ 对 $w$ , $b$ 进行更新 :</p>
<p>$$<br>w \leftarrow w + \eta y_i x_i<br>$$</p>
<p>$$<br>b \leftarrow b + \eta y_i<br>$$</p>
<p>其中 $\eta (0 \lt \eta \le 1)$  为步长，也称为学习速率（learning rate），一般在0到1之间取值，步长越大，我们梯度下降的速度越快，也就能更快接近极小点。如果步长过大，就有直接跨过极小点导致函数发散的问题；如果步长过小，可能会耗费比较长的时间才能达到极小点。通过这样的迭代，我们的损失函数就不断减小，直到为0。综上所述，得到如下算法：</p>
<p><strong>算法1 (感知机学习算法的原始形式)</strong></p>
<p>输入 : 训练数据集 </p>
<p>$$<br>T = { (x_1, y_1), (x_2, y_2), … , (x_N, y_N) }<br>$$ </p>
<p>输出 : $w，b$ ; 感知机模型 $<br>f(x) = sign (w \bullet x + b)<br>$</p>
<p>（1）选取初始值，$w_0, b_0$</p>
<p>（2）在训练集中选取数据 $(x_i, y_i)$</p>
<p>（3）如果 $y_i(w \bullet x_i + b) \le 0$</p>
<p>$$<br>w \leftarrow w + \eta y_i x_i<br>$$</p>
<p>$$<br>b \leftarrow b + \eta y_i<br>$$</p>
<p>（4）转至（2），直至训练集中没有误分类点</p>
<p>这种学习算法直观上有如下解释：当一个样本被误分类时，就调整 $w$ 和 $b$ 的值，使超平面 $S$ 向误分类点的一侧移动，以减少该误分类点到超平面的距离，直至超平面越过该点使之被正确分类。</p>
<blockquote>
<p>凡是只讲理论不给例子的行为都是耍流氓！</p>
</blockquote>
<p>例1 : 如图3所示的训练数据集，其正实例点是 $x_1 = (3, 3)^T$, $x_2 = (4, 3)^T$, 负实例点是 $x_3 = (1, 1)^T$, 试用 Perceptron learning algorithm 的原始形式 求 Perceptron model，即求出 $w$ 和 $b$。这里，</p>
<p></p><p align="center"><img src="http://images.cnitblog.com/blog/414721/201304/12175006-26817264c38a47f5b93d2f56aee2f9d1.png" alt="图3"></p>
<h3 id="3-2-算法收敛性"><a href="#3-2-算法收敛性" class="headerlink" title="3.2 算法收敛性"></a>3.2 算法收敛性</h3><p>纯数学的东西，Novikoff于1962年证明了感知机算法的收敛性，具体请参见 哥伦比亚大学有这样的一篇叫 <a href="http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf" target="_blank" rel="external">《Convergence Proof for the Perceptron Algorithm》</a> 讲解了这个定理的证明过程</p>
<h3 id="3-3-对偶形式"><a href="#3-3-对偶形式" class="headerlink" title="3.3 对偶形式"></a>3.3 对偶形式</h3><p>对偶形式的基本想法是，将 $w$ 和 $b$ 表示为实例和的 线性组合形式，通过求解其系数而求得 $w$ 和 $b$.</p>
<blockquote>
<p>2016-06-22 第 18 周, 我欠下的债…</p>
</blockquote>
<p><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="external">更多参见.</a></p>
<h2 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h2><p>虽然大部分是抄书，但是自己整理出来，对以后复习 有莫大的好处。</p>
<p>本章介绍了统计学习中最简单的一种算法——感知机，但是 它是很多现在流行算法的基础，比如 Neural network、 SVM、etc..</p>
<p><a href="http://wenku.baidu.com/view/27b97e69b84ae45c3b358cb0" target="_blank" rel="external">感知机和多分类</a></p>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      <div class="well">
  原创文章，转载请注明： 转载自<a href="http://blairos.org"> Blair Chan's Blog</a>，作者：
  <a href="http://blairos.org/about">Blair Chan</a> <br>
  本文基于<a target="_blank" title="Creative Commons Attribution 3.0 China Mainland License" href="http://creativecommons.org/licenses/by/3.0/cn/">署名3.0中国大陆许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。

</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Perceptron-model"><span class="toc-number"></span> <span class="toc-text">1. Perceptron model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Perceptron-learning-strategy"><span class="toc-number"></span> <span class="toc-text">2. Perceptron learning strategy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Perceptron-learning-algorithm"><span class="toc-number"></span> <span class="toc-text">3. Perceptron learning algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-原始形式"><span class="toc-number"></span> <span class="toc-text">3.1 原始形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-算法收敛性"><span class="toc-number"></span> <span class="toc-text">3.2 算法收敛性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-对偶形式"><span class="toc-number"></span> <span class="toc-text">3.3 对偶形式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-小结"><span class="toc-number"></span> <span class="toc-text">4. 小结</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/machine-learning/">machine-learning</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/classification/">classification</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/06/04/elasticsearch-1.7-Introduce/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Elasticsearch * Reference1.7 Introduce
        
      </div>
    </a>
  
  
    <a href="/2016/05/17/elasticsearch1-indoor/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Elasticsearch * 入门&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Libin Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/blairchan">blairos</a>
    </div>
  </div>
</footer>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2016/05/20/ml-perceptron-model/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
