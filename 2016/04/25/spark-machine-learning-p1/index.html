<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark Machine Learning p1 - Spark编程入门 - Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Spark 的环境搭建与运行, 接触了 RDD 与 SparkContext, 启动 Spark-Shell 以及如何使用 Scala、Python 编写 Spark 程序.">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Machine Learning p1 - Spark编程入门">
<meta property="og:url" content="http://iequa.com/2016/04/25/spark-machine-learning-p1/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="Spark 的环境搭建与运行, 接触了 RDD 与 SparkContext, 启动 Spark-Shell 以及如何使用 Scala、Python 编写 Spark 程序.">
<meta property="og:updated_time" content="2017-10-22T08:50:43.630Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark Machine Learning p1 - Spark编程入门">
<meta name="twitter:description" content="Spark 的环境搭建与运行, 接触了 RDD 与 SparkContext, 启动 Spark-Shell 以及如何使用 Scala、Python 编写 Spark 程序.">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <!-- blair add baidu tongji start... @2017.10.03 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8864dc75a81a27b7e44c00138af95d66";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- blair add baidu tongji end ! @2017.10.03 -->

<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/nlp">NLP</a>
        
          <a class="main-nav-link" href="/english">English</a>
        
          <a class="main-nav-link" href="/tweet">Tweet</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://iequa.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="post-spark-machine-learning-p1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark Machine Learning p1 - Spark编程入门
      <small class=article-detail-date-index>&nbsp; 2016-04-25</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/2016/04/25/spark-machine-learning-p1/" class="article-date">
  <time datetime="2016-04-25T02:07:21.000Z" itemprop="datePublished">2016-04-25</time>
</a>-->
      <!-- 
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

--><!-- by blair 160724 -->
      <!-- by blair
      
        <div class="article-comment-link-wrap">
          <a href="http://iequa.com/2016/04/25/spark-machine-learning-p1/#disqus_thread" class="article-comment-link">Comments</a>
        </div>
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>Spark 的环境搭建与运行, 接触了 RDD 与 SparkContext, 启动 Spark-Shell 以及如何使用 Scala、Python 编写 Spark 程序. </p>
<a id="more"></a>
<p><strong>Apache Spark</strong></p>
<p>Spark 的设计目标 即: <code>迭代式+低延迟</code> 适合 Machine Learning 算法的特性<br>Spark 分布式计算框架, 将中间数据和结果保存在内存中<br>Spark 提供函数式API, 并兼容 Hadoop 生态<br>Spark 框架对 资源调度、任务提交、执行、跟踪， 节点间通信以及数据并行处理的内在底层操作都进行了抽象</p>
<blockquote>
<p>简化了海量数据的存储(HDFS) 和 计算(MR) 流程。MapReduce 缺点, 如: 启动任务时的高开销、对中间数据 和 计算结果 写入磁盘的依赖。这使得 Hadoop 不适合 迭代式 或 低延迟 的任务。</p>
</blockquote>
<p><strong>Spark 的四种运行模式</strong></p>
<ol>
<li>本地单机模式 – Spark 进程 all run in one JVM</li>
<li>集群单机模式 – 使用 Spark 自己内置的 任务调度框架</li>
<li>基于 Mesos 一个开源集群计算框架</li>
<li>基于 YARN 与 Hadoop2 关联形成集群计算和资源调度框架</li>
</ol>
<h2 id="1-Spark运行"><a href="#1-Spark运行" class="headerlink" title="1. Spark运行"></a>1. Spark运行</h2><p>运行示例程序来测试是否一切正常：</p>
<blockquote>
<p>./bin/run-example org.apache.spark.examples.SparkPi</p>
</blockquote>
<p>该命令将在本地单机模式下执行SparkPi这个示例。在该模式下，所有的Spark进程均运行于同一个JVM中，而并行处理则通过多线程来实现。默认情况下，该示例会启用与本地系统的CPU核心数目相同的线程。</p>
<p>要在本地模式下设置并行的级别，以local[N]的格式来指定一个master变量即可。比如只使用两个线程时，可输入如下命令：</p>
<blockquote>
<p>MASTER=local[2] ./bin/run-example org.apache.spark.examples.SparkPi</p>
</blockquote>
<h2 id="2-Spark集群"><a href="#2-Spark集群" class="headerlink" title="2. Spark集群"></a>2. Spark集群</h2><p>Spark集群由两类程序构成: </p>
<ol>
<li>一个驱动程序</li>
<li>多个执行程序</li>
</ol>
<blockquote>
<p>本地模式下所有的处理都运行在同一个JVM内，而在集群模式时它们通常运行在不同的节点上。</p>
</blockquote>
<p>一个采用单机模式的Spark集群包括：</p>
<ol>
<li>一个运行Spark单机主进程和驱动程序的 Master；</li>
<li>各自运行一个执行程序进程的多个 Worker。</li>
</ol>
<p>比如在一个Spark单机集群上运行，只需传入主节点的URL即可：</p>
<blockquote>
<p>MASTER=spark://IP:PORT ./bin/run-example org.apache.spark.examples.SparkPi<br>其中的IP和PORT分别是主节点IP地址和端口号。这是告诉Spark让示例程序运行在主节点所对应的集群上</p>
</blockquote>
<h2 id="3-Spark编程模型"><a href="#3-Spark编程模型" class="headerlink" title="3. Spark编程模型"></a>3. Spark编程模型</h2><h3 id="3-1-SparkContext类"><a href="#3-1-SparkContext类" class="headerlink" title="3.1 SparkContext类"></a>3.1 SparkContext类</h3><p><strong>SparkContext类与SparkConf类</strong></p>
<p>任何Spark程序的编写都是从SparkContext开始的。SparkContext的初始化需要一个SparkConf对象，后者包含了Spark集群配置的各种参数（比如主节点的URL）。</p>
<p>初始化后，我们便可用SparkContext对象所包含的各种方法来创建和操作RDD。Spark shell（在Scala和Python下可以，但不支持Java）能自动完成上述初始化。若要用Scala代码来实现的话，可参照下面的代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Test Spark App"</span>).setMaster(<span class="string">"local[4]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>
<p>这段代码会创建一个4线程的SparkContext对象，并将其相应的任务命名为Test Spark APP。我们也可通过如下方式调用SparkContext的简单构造函数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local[4]"</span>, <span class="string">"Test Spark App"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-Spark-shell"><a href="#3-2-Spark-shell" class="headerlink" title="3.2 Spark shell"></a>3.2 Spark shell</h3><p>Spark支持 用 Scala or Python REPL（Read-Eval-Print-Loop，即交互式shell）来进行交互式的程序编写。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/spark-shell</span><br></pre></td></tr></table></figure>
<p>会启动Scala shell 并初始化一个SparkContext对象。我们可以通过 sc这个Scala值来调用这个对象</p>
<h3 id="3-3-RDD"><a href="#3-3-RDD" class="headerlink" title="3.3 RDD"></a>3.3 RDD</h3><p>一个 RDD 代表一系列的“记录”（严格来说，某种类型的对象）。<br>这些记录被分配或分区到一个集群的多个节点上（在本地模式下，可以类似地理解为单个进程里的多个线程上）。</p>
<p>Spark中的RDD具备容错性，即当某个节点或任务失败时（因非用户代码原因而引起，如硬件故障、网络不通等），RDD会在余下的节点上自动重建，以便任务能最终完成。</p>
<p><strong>1. 创建RDD</strong></p>
<p>RDD可从现有的集合创建 ：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> collection = <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>)</span><br><span class="line"><span class="keyword">val</span> rddFromCollection = sc.parallelize(collection)</span><br></pre></td></tr></table></figure>
<p>RDD也可以基于Hadoop的输入源创建，比如本地文件系统、HDFS。基于Hadoop的RDD可以使用任何实现了Hadoop InputFormat接口的输入格式，包括文本文件、其他Hadoop标准格式、HBase等。以下举例说明如何用一个本地文件系统里的文件创建RDD：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val rddFromTextFile = sc.textFile(&quot;LICENSE&quot;)</span><br></pre></td></tr></table></figure>
<p>上述代码中的textFile函数（方法）会返回一个RDD对象。该对象的每一条记录都是一个表示文本文件中某一行文字的String（字符串）对象。</p>
<p><strong>2. Spark操作</strong></p>
<p>在Spark编程模式下，所有的操作被分为 <code>transformation</code> 和 <code>action</code> 两种。</p>
<p><strong>transformation</strong> 操作是对一个数据集里的所有记录执行某种函数，从而使记录发生改变；</p>
<p><strong>action</strong> 通常是运行某些计算或聚合操作，并将结果返回运行 SparkContext 的那个驱动程序。</p>
<p>Spark 的操作通常采用<code>函数式</code>风格。</p>
<p>Spark程序中最常用的转换操作便是map操作。该操作对一个RDD里的每一条记录都执行某个函数，从而将输入映射成为新的输出。</p>
<p>比如，下面这段代码便对一个从本地文本文件创建的RDD进行操作。它对该RDD中的每一条记录都执行size函数。<br>创建一个这样的由若干String构成的RDD对象。通过map函数，我们将每一个字符串都转换为一个整数，从而返回一个由若干Int构成的RDD对象。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; rddFromTextFile.count</span><br><span class="line">res2: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> intsFromStringsRDD = rddFromTextFile.map(line =&gt; line.size)</span><br><span class="line">intsFromStringsRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; intsFromStringsRDD.count</span><br><span class="line">res3: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sumOfRecords = intsFromStringsRDD.sum</span><br><span class="line">sumOfRecords: <span class="type">Double</span> = <span class="number">17062.0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> numRecords = intsFromStringsRDD.count</span><br><span class="line">numRecords: <span class="type">Long</span> = <span class="number">294</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> aveLengthOfRecord = sumOfRecords / numRecords</span><br><span class="line">aveLengthOfRecord: <span class="type">Double</span> = <span class="number">58.034013605442176</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价于</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> aveLengthOfRecordChained = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</span><br></pre></td></tr></table></figure>
<blockquote>
<p>示例中 <strong>=&gt;</strong> 是Scala下表示匿名函数的语法。语法 <strong>line =&gt; line.size</strong> 表示以 <strong>=&gt;</strong> 操作符左边的部分作为输入，对其执行一个函数，并以 <strong>=&gt;</strong> 操作符右边代码的执行结果为输出。在这个例子中，输入为line，输出则是 <strong>line.size</strong> 函数的执行结果。在Scala语言中，这种将一个String对象映射为一个Int的函数被表示为String =&gt; Int。</p>
</blockquote>
<p>Spark的大多数操作都会返回一个新RDD，但多数的Action操作则是返回计算的结果</p>
<blockquote>
<p>注 : Spark 中的转换操作是延后的。也就是说，在RDD上调用一个转换操作并不会立即触发相应的计算。 只有必要时才计算结果并将其返回给驱动程序，从而提高了Spark的效率。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> transformedRDD = rddFromTextFile.map(line =&gt; line.size).</span><br><span class="line">     | filter(size =&gt; size &gt; <span class="number">10</span>).map(size =&gt; size * <span class="number">2</span>)</span><br><span class="line">transformedRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at map at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<p>没有触发任何计算，也没有结果被返回。<br>如果我们现在在新的RDD上调用一个执行操作，比如sum，该计算将会被触发：</p>
<p><strong><em>触发计算</em></strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> computation = transformedRDD.sum</span><br><span class="line">computation: <span class="type">Double</span> = <span class="number">34106.0</span></span><br></pre></td></tr></table></figure>
<p><strong>3. RDD缓存策略</strong></p>
<p>Spark最为强大的功能之一便是能够把数据缓存在集群的内存里。这通过调用RDD的cache函数来实现：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; rddFromTextFile.cache</span><br><span class="line">res4: rddFromTextFile.<span class="keyword">type</span> = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> aveLengthOfRecordChainedFromCached = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</span><br><span class="line">aveLengthOfRecordChainedFromCached: <span class="type">Double</span> = <span class="number">58.034013605442176</span></span><br></pre></td></tr></table></figure>
<p>在RDD首次调用一个执行操作时，这个操作对应的计算会立即执行，数据会从数据源里读出并保存到内存。因此，首次调用cache函数所需要的时间会部分取决于Spark从输入源读取数据所需要的时间。但是，当下一次访问该数据集的时候，数据可以直接从内存中读出从而减少低效的I/O操作，加快计算。多数情况下，这会取得数倍的速度提升。</p>
<blockquote>
<p>Spark支持更为细化的缓存策略。通过persist函数可以指定Spark的数据缓存策略。关于RDD缓存的更多信息可参见：<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence。</a></p>
</blockquote>
<h3 id="3-4-广播变量和累加器"><a href="#3-4-广播变量和累加器" class="headerlink" title="3.4 广播变量和累加器"></a>3.4 广播变量和累加器</h3><p>Spark的另一个核心功能是能创建两种特殊类型的变量：<strong>广播变量</strong> 和 累加器。</p>
<p>广播变量（broadcast variable）为只读变量，它由运行SparkContext的驱动程序创建后发送给会参与计算的节点。对那些需要让各工作节点高效地访问相同数据的应用场景，比如机器学习，这非常有用。</p>
<p>Spark下创建广播变量只需在SparkContext上调用一个方法即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val broadcastAList = sc.broadcast(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;))</span><br><span class="line">broadcastAList: org.apache.spark.broadcast.Broadcast[List[String]] = Broadcast(11)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<p><strong>广播变量</strong> 也可以被非驱动程序所在的节点（即工作节点）访问，访问的方法是调用该变量的<code>value</code>方法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastAList = sc.broadcast(<span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>))</span><br><span class="line">broadcastAList: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">List</span>[<span class="type">String</span>]] = <span class="type">Broadcast</span>(<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(<span class="type">List</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)).map(x =&gt; broadcastAList.value ++ x).collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">List</span>[<span class="type">Any</span>]] = <span class="type">Array</span>(<span class="type">List</span>(a, b, c, d, e, <span class="number">1</span>), <span class="type">List</span>(a, b, c, d, e, <span class="number">2</span>), <span class="type">List</span>(a, b, c, d, e, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，collect 函数一般仅在的确需要将整个结果集返回驱动程序并进行后续处理时才有必要调用。如果在一个非常大的数据集上调用该函数，可能耗尽驱动程序的可用内存，进而导致程序崩溃。</p>
</blockquote>
<p>高负荷的处理应尽可能地在整个集群上进行，从而避免驱动程序成为系统瓶颈。然而在不少情况下，将结果收集到驱动程序的确是有必要的。很多机器学习算法的迭代过程便属于这类情况。</p>
<p><strong>累加器</strong>（accumulator）也是一种被广播到工作节点的变量。累加器与广播变量的关键不同，是后者只能读取而前者却可累加。</p>
<blockquote>
<p>关于累加器的更多信息，可参见《Spark编程指南》：<a href="http://spark.apache.org/docs/latest/programming-guide.html#shared-variables。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#shared-variables。</a></p>
</blockquote>
<h2 id="4-Spark-Scala-编程入门"><a href="#4-Spark-Scala-编程入门" class="headerlink" title="4. Spark Scala 编程入门"></a>4. Spark Scala 编程入门</h2><p><a href="https://github.com/blair1/spark/tree/master/Spark-Machine-Learning_8519OSCode/Chapter%2001/scala-spark-app" target="_blank" rel="external">scala-spark-app</a></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * A simple Spark app in Scala</span><br><span class="line"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local[2]"</span>, <span class="string">"First Spark App"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)</span></span><br><span class="line">    <span class="keyword">val</span> data = sc.textFile(<span class="string">"data/UserPurchaseHistory.csv"</span>)</span><br><span class="line">      .map(line =&gt; line.split(<span class="string">","</span>))</span><br><span class="line">      .map(purchaseRecord =&gt; (purchaseRecord(<span class="number">0</span>), purchaseRecord(<span class="number">1</span>), purchaseRecord(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's count the number of purchases</span></span><br><span class="line">    <span class="keyword">val</span> numPurchases = data.count()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's count how many unique users made purchases</span></span><br><span class="line">    <span class="keyword">val</span> uniqueUsers = data.map &#123; <span class="keyword">case</span> (user, product, price) =&gt; user &#125;.distinct().count()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's sum up our total revenue</span></span><br><span class="line">    <span class="keyword">val</span> totalRevenue = data.map &#123; <span class="keyword">case</span> (user, product, price) =&gt; price.toDouble &#125;.sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// let's find our most popular product</span></span><br><span class="line">    <span class="keyword">val</span> productsByPopularity = data</span><br><span class="line">      .map &#123; <span class="keyword">case</span> (user, product, price) =&gt; (product, <span class="number">1</span>) &#125;</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">      .collect()</span><br><span class="line">      .sortBy(-_._2)</span><br><span class="line">    <span class="keyword">val</span> mostPopular = productsByPopularity(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// finally, print everything out</span></span><br><span class="line">    println(<span class="string">"Total purchases: "</span> + numPurchases)</span><br><span class="line">    println(<span class="string">"Unique users: "</span> + uniqueUsers)</span><br><span class="line">    println(<span class="string">"Total revenue: "</span> + totalRevenue)</span><br><span class="line">    println(<span class="string">"Most popular product: %s with %d purchases"</span>.format(mostPopular._1, mostPopular._2))</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-Spark-Java-编程入门"><a href="#5-Spark-Java-编程入门" class="headerlink" title="5. Spark Java 编程入门"></a>5. Spark Java 编程入门</h2><p>Java API与Scala API本质上很相似。Scala代码可以很方便地调用Java代码，但某些Scala代码却无法在Java里调用，特别是那些使用了隐式类型转换、默认参数和采用了某些Scala反射机制的代码。</p>
<p>SparkContext有了对应的Java版本JavaSparkContext，而RDD则对应JavaRDD。<br>Spark提供对Java 8匿名函数（lambda）语法的支持。</p>
<p>用Scala编写时，键/值对记录的RDD能支持一些特别的操作（比如reduceByKey和saveAsSequenceFile）。这些操作可以通过隐式类型转换而自动被调用。用Java编写时，则需要特别类型的JavaRDD来支持这些操作。它们包括用于键/值对的JavaPairRDD，以及用于数值记录的JavaDoubleRDD。</p>
<p>Java 8 RDD以及Java 8 lambda表达式更多信息可参见《Spark编程指南》：<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations。" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations。</a></p>
<h2 id="6-Spark-Python-编程入门"><a href="#6-Spark-Python-编程入门" class="headerlink" title="6. Spark Python 编程入门"></a>6. Spark Python 编程入门</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="string">"""用Python编写的一个简单Spark应用"""</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line">sc = SparkContext(<span class="string">"local[2]"</span>, <span class="string">"First Spark App"</span>)</span><br><span class="line"><span class="comment"># 将CSV格式的原始数据转化为(user,product,price)格式的记录集</span></span><br><span class="line">data = sc.textFile(<span class="string">"data/UserPurchaseHistory.csv"</span>).map(<span class="keyword">lambda</span> line:</span><br><span class="line">line.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> record: (record[<span class="number">0</span>], record[<span class="number">1</span>], record[<span class="number">2</span>]))</span><br><span class="line"><span class="comment"># 求总购买次数</span></span><br><span class="line">numPurchases = data.count()</span><br><span class="line"><span class="comment"># 求有多少不同客户购买过商品</span></span><br><span class="line">uniqueUsers = data.map(<span class="keyword">lambda</span> record: record[<span class="number">0</span>]).distinct().count()</span><br><span class="line"><span class="comment"># 求和得出总收入</span></span><br><span class="line">totalRevenue = data.map(<span class="keyword">lambda</span> record: float(record[<span class="number">2</span>])).sum()</span><br><span class="line"><span class="comment"># 求最畅销的产品是什么</span></span><br><span class="line">products = data.map(<span class="keyword">lambda</span> record: (record[<span class="number">1</span>], <span class="number">1.0</span>)).</span><br><span class="line">reduceByKey(<span class="keyword">lambda</span> a, b: a + b).collect()</span><br><span class="line">mostPopular = sorted(products, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Total purchases: %d"</span> % numPurchases</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Unique users: %d"</span> % uniqueUsers</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Total revenue: %2.2f"</span> % totalRevenue</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Most popular product: %s with %d purchases"</span> % (mostPopular[<span class="number">0</span>], mostPopular[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>匿名函数在Python语言中亦称lambda函数，lambda也是语法表达上的关键字。</p>
<p>用Scala编写时，一个将输入x映射为输出y的匿名函数表示为x =&gt; y，而在Python中则是lambda x : y。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  python-spark-app git:(master) ✗ <span class="built_in">pwd</span></span><br><span class="line">/Users/hp/ghome/hadoop-spark/spark/Spark-Machine-Learning_8519OSCode/Chapter01/python-spark-app</span><br><span class="line">➜  python-spark-app git:(master) ✗ <span class="variable">$SPARK_HOME</span>/bin/spark-submit pythonapp.py</span><br><span class="line">Using Spark<span class="string">'s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">16/08/26 15:56:02 INFO SparkContext: Running Spark version 1.5.2</span><br><span class="line">...</span><br><span class="line">Total purchases: 5</span><br><span class="line">Unique users: 4</span><br><span class="line">Total revenue: 39.91</span><br><span class="line">Most popular product: iPhone Cover with 2 purchases</span><br><span class="line">16/08/26 15:56:07 INFO SparkUI: Stopped Spark web UI at http://192.168.143.84:4040</span><br><span class="line">...</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Spark的Python API几乎覆盖了所有Scala API所能提供的功能. 但的确有些特性，比如Spark Streaming和个别的API方法，暂不支持。<br><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">具体参见《Spark编程指南》的Python部分</a></p>
</blockquote>
<h2 id="7-小结"><a href="#7-小结" class="headerlink" title="7. 小结"></a>7. 小结</h2><p>体会了 函数式 编程的威力， scala、python 都可以。java 不适合写 spark 程序</p>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      <!--<p>（写这样一篇文章花费了我很多的精力。如果你从中受益，请考虑<a href="/2017/11/05/pay-blog/">付款购买</a>）</p>-->
<!--<br>-->
<div class="well">
  原创文章，转载请注明： 转载自<a href="http://www.iequa.com"> Blair Chan's Blog</a>，作者：
  <a href="http://www.iequa.com/about">Blair Chan</a> <br>
  本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。
</div>
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Spark运行"><span class="toc-number"></span> <span class="toc-text">1. Spark运行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Spark集群"><span class="toc-number"></span> <span class="toc-text">2. Spark集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Spark编程模型"><span class="toc-number"></span> <span class="toc-text">3. Spark编程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-SparkContext类"><span class="toc-number"></span> <span class="toc-text">3.1 SparkContext类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Spark-shell"><span class="toc-number"></span> <span class="toc-text">3.2 Spark shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-RDD"><span class="toc-number"></span> <span class="toc-text">3.3 RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-广播变量和累加器"><span class="toc-number"></span> <span class="toc-text">3.4 广播变量和累加器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Spark-Scala-编程入门"><span class="toc-number"></span> <span class="toc-text">4. Spark Scala 编程入门</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Spark-Java-编程入门"><span class="toc-number"></span> <span class="toc-text">5. Spark Java 编程入门</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Spark-Python-编程入门"><span class="toc-number"></span> <span class="toc-text">6. Spark Python 编程入门</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-小结"><span class="toc-number"></span> <span class="toc-text">7. 小结</span></a></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        
  <div class="article-tag">
    <a class="article-tag-link" href="/tags/spark/">spark</a>
  </div>


      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/05/17/math-distance-formula-of-point-to-plain/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Distance formula of point to plane
        
      </div>
    </a>
  
  
    <a href="/2016/04/19/ops-centos7-install-common-software/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">CentOS 7.1 install Common software&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Libin Chan&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

    
<script type="text/javascript"> <!-- add by blair 0724 type=text/javascript -->
  var disqus_shortname = 'blairos-sn';
  
  var disqus_url = 'http://iequa.com/2016/04/25/spark-machine-learning-p1/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
